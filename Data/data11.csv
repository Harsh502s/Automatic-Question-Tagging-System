Head,Body,Tags,First Answer
"Making sense of principal component analysis, eigenvectors & eigenvalues","
In today's pattern recognition class my professor talked about PCA, eigenvectors and eigenvalues. 
I understood the mathematics of it. If I'm asked to find eigenvalues etc. I'll do it correctly like a machine. But I didn't understand it. I didn't get the purpose of it. I didn't get the feel of it.      
I strongly believe in the following quote:

You do not really understand something unless you can explain it to your grandmother. -- Albert Einstein

Well, I can't explain these concepts to a layman or grandma.

Why PCA, eigenvectors & eigenvalues? What was the need for these concepts?
How would you explain these to a layman?

","['pca', 'intuition', 'eigenvalues', 'faq']","Imagine a big family dinner where everybody starts asking you about PCA. First, you explain it to your great-grandmother; then to your grandmother; then to your mother; then to your spouse; finally, to your daughter (a mathematician). Each time the next person is less of a layman. Here is how the conversation might go.Great-grandmother: I heard you are studying ""Pee-See-Ay"". I wonder what that is...You: Ah, it's just a method of summarizing some data. Look, we have some wine bottles standing here on the table. We can describe each wine by its colour, how strong it is, how old it is, and so on.

Visualization originally found here.We can compose a whole list of different characteristics of each wine in our cellar. But many of them will measure related properties and so will be redundant. If so, we should be able to summarize each wine with fewer characteristics! This is what PCA does.Grandmother: This is interesting! So this PCA thing checks what characteristics are redundant and discards them?You: Excellent question, granny! No, PCA is not selecting some characteristics and discarding the others. Instead, it constructs some new characteristics that turn out to summarize our list of wines well. Of course, these new characteristics are constructed using the old ones; for example, a new characteristic might be computed as wine age minus wine acidity level or some other combination (we call them linear combinations).In fact, PCA finds the best possible characteristics, the ones that summarize the list of wines as well as only possible (among all conceivable linear combinations). This is why it is so useful.Mother: Hmmm, this certainly sounds good, but I am not sure I understand. What do you actually mean when you say that these new PCA characteristics ""summarize"" the list of wines?You: I guess I can give two different answers to this question. The first answer is that you are looking for some wine properties (characteristics) that strongly differ across wines. Indeed, imagine that you come up with a property that is the same for most of the wines - like the stillness of wine after being poured. This would not be very useful, would it? Wines are very different, but your new property makes them all look the same! This would certainly be a bad summary. Instead, PCA looks for properties that show as much variation across wines as possible.The second answer is that you look for the properties that would allow you to predict, or ""reconstruct"", the original wine characteristics. Again, imagine that you come up with a property that has no relation to the original characteristics - like the shape of a wine bottle; if you use only this new property, there is no way you could reconstruct the original ones! This, again, would be a bad summary. So PCA looks for properties that allow reconstructing the original characteristics as well as possible.Surprisingly, it turns out that these two aims are equivalent and so PCA can kill two birds with one stone.Spouse: But darling, these two ""goals"" of PCA sound so different! Why would they be equivalent?You: Hmmm. Perhaps I should make a little drawing (takes a napkin and starts scribbling). Let us pick two wine characteristics, perhaps wine darkness and alcohol content -- I don't know if they are correlated, but let's imagine that they are. Here is what a scatter plot of different wines could look like:Each dot in this ""wine cloud"" shows one particular wine. You see that the two properties ($x$ and $y$ on this figure) are correlated. A new property can be constructed by drawing a line through the centre of this wine cloud and projecting all points onto this line. This new property will be given by a linear combination $w_1 x + w_2 y$, where each line corresponds to some particular values of $w_1$ and $w_2$.Now, look here very carefully -- here is what these projections look like for different lines (red dots are projections of the blue dots):As I said before, PCA will find the ""best"" line according to two different criteria of what is the ""best"". First, the variation of values along this line should be maximal. Pay attention to how the ""spread"" (we call it ""variance"") of the red dots changes while the line rotates; can you see when it reaches maximum? Second, if we reconstruct the original two characteristics (position of a blue dot) from the new one (position of a red dot), the reconstruction error will be given by the length of the connecting red line. Observe how the length of these red lines changes while the line rotates; can you see when the total length reaches minimum?If you stare at this animation for some time, you will notice that ""the maximum variance"" and ""the minimum error"" are reached at the same time, namely when the line points to the magenta ticks I marked on both sides of the wine cloud. This line corresponds to the new wine property that will be constructed by PCA.By the way, PCA stands for ""principal component analysis"", and this new property is called ""first principal component"". And instead of saying ""property"" or ""characteristic"", we usually say ""feature"" or ""variable"".Daughter: Very nice, papa! I think I can see why the two goals yield the same result: it is essentially because of the Pythagoras theorem, isn't it? Anyway, I heard that PCA is somehow related to eigenvectors and eigenvalues; where are they in this picture?You: Brilliant observation. Mathematically, the spread of the red dots is measured as the average squared distance from the centre of the wine cloud to each red dot; as you know, it is called the variance. On the other hand, the total reconstruction error is measured as the average squared length of the corresponding red lines. But as the angle between red lines and the black line is always $90^\circ$, the sum of these two quantities is equal to the average squared distance between the centre of the wine cloud and each blue dot; this is precisely Pythagoras theorem. Of course, this average distance does not depend on the orientation of the black line, so the higher the variance, the lower the error (because their sum is constant). This hand-wavy argument can be made precise (see here).By the way, you can imagine that the black line is a solid rod, and each red line is a spring. The energy of the spring is proportional to its squared length (this is known in physics as Hooke's law), so the rod will orient itself such as to minimize the sum of these squared distances. I made a simulation of what it will look like in the presence of some viscous friction:Regarding eigenvectors and eigenvalues. You know what a covariance matrix is; in my example it is a $2\times 2$ matrix that is given by $$\begin{pmatrix}1.07 &0.63\\0.63 & 0.64\end{pmatrix}.$$ What this means is that the variance of the $x$ variable is $1.07$, the variance of the $y$ variable is $0.64$, and the covariance between them is $0.63$. As it is a square symmetric matrix, it can be diagonalized by choosing a new orthogonal coordinate system, given by its eigenvectors (incidentally, this is called spectral theorem); corresponding eigenvalues will then be located on the diagonal. In this new coordinate system, the covariance matrix is diagonal and looks like that: $$\begin{pmatrix}1.52 &0\\0 & 0.19\end{pmatrix},$$ meaning that the correlation between points is now zero. It becomes clear that the variance of any projection will be given by a weighted average of the eigenvalues (I am only sketching the intuition here). Consequently, the maximum possible variance ($1.52$) will be achieved if we simply take the projection on the first coordinate axis. It follows that the direction of the first principal component is given by the first eigenvector of the covariance matrix. (More details here.)You can see this on the rotating figure as well: there is a gray line there orthogonal to the black one; together, they form a rotating coordinate frame. Try to notice when the blue dots become uncorrelated in this rotating frame. The answer, again, is that it happens precisely when the black line points at the magenta ticks. Now I can tell you how I found them (the magenta ticks): they mark the direction of the first eigenvector of the covariance matrix, which in this case is equal to $(0.81, 0.58)$.Per popular request, I shared the Matlab code to produce the above animations."
How to choose the number of hidden layers and nodes in a feedforward neural network?,"
Is there a standard and accepted method for selecting the number of layers, and the number of nodes in each layer, in a feed-forward neural network? I'm interested in automated ways of building neural networks.
","['model-selection', 'neural-networks']","I realize this question has been answered, but I don't think the extant answer really engages the question beyond pointing to a link generally related to the question's subject matter. In particular, the link describes one technique for programmatic network configuration, but that is not a ""[a] standard and accepted method"" for network configuration.By following a small set of clear rules, one can programmatically set a competent network architecture (i.e., the number and type of neuronal layers and the number of neurons comprising each layer). Following this schema will give you a competent architecture but probably not an optimal one.But once this network is initialized, you can iteratively tune the configuration during training using a number of ancillary algorithms; one family of these works by pruning nodes based on (small) values of the weight vector after a certain number of training epochs--in other words, eliminating unnecessary/redundant nodes (more on this below).So every NN has three types of layers: input, hidden, and output.Creating the NN architecture, therefore, means coming up with values for the number of layers of each type and the number of nodes in each of these layers.The Input LayerSimple--every NN has exactly one of them--no exceptions that I'm aware of.With respect to the number of neurons comprising this layer, this parameter is completely and uniquely determined once you know the shape of your training data. Specifically, the number of neurons comprising that layer is equal to the number of features (columns) in your data. Some NN configurations add one additional node for a bias term.The Output LayerLike the Input layer, every NN has exactly one output layer. Determining its size (number of neurons) is simple; it is completely determined by the chosen model configuration.Is your NN going to run in Machine Mode or Regression Mode (the ML convention of using a term that is also used in statistics but assigning a different meaning to it is very confusing)? Machine mode: returns a class label (e.g., ""Premium Account""/""Basic Account""). Regression Mode returns a value (e.g., price).If the NN is a regressor, then the output layer has a single node.If the NN is a classifier, then it also has a single node unless softmax is used
in which case the output layer has one node per class label in your model.The Hidden LayersSo those few rules set the number of layers and size (neurons/layer) for both the input and output layers. That leaves the hidden layers.How many hidden layers? Well, if your data is linearly separable (which you often know by the time you begin coding a NN), then you don't need any hidden layers at all. Of course, you don't need an NN to resolve your data either, but it will still do the job.Beyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the insanely thorough and insightful NN FAQ for an excellent summary of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very few. One hidden layer is sufficient for the large majority of problems.So what about the size of the hidden layer(s)--how many neurons? There are some empirically derived rules of thumb; of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. Jeff Heaton, the author of Introduction to Neural Networks in Java, offers a few more.In sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) the number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers. Optimization of the Network ConfigurationPruning describes a set of techniques to trim network size (by nodes, not layers) to improve computational performance and sometimes resolution performance. The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look at weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training, then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on network architecture, err on the side of more neurons, if you add a pruning step.Put another way, by applying a pruning algorithm to your network during training, you can approach optimal network configuration; whether you can do that in a single ""up-front"" (such as a genetic-algorithm-based algorithm), I don't know, though I do know that for now, this two-step optimization is more common."
"What is the difference between ""likelihood"" and ""probability""?","
The wikipedia page claims that likelihood and probability are distinct concepts.

In non-technical parlance, ""likelihood"" is usually a synonym for ""probability,"" but in statistical usage there is a clear distinction in perspective: the number that is the probability of some observed outcomes given a set of parameter values is regarded as the likelihood of the set of parameter values given the observed outcomes. 

Can someone give a more down-to-earth description of what this means?  In addition, some examples of how ""probability"" and ""likelihood"" disagree would be nice.
","['probability', 'terminology', 'likelihood', 'intuition']","The answer depends on whether you are dealing with discrete or continuous random variables. So, I will split my answer accordingly. I will assume that you want some technical details and not necessarily an explanation in plain English.Discrete Random VariablesSuppose that you have a stochastic process that takes discrete values (e.g., outcomes of tossing a coin 10 times, number of customers who arrive at a store in 10 minutes etc). In such cases, we can calculate the probability of observing a particular set of outcomes by making suitable assumptions about the underlying stochastic process (e.g., probability of coin landing heads is $p$ and that coin tosses are independent).Denote the observed outcomes by $O$ and the set of parameters that describe the stochastic process as $\theta$. Thus, when we speak of probability we want to calculate $P(O|\theta)$. In other words, given specific values for $\theta$, $P(O|\theta)$ is the probability that we would observe the outcomes represented by $O$.However, when we model a real life stochastic process, we often do not know $\theta$. We simply observe $O$ and the goal then is to arrive at an estimate for $\theta$ that would be a plausible choice given the observed outcomes $O$. We know that given a value of $\theta$ the probability of observing $O$ is $P(O|\theta)$. Thus, a 'natural' estimation process is to choose that value of $\theta$ that would maximize the probability that we would actually observe $O$. In other words, we find the parameter values $\theta$ that maximize the following function:$L(\theta|O) = P(O|\theta)$$L(\theta|O)$ is called the likelihood function. Notice that by definition the likelihood function is conditioned on the observed $O$ and that it is a function of the unknown parameters $\theta$.Continuous Random VariablesIn the continuous case the situation is similar with one important difference. We can no longer talk about the probability that we observed $O$ given $\theta$ because in the continuous case $P(O|\theta) = 0$. Without getting into technicalities, the basic idea is as follows:Denote the probability density function (pdf) associated with the outcomes $O$ as: $f(O|\theta)$. Thus, in the continuous case we estimate $\theta$ given observed outcomes $O$ by maximizing the following function:$L(\theta|O) = f(O|\theta)$In this situation, we cannot technically assert that we are finding the parameter value that maximizes the probability that we observe $O$ as we maximize the PDF associated with the observed outcomes $O$. "
Relationship between SVD and PCA. How to use SVD to perform PCA?,"
Principal component analysis (PCA) is usually explained via an eigen-decomposition of the covariance matrix. However, it can also be performed via singular value decomposition (SVD) of the data matrix $\mathbf X$. How does it work? What is the connection between these two approaches? What is the relationship between SVD and PCA?
Or in other words, how to use SVD of the data matrix to perform dimensionality reduction?
","['pca', 'dimensionality-reduction', 'matrix', 'svd', 'faq']","Let the real values data matrix $\mathbf X$ be of $n \times p$ size, where $n$ is the number of samples and $p$ is the number of variables. Let us assume that it is centered, i.e. column means have been subtracted and are now equal to zero.Then the $p \times p$ covariance matrix $\mathbf C$ is given by $\mathbf C = \mathbf X^\top \mathbf X/(n-1)$. It is a symmetric matrix and so it can be diagonalized: $$\mathbf C = \mathbf V \mathbf L \mathbf V^\top,$$ where $\mathbf V$ is a matrix of eigenvectors (each column is an eigenvector) and $\mathbf L$ is a diagonal matrix with eigenvalues $\lambda_i$ in the decreasing order on the diagonal. The eigenvectors are called principal axes or principal directions of the data. Projections of the data on the principal axes  are called principal components, also known as PC scores; these can be seen as new, transformed, variables. The $j$-th principal component is given by $j$-th column of $\mathbf {XV}$. The coordinates of the $i$-th data point in the new PC space are given by the $i$-th row of $\mathbf{XV}$.If we now perform singular value decomposition of $\mathbf X$, we obtain a decomposition $$\mathbf X = \mathbf U \mathbf S \mathbf V^\top,$$ where $\mathbf U$ is a unitary matrix (with columns called left singular vectors), $\mathbf S$ is the diagonal matrix of singular values $s_i$ and $\mathbf V$ columns are called right singular vectors. From here one can easily see that $$\mathbf C = \mathbf V \mathbf S \mathbf U^\top \mathbf U \mathbf S \mathbf V^\top /(n-1) = \mathbf V \frac{\mathbf S^2}{n-1}\mathbf V^\top,$$ meaning that right singular vectors $\mathbf V$ are principal directions (eigenvectors) and that singular values are related to the eigenvalues of covariance matrix via $\lambda_i = s_i^2/(n-1)$. Principal components are given by $\mathbf X \mathbf V = \mathbf U \mathbf S \mathbf V^\top \mathbf V = \mathbf U \mathbf S$.To summarize:What is the intuitive relationship between SVD and PCA -- a very popular and very similar thread on math.SE.Why PCA of data by means of SVD of the data? -- a discussion of what are the benefits of performing PCA via SVD [short answer: numerical stability].PCA and Correspondence analysis in their relation to Biplot -- PCA in the context of some congeneric techniques, all based on SVD.Is there any advantage of SVD over PCA? -- a question asking if there any benefits in using SVD instead of PCA [short answer: ill-posed question].Making sense of principal component analysis, eigenvectors & eigenvalues -- my answer giving a non-technical explanation of PCA. To draw attention, I reproduce one figure here:"
What is the difference between test set and validation set?,"
I found this confusing when I use the neural network toolbox in Matlab.
It divided the raw data set into three parts:

training set
validation set
test set

I notice in many training or learning algorithm, the data is often divided into 2 parts, the training set and the test set.
My questions are:

what is the difference between validation set and test set? 
Is the validation set really specific to neural network? Or it is optional.
To go further, is there a difference between validation and testing in context of machine learning?

","['machine-learning', 'validation']","Typically to perform supervised learning, you need two types of data sets: In one dataset (your ""gold standard""), you have the input data together with correct/expected output; This dataset is usually duly prepared either by humans or by collecting some data in a semi-automated way. But you must have the expected output for every data row here because you need this for supervised learning.The data you are going to apply your model to. In many cases, this is the data in which you are interested in the output of your model, and thus you don't have any ""expected"" output here yet.While performing machine learning, you do the following:The validation phase is often split into two parts:Hence the separation to 50/25/25.In case if you don't need to choose an appropriate model from several rivaling approaches, you can just re-partition your set that you basically have only training set and test set,  without performing the validation of your trained model. I personally partition them 70/30 then.See also this question."
What is the intuition behind beta distribution?,"
Disclaimer: I'm not a statistician but a software engineer. Most of my knowledge in statistics comes from self-education, thus I still have many gaps in understanding concepts that may seem trivial for other people here. So I would be very thankful if answers included less specific terms and more explanation. Imagine that you are talking to your grandma :)
I'm trying to grasp the nature of beta distribution – what it should be used for and how to interpret it in each case. If we were talking about, say, normal distribution, one could describe it as arrival time of a train: most frequently it arrives just in time, a bit less frequently it is 1 minute earlier or 1 minute late and very rarely it arrives with difference of 20 minutes from the mean. Uniform distribution describes, in particular, chance of each ticket in lottery. Binomial distribution may be described with coin flips and so on. But is there such intuitive explanation of beta distribution?  
Let's say, $\alpha=.99$ and $\beta=.5$. Beta distribution $B(\alpha, \beta)$ in this case looks like this (generated in R): 

But what does it actually mean? Y-axis is obviously a probability density, but what is on the X-axis? 
I would highly appreciate any explanation, either with this example or any other. 
","['distributions', 'beta-distribution', 'intuition', 'beta-binomial-distribution']","The short version is that the Beta distribution can be understood as representing a distribution of probabilities, that is, it represents all the possible values of a probability when we don't know what that probability is. Here is my favorite intuitive explanation of this:Anyone who follows baseball is familiar with batting averages—simply the number of times a player gets a base hit divided by the number of times he goes up at bat (so it's just a percentage between 0 and 1). .266 is in general considered an average batting average, while .300 is considered an excellent one.Imagine we have a baseball player, and we want to predict what his season-long batting average will be. You might say we can just use his batting average so far- but this will be a very poor measure at the start of a season! If a player goes up to bat once and gets a single, his batting average is briefly 1.000, while if he strikes out, his batting average is 0.000. It doesn't get much better if you go up to bat five or six times- you could get a lucky streak and get an average of 1.000, or an unlucky streak and get an average of 0, neither of which are a remotely good predictor of how you will bat that season.Why is your batting average in the first few hits not a good predictor of your eventual batting average? When a player's first at-bat is a strikeout, why does no one predict that he'll never get a hit all season? Because we're going in with prior expectations. We know that in history, most batting averages over a season have hovered between something like .215 and .360, with some extremely rare exceptions on either side. We know that if a player gets a few strikeouts in a row at the start, that might indicate he'll end up a bit worse than average, but we know he probably won't deviate from that range.Given our batting average problem, which can be represented with a binomial distribution (a series of successes and failures), the best way to represent these prior expectations (what we in statistics just call a prior) is with the Beta distribution- it's saying, before we've seen the player take his first swing, what we roughly expect his batting average to be. The domain of the Beta distribution is (0, 1), just like a probability, so we already know we're on the right track, but the appropriateness of the Beta for this task goes far beyond that.We expect that the player's season-long batting average will be most likely around .27, but that it could reasonably range from .21 to .35. This can be represented with a Beta distribution with parameters $\alpha=81$ and $\beta=219$:I came up with these parameters for two reasons:You asked what the x axis represents in a beta distribution density plot—here it represents his batting average. Thus notice that in this case, not only is the y-axis a probability (or more precisely a probability density), but the x-axis is as well (batting average is just a probability of a hit, after all)! The Beta distribution is representing a probability distribution of probabilities.But here's why the Beta distribution is so appropriate. Imagine the player gets a single hit. His record for the season is now 1 hit; 1 at bat. We have to then update our probabilities- we want to shift this entire curve over just a bit to reflect our new information. While the math for proving this is a bit involved (it's shown here), the result is very simple. The new Beta distribution will be:$\mbox{Beta}(\alpha_0+\mbox{hits}, \beta_0+\mbox{misses})$Where $\alpha_0$ and $\beta_0$ are the parameters we started with- that is, 81 and 219. Thus, in this case, $\alpha$  has increased by 1 (his one hit), while $\beta$ has not increased at all (no misses yet). That means our new distribution is $\mbox{Beta}(81+1, 219)$, or:Notice that it has barely changed at all- the change is indeed invisible to the naked eye! (That's because one hit doesn't really mean anything).However, the more the player hits over the course of the season, the more the curve will shift to accommodate the new evidence, and furthermore the more it will narrow based on the fact that we have more proof. Let's say halfway through the season he has been up to bat 300 times, hitting 100 out of those times. The new distribution would be $\mbox{Beta}(81+100, 219+200)$, or:Notice the curve is now both thinner and shifted to the right (higher batting average) than it used to be- we have a better sense of what the player's batting average is.One of the most interesting outputs of this formula is the expected value of the resulting Beta distribution, which is basically your new estimate. Recall that the expected value of the Beta distribution is $\frac{\alpha}{\alpha+\beta}$. Thus, after 100 hits of 300 real at-bats, the expected value of the new Beta distribution is $\frac{81+100}{81+100+219+200}=.303$- notice that it is lower than the naive estimate of $\frac{100}{100+200}=.333$, but higher than the estimate you started the season with ($\frac{81}{81+219}=.270$). You might notice that this formula is equivalent to adding a ""head start"" to the number of hits and non-hits of a player- you're saying ""start him off in the season with 81 hits and 219 non hits on his record"").Thus, the Beta distribution is best for representing a probabilistic distribution of probabilities: the case where we don't know what a probability is in advance, but we have some reasonable guesses."
Why square the difference instead of taking the absolute value in standard deviation?,"
In the definition of standard deviation, why do we have to square the difference from the mean to get the mean (E) and take the square root back at the end? Can't we just simply take the absolute value of the difference instead and get the expected value (mean) of those, and wouldn't that also show the variation of the data? The number is going to be different from square method (the absolute-value method will be smaller), but it should still show the spread of data. Anybody know why we take this square approach as a standard?
The definition of standard deviation:
$\sigma = \sqrt{E\left[\left(X - \mu\right)^2\right]}.$
Can't we just take the absolute value instead and still be a good measurement?
$\sigma = E\left[|X - \mu|\right]$
","['standard-deviation', 'definition', 'absolute-value', 'faq']","If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.The benefits of squaring include:Squaring however does have a problem as a measure of spread and that is that the units are all squared, whereas we might prefer the spread to be in the same units as the original data (think of squared pounds, squared dollars, or squared apples). Hence the square root allows us to return to the original units.I suppose you could say that absolute difference assigns equal weight to the spread of data whereas squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)It is important to note however that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical threshold for $p$-values, when in fact it is situation dependent). Indeed, there are in fact several competing methods for measuring spread.My view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: $c = \sqrt{a^2 + b^2}$  …this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference which I mostly only use as a memory aid, feel free to ignore this paragraph.An interesting analysis can be read here:"
The Two Cultures: statistics vs. machine learning?,"
Last year, I read a blog post from Brendan O'Connor entitled ""Statistics vs. Machine Learning, fight!"" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:
Simon Blomberg: 

From R's fortunes
  package: To paraphrase provocatively,
  'machine learning is statistics minus
  any checking of models and
  assumptions'.
  -- Brian D. Ripley (about the difference between machine learning
  and statistics) useR! 2004, Vienna
  (May 2004) :-) Season's Greetings!

Andrew Gelman:

In that case, maybe we should get rid
  of checking of models and assumptions
  more often. Then maybe we'd be able to
  solve some of the problems that the
  machine learning people can solve but
  we can't!

There was also the ""Statistical Modeling: The Two Cultures"" paper by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the predictive accuracy of models.
Has the statistics field changed over the last decade in response to these critiques?  Do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?
","['machine-learning', 'pac-learning']",
How to understand the drawbacks of K-means,"
K-means is a widely used method in cluster analysis. In my understanding, this method does NOT require ANY assumptions, i.e., give me a dataset and a pre-specified number of clusters, k, and I just apply this algorithm which minimizes the sum of squared errors (SSE), the within cluster squared error.
So k-means is essentially an optimization problem.
I read some material about the drawbacks of k-means.  Most of them say that:

k-means assumes the variance of the distribution of each attribute (variable) is spherical; 
all variables have the same variance;
the prior probability for all k clusters is the same, i.e., each cluster has roughly equal number of observations;

If any one of these 3 assumptions are violated, then k-means will fail.
I could not understand the logic behind this statement. I think the k-means method makes essentially no assumptions, it just minimizes the SSE, so I cannot see the link between minimizing the SSE and those 3 ""assumptions"".
","['machine-learning', 'clustering', 'data-mining', 'k-means']","While I like David Robinson's answer here a lot, here's some additional critique of k-means. Run k-means on uniform data,  and you will still get clusters! It doesn't tell you when the data just does not cluster, and can take your research into a dead end this way.Rescaling your datasets will completely change results. While this itself is not bad, not realizing that you have to spend extra attention to scaling your data is bad. Scaling factors are extra $d$ hidden parameters in k-means that ""default"" to 1 and thus are easily overlooked, yet have a major impact (but of course this applies to many other algorithms, too).This is probably what you referred to as ""all variables have the same variance"".
Except that ideally, you would also consider non-linear scaling when appropriate.Also be aware that it is only a heuristic to scale every axis to have unit variance. This doesn't ensure that k-means works. Scaling depends on the meaning of your data set. And if you have more than one cluster, you would want every cluster (independently) to have the same variance in every variable, too.Here is a classic counterexample of data sets that k-means cannot cluster. Both axes are i.i.d. in each cluster, so it would be sufficient to do this in 1 dimension. But the clusters have varying variances, and k-means thus splits them incorrectly.I don't think this counterexample for k-means is covered by your points:Yet, k-means still fails badly (and it gets worse if I increase the variance beyond 0.5 for the larger cluster) But: it is not the algorithm that failed. It's the assumptions, which don't hold. K-means is working perfectly, it's just optimizing the wrong criterion.Below is the best of 10 runs of k-means on the classic A3 data set. This is a synthetic data set, designed for k-means. 50 clusters, each of Gaussian shape, reasonably well separated. Yet, it only with k-means++ and 100 iterations I did get the expected result... (below is 10 iterations of regular k-means, for illustration).You'll quickly find many clusters in this data set, where k-means failed to find the correct structure. For example in the bottom right, a cluster was broken into three parts. But there is no way, k-means is going to move one of these centroids to an entirely different place of the data set - it's trapped in a local minimum (and this already was the best of 10 runs!)And there are many of such local minima in this data set. Very often when you get two samples from the same cluster, it will get stuck in a minimum where this cluster remains split, and two other clusters merged instead. Not always, but very often. So you need a lot of iterations to have a lucky pick. With 100 iterations of k-means, I still counted 6 errors, and with 1000 iterations I got this down to 4 errors. K-means++ by the way it weights the random samples, works much better on this data set.While you can run k-means on binary data (or one-hot encoded categorical data) the results will not be binary anymore. So you do get a result out, but you may be unable to interpret it in the end, because it has a different data type than your original data.This is essentially already present in above answer, nicely demonstrated with linear regression. There are some use cases where k-means makes perfect sense. When Lloyd had to decode PCM signals, he did know the number of different tones, and least squared error minimizes the chance of decoding errors. And in color quantization of imaged, you do minimize color error when reducing the palette, too. But on your data, is the sum of squared deviations a meaningful criterion to minimize? In above counterexample, the variance is not worth minimizing, because it depends on the cluster. Instead, a Gaussian Mixture Model should be fit to the data, as in the figure below:(But this is not the ultimate method either. It's just as easy to construct data that does not satisfy the ""mixture of k Gaussian distributions"" assumptions, e.g., by adding a lot of background noise)All in all, it's too easy to throw k-means on your data, and nevertheless get a result out (that is pretty much random, but you won't notice). I think it would be better to have a method which can fail if you haven't understood your data... If you want a theoretical model of what k-means does, consider it a quantization approach, not a clustering algorithm.The objective of k-means - minimizing the squared error - is a reasonable choice if you replace every object by its nearest centroid. (It makes a lot less sense if you inspect the groups original data IMHO.)There are very good use cases for this. The original PCM use case of Lloyd comes to mind, or
e.g. color quanization (Wikipedia). If you want to reduce an image to k colors, you do want to replace every pixel with the nearest centroid. Minimizing the squared color deviation then does measure L2 optimality in image approximation using $k$ colors only.This quantization is probably quite similar to the linear regression example. Linear regression finds the best linear model. And k-means finds (sometimes) the best reduction to k values of a multidimensional data set. Where ""best"" is the least squared error.IMHO, k-means is a good quantization algorithm (see the first image in this post - if you want to approximate the data set to two points, this is a reasonable choice!). If you want to do cluster analysis as in discover structure then k-means is IMHO not the best choice. It tends to cluster when there are not clusters, and it cannot recognize various structures you do see a lot in data.Fine print: all images were generated with ELKI. Data were generated using the .xml data generation format, but they are so basic it is not worth sharing them."
Bayesian and frequentist reasoning in plain English,"
How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?
","['bayesian', 'frequentist']",
"What is the difference between fixed effect, random effect and mixed effect models?","
In simple terms, how would you explain (perhaps with simple examples) the difference between fixed effect, random effect and mixed effect models? 
","['mixed-model', 'random-effects-model', 'definition', 'fixed-effects-model']","Statistician Andrew Gelman says that the terms 'fixed effect' and 'random effect' have variable meanings depending on who uses them. Perhaps you can pick out which one of the 5 definitions applies to your case. In general it may be better to either look for equations which describe the probability model the authors are using (when reading) or write out the full probability model you want to use (when writing).Here we outline five definitions that we have seen:Fixed effects are constant across individuals, and random effects vary. For example, in a growth study, a model with random intercepts $a_i$ and fixed slope $b$ corresponds to parallel lines for different individuals $i$, or the model $y_{it} = a_i + b t$. Kreft and De Leeuw (1998) thus distinguish between fixed and random coefficients.Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population. Searle, Casella, and McCulloch (1992, Section 1.4) explore this distinction in depth.“When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random.” (Green and Tukey, 1960)“If an effect is assumed to be a realized value of a random variable, it is called a random effect.” (LaMotte, 1983)Fixed effects are estimated using least squares (or, more generally, maximum likelihood) and random effects are estimated with shrinkage (“linear unbiased prediction” in the terminology of Robinson, 1991). This definition is standard in the multilevel modeling literature (see, for example, Snijders and Bosker, 1999, Section 4.2) and in econometrics.[Gelman, 2004, Analysis of variance—why it is more important than ever. The Annals of Statistics.]"
Explaining to laypeople why bootstrapping works ,"
I recently used bootstrapping to estimate confidence intervals for a project. Someone who doesn't know much about statistics recently asked me to explain why bootstrapping works, i.e., why is it that resampling the same sample over and over gives good results. I realized that although I'd spent a lot of time understanding how to use it, I don't really understand why bootstrapping works.
Specifically: if we are resampling from our sample, how is it that we are learning something about the population rather than only about the sample? There seems to be a leap there which is somewhat counter-intuitive.
I have found a few answers to this question here which I half-understand. Particularly this one. I am a ""consumer"" of statistics, not a statistician, and I work with people who know much less about statistics than I do. So, can someone explain, with a minimum of references to theorems, etc., the basic reasoning behind the bootstrap? That is, if you had to explain it to your neighbor, what would you say?
","['bootstrap', 'intuition', 'communication']","fwiw the medium length version I usually give goes like this:You want to ask a question of a population but you can't.  So you take a sample and ask the question of it instead.  Now, how confident you should be that the sample answer is close to the population answer obviously depends on the structure of population.  One way you might learn about this is to take samples from the population again and again, ask them the question, and see how variable the sample answers tended to be.  Since this isn't possible you can either make some assumptions about the shape of the population, or you can use the information in the sample you actually have to learn about it.  Imagine you decide to make assumptions, e.g. that it is Normal, or Bernoulli or some other convenient fiction.  Following the previous strategy you could again learn about how much the answer to your question when asked of a sample might vary depending on which particular sample you happened to get by repeatedly generating samples of the same size as the one you have and asking them the same question.  That would be straightforward to the extent that you chose computationally convenient assumptions.  (Indeed particularly convenient assumptions plus non-trivial math may allow you to bypass the sampling part altogether, but we will deliberately ignore that here.)This seems like a good idea provided you are happy to make the assumptions.  Imagine you are not.  An alternative is to take the sample you have and sample from it instead.  You can do this because the sample you have is also a population, just a very small discrete one; it looks like the histogram of your data.  Sampling 'with replacement' is just a convenient way to treat the sample like it's a population and to sample from it in a way that reflects its shape.  This is a reasonable thing to do because not only is the sample you have the best, indeed the only information you have about what the population actually looks like, but also because most samples will, if they're randomly chosen, look quite like the population they came from.  Consequently it is likely that yours does too.For intuition it is important to think about how you could learn about variability by aggregating sampled information that is generated in various ways and on various assumptions.  Completely ignoring the possibility of closed form mathematical solutions is important to get clear about this."
"When conducting multiple regression, when should you center your predictor variables & when should you standardize them?","
In some literature, I have read that a regression with multiple explanatory variables, if in different units, needed to be standardized.  (Standardizing consists in subtracting the mean and dividing by the standard deviation.)  In which other cases do I need to standardize my data?  Are there cases in which I should only center my data (i.e., without dividing by standard deviation)? 
","['multiple-regression', 'standardization', 'centering']",
What happens if the explanatory and response variables are sorted independently before regression?,"
Suppose we have data set $(X_i,Y_i)$ with $n$ points. We want to perform a linear regression, but first we sort the $X_i$ values and the $Y_i$ values independently of each other, forming data set $(X_i,Y_j)$. Is there any meaningful interpretation of the regression on the new data set? Does this have a name?
I imagine this is a silly question so I apologize, I'm not formally trained in statistics. In my mind this completely destroys our data and the regression is meaningless. But my manager says he gets ""better regressions most of the time"" when he does this (here ""better"" means more predictive). I have a feeling he is deceiving himself.
EDIT: Thank you for all of your nice and patient examples. I showed him the examples by @RUser4512 and @gung and he remains staunch. He's becoming irritated and I'm becoming exhausted. I feel crestfallen. I will probably begin looking for other jobs soon.
","['regression', 'correlation']","I'm not sure what your boss thinks ""more predictive"" means.  Many people incorrectly believe that lower $p$-values mean a better / more predictive model.  That is not necessarily true (this being a case in point).  However, independently sorting both variables beforehand will guarantee a lower $p$-value.  On the other hand, we can assess the predictive accuracy of a model by comparing its predictions to new data that were generated by the same process.  I do that below in a simple example (coded with R).  The upper left plot shows the original data.  There is some relationship between $x$ and $y$ (viz., the correlation is about $.31$.)  The upper right plot shows what the data look like after independently sorting both variables.  You can easily see that the strength of the correlation has increased substantially (it is now about $.99$).  However, in the lower plots, we see that the distribution of predictive errors is much closer to $0$ for the model trained on the original (unsorted) data.  The mean absolute predictive error for the model that used the original data is $1.1$, whereas the mean absolute predictive error for the model trained on the sorted data is $1.98$—nearly twice as large.  That means the sorted data model's predictions are much further from the correct values.  The plot in the lower right quadrant is a dot plot.  It displays the differences between the predictive error with the original data and with the sorted data.  This lets you compare the two corresponding predictions for each new observation simulated.  Blue dots to the left are times when the original data were closer to the new $y$-value, and red dots to the right are times when the sorted data yielded better predictions.  There were more accurate predictions from the model trained on the original data $68\%$ of the time.  The degree to which sorting will cause these problems is a function of the linear relationship that exists in your data.  If the correlation between $x$ and $y$ were $1.0$ already, sorting would have no effect and thus not be detrimental.  On the other hand, if the correlation were $-1.0$, the sorting would completely reverse the relationship, making the model as inaccurate as possible.  If the data were completely uncorrelated originally, the sorting would have an intermediate, but still quite large, deleterious effect on the resulting model's predictive accuracy.  Since you mention that your data are typically correlated, I suspect that has provided some protection against the harms intrinsic to this procedure.  Nonetheless, sorting first is definitely harmful.  To explore these possibilities, we can simply re-run the above code with different values for B1 (using the same seed for reproducibility) and examine the output:  B1 = -5:  B1 = 0:  B1 = 5:  "
How to normalize data to 0-1 range?,"
I am lost in normalizing, could anyone guide me please.
I have a minimum and maximum values, say -23.89 and 7.54990767, respectively.
If I get a value of 5.6878 how can I scale this value on a scale of 0 to 1.
",['normalization'],"If you want to normalize your data, you can do so as you suggest and simply calculate the following:$$z_i=\frac{x_i-\min(x)}{\max(x)-\min(x)}$$where $x=(x_1,...,x_n)$ and $z_i$ is now your $i^{th}$ normalized data.  As a proof of concept (although you did not ask for it) here is some R code and accompanying graph to illustrate this point:"
Difference between logit and probit models,"
What is the difference between Logit and Probit model?
I'm more interested here in knowing when to use logistic regression, and when to use Probit.
If there is any literature which defines it using R, that would be helpful as well.
","['r', 'generalized-linear-model', 'logistic', 'probit', 'link-function']","They mainly differ in the link function.In Logit:
$\Pr(Y=1 \mid X) = [1 + e^{-X'\beta}]^{-1} $In Probit:
$\Pr(Y=1 \mid X) = \Phi(X'\beta)$   (Cumulative standard normal pdf)In other way, logistic has slightly flatter tails. i.e the probit curve approaches the axes more quickly than the logit curve.Logit has easier interpretation than probit. Logistic regression can be interpreted as modelling log odds (i.e those who smoke >25 cigarettes a day are 6 times more likely to die before 65 years of age). Usually people start the modelling with logit. You could use the likelihood value of each model to decide for logit vs probit."
"What is your favorite ""data analysis"" cartoon?","
Data analysis cartoons can be useful for many reasons: they help communicate; they show that quantitative people have a sense of humor too; they can instigate good teaching moments; and they can help us remember important principles and lessons.
This is one of my favorites:

As a service to those who value this kind of resource, please share your favorite data analysis cartoon.  They probably don't need any explanation (if they do, they're probably not good cartoons!)  As always, one entry per answer. (This is in the vein of the Stack Overflow question What’s your favorite “programmer” cartoon?.)
P.S. Do not hotlink the cartoon without the site's permission please.
","['references', 'teaching', 'humor']",
Python as a statistics workbench,"
Lots of people use a main tool like Excel or another spreadsheet, SPSS, Stata, or R for their statistics needs. They might turn to some specific package for very special needs, but a lot of things can be done with a simple spreadsheet or a general stats package or stats programming environment.
I've always liked Python as a programming language, and for simple needs, it's easy to write a short program that calculates what I need. Matplotlib allows me to plot it.
Has anyone switched completely from, say R, to Python? R (or any other statistics package) has a lot of functionality specific to statistics, and it has data structures that allow you to think about the statistics you want to perform and less about the internal representation of your data. Python (or some other dynamic language) has the benefit of allowing me to program in a familiar, high-level language, and it lets me programmatically interact with real-world systems in which the data resides or from which I can take measurements. But I haven't found any Python package that would allow me to express things with ""statistical terminology"" – from simple descriptive statistics to more complicated multivariate methods.
What can you recommend if I wanted to use Python as a ""statistics workbench"" to replace R, SPSS, etc.?
What would I gain and lose, based on your experience?
","['r', 'spss', 'stata', 'python']","It's hard to ignore the wealth of statistical packages available in R/CRAN.  That said, I spend a lot of time in Python land and would never dissuade anyone from having as much fun as I do.  :)  Here are some libraries/links you might find useful for statistical work.  NumPy/Scipy You probably know about these already.  But let me point out the Cookbook where you can read about many statistical facilities already available and the Example List which is a great reference for functions (including data manipulation and other operations).  Another handy reference is John Cook's Distributions in Scipy.pandas This is a really nice library for working with statistical data -- tabular data, time series, panel data.  Includes many builtin functions for data summaries, grouping/aggregation, pivoting.  Also has a statistics/econometrics library.larry  Labeled array that plays nice with NumPy.  Provides statistical functions not present in NumPy and good for data manipulation.python-statlib A fairly recent effort which combined a number of scattered statistics libraries.  Useful for basic and descriptive statistics if you're not using NumPy or pandas.statsmodels Statistical modeling: Linear models, GLMs, among others.  scikits  Statistical and scientific computing packages -- notably smoothing, optimization and machine learning.PyMC For your Bayesian/MCMC/hierarchical modeling needs. Highly recommended.PyMix Mixture models.Biopython Useful for loading your biological data into python, and provides some rudimentary statistical/ machine learning tools for analysis.If speed becomes a problem, consider Theano -- used with good success by the deep learning people.There's plenty of other stuff out there, but this is what I find the most useful along the lines you mentioned."
What is the trade-off between batch size and number of iterations to train a neural network?,"
When training a neural network, what difference does it make to set:

batch size to $a$ and number of iterations to $b$
vs. batch size to $c$ and number of iterations to $d$

where $ ab = cd $?
To put it otherwise, assuming that we train the neural network with the same amount of training examples, how to set the optimal batch size and number of iterations? (where batch size * number of iterations = number of training examples shown to the neural network, with the same training example being potentially shown several times)
I am aware that the higher the batch size, the more memory space one needs, and it often makes computations faster. But in terms of performance of the trained network, what difference does it make?
","['neural-networks', 'train']",
Is normality testing 'essentially useless'?,"
A former colleague once argued to me as follows: 

We usually apply normality tests to the results of processes that,
  under the null, generate random variables that are only
  asymptotically or nearly normal (with the 'asymptotically' part dependent on some quantity which we cannot make large); In the era of
  cheap memory, big data, and fast processors, normality tests should
  always reject the null of normal distribution for large (though not insanely large) samples. And so, perversely, normality tests should
  only be used for small samples, when they presumably have lower power
  and less control over type I rate.

Is this a valid argument? Is this a well-known argument? Are there well known tests for a 'fuzzier' null hypothesis than normality?
","['hypothesis-testing', 'normality-assumption', 'philosophical']","It's not an argument. It is a (a bit strongly stated) fact that formal normality tests always reject on the huge sample sizes we work with today. It's even easy to prove that when n gets large, even the smallest deviation from perfect normality will lead to a significant result. And as every dataset has some degree of randomness, no single dataset will be a perfectly normally distributed sample. But in applied statistics the question is not whether the data/residuals ... are perfectly normal, but normal enough for the assumptions to hold.Let me illustrate with the Shapiro-Wilk test. The code below constructs a set of distributions that approach normality but aren't completely normal. Next, we test with shapiro.test whether a sample from these almost-normal distributions deviate from normality. In R:The last line checks which fraction of the simulations for every sample size deviate significantly from normality. So in 87% of the cases, a sample of 5000 observations deviates significantly from normality according to Shapiro-Wilks. Yet, if you see the qq plots, you would never ever decide on a deviation from normality. Below you see as an example the qq-plots for one set of random samples with p-values"
What should I do when my neural network doesn't learn?,"
I'm training a neural network but the training loss doesn't decrease. How can I fix this?
I'm not asking about overfitting or regularization. I'm asking about how to solve the problem where my network's performance doesn't improve on the training set.

This question is intentionally general so that other questions about how to train a neural network can be closed as a duplicate of this one, with the attitude that ""if you give a man a fish you feed him for a day, but if you teach a man to fish, you can feed him for the rest of his life."" See this Meta thread for a discussion: What's the best way to answer ""my neural network doesn't work, please fix"" questions?
If your neural network does not generalize well, see: What should I do when my neural network doesn't generalize well?
","['neural-networks', 'faq']","There's a saying among writers that ""All writing is re-writing"" -- that is, the greater part of writing is revising. For programmers (or at least data scientists) the expression could be re-phrased as ""All coding is debugging.""Any time you're writing code, you need to verify that it works as intended. The best method I've ever found for verifying correctness is to break your code into small segments, and verify that each segment works. This can be done by comparing the segment output to what you know to be the correct answer. This is called unit testing. Writing good unit tests is a key piece of becoming a good statistician/data scientist/machine learning expert/neural network practitioner. There is simply no substitute.You have to check that your code is free of bugs before you can tune network performance! Otherwise, you might as well be re-arranging deck chairs on the RMS Titanic.There are two features of neural networks that make verification even more important than for other types of machine learning or statistical models.Neural networks are not ""off-the-shelf"" algorithms in the way that random forest or logistic regression are. Even for simple, feed-forward networks, the onus is largely on the user to make numerous decisions about how the network is configured, connected, initialized and optimized. This means writing code, and writing code means debugging.Even when a neural network code executes without raising an exception, the network can still have bugs! These bugs might even be the insidious kind for which the network will train, but get stuck at a sub-optimal solution, or the resulting network does not have the desired architecture. (This is an example of the difference between a syntactic and semantic error.)This Medium post, ""How to unit test machine learning code,"" by Chase Roberts discusses unit-testing for machine learning models in more detail. I borrowed this example of buggy code from the article:Do you see the error? Many of the different operations are not actually used because previous results are over-written with new variables. Using this block of code in a network will still train and the weights will update and the loss might even decrease -- but the code definitely isn't doing what was intended.  (The author is also inconsistent about using single- or double-quotes but that's purely stylistic.)The most common programming errors pertaining to neural networks areUnit testing is not just limited to the neural network itself. You need to test all of the steps that produce or transform data and feed into the network. Some common mistakes here areThe scale of the data can make an enormous difference on training. Sometimes, networks simply won't reduce the loss if the data isn't scaled. Other networks will decrease the loss, but only very slowly. Scaling the inputs (and certain times, the targets) can dramatically improve the network's training.Data normalization and standardization in neural networksWide and deep neural networks, and neural networks with exotic wiring, are the Hot Thing right now in machine learning. But these networks didn't spring fully-formed into existence; their designers built up to them from smaller units. First, build a small network with a single hidden layer and verify that it works correctly. Then incrementally add additional model complexity, and verify that each of those works as well.Too few neurons in a layer can restrict the representation that the network learns, causing under-fitting. Too many neurons can cause over-fitting because the network will ""memorize"" the training data.Even if you can prove that there is, mathematically, only a small number of neurons necessary to model a problem, it is often the case that having ""a few more"" neurons makes it easier for the optimizer to find a ""good"" configuration. (But I don't think anyone fully understands why this is the case.) I provide an example of this in the context of the XOR problem here: Aren't my iterations needed to train NN for XOR with MSE < 0.001 too high?.Choosing the number of hidden layers lets the network learn an abstraction from the raw data. Deep learning is all the rage these days, and networks with a large number of layers have shown impressive results. But adding too many hidden layers can make risk overfitting or make it very hard to optimize the network.Choosing a clever network wiring can do a lot of the work for you. Is your data source amenable to specialized network architectures? Convolutional neural networks can achieve impressive results on ""structured"" data sources, image or audio data. Recurrent neural networks can do well on sequential data types, such as natural language or time series data. Residual connections can improve deep feed-forward networks.To achieve state of the art, or even merely good, results, you have to set up all of the parts configured to work well together. Setting up a neural network configuration that actually learns is a lot like picking a lock: all of the pieces have to be lined up just right. Just as it is not sufficient to have a single tumbler in the right place, neither is it sufficient to have only the architecture, or only the optimizer, set up correctly.Tuning configuration choices is not really as simple as saying that one kind of configuration choice (e.g. learning rate) is more or less important than another (e.g. number of units), since all of these choices interact with all of the other choices, so one choice can do well in combination with another choice made elsewhere.This is a non-exhaustive list of the configuration options which are not also regularization options or numerical optimization options.All of these topics are active areas of research.The network initialization is often overlooked as a source of neural network bugs. Initialization over too-large an interval can set initial weights too large, meaning that single neurons have an outsize influence over the network behavior.The key difference between a neural network and a regression model is that a neural network is a composition of many nonlinear functions, called activation functions. (See: What is the essential difference between neural network and linear regression)Classical neural network results focused on sigmoidal activation functions (logistic or $\tanh$ functions). A recent result has found that ReLU (or similar) units tend to work better because the have steeper gradients, so updates can be applied quickly. (See: Why do we use ReLU in neural networks and how do we use it?) One caution about ReLUs is the ""dead neuron"" phenomenon, which can stymie learning; leaky relus and similar variants avoid this problem. SeeWhy can't a single ReLU learn a ReLU?My ReLU network fails to launchThere are a number of other options. See: Comprehensive list of activation functions in neural networks with pros/consThe objective function of a neural network is only convex when there are no hidden units, all activations are linear, and the design matrix is full-rank -- because this configuration is identically an ordinary regression problem.In all other cases, the optimization problem is non-convex, and non-convex optimization is hard. The challenges of training neural networks are well-known (see: Why is it hard to train deep neural networks?). Additionally, neural networks have a very large number of parameters, which restricts us to solely first-order methods (see: Why is Newton's method not widely used in machine learning?). This is a very active area of research.Setting the learning rate too large will cause the optimization to diverge, because you will leap from one side of the ""canyon"" to the other. Setting this too small will prevent you from making any real progress, and possibly allow the noise inherent in SGD to overwhelm your gradient estimates. See:Gradient clipping re-scales the norm of the gradient if it's above some threshold. I used to think that this was a set-and-forget parameter, typically at 1.0, but I found that I could make an LSTM language model dramatically better by setting it to 0.25. I don't know why that is.Learning rate scheduling can decrease the learning rate over the course of training. In my experience, trying to use scheduling is a lot like regex: it replaces one problem (""How do I get learning to continue after a certain epoch?"") with two problems (""How do I get learning to continue after a certain epoch?"" and ""How do I choose a good schedule?""). Other people insist that scheduling is essential. I'll let you decide.Choosing a good minibatch size can influence the learning process indirectly, since a larger mini-batch will tend to have a smaller variance (law-of-large-numbers) than a smaller mini-batch. You want the mini-batch to be large enough to be informative about the direction of the gradient, but small enough that SGD can regularize your network.There are a number of variants on stochastic gradient descent which use momentum, adaptive learning rates, Nesterov updates and so on to improve upon vanilla SGD. Designing a better optimizer is very much an active area of research. Some examples:When it first came out, the Adam optimizer generated a lot of interest. But some recent research has found that SGD with momentum can out-perform adaptive gradient methods for neural networks. ""The Marginal Value of Adaptive Gradient Methods in Machine Learning"" by Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin RechtBut on the other hand, this very recent paper proposes a new adaptive learning-rate optimizer which supposedly closes the gap between adaptive-rate methods and SGD with momentum. ""Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks"" by Jinghui Chen, Quanquan GuAdaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, have been observed to generalize worse than stochastic gradient descent (SGD) with momentum in training deep neural networks. This leaves how to close the generalization gap of adaptive gradient methods an open problem. In this work, we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes ""over adapted"". We design a new algorithm, called Partially adaptive momentum estimation method (Padam), which unifies the Adam/Amsgrad with SGD to achieve the best from both worlds. Experiments on standard benchmarks show that Padam can maintain fast convergence rate as Adam/Amsgrad while generalizing as well as SGD in training deep neural networks. These results would suggest practitioners pick up adaptive gradient methods once again for faster training of deep neural networks.Specifically for triplet-loss models, there are a number of tricks which can improve training time and generalization. See: In training a triplet network, I first have a solid drop in loss, but eventually the loss slowly but consistently increases. What could cause this?Choosing and tuning network regularization is a key part of building a model that generalizes well (that is, a model that is not overfit to the training data). However, at the time that your network is struggling to decrease the loss on the training data -- when the network is not learning -- regularization can obscure what the problem is.When my network doesn't learn, I turn off all regularization and verify that the non-regularized network works correctly. Then I add each regularization piece back, and verify that each of those works along the way.This tactic can pinpoint where some regularization might be poorly set. Some examples are$L^2$ regularization (aka weight decay) or $L^1$ regularization is set too large, so the weights can't move.Two parts of regularization are in conflict. For example, it's widely observed that layer normalization and dropout are difficult to use together. Since either on its own is very useful, understanding how to use both is an active area of research.When I set up a neural network, I don't hard-code any parameter settings. Instead, I do that in a configuration file (e.g., JSON) that is read and used to populate network configuration details at runtime. I keep all of these configuration files. If I make any parameter modification, I make a new configuration file. Finally, I append as comments all of the per-epoch losses for training and validation.The reason that I'm so obsessive about retaining old results is that this makes it very easy to go back and review previous experiments. It also hedges against mistakenly repeating the same dead-end experiment. Psychologically, it also lets you look back and observe ""Well, the project might not be where I want it to be today, but I am making progress compared to where I was $k$ weeks ago.""As an example, I wanted to learn about LSTM language models, so I decided to make a Twitter bot that writes new tweets in response to other Twitter users. I worked on this in my free time, between grad school and my job. It took about a year, and I iterated over about 150 different models before getting to a model that did what I wanted: generate new English-language text that (sort of) makes sense. (One key sticking point, and part of the reason that it took so many attempts, is that it was not sufficient to simply get a low out-of-sample loss, since early low-loss models had managed to memorize the training data, so it was just reproducing germane blocks of text verbatim in reply to prompts -- it took some tweaking to make the model more spontaneous and still have low loss.)"
Why is Euclidean distance not a good metric in high dimensions?,"
I read that 'Euclidean distance is not a good distance in high dimensions'. I guess this statement has something to do with the curse of dimensionality, but what exactly? Besides, what is 'high dimensions'? I have been applying hierarchical clustering using Euclidean distance with 100 features. Up to how many features is it 'safe' to use this metric?
","['machine-learning', 'clustering', 'distance-functions', 'metric', 'high-dimensional']","A great summary of non-intuitive results in higher dimensions comes from ""A Few Useful Things to Know about Machine Learning"" by Pedro Domingos at the University of Washington:[O]ur intuitions, which come from a three-dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant “shell” around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor. And if we approximate a hypersphere by inscribing it in a hypercube, in high dimensions almost all the volume of the hypercube is outside the hypersphere. This is bad news for machine learning, where shapes of one type are often approximated by shapes of another.The article is also full of many additional pearls of wisdom for machine learning.Another application, beyond machine learning, is nearest neighbor search: given an observation of interest, find its nearest neighbors (in the sense that these are the points with the smallest distance from the query point). But in high dimensions, a curious phenomenon arises: the ratio between the nearest and farthest points approaches 1, i.e. the points essentially become uniformly distant from each other. This phenomenon can be observed for wide variety of distance metrics, but it is more pronounced for the Euclidean metric than, say, Manhattan distance metric. The premise of nearest neighbor search is that ""closer"" points are more relevant than ""farther"" points, but if all points are essentially uniformly distant from each other, the distinction is meaningless.From Charu C. Aggarwal, Alexander Hinneburg, Daniel A. Keim, ""On the Surprising Behavior of Distance Metrics in High Dimensional Space"":It has been argued in [Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft, ""When Is 'Nearest Neighbor' Meaningful?""] that under certain reasonable assumptions on the data distribution, the ratio of the distances of the nearest and farthest neighbors to a given target in high dimensional space is almost 1 for a wide variety of data distributions and distance functions. In such a case, the nearest neighbor problem becomes ill defined, since the contrast between the distances to diferent data points does not exist. In such cases, even the concept of proximity may not be meaningful from a qualitative perspective: a problem which is even more fundamental than the performance degradation of high dimensional algorithms.... Many high-dimensional indexing structures and algorithms use the [E]uclidean distance metric as a natural extension of its traditional use in two- or three-dimensional spatial applications. ... In this paper we provide some surprising theoretical and experimental results in analyzing the dependency of the $L_k$ norm on the value of $k$. More specifically, we show that the relative contrasts of the distances to a query point depend heavily on the $L_k$ metric used. This provides considerable evidence that the meaningfulness of the $L_k$ norm worsens faster within increasing dimensionality for higher values of $k$. Thus, for a given problem with a fixed (high) value for the dimensionality $d$, it may be preferable to use lower values of $k$. This means that the $L_1$ distance metric (Manhattan distance metric) is the most preferable for high dimensional applications, followed by the Euclidean metric ($L_2$). ...The authors of the ""Surprising Behavior"" paper then propose using $L_k$ norms with $k<1$. They produce some results which demonstrate that these ""fractional norms"" exhibit the property of increasing the contrast between farthest and nearest points. However, later research has concluded against fractional norms. See: ""Fractional norms and quasinorms do not help to overcome the curse of dimensionality"" by Mirkes, Allohibi, & Gorban (2020). (Thanks to michen00 for the comment and helpful citation.)"
How to understand degrees of freedom?,"
From Wikipedia, there are three interpretations of the degrees of freedom of a statistic:

In statistics, the number of degrees of freedom is the number of
  values in the final calculation of a statistic that are free to vary.
Estimates of statistical parameters can be based upon different
  amounts of information or data. The number of independent pieces of
  information that go into the estimate of a parameter is called the
  degrees of freedom (df). In general, the degrees of freedom of an
  estimate of a parameter is equal to the number of independent scores
  that go into the estimate minus the number of parameters used as
  intermediate steps in the estimation of the parameter itself (which,
  in sample variance, is one, since the sample mean is the only
  intermediate step).
Mathematically, degrees of freedom is the dimension of the domain of a
  random vector, or essentially the number of 'free' components: how
  many components need to be known before the vector is fully
  determined.

The bold words are what I don't quite understand. If possible, some mathematical formulations will help clarify the concept.
Also do the three interpretations agree with each other?
","['interpretation', 'degrees-of-freedom', 'intuition']","This is a subtle question.  It takes a thoughtful person not to understand those quotations!  Although they are suggestive, it turns out that none of them is exactly or generally correct.  I haven't the time (and there isn't the space here) to give a full exposition, but I would like to share one approach and an insight that it suggests.Where does the concept of degrees of freedom (DF) arise?  The contexts in which it's found in elementary treatments are:The Student t-test and its variants such as the Welch or Satterthwaite solutions to the Behrens-Fisher problem (where two populations have different variances).The Chi-squared distribution (defined as a sum of squares of independent standard Normals), which is implicated in the sampling distribution of the variance.The F-test (of ratios of estimated variances).The Chi-squared test, comprising its uses in (a) testing for independence in contingency tables and (b) testing for goodness of fit of distributional estimates.In spirit, these tests run a gamut from being exact (the Student t-test and F-test for Normal variates) to being good approximations (the Student t-test and the Welch/Satterthwaite tests for not-too-badly-skewed data) to being based on asymptotic approximations (the Chi-squared test).  An interesting aspect of some of these is the appearance of non-integral ""degrees of freedom"" (the Welch/Satterthwaite tests and, as we will see, the Chi-squared test).  This is of especial interest because it is the first hint that DF is not any of the things claimed of it.We can dispose right away of some of the claims in the question.  Because ""final calculation of a statistic"" is not well-defined (it apparently depends on what algorithm one uses for the calculation), it can be no more than a vague suggestion and is worth no further criticism.  Similarly, neither ""number of independent scores that go into the estimate"" nor ""the number of parameters used as intermediate steps"" are well-defined.""Independent pieces of information that go into [an] estimate"" is difficult to deal with, because there are two different but intimately related senses of ""independent"" that can be relevant here.  One is independence of random variables; the other is functional independence.  As an example of the latter, suppose we collect morphometric measurements of subjects--say, for simplicity, the three side lengths $X$, $Y$, $Z$, surface areas $S=2(XY+YZ+ZX)$, and volumes $V=XYZ$ of a set of wooden blocks.  The three side lengths can be considered independent random variables, but all five variables are dependent RVs.  The five are also functionally dependent because the codomain (not the ""domain""!) of the vector-valued random variable $(X,Y,Z,S,V)$ traces out a three-dimensional manifold in $\mathbb{R}^5$.  (Thus, locally at any point $\omega\in\mathbb{R}^5$, there are two functions $f_\omega$ and $g_\omega$ for which $f_\omega(X(\psi),\ldots,V(\psi))=0$ and $g_\omega(X(\psi),\ldots,V(\psi))=0$ for points $\psi$ ""near"" $\omega$ and the derivatives of $f$ and $g$ evaluated at $\omega$ are linearly independent.)  However--here's the kicker--for many probability measures on the blocks, subsets of the variables such as $(X,S,V)$ are dependent as random variables but functionally independent.Having been alerted by these potential ambiguities, let's hold up the Chi-squared goodness of fit test for examination, because (a) it's simple, (b) it's one of the common situations where people really do need to know about DF to get the p-value right and (c) it's often used incorrectly.  Here's a brief synopsis of the least controversial application of this test:You have a collection of data values $(x_1, \ldots, x_n)$, considered as a sample of a population.You have estimated some parameters $\theta_1, \ldots, \theta_p$ of a distribution. For example, you estimated the mean $\theta_1$ and standard deviation $\theta_2 = \theta_p$ of a Normal distribution, hypothesizing that the population is normally distributed but not knowing (in advance of obtaining the data) what $\theta_1$ or $\theta_2$ might be.In advance, you created a set of $k$ ""bins"" for the data.  (It may be problematic when the bins are determined by the data, even though this is often done.)  Using these bins, the data are reduced to the set of counts within each bin.  Anticipating what the true values of $(\theta)$ might be, you have arranged it so (hopefully) each bin will receive approximately the same count.  (Equal-probability binning assures the chi-squared distribution really is a good approximation to the true distribution of the chi-squared statistic about to be described.)You have a lot of data--enough to assure that almost all bins ought to have counts of 5 or greater.  (This, we hope, will enable the sampling distribution of the $\chi^2$ statistic to be approximated adequately by some $\chi^2$ distribution.)Using the parameter estimates, you can compute the expected count in each bin.  The Chi-squared statistic is the sum of the ratios$$\frac{(\text{observed}-\text{expected})^2}{\text{expected}}.$$This, many authorities tell us, should have (to a very close approximation) a Chi-squared distribution.  But there's a whole family of such distributions.  They are differentiated by a parameter $\nu$ often referred to as the ""degrees of freedom.""  The standard reasoning about how to determine $\nu$ goes like thisI have $k$ counts.  That's $k$ pieces of data.  But there are (functional) relationships among them.  To start with, I know in advance that the sum of the counts must equal $n$.  That's one relationship.  I estimated two (or $p$, generally) parameters from the data.  That's two (or $p$) additional relationships, giving $p+1$ total relationships.  Presuming they (the parameters) are all (functionally) independent, that leaves only $k-p-1$ (functionally) independent ""degrees of freedom"": that's the value to use for $\nu$.The problem with this reasoning (which is the sort of calculation the quotations in the question are hinting at) is that it's wrong except when some special additional conditions hold.  Moreover, those conditions have nothing to do with independence (functional or statistical), with numbers of ""components"" of the data, with the numbers of parameters, nor with anything else referred to in the original question.Let me show you with an example.  (To make it as clear as possible, I'm using a small number of bins, but that's not essential.)  Let's generate 20 independent and identically distributed (iid) standard Normal variates and estimate their mean and standard deviation with the usual formulas (mean = sum/count, etc.).  To test goodness of fit, create four bins with cutpoints at the quartiles of a standard normal: -0.675, 0, +0.657, and use the bin counts to generate a Chi-squared statistic.  Repeat as patience allows; I had time to do 10,000 repetitions.The standard wisdom about DF says we have 4 bins and 1+2 = 3 constraints, implying the distribution of these 10,000 Chi-squared statistics should follow a Chi-squared distribution with 1 DF.  Here's the histogram:The dark blue line graphs the PDF of a $\chi^2(1)$ distribution--the one we thought would work--while the dark red line graphs that of a $\chi^2(2)$ distribution (which would be a good guess if someone were to tell you that $\nu=1$ is incorrect).  Neither fits the data.You might expect the problem to be due to the small size of the data sets ($n$=20) or perhaps the small size of the number of bins.  However, the problem persists even with very large datasets and larger numbers of bins: it is not merely a failure to reach an asymptotic approximation.Things went wrong because I violated two requirements of the Chi-squared test: You must use the Maximum Likelihood estimate of the parameters.  (This requirement can, in practice, be slightly violated.)You must base that estimate on the counts, not on the actual data!  (This is crucial.)The red histogram depicts the chi-squared statistics for 10,000 separate iterations, following these requirements.  Sure enough, it visibly follows the $\chi^2(1)$ curve (with an acceptable amount of sampling error), as we had originally hoped.The point of this comparison--which I hope you have seen coming--is that the correct DF to use for computing the p-values depends on many things other than dimensions of manifolds, counts of functional relationships, or the geometry of Normal variates.  There is a subtle, delicate interaction between certain functional dependencies, as found in mathematical relationships among quantities, and distributions of the data, their statistics, and the estimators formed from them.  Accordingly, it cannot be the case that DF is adequately explainable in terms of the geometry of multivariate normal distributions, or in terms of functional independence, or as counts of parameters, or anything else of this nature.We are led to see, then, that ""degrees of freedom"" is merely a heuristic that suggests what the sampling distribution of a (t, Chi-squared, or F) statistic ought to be, but it is not dispositive.  Belief that it is dispositive leads to egregious errors.  (For instance, the top hit on Google when searching ""chi squared goodness of fit"" is a Web page from an Ivy League university that gets most of this completely wrong!  In particular, a simulation based on its instructions shows that the chi-squared value it recommends as having 7 DF actually has 9 DF.)With this more nuanced understanding, it's worthwhile to re-read the Wikipedia article in question: in its details it gets things right, pointing out where the DF heuristic tends to work and where it is either an approximation or does not apply at all.A good account of the phenomenon illustrated here (unexpectedly high DF in Chi-squared GOF tests) appears in Volume II of Kendall & Stuart, 5th edition.  I am grateful for the opportunity afforded by this question to lead me back to this wonderful text, which is full of such useful analyses.Here is R code to produce the figure following ""The standard wisdom about DF..."""
What's the difference between a confidence interval and a credible interval?,"
Joris and Srikant's exchange here got me wondering (again) if my internal explanations for the difference between confidence intervals and credible intervals were the correct ones.  How you would explain the difference?
","['bayesian', 'confidence-interval', 'frequentist', 'credible-interval', 'fiducial']","I agree completely with Srikant's explanation. To give a more heuristic spin on it:Classical approaches generally posit that the world is one way (e.g., a parameter has one particular true value), and try to conduct experiments whose resulting conclusion -- no matter the true value of the parameter -- will be correct with at least some minimum probability.As a result, to express uncertainty in our knowledge after an experiment, the frequentist approach uses a ""confidence interval"" -- a range of values designed to include the true value of the parameter with some minimum probability, say 95%. A frequentist will design the experiment and 95% confidence interval procedure so that out of every 100 experiments run start to finish, at least 95 of the resulting confidence intervals will be expected to include the true value of the parameter. The other 5 might be slightly wrong, or they might be complete nonsense -- formally speaking that's ok as far as the approach is concerned, as long as 95 out of 100 inferences are correct. (Of course we would prefer them to be slightly wrong, not total nonsense.)Bayesian approaches formulate the problem differently. Instead of saying the parameter simply has one (unknown) true value, a Bayesian method says the parameter's value is fixed but has been chosen from some probability distribution -- known as the prior probability distribution. (Another way to say that is that before taking any measurements, the Bayesian assigns a probability distribution, which they call a belief state, on what the true value of the parameter happens to be.) This ""prior"" might be known (imagine trying to estimate the size of a truck, if we know the overall distribution of truck sizes from the DMV) or it might be an assumption drawn out of thin air. The Bayesian inference is simpler -- we collect some data, and then calculate the probability of different values of the parameter GIVEN the data. This new probability distribution is called the ""a posteriori probability"" or simply the ""posterior."" Bayesian approaches can summarize their uncertainty by giving a range of values on the posterior probability distribution that includes 95% of the probability -- this is called a ""95% credibility interval.""A Bayesian partisan might criticize the frequentist confidence interval like this: ""So what if 95 out of 100 experiments yield a confidence interval that includes the true value? I don't care about 99 experiments I DIDN'T DO; I care about this experiment I DID DO. Your rule allows 5 out of the 100 to be complete nonsense [negative values, impossible values] as long as the other 95 are correct; that's ridiculous.""A frequentist die-hard might criticize the Bayesian credibility interval like this: ""So what if 95% of the posterior probability is included in this range? What if the true value is, say, 0.37? If it is, then your method, run start to finish, will be WRONG 75% of the time. Your response is, 'Oh well, that's ok because according to the prior it's very rare that the value is 0.37,' and that may be so, but I want a method that works for ANY possible value of the parameter. I don't care about 99 values of the parameter that IT DOESN'T HAVE; I care about the one true value IT DOES HAVE. Oh also, by the way, your answers are only correct if the prior is correct. If you just pull it out of thin air because it feels right, you can be way off.""In a sense both of these partisans are correct in their criticisms of each others' methods, but I would urge you to think mathematically about the distinction -- as Srikant explains.Here's an extended example from that talk that shows the difference precisely in a discrete example.When I was a child my mother used to occasionally surprise me by ordering a jar of chocolate-chip cookies to be delivered by mail. The delivery company stocked four different kinds of cookie jars -- type A, type B, type C, and type D, and they were all on the same truck and you were never sure what type you would get. Each jar had exactly 100 cookies, but the feature that distinguished the different cookie jars was their respective distributions of chocolate chips per cookie. If you reached into a jar and took out a single cookie uniformly at random, these are the probability distributions you would get on the number of chips:A type-A cookie jar, for example, has 70 cookies with two chips each, and no cookies with four chips or more! A type-D cookie jar has 70 cookies with one chip each. Notice how each vertical column is a probability mass function -- the conditional probability of the number of chips you'd get, given that the jar = A, or B, or C, or D, and each column sums to 100.I used to love to play a game as soon as the deliveryman dropped off my new cookie jar. I'd pull one single cookie at random from the jar, count the chips on the cookie, and try to express my uncertainty -- at the 70% level -- of which jars it could be. Thus it's the identity of the jar (A, B, C or D) that is the value of the parameter being estimated. The number of chips (0, 1, 2, 3 or 4) is the outcome or the observation or the sample.Originally I played this game using a frequentist, 70% confidence interval. Such an interval needs to make sure that no matter the true value of the parameter, meaning no matter which cookie jar I got, the interval would cover that true value with at least 70% probability.An interval, of course, is a function that relates an outcome (a row) to a set of values of the parameter (a set of columns). But to construct the confidence interval and guarantee 70% coverage, we need to work ""vertically"" -- looking at each column in turn, and making sure that 70% of the probability mass function is covered so that 70% of the time, that column's identity will be part of the interval that results. Remember that it's the vertical columns that form a p.m.f.So after doing that procedure, I ended up with these intervals:For example, if the number of chips on the cookie I draw is 1, my confidence interval will be {B,C,D}. If the number is 4, my confidence interval will be {B,C}. Notice that since each column sums to 70% or greater, then no matter which column we are truly in (no matter which jar the deliveryman dropped off), the interval resulting from this procedure will include the correct jar with at least 70% probability.Notice also that the procedure I followed in constructing the intervals had some discretion. In the column for type-B, I could have just as easily made sure that the intervals that included B would be 0,1,2,3 instead of 1,2,3,4. That would have resulted in 75% coverage for type-B jars (12+19+24+20), still meeting the lower bound of 70%.My sister Bayesia thought this approach was crazy, though. ""You have to consider the deliverman as part of the system,"" she said. ""Let's treat the identity of the jar as a random variable itself, and let's assume that the deliverman chooses among them uniformly -- meaning he has all four on his truck, and when he gets to our house he picks one at random, each with uniform probability.""""With that assumption, now let's look at the joint probabilities of the whole event -- the jar type and the number of chips you draw from your first cookie,"" she said, drawing the following table:Notice that the whole table is now a probability mass function -- meaning the whole table sums to 100%.""Ok,"" I said, ""where are you headed with this?""""You've been looking at the conditional probability of the number of chips, given the jar,"" said Bayesia. ""That's all wrong! What you really care about is the conditional probability of which jar it is, given the number of chips on the cookie! Your 70% interval should simply include the list jars that, in total, have 70% probability of being the true jar. Isn't that a lot simpler and more intuitive?""""Sure, but how do we calculate that?"" I asked.""Let's say we know that you got 3 chips. Then we can ignore all the other rows in the table, and simply treat that row as a probability mass function. We'll need to scale up the probabilities proportionately so each row sums to 100, though."" She did:""Notice how each row is now a p.m.f., and sums to 100%. We've flipped the conditional probability from what you started with -- now it's the probability of the man having dropped off a certain jar, given the number of chips on the first cookie.""""Interesting,"" I said. ""So now we just circle enough jars in each row to get up to 70% probability?"" We did just that, making these credibility intervals:Each interval includes a set of jars that, a posteriori, sum to 70% probability of being the true jar.""Well, hang on,"" I said. ""I'm not convinced. Let's put the two kinds of intervals side-by-side and compare them for coverage and, assuming that the deliveryman picks each kind of jar with equal probability, credibility.""Here they are:Confidence intervals:Credibility intervals:""See how crazy your confidence intervals are?"" said Bayesia. ""You don't even have a sensible answer when you draw a cookie with zero chips! You just say it's the empty interval. But that's obviously wrong -- it has to be one of the four types of jars. How can you live with yourself, stating an interval at the end of the day when you know the interval is wrong? And ditto when you pull a cookie with 3 chips -- your interval is only correct 41% of the time. Calling this a '70%' confidence interval is bullshit.""""Well, hey,"" I replied. ""It's correct 70% of the time, no matter which jar the deliveryman dropped off. That's a lot more than you can say about your credibility intervals. What if the jar is type B? Then your interval will be wrong 80% of the time, and only correct 20% of the time!""""This seems like a big problem,"" I continued, ""because your mistakes will be correlated with the type of jar. If you send out 100 'Bayesian' robots to assess what type of jar you have, each robot sampling one cookie, you're telling me that on type-B days, you will expect 80 of the robots to get the wrong answer, each having >73% belief in its incorrect conclusion! That's troublesome, especially if you want most of the robots to agree on the right answer.""""PLUS we had to make this assumption that the deliveryman behaves uniformly and selects each type of jar at random,"" I said. ""Where did that come from? What if it's wrong? You haven't talked to him; you haven't interviewed him. Yet all your statements of a posteriori probability rest on this statement about his behavior. I didn't have to make any such assumptions, and my interval meets its criterion even in the worst case.""""It's true that my credibility interval does perform poorly on type-B jars,"" Bayesia said. ""But so what? Type B jars happen only 25% of the time. It's balanced out by my good coverage of type A, C, and D jars. And I never publish nonsense.""""It's true that my confidence interval does perform poorly when I've drawn a cookie with zero chips,"" I said. ""But so what? Chipless cookies happen, at most, 27% of the time in the worst case (a type-D jar). I can afford to give nonsense for this outcome because NO jar will result in a wrong answer more than 30% of the time.""""The column sums matter,"" I said.""The row sums matter,"" Bayesia said.""I can see we're at an impasse,"" I said. ""We're both correct in the mathematical statements we're making, but we disagree about the appropriate way to quantify uncertainty.""""That's true,"" said my sister. ""Want a cookie?"""
What is batch size in neural network?,"
I'm using Python Keras package for neural network. This is the link. Is batch_size equals to number of test samples? From Wikipedia we have this information:

However, in other cases, evaluating the sum-gradient may require
  expensive evaluations of the gradients from all summand functions.
  When the training set is enormous and no simple formulas exist,
  evaluating the sums of gradients becomes very expensive, because
  evaluating the gradient requires evaluating all the summand functions'
  gradients. To economize on the computational cost at every iteration,
  stochastic gradient descent samples a subset of summand functions at
  every step. This is very effective in the case of large-scale machine
  learning problems.

Above information is describing test data? Is this same as batch_size in keras (Number of samples per gradient update)?
","['neural-networks', 'python', 'terminology', 'keras']","The batch size defines the number of samples that will be propagated through the network.For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network. Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network.Advantages of using a batch size < number of all samples:It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory.Typically networks train faster with mini-batches. That's because we update the weights after each propagation. In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) and after each of them we've updated our network's parameters. If we used all samples during propagation we would make only 1 update for the network's parameter.Disadvantages of using a batch size < number of all samples:Stochastic is just a mini-batch with batch_size equal to 1. In that case, the gradient changes its direction even more often than a mini-batch gradient."
"Bagging, boosting and stacking in machine learning","
What's the similarities and differences between these 3 methods: 

Bagging, 
Boosting, 
Stacking?

Which is the best one? And why?
Can you give me an example for each?
","['machine-learning', 'boosting', 'ensemble-learning', 'bagging', 'model-averaging']",
Why does a 95% Confidence Interval (CI) not imply a 95% chance of containing the mean?,"
It seems that through various related questions here, there is consensus that the ""95%"" part of what we call a ""95% confidence interval"" refers to the fact that if we were to exactly replicate our sampling and CI-computation procedures  many times, 95% of thusly computed CIs would contain the population mean. It also seems to be the consensus that this definition does not permit one to conclude from a single 95%CI that there is a 95% chance that the mean falls somewhere within the CI. However, I don't understand how the former doesn't imply the latter insofar as, having imagined many CIs 95% of which contain the population mean, shouldn't our uncertainty (with regards to whether our actually-computed CI contains the population mean or not) force us to use the base-rate of the imagined cases (95%) as our estimate of the probability that our actual case contains the CI? 
I've seen posts argue along the lines of ""the actually-computed CI either contains the population mean or it doesn't, so its probability is either 1 or 0"", but this seems to imply a strange definition of probability that is dependent on unknown states (i.e. a friend flips fair coin, hides the result, and I am disallowed from saying there is a 50% chance that it's heads).
Surely I'm wrong, but I don't see where my logic has gone awry...
","['probability', 'confidence-interval', 'sampling', 'mean', 'population']",
What is the meaning of p values and t values in statistical tests?,"
After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statistical hypothesis tests.  It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the results.  Many computerized tools report test results in terms of ""p values"" or ""t values"".
How would you explain the following points to college students taking their first course in statistics:

What does a ""p-value"" mean in relation to the hypothesis being tested?  Are there cases when one should be looking for a high p-value or a low p-value?
What is the relationship between a p-value and a t-value?

","['hypothesis-testing', 'p-value', 'interpretation', 'intuition', 'faq']",
What does AUC stand for and what is it?,"
Searched high and low and have not been able to find out what AUC, as in related to prediction, stands for or means.
","['classification', 'prediction', 'roc', 'auc', 'abbreviation']","AUC is used most of the time to mean AUROC, which is a bad practice since as Marc Claesen pointed out AUC is ambiguous (could be any curve) while AUROC is not. The AUROC has several equivalent interpretations:Going further: How to derive the probabilistic interpretation of the AUROC?Assume we have a probabilistic, binary classifier such as logistic regression.Before presenting the ROC curve (= Receiver Operating Characteristic curve), the concept of confusion matrix must be understood. When we make a binary prediction, there can be 4 types of outcomes:To get the confusion matrix, we go over all the predictions made by the model, and count how many times each of those 4 types of outcomes occur:In this example of a confusion matrix, among the 50 data points that are classified, 45 are correctly classified and the 5 are misclassified.Since to compare two different models it is often more convenient to have a single metric rather than several ones, we compute two metrics from the confusion matrix, which we will later combine into one:To combine the FPR and the TPR into one single metric, we first compute the two former metrics with many different threshold (for example $0.00; 0.01, 0.02, \dots, 1.00$) for the logistic regression, then plot them on a single graph, with the FPR values on the abscissa and the TPR values on the ordinate. The resulting curve is called ROC curve, and the metric we consider is the AUC of this curve, which we call AUROC. The following figure shows the AUROC graphically:In this figure, the blue area corresponds to the Area Under the curve of the Receiver Operating Characteristic (AUROC). The dashed line in the diagonal we present the ROC curve of a random predictor: it has an AUROC of 0.5. The random predictor is commonly used as a baseline to see whether the model is useful.If you want to get some first-hand experience:"
Is there any reason to prefer the AIC or BIC over the other?,"
The AIC and BIC are both methods of assessing model fit penalized for the number of estimated parameters.  As I understand it, BIC penalizes models more for free parameters than does AIC.  Beyond a preference based on the stringency of the criteria, are there any other reasons to prefer AIC over BIC or vice versa?
","['modeling', 'aic', 'cross-validation', 'bic', 'model-selection']","Your question implies that AIC and BIC try to answer the same question, which is not true. The AIC tries to select the model that most adequately describes an unknown, high dimensional reality. This means that reality is never in the set of candidate models that are being considered.  On the contrary, BIC tries to find the TRUE model among the set of candidates. I find it quite odd the assumption that reality is instantiated in one of the models that the researchers built along the way. This is a real issue for BIC.Nevertheless, there are a lot of researchers who say BIC is better than AIC, using model recovery simulations as an argument. These simulations consist of generating data from models A and B, and then fitting both datasets with the two models. Overfitting occurs when the wrong model fits the data better than the generating. The point of these simulations is to see how well AIC and BIC correct these overfits. Usually, the results point to the fact that AIC is too liberal and still frequently prefers a more complex, wrong model over a simpler, true model. At first glance these simulations seem to be really good arguments, but the problem with them is that they are meaningless for AIC. As I said before, AIC does not consider that any of the candidate models being tested is actually true. According to AIC, all models are approximations to reality, and reality should never have a low dimensionality. At least lower than some of the candidate models. My recommendation is to use both AIC and BIC. Most of the times they will agree on the preferred model, when they don't, just report it.If you are unhappy with both AIC and BIC and have free time to invest, look up Minimum Description Length (MDL), a totally different approach that overcomes the limitations of AIC and BIC. There are several measures stemming from MDL, like normalized maximum likelihood or the Fisher Information approximation. The problem with MDL is that its mathematically demanding and/or computationally intensive. Still, if you want to stick to simple solutions, a nice way for assessing model flexibility (especially when the number of parameters are equal, rendering AIC and BIC useless) is doing Parametric Bootstrap, which is quite easy to implement. Here is a link to a paper on it. Some people here advocate for the use of cross-validation. I personally have used it and don't have anything against it, but the issue with it is that the choice among the sample-cutting rule (leave-one-out, K-fold, etc) is an unprincipled one. "
Is $R^2$ useful or dangerous?,"
I was skimming through some lecture notes by Cosma Shalizi (in particular, section 2.1.1 of the second lecture), and was reminded that you can get very low $R^2$ even when you have a completely linear model.
To paraphrase Shalizi's example: suppose you have a model $Y = aX + \epsilon$, where $a$ is known. Then $\newcommand{\Var}{\mathrm{Var}}\Var[Y] = a^2 \Var[x] + \Var[\epsilon]$ and the amount of explained variance is $a^2 \Var[X]$, so $R^2 = \frac{a^2 \Var[x]}{a^2 \Var[X] + \Var[\epsilon]}$. This goes to 0 as $\Var[X] \rightarrow 0$ and to 1 as $\Var[X] \rightarrow \infty$.
Conversely, you can get high $R^2$ even when your model is noticeably non-linear. (Anyone have a good example offhand?)
So when is $R^2$ a useful statistic, and when should it be ignored?
","['regression', 'r-squared']","To address the first question, consider the model$$Y = X + \sin(X) + \varepsilon$$with iid $\varepsilon$ of mean zero and finite variance.  As the range of $X$ (thought of as fixed or random) increases, $R^2$ goes to 1.  Nevertheless, if the variance of $\varepsilon$ is small (around 1 or less), the data are ""noticeably non-linear.""  In the plots, $var(\varepsilon)=1$.Incidentally, an easy way to get a small $R^2$ is to slice the independent variables into narrow ranges.  The regression (using exactly the same model) within each range will have a low $R^2$ even when the full regression based on all the data has a high $R^2$.  Contemplating this situation is an informative exercise and good preparation for the second question.Both the following plots use the same data.  The $R^2$ for the full regression is 0.86.  The $R^2$ for the slices (of width 1/2 from -5/2 to 5/2) are .16, .18, .07, .14, .08, .17, .20, .12, .01, .00, reading left to right.  If anything, the fits get better in the sliced situation because the 10 separate lines can more closely conform to the data within their narrow ranges.  Although the $R^2$ for all the slices are far below the full $R^2$, neither the strength of the relationship, the linearity, nor indeed any aspect of the data (except the range of $X$ used for the regression) has changed.(One might object that this slicing procedure changes the distribution of $X$.  That is true, but it nevertheless corresponds with the most common use of $R^2$ in fixed-effects modeling and reveals the degree to which $R^2$ is telling us about the variance of $X$ in the random-effects situation.  In particular, when $X$ is constrained to vary within a smaller interval of its natural range, $R^2$ will usually drop.)The basic problem with $R^2$ is that it depends on too many things (even when adjusted in multiple regression), but most especially on the variance of the independent variables and the variance of the residuals.  Normally it tells us nothing about ""linearity"" or ""strength of relationship"" or even ""goodness of fit"" for comparing a sequence of models. Most of the time you can find a better statistic than $R^2$.  For model selection you can look to AIC and BIC; for expressing the adequacy of a model, look at the variance of the residuals.  This brings us finally to the second question.  One situation in which $R^2$ might have some use is when the independent variables are set to standard values, essentially controlling for the effect of their variance.  Then $1 - R^2$ is really a proxy for the variance of the residuals, suitably standardized."
How to choose a predictive model after k-fold cross-validation?,"
I am wondering how to choose a predictive model after doing K-fold cross-validation. 
This may be awkwardly phrased, so let me explain in more detail: whenever I run K-fold cross-validation, I use K subsets of the training data, and end up with K different models. 
I would like to know how to pick one of the K models, so that I can present it to someone and say ""this is the best model that we can produce."" 
Is it OK to pick any one of the K models? Or is there some kind of best practice that is involved, such as picking the model that achieves the median test error?
","['cross-validation', 'model-selection']","I think that you are missing something still in your understanding of the purpose of cross-validation.Let's get some terminology straight, generally when we say 'a model' we refer to a particular method for describing how some input data relates to what we are trying to predict. We don't generally refer to particular instances of that method as different models. So you might say 'I have a linear regression model' but you wouldn't call two different sets of the trained coefficients different models. At least not in the context of model selection.So, when you do K-fold cross validation, you are testing how well your model is able to get trained by some data and then predict data it hasn't seen. We use cross validation for this because if you train using all the data you have, you have none left for testing. You could do this once, say by using 80% of the data to train and 20% to test, but what if the 20% you happened to pick to test happens to contain a bunch of points that are particularly easy (or particularly hard) to predict? We will not have come up with the best estimate possible of the models ability to learn and predict.We want to use all of the data. So to continue the above example of an 80/20 split, we would do 5-fold cross validation by training the model 5 times on 80% of the data and testing on 20%. We ensure that each data point ends up in the 20% test set exactly once. We've therefore used every data point we have to contribute to an understanding of how well our model performs the task of learning from some data and predicting some new data.But the purpose of cross-validation is not to come up with our final model. We don't use these 5 instances of our trained model to do any real prediction. For that we want to use all the data we have to come up with the best model possible. The purpose of cross-validation is model checking, not model building.Now, say we have two models, say a linear regression model and a neural network. How can we say which model is better? We can do K-fold cross-validation and see which one proves better at predicting the test set points. But once we have used cross-validation to select the better performing model, we train that model (whether it be the linear regression or the neural network) on all the data. We don't use the actual model instances we trained during cross-validation for our final predictive model.Note that there is a technique called bootstrap aggregation (usually shortened to 'bagging') that does in a way use model instances produced in a way similar to cross-validation to build up an ensemble model, but that is an advanced technique beyond the scope of your question here."
Famous statistical quotations,"
What is your favorite statistical quote?  
This is community wiki, so please one quote per answer.  
","['references', 'history']",
How would you explain covariance to someone who understands only the mean?,"
...assuming that I'm able to augment their knowledge about variance in an intuitive fashion ( Understanding ""variance"" intuitively ) or by saying: It's the average distance of the data values from the 'mean' - and since variance is in square units, we take the square root to keep the units same and that is called standard deviation.
Let's assume this much is articulated and (hopefully) understood by the 'receiver'. Now what is covariance and how would one explain it in simple English without the use of any mathematical terms/formulae? (I.e., intuitive explanation. ;)
Please note: I do know the formulae and the math behind the concept. I want to be able to 'explain' the same in an easy to understand fashion, without including the math; i.e., what does 'covariance' even mean?
","['variance', 'covariance', 'intuition', 'teaching']","Sometimes we can ""augment knowledge"" with an unusual or different approach.  I would like this reply to be accessible to kindergartners and also have some fun, so everybody get out your crayons!Given paired $(x,y)$ data, draw their scatterplot.  (The younger students may need a teacher to produce this for them. :-)  Each pair of points $(x_i,y_i)$, $(x_j,y_j)$ in that plot determines a rectangle: it's the smallest rectangle, whose sides are parallel to the axes, containing those points.  Thus the points are either at the upper right and lower left corners (a ""positive"" relationship) or they are at the upper left and lower right corners (a ""negative"" relationship).Draw all possible such rectangles. Color them transparently, making the positive rectangles red (say) and the negative rectangles ""anti-red"" (blue).  In this fashion, wherever rectangles overlap, their colors are either enhanced when they are the same (blue and blue or red and red) or cancel out when they are different.(In this illustration of a positive (red) and negative (blue) rectangle, the overlap ought to be white; unfortunately, this software does not have a true ""anti-red"" color.   The overlap is gray, so it will darken the plot, but on the whole the net amount of red is correct.)Now we're ready for the explanation of covariance.The covariance is the net amount of red in the plot (treating blue as negative values).Here are some examples with 32 binormal points drawn from distributions with the given covariances, ordered from most negative (bluest) to most positive (reddest).They are drawn on common axes to make them comparable.  The rectangles are lightly outlined to help you see them.  This is an updated (2019) version of the original: it uses software that properly cancels the red and cyan colors in overlapping rectangles.Let's deduce some properties of covariance.  Understanding of these properties will be accessible to anyone who has actually drawn a few of the rectangles. :-)Bilinearity. Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis.Correlation. Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line.  This is because in the former case most of the rectangles are positive and in the latter case, most are negative.Relationship to linear associations. Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances.  Linear associations can be fully interpreted by means of the preceding two characterizations.Sensitivity to outliers. A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points.  It alone can create a net positive or negative amount of red in the overall picture.Incidentally, this definition of covariance differs from the usual one only by a constant of proportionality.  The mathematically inclined will have no trouble performing the algebraic demonstration that the formula given here is always twice the usual covariance.  For a full explanation, see the follow-up thread at https://stats.stackexchange.com/a/222091/919."
Interpretation of R's lm() output,"
The help pages in R assume I know what those numbers mean, but I don't.
I'm trying to really intuitively understand every number here. I will just post the output and comment on what I found out. There might (will) be mistakes, as I'll just write what I assume. Mainly I'd like to know what the t-value in the coefficients mean, and why they print the residual standard error. 
Call:
lm(formula = iris$Sepal.Width ~ iris$Petal.Width)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.09907 -0.23626 -0.01064  0.23345  1.17532 

This is a 5-point-summary of the residuals (their mean is always 0, right?). The numbers can be used (I'm guessing here) to quickly see if there are any big outliers. Also you can already see it here if the residuals are far from normally distributed (they should be normally distributed).
Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       3.30843    0.06210  53.278  < 2e-16 ***
iris$Petal.Width -0.20936    0.04374  -4.786 4.07e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Estimates $\hat{\beta_i}$, computed by least squares regression. Also, the standard error is $\sigma_{\beta_i}$. I'd like to know how this is calculated.  I have no idea where the t-value and the corresponding p-value come from. I know $\hat{\beta}$ should be normal distributed, but how is the t-value calculated?
Residual standard error: 0.407 on 148 degrees of freedom

$\sqrt{ \frac{1}{n-p} \epsilon^T\epsilon }$, I guess. But why do we calculate that, and what does it tell us?
Multiple R-squared: 0.134,  Adjusted R-squared: 0.1282 

$ R^2 = \frac{s_\hat{y}^2}{s_y^2} $, which is $ \frac{\sum_{i=1}^n (\hat{y_i}-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} $. The ratio is close to 1 if the points lie on a straight line, and 0 if they are random.  What is the adjusted R-squared?
F-statistic: 22.91 on 1 and 148 DF,  p-value: 4.073e-06 

F and p for the whole model, not only for single $\beta_i$s as previous. The F value is $ \frac{s^2_{\hat{y}}}{\sum\epsilon_i} $. The bigger it grows, the more unlikely it is that the $\beta$'s do not have any effect at all.
","['r', 'regression', 'interpretation']","yes, the idea is to give a quick summary of the distribution. It should be roughly symmetrical about mean, the median should be close to 0, the 1Q and 3Q values should ideally be roughly similar values.Each coefficient in the model is a Gaussian (Normal) random variable. The $\hat{\beta_i}$ is the estimate of the mean of the distribution of that random variable, and the standard error is the square root of the variance of that distribution. It is a measure of the uncertainty in the estimate of the $\hat{\beta_i}$.You can look at how these are computed (well the mathematical formulae used) on Wikipedia. Note that any self-respecting stats programme will not use the standard mathematical equations to compute the $\hat{\beta_i}$ because doing them on a computer can lead to a large loss of precision in the computations.The $t$ statistics are the estimates ($\hat{\beta_i}$) divided by their standard errors ($\hat{\sigma_i}$), e.g. $t_i = \frac{\hat{\beta_i}}{\hat{\sigma_i}}$. Assuming you have the same model in object modas your Q:then the $t$ values R reports are computed as:Where coef(mod) are the $\hat{\beta_i}$, and sqrt(diag(vcov(mod))) gives the square roots of the diagonal elements of the covariance matrix of the model parameters, which are the standard errors of the parameters ($\hat{\sigma_i}$).The p-value is the probability of achieving a $|t|$ as large as or larger than the observed absolute t value if the null hypothesis ($H_0$) was true, where $H_0$ is $\beta_i = 0$. They are computed as (using tstats from above):So we compute the upper tail probability of achieving the $t$ values we did from a $t$ distribution with degrees of freedom equal to the residual degrees of freedom of the model. This represents the probability of achieving a $t$ value greater than the absolute values of the observed $t$s. It is multiplied by 2, because of course $t$ can be large in the negative direction too.The residual standard error is an estimate of the parameter $\sigma$. The assumption in ordinary least squares is that the residuals are individually described by a Gaussian (normal) distribution with mean 0 and standard deviation $\sigma$. The $\sigma$ relates to the constant variance assumption; each residual has the same variance and that variance is equal to $\sigma^2$.Adjusted $R^2$ is computed as:$$1 - (1 - R^2) \frac{n - 1}{n - p - 1}$$The adjusted $R^2$ is the same thing as $R^2$, but adjusted for the complexity (i.e. the number of parameters) of the model. Given a model with a single parameter, with a certain $R^2$, if we add another parameter to this model, the $R^2$ of the new model has to increase, even if the added parameter has no statistical power. The adjusted $R^2$ accounts for this by including the number of parameters in the model.The $F$ is the ratio of two variances ($SSR/SSE$), the variance explained by the parameters in the model (sum of squares of regression, SSR) and the residual or unexplained variance (sum of squares of error, SSE). You can see this better if we get the ANOVA table for the model via anova():The $F$s are the same in the ANOVA output and the summary(mod) output. The Mean Sq column contains the two variances and $3.7945 / 0.1656 = 22.91$. We can compute the probability of achieving an $F$ that large under the null hypothesis of no effect, from an $F$-distribution with 1 and 148 degrees of freedom. This is what is reported in the final column of the ANOVA table. In the simple case of a single, continuous predictor (as per your example), $F = t_{\mathrm{Petal.Width}}^2$, which is why the p-values are the same. This equivalence only holds in this simple case."
How would you explain Markov Chain Monte Carlo (MCMC) to a layperson?,"
Maybe the concept, why it's used, and an example.
","['bayesian', 'markov-chain-montecarlo', 'intuition', 'teaching']","First, we need to understand what is a Markov chain. Consider the following weather example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:$P(\text{Next day is Sunny}\,\vert \,\text{Given today is Rainy)}=0.50$Since, the next day's weather is either sunny or rainy it follows that:$P(\text{Next day is Rainy}\,\vert \,\text{Given today is Rainy)}=0.50$Similarly, let:$P(\text{Next day is Rainy}\,\vert \,\text{Given today is Sunny)}=0.10$Therefore, it follows that:$P(\text{Next day is Sunny}\,\vert \,\text{Given today is Sunny)}=0.90$The above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:$P = \begin{bmatrix}
& S & R \\
S& 0.9 & 0.1 \\
R& 0.5 & 0.5
\end{bmatrix}$We might ask several questions whose answers follow:Q1: If the weather is sunny today then what is the weather likely to be tomorrow?A1: Since, we do not know what is going to happen for sure, the best we can say is that there is a $90\%$ chance that it is likely to be sunny and $10\%$ that it will be rainy. Q2: What about two days from today?A2: One day prediction: $90\%$ sunny, $10\%$ rainy. Therefore, two days from now:First day it can be sunny and the next day also it can be sunny. Chances of this happening are: $0.9 \times 0.9$. OrFirst day it can be rainy and second day it can be sunny. Chances of this happening are: $0.1 \times 0.5$.Therefore, the probability that the weather will be sunny in two days is:$P(\text{Sunny 2 days from now} = 0.9 \times 0.9 + 0.1 \times 0.5 = 0.81 + 0.05 = 0.86$Similarly, the probability that it will be rainy is:$P(\text{Rainy 2 days from now} = 0.1 \times 0.5 + 0.9 \times 0.1 = 0.05 + 0.09 = 0.14$In linear algebra (transition matrices) these calculations correspond to all the permutations in transitions from one step to the next (sunny-to-sunny ($S_2S$), sunny-to-rainy ($S_2R$), rainy-to-sunny ($R_2S$) or rainy-to-rainy ($R_2R$)) with their calculated probabilities:On the lower part of the image we see how to calculate the probability of a future state ($t+1$ or $t+2$) given the probabilities (probability mass function, $PMF$) for every state (sunny or rainy) at time zero (now or $t_0$) as simple matrix multiplication.If you keep forecasting weather like this you will notice that eventually the $n$-th day forecast, where $n$ is very large (say $30$), settles to the following 'equilibrium' probabilities:$P(\text{Sunny}) = 0.833$and $P(\text{Rainy}) = 0.167$In other words, your forecast for the $n$-th day and the $n+1$-th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.The above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' Markov chain (nice = transition probabilities satisfy conditions):Irrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.Markov Chain Monte Carlo exploits the above feature as follows: We want to generate random draws from a target distribution. We then identify a way to construct a 'nice' Markov chain such that its equilibrium probability distribution is our target distribution. If we can construct such a chain then we arbitrarily start from some point and iterate the Markov chain many times (like how we forecast the weather $n$ times). Eventually, the draws we generate would appear as if they are coming from our target distribution. We then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the Monte Carlo component.There are several ways to construct 'nice' Markov chains (e.g., Gibbs sampler, Metropolis-Hastings algorithm)."
How to know that your machine learning problem is hopeless?,"
Imagine a standard machine-learning scenario:

You are confronted with a large multivariate dataset and you have a
  pretty blurry understanding of it. What you need to do is to make
  predictions about some variable based on what you have. As usual, you
  clean the data, look at descriptive statistics, run some models,
  cross-validate them etc., but after several attempts, going back and
  forth and trying multiple models nothing seems to work and your
  results are miserable. You can spend hours, days, or weeks on such a
  problem...

The question is: when to stop? How do you know that your data actually is hopeless and all the fancy models wouldn't do you any more good than predicting the average outcome for all cases or some other trivial solution?
Of course, this is a forecastability issue, but as far as I know, it is hard to assess forecastability for multivariate data before trying something on it. Or am I wrong?

Disclaimer: this question was inspired by this one
When have I to stop looking for a model? that did not attract much attention. It would be nice to have detailed answer to such question for reference.

","['machine-learning', 'forecasting', 'modeling', 'model-selection', 'forecastability']","You are right that this is a question of forecastability. There have been a few articles on forecastability in the IIF's practitioner-oriented journal Foresight. (Full disclosure: I'm an Associate Editor.)The problem is that forecastability is already hard to assess in ""simple"" cases.Suppose you have a time series like this but don't speak German:How would you model the large peak in April, and how would you include this information in any forecasts?Unless you knew that this time series is the sales of eggs in a Swiss supermarket chain, which peaks right before western calendar Easter, you would not have a chance. Plus, with Easter moving around the calendar by as much as six weeks, any forecasts that don't include the specific date of Easter (by assuming, say, that this was just some seasonal peak that would recur in a specific week next year) would probably be very off.Similarly, assume you have the blue line below and want to model whatever happened on 2010-02-28 so differently from ""normal"" patterns on 2010-02-27:Again, without knowing what happens when a whole city full of Canadians watches an Olympic ice hockey finals game on TV, you have no chance whatsoever to understand what happened here, and you won't be able to predict when something like this will recur.Finally, look at this:This is a time series of daily sales at a cash and carry store. (On the right, you have a simple table: 282 days had zero sales, 42 days saw sales of 1... and one day saw sales of 500.) I don't know what item it is.To this day, I don't know what happened on that one day with sales of 500. My best guess is that some customer pre-ordered a large amount of whatever product this was and collected it. Now, without knowing this, any forecast for this particular day will be far off. Conversely, assume that this happened right before Easter, and we have a dumb-smart algorithm that believes this could be an Easter effect (maybe these are eggs?) and happily forecasts 500 units for the next Easter. Oh my, could that go wrong.In all cases, we see how forecastability can only be well understood once we have a sufficiently deep understanding of likely factors that influence our data. The problem is that unless we know these factors, we don't know that we may not know them. As per Donald Rumsfeld:[T]here are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns – the ones we don't know we don't know.If Easter or Canadians' predilection for Hockey are unknown unknowns to us, we are stuck - and we don't even have a way forward, because we don't know what questions we need to ask.The only way of getting a handle on these is to gather domain knowledge.I draw three conclusions from this:Here is how I would recommend building models - and noticing when to stop:Note that I am not advocating trying different classes of models if your original model plateaus. Typically, if you started out with a reasonable model, using something more sophisticated will not yield a strong benefit and may simply be ""overfitting on the test set"". I have seen this often, and other people agree."
What are the differences between Factor Analysis and Principal Component Analysis?,"
It seems that a number of the statistical packages that I use wrap these two concepts together. However, I'm wondering if there are different assumptions or data 'formalities' that must be true to use one over the other. A real example would be incredibly useful. 
","['pca', 'factor-analysis']","Principal component analysis involves extracting linear composites of observed variables.Factor analysis is based on a formal model predicting observed variables from theoretical latent factors.In psychology these two techniques are often applied in the construction of multi-scale tests
 to determine which items load on which scales.
They typically yield similar substantive conclusions (for a discussion see Comrey (1988) Factor-Analytic Methods of Scale Development in Personality and Clinical Psychology).
This helps to explain why some statistics packages seem to bundle them together.
I have also seen situations where ""principal component analysis"" is incorrectly labelled ""factor analysis"".In terms of a simple rule of thumb, I'd suggest that you:Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.Run principal component analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables."
What are common statistical sins?,"
I'm a grad student in psychology, and as I pursue more and more independent studies in statistics, I am increasingly amazed by the inadequacy of my formal training. Both personal and second hand experience suggests that the paucity of statistical rigor in undergraduate and graduate training is rather ubiquitous within psychology. As such, I thought it would be useful for independent learners like myself to create a list of ""Statistical Sins"", tabulating statistical practices taught to grad students as standard practice that are in fact either superseded by superior (more powerful, or flexible, or robust, etc.) modern methods or shown to be frankly invalid. Anticipating that other fields might also experience a similar state of affairs, I propose a community wiki where we can collect a list of statistical sins across disciplines. Please, submit one ""sin"" per answer.
",['fallacy'],
"What exactly are keys, queries, and values in attention mechanisms?","
How should one understand the keys, queries, and values that are often mentioned in attention mechanisms?
I've tried searching online, but all the resources I find only speak of them as if the reader already knows what they are.
Judging by the paper written by Bahdanau (Neural Machine Translation by Jointly Learning to Align and Translate), it seems as though values are the annotation vector $h$ but it's not clear as to what is meant by ""query"" and ""key.""
The paper that I mentioned states that attention is calculated by
$$c_i = \sum^{T_x}_{j = 1} \alpha_{ij} h_j$$
with
$$
\begin{align}
\alpha_{ij} & = \frac{e^{e_{ij}}}{\sum^{T_x}_{k = 1} e^{ik}} \\\\
e_{ij} & = a(s_{i - 1}, h_j)
\end{align}
$$
Where are people getting the key, query, and value from these equations?
Thank you.
","['neural-networks', 'natural-language', 'attention', 'machine-translation']","The key/value/query formulation of attention is from the paper Attention Is All You Need.How should one understand the queries, keys, and valuesThe key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).The attention operation can be thought of as a retrieval process as well.As mentioned in the paper you referenced (Neural Machine Translation by Jointly Learning to Align and Translate), attention by definition is just a weighted average of values,$$c=\sum_{j}\alpha_jh_j$$
where $\sum \alpha_j=1$.If we restrict $\alpha$ to be a one-hot vector, this operation becomes the same as retrieving from a set of elements $h$ with index $\alpha$. With the restriction removed, the attention operation can be thought of as doing ""proportional retrieval"" according to the probability vector $\alpha$.It should be clear that $h$ in this context is the value. The difference between the two papers lies in how the probability vector $\alpha$ is calculated. The first paper (Bahdanau et al. 2015) computes the score through a neural network $$e_{ij}=a(s_i,h_j), \qquad \alpha_{i,j}=\frac{\exp(e_{ij})}{\sum_k\exp(e_{ik})}$$
where $h_j$ is from the encoder sequence, and $s_i$ is from the decoder sequence. One problem of this approach is, say the encoder sequence is of length $m$ and the decoding sequence is of length $n$, we have to go through the network $m*n$ times to acquire all the attention scores $e_{ij}$.A more efficient model would be to first project $s$ and $h$ onto a common space, then choose a similarity measure (e.g. dot product) as the attention score, like
$$e_{ij}=f(s_i)g(h_j)^T$$
so we only have to compute $g(h_j)$ $m$ times and $f(s_i)$ $n$ times to get the projection vectors and $e_{ij}$ can be computed efficiently by matrix multiplication.This is essentially the approach proposed by the second paper (Vaswani et al. 2017), where the two projection vectors are called query (for decoder) and key (for encoder), which is well aligned with the concepts in retrieval systems. (There are later techniques to further reduce the computational complexity, for example Reformer, Linformer, FlashAttention.)How are the queries, keys, and values obtainedThe proposed multihead attention alone doesn't say much about how the queries, keys, and values are obtained, they can come from different sources depending on the application scenario.$$
\begin{align}\text{MultiHead($Q$, $K$, $V$)} & = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^{O} \\
\text{where head$_i$} &  = \text{Attention($QW_i^Q$, $KW_i^K$, $VW_i^V$)}
\end{align}$$
Where the projections are parameter matrices:
$$
\begin{align}
W_i^Q & \in \mathbb{R}^{d_\text{model} \times d_k}, \\
W_i^K & \in \mathbb{R}^{d_\text{model} \times d_k}, \\
W_i^V & \in \mathbb{R}^{d_\text{model} \times d_v}, \\
W_i^O & \in \mathbb{R}^{hd_v \times d_{\text{model}}}.
\end{align}$$For unsupervised language model training like GPT, $Q, K, V$ are usually from the same source, so such operation is also called self-attention.For the machine translation task in the second paper, it first applies self-attention separately to source and target sequences, then on top of that it applies another attention where $Q$ is from the target sequence and $K, V$ are from the source sequence.For recommendation systems, $Q$ can be from the target items, $K, V$ can be from the user profile and history."
ROC vs precision-and-recall curves,"
I understand the formal differences between them, what I want to know is when it is more relevant to use one vs. the other. 

Do they always provide complementary insight about the performance of a given classification/detection system? 
When is it reasonable to provide them both, say, in a paper? instead of just one?
Are there any alternative (maybe more modern) descriptors that capture the relevant aspects of both ROC and precision recall for a classification system?

I am interested in arguments for both binary and multi-class (e.g. as one-vs-all) cases.
","['machine-learning', 'roc', 'precision-recall']","The key difference is that ROC curves will be the same no matter what the baseline probability is, but PR curves may be more useful in practice for needle-in-haystack type problems or problems where the ""positive"" class is more interesting than the negative class.To show this, first let's start with a very nice way to define precision, recall and specificity.  Assume you have a ""positive"" class called 1 and a ""negative"" class called 0.  $\hat{Y}$ is your estimate of the true class label $Y$.  Then:
$$
\begin{aligned}
&\text{Precision} &= P(Y = 1 | \hat{Y} = 1)  \\
&\text{Recall} = \text{Sensitivity} &= P(\hat{Y} = 1 | Y = 1)  \\
&\text{Specificity} &= P(\hat{Y} = 0 | Y = 0)
\end{aligned}
$$
The key thing to note is that sensitivity/recall and specificity, which make up the ROC curve, are probabilities conditioned on the true class label.  Therefore, they will be the same regardless of what $P(Y = 1)$ is.  Precision is a probability conditioned on your estimate of the class label and will thus vary if you try your classifier in different populations with different baseline $P(Y = 1)$.  However, it may be more useful in practice if you only care about one population with known background probability and the ""positive"" class is much more interesting than the ""negative"" class.  (IIRC precision is popular in the document retrieval field, where this is the case.)  This is because it directly answers the question, ""What is the probability that this is a real hit given my classifier says it is?"". Interestingly, by Bayes' theorem you can work out cases where specificity can be very high and precision very low simultaneously.  All you have to do is assume $P(Y = 1)$ is very close to zero.  In practice I've developed several classifiers with this performance characteristic when searching for needles in DNA sequence haystacks.IMHO when writing a paper you should provide whichever curve answers the question you want answered (or whichever one is more favorable to your method, if you're cynical).  If your question is: ""How meaningful is a positive result from my classifier given the baseline probabilities of my problem?"", use a PR curve.  If your question is, ""How well can this classifier be expected to perform in general, at a variety of different baseline probabilities?"", go with a ROC curve."
Why is Newton's method not widely used in machine learning?,"
This is something that has been bugging me for a while, and I couldn't find any satisfactory answers online, so here goes:
After reviewing a set of lectures on convex optimization, Newton's method seems to be a far superior algorithm than gradient descent to find globally optimal solutions, because Newton's method can provide a guarantee for its solution, it's affine invariant, and most of all it converges in far fewer steps. Why is second-order optimization algorithms, such as Newton's method not as widely used as stochastic gradient descent in machine learning problems?
","['machine-learning', 'optimization', 'gradient-descent', 'hessian']",
What is the best introductory Bayesian statistics textbook?,"
Which is the best introductory textbook for Bayesian statistics?
One book per answer, please.
","['bayesian', 'references']",
Algorithms for automatic model selection,"
I would like to implement an algorithm for automatic model selection. 
I am thinking of doing stepwise regression but anything will do (it has to be based on linear regressions though). 
My problem is that I am unable to find a methodology, or an open source implementation (I am woking in java). The methodology I have in mind would be something like:

calculate the correlation matrix of all the factors
pick the factors that have a low correlation to each other
remove the factors that have a low t-stat
add other factors (still based on the low correlation factor found in 2.).
reiterate several times until some criterion (e.g AIC) is over a certain threshold or cannot or we can't find a larger value.

I realize there is an R implementation for this (stepAIC), but I find the code quite hard to understand. Also I have not been able to find articles describing the stepwise regression.
","['feature-selection', 'model-selection', 'aic', 'stepwise-regression', 'faq']",
When (and why) should you take the log of a distribution (of numbers)?,"
Say I have some historical data e.g., past stock prices, airline ticket price fluctuations, past financial data of the company...
Now someone (or some formula) comes along and says ""let's take/use the log of the distribution"" and here's where I go WHY?
Questions:

WHY should one take the log of the distribution in the first place?
WHAT does the log of the distribution 'give/simplify' that the original distribution couldn't/didn't?
Is the log transformation 'lossless'? I.e., when transforming to log-space and analyzing the data, do the same conclusions hold for the original distribution? How come?
And lastly WHEN to take the log of the distribution? Under what conditions does one decide to do this?

I've really wanted to understand log-based distributions (for example lognormal) but I never understood the when/why aspects - i.e., the log of the distribution is a normal distribution, so what? What does that even tell and me and why bother? Hence the question!
UPDATE: As per @whuber's comment I looked at the posts and for some reason I do understand the use of log transforms and their application in linear regression, since you can draw a relation between the independent variable and the log of the dependent variable. However, my question is generic in the sense of analyzing the distribution itself - there is no relation per se that I can conclude to help understand the reason of taking logs to analyze a distribution. I hope I'm making sense :-/
In regression analysis you do have constraints on the type/fit/distribution of the data and you can transform it and define a relation between the independent and (not transformed) dependent variable. But when/why would one do that for a distribution in isolation where constraints of type/fit/distribution are not necessarily applicable in a framework (like regression). I hope the clarification makes things more clear than confusing :)
This question deserves a clear answer as to ""WHY and WHEN""
","['distributions', 'data-transformation', 'logarithm']","If you assume a model form that is non-linear but can be transformed to a linear model such as $\log Y = \beta_0 + \beta_1t$ then one would be justified in taking logarithms of $Y$ to meet the specified model form. In general whether or not you have causal series , the only time you would be justified or correct in taking the Log of $Y$ is when it can be proven that the Variance of $Y$ is proportional to the Expected Value of $Y^2$ . I don't remember the original source for the following but it nicely summarizes the role of power transformations. It is important to note that the distributional assumptions are always about the error process not the observed Y, thus it is a definite ""no-no"" to analyze the original series for an appropriate transformation unless the series is defined by a simple constant.Unwarranted or incorrect transformations including differences should be studiously avoided as they are often an ill-fashioned /ill-conceived attempt to deal with unidentified anomalies/level shifts/time trends or changes in parameters or changes in error variance. A classic example of this is discussed starting at slide 60 here http://www.autobox.com/cms/index.php/afs-university/intro-to-forecasting/doc_download/53-capabilities-presentation where three pulse anomalies (untreated) led to an unwarranted log transformation by early researchers. Unfortunately some of our current researchers are still making the same mistake.The optimal power transformation is found via the Box-Cox Test  whereNote that when you have no predictor/causal/supporting input series, the model is $Y_t=u +a_t$ and that there are no requirements made about the distribution of $Y$ BUT are made about $a_t$, the error process. In this case the distributional requirements about $a_t$ pass directly on to $Y_t$. When you have supporting series such as in a regression or in a Autoregressive–moving-average model with exogenous inputs model (ARMAX model) the distributional assumptions are all about $a_t$ and have nothing whatsoever to do with the distribution of $Y_t$. Thus in the case of ARIMA model or an ARMAX Model one would never assume any transformation on $Y$ before finding the optimal Box-Cox transformation which would then suggest the remedy (transformation) for $Y$. In earlier times some analysts would transform both $Y$ and $X$ in a presumptive way just to be able to reflect upon the percent change in $Y$ as a result in the percent change in $X$ by examining the regression coefficient between $\log Y$ and $\log X$. In summary, transformations are like drugs some are good and some are bad for you! They should only be used when necessary and then with caution."
How to interpret a QQ plot,"
I am working with a small dataset (21 observations) and have the following normal QQ plot in R: 

Seeing that the plot does not support normality, what could I infer about the underlying distribution? It seems to me that a distribution more skewed to the right would be a better fit, is that right? Also, what other conclusions can we draw from the data?
","['r', 'data-visualization', 'inference', 'qq-plot', 'faq']","If the values lie along a line the distribution has the same shape (up to location and scale) as the theoretical distribution we have supposed.Local behaviour: When  looking at sorted sample values on the y-axis and (approximate) expected quantiles on the x-axis, we can identify from how the values in some section of the plot differ  locally from an overall linear trend by seeing whether the values are more or less concentrated than the theoretical distribution would suppose in that section of a plot:As we see, less concentrated points increase more and more concentrated points increase less rapidly than an overall linear relation would suggest, and in the extreme cases correspond to a gap in the density of the sample (shows as a near-vertical jump) or a spike of constant values (values aligned horizontally). This allows us to spot a heavy tail or a light tail and hence, skewness greater or smaller than the theoretical distribution, and so on.Overall apppearance:Here's what QQ-plots look like (for particular choices of distribution) on average:But randomness tends to obscure things, especially with small samples:Note that at $n=21$ the results may be much more variable than shown there - I generated several such sets of six plots and chose a 'nice' set where you could kind of see the shape in all six plots at the same time. Sometimes straight relationships look curved, curved relationships look straight, heavy-tails just look skew, and so on - with such small samples, often the situation may be much less clear:It's possible to discern more features than those (such as discreteness, for one example), but with $n=21$, even such basic features may be hard to spot; we shouldn't try to 'over-interpret' every little wiggle. As sample sizes become larger, generally speaking the plots 'stabilize' and the features become more clearly interpretable rather than representing noise. [With some very heavy-tailed distributions, the rare large outlier might prevent the picture stabilizing nicely even at quite large sample sizes.]You may also find the suggestion here useful when trying to decide how much you should worry about a particular amount of curvature or wiggliness.A more suitable guide for interpretation in general would also include displays at smaller and larger sample sizes."
How should I transform non-negative data including zeros?,"
If I have highly skewed positive data I often take logs. But what should I do with highly skewed non-negative data that include zeros? I have seen two transformations used:

$\log(x+1)$ which has the neat feature that 0 maps to 0.
$\log(x+c)$ where c is either estimated or set to be some very small positive value.

Are there any other approaches? Are there any good reasons to prefer one approach over the others?
","['data-transformation', 'large-data']","It seems to me that the most appropriate choice of transformation is contingent on the model and the context. The '0' point can arise from several different reasons each of which may have to be treated differently:I am not really offering an answer as I suspect there is no universal, 'correct' transformation when you have zeros."
Intuitive explanation for dividing by $n-1$ when calculating standard deviation?,"
I was asked today in class why you divide the sum of square error by $n-1$ instead of with $n$, when calculating the standard deviation.
I said I am not going to answer it in class (since I didn't wanna go into unbiased estimators), but later I wondered - is there an intuitive explanation for this?!
","['standard-error', 'intuition', 'teaching', 'bessels-correction', 'faq']","The standard deviation calculated with a divisor of $n-1$ is a standard deviation calculated from the sample as an estimate of the standard deviation of the population from which the sample was drawn. Because the observed values fall, on average, closer to the sample mean than to the population mean, the standard deviation which is calculated using deviations from the sample mean underestimates the desired standard deviation of the population. Using $n-1$ instead of $n$ as the divisor corrects for that by making the result a little bit bigger.Note that the correction has a larger proportional effect when $n$ is small than when it is large, which is what we want because when n is larger the sample mean is likely to be a good estimator of the population mean.When the sample is the whole population we use the standard deviation with $n$ as the divisor because the sample mean is population mean.(I note parenthetically that nothing that starts with ""second moment recentered around a known, definite mean"" is going to fulfil the questioner's request for an intuitive explanation.)"
Why is accuracy not the best measure for assessing classification models?,"
This is a general question that was asked indirectly multiple times in here, but it lacks a single authoritative answer. It would be great to have a detailed answer to this for the reference.
Accuracy, the proportion of correct classifications among all classifications, is very simple and very ""intuitive"" measure, yet it may be a poor measure for imbalanced data. Why does our intuition misguide us here and are there any other problems with this measure? 
","['machine-learning', 'model-evaluation', 'accuracy', 'scoring-rules', 'faq']",
What are the advantages of ReLU over sigmoid function in deep neural networks?,"
The state of the art of non-linearity is to use rectified linear units (ReLU) instead of sigmoid function in deep neural network. What are the advantages?
I know that training a network when ReLU is used would be faster, and it is more biological inspired, what are the other advantages? (That is, any disadvantages of using sigmoid)?
","['machine-learning', 'neural-networks', 'sigmoid-curve']","Two additional major benefits of ReLUs are sparsity and a reduced likelihood of vanishing gradient. But first recall the definition of a ReLU is $h = \max(0, a)$ where $a = Wx + b$.One major benefit is the reduced likelihood of the gradient to vanish. This arises when $a > 0$. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.The other benefit of ReLUs is sparsity. Sparsity arises when $a \le 0$. The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations. Sparse representations seem to be more beneficial than dense representations."
"Which ""mean"" to use and when?","
So we have arithmetic mean (AM), geometric mean (GM) and harmonic mean (HM). Their mathematical formulation is also well known along with their associated stereotypical examples (e.g., Harmonic mean and it's application to 'speed' related problems).
However, a question that has always intrigued me is ""how do I decide which mean is the most appropriate to use in a given context?"" There must be at least some rule of thumb to help understand the applicability and yet the most common answer I've come across is: ""It depends"" (but on what?).
This may seem to be a rather trivial question but even high-school texts failed to explain this -- they only provide mathematical definitions!
I prefer an English explanation over a mathematical one -- simple test would be ""would your mom/child understand it?""
","['mean', 'geometric-mean', 'types-of-averages', 'harmonic-mean']","This answer may have a slightly more mathematical bent than you were looking for.The important thing to recognize is that all of these means are simply the arithmetic mean in disguise.The important characteristic in identifying which (if any!) of the three common means (arithmetic, geometric or harmonic) is the ""right"" mean is to find the ""additive structure"" in the question at hand.In other words suppose we're given some abstract quantities $x_1, x_2,\ldots,x_n$, which I will call ""measurements"", somewhat abusing this term below for the sake of consistency. Each of these three means can be obtained by (1) transforming each $x_i$ into some $y_i$, (2) taking the arithmetic mean and then (3) transforming back to the original scale of measurement.Arithmetic mean: Obviously, we use the ""identity"" transformation: $y_i = x_i$. So, steps (1) and (3) are trivial (nothing is done) and $\bar x_{\mathrm{AM}} = \bar y$.Geometric mean: Here the additive structure is on the logarithms of the original observations. So, we take $y_i = \log x_i$ and then to get the GM in step (3) we convert back via the inverse function of the $\log$, i.e., $\bar x_{\mathrm{GM}} = \exp(\bar{y})$.Harmonic mean: Here the additive structure is on the reciprocals of our observations. So, $y_i = 1/x_i$, whence $\bar x_{\mathrm{HM}} = 1/\bar{y}$.In physical problems, these often arise through the following process: We have some quantity $w$ that remains fixed in relation to our measurements $x_1,\ldots,x_n$ and some other quantities, say $z_1,\ldots,z_n$. Now, we play the following game: Keep $w$ and $z_1+\cdots+z_n$ constant and try to find some $\bar x$ such that if we replace each of our individual observations $x_i$ by $\bar x$, then the ""total"" relationship is still conserved.The distance–velocity–time example appears to be popular, so let's use it.Constant distance, varying timesConsider a fixed distance traveled $d$. Now suppose we travel this distance $n$ different times at speeds $v_1,\ldots,v_n$, taking times $t_1,\ldots,t_n$. We now play our game. Suppose we wanted to replace our individual velocities with some fixed velocity $\bar v$ such that the total time remains constant. Note that we have
$$
d - v_i t_i = 0 \>,
$$
so that $\sum_i (d - v_i t_i) = 0$. We want this total relationship (total time and total distance traveled) conserved when we replace each of the $v_i$ by $\bar v$ in our game. Hence,
$$
n d - \bar v \sum_i t_i = 0 \>,
$$
and since each $t_i = d / v_i$, we get that
$$
\bar v = \frac{n}{\frac{1}{v_1}+\cdots+\frac{1}{v_n}} = \bar v_{\mathrm{HM}} \>.
$$Note that the ""additive structure"" here is with respect to the individual times, and our measurements are inversely related to them, hence the harmonic mean applies.Varying distances, constant timeNow, let's change the situation. Suppose that for $n$ instances we travel a fixed time $t$ at velocities $v_1,\ldots,v_n$ over distances $d_1,\ldots,d_n$. Now, we want the total distance conserved. We have
$$
d_i - v_i t = 0 \>,
$$
and the total system is conserved if $\sum_i (d_i - v_i t) = 0$. Playing our game again, we seek a $\bar v$ such that
$$
\sum_i (d_i - \bar v t) = 0 \>,
$$
but, since $d_i = v_i t$, we get that 
$$
\bar v = \frac{1}{n} \sum_i v_i = \bar v_{\mathrm{AM}} \>.
$$Here the additive structure we are trying to maintain is proportional to the measurements we have, so the arithmetic mean applies.Equal volume cubeSuppose we have constructed an $n$-dimensional box with a given volume $V$ and our measurements are the side-lengths of the box. Then
$$
V = x_1 \cdot x_2 \cdots x_n \>,
$$
and suppose we wanted to construct an $n$-dimensional (hyper)cube with the same volume. That is, we want to replace our individual side-lengths $x_i$ by a common side-length $\bar x$. Then
$$
V = \bar x \cdot \bar x \cdots \bar x = \bar x^n \>.
$$This easily indicates that we should take $\bar x = (x_i \cdots x_n)^{1/n} = \bar x_{\mathrm{GM}}$.Note that the additive structure is in the logarithms, that is, $\log V = \sum_i \log x_i$ and we are trying to conserve the left-hand quantity.New means from oldAs an exercise, think about what the ""natural"" mean is in the situation where you let both the distances and times vary in the first example. That is, we have distances $d_i$, velocities $v_i$ and times $t_i$. We want to conserve the total distance and time traveled and find a constant $\bar v$ to achieve this.Exercise: What is the ""natural"" mean in this situation?"
R's lmer cheat sheet,"
There's a lot of discussion going on on this forum about the proper way to specify various hierarchical models using lmer.
I thought it would be great to have all the information in one place.
A couple of questions to start:

How to specify multiple levels, where one group is nested within the other: is it (1|group1:group2) or (1+group1|group2)?
What's the difference between (~1 + ....) and (1 | ...) and (0 | ...) etc.?
How to specify group-level interactions?

","['r', 'mixed-model', 'random-effects-model', 'fixed-effects-model', 'lme4-nlme']","What's the difference between (~1 +....) and (1 | ...) and (0 | ...) etc.?Say you have variable V1 predicted by categorical variable V2, which is treated as a random effect, and continuous variable V3, which is treated as a linear fixed effect. Using lmer syntax, simplest model (M1) is:This model will estimate:P1: A global interceptP2: Random effect intercepts for V2 (i.e. for each level of V2, that level's intercept's deviation from the global intercept)P3: A single global estimate for the effect (slope) of V3The next most complex model (M2) is:This model estimates all the parameters from M1, but will additionally estimate:P4: The effect of V3 within each level of V2 (more specifically, the degree to which the V3 effect within a given level deviates from the global effect of V3), while enforcing a zero correlation between the intercept deviations and V3 effect deviations across levels of V2. This latter restriction is relaxed in a final most complex model (M3):In which all parameters from M2 are estimated while allowing correlation between the intercept deviations and V3 effect deviations within levels of V2. Thus, in M3, an additional parameter is estimated:P5: The correlation between intercept deviations and V3 deviations across levels of V2Usually model pairs like M2 and M3 are computed then compared to evaluate the evidence for correlations between fixed effects (including the global intercept).Now consider adding another fixed effect predictor, V4. The model:would estimate:P1: A global interceptP2: A single global estimate for the effect of V3P3: A single global estimate for the effect of V4P4: A single global estimate for the interaction between V3 and V4P5: Deviations of the intercept from P1 in each level of V2P6: Deviations of the V3 effect from P2 in each level of V2P7: Deviations of the V4 effect from P3 in each level of V2P8: Deviations of the V3-by-V4 interaction from P4 in each level of V2P9 Correlation between P5 and P6 across levels of V2P10 Correlation between P5 and P7 across levels of V2P11 Correlation between P5 and P8 across levels of V2P12 Correlation between P6 and P7 across levels of V2P13 Correlation between P6 and P8 across levels of V2P14 Correlation between P7 and P8 across levels of V2Phew, That's a lot of parameters! And I didn't even bother to list the variance parameters estimated by the model. What's more, if you have a categorical variable with more than 2 levels that you want to model as a fixed effect, instead of a single effect for that variable you will always be estimating k-1 effects (where k is the number of levels), thereby exploding the number of parameters to be estimated by the model even further."
"What is the difference between data mining, statistics, machine learning and AI?","
What is the difference between data mining, statistics, machine learning and AI?
Would it be accurate to say that they are 4 fields attempting to solve very similar problems but with different approaches? What exactly do they have in common and where do they differ? If there is some kind of hierarchy between them, what would it be?
Similar questions have been asked previously but I still don't get it:

Data Mining and Statistical Analysis
The Two Cultures: statistics vs. machine learning?

","['machine-learning', 'data-mining']",
Can principal component analysis be applied to datasets containing a mix of continuous and categorical variables?,"
I have a dataset that has both continuous and categorical data. I am analyzing by using PCA and am wondering if it is fine to include the categorical variables as a part of the analysis. My understanding is that PCA can only be applied to continuous variables. Is that correct? If it cannot be used for categorical data, what alternatives exist for their analysis? 
","['categorical-data', 'pca', 'correspondence-analysis', 'mixed-type-data']",
"In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?","
Am I looking for a better behaved distribution for the independent variable in question, or to reduce the effect of outliers, or something else?
","['regression', 'distributions', 'data-transformation', 'logarithm', 'faq']",
What does the hidden layer in a neural network compute?,"
I'm sure many people will respond with links to 'let me google that for you', so I want to say that I've tried to figure this out so please forgive my lack of understanding here, but I cannot figure out how the practical implementation of a neural network actually works. 
I understand the input layer and how to normalize the data, I also understand the bias unit, but when it comes to the hidden layer, what the actual computation is in that layer, and how it maps to the output is just a little foggy. I've seen diagrams with question marks in the hidden layer, boolean functions like AND/OR/XOR, activation functions, and input nodes that map to all of the hidden units and input nodes that map to only a few hidden units each and so I just have a few questions on the practical aspect. Of course, a simple explanation of the entire neural network process like you would explain to a child, would be awesome. 
What computations are done in the hidden layer?
How are those computations mapped to the output layer? 
How does the ouput layer work? De-normalizing the data from the hidden layer? 
Why are some layers in the input layer connected to the hidden layer and some are not?
","['machine-learning', 'neural-networks', 'nonlinear-regression']","Three sentence version:Each layer can apply any function you want to the previous layer (usually a linear transformation followed by a squashing nonlinearity). The hidden layers' job is to transform the inputs into something that the output layer can use.The output layer transforms the hidden layer activations into whatever scale you wanted your output to be on.Like you're 5:If you want a computer to tell you if there's a bus in a picture, the computer might have an easier time if it had the right tools.So your bus detector might be made of a wheel detector (to help tell you it's a vehicle) and a box detector (since the bus is shaped like a big box) and a size detector (to tell you it's too big to be a car).  These are the three elements of your hidden layer: they're not part of the raw image, they're tools you designed to help you identify busses.If all three of those detectors turn on (or perhaps if they're especially active), then there's a good chance you have a bus in front of you.Neural nets are useful because there are good tools (like backpropagation) for building lots of detectors and putting them together.Like you're an adultA feed-forward neural network applies a series of functions to the data.  The exact functions will depend on the neural network you're using: most frequently, these functions each compute a linear transformation of the previous layer, followed by a squashing nonlinearity.  Sometimes the functions will do something else (like computing logical functions in your examples, or averaging over adjacent pixels in an image).  So the roles of the different layers could depend on what functions are being computed, but I'll try to be very general.Let's call the input vector $x$, the hidden layer activations $h$, and the output activation $y$.  You have some function $f$ that maps from $x$ to $h$ and another function $g$ that maps from $h$ to $y$.  So the hidden layer's activation is $f(x)$ and the output of the network is $g(f(x))$.Why have two functions ($f$ and $g$) instead of just one?If the level of complexity per function is limited, then $g(f(x))$ can compute things that $f$ and $g$ can't do individually.  An example with logical functions:For example, if we only allow $f$ and $g$ to be simple logical operators like ""AND"", ""OR"", and ""NAND"", then you can't compute other functions like ""XOR"" with just one of them.  On the other hand, we could compute ""XOR"" if we were willing to layer these functions on top of each other: First layer functions:Second layer function:The network's output is just the result of this second function.  The first layer transforms the inputs into something that the second layer can use so that the whole network can perform XOR.An example with images:Slide 61 from this talk--also available here as a single image--shows (one way to visualize) what the different hidden layers in a particular neural network are looking for.The first layer looks for short pieces of edges in the image: these are very easy to find from raw pixel data, but they're not very useful by themselves for telling you if you're looking at a face or a bus or an elephant.The next layer composes the edges: if the edges from the bottom hidden layer fit together in a certain way, then one of the eye-detectors in the middle of left-most column might turn on.  It would be hard to make a single layer that was so good at finding something so specific from the raw pixels: eye detectors are much easier to build out of edge detectors than out of raw pixels.The next layer up composes the eye detectors and the nose detectors into faces.  In other words, these will light up when the eye detectors and nose detectors from the previous layer turn on with the right patterns.  These are very good at looking for particular kinds of faces: if one or more of them lights up, then your output layer should report that a face is present.This is useful because face detectors are easy to build out of eye detectors and nose detectors, but really hard to build out of pixel intensities.So each layer gets you farther and farther from the raw pixels and closer to your ultimate goal (e.g. face detection or bus detection).Answers to assorted other questions""Why are some layers in the input layer connected to the hidden layer and some are not?""The disconnected nodes in the network are called ""bias"" nodes. There's a really nice explanation here. The short answer is that they're like intercept terms in regression.""Where do the ""eye detector"" pictures in the image example come from?""I haven't double-checked the specific images I linked to, but in general, these visualizations show the set of pixels in the input layer that maximize the activity of the corresponding neuron.  So if we think of the neuron as an eye detector, this is the image that the neuron considers to be most eye-like.  Folks usually find these pixel sets with an optimization (hill-climbing) procedure.In this paper by some Google folks with one of the world's largest neural nets, they show a ""face detector"" neuron and a ""cat detector"" neuron this way, as well as a second way: They also show the actual images that activate the neuron most strongly (figure 3, figure 16).  The second approach is nice because it shows how flexible and nonlinear the network is--these high-level ""detectors"" are sensitive to all these images, even though they don't particularly look similar at the pixel level.Let me know if anything here is unclear or if you have any more questions."
Generative vs. discriminative,"
I know that generative means ""based on $P(x,y)$"" and discriminative means ""based on $P(y|x)$,"" but I'm confused on several points:

Wikipedia (+ many other hits on the web) classify things like SVMs and decision trees as being discriminative. But these don't even have probabilistic interpretations. What does discriminative mean here? Has discriminative just come to mean anything that isn't generative?
Naive Bayes (NB) is generative because it captures $P(x|y)$ and $P(y)$, and thus you have $P(x,y)$ (as well as $P(y|x)$). Isn't it trivial to make, say, logistic regression (the poster boy of discriminative models) ""generative"" by simply computing $P(x)$ in a similar fashion (same independence assumption as NB, such that $P(x) = P(x_0) P(x_1) ... P(x_d)$, where the MLE for $P(x_i)$ are just frequencies)?
I know that discriminative models tend to outperform generative ones. What's the practical use of working with generative models? Being able to generate/simulate data is cited, but when does this come up? I personally only have experience with regression, classification, collab. filtering over structured data, so are the uses irrelevant to me here? The ""missing data"" argument ($P(x_i|y)$ for missing $x_i$) seems to only give you an edge with training data (when you actually know $y$ and don't need to marginalize over $P(y)$ to get the relatively dumb $P(x_i)$ which you could've estimated directly anyway), and even then imputation is much more flexible (can predict based not just on $y$ but other $x_i$'s as well).
What's with the completely contradictory quotes from Wikipedia? ""generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks"" vs. ""discriminative models can generally express more complex relationships between the observed and target variables""

Related question that got me thinking about this.
","['machine-learning', 'generative-models']","The fundamental difference between discriminative models and generative models is:To answer your direct questions:SVMs (Support Vector Machines) and DTs (Decision Trees) are discriminative because they learn explicit boundaries between classes.  SVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel.  The distance between a sample and the learned decision boundary can be used to make the SVM a ""soft"" classifier.  DTs learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion).It is possible to make a generative form of logistic regression in this manner.  Note that you are not using the full generative model to make classification decisions, though.There are a number of advantages generative models may offer, depending on the application.  Say you are dealing with non-stationary distributions, where the online test data may be generated by different underlying distributions than the training data.  It is typically more straightforward to detect distribution changes and update a generative model accordingly than do this for a decision boundary in an SVM, especially if the online updates need to be unsupervised.  Discriminative models also do not generally function for outlier detection, though generative models generally do.  What's best for a specific application should, of course, be evaluated based on the application.(This quote is convoluted, but this is what I think it's trying to say) Generative models are typically specified as probabilistic graphical models, which offer rich representations of the independence relations in the dataset.  Discriminative models do not offer such clear representations of relations between features and classes in the dataset.  Instead of using resources to fully model each class, they focus on richly modeling the boundary between classes.  Given the same amount of capacity (say, bits in a computer program executing the model), a discriminative model thus may yield more complex representations of this boundary than a generative model."
How exactly does one “control for other variables”?,"
Here is the article that motivated this question: Does impatience make us fat?
I liked this article, and it nicely demonstrates the concept of “controlling for other variables” (IQ, career, income, age, etc) in order to best isolate the true relationship between just the 2 variables in question.  
Can you explain to me how you actually control for variables on a typical data set?    
E.g., if you have 2 people with the same impatience level and BMI, but different incomes, how do you treat these data?  Do you categorize them into different subgroups that do have similar income, patience, and BMI?  But, eventually there are dozens of variables to control for (IQ, career, income, age, etc)  How do you then aggregate these (potentially) 100’s of subgroups?  In fact, I have a feeling this approach is barking up the wrong tree, now that I’ve verbalized it.
Thanks for shedding any light on something I've meant to get to the bottom of for a few years now...!
","['regression', 'causality', 'confounding', 'controlling-for-a-variable', 'statistics-in-media']",
How to deal with perfect separation in logistic regression?,"
If you have a variable which perfectly separates zeroes and ones in target variable, R will yield the following ""perfect or quasi perfect separation"" warning message:
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 

We still get the model but the coefficient estimates are inflated. 
How do you deal with this in practice?
","['r', 'regression', 'logistic', 'separation', 'faq']","A solution to this is to utilize a form of penalized regression. In fact, this is the original reason some of the penalized regression forms were developed (although they turned out to have other interesting properties.Install and load package glmnet in R and you're mostly ready to go. One of the less user-friendly aspects of glmnet is that you can only feed it matrices, not formulas as we're used to. However, you can look at model.matrix and the like to construct this matrix from a data.frame and a formula...Now, when you expect that this perfect separation is not just a byproduct of your sample, but could be true in the population, you specifically don't want to handle this: use this separating variable simply as the sole predictor for your outcome, not employing a model of any kind."
PCA on correlation or covariance?,"
What are the main differences between performing principal component analysis (PCA) on the correlation matrix and on the covariance matrix? Do they give the same results?
","['correlation', 'pca', 'covariance', 'factor-analysis']","You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales.Using the correlation matrix is equivalent to standardizing each of the variables (to mean 0 and standard deviation 1). In general, PCA with and without standardizing will give different results. Especially when the scales are different.As an example, take a look at this R heptathlon data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (run 800m) are around 120.This outputs:Now let's do PCA on covariance and on correlation:Notice that PCA on covariance is dominated by run800m and javelin: PC1 is almost equal to run800m (and explains $82\%$ of the variance) and PC2 is almost equal to javelin (together they explain $97\%$). PCA on correlation is much more informative and reveals some structure in the data and relationships between variables (but note that the explained variances drop to $64\%$ and $71\%$).Notice also that the outlying individuals (in this data set) are outliers regardless of whether the covariance or correlation matrix is used."
When should I use lasso vs ridge?,"
Say I want to estimate a large number of parameters, and I want to penalize some of them because I believe they should have little effect compared to the others. How do I decide what penalization scheme to use? When is ridge regression more appropriate? When should I use lasso?
","['regression', 'lasso', 'ridge-regression']","Keep in mind that ridge regression can't zero out coefficients; thus, you either end up including all the coefficients in the model, or none of them. In contrast, the LASSO does both parameter shrinkage and variable selection automatically. If some of your covariates are highly correlated, you may want to look at the Elastic Net [3] instead of the LASSO.I'd personally recommend using the Non-Negative Garotte (NNG) [1] as it's consistent in terms of estimation and variable selection [2]. Unlike LASSO and ridge regression, NNG requires an initial estimate that is then shrunk towards the origin. In the original paper, Breiman recommends the least-squares solution for the initial estimate (you may however want to start the search from a ridge regression solution and use something like GCV to select the penalty parameter).In terms of available software, I've implemented the original NNG in MATLAB (based on Breiman's original FORTRAN code). You can download it from: http://www.emakalic.org/blog/wp-content/uploads/2010/04/nngarotte.zipBTW, if you prefer a Bayesian solution, check out [4,5].References:[1] Breiman, L. Better Subset Regression Using the Nonnegative Garrote Technometrics, 1995, 37, 373-384[2] Yuan, M. & Lin, Y. On the non-negative garrotte estimator Journal of the Royal Statistical Society (Series B), 2007, 69, 143-161[3] Zou, H. & Hastie, T. Regularization and variable selection via the elastic net Journal of the Royal Statistical Society (Series B), 2005, 67, 301-320[4] Park, T. & Casella, G. The Bayesian Lasso Journal of the American Statistical Association, 2008, 103, 681-686[5] Kyung, M.; Gill, J.; Ghosh, M. & Casella, G. Penalized Regression, Standard Errors, and Bayesian Lassos Bayesian Analysis, 2010, 5, 369-412"
What does 1x1 convolution mean in a neural network?,"
I am currently doing the Udacity Deep Learning Tutorial. In Lesson 3, they talk about a 1x1 convolution. This 1x1 convolution is used in Google Inception Module. I'm having trouble understanding what is a 1x1 convolution.
I have also seen this post by Yann Lecun.
Could someone kindly explain this to me?
","['neural-networks', 'deep-learning', 'convolution', 'conv-neural-network']","Suppose that I have a conv layer which outputs an $(N, F, H, W)$ shaped tensor where:Suppose the input is fed into a conv layer with $F_1$ 1x1 filters, zero padding and stride 1. Then the output of this 1x1 conv layer will have shape $(N, F_1, H , W)$.So 1x1 conv filters can be used to change the dimensionality in the filter space. If $F_1 > F$ then we are increasing dimensionality, if $F_1 < F$ we are decreasing dimensionality, in the filter dimension.Indeed, in the Google Inception article Going Deeper with Convolutions, they state (bold is mine, not by original authors):One big problem with the above modules, at least in this naive form, is that even a modest number of 5x5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters.This leads to the second idea of the proposed architecture:  judiciously applying dimension reductions and projections wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch...1x1
convolutions are used to compute reductions before the expensive 3x3 and 5x5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose.So in the Inception architecture, we use the 1x1 convolutional filters to reduce dimensionality in the filter dimension. As I explained above, these 1x1 conv layers can be used in general to change the filter space dimensionality (either increase or decrease) and in the Inception architecture we see how effective these 1x1 filters can be for dimensionality reduction, explicitly in the filter dimension space, not the spatial dimension space.Perhaps there are other interpretations of 1x1 conv filters, but I prefer this explanation, especially in the context of the Google Inception architecture."
How to determine which distribution fits my data best?,"
I have a dataset and would like to figure out which distribution fits my data best. 
I used the fitdistr() function to estimate the necessary parameters to describe the assumed distribution (i.e. Weibull, Cauchy, Normal). Using those parameters I can conduct a Kolmogorov-Smirnov Test to estimate whether my sample data is from the same distribution as my assumed distribution.
If the p-value is > 0.05 I can assume that the sample data is drawn from the same distribution. But the p-value doesn't provide any information about the godness of fit, isn't it? 
So in case the p-value of my sample data is > 0.05 for a normal distribution as well as a weibull distribution, how can I know which distribution fits my data better? 
This is basically the what I have done:
> mydata
 [1] 37.50 46.79 48.30 46.04 43.40 39.25 38.49 49.51 40.38 36.98 40.00
[12] 38.49 37.74 47.92 44.53 44.91 44.91 40.00 41.51 47.92 36.98 43.40
[23] 42.26 41.89 38.87 43.02 39.25 40.38 42.64 36.98 44.15 44.91 43.40
[34] 49.81 38.87 40.00 52.45 53.13 47.92 52.45 44.91 29.54 27.13 35.60
[45] 45.34 43.37 54.15 42.77 42.88 44.26 27.14 39.31 24.80 16.62 30.30
[56] 36.39 28.60 28.53 35.84 31.10 34.55 52.65 48.81 43.42 52.49 38.00
[67] 38.65 34.54 37.70 38.11 43.05 29.95 32.48 24.63 35.33 41.34

# estimate shape and scale to perform KS-test for weibull distribution
> fitdistr(mydata, ""weibull"")
     shape        scale   
   6.4632971   43.2474500 
 ( 0.5800149) ( 0.8073102)

# KS-test for weibull distribution
> ks.test(mydata, ""pweibull"", scale=43.2474500, shape=6.4632971)

        One-sample Kolmogorov-Smirnov test

data:  mydata
D = 0.0686, p-value = 0.8669
alternative hypothesis: two-sided

# KS-test for normal distribution
> ks.test(mydata, ""pnorm"", mean=mean(mydata), sd=sd(mydata))

        One-sample Kolmogorov-Smirnov test

data:  mydata
D = 0.0912, p-value = 0.5522
alternative hypothesis: two-sided

The p-values are 0.8669 for the Weibull distribution, and 0.5522 for the normal distribution. Thus I can assume that my data follows a Weibull as well as a normal distribution. But which distribution function describes my data better? 

Referring to elevendollar I found the following code, but don't know how to interpret the results:
fits <- list(no = fitdistr(mydata, ""normal""),
             we = fitdistr(mydata, ""weibull""))
sapply(fits, function(i) i$loglik)
       no        we 
-259.6540 -257.9268 

","['r', 'distributions', 'goodness-of-fit', 'kolmogorov-smirnov-test', 'distribution-identification']","First, here are some quick comments:But let's do some exploration. I will use the excellent fitdistrplus package which offers some nice functions for distribution fitting. We will use the functiondescdist to gain some ideas about possible candidate distributions.Now let's use descdist:The kurtosis and squared skewness of your sample are plotted as a blue point named ""Observation"". It seems that possible distributions include the Weibull, Lognormal and possibly the Gamma distribution.Let's fit a Weibull distribution and a normal distribution:Now inspect the fit for the normal:And for the Weibull fit:Both look good but judged by the QQ-Plot, the Weibull maybe looks a bit better, especially in the tails. Correspondingly, the AIC of the Weibull fit is lower compared with the normal fit:I will use @Aksakal's procedure explained here to simulate the KS-statistic under the null.The ECDF of the simulated KS-statistics looks as follows:Finally, our $p$-value using the simulated null distribution of the KS-statistics is:This confirms our graphical conclusion that the sample is compatible with a Weibull distribution.As explained here, we can use bootstrapping to add pointwise confidence intervals to the estimated Weibull PDF or CDF:The gamlss package for R offers the ability to try many different distributions and select the ""best"" according to the GAIC (the generalized Akaike information criterion). The main function is fitDist. An important option in this function is the type of the distributions that are tried. For example, setting type = ""realline"" will try all implemented distributions defined on the whole real line whereas type = ""realsplus"" will only try distributions defined on the real positive line. Another important option is the parameter $k$, which is the penalty for the GAIC. In the example below, I set the parameter $k = 2$ which means that the ""best"" distribution is selected according to the classic AIC. You can set $k$ to anything you like, such as $\log(n)$ for the BIC.According to the AIC, the Weibull distribution (more specifically WEI2, a special parametrization of it) fits the data best. The exact parameterization of the distribution WEI2 is detailed in this document on page 279. Let's inspect the fit by looking at the residuals in a worm plot (basically a de-trended Q-Q-plot):We expect the residuals to be close to the middle horizontal line and 95% of them to lie between the upper and lower dotted curves, which act as 95% pointwise confidence intervals. In this case, the worm plot looks fine to me indicating that the Weibull distribution is an adequate fit."
What intuitive explanation is there for the central limit theorem?,"
In several different contexts we invoke the central limit theorem to justify whatever statistical method we want to adopt (e.g., approximate the binomial distribution by a normal distribution). I understand the technical details as to why the theorem is true but it just now occurred to me that I do not really understand the intuition behind the central limit theorem.
So, what is the intuition behind the central limit theorem? 
Layman explanations would be ideal. If some technical detail is needed please assume that I understand the concepts of a pdf, cdf, random variable etc but have no knowledge of convergence concepts, characteristic functions or anything to do with measure theory.
","['intuition', 'central-limit-theorem']",
What is a data scientist?,"
Having recently graduated from my PhD program in statistics, I had for the last couple of months began searching for work in the field of statistics. Almost every company I considered had a job posting with a job title of ""Data Scientist"". In fact, it felt like long gone were the days of seeing job titles of Statistical Scientist or Statistician. Had being a data scientist really replaced what being a statistician was or were the titles synonymous I wondered?
Well, most of the qualifications for the jobs felt like things that would qualify under the title of statistician.  Most jobs wanted a PhD in statistics ($\checkmark$), most required understanding experimental design ($\checkmark$), linear regression and anova ($\checkmark$), generalized linear models ($\checkmark$), and other multivariate methods such as PCA ($\checkmark$), as well as knowledge in a  statistical computing environment such as R or SAS ($\checkmark$). Sounds like a data scientist is really just a code name for statistician. 
However, every interview I went to started with the question: ""So are you familiar with machine learning algorithms?""  More often than not, I found myself having to try and answer questions about big data, high performance computing, and topics on  neural networks, CART, support vector machines, boosting trees, unsupervised models, etc. Sure I convinced myself that these were all statistical questions at heart, but at the end of every interview I couldn't help but leave feeling like I knew less and less about what a data scientist is. 
I am a statistician, but am I a data scientist? I work on scientific problems so I must be a scientist! And also I work with data, so I must be a data scientist! And according to Wikipedia, most academics would agree with me (https://en.wikipedia.org/wiki/Data_science, etc. )

Although use of the term ""data science"" has exploded in business
  environments, many academics and journalists see no distinction
  between data science and statistics.

But if I am going on all these job interviews for a data scientist position, why does it feel like they are never asking me statistical questions?  
Well after my last interview I did want any good scientist would do and I sought out data to solve this problem (hey, I am a data scientist after all). However, after many countless Google searches later, I ended up right where I started feeling as if I was once again grappling with the definition of what a data scientist was. I didn't know what a data scientist was exactly since there was so many definitions of it, (http://blog.udacity.com/2014/11/data-science-job-skills.html, http://www-01.ibm.com/software/data/infosphere/data-scientist/) but it seemed like everyone was telling me I wanted to be one: 

https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/
http://mashable.com/2014/12/25/data-scientist/#jjgsyhcERZqL
etc....the list goes on.

Well at the end of the day, what I figured out was ""what is a data scientist"" is a very hard question to answer. Heck, there were two entire months in Amstat where they devoted time to trying to answer this question: 

http://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/
http://magazine.amstat.org/blog/2015/11/01/statnews2015/

Well for now, I have to be a sexy statistician to be a data scientist but hopefully the cross validated community might be able to shed some light and help me understand what it means to be a data scientist.  Aren't all statisticians data scientists?

(Edit/Update)
I thought this might spice up the conversation. I just received an email from the American Statistical Association about a job positing with Microsoft looking for a Data Scientist. Here is the link: Data Scientist Position. I think this is interesting because the role of the position hits on a lot of specific traits we have been talking about, but I think lots of them require a very rigorous background in statistics, as well as contradicting many of the answers posted below.  In case the link goes dead, here are the qualities Microsoft seeks in a data scientist:

Core Job Requirements and Skills:
Business Domain Experience using Analytics

Must have experience across several relevant business domains in the utilization of critical thinking skills to conceptualize complex business problems and their solutions using advanced analytics in large scale real-world business data sets
The candidate must be able to independently run analytic projects and help our internal clients understand the findings and translate them into action to benefit their business.

Predictive Modeling

Experience across industries in predictive modeling
Business problem definition and conceptual modeling with the client to elicit important relationships and to define the system scope

Statistics/Econometrics

Exploratory data analytics for continuous and categorical data
Specification and estimation of structural model equations for enterprise and consumer behavior, production cost, factor demand, discrete choice, and other technology relationships as needed
Advanced statistical techniques to analyze continuous and categorical data
Time series analysis and implementation of forecasting models
Knowledge and experience in working with multiple variables problems
Ability to assess model correctness and conduct diagnostic tests
Capability to interpret statistics or economic models
Knowledge and experience in building discrete event simulation, and dynamic simulation models

Data Management

Familiarity with use of T-SQL and analytics for data transformation and the application of exploratory data analysis techniques for very large real-world data sets
Attention to data integrity including data redundancy, data accuracy, abnormal or extreme values, data interactions and missing values.

Communication and Collaboration Skills

Work independently and able to work with a virtual project team that will research innovative solutions to challenging business problems
Collaborate with partners, apply critical thinking skills, and drive analytic projects end-to-end
Superior communication skills, both verbal and written
Visualization of analytic results in a form that is consumable by a diverse set of stakeholders

Software Packages

Advanced Statistical/Econometric software packages: Python, R, JMP, SAS, Eviews, SAS Enterprise Miner
Data exploration, visualization, and management: T-SQL, Excel, PowerBI, and equivalent tools

Qualifications:

Minimum 5+   years of related experience required
Post graduate degree in quantitative field is desirable.


","['terminology', 'definition', 'careers']","There are a few humorous definitions which were not yet given:Data Scientist: Someone who does statistics on a Mac.I like this one, as it plays nicely on the more-hype-than-substance angle.Data Scientist: A Statistician who lives in San Francisco.Similarly, this riffs on the West Coast flavour of all this.Personally, I find the discussion (in general, and here) somewhat boring and repetitive.  When I was thinking about what I wanted to---maybe a quarter century or longer ago---I aimed for quantitative analyst. That is still what I do (and love!) and it mostly overlaps and covers what was given here in various answers.(Note: There is an older source for quote two but I can't find it right now.)"
Training on the full dataset after cross-validation?,"
TL:DR: Is it ever a good idea to train an ML model on all the data available before shipping it to production? Put another way, is it ever ok to train on all data available and not check if the model overfits, or get a final read of the expected performance of the model?

Say I have a family of models parametrized by $\alpha$. I can do a search (e.g. a grid search) on $\alpha$ by, for example, running k-fold cross-validation for each candidate. 
The point of using cross-validation for choosing $\alpha$ is that I can check if a learned model $\beta_i$ for that particular $\alpha_i$ had e.g. overfit, by testing it on the ""unseen data"" in each CV iteration (a validation set). After iterating through all $\alpha_i$'s, I could then choose a model $\beta_{\alpha^*}$ learned for the parameters $\alpha^*$ that seemed to do best on the grid search, e.g. on average across all folds.
Now, say that after model selection I would like to use all the the data that I have available in an attempt to ship the best possible model in production.  For this, I could use the parameters $\alpha^*$ that I chose via grid search with cross-validation, and then, after training the model on the full ($F$) dataset, I would a get a single new learned model $\beta^{F}_{\alpha^*}$ 
The problem is that, if I use my entire dataset for training,  I can't reliably check if this new learned model $\beta^{F}_{\alpha^*}$ overfits or how it may perform on unseen data. So is this at all good practice? What is a good way to think about this problem?
","['machine-learning', 'cross-validation', 'model-selection']","The way to think of cross-validation is as estimating the performance obtained using a method for building a model, rather than for estimating the performance of a model.If you use cross-validation to estimate the hyperparameters of a model (the $\alpha$s) and then use those hyper-parameters to fit a model to the whole dataset, then that is fine, provided that you recognise that the cross-validation estimate of performance is likely to be (possibly substantially) optimistically biased.  This is because part of the model (the hyper-parameters) have been selected to minimise the cross-validation performance, so if the cross-validation statistic has a non-zero variance (and it will) there is the possibility of over-fitting the model selection criterion.If you want to choose the hyper-parameters and estimate the performance of the resulting model then you need to perform a nested cross-validation, where the outer cross-validation is used to assess the performance of the model, and in each fold cross-validation is used to determine the hyper-parameters separately in each fold.  You build the final model by using cross-validation on the whole set to choose the hyper-parameters and then build the classifier on the whole dataset using the optimized hyper-parameters.This is of course computationally expensive, but worth it as the bias introduced by improper performance estimation can be large.  See my paper G. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010. (www, pdf) However, it is still possible to have over-fitting in model selection (nested cross-validation just allows you to test for it).  A method I have found useful is to add a regularisation term to the cross-validation error that penalises hyper-parameter values likely to result in overly-complex models, seeG. C. Cawley and N. L. C. Talbot, Preventing over-fitting in model selection via Bayesian regularisation of the hyper-parameters, Journal of Machine Learning Research, volume 8, pages 841-861, April 2007. (www,pdf) So the answers to your question are (i) yes, you should use the full dataset to produce your final model as the more data you use the more likely it is to generalise well but (ii) make sure you obtain an unbiased performance estimate via nested cross-validation and potentially consider penalising the cross-validation statistic to further avoid over-fitting in model selection."
Why do we need sigma-algebras to define probability spaces?,"
We have a random experiment with different outcomes forming the sample space $\Omega,$ on which we look with interest at certain patterns, called events $\mathscr{F}.$ Sigma-algebras (or sigma-fields) are made up of events to which a probability measure $\mathbb{P}$ can be assigned. Certain properties are fulfilled, including the inclusion of the null set $\varnothing$ and the entire sample space, and an algebra that describes unions and intersections with Venn diagrams.
Probability is defined as a function between the $\sigma$-algebra and the interval $[0,1]$.
 Altogether, the triple $(\Omega, \mathscr{F}, \mathbb{P})$ forms a probability space.
Could someone explain in plain English why the probability edifice would collapse if we didn't have a $\sigma$-algebra? They are just wedged in the middle with that impossibly calligraphic ""F"". I trust they are necessary; I see that an event is different from an outcome, but what would go awry without a $\sigma$-algebras?

The question is: In what type of probability problems the definition of a probability space including a $\sigma$-algebra becomes a necessity? 


This online document on the Dartmouth University website provides a plain English accessible explanation. The idea is a spinning pointer rotating counterclockwise on a circle of unit perimeter:


We begin by constructing a spinner, which consists of a circle of unit
  circumference and a pointer as shown in [the] Figure. We pick a point
  on the circle and label it $0$, and then label every other point on the
  circle with the distance, say $x$, from $0$ to that point, measured
  counterclockwise. The  experiment  consists  of spinning the pointer and 
  recording the label of the point at the tip of the pointer. We let the random
  variable $X$ denote the value of this outcome.  The sample space is
  clearly the interval $[0,1)$.  We would like to construct a
  probability model in which each outcome is equally likely to occur. If
  we proceed as we did [...] for experiments with a finite number of
  possible outcomes,  then we must assign the probability $0$ to each
  outcome,  since otherwise,  the  sum  of  the  probabilities,  over 
  all  of  the  possible  outcomes,  would not equal 1.  (In fact,
  summing an uncountable number of real numbers is a tricky business; 
  in  particular,  in  order  for  such  a  sum  to  have  any  meaning,
  at  most countably many of the summands can be different than $0$.) 
  However, if all of the assigned probabilities are $0$, then the sum is
  $0$, not $1$, as it should be.

So if we assigned to each point any probability, and given that there is an (uncountably) infinity number of points, their sum would add up to $> 1$. 
","['probability', 'intuition', 'measure-theory', 'sigma-algebra']","To Xi'an's first point: When you're talking about $\sigma$-algebras, you're asking about measurable sets, so unfortunately any answer must focus on measure theory. I'll try to build up to that gently, though.Consider this example. Suppose you have a unit square in $\mathbb{R}^2$, and you're interested in the probability of randomly selecting a point that is a member of a specific set in the unit square. In lots of circumstances, this can be readily answered based on a comparison of areas of the different sets. For example, we can draw some circles, measure their areas, and then take the probability as the fraction of the square falling in the circle. Very simple.But what if the area of the set of interest is not well-defined?If the area is not well-defined, then we can reason to two different but completely valid (in some sense) conclusions about what the area is. So we could have $P(A)=1$ on the one hand and $P(A)=0$ on the other hand, which implies $0=1$. This breaks all of math beyond repair. You can now prove $5<0$ and a number of other preposterous things. Clearly this isn't too useful.What is a $\sigma$-algebra, precisely? It's actually not that frightening. It's just a definition of which sets may be considered as events. Elements not in $\mathscr{F}$ simply have no defined probability measure. Basically, $\sigma$-algebras are the ""patch"" that lets us avoid some pathological behaviors of mathematics, namely non-measurable sets.The three requirements of a $\sigma$-field can be considered as consequences of what we would like to do with probability:
A $\sigma$-field is a set that has three properties:The countable unions and countable intersections components are direct consequences of the non-measurable set issue. Closure under complements is a consequence of the Kolmogorov axioms: if $P(A)=2/3$, $P(A^c)$ ought to be $1/3$. But without (3), it could happen that $P(A^c)$ is undefined. That would be strange. Closure under complements and the Kolmogorov axioms let us to say things like $P(A\cup A^c)=P(A)+1-P(A)=1$.Finally, We are considering events in relation to $\Omega$, so we further require that $\Omega\in\mathscr{F}$But! There's good news here, also. Or, at least, a way to skirt the issue. We only need $\sigma$-algebras if we're working in a set with uncountable cardinality. If we restrict ourselves to countable sets, then we can take $\mathscr{F}=2^\Omega$ the power set of $\Omega$ and we won't have any of these problems because for countable $\Omega$, $2^\Omega$ consists only of measurable sets. (This is alluded to in Xi'an's second comment.) You'll notice that some textbooks will actually commit a subtle sleight-of-hand here, and only consider countable sets when discussing probability spaces.Additionally, in geometric problems in $\mathbb{R}^n$, it's perfectly sufficient to only consider $\sigma$-algebras composed of sets for which the $\mathcal{L}^n$ measure is defined. To ground this somewhat more firmly, $\mathcal{L}^n$ for $n=1,2,3$ corresponds to the usual notions of length, area and volume. So what I'm saying in the previous example is that the set needs to have a well-defined area for it to have a geometric probability assigned to it. And the reason is this: if we admit non-measureable sets, then we can end up in situations where we can assign probability 1 to some event based on some proof, and probability 0 to the same event event based on some other proof.But don't let the connection to uncountable sets confuse you! A common misconception that $\sigma$-algebras are countable sets. In fact, they may be countable or uncountable. Consider this illustration: as before, we have a unit square. Define $$\mathscr{F}=\text{All subsets of the unit square with defined $\mathcal{L}^2$ measure}.$$ You can draw a square $B$ with side length $s$ for all $s \in (0,1)$, and with one corner at $(0,0)$. It should be clear that this square is a subset of the unit square. Moreover, all of these squares have defined area, so these squares are elements of $\mathscr{F}$. But it should also be clear that there are uncountably many squares $B$: the number of such squares is uncountable, and each square has defined Lebesgue measure.So as a practical matter, simply making that observation is often enough to make the observation that you only consider Lebesgue-measurable sets to gain headway against the problem of interest.I'm afraid I can only shed a little bit of light on this myself. But the Banach-Tarski paradox (sometimes the ""sun and pea"" paradox) can help us some:Given a solid ball in 3‑dimensional space, there exists a decomposition of the ball into a finite number of disjoint subsets, which can then be put back together in a different way to yield two identical copies of the original ball. Indeed, the reassembly process involves only moving the pieces around and rotating them, without changing their shape. However, the pieces themselves are not ""solids"" in the usual sense, but infinite scatterings of points. The reconstruction can work with as few as five pieces.A stronger form of the theorem implies that given any two ""reasonable"" solid objects (such as a small ball and a huge ball), either one can be reassembled into the other. This is often stated informally as ""a pea can be chopped up and reassembled into the Sun"" and called the ""pea and the Sun paradox"".1So if you're working with probabilities in $\mathbb{R}^3$ and you're using the geometric probability measure (the ratio of volumes), you want to work out the probability of some event. But you'll struggle to define that probability precisely, because you can rearrange the sets of your space to change volumes!
If probability depends on volume, and you can change the volume of the set to be the size of the sun or the size of a pea, then the probability will also change. So no event will have a single probability ascribed to it. Even worse, you can rearrange $S\in\Omega$ such that the volume of $S$ has $V(S)>V(\Omega)$, which implies that the geometric probability measure reports a probability $P(S)>1$, in flagrant violation of the Kolmogorov axioms which require that probability has measure 1.To resolve this paradox, one could make one of four concessions:Option (1) doesn't help use define probabilities, so it's out. Option (2) violates the second Kolmogorov axiom, so it's out. Option (3) seems like a terrible idea because ZFC fixes so many more problems than it creates. But option (4) seems attractive: if we develop a theory of what is and is not measurable, then we will have well-defined probabilities in this problem! This brings us back to measure theory, and our friend the $\sigma$-algebra."
Why the sudden fascination with tensors?,"
I've noticed lately that a lot of people are developing tensor equivalents of many methods (tensor factorization, tensor kernels, tensors for topic modeling, etc) I'm wondering, why is the world suddenly fascinated with tensors? Are there recent papers/ standard results that are particularly surprising, that brought about this? Is it computationally a lot cheaper than previously suspected?
I'm not being glib, I sincerely am interested, and if there are any pointers to papers about this, I'd love to read them.
","['machine-learning', 'references', 'matrix', 'linear-algebra', 'tensor']",
What is the difference between a consistent estimator and an unbiased estimator?,"
What is the difference between a consistent estimator and an unbiased estimator?
The precise technical definitions of these terms are fairly complicated, and it's difficult to get an intuitive feel for what they mean. I can imagine a good estimator, and a bad estimator, but I'm having trouble seeing how any estimator could satisfy one condition and not the other.
","['unbiased-estimator', 'estimators', 'consistency']","To define the two terms without using too much technical language:An estimator is consistent if, as the sample size increases, the estimates (produced by the estimator) ""converge"" to the true value of the parameter being estimated. To be slightly more precise - consistency means that, as the sample size increases, the sampling distribution of the estimator becomes increasingly concentrated at the true parameter value.An estimator is unbiased if, on average, it hits the true parameter value. That is, the mean of the sampling distribution of the estimator is equal to the true parameter value.The two are not equivalent: Unbiasedness is a statement about the expected value of the sampling distribution of the estimator. Consistency is a statement about ""where the sampling distribution of the estimator is going"" as the sample size increases.It certainly is possible for one condition to be satisfied but not the other - I will give two examples. For both examples consider a sample $X_1, ..., X_n$ from a $N(\mu, \sigma^2)$ population.Unbiased but not consistent: Suppose you're estimating $\mu$. Then $X_1$ is an unbiased estimator of $\mu$ since $E(X_1) = \mu$. But, $X_1$ is not consistent since its distribution does not become more concentrated around $\mu$ as the sample size increases - it's always $N(\mu, \sigma^2)$!Consistent but not unbiased: Suppose you're estimating $\sigma^2$. The maximum likelihood estimator is $$ \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2 $$ where $\overline{X}$ is the sample mean. It is a fact that $$ E(\hat{\sigma}^2) = \frac{n-1}{n} \sigma^2 $$ which can be derived using the information here. Therefore $\hat{\sigma}^2$ is biased for any finite sample size. We can also easily derive that $${\rm var}(\hat{\sigma}^2) = \frac{ 2\sigma^4(n-1)}{n^2}$$ From these facts we can informally see that the distribution of $\hat{\sigma}^2$ is becoming more and more concentrated at $\sigma^2$ as the sample size increases since the mean is converging to $\sigma^2$ and the variance is converging to $0$. (Note: This does constitute a proof of consistency, using the same argument as the one used in the answer here)"
What is the influence of C in SVMs with linear kernel?,"
I am currently using an SVM with a linear kernel to classify my data.  There is
no error on the training set.  I tried several values for the parameter $C$
($10^{-5}, \dots, 10^2$).  This did not change the error on the test set.
Now I
wonder: is this an error caused by the ruby bindings for libsvm I am using
(rb-libsvm) or is this theoretically explainable?
Should the parameter $C$ always change the performance of the classifier?
","['machine-learning', 'svm', 'libsvm']","The C hyperparameter tells the SVM optimization how much you want to avoid misclassifying each training example.  For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly.  Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.  For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable."
How does Keras 'Embedding' layer work?,"
Need to understand the working of 'Embedding' layer in Keras library. I execute the following code in Python 
import numpy as np
from keras.models import Sequential
from keras.layers import Embedding

model = Sequential()
model.add(Embedding(5, 2, input_length=5))

input_array = np.random.randint(5, size=(1, 5))

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)

which gives the following output
input_array = [[4 1 3 3 3]]
output_array = 
[[[ 0.03126476  0.00527241]
  [-0.02369716 -0.02856163]
  [ 0.0055749   0.01492429]
  [ 0.0055749   0.01492429]
  [ 0.0055749   0.01492429]]]

I understand that each value in the input_array is mapped to 2 element vector in the output_array, so a 1 X 4 vector gives 1 X 4 X 2 vectors. But how are the mapped values computed?
","['text-mining', 'word-embeddings', 'keras']",
How to summarize data by group in R? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question appears to be off-topic because EITHER it is not about statistics, machine learning, data analysis, data mining, or data visualization, OR it focuses on programming, debugging, or performing routine operations within a statistical computing platform. If the latter, you could try the support links we maintain.


Closed 7 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I have R data frame like this:
        age group
1   23.0883     1
2   25.8344     1
3   29.4648     1
4   32.7858     2
5   33.6372     1
6   34.9350     1
7   35.2115     2
8   35.2115     2
9   35.2115     2
10  36.7803     1
...

I need to get data frame in the following form:
group mean     sd
1     34.5     5.6
2     32.3     4.2
...

Group number may vary, but their names and quantity could be obtained by calling levels(factor(data$group))
What manipulations should be done with the data to get the result?
","['r', 'data-transformation']","Here is the plyr one line variant using ddply:Here is another one line variant using new package data.table. This one is faster, though this is noticeable only on table with 100k rows. Timings on my Macbook Pro with 2.53 Ghz Core 2 Duo processor and R 2.11.1:Further savings are possible if we use setkey:"
How do I get the number of rows of a data.frame in R? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 Questions about programming are off-topic here unless they involve statistical analysis in some fashion, but they can be asked on Stack Overflow.


Closed 9 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






After reading a dataset:
dataset <- read.csv(""forR.csv"")


How can I get R to give me the number of cases it contains?
Also, will the returned value include of exclude cases omitted with na.omit(dataset)?

",['r'],"dataset will be a data frame. As I don't have forR.csv, I'll make up a small data frame for illustration:To get the number of cases, count the number of rows using nrow() or NROW():To count the data after omitting the NA, use the same tools, but wrap dataset in na.omit():The difference between NROW() and NCOL() and their lowercase variants (ncol() and nrow()) is that the lowercase versions will only work for objects that have dimensions (arrays, matrices, data frames). The uppercase versions will work with vectors, which are treated as if they were a 1 column matrix, and are robust if you end up subsetting your data such that R drops an empty dimension.Alternatively, use complete.cases() and sum it (complete.cases() returns a logical vector [TRUE or FALSE] indicating if any observations are NA for any rows."
Statistics Jokes,"
Well, we've got favourite statistics quotes. What about statistics jokes?
","['references', 'humor']",
Can a probability distribution value exceeding 1 be OK?,"
On the Wikipedia page about naive Bayes classifiers, there is this line: 

$p(\mathrm{height}|\mathrm{male}) = 1.5789$ (A probability distribution over 1 is OK. It is the area under the bell curve that is equal to 1.) 

How can a value $>1$ be OK? I thought all probability values were expressed in the range $0 \leq p \leq 1$. Furthermore, given that it is possible to have such a value, how is that value obtained in the example shown on the page? 
","['probability', 'distributions', 'normal-distribution', 'density-function', 'faq']","That Wiki page is abusing language by referring to this number as a probability.  You are correct that it is not.  It is actually a probability per foot.  Specifically, the value of 1.5789 (for a height of 6 feet) implies that the probability of a height between, say, 5.99 and 6.01 feet is close to the following unitless value:$$1.5789\, [1/\text{foot}] \times (6.01 - 5.99)\, [\text{feet}] = 0.0316$$  This value must not exceed 1, as you know.  (The small range of heights (0.02 in this example) is a crucial part of the probability apparatus.  It is the ""differential"" of height, which I will abbreviate $d(\text{height})$.)  Probabilities per unit of something are called densities by analogy to other densities, like mass per unit volume.Bona fide probability densities can have arbitrarily large values, even infinite ones.This example shows the probability density function for a Gamma distribution (with shape parameter of $3/2$ and scale of $1/5$). Because most of the density is less than $1$, the curve has to rise higher than $1$ in order to have a total area of $1$ as required for all probability distributions.This density (for a beta distribution with parameters $1/2, 1/10$) becomes infinite at $0$ and at $1$.  The total area still is finite (and equals $1$)!The value of 1.5789 /foot is obtained in that example by estimating that the heights of males have a normal distribution with mean 5.855 feet and variance 3.50e-2 square feet.  (This can be found in a previous table.) The square root of that variance is the standard deviation, 0.18717 feet.  We re-express 6 feet as the number of SDs from the mean:$$z = (6 - 5.855) / 0.18717 = 0.7747$$The division by the standard deviation produces a relation$$dz = d(\text{height})/0.18717$$The Normal probability density, by definition, equals$$\frac{1}{\sqrt{2 \pi}}\exp(-z^2/2)dz = 0.29544\ d(\text{height}) / 0.18717 = 1.5789\  d(\text{height}).$$(Actually, I cheated: I simply asked Excel to compute NORMDIST(6, 5.855, 0.18717, FALSE).  But then I really did check it against the formula, just to be sure.)  When we strip the essential differential $d(\text{height})$ from the formula only the number $1.5789$ remains, like the Cheshire Cat's smile.  We, the readers, need to understand that the number has to be multiplied by a small difference in heights in order to produce a probability."
"A list of cost functions used in neural networks, alongside applications","
What are common cost functions used in evaluating the performance of neural networks?
Details
(feel free to skip the rest of this question, my intent here is simply to provide clarification on notation that answers may use to help them be more understandable to the general reader)
I think it would be useful to have a list of common cost functions, alongside a few ways that they have been used in practice. So if others are interested in this I think a community wiki is probably the best approach, or we can take it down if it's off topic.
Notation
So to start, I'd like to define a notation that we all use when describing these, so the answers fit well with each other.
This notation is from Neilsen's book.
A Feedforward Neural Network is a many layers of neurons connected together. Then it takes in an input, that input ""trickles"" through the network and then the neural network returns an output vector.
More formally, call $a^i_j$ the activation (aka output) of the $j^{th}$ neuron in the $i^{th}$ layer, where $a^1_j$ is the $j^{th}$ element in the input vector.
Then we can relate the next layer's input to it's previous via the following relation:
$a^i_j = \sigma(\sum\limits_k (w^i_{jk} \cdot a^{i-1}_k) + b^i_j)$
where
$\sigma$ is the activation function,
$w^i_{jk}$ is the weight from the $k^{th}$ neuron in the $(i-1)^{th}$ layer to the $j^{th}$ neuron in the $i^{th}$ layer,
$b^i_j$ is the bias of the $j^{th}$ neuron in the $i^{th}$ layer, and
$a^i_j$ represents the activation value of the $j^{th}$ neuron in the $i^{th}$ layer.
Sometimes we write $z^i_j$ to represent $\sum\limits_k (w^i_{jk} \cdot a^{i-1}_k) + b^i_j$, in other words, the activation value of a neuron before applying the activation function.

For more concise notation we can write
$a^i = \sigma(w^i \times a^{i-1} + b^i)$
To use this formula to compute the output of a feedforward network for some input $I \in \mathbb{R}^n$, set $a^1 = I$, then compute $a^2$, $a^3$, ...,$a^m$, where m is the number of layers.
Introduction
A cost function is a measure of ""how good"" a neural network did with respect to it's given training sample and the expected output. It also may depend on variables such as weights and biases.
A cost function is a single value, not a vector, because it rates how good the neural network did as a whole.
Specifically, a cost function is of the form
$$C(W, B, S^r, E^r)$$
where $W$ is our neural network's weights, $B$ is our neural network's biases, $S^r$ is the input of a single training sample, and $E^r$ is the desired output of that training sample. Note this function can also potentially be dependent on $y^i_j$ and $z^i_j$ for any neuron $j$ in layer $i$, because those values are dependent on $W$, $B$, and $S^r$.
In backpropagation, the cost function is used to compute the error of our output layer, $\delta^L$, via
$$\delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma^{ \prime}(z^i_j)$$.
Which can also be written as a vector via
$$\delta^L = \nabla_a C \odot \sigma^{ \prime}(z^i)$$.
We will provide the gradient of the cost functions in terms of the second equation, but if one wants to prove these results themselves, using the first equation is recommended because it's easier to work with.
Cost function requirements
To be used in backpropagation, a cost function must satisfy two properties:
1: The cost function $C$ must be able to be written as an average
$$C=\frac{1}{n} \sum\limits_x C_x$$
over cost functions $C_x$ for individual training examples, $x$.
This is so it allows us to compute the gradient (with respect to weights and biases) for a single training example, and run Gradient Descent.
2: The cost function $C$ must not be dependent on any activation values of a neural network besides the output values $a^L$.
Technically a cost function can be dependent on any $a^i_j$ or $z^i_j$. We just make this restriction so we can backpropagte, because the equation for finding the gradient of the last layer is the only one that is dependent on the cost function (the rest are dependent on the next layer). If the cost function is dependent on other activation layers besides the output one, backpropagation will be invalid because the idea of ""trickling backwards"" no longer works.
Also, activation functions are required to have an output $0\leq a^L_j \leq 1$ for all $j$. Thus these cost functions need to only be defined within that range (for example, $\sqrt{a^L_j}$ is valid since we are guaranteed $a^L_j \geq 0$).
","['machine-learning', 'neural-networks']",
How to intuitively explain what a kernel is?,"
Many machine learning classifiers (e.g. support vector machines) allow one to specify a kernel. What would be an intuitive way of explaining what a kernel is?
One aspect I have been thinking of is the distinction between linear and non-linear kernels. In simple terms, I could speak of 'linear decision functions' an 'non-linear decision functions'. However, I am not sure if calling a kernel a 'decision function' is a good idea.
Suggestions?
","['machine-learning', 'svm', 'references', 'kernel-trick', 'intuition']",
Choice of K in K-fold cross-validation,"
I've been using the $K$-fold cross-validation a few times now to evaluate performance of some learning algorithms, but I've always been puzzled as to how I should choose the value of $K$.
I've often seen and used a value of $K = 10$, but this seems totally arbitrary to me, and I now just use $10$ by habit instead of thinking it over. To me it seems that you're getting a better granularity as you improve the value of $K$, so ideally you should make your $K$ very large, but there is also a risk to be biased.
I'd like to know what the value of $K$ should depend on, and how I should be thinking about this when I evaluate my algorithm. Does it change something if I use the stratified version of the cross-validation or not?
","['machine-learning', 'classification', 'cross-validation']","The choice of $k = 10$ is somewhat arbitrary. Here's how I decide $k$:first of all, in order to lower the variance of the CV result, you can and should repeat/iterate the CV with new random splits.  This makes the argument of high $k$ => more computation time largely irrelevant, as you anyways want to calculate many models. I tend to think mainly of the total number of models calculated (in analogy to bootstrapping). So I may decide for 100 x 10-fold CV or 200 x 5-fold CV.@ogrisel already explained that usually large $k$ mean less (pessimistic) bias. (Some exceptions are known particularly for $k = n$, i.e. leave-one-out).If possible, I use a $k$ that is a divisor of the sample size, or the size of the groups in the sample that should be stratified.Too large $k$ mean that only a low number of sample combinations is possible, thus limiting the number of iterations that are different. These thoughts have more weight with small sample sizes. With more samples available $k$ doesn't matter very much. The possible number of combinations soon becomes large enough so the (say) 100 iterations of 10-fold CV do not run a great risk of being duplicates. Also, more training samples usually means that you are at a flatter part of the learning curve, so the difference between the surrogate models and the ""real"" model trained on all $n$ samples becomes negligible. "
When is it ok to remove the intercept in a linear regression model?,"
I am running linear regression models and wondering what the conditions are for removing the intercept term. 
In comparing results from two different regressions where one has the intercept and the other does not, I notice that the $R^2$ of the function without the intercept is much higher. Are there certain conditions or assumptions I should be following to make sure the removal of the intercept term is valid? 
","['regression', 'linear-model', 'r-squared', 'intercept', 'faq']","The shortest answer: never, unless you are sure that your linear approximation of the data generating process (linear regression model) either by some theoretical or any other reasons is forced to go through the origin. If not the other regression parameters will be biased even if intercept is statistically insignificant (strange but it is so, consult Brooks Introductory Econometrics for instance). Finally, as I do often explain to my students, by leaving the intercept term you insure that the residual term is zero-mean.For your two models case we need more context. It may happen that linear model is not suitable here. For example, you need to log transform first if the model is multiplicative. Having exponentially growing processes it may occasionally happen that $R^2$ for the model without the intercept is ""much"" higher. Screen the data, test the model with RESET test or any other linear specification test, this may help to see if my guess is true. And, building the models highest $R^2$ is one of the last statistical properties I do really concern about, but it is nice to present to the people who are not so well familiar with econometrics (there are many dirty tricks to make determination close to 1 :))."
Bottom to top explanation of the Mahalanobis distance?,"
I'm studying pattern recognition and statistics and almost every book I open on the subject I bump into the concept of Mahalanobis distance. The books give sort of intuitive explanations, but still not good enough ones for me to actually really understand what is going on. If someone would ask me ""What is the Mahalanobis distance?"" I could only answer: ""It's this nice thing, which measures distance of some kind"" :) 
The definitions usually also contain eigenvectors and eigenvalues, which I have a little trouble connecting to the Mahalanobis distance. I understand the definition of eigenvectors and eigenvalues, but how are they related to the Mahalanobis distance? Does it have something to do with changing the base in Linear Algebra etc.?
I have also read these former questions on the subject:

What is Mahalanobis distance, & how is it used in pattern recognition?
Intuitive explanations for Gaussian distribution function and mahalanobis distance (Math.SE)

I have also read this explanation.
The answers are good and pictures nice, but still I don't really get it...I have an idea but it's still in the dark. Can someone give a ""How would you explain it to your grandma""-explanation so that I could finally wrap this up and never again wonder what the heck is a Mahalanobis distance? :) Where does it come from, what, why? 
UPDATE: 
Here is something which helps understanding the Mahalanobis formula: 
https://math.stackexchange.com/questions/428064/distance-of-a-test-point-from-the-center-of-an-ellipsoid
","['normal-distribution', 'mathematical-statistics', 'distance', 'pattern-recognition', 'intuition']","Here is a scatterplot of some multivariate data (in two dimensions):What can we make of it when the axes are left out?Introduce coordinates that are suggested by the data themselves.The origin will be at the centroid of the points (the point of their averages).  The first coordinate axis (blue in the next figure) will extend along the ""spine"" of the points, which (by definition) is any direction in which the variance is the greatest.  The second coordinate axis (red in the figure) will extend perpendicularly to the first one.  (In more than two dimensions, it will be chosen in that perpendicular direction in which the variance is as large as possible, and so on.)We need a scale.  The standard deviation along each axis will do nicely to establish the units along the axes.  Remember the 68-95-99.7 rule: about two-thirds (68%) of the points should be within one unit of the origin (along the axis); about 95% should be within two units.  That makes it easy to eyeball the correct units.  For reference, this figure includes the unit circle in these units:That doesn't really look like a circle, does it?  That's because this picture is distorted (as evidenced by the different spacings among the numbers on the two axes).  Let's redraw it with the axes in their proper orientations--left to right and bottom to top--and with a unit aspect ratio so that one unit horizontally really does equal one unit vertically:You measure the Mahalanobis distance in this picture rather than in the original.What happened here?  We let the data tell us how to construct a coordinate system for making measurements in the scatterplot.  That's all it is.  Although we had a few choices to make along the way (we could always reverse either or both axes; and in rare situations the directions along the ""spines""--the principal directions--are not unique), they do not change the distances in the final plot.(Not for grandma, who probably started to lose interest as soon as numbers reappeared on the plots, but to address the remaining questions that were posed.)Unit vectors along the new axes are the eigenvectors (of either the covariance matrix or its inverse).We noted that undistorting the ellipse to make a circle divides the distance along each eigenvector by the standard deviation: the square root of the covariance.  Letting $C$ stand for the covariance function, the new (Mahalanobis) distance between two points $x$ and $y$ is the distance from $x$ to $y$ divided by the square root of $C(x-y, x-y)$.  The corresponding algebraic operations, thinking now of $C$ in terms of its representation as a matrix and $x$ and $y$ in terms of their representations as vectors, are written $\sqrt{(x-y)'C^{-1}(x-y)}$.  This works regardless of what basis is used to represent vectors and matrices.  In particular, this is the correct formula for the Mahalanobis distance in the original coordinates.The amounts by which the axes are expanded in the last step are the (square roots of the) eigenvalues of the inverse covariance matrix.  Equivalently, the axes are shrunk by the (roots of the) eigenvalues of the covariance matrix.  Thus, the more the scatter, the more the shrinking needed to convert that ellipse into a circle.Although this procedure always works with any dataset, it looks this nice (the classical football-shaped cloud) for data that are approximately multivariate Normal.  In other cases, the point of averages might not be a good representation of the center of the data or the ""spines"" (general trends in the data) will not be identified accurately using variance as a measure of spread.The shifting of the coordinate origin, rotation, and expansion of the axes collectively form an affine transformation.  Apart from that initial shift, this is a change of basis from the original one (using unit vectors pointing in the positive coordinate directions) to the new one (using a choice of unit eigenvectors).There is a strong connection with Principal Components Analysis (PCA).  That alone goes a long way towards explaining the ""where does it come from"" and ""why"" questions--if you weren't already convinced by the elegance and utility of letting the data determine the coordinates you use to describe them and measure their differences.For multivariate Normal distributions (where we can carry out the same construction using properties of the probability density instead of the analogous properties of the point cloud), the Mahalanobis distance (to the new origin) appears in place of the ""$x$"" in the expression $\exp(-\frac{1}{2} x^2)$ that characterizes the probability density of the standard Normal distribution.  Thus, in the new coordinates, a multivariate Normal distribution looks standard Normal when projected onto any line through the origin.  In particular, it is standard Normal in each of the new coordinates.  From this point of view, the only substantial sense in which multivariate Normal distributions differ among one another is in terms of how many dimensions they use.  (Note that this number of dimensions may be, and sometimes is, less than the nominal number of dimensions.)"
Cohen's kappa in plain English,"
I am reading a data mining book and it mentioned the Kappa statistic as a means for evaluating the prediction performance of classifiers. However, I just can't understand this. I also checked Wikipedia but it didn't help too: https://en.wikipedia.org/wiki/Cohen's_kappa. 
How does Cohen's kappa help in evaluating the prediction performance of classifiers? What does it tell?
I understand that 100% kappa means that the classifier is in total agreement with a random classifier, but I don't understand how does this help in evaluating the performance of the classifier?
What does 40% kappa mean? Does it mean that 40% of the time, the classifier is in agreement with the random classifier? If so, what does that tell me or help me in evaluating the classifier?
","['classification', 'data-mining', 'cohens-kappa']",
Are large data sets inappropriate for hypothesis testing?,"
In a recent article of Amstat News, the authors (Mark van der Laan and Sherri Rose) stated that ""We know that for large enough sample sizes, every study—including ones in which the null hypothesis of no effect is true — will declare a statistically significant effect."".
Well, I for one didn't know that. Is this true? Does it mean that hypothesis testing is worthless for large data sets?
","['hypothesis-testing', 'statistical-significance', 'dataset', 'sample-size', 'large-data']","It is not true.  If the null hypothesis is true then it will not be rejected more frequently at large sample sizes than small. There is an erroneous rejection rate that's usually set to 0.05 (alpha) but it is independent of sample size. Therefore, taken literally the statement is false. Nevertheless, it's possible that in some situations (even whole fields) all nulls are false and therefore all will be rejected if N is high enough. But is this a bad thing?What is true is that trivially small effects can be found to be ""significant"" with very large sample sizes.  That does not suggest that you shouldn't have such large samples sizes.  What it means is that the way you interpret your finding is dependent upon the effect size and sensitivity of the test.  If you have a very small effect size and highly sensitive test you have to recognize that the statistically significant finding may not be meaningful or useful.Given some people don't believe that a test of the null hypothesis, when the null is true, always has an error rate equal to the cutoff point selected for any sample size, here's a simple simulation in R proving the point. Make N as large as you like and the rate of Type I errors will remain constant."
Deriving the conditional distributions of a multivariate normal distribution,"
We have a multivariate normal vector ${\boldsymbol Y} \sim \mathcal{N}(\boldsymbol\mu, \Sigma)$. Consider partitioning $\boldsymbol\mu$ and ${\boldsymbol Y}$ into
$$\boldsymbol\mu
=
\begin{bmatrix}
 \boldsymbol\mu_1 \\
 \boldsymbol\mu_2
\end{bmatrix}
$$
$${\boldsymbol Y}=\begin{bmatrix}{\boldsymbol y}_1 \\ 
{\boldsymbol y}_2 \end{bmatrix}$$
with a similar partition of $\Sigma$ into
$$ 
\begin{bmatrix}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{bmatrix}
$$
Then, $({\boldsymbol y}_1|{\boldsymbol y}_2={\boldsymbol a})$, the conditional distribution of the first partition given the second, is
$\mathcal{N}(\overline{\boldsymbol\mu},\overline{\Sigma})$, with mean
$$
\overline{\boldsymbol\mu}=\boldsymbol\mu_1+\Sigma_{12}{\Sigma_{22}}^{-1}({\boldsymbol a}-\boldsymbol\mu_2)
$$
and covariance matrix
$$
\overline{\Sigma}=\Sigma_{11}-\Sigma_{12}{\Sigma_{22}}^{-1}\Sigma_{21}$$
Actually these results are provided in Wikipedia too, but I have no idea how the $\overline{\boldsymbol\mu}$ and $\overline{\Sigma}$ is derived. These results are crucial, since they are important statistical formula for deriving Kalman filters. Would anyone provide me a derivation steps of deriving $\overline{\boldsymbol\mu}$ and $\overline{\Sigma}$ ?
","['normal-distribution', 'conditional-probability', 'multivariate-normal-distribution', 'faq']","You can prove it by explicitly calculating the conditional density by brute force, as in Procrastinator's link (+1) in the comments. But, there's also a theorem that says all conditional distributions of a multivariate normal distribution are normal. Therefore, all that's left is to calculate the mean vector and covariance matrix. I remember we derived this in a time series class in college by cleverly defining a third variable and using its properties to derive the result more simply than the brute force solution in the link (as long as you're comfortable with matrix algebra). I'm going from memory but it was something like this:It is worth pointing out that the proof below only assumes that $\Sigma_{22}$ is nonsingular, $\Sigma_{11}$ and $\Sigma$ may well be singular.Let ${\bf x}_{1}$ be the first partition and ${\bf x}_2$ the second. Now define ${\bf z} = {\bf x}_1 + {\bf A} {\bf x}_2 $ where ${\bf A} = -\Sigma_{12} \Sigma^{-1}_{22}$. Now we can write\begin{align*} {\rm cov}({\bf z}, {\bf x}_2) &= {\rm cov}( {\bf x}_{1}, {\bf x}_2 ) + 
{\rm cov}({\bf A}{\bf x}_2, {\bf x}_2) \\
&= \Sigma_{12} + {\bf A} {\rm var}({\bf x}_2) \\
&= \Sigma_{12} - \Sigma_{12} \Sigma^{-1}_{22} \Sigma_{22} \\
&= 0
\end{align*}Therefore ${\bf z}$ and ${\bf x}_2$ are uncorrelated and, since they are jointly normal, they are independent. Now, clearly $E({\bf z}) = {\boldsymbol \mu}_1 + {\bf A}  {\boldsymbol \mu}_2$, therefore it follows that\begin{align*}
E({\bf x}_1 | {\bf x}_2) &= E( {\bf z} - {\bf A} {\bf x}_2 | {\bf x}_2) \\
& = E({\bf z}|{\bf x}_2) -  E({\bf A}{\bf x}_2|{\bf x}_2) \\
& = E({\bf z}) - {\bf A}{\bf x}_2 \\
& = {\boldsymbol \mu}_1 + {\bf A}  ({\boldsymbol \mu}_2 - {\bf x}_2) \\
& = {\boldsymbol \mu}_1 + \Sigma_{12} \Sigma^{-1}_{22} ({\bf x}_2- {\boldsymbol \mu}_2)
\end{align*}which proves the first part. For the covariance matrix, note that\begin{align*}
{\rm var}({\bf x}_1|{\bf x}_2) &= {\rm var}({\bf z} - {\bf A} {\bf x}_2 | {\bf x}_2) \\
&= {\rm var}({\bf z}|{\bf x}_2) + {\rm var}({\bf A} {\bf x}_2 | {\bf x}_2) - {\bf A}{\rm cov}({\bf z}, -{\bf x}_2) - {\rm cov}({\bf z}, -{\bf x}_2) {\bf A}' \\
&= {\rm var}({\bf z}|{\bf x}_2) \\
&= {\rm var}({\bf z})
\end{align*}Now we're almost done:\begin{align*}
{\rm var}({\bf x}_1|{\bf x}_2) = {\rm var}( {\bf z} ) &= {\rm var}( {\bf x}_1 + {\bf A} {\bf x}_2 ) \\
&= {\rm var}( {\bf x}_1 ) + {\bf A} {\rm var}( {\bf x}_2 ) {\bf A}'
+ {\bf A} {\rm cov}({\bf x}_1,{\bf x}_2) + {\rm cov}({\bf x}_2,{\bf x}_1) {\bf A}' \\
&= \Sigma_{11} +\Sigma_{12} \Sigma^{-1}_{22} \Sigma_{22}\Sigma^{-1}_{22}\Sigma_{21}
- 2 \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \\
&= \Sigma_{11} +\Sigma_{12} \Sigma^{-1}_{22}\Sigma_{21}
- 2 \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \\
&= \Sigma_{11} -\Sigma_{12} \Sigma^{-1}_{22}\Sigma_{21}
\end{align*}which proves the second part.Note: For those not very familiar with the matrix algebra used here, this is an excellent resource.Edit: One property used here this is not in the matrix cookbook (good catch @FlyingPig) is property 6 on the wikipedia page about covariance matrices: which is that for two random vectors $\bf x, y$, $${\rm var}({\bf x}+{\bf y}) = {\rm var}({\bf x})+{\rm var}({\bf y}) + {\rm cov}({\bf x},{\bf y}) + {\rm cov}({\bf y},{\bf x})$$ For scalars, of course, ${\rm cov}(X,Y)={\rm cov}(Y,X)$ but for vectors they are different insofar as the matrices are arranged differently."
Crossed vs nested random effects: how do they differ and how are they specified correctly in lme4?,"
Here is how I have understood nested vs. crossed random effects:  
Nested random effects occur when a lower level factor appears only within a particular level of an upper level factor.  

For example, pupils within classes at a fixed point in time.  
In lme4 I thought that we represent the random effects for nested data in either of two equivalent ways:  
(1|class/pupil)  # or  
(1|class) + (1|class:pupil)


Crossed random effects means that a given factor appears in more than one level of the upper level factor.  

For example, there are pupils within classes measured over several years.  
In lme4, we would write: 
(1|class) + (1|pupil)


However, when I was looking at a particular nested dataset, I noticed that both model formulas gave identical results (code and output below). However I have seen other datasets where the two formulas produced different results.  So what is going on here? 
mydata <- read.csv(""https://web.archive.org/web/20160624172041if_/http://www-personal.umich.edu/~bwest/classroom.csv"")
# (the data is no longer at `http://www-personal.umich.edu/~bwest/classroom.csv`
# hence the link to web.archive.org)
# Crossed version: 
Linear mixed model fit by REML ['lmerMod']
Formula: mathgain ~ (1 | schoolid) + (1 | classid)
   Data: mydata

REML criterion at convergence: 11768.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.6441 -0.5984 -0.0336  0.5334  5.6335 

Random effects:
 Groups   Name        Variance Std.Dev.
 classid  (Intercept)   99.23   9.961  
 schoolid (Intercept)   77.49   8.803  
 Residual             1028.23  32.066  
Number of obs: 1190, groups:  classid, 312; schoolid, 107


# Nested version:
Formula: mathgain ~ (1 | schoolid/classid)

REML criterion at convergence: 11768.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.6441 -0.5984 -0.0336  0.5334  5.6335 

Random effects:
 Groups           Name        Variance Std.Dev.
 classid:schoolid (Intercept)   99.23   9.961  
 schoolid         (Intercept)   77.49   8.803  
 Residual                     1028.23  32.066  
Number of obs: 1190, groups:  classid:schoolid, 312; schoolid, 107

","['mixed-model', 'lme4-nlme', 'multilevel-analysis', 'random-effects-model', 'crossed-random-effects']","(This is a fairly long answer, there is a summary at the end)You are not wrong in your understanding of what nested and crossed random effects are in the scenario that you describe. However, your definition of crossed random effects is a little narrow. A more general definition of crossed random effects is simply: not nested. We will look at this at the end of this answer, but the bulk of the answer will focus on the scenario you presented, of classrooms within schools.First note that:Nesting is a property of the data, or rather the experimental design, not the model.Also,Nested data can be encoded in at least 2 different ways, and this is at the heart of the issue you found.The dataset in your example is rather large, so I will use another schools example from the internet to explain the issues. But first, consider the following over-simplified example:Here we have classes nested in schools, which is a familiar scenario. The important point here is that, between each school, the classes have the same identifier, even though they are distinct if they are nested. Class1 appears in School1, School2 and School3. However if the data are nested then Class1 in School1 is not the same unit of measurement as Class1 in School2 and School3. If they were the same, then we would have this situation:which means that every class belongs to every school. The former is a nested design, and the latter is a crossed design (some might also call it multiple membership. Edit: For a discussion of the differences between multiple membership and crossed random effects, see here ), and we would formulate these in lme4 using:(1|School/Class) or equivalently (1|School) + (1|Class:School)and(1|School) + (1|Class)respectively. Due to the ambiguity of whether there is nesting or crossing of random effects, it is very important to specify the model correctly as these models will produce different results, as we shall show below. Moreover, it is not possible to know, just by inspecting the data, whether we have nested or crossed random effects. This can only be determined with knowledge of the data and the experimental design.But first let us consider a case where the Class variable is coded uniquely across schools:There is no longer any ambiguity concerning nesting or crossing.  The nesting is explicit. Let us now see this with an example in R, where we have 6 schools (labelled I-VI) and 4 classes within each school (labelled a to d):We can see from this cross tabulation that every class ID appears in every school, which satisfies your definition of crossed random effects (in this case we have fully, as opposed to partially, crossed random effects,  because every class occurs in every school). So this is the same situation that we had in the first figure above. However, if the data are really nested and not crossed, then we need to explicitly tell lme4:As expected, the results differ because m0 is a nested model while m1 is a crossed model.Now, if we introduce a new variable for the  class identifier:The cross tabulation shows that each level of class occurs only in one level of school, as per your definition of nesting. This is also the case with your data, however it is difficult to show that with your data because it is very sparse. Both model formulations will now produce the same output (that of the nested model m0 above):It is worth noting that crossed random effects do not have to occur within the same factor - in the above the crossing was completely within school. However, this does not have to be the case, and very often it is not. For example, sticking with a school scenario, if instead of classes within schools we have pupils within schools, and we were also interested in the doctors that the pupils were registered with, then we would also have nesting of pupils within doctors. There is no nesting of schools within doctors, or vice versa, so this is also an example of crossed random effects, and we say that schools and doctors are crossed. A similar scenario where crossed random effects occur is when individual observations are nested within two factors simultaneously, which commonly occurs with so-called repeated measures subject-item data. Typically each subject is measured/tested multiple times with/on different items and these same items are measured/tested by different subjects. Thus, observations are clustered within subjects and within items, but items are not nested within subjects or vice-versa. Again, we say that subjects and items are crossed.Summary: TL;DRThe difference between crossed and nested random effects is that nested random effects occur when one factor (grouping variable) appears only within a particular level of another factor (grouping variable). This is specified in lme4 with:(1|group1/group2)where group2 is nested within group1.Crossed random effects are simply: not nested. This can occur with three or more grouping variables (factors) where one factor is separately nested in both of the others, or with two or more factors where individual observations are nested separately within the two factors. These are specified in lme4 with:(1|group1) + (1|group2)"
"What is the difference between a neural network and a deep neural network, and why do the deep ones work better?","
I haven't seen the question stated precisely in these terms, and this is why I make a new question.
What I am interested in knowing is not the definition of a neural network, but understanding the actual difference with a deep neural network.
For more context: I know what a neural network is and how backpropagation works. I know that a DNN must have multiple hidden layers. However, 10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a neural network is able to represent (see Cybenko's Universal approximation theorem), and that having more layers made it more complex to analyse without gain in performance. Obviously, that is not the case anymore.
I suppose, maybe wrongly, that the differences are in terms of training algorithm and properties rather than structure, and therefore I would really appreciate if the answer could underline the reasons that made the move to DNN possible (e.g. mathematical proof or randomly playing with networks?) and desirable (e.g. speed of convergence?)
","['neural-networks', 'deep-learning']","Let's start with a triviliaty: Deep neural network is simply a feedforward network with many hidden layers.This is more or less all there is to say about the definition. Neural networks can be recurrent or feedforward; feedforward ones do not have any loops in their graph and can be organized in layers. If there are ""many"" layers, then we say that the network is deep.How many layers does a network have to have in order to qualify as deep? There is no definite answer to this  (it's a bit like asking how many grains make a heap), but usually having two or more hidden layers counts as deep. In contrast, a network with only a single hidden layer is conventionally called ""shallow"". I suspect that there will be some inflation going on here, and in ten years people might think that anything with less than, say, ten layers is shallow and suitable only for kindergarten exercises. Informally, ""deep"" suggests that the network is tough to handle.Here is an illustration, adapted from here:But the real question you are asking is, of course, Why would having many layers be beneficial?I think that the somewhat astonishing answer is that nobody really knows. There are some common explanations that I will briefly review below, but none of them has been convincingly demonstrated to be true, and one cannot even be sure that having many layers is really beneficial.I say that this is astonishing, because deep learning is massively popular, is breaking all the records (from image recognition, to playing Go, to automatic translation, etc.) every year, is getting used by the industry, etc. etc. And we are still not quite sure why it works so well.I base my discussion on the Deep Learning book by Goodfellow, Bengio, and Courville which went out in 2017 and is widely considered to be the book on deep learning. (It's freely available online.) The relevant section is 6.4.1 Universal Approximation Properties and Depth.You wrote that 10 years ago in class I learned that having several layers or one layer (not counting the input and output layers) was equivalent in terms of the functions a neural network is able to represent [...]You must be referring to the so called Universal approximation theorem, proved by Cybenko in 1989 and generalized by various people in the 1990s. It basically says that a shallow neural network (with 1 hidden layer) can approximate any function, i.e. can in principle learn anything. This is true for various nonlinear activation functions, including rectified linear units that most neural networks are using today (the textbook references Leshno et al. 1993 for this result).If so, then why is everybody using deep nets?Well, a naive answer is that because they work better. Here is a figure from the Deep Learning book showing that it helps to have more layers in one particular task, but the same phenomenon is often observed across various tasks and domains:We know that a shallow network could perform as good as the deeper ones. But it does not; and they usually do not. The question is --- why? Possible answers:The Deep Learning book argues for bullet points #1 and #3. First, it argues that the number of units in a shallow network grows exponentially with task complexity. So in order to be useful a shallow network might need to be very big; possibly much bigger than a deep network. This is based on a number of papers proving that shallow networks would in some cases need exponentially many neurons; but whether e.g. MNIST classification or Go playing are such cases is not really clear. Second, the book says this:Choosing a deep model encodes a very general belief that the function we
  want to learn should involve composition of several simpler functions. This can be
  interpreted from a representation learning point of view as saying that we believe
  the learning problem consists of discovering a set of underlying factors of variation
  that can in turn be described in terms of other, simpler underlying factors of
  variation.I think the current ""consensus"" is that it's a combination of bullet points #1 and #3: for real-world tasks deep architecture are often beneficial and shallow architecture would be inefficient and require a lot more neurons for the same performance.But it's far from proven. Consider e.g. Zagoruyko and  Komodakis, 2016, Wide Residual Networks. Residual networks with 150+ layers appeared in 2015 and won various image recognition contests. This was a big success and looked like a compelling argument in favour of deepness; here is one figure from a presentation by the first author on the residual network paper (note that the time confusingly goes to the left here):But the paper linked above shows that a ""wide"" residual network with ""only"" 16 layers can outperform ""deep"" ones with 150+ layers. If this is true, then the whole point of the above figure breaks down.Or consider Ba and Caruana, 2014, Do Deep Nets Really Need to be Deep?:In this paper we provide empirical evidence that shallow nets are capable of learning the same
  function as deep nets, and in some cases with the same number of parameters as the deep nets. We
  do this by first training a state-of-the-art deep model, and then training a shallow model to mimic the
  deep model. The mimic model is trained using the model compression scheme described in the next
  section. Remarkably, with model compression we are able to train shallow nets to be as accurate
  as some deep models, even though we are not able to train these shallow nets to be as accurate as
  the deep nets when the shallow nets are trained directly on the original labeled training data. If a
  shallow net with the same number of parameters as a deep net can learn to mimic a deep net with
  high fidelity, then it is clear that the function learned by that deep net does not really have to be deep.If true, this would mean that the correct explanation is rather my bullet #2, and not #1 or #3.As I said --- nobody really knows for sure yet.Concluding remarksThe amount of progress achieved in the deep learning over the last ~10 years is truly amazing, but most of this progress was achieved by trial and error, and we still lack very basic understanding about what exactly makes deep nets to work so well. Even the list of things that people consider to be crucial for setting up an effective deep network seems to change every couple of years.The deep learning renaissance started in 2006 when Geoffrey Hinton (who had been working on neural networks for 20+ years without much interest from anybody) published a couple of breakthrough papers offering an effective way to train deep networks (Science paper, Neural computation paper). The trick was to use unsupervised pre-training before starting the gradient descent. These papers revolutionized the field, and for a couple of years people thought that unsupervised pre-training was the key. Then in 2010 Martens showed that deep neural networks can be trained with second-order methods (so called Hessian-free methods) and can outperform networks trained with pre-training: Deep learning via Hessian-free optimization. Then in 2013 Sutskever et al. showed that  stochastic gradient descent with some very clever tricks can outperform  Hessian-free methods: On the importance of initialization and momentum in deep learning. Also, around 2010 people realized that using rectified linear units instead of sigmoid units makes a huge difference for gradient descent. Dropout appeared in 2014. Residual networks appeared in 2015. People keep coming up with more and more effective ways to train deep networks and what seemed like a key insight 10 years ago is often considered a nuisance today. All of that is largely driven by trial and error and there is little understanding of what makes some things work so well and some other things not. Training deep networks is like a big bag of tricks. Successful tricks are usually rationalized post factum.We don't even know why deep networks reach a performance plateau; just 10 years people used to blame local minima, but the current thinking is that this is not the point (when the perfomance plateaus, the gradients tend to stay large). This is such a basic question about deep networks, and we don't even know this.Update: This is more or less the subject of Ali Rahimi's NIPS 2017 talk on machine learning as alchemy: https://www.youtube.com/watch?v=Qi1Yry33TQE.[This answer was entirely re-written in April 2017, so some of the comments below do not apply anymore.]"
What is the difference between off-policy and on-policy learning?,"
Artificial intelligence website defines off-policy and on-policy learning as follows: 

""An off-policy learner learns the value of the optimal policy independently of the agent's actions. Q-learning is an off-policy learner. An on-policy learner learns the value of the policy being carried out by the agent including the exploration steps.""

I would like to ask your clarification regarding this, because they don't seem to make any difference to me. Both the definitions seem like they are identical. What I actually understood are the model-free and model-based learning, and I don't know if they have anything to do with the ones in question. 
How is it possible that the optimal policy is learned independently of the agent's actions? Isn't the policy learned when the agent performs the actions? 
","['machine-learning', 'reinforcement-learning', 'artificial-intelligence']","First of all, there's no reason that an agent has to do the greedy action;  Agents can explore or they can follow options.   This is not what separates on-policy from off-policy learning.The reason that Q-learning is off-policy is that it updates its Q-values using the Q-value of the next state $s'$ and the greedy action $a'$.  In other words, it estimates the return (total discounted future reward) for state-action pairs assuming a greedy policy were followed despite the fact that it's not following a greedy policy.The reason that SARSA is on-policy is that it updates its Q-values using the Q-value of the next state $s'$ and the current policy's action $a''$.  It estimates the return for state-action pairs assuming the current policy continues to be followed.The distinction disappears if the current policy is a greedy policy.  However, such an agent would not be good since it never explores.Have you looked at the book available for free online?  Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. Second edition, 
MIT Press, Cambridge, MA, 2018."
What's the difference between Normalization and Standardization?,"
At work we were discussing this as my boss has never heard of normalization. In Linear Algebra, Normalization seems to refer to the dividing of a vector by its length. And in statistics, Standardization seems to refer to the subtraction of a mean then dividing by its SD. But they seem interchangeable with other possibilities as well. 
When creating some kind of universal score, that makes up $2$ different metrics, which have different means and different SD's, would you Normalize, Standardize, or something else? One person told me it's just a matter of taking each metric and dividing them by their SD, individually. Then summing the two. And that will result in a universal score that can be used to judge both metrics.
For instance, say you had the number of people who take the subway to work (in NYC) and the number of people who drove to work (in NYC). 
$$\text{Train} \longrightarrow x$$
$$\text{Car} \longrightarrow y$$
If you wanted to create a universal score to quickly report traffic fluctuations, you can't just add $\text{mean}(x)$ and $\text{mean}(y)$ because there will be a LOT more people who ride the train. There's 8 million people living in NYC, plus tourists. That's millions of people taking the train everyday verse hundreds of thousands of people in cars. So they need to be transformed to a similar scale in order to be compared.
If $\text{mean}(x) = 8,000,000$
and $\text{mean}(y) = 800,000$
Would you normalize $x$ & $y$ then sum? Would you standardize $x$ & $y$ then sum? Or would you divide each by their respective SD then sum?  In order to get to a number that when fluctuates, represents total traffic fluctuations.
Any article or chapters of books for reference would be much appreciated. THANKS!
Also here's another example of what I'm trying to do. 
Imagine you're a college dean, and you're discussing admission requirements. You may want students with at least a certain GPA and a certain test score. It'd be nice if they were both on the same scale because then you could just add the two together and say, ""anyone with at least a 7.0 can get admitted."" That way, if a prospective student has a 4.0 GPA, they could get as low as a 3.0 test score and still get admitted.  Inversely, if someone had a 3.0 GPA, they could still get admitted with a 4.0 test score.
But it's not like that. The ACT is on a 36 point scale and most GPA's are on 4.0 (some are 4.3, yes annoying). Since I can't just add an ACT and GPA to get some kind of universal score, how can I transform them so they can be added, thus creating a universal admission score.  And then as a Dean, I could just automatically accept anyone with a score above a certain threshold.  Or even automatically accept everyone whose score is within the top 95%.... those sorts of things.
Would that be normalization? standardization? or just dividing each by their SD then summing?
","['descriptive-statistics', 'normalization', 'standardization']",
Does Julia have any hope of sticking in the statistical community?,"
I recently read a post from R-Bloggers, that linked to this blog post from John Myles White about a new language called Julia. Julia takes advantage of a just-in-time compiler that gives it wicked fast run times and puts it on the same order of magnitude of speed as C/C++ (the same order, not equally fast). Furthermore, it uses the orthodox looping mechanisms that those of us who started programming on traditional languages are familiar with, instead of R's apply statements and vector operations.  
R is not going away by any means, even with such awesome timings from Julia. It has extensive support in industry, and numerous wonderful packages to do just about anything.    
My interests are Bayesian in nature, where vectorizing is often not possible. Certainly serial tasks must be done using loops and involve heavy computation at each iteration. R can be very slow at these serial looping tasks, and C/++ is not a walk in the park to write. Julia seems like a great alternative to writing in C/++, but it's in its infancy, and lacks a lot of the functionality I love about R. It would only make sense to learn Julia as a computational statistics workbench if it garners enough support from the statistics community and people start writing useful packages for it. 
My questions follow:  

What features does Julia need to have in order to have the allure that made R the de facto language of statistics?
What are the advantages and disadvantages of learning Julia to do computationally-heavy tasks, versus learning a low-level language like C/++?

","['r', 'software', 'computational-statistics', 'julia']",
Gradient Boosting Tree vs Random Forest,"
Gradient tree boosting as proposed by Friedman uses decision trees as base learners. I'm wondering if we should make the base decision tree as complex as possible (fully grown) or simpler? Is there any explanation for the choice?
Random Forest is another ensemble method using decision trees as base learners. 
Based on my understanding, we generally use the almost fully grown decision trees in each iteration. Am I right?
","['machine-learning', 'random-forest', 'cart', 'boosting', 'ensemble-learning']",
When is R squared negative? [duplicate],"







This question already has answers here:
                                
                            




What does negative R-squared mean?

                                (6 answers)
                            

Closed 11 months ago.



My understanding is that $R^2$ cannot be negative as it is the square of R. However I ran a simple linear regression in SPSS with a single independent variable and a dependent variable. My SPSS output give me a negative value for $R^2$. If I was to calculate this by hand from R then $R^2$ would be 
positive. What has SPSS done to calculate this as negative? 
R=-.395
R squared =-.156
B (un-standardized)=-1261.611

Code I've used:
DATASET ACTIVATE DataSet1. 
REGRESSION /MISSING LISTWISE /STATISTICS COEFF OUTS R ANOVA 
           /CRITERIA=PIN(.05) POUT(.10) /NOORIGIN 
           /DEPENDENT valueP /METHOD=ENTER ageP

I get a negative value. Can anyone explain what this means?


","['regression', 'spss', 'r-squared']","$R^2$ compares the fit of the chosen model with that of a horizontal straight line (the null hypothesis). If the chosen model fits worse than a horizontal line, then $R^2$ is negative. Note that $R^2$ is not always the square of anything, so it can have a negative value without violating any rules of math. $R^2$ is negative only when the chosen model does not follow the trend of the data, so fits worse than a horizontal line.Example: fit data to a linear regression model constrained so that the $Y$ intercept must equal $1500$.The model makes no sense at all given these data. It is clearly the wrong model, perhaps chosen by accident.The fit of the model (a straight line constrained to go through the point (0,1500)) is worse than the fit of a horizontal line. Thus the sum-of-squares from the model $(SS_\text{res})$ is larger than the sum-of-squares from the horizontal line $(SS_\text{tot})$.If $R^2$ is computed as $1 - \frac{SS_\text{res}}{SS_\text{tot}}$.
(here, $SS_{res}$ = residual error.)
When $SS_\text{res}$ is greater than $SS_\text{tot}$, that equation could compute a negative value for $R^2$, if the value of the coeficient is greater than 1.With linear regression with no constraints, $R^2$ must be positive (or zero) and equals the square of the correlation coefficient, $r$. A negative $R^2$ is only possible with linear regression when either the intercept or the slope are constrained so that the ""best-fit"" line (given the constraint) fits worse than a horizontal line. With nonlinear regression, the $R^2$ can be negative whenever the best-fit model (given the chosen equation, and its constraints, if any) fits the data worse than a horizontal line.Bottom line: a negative $R^2$ is not a mathematical impossibility or the sign of a computer bug. It simply means that the chosen model (with its constraints) fits the data really poorly."
How are the standard errors of coefficients calculated in a regression?,"
For my own understanding, I am interested in manually replicating the calculation of the standard errors of estimated coefficients as, for example, come with the output of the lm() function in R, but haven't been able to pin it down. What is the formula / implementation used?
","['r', 'regression', 'standard-error', 'lm', 'faq']","The linear model is written as
$$
\left|
\begin{array}{l}
\mathbf{y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon} \\
 \mathbf{\epsilon} \sim N(0, \sigma^2 \mathbf{I}),
\end{array}
\right.$$
where $\mathbf{y}$ denotes the vector of responses, $\mathbf{\beta}$ is the vector of fixed effects parameters, $\mathbf{X}$ is the corresponding design matrix whose columns are the values of the explanatory variables, and $\mathbf{\epsilon}$ is the vector of random errors.It is well known that an estimate of $\mathbf{\beta}$ is given by (refer, e.g., to the wikipedia article)
$$\hat{\mathbf{\beta}} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{y}.$$
Hence
$$
\textrm{Var}(\hat{\mathbf{\beta}}) =
 (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
 \;\sigma^2 \mathbf{I} \; \mathbf{X}  (\mathbf{X}^{\prime} \mathbf{X})^{-1}
= \sigma^2 (\mathbf{X}^{\prime} \mathbf{X})^{-1} (\mathbf{X}^{\prime}
 \mathbf{X})  (\mathbf{X}^{\prime} \mathbf{X})^{-1}
= \sigma^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1},
$$
[reminder: $\textrm{Var}(AX)=A\times \textrm{Var}(X) \times A′$, for some random vector $X$ and some non-random matrix $A$]so that
$$
\widehat{\textrm{Var}}(\hat{\mathbf{\beta}}) = \hat{\sigma}^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1},
$$
where $\hat{\sigma}^2$ can be obtained by the Mean Square Error (MSE) in the ANOVA  table.Example with a simple linear regression in RWhen there is a single explanatory variable, the model reduces to
$$y_i = a + bx_i + \epsilon_i, \qquad i = 1, \dotsc, n$$
and
$$\mathbf{X} = \left(
\begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{array}
\right), \qquad \mathbf{\beta} = \left(
\begin{array}{c}
a\\b
\end{array}
\right)$$
so that
$$(\mathbf{X}^{\prime} \mathbf{X})^{-1} = \frac{1}{n\sum x_i^2 - (\sum x_i)^2} 
\left(
\begin{array}{cc}
\sum x_i^2 & -\sum x_i \\
-\sum x_i  & n
\end{array}
\right)$$
and formulas become more transparant. For example, the standard error of the estimated slope is
$$\sqrt{\widehat{\textrm{Var}}(\hat{b})} = \sqrt{[\hat{\sigma}^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1}]_{22}} = \sqrt{\frac{n \hat{\sigma}^2}{n\sum x_i^2 - (\sum x_i)^2}}.$$"
How to reverse PCA and reconstruct original variables from several principal components?,"
Principal component analysis (PCA) can be used for dimensionality reduction. After such dimensionality reduction is performed, how can one approximately reconstruct the original variables/features from a small number of principal components?
Alternatively, how can one remove or discard several principal components from the data?
In other words, how to reverse PCA?

Given that PCA is closely related to singular value decomposition (SVD), the same question can be asked as follows: how to reverse SVD?
","['pca', 'dimensionality-reduction', 'svd']","PCA computes eigenvectors of the covariance matrix (""principal axes"") and sorts them by their eigenvalues (amount of explained variance). The centered data can then be projected onto these principal axes to yield principal components (""scores""). For the purposes of dimensionality reduction, one can keep only a subset of principal components and discard the rest. (See here for a layman's introduction to PCA.)Let $\mathbf X_\text{raw}$ be the $n\times p$ data matrix with $n$ rows (data points) and $p$ columns (variables, or features). After subtracting the mean vector $\boldsymbol \mu$ from each row, we get the centered data matrix $\mathbf X$. Let $\mathbf V$ be the $p\times k$ matrix of some $k$ eigenvectors that we want to use; these would most often be the $k$ eigenvectors with the largest eigenvalues. Then the $n\times k$ matrix of PCA projections (""scores"") will be simply given by $\mathbf Z=\mathbf {XV}$.This is illustrated on the figure below: the first subplot shows some centered data (the same data that I use in my animations in the linked thread) and its projections on the first principal axis. The second subplot shows only the values of this projection; the dimensionality has been reduced from two to one: In order to be able to reconstruct the original two variables from this one principal component, we can map it back to $p$ dimensions with $\mathbf V^\top$. Indeed, the values of each PC should be placed on the same vector as was used for projection; compare subplots 1 and 3. The result is then given by $\hat{\mathbf X} = \mathbf{ZV}^\top = \mathbf{XVV}^\top$. I am displaying it on the third subplot above. To get the final reconstruction $\hat{\mathbf X}_\text{raw}$, we need to add the mean vector $\boldsymbol \mu$ to that:$$\boxed{\text{PCA reconstruction} = \text{PC scores} \cdot \text{Eigenvectors}^\top + \text{Mean}}$$Note that one can go directly from the first subplot to the third one by multiplying $\mathbf X$ with the $\mathbf {VV}^\top$ matrix; it is called a projection matrix. If all $p$ eigenvectors are used, then $\mathbf {VV}^\top$ is the identity matrix (no dimensionality reduction is performed, hence ""reconstruction"" is perfect). If only a subset of eigenvectors is used, it is not identity.This works for an arbitrary point $\mathbf z$ in the PC space; it can be mapped to the original space via $\hat{\mathbf x} = \mathbf{zV}^\top$.Discarding (removing) leading PCs Sometimes one wants to discard (to remove) one or few of the leading PCs and to keep the rest, instead of keeping the leading PCs and discarding the rest (as above). In this case all the formulas stay exactly the same, but $\mathbf V$ should consist of all principal axes except for the ones one wants to discard. In other words, $\mathbf V$ should always include all PCs that one wants to keep.Caveat about PCA on correlationWhen PCA is done on correlation matrix (and not on covariance matrix), the raw data $\mathbf X_\mathrm{raw}$ is not only centered by subtracting  $\boldsymbol \mu$ but also scaled by dividing each column by its standard deviation $\sigma_i$. In this case, to reconstruct the original data, one needs to back-scale the columns of $\hat{\mathbf X}$ with $\sigma_i$ and only then to add back the mean vector $\boldsymbol \mu$.This topic often comes up in the context of image processing. Consider Lenna -- one of the standard images in image processing literature (follow the links to find where it comes from). Below on the left, I display the grayscale variant of this $512\times 512$ image (file available here).We can treat this grayscale image as a $512\times 512$ data matrix $\mathbf X_\text{raw}$. I perform PCA on it and compute $\hat {\mathbf X}_\text{raw}$ using the first 50 principal components. The result is displayed on the right. PCA is very closely related to singular value decomposition (SVD), see 
Relationship between SVD and PCA. How to use SVD to perform PCA? for more details. If a $n\times p$ matrix $\mathbf X$ is SVD-ed as $\mathbf X = \mathbf {USV}^\top$ and one selects a $k$-dimensional vector $\mathbf z$ that represents the point in the ""reduced"" $U$-space of $k$ dimensions, then to map it back to $p$ dimensions one needs to multiply it with $\mathbf S^\phantom\top_{1:k,1:k}\mathbf V^\top_{:,1:k}$.I will conduct PCA on the Fisher Iris data and then reconstruct it using the first two principal components. I am doing PCA on the covariance matrix, not on the correlation matrix, i.e. I am not scaling the variables here. But I still have to add the mean back. Some packages, like Stata, take care of that through the standard syntax. Thanks to @StasK and @Kodiologist for their help with the code.We will check the reconstruction of the first datapoint, which is:MatlabOutput:ROutput:For worked out R example of PCA reconstruction of images see also this answer.PythonOutput:Note that this differs slightly from the results in other languages. That is because Python's version of the Iris dataset contains mistakes. Stata"
"Objective function, cost function, loss function: are they the same thing?","
In machine learning, people talk about objective function, cost function, loss function. Are they just different names of the same thing? When to use them? If they are not always refer to the same thing, what are the differences?
","['machine-learning', 'terminology', 'loss-functions', 'artificial-intelligence']","These are not very strict terms and they are highly related. However:Long story short, I would say that:A loss function is a part of a cost function which is a type of an objective function.All that being said, thse terms are far from strict, and depending on context, research group, background, can shift and be used in a different meaning. With the main (only?) common thing being ""loss"" and ""cost"" functions being something that want wants to minimise, and objective function being something one wants to optimise (which can be both maximisation or minimisation)."
What's the difference between principal component analysis and multidimensional scaling?,"
How are PCA and classical MDS different? How about MDS versus nonmetric MDS? Is there a time when you would prefer one over the other? How do the interpretations differ?
","['pca', 'multidimensional-scaling', 'pcoa']","Classic Torgerson's metric MDS is actually done by transforming distances into similarities and performing PCA (eigen-decomposition or singular-value-decomposition) on those. [The other name of this procedure (distances between objects -> similarities between them -> PCA, whereby loadings are the sought-for coordinates) is Principal Coordinate Analysis or PCoA.] So, PCA might be called the algorithm of the simplest MDS.Non-metric MDS is based on iterative ALSCAL or PROXSCAL algorithm (or algorithm similar to them) which is a more versatile mapping technique than PCA and can be applied to metric MDS as well. While PCA retains m important dimensions for you, ALSCAL/PROXSCAL fits configuration to m dimensions (you pre-define m) and it reproduces dissimilarities on the map more directly and accurately than PCA usually can (see Illustration section below).Thus, MDS and PCA are probably not at the same level to be in line or opposite to each other. PCA is just a method while MDS is a class of analysis. As mapping, PCA is a particular case of MDS. On the other hand, PCA is a particular case of Factor analysis which, being a data reduction, is more than only a mapping, while MDS is only a mapping.As for your question about metric MDS vs non-metric MDS there's little to comment because the answer is straightforward. If I believe my input dissimilarities are so close to be euclidean distances that a linear transform will suffice to map them in m-dimensional space, I will prefer metric MDS. If I don't believe, then monotonic transform is necessary, implying use of non-metric MDS.A note on terminology for a reader. Term Classic(al) MDS (CMDS) can have two different meanings in a vast literature on MDS, so it is ambiguous and should be avoided. One definition is that CMDS is a synonym of Torgerson's metric MDS. Another definition is that CMDS is any MDS (by any algorithm; metric or nonmetric analysis) with single matrix input (for there exist models analyzing many matrices at once - Individual ""INDSCAL"" model and Replicated model).Illustration to the answer. Some cloud of points (ellipse) is being mapped on a one-dimensional mds-map. A pair of points is shown in red dots.Iterative or ""true"" MDS aims straight to reconstruct pairwise distances between objects. For it is the task of any MDS. Various stress or misfit criteria could be minimized between original distances and distances on the map: $\|D_o-D_m\|_2^2$, $\|D_o^2-D_m^2\|_1$, $\|D_o-D_m\|_1$. An algorithm may (non-metric MDS) or may not (metric MDS) include monotonic transformation on this way.PCA-based MDS (Torgerson's, or PCoA) is not straight. It minimizes the squared distances between objects in the original space and their images on the map. This is not quite genuine MDS task; it is successful, as MDS, only to the extent to which the discarded junior principal axes are weak. If $P_1$ explains much more variance than $P_2$ the former can alone substantially reflect pairwise distances in the cloud, especially for points lying far apart along the ellipse. Iterative MDS will always win, and especially when the map is wanted very low-dimensional. Iterative MDS, too, will succeed more when a cloud ellipse is thin, but will fulfill the MDS-task better than PCoA. By the property of the double-centration matrix (described here) it appears that PCoA minimizes $\|D_o\|_2^2-\|D_m\|_2^2$, which is different from any of the above minimizations.Once again, PCA projects cloud's points on the most advantageous all-corporal saving subspace. It does not project pairwise distances, relative locations of points on a subspace most saving in that respect, as iterative MDS does it. Nevertheless, historically PCoA/PCA is considered among the methods of metric MDS."
The Sleeping Beauty Paradox,"
The situation
Some researchers would like to put you to sleep.  Depending on the secret toss of a fair coin, they will briefly awaken you either once (Heads) or twice (Tails).  After each waking, they will put you back to sleep with a drug that makes you forget that awakening.  When you are awakened, to what degree should you believe that the outcome of the coin toss was Heads?
(OK, maybe you don’t want to be the subject of this experiment!  Suppose instead that Sleeping Beauty (SB) agrees to it (with the full approval of the Magic Kingdom’s Institutional Review Board, of course).  She’s about to go to sleep for one hundred years, so what are one or two more days, anyway?)

[Detail of a Maxfield Parrish illustration.]
Are you a Halfer or a Thirder?
The Halfer position.  Simple! The coin is fair--and SB knows it--so she should believe there's a one-half chance of heads.
The Thirder position. Were this experiment to be repeated many times, then the coin will be heads only one third of the time SB is awakened.  Her probability for heads will be one third.
Thirders have a problem
Most, but not all, people who have written about this are thirders.  But:

On Sunday evening, just before SB falls asleep, she must believe the chance of heads is one-half: that’s what it means to be a fair coin.

Whenever SB awakens, she has learned absolutely nothing she did not know Sunday night.  What rational argument can she give, then, for stating that her belief in heads is now one-third and not one-half?


Some attempted explanations

SB would necessarily lose money if she were to bet on heads with any odds other than 1/3.  (Vineberg, inter alios)

One-half really is correct: just use the Everettian “many-worlds” interpretation of Quantum Mechanics!  (Lewis).

SB updates her belief based on self-perception of her “temporal location” in the world.  (Elga, i.a.)

SB is confused: “[It] seems more plausible to say that her epistemic state upon waking up should not include a definite degree of belief in heads. … The real issue is how one deals with known, unavoidable, cognitive malfunction.”  [Arntzenius]



The question
Accounting for what has already been written on this subject (see the references as well as a previous post), how can this paradox be resolved in a statistically rigorous way?  Is this even possible?

References
Arntzenius, Frank (2002).  Reflections on Sleeping Beauty Analysis 62.1 pp 53-62.
Bradley, DJ (2010).  Confirmation in a Branching World: The Everett Interpretation and Sleeping Beauty.  Brit. J. Phil. Sci. 0 (2010), 1–21.
Elga, Adam (2000).  Self-locating belief and the Sleeping Beauty Problem.  Analysis 60 pp 143-7.
Franceschi, Paul (2005).  Sleeping Beauty and the Problem of World Reduction.  Preprint.
Groisman, Berry (2007).  The end of Sleeping Beauty’s nightmare.  Preprint.
Lewis, D (2001).  Sleeping Beauty: reply to Elga.  Analysis 61.3 pp 171-6.
Papineau, David and Victor Dura-Vila (2008).  A Thirder and an Everettian: a reply to Lewis’s ‘Quantum Sleeping Beauty’.
Pust, Joel (2008).  Horgan on Sleeping Beauty.  Synthese 160 pp 97-101.
Vineberg, Susan (undated, perhaps 2003).  Beauty’s Cautionary Tale.
","['decision-theory', 'paradox']",
Percentile vs quantile vs quartile,"
What is the difference between the three terms below?

percentile
quantile
quartile

","['descriptive-statistics', 'quantiles', 'median', 'percentage']",
Why are p-values uniformly distributed under the null hypothesis?,"
Recently, I have found in a paper by Klammer, et al. a statement that p-values should be uniformly distributed. I believe the authors, but cannot understand why it is so. 
Klammer, A. A., Park, C. Y., and Stafford Noble, W. (2009) Statistical Calibration of the SEQUEST XCorr Function. Journal of Proteome Research. 8(4): 2106–2113.
","['p-value', 'uniform-distribution']","To clarify a bit.  The p-value is uniformly distributed when the null hypothesis is true and all other assumptions are met.  The reason for this is really the definition of alpha as the probability of a type I error.  We want the probability of rejecting a true null hypothesis to be alpha, we reject when the observed $\text{p-value} < \alpha$, the only way this happens for any value of alpha is when the p-value comes from a uniform distribution.  The whole point of using the correct distribution (normal, t, f, chisq, etc.) is to transform from the test statistic to a uniform p-value.  If the null hypothesis is false then the distribution of the p-value will (hopefully) be more weighted towards 0.The Pvalue.norm.sim and Pvalue.binom.sim functions in the TeachingDemos package for R will simulate several data sets, compute the p-values and plot them to demonstrate this idea. Also see: Murdoch, D, Tsai, Y, and Adcock, J (2008). P-Values are Random
  Variables. The American Statistician, 62, 242-245.for some more details.Since people are still reading this answer and commenting, I thought that I would address @whuber's comment.It is true that when using a composite null hypothesis like $\mu_1 \leq \mu_2$ that the p-values will only be uniformly distributed when the 2 means are exactly equal and will not be a uniform if $\mu_1$ is any value that is less than $\mu_2$.  This can easily be seen using the Pvalue.norm.sim function and setting it to do a one sided test and simulating with the simulation and hypothesized means different (but in the direction to make the null true).As far as statistical theory goes, this does not matter.  Consider if I claimed that I am taller than every member of your family, one way to test this claim would be to compare my height to the height of each member of your family one at a time.  Another option would be to find the member of your family that is the tallest and compare their height with mine.  If I am taller than that one person then I am taller than the rest as well and my claim is true, if I am not taller than that one person then my claim is false.  Testing a composite null can be seen as a similar process, rather than testing all the possible combinations where $\mu_1 \leq \mu_2$ we can test just the equality part because if we can reject that $\mu_1 = \mu_2$ in favour of $\mu_1 > \mu_2$ then we know that we can also reject all the possibilities of $\mu_1 < \mu_2$.   If we look at the distribution of p-values for cases where $\mu_1 < \mu_2$ then the distribution will not be perfectly uniform but will have more values closer to 1 than to 0 meaning that the probability of a type I error will be less than the selected $\alpha$ value making it a conservative test.  The uniform becomes the limiting distribution as $\mu_1$ gets closer to $\mu_2$ (the people who are more current on the stat-theory terms could probably state this better in terms of distributional supremum or something like that).  So by constructing our test assuming the equal part of the null even when the null is composite, then we are designing our test to have a probability of a type I error that is at most $\alpha$ for any conditions where the null is true."
How to choose between Pearson and Spearman correlation?,"
How do I know when to choose between Spearman's $\rho$ and Pearson's $r$? My variable includes satisfaction and the scores were interpreted using the sum of the scores. However, these scores could also be ranked. 
","['correlation', 'pearson-r', 'spearman-rho']",
Does causation imply correlation?,"
Correlation does not imply causation, as there could be many explanations for the correlation. But does causation imply correlation? Intuitively, I would think that the presence of causation means there is necessarily some correlation. But my intuition has not always served me well in statistics. Does causation imply correlation?
","['correlation', 'causality']","As many of the answers above have stated, causation does not imply linear correlation. Since a lot of the correlation concepts come from fields that rely heavily on linear statistics, usually correlation is seen as equal to linear correlation. The wikipedia article is an alright source for this, I really like this image:Look at some of the figures in the bottom row, for instance the parabola-ish shape in the 4th example. This is kind of what happens in @StasK answer (with a little bit of noise added). Y can be fully caused by X but if the numeric relationship is not linear and symmetric, you will still have a correlation of 0.The word you are looking for is mutual information: this is sort of the general non-linear version of correlation. In that case, your statement would be true: causation implies high mutual information."
"R vs SAS, why is SAS preferred by private companies?","
I learned R but it seems that companies are much more interested in SAS experience. What are the advantages of SAS over R? 
","['r', 'sas']","I think there are several issues (in ascending order of possible validity):  Personally, I only think #3 has any legitimate merit, although there are approaches to big data that have been developed with R.  The issues with #1 speak for themselves. I think #2 ignores several facts: there is some vetting that goes on with R, many of the main packages are written by some of the biggest names in statistics, and there have been studies that compare the accuracy of different statistical software & R has certainly been competitive. "
Help me understand Bayesian prior and posterior distributions,"

In a group of students, there are 2 out of 18 that are left-handed. Find the posterior distribution of left-handed students in the population assuming uninformative prior. Summarize the results. According to the literature 5-20% of people are left-handed. Take this information into account in your prior and calculate new posterior. 

I know the beta distribution should be used here. First, with $\alpha$ and $\beta$ values as 1? The equation I found in the material for posterior is 
$$\pi(r \vert Y ) \propto r^{(Y +−1)} \times (1 − r)^{(N−Y +−1)} \\
$$
$Y=2$, $N=18$
Why is that $r$ in the equation? ($r$ denoting the proportion of left-handed people). It is unknown, so how can it be in this equation? To me it seems ridiculous to calculate $r$ given $Y$ and use that $r$ in the equation giving $r$. Well, with the sample $r=2/18$ the result was $0,0019$. The $f$ should I deduce from that?
The equation giving an expected value of $R$ given known $Y$ and $N$ worked better and gave me $0,15$ which sounds about right. The equation being $E(r | X, N, α, β) = (α + X)/(α + β + N)$ with value $1$ assigned to $α$ and $β$. What values should I give $α$ and $β$ to take into account prior information?
Some tips would be much appreciated. A general lecture on prior and posterior distributions wouldn't hurt either (I have vague understanding what they are but only vague) Also bear in mind I'm not very advanced statistician (actually I'm a political scientist by my main trade) so advanced mathematics will probably fly over my head. 
","['distributions', 'bayesian', 'prior', 'posterior']",
Pearson's or Spearman's correlation with non-normal data,"
I get this question frequently enough in my statistics consulting work, that I thought I'd post it here. I have an answer, which is posted below, but I was keen to hear what others have to say.
Question: If you have two variables that are not normally distributed, should you use Spearman's rho for the correlation?
","['correlation', 'normality-assumption', 'pearson-r', 'spearman-rho']","Pearson's correlation is a measure of the linear relationship between two continuous random variables. It does not assume normality although it does assume finite variances and finite covariance. When the variables are bivariate normal, Pearson's correlation provides a complete description of the association.Spearman's correlation applies to ranks and so provides a measure of a monotonic relationship between two continuous random variables. It is also useful with ordinal data and is robust to outliers (unlike Pearson's correlation).The distribution of either correlation coefficient will depend on the underlying distribution, although both are asymptotically normal because of the central limit theorem."
Amazon interview question—probability of 2nd interview,"
I got this question during an interview with Amazon:

50% of all people who receive a first interview receive a second interview
95% of your friends that got a second interview felt they had a good first interview
75% of your friends that DID NOT get a second interview felt they had a good first interview

If you feel that you had a good first interview, what is the probability you will receive a second interview?
Can someone please explain how to solve this? I'm having trouble breaking down the word problem into math (the interview is long over now). I understand there may not be an actual numerical solution, but an explanation of how you would walk through this problem would help.
edit: Well I did get a second interview. If anyone is curious I had gone with an explanation that was a combination of a bunch of the responses below: not enough info, friends not representative sample, etc and just talked through some probabilities. The question left me puzzled at the end though, thanks for all of the responses.
","['probability', 'bayesian', 'conditional-probability']","Say 200 people took the interview, so that 100 received a 2nd interview and 100 did not. Out of the first lot, 95 felt they had a great first interview. Out of the 2nd lot, 75 felt they had a great first interview. So in total 95 + 75 people felt they had a great first interview. Of those 95 + 75 = 170 people, only 95 actually got a 2nd interview. Thus the probability is:
$$\frac{95}{(95 + 75)}=\frac{95}{170}=\frac{19}{34}$$Note that, as many commenters graciously point out, this computation is only justifiable if you assume that your friends form an unbiased and well distributed sampling set, which may be a strong assumption."
Why L1 norm for sparse models,"
I am reading books about linear regression. There are some sentences about the L1 and L2 norm. I know the formulas, but I don't understand why the L1 norm enforces sparsity in models. Can someone give a simple explanation?
","['regression', 'lasso', 'regularization', 'ridge-regression']","Consider the vector $\vec{x}=(1,\varepsilon)\in\mathbb{R}^2$ where $\varepsilon>0$ is small. The $l_1$ and $l_2$ norms of $\vec{x}$, respectively, are given by$$||\vec{x}||_1 = 1+\varepsilon,\ \ ||\vec{x}||_2^2 = 1+\varepsilon^2$$Now say that, as part of some regularization procedure, we are going to reduce the magnitude of one of the elements of $\vec{x}$ by $\delta\leq\varepsilon$. If we change $x_1$ to $1-\delta$, the resulting norms are$$||\vec{x}-(\delta,0)||_1 = 1-\delta+\varepsilon,\ \ ||\vec{x}-(\delta,0)||_2^2 = 1-2\delta+\delta^2+\varepsilon^2$$On the other hand, reducing $x_2$ by $\delta$ gives norms$$||\vec{x}-(0,\delta)||_1 = 1-\delta+\varepsilon,\ \ ||\vec{x}-(0,\delta)||_2^2 = 1-2\varepsilon\delta+\delta^2+\varepsilon^2$$The thing to notice here is that, for an $l_2$ penalty, regularizing the larger term $x_1$ results in a much greater reduction in norm than doing so to the smaller term $x_2\approx 0$. For the $l_1$ penalty, however, the reduction is the same. Thus, when penalizing a model using the $l_2$ norm, it is highly unlikely that anything will ever be set to zero, since the reduction in $l_2$ norm going from $\varepsilon$ to $0$ is almost nonexistent when $\varepsilon$ is small. On the other hand, the reduction in $l_1$ norm is always equal to $\delta$, regardless of the quantity being penalized.Another way to think of it: it's not so much that $l_1$ penalties encourage sparsity, but that $l_2$ penalties in some sense discourage sparsity by yielding diminishing returns as elements are moved closer to zero."
"Why normalize images by subtracting dataset's image mean, instead of the current image mean in deep learning?","
There are some variations on how to normalize the images but most seem to use these two methods:

Subtract the mean per channel calculated over all images (e.g.  VGG_ILSVRC_16_layers)
Subtract by pixel/channel calculated over all images (e.g. CNN_S, also see Caffe's reference network)

The natural approach would in my mind to normalize each image. An image taken in broad daylight will cause more neurons to fire than a night-time image and while it may inform us of the time we usually care about more interesting features present in the edges etc. 
Pierre Sermanet refers in 3.3.3 that local contrast normalization that would be per-image based but I haven't come across this in any of the examples/tutorials that I've seen. I've also seen an interesting Quora question and Xiu-Shen Wei's post but they don't seem to support the two above approaches. 
What exactly am I missing? Is this a color normalization issue or is there a paper that actually explain why so many use this approach?
","['deep-learning', 'image-processing']","Subtracting the dataset mean serves to ""center"" the data. Additionally, you ideally would like to divide by the sttdev of that feature or pixel as well if you want to normalize each feature value to a z-score. The reason we do both of those things is because in the process of training our network, we're going to be multiplying (weights) and adding to (biases) these initial inputs in order to cause activations that we then backpropogate with the gradients to train the model. We'd like in this process for each feature to have a similar range so that our gradients don't go out of control (and that we only need one global learning rate multiplier). Another way you can think about it is deep learning networks traditionally share many parameters - if you didn't scale your inputs in a way that resulted in similarly-ranged feature values (ie: over the whole dataset by subtracting mean) sharing wouldn't happen very easily because to one part of the image weight w is a lot and to another it's too small.You will see in some CNN models that per-image whitening is used, which is more along the lines of your thinking.  "
Batch gradient descent versus stochastic gradient descent,"
Suppose we have some training set $(x_{(i)}, y_{(i)})$ for $i = 1, \dots, m$. Also suppose we run some type of supervised learning algorithm on the training set. Hypotheses are represented as $h_{\theta}(x_{(i)}) = \theta_0+\theta_{1}x_{(i)1} + \cdots +\theta_{n}x_{(i)n}$. We need to find the parameters $\mathbf{\theta}$ that minimize the ""distance"" between $y_{(i)}$ and $h_{\theta}(x_{(i)})$. Let $$J(\theta) = \frac{1}{2} \sum_{i=1}^{m} (y_{(i)}-h_{\theta}(x_{(i)}))^{2}$$
Then we want to find $\theta$ that minimizes $J(\theta)$. In gradient descent we initialize each parameter and perform the following update: $$\theta_j := \theta_j-\alpha \frac{\partial J(\theta)}{\partial \theta_{j}} $$
What is the key difference between batch gradient descent and stochastic gradient descent?
Both use the above update rule. But is one better than the other?
","['optimization', 'gradient-descent', 'stochastic-gradient-descent']",
KL divergence between two univariate Gaussians,"
I need to determine the KL-divergence between two Gaussians. I am comparing my results to these, but I can't reproduce their result. My result is obviously wrong, because the KL is not 0 for KL(p, p).
I wonder where I am doing a mistake and ask if anyone can spot it.
Let $p(x) = N(\mu_1, \sigma_1)$ and $q(x) = N(\mu_2, \sigma_2)$. From Bishop's
PRML I know that
$$KL(p, q) = - \int p(x) \log q(x) dx + \int p(x) \log p(x) dx$$
where integration is done over all real line, and that
$$\int p(x) \log p(x) dx = -\frac{1}{2} (1 + \log 2 \pi \sigma_1^2),$$
so I restrict myself to $\int p(x) \log q(x) dx$, which I can write out as
$$-\int p(x) \log \frac{1}{(2 \pi \sigma_2^2)^{(1/2)}} e^{-\frac{(x-\mu_2)^2}{2 \sigma_2^2}} dx,$$
which can be separated into
$$\frac{1}{2} \log (2 \pi \sigma_2^2) - \int p(x) \log e^{-\frac{(x-\mu_2)^2}{2 \sigma_2^2}} dx.$$
Taking the log I get
$$\frac{1}{2} \log (2 \pi \sigma_2^2) - \int p(x) \bigg(-\frac{(x-\mu_2)^2}{2 \sigma_2^2} \bigg) dx,$$
where I separate the sums and get $\sigma_2^2$ out of the integral.
$$\frac{1}{2} \log (2 \pi \sigma^2_2) + \frac{\int p(x) x^2 dx - \int p(x) 2x\mu_2 dx + \int p(x) \mu_2^2 dx}{2 \sigma_2^2}$$
Letting $\langle \rangle$ denote the expectation operator under $p$, I can rewrite this as
$$\frac{1}{2} \log (2 \pi \sigma_2^2) + \frac{\langle x^2 \rangle - 2 \langle x \rangle \mu_2 + \mu_2^2}{2 \sigma_2^2}.$$
We know that $var(x) = \langle x^2 \rangle - \langle x \rangle ^2$. Thus
$$\langle x^2 \rangle = \sigma_1^2 + \mu_1^2$$
and therefore 
$$\frac{1}{2} \log (2 \pi \sigma_2^2) + \frac{\sigma_1^2 + \mu_1^2 - 2 \mu_1 \mu_2 + \mu_2^2}{2 \sigma_2^2},$$
which I can put as
$$\frac{1}{2} \log (2 \pi \sigma_2^2) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2}.$$
Putting everything together, I get to
\begin{align*}
KL(p, q) &= - \int p(x) \log q(x) dx + \int p(x) \log p(x) dx\\\\
&= \frac{1}{2} \log (2 \pi \sigma_2^2) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2} - \frac{1}{2} (1 + \log 2 \pi \sigma_1^2)\\\\
&= \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2}.
\end{align*}
Which is wrong since it equals $1$ for two identical Gaussians.
Can anyone spot my error?
Update
Thanks to mpiktas for clearing things up. The correct answer is:
$KL(p, q) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2} - \frac{1}{2}$
","['normal-distribution', 'kullback-leibler']","OK, my bad. The error is in the last equation:\begin{align}
KL(p, q) &= - \int p(x) \log q(x) dx + \int p(x) \log p(x) dx\\\\
&=\frac{1}{2} \log (2 \pi \sigma_2^2) + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2} - \frac{1}{2} (1 + \log 2 \pi \sigma_1^2)\\\\
&= \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2} - \frac{1}{2}
\end{align}Note the missing $-\frac{1}{2}$. The last line becomes zero when $\mu_1=\mu_2$ and $\sigma_1=\sigma_2$."
Where should I place dropout layers in a neural network?,"
Is there any general guidelines on where to place dropout layers in a neural network?
","['neural-networks', 'dropout']",
Clustering on the output of t-SNE,"
I've got an application where it'd be handy to cluster a noisy dataset before looking for subgroup effects within the clusters.  I first looked at PCA, but it takes ~30 components to get to 90% of the variability, so clustering on just a couple of PC's will throw away a lot of information.  
I then tried t-SNE (for the first time), which gives me an odd shape in two dimensions that is very amenable to clustering via k-means.  What's more, running a random forest on the data with the cluster assignment as the outcome shows that the clusters have a fairly sensible interpretation given the context of the problem, in terms of the variables that make up the raw data.
But if I'm going to report on these clusters, how do I describe them?  K-means clusters on principal components reveal individuals who are nearby to one another in terms of the derived variables that comprise X% of the variance in the dataset.  What equivalent statement can be made about t-SNE clusters?
Perhaps something to the effect of:

t-SNE reveals approximate contiguity in an underlying high-dimensional manifold, so clusters on the low-dimensional representation of the high-dimensional space maximize the ""likelihood"" that contiguous individuals will not be in the same cluster

Can anyone propose a better blurb than that?
","['clustering', 'interpretation', 'k-means', 'tsne']",
What is the difference between linear regression on y with x and x with y?,"
The Pearson correlation coefficient of x and y is the same, whether you compute pearson(x, y) or pearson(y, x). This suggests that doing a linear regression of y given x or x given y should be the same, but I don't think that's the case. 
Can someone shed light on when the relationship is not symmetric, and how that relates to the Pearson correlation coefficient (which I always think of as summarizing the best fit line)?
","['regression', 'correlation', 'linear-model', 'pearson-r']","The best way to think about this is to imagine a scatterplot of points with $y$ on the vertical axis and $x$ represented by the horizontal axis.  Given this framework, you see a cloud of points, which may be vaguely circular, or may be elongated into an ellipse.  What you are trying to do in regression is find what might be called the 'line of best fit'.  However, while this seems straightforward, we need to figure out what we mean by 'best', and that means we must define what it would be for a line to be good, or for one line to be better than another, etc.  Specifically, we must stipulate a loss function.  A loss function gives us a way to say how 'bad' something is, and thus, when we minimize that, we make our line as 'good' as possible, or find the 'best' line.  Traditionally, when we conduct a regression analysis, we find estimates of the slope and intercept so as to minimize the sum of squared errors.  These are defined as follows: $$
SSE=\sum_{i=1}^N(y_i-(\hat\beta_0+\hat\beta_1x_i))^2
$$  In terms of our scatterplot, this means we are minimizing the (sum of the squared) vertical distances between the observed data points and the line.  On the other hand, it is perfectly reasonable to regress $x$ onto $y$, but in that case, we would put $x$ on the vertical axis, and so on.  If we kept our plot as is (with $x$ on the horizontal axis), regressing $x$ onto $y$ (again, using a slightly adapted version of the above equation with $x$ and $y$ switched) means that we would be minimizing the sum of the horizontal distances between the observed data points and the line.  This sounds very similar, but is not quite the same thing.  (The way to recognize this is to do it both ways, and then algebraically convert one set of parameter estimates into the terms of the other.  Comparing the first model with the rearranged version of the second model, it becomes easy to see that they are not the same.)  Note that neither way would produce the same line we would intuitively draw if someone handed us a piece of graph paper with points plotted on it.  In that case, we would draw a line straight through the center, but minimizing the vertical distance yields a line that is slightly flatter (i.e., with a shallower slope), whereas minimizing the horizontal distance yields a line that is slightly steeper.A correlation is symmetrical; $x$ is as correlated with $y$ as $y$ is with $x$.  The Pearson product-moment correlation can be understood within a regression context, however.  The correlation coefficient, $r$, is the slope of the regression line when both variables have been standardized first.  That is, you first subtracted off the mean from each observation, and then divided the differences by the standard deviation.  The cloud of data points will now be centered on the origin, and the slope would be the same whether you regressed $y$ onto $x$, or $x$ onto $y$ (but note the comment by @DilipSarwate below).  Now, why does this matter?  Using our traditional loss function, we are saying that all of the error is in only one of the variables (viz., $y$).  That is, we are saying that $x$ is measured without error and constitutes the set of values we care about, but that $y$ has sampling error.  This is very different from saying the converse.  This was important in an interesting historical episode:  In the late 70's and early 80's in the US, the case was made that there was discrimination against women in the workplace, and this was backed up with regression analyses showing that women with equal backgrounds (e.g., qualifications, experience, etc.) were paid, on average, less than men.  Critics (or just people who were extra thorough) reasoned that if this was true, women who were paid equally with men would have to be more highly qualified, but when this was checked, it was found that although the results were 'significant' when assessed the one way, they were not 'significant' when checked the other way, which threw everyone involved into a tizzy.  See here for a famous paper that tried to clear the issue up.  (Updated much later)  Here's another way to think about this that approaches the topic through the formulas instead of visually:  The formula for the slope of a simple regression line is a consequence of the loss function that has been adopted.  If you are using the standard Ordinary Least Squares loss function (noted above), you can derive the formula for the slope that you see in every intro textbook.  This formula can be presented in various forms; one of which I call the 'intuitive' formula for the slope.  Consider this form for both the situation where you are regressing $y$ on $x$, and where you are regressing $x$ on $y$:
$$
\overbrace{\hat\beta_1=\frac{\text{Cov}(x,y)}{\text{Var}(x)}}^{y\text{ on } x}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\overbrace{\hat\beta_1=\frac{\text{Cov}(y,x)}{\text{Var}(y)}}^{x\text{ on }y}
$$
Now, I hope it's obvious that these would not be the same unless $\text{Var}(x)$ equals $\text{Var}(y)$.  If the variances are equal (e.g., because you standardized the variables first), then so are the standard deviations, and thus the variances would both also equal $\text{SD}(x)\text{SD}(y)$.  In this case, $\hat\beta_1$ would equal Pearson's $r$, which is the same either way by virtue of the principle of commutativity: 
$$
\overbrace{r=\frac{\text{Cov}(x,y)}{\text{SD}(x)\text{SD}(y)}}^{\text{correlating }x\text{ with }y}~~~~~~~~~~~~~~~~~~~~~~~~~~~\overbrace{r=\frac{\text{Cov}(y,x)}{\text{SD}(y)\text{SD}(x)}}^{\text{correlating }y\text{ with }x}
$$"
Correlations with unordered categorical variables,"
I have a dataframe with many observations and many variables. Some of them are categorical (unordered) and the others are numerical.
I'm looking for associations between these variables. I've been able to compute correlation for numerical variables (Spearman's correlation) but :

I don't know how to measure correlation between unordered categorical variables.
I don't know how to measure correlation between unordered categorical variables and numerical variables.

Does anyone know how this could be done? If so, are there R functions implementing these methods?
","['r', 'correlation', 'categorical-data', 'continuous-data', 'mixed-type-data']","It depends on what sense of a correlation you want.  When you run the prototypical Pearson's product moment correlation, you get a measure of the strength of association and you get a test of the significance of that association.  More typically however, the significance test and the measure of effect size differ.  Significance tests: Effect size (strength of association):  "
What's the difference between probability and statistics?,"
What's the difference between probability and statistics, and why are they studied together?
","['probability', 'teaching', 'mathematical-statistics']",
Why does a time series have to be stationary?,"
Would like to understand primary reasons for making a data stationary?
I understand that a stationary time series is one whose mean and variance is constant over time. Can someone please explain why we have to make sure our data set is stationary before we can run different ARIMA or ARMA models on it?
Does this also apply to normal regression models where autocorrelation and/or time is not a factor?
","['regression', 'time-series', 'stationarity']",
"Why are neural networks becoming deeper, but not wider?","
In recent years, convolutional neural networks (or perhaps deep neural networks in general) have become deeper and deeper, with state-of-the-art networks going from 7 layers (AlexNet) to 1000 layers (Residual Nets) in the space of 4 years. The reason behind the boost in performance from a deeper network, is that a more complex, non-linear function can be learned. Given sufficient training data, this enables the networks to more easily discriminate between different classes.
However, the trend seems to not have followed with the number of parameters in each layer. For example, the number of feature maps in the convolutional layers, or the number of nodes in the fully-connected layers, has remained roughly the same and is still relatively small in magnitude, despite the large increase in the number of layers. From my intuition though, it would seem that increasing the number of parameters per layer would give each layer a richer source of data from which to learn its non-linear function; but this idea seems to have been overlooked in favour of simply adding more layers, each with a small number of parameters.
So whilst networks have become ""deeper"", they have not become ""wider"". Why is this?
","['machine-learning', 'classification', 'neural-networks', 'deep-learning', 'conv-neural-network']","As a disclaimer, I work on neural nets in my research, but I generally use relatively small, shallow neural nets rather than the really deep networks at the cutting edge of research you cite in your question.  I am not an expert on the quirks and peculiarities of very deep networks and I will defer to someone who is.First, in principle, there is no reason you need deep neural nets at all.  A sufficiently wide neural network with just a single hidden layer can approximate any (reasonable) function given enough training data.  There are, however, a few difficulties with using an extremely wide, shallow network.  The main issue is that these very wide, shallow networks are very good at memorization, but not so good at generalization.  So, if you train the network with every possible input value, a super wide network could eventually memorize the corresponding output value that you want.  But that's not useful because for any practical application you won't have every possible input value to train with.The advantage of multiple layers is that they can learn features at various levels of abstraction.  For example, if you train a deep convolutional neural network to classify images, you will find that the first layer will train itself to recognize very basic things like edges, the next layer will train itself to recognize collections of edges such as shapes, the next layer will train itself to recognize collections of shapes like eyes or noses, and the next layer will learn even higher-order features like faces.  Multiple layers are much better at generalizing because they learn all the intermediate features between the raw data and the high-level classification.So that explains why you might use a deep network rather than a very wide but shallow network.  But why not a very deep, very wide network?  I think the answer there is that you want your network to be as small as possible to produce good results.  As you increase the size of the network, you're really just introducing more parameters that your network needs to learn, and hence increasing the chances of overfitting.  If you build a very wide, very deep network, you run the chance of each layer just memorizing what you want the output to be, and you end up with a neural network that fails to generalize to new data.Aside from the specter of overfitting, the wider your network, the longer it will take to train.  Deep networks already can be very computationally expensive to train, so there's a strong incentive to make them wide enough that they work well, but no wider."
Should one remove highly correlated variables before doing PCA?,"
I'm reading a paper where author discards several variables due to high correlation to other variables before doing PCA. The total number of variables is around 20.
Does this give any benefits? It looks like an overhead to me as PCA should handle this automatically.
","['correlation', 'pca']","This expounds upon the insightful hint provided in a comment by @ttnphns.Adjoining nearly correlated variables increases the contribution of their common underlying factor to the PCA.  We can see this geometrically.  Consider these data in the XY plane, shown as a point cloud:There is little correlation, approximately equal covariance, and the data are centered: PCA (no matter how conducted) would report two approximately equal components.Let us now throw in a third variable $Z$ equal to $Y$ plus a tiny amount of random error.  The correlation matrix of $(X,Y,Z)$ shows this with the small off-diagonal coefficients except between the second and third rows and columns ($Y$ and $Z$):$$\left(
\begin{array}{ccc}
 1. & -0.0344018 & -0.046076 \\
 -0.0344018 & 1. & 0.941829 \\
 -0.046076 & 0.941829 & 1.
\end{array}
\right)$$Geometrically, we have displaced all the original points nearly vertically, lifting the previous picture right out of the plane of the page.  This pseudo 3D point cloud attempts to illustrate the lifting with a side perspective view (based on a different dataset, albeit generated in the same way as before):The points originally lie in the blue plane and are lifted to the red dots.  The original $Y$ axis points to the right.  The resulting tilting also stretches the points out along the YZ directions, thereby doubling their contribution to the variance.  Consequently, a PCA of these new data would still identify two major principal components, but now one of them will have twice the variance of the other.This geometric expectation is borne out with some simulations in R.  For this I repeated the ""lifting"" procedure by creating near-collinear copies of the second variable a second, third, fourth, and fifth time, naming them $X_2$ through $X_5$.  Here is a scatterplot matrix showing how those last four variables are well correlated:The PCA is done using correlations (although it doesn't really matter for these data), using the first two variables, then three, ..., and finally five.  I show the results using plots of the contributions of the principal components to the total variance.Initially, with two almost uncorrelated variables, the contributions are almost equal (upper left corner).  After adding one variable correlated with the second--exactly as  in the geometric illustration--there are still just two major components, one now twice the size of the other.  (A third component reflects the lack of perfect correlation; it measures the ""thickness"" of the pancake-like cloud in the 3D scatterplot.)  After adding another correlated variable ($X_4$), the first component is now about three-fourths of the total; after a fifth is added, the first component is nearly four-fifths of the total.  In all four cases components after the second would likely be considered inconsequential by most PCA diagnostic procedures; in the last case it's possible some procedures would conclude there is only one principal component worth considering.We can see now that there may be merit in discarding variables thought to be measuring the same underlying (but ""latent"") aspect of a collection of variables, because including the nearly-redundant variables can cause the PCA to overemphasize their contribution.  There is nothing mathematically right (or wrong) about such a procedure; it's a judgment call based on the analytical objectives and knowledge of the data.  But it should be abundantly clear that setting aside variables known to be strongly correlated with others can have a substantial effect on the PCA results.Here is the R code."
What is the .632+ rule in bootstrapping?,"
Here @gung makes reference to the .632+ rule.  A quick Google search doesn't yield an easy to understand answer as to what this rule means and for what purpose it is used.  Would someone please elucidate the .632+ rule?
",['bootstrap'],"I will get to the 0.632 estimator, but it'll be a somewhat long development:Suppose we want to predict $Y$ with $X$ using the function $f$, where $f$ may depend on some parameters that are estimated using the data $(\mathbf{Y}, \mathbf{X})$, e.g. $f(\mathbf{X}) = \mathbf{X}\mathbf{\beta}$ A naïve estimate of prediction error is $$\overline{err} = \dfrac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))$$ where $L$ is some loss function, e.g. squared error loss. This is often called training error. Efron et al. calls it apparent error rate or resubstitution rate. It's not very good since we use our data $(x_i,y_i)$ to fit $f$. This results in $\overline{err}$ being downward biased. You want to know how well your model $f$ does in predicting new values. Often we use cross-validation as a simple way to estimate the expected extra-sample prediction error (how well does our model do on data not in our training set?). $$Err = \text{E}\left[ L(Y, f(X))\right]$$ A popular way to do this is to do $K$-fold cross-validation. Split your data into $K$ groups (e.g. 10). For each group $k$, fit your model on the remaining $K-1$ groups and test it on the $k$th group. Our cross-validated extra-sample prediction error is just the average $$Err_{CV} = \dfrac{1}{N}\sum_{i=1}^N L(y_i, f_{-\kappa(i)}(x_i))$$ where $\kappa$ is some index function that indicates the partition to which observation $i$ is allocated and $f_{-\kappa(i)}(x_i)$ is the predicted value of $x_i$ using data not in the $\kappa(i)$th set. This estimator is approximately unbiased for the true prediction error when $K=N$ and has larger variance and is more computationally expensive for larger $K$. So once again we see the bias–variance trade-off at play.Instead of cross-validation we could use the bootstrap to estimate the extra-sample prediction error. Bootstrap resampling can be used to estimate the sampling distribution of any statistic. If our training data is $\mathbf{X} = (x_1,\ldots,x_N)$, then we can think of taking $B$ bootstrap samples (with replacement) from this set $\mathbf{Z}_1,\ldots,\mathbf{Z}_B$ where each $\mathbf{Z}_i$ is a set of $N$ samples. Now we can use our bootstrap samples to estimate extra-sample prediction error: $$Err_{boot} = \dfrac{1}{B}\sum_{b=1}^B\dfrac{1}{N}\sum_{i=1}^N L(y_i, f_b(x_i))$$ where $f_b(x_i)$ is the predicted value at $x_i$ from the model fit to the $b$th bootstrap dataset. Unfortunately, this is not a particularly good estimator because bootstrap samples used to produce $f_b(x_i)$ may have contained $x_i$. The leave-one-out bootstrap estimator offers an improvement by mimicking cross-validation and is defined as: $$Err_{boot(1)} = \dfrac{1}{N}\sum_{i=1}^N\dfrac{1}{|C^{-i}|}\sum_{b\in C^{-i}}L(y_i,f_b(x_i))$$ where $C^{-i}$ is the set of indices for the bootstrap samples that do not contain observation $i$, and $|C^{-i}|$ is the number of such samples. $Err_{boot(1)}$ solves the overfitting problem, but is still biased (this one is upward biased). The bias is due to non-distinct observations in the bootstrap samples that result from sampling with replacement. The average number of distinct observations in each sample is about $0.632N$ (see this answer for an explanation of why Why on average does each bootstrap sample contain roughly two thirds of observations?). To solve the bias problem, Efron and Tibshirani proposed the 0.632 estimator:
$$ Err_{.632} = 0.368\overline{err} + 0.632Err_{boot(1)}$$ where $$\overline{err} = \dfrac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))$$ is the naïve estimate of prediction error often called training error. The idea is to average a downward biased estimate and an upward biased estimate. However, if we have a highly overfit prediction function (i.e. $\overline{err}=0$) then even the .632 estimator will be downward biased. The .632+ estimator is designed to be a less-biased compromise between $\overline{err}$ and $Err_{boot(1)}$. 
$$
Err_{.632+} = (1 - w) \overline{err} + w Err_{boot(1)}
$$
with 
$$w = \dfrac{0.632}{1 - 0.368R} \quad\text{and}\quad R = \dfrac{Err_{boot(1)} - \overline{err}}{\gamma - \overline{err}}
$$
where $\gamma$ is the no-information error rate, estimated by evaluating the prediction model on all possible combinations of targets $y_i$ and predictors $x_i$. $$\gamma = \dfrac{1}{N^2}\sum_{i=1}^N\sum_{j=1}^N L(y_i, f(x_j))$$.Here $R$ measures the relative overfitting rate. If there is no overfitting (R=0, when the $Err_{boot(1)} = \overline{err}$) this is equal to the .632 estimator. "
Is Facebook coming to an end?,"
Recently, this paper has received a lot of attention (e.g. from WSJ). Basically, the authors conclude that Facebook will lose 80% of its members by 2017. 
They base their claims on an extrapolation of the SIR model, a compartmental model frequently used in epidemiology. Their data is drawn from Google searches for ""Facebook"", and the authors use the demise of Myspace to validate their conclusion.
Question:
Are the authors making a ""correlation does not imply causation"" mistake? This model and logic may have worked for Myspace, but is it valid for any social network?
Update: Facebook hits back

In keeping with the scientific principle ""correlation equals causation,"" our research unequivocally demonstrated that Princeton may be in danger of disappearing entirely.
We don’t really think Princeton or the world’s air supply is going anywhere soon. We love Princeton (and air),” and adding a final reminder that “not all research is created equal – and some methods of analysis lead to pretty crazy conclusions.

","['hypothesis-testing', 'correlation', 'epidemiology', 'social-network', 'compartmental-models']","The answers so far have focused on the data itself, which makes sense with the site this is on, and the flaws about it.But I'm a computational/mathematical epidemiologist by inclination, so I'm also going to talk about the model itself for a little bit, because it's also relevant to the discussion.In my mind, the biggest problem with the paper is not the Google data. Mathematical models in epidemiology handle messy data all the time, and to my mind the problems with it could be addressed with a fairly straightforward sensitivity analysis.The biggest problem, to me, is that the researchers have ""doomed themselves to success"" — something that should always be avoided in research. They do this in the model they decided to fit to the data: a standard SIR model.Briefly, a SIR model (which stands for susceptible (S) infectious (I) recovered (R)) is a series of differential equations that track the health states of a population as it experiences an infectious disease. Infected individuals interact with susceptible individuals and infect them, and then in time move on to the recovered category.This produces a curve that looks like this:Beautiful, is it not? And yes, this one is for a zombie epidemic. Long story.In this case, the red line is what's being modeled as ""Facebook users"". The problem is this:In the basic SIR model, the I class will eventually, and inevitably, asymptotically approach zero.It must happen. It doesn't matter if you're modeling zombies, measles, Facebook, or Stack Exchange, etc. If you model it with a SIR model, the inevitable conclusion is that the population in the infectious (I) class drops to approximately zero.There are extremely straightforward extensions to the SIR model that make this not true — either you can have people in the recovered (R) class come back to susceptible (S) (essentially, this would be people who left Facebook changing from ""I'm never going back"" to ""I might go back someday""), or you can have new people come into the population (this would be little Timmy and Claire getting their first computers).Unfortunately, the authors didn't fit those models. This is, incidentally, a widespread problem in mathematical modeling. A statistical model is an attempt to describe the patterns of variables and their interactions within the data. A mathematical model is an assertion about reality. You can get a SIR model to fit lots of things, but your choice of a SIR model is also an assertion about the system. Namely, that once it peaks, it's heading to zero.Incidentally, Internet companies do use user-retention models that look a heck of a lot like epidemic models, but they're also considerably more complex than the one presented in the paper."
Removal of statistically significant intercept term increases $R^2$ in linear model,"
In a simple linear model with a single explanatory variable,
$\alpha_i = \beta_0 + \beta_1 \delta_i + \epsilon_i$
I find that removing the intercept term improves the fit greatly (value of $R^2$ goes from 0.3 to 0.9). However, the intercept term appears to be statistically significant.
With intercept:

Call:
lm(formula = alpha ~ delta, data = cf)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.72138 -0.15619 -0.03744  0.14189  0.70305 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.48408    0.05397    8.97   <2e-16 ***
delta        0.46112    0.04595   10.04   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.2435 on 218 degrees of freedom
Multiple R-squared: 0.316,    Adjusted R-squared: 0.3129 
F-statistic: 100.7 on 1 and 218 DF,  p-value: < 2.2e-16


Without intercept:

Call:
lm(formula = alpha ~ 0 + delta, data = cf)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.92474 -0.15021  0.05114  0.21078  0.85480 

Coefficients:
      Estimate Std. Error t value Pr(>|t|)    
delta  0.85374    0.01632   52.33   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.2842 on 219 degrees of freedom
Multiple R-squared: 0.9259,   Adjusted R-squared: 0.9256 
F-statistic:  2738 on 1 and 219 DF,  p-value: < 2.2e-16


How would you interpret these results? Should an intercept term be included in the model or not?
Edit
Here's the residual sums of squares:
RSS(with intercept) = 12.92305
RSS(without intercept) = 17.69277

","['r', 'linear-model', 'interpretation', 'r-squared', 'intercept']","First of all, we should understand what the R software is doing when no intercept
is included in the model. Recall that the usual computation of $R^2$
when an intercept is present is
$$
R^2 = \frac{\sum_i (\hat y_i - \bar y)^2}{\sum_i (y_i - \bar
y)^2} = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar
y)^2} \>.
$$
The first equality only occurs because of the inclusion of the
intercept in the model even though this is probably the more popular
of the two ways of writing it. The second equality actually provides
the more general interpretation! This point is also address in this
related question.But, what happens if there is no intercept in the model? Well, in that
case, R (silently!) uses the modified form
$$
R_0^2 = \frac{\sum_i \hat y_i^2}{\sum_i y_i^2} = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i y_i^2} \>.
$$It helps to recall what $R^2$ is trying to measure. In the former
case, it is comparing your current model to the reference
model that only includes an intercept (i.e., constant term). In the
second case, there is no intercept, so it makes little sense to
compare it to such a model. So, instead, $R_0^2$ is computed, which
implicitly uses a reference model corresponding to noise only.In what follows below, I focus on the second expression for both $R^2$ and $R_0^2$ since that expression generalizes to other contexts and it's generally more natural to think about things in terms of residuals.But, how are they different, and when?Let's take a brief digression into some linear algebra and see if we
can figure out what is going on. First of all, let's call the fitted
values from the model with intercept $\newcommand{\yhat}{\hat
{\mathbf y}}\newcommand{\ytilde}{\tilde {\mathbf y}}\yhat$ and the
fitted values from the model without intercept $\ytilde$.  We can rewrite
the expressions for $R^2$ and $R_0^2$ as 
$$\newcommand{\y}{\mathbf y}\newcommand{\one}{\mathbf 1} 
R^2 = 1 - \frac{\|\y - \yhat\|_2^2}{\|\y - \bar y \one\|_2^2} \>, 
$$
and
$$
R_0^2 = 1 - \frac{\|\y - \ytilde\|_2^2}{\|\y\|_2^2} \>,
$$
respectively.Now, since $\|\y\|_2^2 = \|\y - \bar y \one\|_2^2 + n \bar y^2$, then $R_0^2 > R^2$ if and only if
$$
\frac{\|\y - \ytilde\|_2^2}{\|\y - \yhat\|_2^2} < 1 + \frac{\bar
y^2}{\frac{1}{n}\|\y - \bar y \one\|_2^2} \> .
$$The left-hand side is greater than one since the model corresponding
to $\ytilde$ is nested within that of $\yhat$. The second term on the
right-hand side is the squared-mean of the responses divided by the
mean square error of an intercept-only model. So, the larger the mean of the response relative to the other variation, the more ""slack"" we have and a greater chance of $R_0^2$ dominating $R^2$.Notice that all the
model-dependent stuff is on the left side and non-model dependent
stuff is on the right. Ok, so how do we make the ratio on the left-hand side small?Recall that
$\newcommand{\P}{\mathbf P}\ytilde = \P_0 \y$ and $\yhat = \P_1 \y$ where $\P_0$ and $\P_1$ are
projection matrices corresponding to subspaces $S_0$ and $S_1$ such
that $S_0 \subset S_1$.So, in order for the ratio to be close to one, we need the subspaces
$S_0$ and $S_1$ to be very similar. Now $S_0$ and $S_1$ differ only by
whether $\one$ is a basis vector or not, so that means that $S_0$
had better be a subspace that already lies very close to $\one$. In essence, that means our predictor had better have a strong mean
offset itself and that this mean offset should dominate the variation
of the predictor.An exampleHere we try to generate an example with an intercept explicitly in the model and which behaves close to the case in the question. Below is some simple R code to demonstrate.This gives the following output. We begin with the model with intercept.Then, see what happens when we exclude the intercept.Below is a plot of the data with the model-with-intercept in red and the model-without-intercept in blue."
Why does the Cauchy distribution have no mean?,"
From the distribution density function we could identify a mean (=0) for Cauchy distribution just like the graph below shows. But why do we say Cauchy distribution has no mean?

","['distributions', 'mathematical-statistics', 'mean', 'density-function', 'cauchy-distribution']","You can mechanically check that the expected value does not exist, but this should be physically intuitive, at least if you accept Huygens' principle and the Law of Large Numbers. The conclusion of the Law of Large Numbers fails for a Cauchy distribution, so it can't have a mean. If you average $n$ independent Cauchy random variables, the result does not converge to $0$ as $n\to \infty$ with probability $1$. It stays a Cauchy distribution of the same size. This is important in optics. The Cauchy distribution is the normalized intensity of light on a line from a point source. Huygens' principle says that you can determine the intensity by assuming that the light is re-emitted from any line between the source and the target. So, the intensity of light on a line $2$ meters away can be determined by assuming that the light first hits a line $1$ meter away, and is re-emitted at any forward angle. The intensity of light on a line $n$ meters away can be expressed as the $n$-fold convolution of the distribution of light on a line $1$ meter away. That is, the sum of $n$ independent Cauchy distributions is a Cauchy distribution scaled by a factor of $n$. If the Cauchy distribution had a mean, then the $25$th percentile of the $n$-fold convolution divided by $n$ would have to converge to $0$ by the Law of Large Numbers. Instead it stays constant. If you mark the $25$th percentile on a (transparent) line $1$ meter away, $2$ meters away, etc. then these points form a straight line, at $45$ degrees. They don't bend toward $0$.This tells you about the Cauchy distribution in particular, but you should know the integral test because there are other distributions with no mean which don't have a clear physical interpretation. "
Difference between neural net weight decay and learning rate,"
In the context of neural networks, what is the difference between the learning rate and weight decay? 
","['neural-networks', 'terminology']","The learning rate is a parameter that determines how much an updating step influences the current value of the weights. While weight decay is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled.So let's say that we have a cost or error function $E(\mathbf{w})$ that we want to minimize. Gradient descent tells us to modify the weights $\mathbf{w}$ in the direction of steepest descent in $E$:
\begin{equation}
w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i},
\end{equation}
where $\eta$ is the learning rate, and if it's large you will have a correspondingly large modification of the weights $w_i$ (in general it shouldn't be too large, otherwise you'll overshoot the local minimum in your cost function).In order to effectively limit the number of free parameters in your model so as to avoid over-fitting, it is possible to regularize the cost function. An easy way to do that is by introducing a zero mean Gaussian prior over the weights, which is equivalent to changing the cost function to $\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\frac{\lambda}{2}\mathbf{w}^2$. In practice this penalizes large weights and effectively limits the freedom in your model. The regularization parameter $\lambda$ determines how you trade off the original cost $E$ with the large weights penalization.Applying gradient descent to this new cost function we obtain:
\begin{equation}
w_i \leftarrow w_i-\eta\frac{\partial E}{\partial w_i}-\eta\lambda w_i.
\end{equation}
The new term $-\eta\lambda w_i$ coming from the regularization causes the weight to decay in proportion to its size."
Obtaining knowledge from a random forest,"
Random forests are considered to be black boxes, but recently I was thinking what knowledge can be obtained from a random forest?
The most obvious thing is the importance of the variables, in the simplest variant it can be done just by calculating the number of occurrences of a variable.
The second thing I was thinking about are interactions. I think that if the number of trees is sufficiently large then the number of occurrences of pairs of variables can be tested (something like chi square independence). 
The third thing are nonlinearities of variables. My first idea was just to look at a chart of a variable  Vs score, but I'm not sure yet whether it makes any sense.  
Added 23.01.2012
Motivation 
I want to use this knowledge to improve a logit model. I think (or at least I hope) that it is possible to find interactions and nonlinearities that were overlooked.
","['machine-learning', 'data-mining', 'interaction', 'random-forest', 'cart']","Random Forests are hardly a black box.  They are based on decision trees, which are very easy to interpret:This results in a simple decision tree:If Petal.Length < 4.95, this tree classifies the observation as ""other.""  If it's greater than 4.95, it classifies the observation as ""virginica.""  A random forest is simple a collection of many such trees, where each one is trained on a random subset of the data.  Each tree then ""votes"" on the final classification of each observation.You can even pull out individual trees from the rf, and look at their structure.  The format is slightly different than for rpart models, but you could inspect each tree if you wanted and see how it's modeling the data.Furthermore, no model is truly a black box, because you can examine predicted responses vs actual responses for each variable in the dataset.  This is a good idea regardless of what sort of model you are building:I've normalized the variables (sepal and petal length and width) to a 0-1 range.  The response is also 0-1, where 0 is other and 1 is virginica.  As you can see the random forest is a good model, even on the test set.Additionally, a random forest will compute various measure of variable importance, which can be very informative:This table represents how much removing each variable reduces the accuracy of the model.  Finally, there are many other plots you can make from a random forest model, to view what's going on in the black box:You can view the help files for each of these functions to get a better idea of what they display."
What is the difference between linear regression and logistic regression?,"
What is the difference between linear regression and logistic regression?
When would you use each?
","['regression', 'logistic', 'linear-model']","Linear regression uses the general linear equation $Y=b_0+∑(b_i X_i)+\epsilon$ where $Y$ is a continuous dependent variable and independent variables $X_i$ are usually continuous (but can also be binary, e.g. when the linear model is used in a t-test) or other discrete domains. $\epsilon$ is a term for the variance that is not explained by the model and is usually just called ""error"". Individual dependent values denoted by $Y_j$ can be solved by modifying the equation a little: $Y_j=b_0 + \sum{(b_i X_{ij})+\epsilon_j}$Logistic regression is another generalized linear model (GLM) procedure using the same basic formula, but instead of the continuous $Y$, it is regressing for the probability of a categorical outcome. In simplest form, this means that we're considering just one outcome variable and two states of that variable- either 0 or 1.The equation for the probability of $Y=1$ looks like this:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$Your independent variables $X_i$ can be continuous or binary. The regression coefficients $b_i$ can be exponentiated to give you the change in odds of $Y$ per change in $X_i$, i.e., $Odds={P(Y=1) \over P(Y=0)}={P(Y=1) \over 1-P(Y=1)}$ and ${\Delta Odds}=  e^{b_i}$.  $\Delta Odds$ is called the odds ratio, $Odds(X_i+1)\over Odds(X_i)$. In English, you can say that the odds of $Y=1$ increase by a factor of $e^{b_i}$ per unit change in $X_i$.Example: If you wanted to see how body mass index predicts blood cholesterol (a continuous measure), you'd use linear regression as described at the top of my answer. If you wanted to see how BMI predicts the odds of being a diabetic (a binary diagnosis), you'd use logistic regression."
Books for self-studying time series analysis?,"
I started by Time Series Analysis by Hamilton, but I am lost hopelessly. This book is really too theoretical for me to learn by myself. 
Does anybody have a recommendation for a textbook on time series analysis that's suitable for self-study? 
","['time-series', 'self-study', 'references']",I would recommed the following books:I hope it helps you. Best of luck!
"What if residuals are normally distributed, but y is not?","
I've got a weird question. Assume that you have a small sample where the dependent variable that you're going to analyze with a simple linear model is highly left skewed. Thus you assume that $u$ is not normally distributed, because this would result in normally distributed $y$. But when you compute the QQ-Normal plot there is evidence, that the residuals are normally distributed. Thus anyone can assume that the error term is normally distributed, although $y$ is not. So what does it mean, when the error term seems to be normally distributed, but $y$ does not?
","['regression', 'residuals', 'error', 'normality-assumption']","It is reasonable for the residuals in a regression problem to be normally distributed, even though the response variable is not.  Consider a univariate regression problem where $y \sim \mathcal{N}(\beta x, \sigma^2)$. so that the regression model is appropriate, and further assume that the true value of $\beta=1$.  In this case, while the residuals of the true regression model are normal, the distribution of $y$ depends on the distribution of $x$, as the conditional mean of $y$ is a function of $x$.  If the dataset has a lot of values of $x$ that are close to zero and progressively fewer the higher the value of $x$, then the distribution of $y$ will be skewed to the right.  If values of $x$ are distributed symmetrically, then $y$ will be distributed symmetrically, and so forth.  For a regression problem, we only assume that the response is normal conditioned on the value of $x$."
"What is the difference between convolutional neural networks, restricted Boltzmann machines, and auto-encoders?","
Recently I have been reading about deep learning and I am confused about the terms (or say technologies). What is the difference between 

Convolutional neural networks (CNN),
Restricted Boltzmann machines (RBM) and 
Auto-encoders?

","['neural-networks', 'deep-learning', 'conv-neural-network', 'autoencoders', 'restricted-boltzmann-machine']","Autoencoder is a simple 3-layer neural network where output units are directly connected back to input units. E.g. in a network like this:output[i] has edge back to input[i] for every i. Typically, number of hidden units is much less then number of visible (input/output) ones. As a result, when you pass data through such a network, it first compresses (encodes) input vector to ""fit"" in a smaller representation, and then tries to reconstruct (decode) it back. The task of training is to minimize an error or reconstruction, i.e. find the most efficient compact representation (encoding) for input data. RBM shares similar idea, but uses stochastic approach. Instead of deterministic (e.g. logistic or ReLU) it uses stochastic units with particular (usually binary of Gaussian) distribution. Learning procedure consists of several steps of Gibbs sampling (propagate: sample hiddens given visibles; reconstruct: sample visibles given hiddens; repeat) and adjusting the weights to minimize reconstruction error. Intuition behind RBMs is that there are some visible random variables (e.g. film reviews from different users) and some hidden variables (like film genres or other internal features), and the task of training is to find out how these two sets of variables are actually connected to each other (more on this example may be found here). Convolutional Neural Networks are somewhat similar to these two, but instead of learning single global weight matrix between two layers, they aim to find a set of locally connected neurons. CNNs are mostly used in image recognition. Their name comes from ""convolution"" operator or simply ""filter"". In short, filters are an easy way to perform complex operation by means of simple change of a convolution kernel. Apply Gaussian blur kernel and you'll get it smoothed. Apply Canny kernel and you'll see all edges. Apply Gabor kernel to get gradient features. (image from here)The goal of convolutional neural networks is not to use one of predefined kernels, but instead to learn data-specific kernels.  The idea is the same as with autoencoders or RBMs - translate many low-level features (e.g. user reviews or image pixels) to the compressed high-level representation (e.g. film genres or edges) - but now weights are learned only from neurons that are spatially close to each other.  All three models have their use cases, pros and cons, but probably the most important properties are: UPD. Dimensionality reductionWhen we represent some object as a vector of $n$ elements, we say that this is a vector in $n$-dimensional space. Thus, dimensionality reduction refers to a process of refining data in such a way, that each data vector $x$ is translated into another vector $x'$ in an $m$-dimensional space (vector with $m$ elements), where $m < n$. Probably the most common way of doing this is PCA. Roughly speaking, PCA finds ""internal axes"" of a dataset (called ""components"") and sorts them by their importance. First $m$ most important components are then used as new basis. Each of these components may be thought of as a high-level feature, describing data vectors better than original axes. Both - autoencoders and RBMs - do the same thing. Taking a vector in $n$-dimensional space they translate it into an $m$-dimensional one, trying to keep as much important information as possible and, at the same time, remove noise. If training of autoencoder/RBM was successful, each element of resulting vector (i.e. each hidden unit) represents something important about the object - shape of an eyebrow in an image, genre of a film, field of study in scientific article, etc. You take lots of noisy data as an input and produce much less data in a much more efficient representation. Deep architecturesSo, if we already had PCA, why the hell did we come up with autoencoders and RBMs? It turns out that PCA only allows linear transformation of a data vectors. That is, having $m$ principal components $c_1..c_m$, you can represent only vectors $x=\sum_{i=1}^{m}w_ic_i$. This is pretty good already, but not always enough. No matter, how many times you will apply PCA to a data - relationship will always stay linear. Autoencoders and RBMs, on other hand, are non-linear by the nature, and thus, they can learn more complicated relations between visible and hidden units. Moreover, they can be stacked, which makes them even more powerful. E.g. you train RBM with $n$ visible and $m$ hidden units, then you put another RBM with $m$ visible and $k$ hidden units on top of the first one and train it too, etc. And exactly the same way with autoencoders. But you don't just add new layers. On each layer you try to learn best possible representation for a data from the previous one: On the image above there's an example of such a deep network. We start with ordinary pixels, proceed with simple filters, then with face elements and finally end up with entire faces! This is the essence of deep learning. Now note, that at this example we worked with image data and sequentially took
larger and larger areas of spatially close pixels. Doesn't it sound similar? Yes, because it's an example of deep convolutional network. Be it based on autoencoders or RBMs, it uses convolution to stress importance of locality. That's why CNNs are somewhat distinct from autoencoders and RBMs. ClassificationNone of models mentioned here work as classification algorithms per se. Instead, they are used for pretraining - learning transformations from low-level and hard-to-consume representation (like pixels) to a high-level one. Once deep (or maybe not that deep) network is pretrained, input vectors are transformed to a better representation and resulting vectors are finally passed to real classifier (such as SVM or logistic regression). In an image above it means that at the very bottom there's one more component that actually does classification. "
Is it possible to have a pair of Gaussian random variables for which the joint distribution is not Gaussian?,"
Somebody asked me this question in a job interview and I replied that their joint distribution is always Gaussian. I thought that I can always write a bivariate Gaussian with their means and variance and covariances. I am wondering if there can be a case for which the joint probability of two Gaussians is not Gaussian?
","['normal-distribution', 'multivariate-analysis', 'copula', 'bivariate']","The bivariate normal distribution is the exception, not the rule!It is important to recognize that ""almost all"" joint distributions with normal marginals are not the bivariate normal distribution. That is, the common viewpoint that joint distributions with normal marginals that are not the bivariate normal are somehow ""pathological"", is a bit misguided.Certainly, the multivariate normal is extremely important due to its stability under linear transformations, and so receives the bulk of attention in applications.ExamplesIt is useful to start with some examples. The figure below contains heatmaps of six bivariate distributions, all of which have standard normal marginals. The left and middle ones in the top row are bivariate normals, the remaining ones are not (as should be apparent). They're described further below.The bare bones of copulasProperties of dependence are often efficiently analyzed using copulas. A bivariate copula is just a fancy name for a probability distribution on the unit square $[0,1]^2$ with uniform marginals.Suppose $C(u,v)$ is a bivariate copula. Then, immediately from the above, we know that $C(u,v) \geq 0$, $C(u,1) = u$ and $C(1,v) = v$, for example. We can construct bivariate random variables on the Euclidean plane with prespecified marginals by a simple transformation of a bivariate copula. Let $F_1$ and $F_2$ be prescribed marginal distributions for a pair of random variables $(X,Y)$. Then, if $C(u,v)$ is a bivariate copula,
$$
F(x,y) = C(F_1(x), F_2(y))
$$
is a bivariate distribution function with marginals $F_1$ and $F_2$. To see this last fact, just note that
$$
\renewcommand{\Pr}{\mathbb P}
\Pr(X \leq x) = \Pr(X \leq x, Y < \infty) = C(F_1(x), F_2(\infty)) = C(F_1(x),1) = F_1(x) \>.
$$
The same argument works for $F_2$.For continuous $F_1$ and $F_2$, Sklar's theorem asserts a converse implying uniqueness. That is, given a bivariate distribution $F(x,y)$ with continuous marginals $F_1$, $F_2$, the corresponding copula is unique (on the appropriate range space).The bivariate normal is exceptionalSklar's theorem tells us (essentially) that there is only one copula that produces the bivariate normal distribution. This is, aptly named, the Gaussian copula which has density on $[0,1]^2$
$$
c_\rho(u,v) := \frac{\partial^2}{\partial u \, \partial v} C_\rho(u,v) = \frac{\varphi_{2,\rho}(\Phi^{-1}(u),\Phi^{-1}(v))}{\varphi(\Phi^{-1}(u)) \varphi(\Phi^{-1}(v))} \>,
$$
where the numerator is the bivariate normal distribution with correlation $\rho$ evaluated at $\Phi^{-1}(u)$ and $\Phi^{-1}(v)$.But, there are lots of other copulas and all of them will give a bivariate distribution with normal marginals which is not the bivariate normal by using the transformation described in the previous section.Some details on the examplesNote that if $C(u,v)$ is am arbitrary copula with density $c(u,v)$, the corresponding bivariate density with standard normal marginals under the transformation $F(x,y) = C(\Phi(x),\Phi(y))$ is
$$
f(x,y) = \varphi(x) \varphi(y) c(\Phi(x), \Phi(y)) \> .
$$Note that by applying the Gaussian copula in the above equation, we recover the bivariate normal density. But, for any other choice of $c(u,v)$, we will not.The examples in the figure were constructed as follows (going across each row, one column at a time):"
Nested cross validation for model selection,"
How can one use nested cross validation for model selection? 
From what I read online, nested CV works as follows:

There is the inner CV loop, where we may conduct a grid search (e.g. running K-fold for every available model, e.g. combination of hyperparameters/features)
There is the outer CV loop, where we measure the performance of the model that won in the inner fold, on a separate external fold. 

At the end of this process we end up with $K$ models ($K$ being the number of folds in the outer loop). These models are the ones that won in the grid search within the inner CV, and they are likely different (e.g. SVMs with different kernels, trained with possibly different features, depending on the grid search).
How do I choose a model from this output? It looks to me that selecting the best model out of those $K$ winning models would not be a fair comparison since each model was trained and tested on different parts of the dataset.
So how can I use nested  CV for model selection? 
Also I have read threads discussing how nested model selection is useful for analyzing the learning procedure. What types of analysis /checks can I do with the scores that I get from the outer K folds? 
","['cross-validation', 'model-selection']","How do I choose a model from this [outer cross validation] output?  Short answer: You don't. Treat the inner cross validation as part of the model fitting procedure. That means that the fitting including the fitting of the hyper-parameters (this is where the inner cross validation hides) is just like any other model esitmation routine.
The outer cross validation estimates the performance of this model fitting approach. For that you use the usual assumptionsDo not pick the seemingly best of the $k$ surrogate models - that would usually be just ""harvesting"" testing uncertainty and leads to an optimistic bias.So how can I use nested CV for model selection? The inner CV does the selection.It looks to me that selecting the best model out of those K winning models would not be a fair comparison since each model was trained and tested on different parts of the dataset.You are right in that it is no good idea to pick one of the $k$ surrogate models. But you are wrong about the reason. Real reason: see above. The fact that they are not trained and tested on the same data does not ""hurt"" here. Which brings me to your last question: What types of analysis /checks can I do with the scores that I get from the outer K folds?check for the stability/variation of the optimized hyper-parameters.
For one thing, wildly scattering hyper-parameters may indicate that the inner optimization didn't work. For another thing, this may allow you to decide on the hyperparameters without the costly optimization step in similar situations in the future. With costly I do not refer to computational resources but to the fact that this ""costs"" information that may better be used for estimating the ""normal"" model parameters.check for the difference between the inner and outer estimate of the chosen model. If there is a large difference (the inner being very overoptimistic), there is a risk that the inner optimization didn't work well because of overfitting. First of all, detecting in the outer CV loop that the models do not yield stable predictions in that respect doesn't really differ from detecting that the prediciton error is too high for the application. It is one of the possible outcomes of model validation (or verification) implying that the model we have is not fit for its purpose. In the comment answering @davips, I was thinking of tackling the instability in the inner CV - i.e. as part of the model optimization process. But you are certainly right: if we change our model based on the findings of the outer CV, yet another round of independent testing of the changed model is necessary.
However, instability in the outer CV would also be a sign that the optimization wasn't set up well - so finding instability in the outer CV implies that the inner CV did not penalize instability in the necessary fashion - this would be my main point of critique in such a situation. In other words, why does the optimization allow/lead to heavily overfit models?However, there is one peculiarity here that IMHO may excuse the further change of the ""final"" model after careful consideration of the exact circumstances: As we did detect overfitting, any proposed change (fewer d.f./more restrictive or aggregation) to the model would be in direction of less overfitting (or at least hyperparameters that are less prone to overfitting). The point of independent testing is to detect overfitting - underfitting can be detected by data that was already used in the training process. So if we are talking, say, about further reducing the number of latent variables in a PLS model that would be comparably benign (if the proposed change would be a totally different type of model, say PLS instead of SVM, all bets would be off), and I'd be even more relaxed about it if I'd know that we are anyways in an intermediate stage of modeling - after all, if the optimized models are still unstable, there's no question that more cases are needed. Also, in many situations, you'll eventually need to perform studies that are designed to properly test various aspects of performance (e.g. generalization to data acquired in the future). 
Still, I'd insist that the full modeling process would need to be reported, and that the implications of these late changes would need to be carefully discussed. Also, aggregation including and out-of-bag analogue CV estimate of performance would be possible from the already available results - which is the other type of ""post-processing"" of the model that I'd be willing to consider benign here. Yet again, it then would have been better if the study were designed from the beginning to check that aggregation provides no advantage over individual predcitions (which is another way of saying that the individual models are stable).Update (2019): the more I think about these situations, the more I come to favor the ""nested cross validation apparently without nesting"" approach. "
How to choose between t-test or non-parametric test e.g. Wilcoxon in small samples,"
Certain hypotheses can be tested using Student's t-test (maybe using Welch's correction for unequal variances in the two-sample case), or by a non-parametric test like the Wilcoxon paired signed rank test, the Wilcoxon-Mann-Whitney U test, or the paired sign test. How can we make a principled decision about which test is most appropriate, particularly if the sample size is ""small""?
Many introductory textbooks and lecture notes give a ""flowchart"" approach where normality is checked (either – inadvisedly – by normality test, or more broadly by QQ plot or similar) to decide between a t-test or non-parametric test. For the unpaired two-sample t-test there may be a further check for homogeneity of variance to decide whether to apply Welch's correction. One issue with this approach is the way the decision on which test to apply depends on the observed data, and how this affects the performance (power, Type I error rate) of the selected test.
Another problem is how hard checking normality is in small data sets: formal testing has low power so violations may well not be detected, but similar issues apply eyeballing the data on a QQ plot. Even egregious violations could go undetected, e.g. if the distribution is mixed but no observations were drawn from one component of the mixture. Unlike for large $n$, we can't lean on the safety-net of the Central Limit Theorem, and the asymptotic normality of the test statistic and t distribution.
One principled response to this is ""safety first"": with no way to reliably verify the normality assumption in a small sample, stick to non-parametric methods.  Another is to consider any grounds for assuming normality, theoretically (e.g. variable is sum of several random components and CLT applies) or empirically (e.g. previous studies with larger $n$ suggest variable is normal), and using a t-test only if such grounds exist. But this usually only justifies approximate normality, and on low degrees of freedom it's hard to judge how near normal it needs to be to avoid invalidating a t-test.
Most guides to choosing a t-test or non-parametric test focus on the normality issue. But small samples also throw up some side-issues:

If performing an ""unrelated samples"" or ""unpaired"" t-test, whether to use a Welch correction? Some people use a hypothesis test for equality of variances, but here it would have low power; others check whether SDs are ""reasonably"" close or not (by various criteria). Is it safer simply to always use the Welch correction for small samples, unless there is some good reason to believe population variances are equal?
If you see the choice of methods as a trade-off between power and robustness, claims about the asymptotic efficiency of the non-parametric methods are unhelpful. The rule of thumb that ""Wilcoxon tests have about 95% of the power of a t-test if the data really are normal, and are often far more powerful if the data is not, so just use a Wilcoxon"" is sometimes heard, but if the 95% only applies to large $n$, this is flawed reasoning for smaller samples.
Small samples may make it very difficult, or impossible, to assess whether a transformation is appropriate for the data since it's hard to tell whether the transformed data belong to a (sufficiently) normal distribution. So if a QQ plot reveals very positively skewed data, which look more reasonable after taking logs, is it safe to use a t-test on the logged data? On larger samples this would be very tempting, but with small $n$ I'd probably hold off unless there had been grounds to expect a log-normal distribution in the first place.
What about checking assumptions for the non-parametrics? Some sources recommend verifying a symmetric distribution before applying a Wilcoxon test (treating it as a test for location rather than stochastic dominance), which brings up similar problems to checking normality. If the reason we are applying a non-parametric test in the first place is a blind obedience to the mantra of ""safety first"", then the difficulty assessing skewness from a small sample would apparently lead us to the lower power of a paired sign test.

With these small-sample issues in mind, is there a good - hopefully citable - procedure to work through when deciding between t and non-parametric tests?
There have been several excellent answers, but a response considering other alternatives to rank tests, such as permutation tests, would also be welcome. 
","['hypothesis-testing', 't-test', 'nonparametric', 'small-sample', 'wilcoxon-mann-whitney-test']","I am going to change the order of questions about.I've found textbooks and lecture notes frequently disagree, and would like a system to work through the choice that can safely be recommended as best practice, and especially a textbook or paper this can be cited to.Unfortunately, some discussions of this issue in books and so on rely on received wisdom. Sometimes that received wisdom is reasonable, sometimes it is less so (at the least in the sense that it tends to focus on a smaller issue when a larger problem is ignored); we should examine the justifications offered for the advice (if any justification is offered at all) with care.Most guides to choosing a t-test or non-parametric test focus on the normality issue.That’s true, but it’s somewhat misguided for several reasons that I address in this answer.If performing an ""unrelated samples"" or ""unpaired"" t-test, whether to use a Welch correction?This (to use it unless you have reason to think variances should be equal) is the advice of numerous references. I point to some in this answer.Some people use a hypothesis test for equality of variances, but here it would have low power. Generally I just eyeball whether the sample SDs are ""reasonably"" close or not (which is somewhat subjective, so there must be a more principled way of doing it) but again, with low n it may well be that the population SDs are rather further apart than the sample ones.Is it safer simply to always use the Welch correction for small samples, unless there is some good reason to believe population variances are equal?
That’s what the advice is. The properties of the tests are affected by the choice based on the assumption test.Some references on this can be seen here and here,  though there are more that say similar things.The equal-variances issue has many similar characteristics to the normality issue – people want to test it, advice suggests conditioning choice of tests on the results of tests can adversely affect the results of both kinds of subsequent test – it’s better simply not to assume what you can’t adequately justify (by reasoning about the data, using information from other studies relating to the same variables and so on).However,  there are differences. One is that – at least in terms of the distribution of the test statistic under the null hypothesis (and hence, its level-robustness) - non-normality is less important in large samples (at least in respect of significance level, though power might still be an issue if you need to find small effects), while the effect of unequal variances under the equal variance assumption doesn’t really go away with large sample size.What principled method can be recommended for choosing which is the most appropriate test when the sample size is ""small""?With hypothesis tests, what matters (under some set of conditions) is primarily two things:What is the actual type I error rate?What is the power behaviour like?We also need to keep in mind that if we're comparing two procedures, changing the first will change the second (that is, if they’re not conducted at the same actual significance level, you would expect that higher $\alpha$ is associated with higher power).(Of course we're usually not so confident we know what distributions we're dealing with, so the sensitivity of those behaviors to changes in circumstances also matter.)With these small-sample issues in mind, is there a good - hopefully citable - checklist to work through when deciding between t and non-parametric tests?I will consider a number of situations in which I’ll make some recommendations, considering both the possibility of non-normality and unequal variances. In every case, take mention of the t-test to imply the Welch-test:Non-normal (or unknown), likely to have near-equal variance:If the distribution is heavy-tailed, you will generally be better with a Mann-Whitney, though if it’s only slightly heavy, the t-test should do okay. With light-tails the t-test may (often) be preferred.  Permutation tests are a good option (you can even do a permutation test using a t-statistic if you're so inclined). Bootstrap tests are also suitable.Non-normal (or unknown), unequal variance (or variance relationship unknown):If the distribution is heavy-tailed, you will generally be better with a Mann-WhitneyZimmerman and Zumbo (1993)$^{[1]}$ suggest a Welch-t-test on the ranks which they say performs better that the Wilcoxon-Mann-Whitney in cases where the variances are unequal.rank tests are reasonable defaults here if you expect non-normality (again with the above caveat). If you have external information about shape or variance, you might consider GLMs . If you expect things not to be too far from normal, t-tests may be fine.Because of the problem with getting suitable significance levels, neither permutation tests nor rank tests may be suitable, and at the smallest sizes, a t-test may be the best option (there’s some possibility of slightly robustifying it). However, there’s a good argument for using higher type I error rates with small samples (otherwise you’re letting type II error rates inflate while holding type I error rates constant).
Also see de Winter (2013)$^{[2]}$.The advice must be modified somewhat when the distributions are both strongly skewed and very discrete, such as Likert scale items where most of the observations are in one of the end categories. Then the Wilcoxon-Mann-Whitney isn’t necessarily a better choice than the t-test.Simulation can help guide choices further when you have some information about likely circumstances.I appreciate this is something of a perennial topic, but most questions concern the questioner's particular data set, sometimes a more general discussion of power, and occasionally what to do if two tests disagree, but I would like a procedure to pick the correct test in the first place!The main problem is how hard it is to check the normality assumption in a small data set:It is difficult to check normality in a small data set, and to some extent that's an important issue, but I think there's another issue of importance that we need to consider. A basic problem is that trying to assess normality as the basis of choosing between tests adversely impacts the properties of the tests you're choosing between.Any formal test for normality would have low power so violations may well not be detected. (Personally I wouldn't test for this purpose, and I'm clearly not alone, but
I've found this little use when clients demand a normality test be performed because that's what their textbook or old lecture notes or some website they found once declare should be done. This is one point where a weightier looking citation would be welcome.)Here’s an example of a reference (there are others)  which is unequivocal (Fay and Proschan, 2010$^{[3]}$):The choice between t- and WMW DRs should not be based on a test of normality.They are similarly unequivocal about not testing for equality of variance.To make matters worse, it is unsafe to use the Central Limit Theorem as a safety net: for small n we can't rely on the convenient asymptotic normality of the test statistic and t distribution.Nor even in large samples -- asymptotic normality of the numerator doesn’t imply that the t-statistic will have a t-distribution. However, that may not matter so much, since  you should still have asymptotic normality (e.g. CLT for the numerator, and Slutsky’s theorem suggest that eventually the t-statistic should begin to look normal, if the conditions for both hold.)One principled response to this is ""safety first"": as there's no way to reliably verify the normality assumption on a small sample, run an equivalent non-parametric test instead.That’s actually the advice that the references I mention (or link to mentions of) give.Another approach I've seen but feel less comfortable with, is to perform a visual check and proceed with a t-test if nothing untowards is observed (""no reason to reject normality"", ignoring the low power of this check). My personal inclination is to consider whether there are any grounds for assuming normality, theoretical (e.g. variable is sum of several random components and CLT applies) or empirical (e.g. previous studies with larger n suggest variable is normal).Both those are good arguments, especially when backed up with the fact that the t-test is reasonably robust against moderate deviations from normality.
(One should keep in mind, however, that ""moderate deviations"" is a tricky phrase; certain kinds of deviations from normality may impact the power performace of the t-test quite a bit even though those deviations are visually very small - the t-test is less robust to some deviations than others. We should keep this in mind whenever we're discussing small deviations from normality.)Beware, however, the phrasing ""suggest the variable is normal"". Being reasonably consistent with normality is not the same thing as normality. We can often reject actual normality with no need even to see the data – for example, if the data cannot be negative, the distribution cannot be normal. Fortunately, what matters is closer to what we might actually have from previous studies or reasoning about how the data are composed, which is that the deviations from normality should be small.If so, I would use a t-test if data passed visual inspection, and otherwise stick to non-parametrics. But any theoretical or empirical grounds usually only justify assuming approximate normality, and on low degrees of freedom it's hard to judge how near normal it needs to be to avoid invalidating a t-test.Well, that’s something we can assess the impact of fairly readily (such as via simulations, as I mentioned earlier). From what I've seen, skewness seems to matter more than heavy tails (but on the other hand I have seen some claims of the opposite - though I don't know what that's based on).For people who see the choice of methods as a trade-off between power and robustness, claims about the asymptotic efficiency of the non-parametric methods are unhelpful. For instance, the rule of thumb that ""Wilcoxon tests have about 95% of the power of a t-test if the data really are normal, and are often far more powerful if the data is not, so just use a Wilcoxon"" is sometimes heard, but if the 95% only applies to large n, this is flawed reasoning for smaller samples.But we can check small-sample power quite easily! It’s easy enough to simulate to obtain power curves as here.
(Again, also see de Winter (2013)$^{[2]}$).Having done such simulations under a variety of circumstances, both for the two-sample and one-sample/paired-difference cases, the small sample efficiency at the normal in both cases seems to be a little lower than the asymptotic efficiency, but the efficiency of the signed rank and Wilcoxon-Mann-Whitney tests is still very high even at very small sample sizes.At least that's if the tests are done at the same actual significance level; you can't do a 5% test with very small samples (and least not without randomized tests for example), but if you're prepared to perhaps do (say) a 5.5% or a 3.2% test instead, then the rank tests hold up very well indeed compared with a t-test at that significance level.Small samples may make it very difficult, or impossible, to assess whether a transformation is appropriate for the data since it's hard to tell whether the transformed data belong to a (sufficiently) normal distribution. So if a QQ plot reveals very positively skewed data, which look more reasonable after taking logs, is it safe to use a t-test on the logged data? On larger samples this would be very tempting, but with small n I'd probably hold off unless there had been grounds to expect a log-normal distribution in the first place.There’s another alternative: make a different parametric assumption. For example, if there’s skewed data, one might, for example, in some situations reasonably consider a gamma distribution, or some other skewed family as a better approximation - in moderately large samples, we might just use a GLM, but in very small samples it may be necessary to look to a small-sample test - in many cases simulation can be useful.Alternative 2: robustify the t-test (but taking care about the choice of robust procedure so as not to heavily discretize the resulting distribution of the test statistic)  -  this has some advantages over a very-small-sample nonparametric procedure such as the ability to consider tests with low type I error rate.Here I'm thinking along the lines of using say M-estimators of location (and related estimators of scale) in the t-statistic to smoothly robustify against deviations from normality. Something akin to the Welch, like:$$\frac{\stackrel{\sim}{x}-\stackrel{\sim}{y}}{\stackrel{\sim}{S}_p}$$where $\stackrel{\sim}{S}_p^2=\frac{\stackrel{\sim}{s}_x^2}{n_x}+\frac{\stackrel{\sim}{s}_y^2}{n_y}$ and $\stackrel{\sim}{x}$, $\stackrel{\sim}{s}_x$ etc being robust estimates of location and scale respectively.I'd aim to reduce any tendency of the statistic to discreteness - so I'd avoid things like trimming and Winsorizing, since if the original data were discrete, trimming etc will exacerbate this; by using M-estimation type approaches with a smooth $\psi$-function you achieve similar effects without contributing to the discreteness. Keep in mind we're trying to deal with the situation where $n$ is very small indeed (around 3-5, in each sample, say), so even M-estimation potentially has its issues.You could, for example, use simulation at the normal to get p-values (if sample sizes are very small, I'd suggest that over bootstrapping - if sample sizes aren't so small, a carefully-implemented bootstrap may do quite well, but then we might as well go back to Wilcoxon-Mann-Whitney). There's be a scaling factor as well as a d.f. adjustment to get to what I'd imagine would then be a reasonable t-approximation. This means we should get the kind of properties we seek very close to the normal, and should have reasonable robustness in the broad vicinity of the normal. There are a number of issues that come up that would be outside the scope of the present question, but I think in very small samples the benefits should outweigh the costs and the extra effort required.[I haven't read the literature on this stuff for a very long time, so I don't have suitable references to offer on that score.]Of course if you didn't expect the distribution to be somewhat normal-like, but rather similar to some other distribution, you could undertake a suitable robustification of a different parametric test.What if you want to check assumptions for the non-parametrics? Some sources recommend verifying a symmetric distribution before applying a Wilcoxon test, which brings up similar problems to checking normality.Indeed. I assume you mean the signed rank test*. In the case of using it on paired data, if you are prepared to assume that the two distributions are the same shape apart from location shift you are safe, since the differences should then be symmetric. Actually, we don't even need that much; for the test to work you need symmetry under the null; it's not required under the alternative (e.g. consider a paired situation with identically-shaped right skewed continuous distributions on the positive half-line, where the scales differ under the alternative but not under the null; the signed rank test should work essentially as expected in that case). The interpretation of the test is easier if the alternative is a location shift though.*(Wilcoxon’s name is  associated with both the one and two sample rank tests – signed rank and rank sum; with their U test, Mann and Whitney generalized the situation studied by Wilcoxon, and introduced important new ideas for evaluating the null distribution, but the priority between the two sets of authors on the Wilcoxon-Mann-Whitney is clearly Wilcoxon’s -- so at least if we only consider Wilcoxon vs Mann&Whitney, Wilcoxon goes first in my book. However, it seems Stigler's Law beats me yet again, and Wilcoxon should perhaps share some of that priority with a number of earlier contributors, and (besides Mann and Whitney) should share credit with several discoverers of an equivalent test.[4][5] )References[1]: Zimmerman DW and Zumbo BN, (1993),
Rank transformations and the power of the Student t-test and Welch t′-test for non-normal populations,
Canadian Journal Experimental Psychology, 47: 523–39.[2]: J.C.F. de Winter (2013),
""Using the Student’s t-test with extremely small sample sizes,""
Practical Assessment, Research and Evaluation,  18:10, August, ISSN 1531-7714
http://pareonline.net/getvn.asp?v=18&n=10[3]: Michael P. Fay and Michael A. Proschan (2010),
""Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules,""
Stat Surv; 4: 1–39.
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/[4]: Berry, K.J., Mielke, P.W. and Johnston, J.E. (2012),
""The Two-sample Rank-sum Test: Early Development,""
Electronic Journal for History of Probability and Statistics, Vol.8, December
pdf[5]: Kruskal, W. H. (1957),
""Historical notes on the Wilcoxon unpaired two-sample test,""
Journal of the American Statistical Association, 52, 356–360."
What's wrong with XKCD's Frequentists vs. Bayesians comic?,"

This xkcd comic (Frequentists vs. Bayesians) makes fun of a frequentist statistician who derives an obviously wrong result.
However it seems to me that his reasoning is actually correct in the sense that it follows  the standard frequentist methodology. 
So my question is ""does he correctly apply the frequentist methodology?"" 

If no: what would be a correct frequentist inference in this scenario? How to integrate ""prior knowledge"" about the sun stability in the frequentist methodology?
If yes: wtf? ;-)

","['bayesian', 'frequentist']","The main issue is that the first experiment (Sun gone nova) is not repeatable, which makes it highly unsuitable for frequentist methodology that interprets probability as estimate of how frequent an event is giving that we can repeat the experiment many times. In contrast, bayesian probability is interpreted as our degree of belief giving all available prior knowledge, making it suitable for common sense reasoning about one-time events. The dice throw experiment is repeatable, but I find it very unlikely that any frequentist would intentionally ignore the influence of the first experiment and be so confident in significance of the obtained results.Although it seems that author mocks frequentist reliance on repeatable experiments and their distrust of priors, giving the unsuitability of the experimental setup to the frequentist methodology I would say that real theme of this comic is not frequentist methodology but blind following of unsuitable methodology in general. Whether it's funny or not is up to you (for me it is) but I think it more misleads than clarifies the differences between the two approaches."
Bias and variance in leave-one-out vs K-fold cross validation,"
How do different cross-validation methods compare in terms of model variance and bias? 
My question is partly motivated by this thread: Optimal number of folds in $K$-fold cross-validation: is leave-one-out CV always the best choice?. The answer there suggests that models learned with leave-one-out cross-validation have higher variance than those learned with regular $K$-fold cross-validation, making leave-one-out CV a worse choice.
However, my intuition tells me that in leave-one-out CV one should see relatively lower variance between models than in the $K$-fold CV, since we are only shifting one data point across folds and therefore the training sets between folds overlap substantially.
Or going in the other direction, if $K$ is low in the $K$-fold CV, the training sets would be quite different across folds, and the resulting models are more likely to be different (hence higher variance).
If the above argument is right, why would models learned with leave-one-out CV have higher variance?
","['machine-learning', 'variance', 'cross-validation', 'bias', 'bias-variance-tradeoff']","why would models learned with leave-one-out CV have higher variance?This topic has been widely discussed both on this site, and in the scientific literature, with conflicting views, intuitions and conclusions. Back in 2013 when this question was first asked, the dominant view was that LOOCV leads to larger variance of the expected generalization error of a training algorithm producing models out of samples of size $n(K−1)/K$. This view, however, appears to be an incorrect generalization of a special case and I would argue that the correct answer is: ""it depends...""Paraphrasing Yves Grandvalet the author of a 2004 paper on the topic I would summarize the intuitive argument as follows:  Experimental simulations from myself and others on this site, as well as those of researchers in the papers linked below will show you that there is no universal truth on the topic. Most experiments have monotonically decreasing or constant variance with $K$, but some special cases show increasing variance with $K$. The rest of this answer proposes a simulation on a toy example and an informal literature review.  [Update] You can find here an alternative simulation for an unstable model in the presence of outliers. Consider the following toy example where we are fitting a degree 4 polynomial to a noisy sine curve. We expect this model to fare poorly for small datasets due to overfitting, as shown by the learning curve. Note that we plot 1 - MSE here to reproduce the illustration from ESLII page 243 You can find the code for this simulation here. The approach was the following:Left Hand Side: Kfolds for 200 data points,  Right Hand Side: Kfolds for 40 data pointsStandard Deviation of MSE (across data sets i) vs Kfolds From this simulation, it seems that:The following three papers investigate the bias and variance of cross validationThis paper is often refered to as the source for the argument that LOOC has higher variance. In section 1: “For example, leave-oneout is almost unbiased, but it has high variance, leading to unreliable estimates (Efron 1983)"" This statement is source of much confusion, because it seems to be from Efron in 1983, not Kohavi. Both Kohavi's theoretical argumentations and experimental results go against this statement:Corollary 2 ( Variance in CV)Given a dataset and an inducer. If the inducer is stable under the perturbations caused by deleting the test instances for the folds in k-fold CV for various values of $k$, then the variance of the estimate will be the sameExperiment
In his experiment, Kohavi compares two algorithms: a C4.5 decision tree and a Naive Bayes classifier across multiple datasets from the UC Irvine repository. His results are below: LHS is accuracy vs folds (i.e. bias) and RHS is standard deviation vs foldsIn fact, only the decision tree on three data sets clearly has higher variance for increasing K. Other results show decreasing or constant variance. Finally, although the conclusion could be worded more strongly, there is no argument for LOO having higher variance, quite the opposite. From section 6. Summary ""k-fold cross validation with moderate k values (10-20) reduces the variance... As k-decreases (2-5) and the samples get smaller, there is variance due to instability of the training sets themselves. The authors take a strong view on this topic and clearly state in Section 7.1In fact, in least squares linear regression, Burman (1989) shows that among the k-fold CVs, in estimating the prediction error, LOO (i.e., n-fold CV) has the smallest asymptotic bias and variance.  ... ... Then a theoretical calculation (Lu, 2007) shows that LOO has the smallest bias and variance at the same time among all delete-n CVs with all possible n_v deletions consideredExperimental results
Similarly, Zhang's experiments point in the direction of decreasing variance with K, as shown below for the True model and the wrong model for Figure 3 and Figure 5. The only experiment for which variance increases with $K$ is for the Lasso and SCAD models. This is explained as follows on page 31: However, if model selection is involved, the performance of LOO worsens in variability as the model selection uncertainty gets higher due to large model space, small penalty coefficients and/or the use of data-driven penalty coefficients"
Free statistical textbooks,"
Are there any free statistical textbooks available? 
","['teaching', 'references']",Online books includeUpdate: I can now add my own forecasting textbook
How does a Support Vector Machine (SVM) work?,"
How does a Support Vector Machine (SVM) work, and what differentiates it from other linear classifiers, such as the Linear Perceptron, Linear Discriminant Analysis, or Logistic Regression? *
(* I'm thinking in terms of the underlying motivations for the algorithm, optimisation strategies, generalisation capabilities, and run-time complexity)
","['machine-learning', 'classification', 'svm', 'statistical-learning']","Support vector machines focus only on the points that are the most difficult to tell apart, whereas other classifiers pay attention to all of the points. The intuition behind the support vector machine approach is that if a classifier is good at the most challenging comparisons (the points in B and A that are closest to each other in Figure 2), then the classifier will be even better at the easy comparisons (comparing points in B and A that are far away from each other). Perceptrons and other classifiers:Perceptrons are built by taking one point at a time and adjusting the dividing line accordingly. As soon as all of the points are separated, the perceptron algorithm stops. But it could stop anywhere. Figure 1 shows that there are a bunch of different dividing lines that separate the data. The perceptron's stopping criteria is simple: ""separate the points and stop improving the line when you get 100% separation"". The perceptron is not explicitly told to find the best separating line. Logistic regression and linear discriminant models are built similarly to perceptrons. The best dividing line maximizes the distance between the B points closest to A and the A points closest to B. It's not necessary to look at all of the points to do this. In fact, incorporating feedback from points that are far away can bump the line a little too far, as seen below.  Support Vector Machines:Unlike other classifiers, the support vector machine is explicitly told to find the best separating line. How? The support vector machine searches for the closest points (Figure 2), which it calls the ""support vectors"" (the name ""support vector machine"" is due to the fact that points are like vectors and that the best line ""depends on"" or is ""supported by"" the closest points). Once it has found the closest points, the SVM draws a line connecting them (see the line labeled 'w' in Figure 2). It draws this connecting line by doing vector subtraction (point A - point B). The support vector machine then declares the best separating line to be the line that bisects -- and is perpendicular to -- the connecting line.  The support vector machine is better because when you get a new sample (new points), you will have already made a line that keeps B and A as far away from each other as possible, and so it is less likely that one will spillover across the line into the other's territory. I consider myself a visual learner, and I struggled with the intuition behind support vector machines for a long time. The paper called Duality and Geometry in SVM Classifiers finally helped me see the light; that's where I got the images from. "
Is there an intuitive interpretation of $A^TA$ for a data matrix $A$?,"
For a given data matrix $A$ (with variables in columns and data points in rows), it seems like $A^TA$ plays an important role in statistics. For example, it is an important part of the analytical solution of ordinary least squares. Or, for PCA, its eigenvectors are the principal components of the data.
I understand how to calculate $A^TA$, but I was wondering if there's an intuitive interpretation of what this matrix represents, which leads to its important role?
","['covariance-matrix', 'matrix', 'correlation-matrix', 'faq']","Geometrically, matrix $\bf A'A$ is called matrix of scalar products (= dot products, = inner products). Algebraically, it is called sum-of-squares-and-cross-products matrix (SSCP).Its $i$-th diagonal element is equal to $\sum a_{(i)}^2$, where $a_{(i)}$ denotes values in the $i$-th column of $\bf A$ and $\sum$ is the sum across rows. The $ij$-th off-diagonal element therein is $\sum a_{(i)}a_{(j)}$.There is a number of important association coefficients and their square matrices  are called angular similarities or SSCP-type similarities:Dividing SSCP matrix by $n$, the sample size or number of rows of $\bf A$, you get MSCP (mean-square-and-cross-product) matrix. The pairwise formula of this association measure is hence $\frac{\sum xy}{n}$ (with vectors $x$ and $y$ being a pair of columns from $\bf A$).If you center columns (variables) of $\bf A$, then $\bf A'A$ is the scatter (or co-scatter, if to be rigorous) matrix and $\mathbf {A'A}/(n-1)$ is the covariance matrix. Pairwise formula of covariance is $\frac{\sum c_xc_y}{n-1}$ with $c_x$ and $c_y$ denoting centerted columns.If you z-standardize columns of $\bf A$ (subtract the column mean and divide by the standard deviation), then $\mathbf {A'A}/(n-1)$ is the Pearson correlation matrix: correlation is covariance for standardized variables. Pairwise formula of correlation is $\frac{\sum z_xz_y}{n-1}$ with $z_x$ and $z_y$ denoting standardized columns. The correlation is also called  coefficient of linearity.If you unit-scale columns of $\bf A$ (bring their SS, sum-of-squares, to 1), then $\bf A'A$ is the cosine similarity matrix. The equivalent pairwise formula thus appears to be $\sum u_xu_y = \frac{\sum{xy}}{\sqrt{\sum x^2}\sqrt{\sum y^2}}$ with $u_x$ and $u_y$ denoting L2-normalized columns. Cosine similarity is also called coefficient of proportionality.If you center and then unit-scale columns of $\bf A$, then $\bf A'A$ is again the Pearson correlation matrix, because correlation is cosine for centered variables$^{1,2}$: $\sum cu_xcu_y = \frac{\sum{c_xc_y}}{\sqrt{\sum c_x^2}\sqrt{\sum c_y^2}}$Alongside these four principal association measures let us also mention some other, also based on of $\bf A'A$, to top it off. They can be seen as measures alternative to cosine similarity because they adopt different from it normalization, the denominator in the formula:Coefficient of identity [Zegers & ten Berge, 1985] has its denominator in the form of arithmetic mean rather than geometric mean: $\frac{\sum{xy}}{(\sum x^2+\sum y^2)/2}$. It can be 1 if and only if the being compared columns of $\bf A$ are identical.Another usable coefficient like it is called similarity ratio: $\frac{\sum{xy}}{\sum x^2 + \sum y^2 -\sum {xy}} = \frac{\sum{xy}}{\sum {xy} + \sum {(x-y)^2}}$.Finally, if values in $\bf A$ are nonnegative and their sum within the columns is 1 (e.g. they are proportions), then $\bf \sqrt {A}'\sqrt A$ is the matrix of fidelity or Bhattacharyya coefficient.$^1$ One way also to compute correlation or covariance matrix, used by many statistical packages, bypasses centering the data and departs straight from SSCP matrix $\bf A'A$ this way. Let $\bf s$ be the row vector of column sums of data $\bf A$ while $n$ is the number of rows in the data. Then (1) compute the scatter matrix as $\bf C = A'A-s's/ \it n$ [thence, $\mathbf C/(n-1)$ will be the covariance matrix]; (2) the diagonal of $\bf C$ is the sums of squared deviations, row vector $\bf d$; (3) compute correlation matrix $\bf R=C/\sqrt{d'd}$.$^2$ An acute but statistically novice reader might find it difficult reconciling the two definitions of correlation - as ""covariance"" (which includes averaging by sample size, the division by df=""n-1"") and as ""cosine"" (which implies no such averaging). But in fact no real averaging in the first formula of correlation takes place. The thing is that st. deviation, by which z-standardization was achieved, had been in turn computed with the division by that same df; and so the denominator ""n-1"" in the formula of correlation-as-covariance entirely cancels if you unwrap the formula: the formula turns into the formula of cosine. To compute empirical correlation value you really need not to know $n$ (except when computing the mean, to center)."
Is it necessary to scale the target value in addition to scaling features for regression analysis?,"
I'm building regression models.  As a preprocessing step, I scale my feature values to have mean 0 and standard deviation 1.  Is it necessary to normalize the target values also?
","['regression', 'machine-learning']","Let's first analyse why feature scaling is performed. Feature scaling improves the convergence of steepest descent algorithms, which do not possess the property of scale invariance.In stochastic gradient descent training examples inform the weight updates iteratively like so, $$w_{t+1} = w_t - \gamma\nabla_w \ell(f_w(x),y)$$Where $w$ are the weights, $\gamma$ is a stepsize, $\nabla_w$ is the gradient wrt weights, $\ell$ is a loss function, $f_w$ is the function parameterized by $w$, $x$ is a training example, and $y$ is the response/label.Compare the following convex functions, representing proper scaling and improper scaling.A step through one weight update of size $\gamma$ will yield much better reduction in the error in the properly scaled case than the improperly scaled case. Shown below is the direction of $\nabla_w \ell(f_w(x),y)$ of length $\gamma$.Normalizing the output will not affect shape of $f$, so it's generally not necessary.The only situation I can imagine scaling the outputs has an impact, is if your response variable is very large and/or you're using f32 variables (which is common with GPU linear algebra). In this case it is possible to get a floating point overflow of an element of the weights. The symptom is either an Inf value or it will wrap-around to the other extreme representation."
Differences between cross validation and bootstrapping to estimate the prediction error,"
I would like your thoughts about the differences between cross validation and bootstrapping to estimate the prediction error.
Does one work better for small dataset sizes or large datasets?
","['cross-validation', 'predictive-models', 'bootstrap']",
Numerical example to understand Expectation-Maximization,"
I am trying to get a good grasp on the EM algorithm, to be able to implement and use it. I spent a full day reading the theory and a paper where EM is used to track an aircraft using the position information coming from a radar. Honestly, I don't think I fully understand the underlying idea. Can someone point me to a numerical example showing a few iterations (3-4) of the EM for a simpler problem (like estimating the parameters of a Gaussian distribution or a sequence of a sinusoidal series or fitting a line). 
Even if someone can point me to a piece of code (with synthetic data), I can try to step through the code. 
","['regression', 'probability', 'mathematical-statistics', 'intuition', 'expectation-maximization']","This is a recipe to learn EM with a practical and (in my opinion) very intuitive 'Coin-Toss' example:  Read this short EM tutorial paper by Do and Batzoglou. This is the schema where the coin toss example is explained:You may have question marks in your head, especially regarding where the probabilities in the Expectation step come from. Please have a look at the explanations on this maths stack exchange page.Look at/run this code that I wrote in Python that simulates the solution to the coin-toss problem in the EM tutorial paper of item 1:   "
How would you explain the difference between correlation and covariance?,"
Following up on this question, How would you explain covariance to someone who understands only the mean?, which addresses the issue of explaining covariance to a lay person, brought up a similar question in my mind.
How would one explain to a statistics neophyte the difference between covariance and correlation? It seems that both refer to the change in one variable linked back to another variable.
Similar to the referred-to question, a lack of formulae would be preferable.
","['correlation', 'covariance']","The problem with covariances is that they are hard to compare: when you calculate the covariance of a set of heights and weights, as expressed in (respectively) meters and kilograms, you will get a different covariance from when you do it in other units (which already gives a problem for people doing the same thing with or without the metric system!), but also, it will be hard to tell if (e.g.) height and weight 'covary more' than, say the length of your toes and fingers, simply because the 'scale' the covariance is calculated on is different.The solution to this is to 'normalize' the covariance: you divide the covariance by something that represents the diversity and scale in both the covariates, and end up with a value that is assured to be between -1 and 1: the correlation. Whatever unit your original variables were in, you will always get the same result, and this will also ensure that you can, to a certain degree, compare whether two variables 'correlate' more than two others, simply by comparing their correlation.Note: the above assumes that the reader already understands the concept of covariance."
How is it possible that validation loss is increasing while validation accuracy is increasing as well,"
I am training a simple neural network on the CIFAR10 dataset. After some time, validation loss started to increase, whereas validation accuracy is also increasing. The test loss and test accuracy continue to improve.
How is this possible? It seems that if validation loss increase, accuracy should decrease. 
P.S. There are several similar questions, but nobody explained what was happening there.
","['neural-networks', 'deep-learning', 'conv-neural-network', 'overfitting']","Other answers explain well how accuracy and loss are not necessarily exactly (inversely) correlated, as loss measures a difference between raw output (float) and a class (0 or 1 in the case of binary classification), while accuracy measures the difference between thresholded output (0 or 1) and class. So if raw outputs change, loss changes but accuracy is more ""resilient"" as outputs need to go over/under a threshold to actually change accuracy.However, accuracy and loss intuitively seem to be somewhat (inversely) correlated, as better predictions should lead to lower loss and higher accuracy, and the case of higher loss and higher accuracy shown by OP is surprising. I have myself encountered this case several times, and I present here my conclusions based on the analysis I had conducted at the time. I stress that this answer is therefore purely based on experimental data I encountered, and there may be other reasons for OP's case.Let's consider the case of binary classification, where the task is to predict whether an image is a cat or a dog, and the output of the network is a sigmoid (outputting a float between 0 and 1), where we train the network to output 1 if the image is one of a cat and 0 otherwise. I believe that in this case, two phenomenons are happening at the same time.Some images with borderline predictions get predicted better and so their output class changes (image C in the figure). This is the classic ""loss decreases while accuracy increases"" behavior that we expect when training is going well.Some images with very bad predictions keep getting worse (image D in the figure). This leads to a less classic ""loss increases while accuracy stays the same"".  Note that when one uses cross-entropy loss for classification as it is usually done, bad predictions are penalized much more strongly than good predictions are rewarded. For a cat image (ground truth : 1), the loss is $log(output)$, so even if many cat images are correctly predicted (eg images A and B in the figure, contributing almost nothing to the mean loss), a single misclassified cat image will have a high loss, hence ""blowing up"" your mean loss. See this answer for further illustration of this phenomenon. (Getting increasing loss and stable accuracy could also be caused by good predictions being classified a little worse, but I find it less likely because of this loss ""asymetry"").So I think that when both accuracy and loss are increasing, the network is starting to overfit, and both phenomena are happening at the same time. The network is starting to learn patterns only relevant for the training set and not great for generalization, leading to phenomenon 2, some images from the validation set get predicted really wrong (image C in the figure), with an effect amplified by the ""loss asymetry"". However, it is at the same time still learning some patterns which are useful for generalization (phenomenon one, ""good learning"") as more and more images are being correctly classified (image C, and also images A and B in the figure).I sadly have no answer for whether or not this ""overfitting"" is a bad thing in this case: should we stop the learning once the network is starting to learn spurious patterns, even though it's continuing to learn useful ones along the way?Finally, I think this effect can be further obscured in the case of multi-class classification, where the network at a given epoch might be severely overfit on some classes but still learning on others."
What are the main differences between K-means and K-nearest neighbours?,"
I know that k-means is unsupervised and is used for clustering etc and that k-NN is supervised. But I wanted to know concrete differences between the two?
","['machine-learning', 'k-means', 'k-nearest-neighbour']","These are completely different methods. The fact that they both have the letter K in their name is a coincidence.K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification.K-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points."
When to use gamma GLMs?,"
The gamma distribution can take on a pretty wide range of shapes, and given the link between the mean and the variance through its two parameters, it seems suited to dealing with heteroskedasticity in non-negative data, in a way that log-transformed OLS can't do without either WLS or some sort of heteroskedasticity-consistent VCV estimator.
I would use it more for routine non-negative data modeling, but I don't know anyone else that uses it, I haven't learned it in a formal classroom setting, and the literature that I read never uses it.  Whenever I Google something like ""practical uses of gamma GLM"", I come up with advice to use it for waiting times between Poisson events.  OK.  But that seems restrictive and can't be its only use.
Naively, it seems like the gamma GLM is a relatively assumption-light means of modeling non-negative data, given gamma's flexibility.  Of course you need to check Q-Q plots and residual plots like any model.  But are there any serious drawbacks that I am missing?  Beyond communication to people who ""just run OLS""?
","['generalized-linear-model', 'gamma-distribution']",
Most interesting statistical paradoxes,"
Because I find them fascinating, I'd like to hear what folks in this community find as the most interesting statistical paradox and why.
",['paradox'],
Difference between confidence intervals and prediction intervals,"
For a prediction interval in linear regression you still use $\hat{E}[Y|x] = \hat{\beta_0}+\hat{\beta}_{1}x$ to generate the interval. You also use this to generate a confidence interval of $E[Y|x_0]$. What's the difference between the two? 
","['regression', 'confidence-interval', 'predictive-models', 'prediction-interval', 'faq']","Your question isn't quite correct. A confidence interval gives a range for $\text{E}[y \mid x]$, as you say. A prediction interval gives a range for $y$ itself. Naturally, our best guess for $y$ is $\text{E}[y \mid x]$, so the intervals will both be centered around the same value, $x\hat{\beta}$. As @Greg says, the standard errors are going to be different---we guess the expected value of $\text{E}[y \mid x]$ more precisely than we estimate $y$ itself. Estimating $y$ requires including the variance that comes from the true error term. To illustrate the difference, imagine that we could get perfect estimates of our $\beta$ coefficients. Then, our estimate of $\text{E}[y \mid x]$ would be perfect. But we still wouldn't be sure what $y$ itself was because there is a true error term that we need to consider. Our confidence ""interval"" would just be a point because we estimate $\text{E}[y \mid x]$ exactly right, but our prediction interval would be wider because we take the true error term into account.Hence, a prediction interval will be wider than a confidence interval."
Maximum Likelihood Estimation (MLE) in layman terms,"
Could anyone explain to me in detail about maximum likelihood estimation (MLE) in layman's terms? I would like to know the underlying concept before going into mathematical derivation or equation.
","['mathematical-statistics', 'maximum-likelihood', 'intuition', 'definition', 'philosophical']","Say you have some data.  Say you're willing to assume that the data comes from some distribution -- perhaps Gaussian.  There are an infinite number of different Gaussians that the data could have come from (which correspond to the combination of the infinite number of means and variances that a Gaussian distribution can have). MLE will pick the Gaussian (i.e., the mean and variance) that is ""most consistent"" with your data (the precise meaning of consistent is explained below).So, say you've got a data set of $y = \{-1, 3, 7\}$.  The most consistent Gaussian from which that data could have come has a mean of 3 and a variance of 16. It could have been sampled from some other Gaussian.  But one with a mean of 3 and variance of 16 is most consistent with the data in the following sense: the probability of getting the particular $y$ values you observed is greater with this choice of mean and variance, than it is with any other choice.Moving to regression: instead of the mean being a constant, the mean is a linear function of the data, as specified by the regression equation. So, say you've got data like $x = \{ 2,4,10 \}$ along with $y$ from before. The mean of that Gaussian is now the fitted regression model $X'\hat\beta$, where $\hat\beta =[-1.9,.9]$Moving to GLMs:  replace Gaussian with some other distribution (from the exponential family).  The mean is now a linear function of the data, as specified by the regression equation, transformed by the link function.  So, it's $g(X'\beta)$, where $g(x) = e^x/(1+e^x)$ for logit (with binomial data)."
Using k-fold cross-validation for time-series model selection,"
Question:
I want to be sure of something, is the use of k-fold cross-validation with time series is straightforward, or does one need to pay special attention before using it? 
Background:
I'm modeling a time series of 6 year (with semi-markov chain), with a data sample every 5 min. To compare several models, I'm using a 6-fold cross-validation by separating the data in 6 year, so my training sets (to calculate the parameters) have a length of 5 years, and the test sets have a length of 1 year. I'm not taking into account the time order, so my different sets are :

fold 1 : training [1 2 3 4 5], test [6]
fold 2 : training [1 2 3 4 6], test [5]
fold 3 : training [1 2 3 5 6], test [4]
fold 4 : training [1 2 4 5 6], test [3]
fold 5 : training [1 3 4 5 6], test [2]
fold 6 : training [2 3 4 5 6], test [1].

I'm making the hypothesis that each year are independent from each other. How can I verify that?
Is there any reference showing the applicability of k-fold cross-validation with time series.
","['time-series', 'modeling', 'cross-validation']","Time-series (or other intrinsically ordered data) can be problematic for cross-validation.  If some pattern emerges in year 3 and stays for years 4-6, then your model can pick up on it, even though it wasn't part of years 1 & 2.An approach that's sometimes more principled for time series is forward chaining, where your procedure would be something like this:That more accurately models the situation you'll see at prediction time, where you'll model on past data and predict on forward-looking data.  It also will give you a sense of the dependence of your modeling on data size."
PCA and proportion of variance explained,"
In general, what is meant by saying that the fraction $x$ of the variance in an analysis like PCA is explained by the first principal component? Can someone explain this intuitively but also give a precise mathematical definition of what ""variance explained"" means in terms of principal component analysis (PCA)?
For simple linear regression, the r-squared of best fit line is always described as the proportion of the variance explained, but I am not sure what to make of that either. Is proportion of variance here just the extend of deviation of points from the best fit line?
","['regression', 'pca', 'linear-model', 'dimensionality-reduction']","In case of PCA, ""variance"" means summative variance or multivariate variability or overall variability or total variability. Below is the covariance matrix of some 3 variables. Their variances are on the diagonal, and the sum of the 3 values (3.448) is the overall variability.Now, PCA replaces original variables with new variables, called principal components, which are orthogonal (i.e. they have zero covariations) and have variances (called eigenvalues) in decreasing order. So, the covariance matrix between the principal components extracted from the above data is this:Note that the diagonal sum is still 3.448, which says that all 3 components account for all the multivariate variability. The 1st principal component accounts for or ""explains"" 1.651/3.448 = 47.9% of the overall variability; the 2nd one explains 1.220/3.448 = 35.4% of it; the 3rd one explains .577/3.448 = 16.7% of it.So, what do they mean when they say that ""PCA maximizes variance"" or ""PCA explains maximal variance""? That is not, of course, that it finds the largest variance among three values 1.343730519 .619205620 1.485549631, no. PCA finds, in the data space, the dimension (direction) with the largest variance out of the overall variance 1.343730519+.619205620+1.485549631 = 3.448. That largest variance would be 1.651354285. Then it finds the dimension of the second largest variance, orthogonal to the first one, out of the remaining 3.448-1.651354285 overall variance. That 2nd dimension would be 1.220288343 variance. And so on. The last remaining dimension is .576843142 variance. See also ""Pt3"" here and the great answer here explaining how it done in more detail.Mathematically, PCA is performed via linear algebra functions called eigen-decomposition or svd-decomposition. These functions will return you all the eigenvalues 1.651354285 1.220288343 .576843142 (and corresponding eigenvectors) at once (see, see)."
"What does a ""closed-form solution"" mean?","
I have come across the term ""closed-form solution"" quite often. What does a closed-form solution mean? How does one determine if a close-form solution exists for a given problem? Searching online, I found some information, but nothing in the context of developing a statistical or probabilistic model / solution. 
I understand regression very well, so if any one can explain the concept with reference to regression or model-fitting, it will be easy to consume. :)
","['regression', 'machine-learning', 'probability', 'terminology', 'stochastic-processes']","""An equation is said to be a closed-form solution if it solves a given
  problem in terms of functions and mathematical operations from a given
  generally accepted set. For example, an infinite sum would generally
  not be considered closed-form. However, the choice of what to call
  closed-form and what not is rather arbitrary since a new ""closed-form""
  function could simply be defined in terms of the infinite sum.""
  --Wolfram Alphaand ""In mathematics, an expression is said to be a closed-form expression
  if it can be expressed analytically in terms of a finite number of
  certain ""well-known"" functions. Typically, these well-known functions
  are defined to be elementary functions—constants, one variable x,
  elementary operations of arithmetic (+ − × ÷), nth roots, exponent and
  logarithm (which thus also include trigonometric functions and inverse
  trigonometric functions). Often problems are said to be tractable if
  they can be solved in terms of a closed-form expression."" -- WikipediaAn example of a closed form solution in linear regression would be the least square equation$$\hat\beta=(X^TX)^{-1}X^Ty$$"
Softmax vs Sigmoid function in Logistic classifier?,"
What decides the choice of function ( Softmax vs Sigmoid ) in a Logistic classifier ?
Suppose there are 4 output classes . Each of the above function gives the probabilities of each class being the correct output . So which one to take for a classifier ?
","['machine-learning', 'logistic', 'classification', 'softmax']","The sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression (a.k.a. MaxEnt, multinomial logistic regression, softmax Regression, Maximum Entropy Classifier).In the two-class logistic regression, the predicted probablies are as follows, using the sigmoid function:$$
\begin{align}
\Pr(Y_i=0) &= \frac{e^{-\boldsymbol\beta \cdot \mathbf{X}_i}} {1 +e^{-\boldsymbol\beta \cdot \mathbf{X}_i}} \, \\
\Pr(Y_i=1) &= 1 - \Pr(Y_i=0) = \frac{1} {1 +e^{-\boldsymbol\beta \cdot \mathbf{X}_i}}
\end{align}
$$In the multiclass logistic regression, with $K$ classes, the predicted probabilities  are as follows, using the softmax function:$$
\begin{align}
\Pr(Y_i=k) &= \frac{e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} {~\sum_{0 \leq c \leq K}^{}{e^{\boldsymbol\beta_c \cdot \mathbf{X}_i}}} \, \\
\end{align}
$$One can observe that the softmax function is an extension of the sigmoid function to the multiclass case, as explained below. Let's look at the multiclass logistic regression, with $K=2$ classes:$$
\begin{align}
\Pr(Y_i=0) &= \frac{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i}} {~\sum_{0 \leq c \leq K}^{}{e^{\boldsymbol\beta_c \cdot \mathbf{X}_i}}} = \frac{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i}}{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i} + e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}} = \frac{e^{(\boldsymbol\beta_0 - \boldsymbol\beta_1) \cdot \mathbf{X}_i}}{e^{(\boldsymbol\beta_0 - \boldsymbol\beta_1) \cdot \mathbf{X}_i} + 1}  = \frac{e^{-\boldsymbol\beta \cdot \mathbf{X}_i}} {1 +e^{-\boldsymbol\beta \cdot \mathbf{X}_i}} \\ \, \\
\Pr(Y_i=1) &= \frac{e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}} {~\sum_{0 \leq c \leq K}^{}{e^{\boldsymbol\beta_c \cdot \mathbf{X}_i}}} = \frac{e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}}{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i} + e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}} = \frac{1}{e^{(\boldsymbol\beta_0-\boldsymbol\beta_1) \cdot \mathbf{X}_i} + 1} = \frac{1} {1 +e^{-\boldsymbol\beta \cdot \mathbf{X}_i}}  \, \\
\end{align}
$$with $\boldsymbol\beta = - (\boldsymbol\beta_0 - \boldsymbol\beta_1)$. We see that we obtain the same probabilities as in the two-class logistic regression using the sigmoid function. Wikipedia expands a bit more on that."
"What loss function for multi-class, multi-label classification tasks in neural networks?","
I'm training a neural network to classify a set of objects into n-classes. Each object can belong to multiple classes at the same time (multi-class, multi-label).
I read that for multi-class problems it is generally recommended to use softmax and categorical cross entropy as the loss function instead of mse and I understand more or less why.
For my problem of multi-label it wouldn't make sense to use softmax of course as each class probability should be independent from the other. So my final layer is just sigmoid units that squash their inputs into a probability range 0..1 for every class.
Now I'm not sure what loss function I should use for this. Looking at the definition of categorical crossentropy I believe it would not apply well to this problem as it will only take into account the output of neurons that should be 1 and ignores the others.
Binary cross entropy sounds like it would fit better, but I only see it ever mentioned for binary classification problems with a single output neuron.
I'm using python and keras for training in case it matters.
","['neural-networks', 'python', 'loss-functions', 'keras', 'cross-entropy']",
Intuitive explanation of unit root,"
How would you explain intuitively what is a unit root, in the context of the unit root test? 
I'm thinking in ways of explaining much like I've founded in this question.
The case with unit root is that I know (little, by the way) that the unit root test is used to test for stationarity in a time series, but it's just it. 
How would you go to explain it to the layperson, or to a person who has studied a very basic probability and statistics course?
UPDATE
I accepted whuber's answer as it is what most reflect what I asked here. But I urge everybody that came here to read Patrick's and Michael's answers also, as they are the natural ""next step"" in understanding the Unit Root. They use mathematics, but in a very intuitive way.
","['intuition', 'unit-root']","He  had  just come to the bridge; and not looking where he was going,
he  tripped  over  something,  and  the  fir-cone jerked out of his
paw into the river.""Bother,""  said  Pooh,  as  it floated slowly under the bridge, and he went back to get another fir-cone  which  had  a rhyme
to it. But then he thought that he would just look at the river
instead, because it was a peaceful sort of day, so he lay down and
looked at it, and it slipped slowly away beneath him . . . and
suddenly, there was his fir-cone slipping away too.""That's  funny,""  said Pooh. ""I dropped it on the other side,"" said Pooh, ""and it came out on this side! I wonder if it would do it again?""A.A. Milne, The House at Pooh Corner (Chapter VI.  In which Pooh invents a new game and eeyore joins in.)Here is a picture of the flow along the surface of the water:The arrows show the direction of flow and are connected by streamlines.  A fir cone will tend to follow the streamline in which it falls.  But it doesn't always do it the same way each time, even when it's dropped in the same place in the stream: random variations along its path, caused by turbulence in the water, wind, and other whims of nature kick it onto neighboring stream lines.Here, the fir cone was dropped near the upper right corner.  It more or less followed the stream lines--which converge and flow away down and to the left--but it took little detours along the way.An ""autoregressive process"" (AR process) is a sequence of numbers thought to behave like certain flows.  The two-dimensional illustration corresponds to a process in which each number is determined by its two preceding values--plus a random ""detour.""  The analogy is made by interpreting each successive pair in the sequence as coordinates of a point in the stream.  Instant by instant, the stream's flow changes the fir cone's coordinates in the same mathematical way given by the AR process.We can recover the original process from the flow-based picture by writing the coordinates of each point occupied by the fir cone and then erasing all but the last number in each set of coordinates.Nature--and streams in particular--is richer and more varied than the flows corresponding to AR processes.  Because each number in the sequence is assumed to depend in the same fixed way on its predecessors--apart from the random detour part--the flows that illustrate AR processes exhibit limited patterns.  They can indeed seem to flow like a stream, as seen here.  They can also look like the swirling around a drain.  The flows can occur in reverse, seeming to gush outwards from a drain.  And they can look like mouths of two streams crashing together: two sources of water flow directly at one another and then split away to the sides.  But that's about it.  You can't have, say, a flowing stream with eddies off to the sides.  AR processes are too simple for that.In this flow, the fir cone was dropped at the lower right corner and quickly carried into the eddy in the upper right, despite the slight random changes in position it underwent.  But it will never quite stop moving, due to those same random movements which rescue it from oblivion.  The fir cone's coordinates move around a bit--indeed, they are seen to oscillate, on the whole, around the coordinates of the center of the eddy.  In the first stream flow, the coordinates progressed inevitably along the center of the stream, which quickly captured the cone and carried it away faster than its random detours could slow it down: they trend in time.  By contrast, circling around an eddy exemplifies a stationary process in which the fir cone is captured; flowing away down the stream, in which the cone flows out of sight--trending--is non-stationary.Incidentally, when the flow for an AR process moves away downstream, it also accelerates.  It gets faster and faster as the cone moves along it.The nature of an AR flow is determined by a few special, ""characteristic,"" directions, which are usually evident in the stream diagram: streamlines seem to converge towards or come from these directions.  One can always find as many characteristic directions as there are coefficients in the AR process: two in these illustrations.  Associated with each characteristic direction is a number, its ""root"" or ""eigenvalue.""  When the size of the number is less than unity, the flow in that characteristic direction is towards a central location.  When the size of the root is greater than unity, the flow accelerates away from a central location.  Movement along a characteristic direction with a unit root--one whose size is $1$--is dominated by the random forces affecting the cone.  It is a ""random walk.""  The cone can wander away slowly but without accelerating.(Some of the figures display the values of both roots in their titles.)Even Pooh--a bear of very little brain--would recognize that the stream will capture his fir cone only when all the flow is toward one eddy or whirlpool; otherwise, on one of those random detours the cone will eventually find itself under the influence of that part of the flow with a root greater than $1$ in magnitude, whence it will wander off downstream and be lost forever.  Consequently, an AR process can be stationary if and only if all characteristic values are less than unity in size.Economists are perhaps the greatest analysts of time series and employers of the AR process technology.  Their series of data typically do not accelerate out of sight.  They are concerned, therefore, only whether there is a characteristic direction whose value may be as large as $1$ in size: a ""unit root.""  Knowing whether the data are consistent with such a flow can tell the economist much about the potential fate of his pooh stick: that is, about what will happen in the future.  That's why it can be important to test for a unit root.  A fine Wikipedia article explains some of the implications.Pooh and his friends found an empirical test of stationarity:Now one day Pooh and Piglet and Rabbit and Roo were all playing
Poohsticks  together.  They had dropped their sticks in when Rabbit
said ""Go!"" and then they had hurried across to  the other  side  of
the bridge, and now they were all leaning over the edge, waiting to
see whose stick would come out first.  But it was a long time coming,
because the river was very lazy that day,  and  hardly seemed to mind
if it didn't ever get there at all.""I can  see  mine!""  cried  Roo.  ""No,  I  can't,  it's something  else.  Can  you see yours, Piglet? I thought I could see
mine, but I couldn't. There it is! No, it  isn't.  Can  you see yours,
Pooh?""""No,"" said Pooh.""I  expect  my  stick's  stuck,""  said Roo. ""Rabbit, my stick's stuck. Is your stick stuck, Piglet?""""They always take longer than you think,"" said Rabbit.This passage, from 1928, could be construed as the very first ""Unit Roo test."""
"Why use gradient descent for linear regression, when a closed-form math solution is available?","
I am taking the Machine Learning courses online and learnt about Gradient Descent for calculating the optimal values in the hypothesis.
h(x) = B0 + B1X

why we need to use Gradient Descent if we can easily find the values with the below formula? This looks straight forward and easy too. but GD needs multiple iterations to get the value.
B1 = Correlation * (Std. Dev. of y/ Std. Dev. of x)

B0 = Mean(Y) – B1 * Mean(X)

NOTE: Taken as in https://www.dezyre.com/data-science-in-r-programming-tutorial/linear-regression-tutorial
I did checked on the below questions and for me it was not clear to understand.
Why is gradient descent required?
Why is optimisation solved with gradient descent rather than with an analytical solution?
The above answers compares GD vs. using derivatives. 
","['regression', 'machine-learning', 'gradient-descent']","The main reason why gradient descent is used for linear regression is the computational complexity: it's computationally cheaper (faster) to find the solution using the gradient descent in some cases.The formula which you wrote looks very simple, even computationally, because it only works for univariate case, i.e. when you have only one variable. In the multivariate case, when you have many variables, the formulae is slightly more complicated on paper and requires much more calculations when you implement it in software:
$$\beta=(X'X)^{-1}X'Y$$
Here, you need to calculate the matrix $X'X$ then invert it (see note below). It's an expensive calculation. For your reference, the (design) matrix X has K+1 columns where K is the number of predictors and N rows of observations. In a machine learning algorithm you can end up with K>1000 and N>1,000,000. The $X'X$ matrix itself takes a little while to calculate, then you have to invert $K\times K$ matrix - this is expensive.So, the gradient descent allows to save a lot of time on calculations. Moreover, the way it's done allows for a trivial parallelization, i.e. distributing the calculations across multiple processors or machines. The linear algebra solution can also be parallelized but it's more complicated and still expensive.Additionally, there are versions of gradient descent when you keep only a piece of your data in memory, lowering the requirements for computer memory. Overall, for extra large problems it's more efficient than linear algebra solution.This becomes even more important as the dimensionality increases, when you have thousands of variables like in machine learning.Remark. I was surprised by how much attention is given to the gradient descent in Ng's lectures. He spends nontrivial amount of time talking about it, maybe 20% of entire course. To me it's just an implementation detail, it's how exactly you find the optimum. The key is in formulating the optimization problem, and how exactly you find it is nonessential. I wouldn't worry about it too much. Leave it to computer science people, and focus on what's important to you as a statistician. Having said this I must qualify by saying that it is indeed important to understand the computational complexity and numerical stability of the solution algorithms. I still don't think you must know the details of implementation and code of the algorithms. It's not the best use of your time as a statistician usually.Note 1. I wrote that you have to invert the matrix for didactic purposes and it's not how usually you solve the equation. In practice, the linear algebra problems are solved by using some kind of factorization such as QR, where you don't directly invert the matrix but do some other mathematically equivalent manipulations to get an answer. You do this because matrix inversion is an expensive and numerically unstable operation in many cases. This brings up another little advantageof the gradient descent algorithm as a side effect: it works even when the design matrix has collinearity issues. The usual linear algebra path would blow up and gradient descent will keep going even for collinear predictors."
Comprehensive list of activation functions in neural networks with pros/cons,"
Are there any reference document(s) that give a comprehensive list of activation functions in neural networks along with their pros/cons (and ideally some pointers to publications where they were successful or not so successful)?
","['neural-networks', 'references']",
Calculating optimal number of bins in a histogram,"
I'm interested in finding as optimal of a method as I can for determining how many bins I should use in a histogram. My data should range from 30 to 350 objects at most, and in particular I'm trying to apply thresholding (like Otsu's method) where ""good"" objects, which I should have fewer of and should be more spread out, are separated from ""bad"" objects, which should be more dense in value. A concrete value would have a score of 1-10 for each object. I'd had 5-10 objects with scores 6-10, and 20-25 objects with scores 1-4. I'd like to find a histogram binning pattern that generally allows something like Otsu's method to threshold off the low scoring objects. However, in the implementation of Otsu's I've seen, the bin size was 256, and often I have many fewer data points that 256, which to me suggests that 256 is not a good bin number. With so few data, what approaches should I take to calculating the number of bins to use?
","['rule-of-thumb', 'histogram']","The Freedman-Diaconis rule is very robust and works well in practice. The bin-width is set to $h=2\times\text{IQR}\times n^{-1/3}$. So the number of bins is $(\max-\min)/h$, where $n$ is the number of observations, max is the maximum value and min is the minimum value.In base R, you can use: For other plotting libraries without this option (e.g., ggplot2), you can calculate binwidth as: "
tanh activation function vs sigmoid activation function,"
The tanh activation function is: 
$$tanh \left( x \right) = 2 \cdot \sigma \left( 2 x \right) - 1$$
Where $\sigma(x)$, the sigmoid function, is defined as:
 $$\sigma(x) = \frac{e^x}{1 + e^x}$$.
Questions: 

Does it really matter between using those two activation functions
(tanh vs. sigma)?
Which function is better in which cases?

","['machine-learning', 'neural-networks', 'optimization']","Yes it matters for technical reasons. Basically for optimization. It is worth reading Efficient Backprop by LeCun et al.There are two reasons for that choice (assuming you have normalized your data, and this is very important):The range of the tanh function is [-1,1] and that of the sigmoid function is [0,1]"
Does an unbalanced sample matter when doing logistic regression?,"
Okay, so I think I have a decent enough sample, taking into account the 20:1 rule of thumb: a fairly large sample (N=374) for a total of 7 candidate predictor variables. 
My problem is the following: whatever set of predictor variables I use, the classifications never get better than a specificity of 100% and a sensitivity of 0%. However unsatisfactory, this could actually be the best possible result, given the set of candidate predictor variables (from which I can't deviate). 
But, I couldn't help but think I could do better, so I noticed that the categories of the dependent variable were quite unevenly balanced, almost 4:1. Could a more balanced subsample improve classifications?
","['regression', 'logistic', 'sample-size', 'unbalanced-classes', 'faq']","Balance in the Training SetFor logistic regression models unbalanced training data affects only the estimate of the model intercept (although this of course skews all the predicted probabilities, which in turn compromises your predictions). Fortunately the intercept correction is straightforward:   Provided you know, or can guess, the true proportion of 0s and 1s and know the proportions in the training set you can apply a rare events correction to the intercept.  Details are in King and Zeng (2001) [PDF].These 'rare event corrections' were designed for case control research designs, mostly used in epidemiology, that select cases by choosing a fixed, usually balanced number of 0 cases and 1 cases, and then need to correct for the resulting sample selection bias.  Indeed, you might train your classifier the same way.  Pick a nice balanced sample and then correct the intercept to take into account the fact that you've selected on the dependent variable to learn more about rarer classes than a random sample would be able to tell you.Making PredictionsOn a related but distinct topic: Don't forget that you should be thresholding intelligently to make predictions.  It is not always best to predict 1 when the model probability is greater 0.5.  Another threshold may be better.  To this end you should look into the Receiver Operating Characteristic (ROC) curves of your classifier, not just its predictive success with a default probability threshold."
Why does the Lasso provide Variable Selection?,"
I've been reading Elements of Statistical Learning, and I would like to know why the Lasso provides variable selection and ridge regression doesn't.
Both methods minimize the residual sum of squares and have a constraint on the possible values of the parameters $\beta$. For the Lasso, the constraint is $||\beta||_1 \le t$, whereas for ridge it is $||\beta||_2 \le t$, for some $t$.
I've seen the diamond vs ellipse picture in the book and I have some intuition as for why the Lasso can hit the corners of the constrained region, which implies that one of the coefficients is set to zero. However, my intuition is rather weak, and I'm not convinced. It should be easy to see, but I don't know why this is true.
So I guess I'm looking for a mathematical justification, or an intuitive explanation of why the contours of the residual sum of squares are likely to hit the corners of the  $||\beta||_1$ constrained region (whereas this situation is unlikely if the constraint is $||\beta||_2$).
","['regression', 'feature-selection', 'lasso', 'regularization']","Let's consider a very simple model: $y = \beta x + e$, with an L1 penalty on $\hat{\beta}$ and a least-squares loss function on $\hat{e}$.  We can expand the expression to be minimized as:$\min y^Ty -2 y^Tx\hat{\beta} + \hat{\beta} x^Tx\hat{\beta} + 2\lambda|\hat{\beta}|$Keep in mind this is a univariate example, with $\beta$ and $x$ being scalars, to show how LASSO can send a coefficient to zero. This can be generalized to the multivariate case.Let us assume the least-squares solution is some $\hat{\beta} > 0$, which is equivalent to assuming that $y^Tx > 0$, and see what happens when we add the L1 penalty.  With $\hat{\beta}>0$, $|\hat{\beta}| = \hat{\beta}$, so the penalty term is equal to $2\lambda\beta$.  The derivative of the objective function w.r.t. $\hat{\beta}$ is:$-2y^Tx +2x^Tx\hat{\beta} + 2\lambda$ which evidently has solution $\hat{\beta} = (y^Tx - \lambda)/(x^Tx)$.  Obviously by increasing $\lambda$ we can drive $\hat{\beta}$ to zero (at $\lambda = y^Tx$).   However, once $\hat{\beta} = 0$, increasing $\lambda$ won't drive it negative, because, writing loosely, the instant $\hat{\beta}$ becomes negative, the derivative of the objective function changes to:$-2y^Tx +2x^Tx\hat{\beta} - 2\lambda$where the flip in the sign of $\lambda$ is due to the absolute value nature of the penalty term; when $\beta$ becomes negative, the penalty term becomes equal to $-2\lambda\beta$, and taking the derivative w.r.t. $\beta$ results in $-2\lambda$.  This leads to the solution $\hat{\beta} = (y^Tx + \lambda)/(x^Tx)$, which is obviously inconsistent with $\hat{\beta} < 0$ (given that the least squares solution $> 0$, which implies $y^Tx > 0$, and $\lambda > 0$).  There is an increase in the L1 penalty AND an increase in the squared error term (as we are moving farther from the least squares solution) when moving $\hat{\beta}$ from $0$ to $ < 0$, so we don't, we just stick at $\hat{\beta}=0$.It should be intuitively clear the same logic applies, with appropriate sign changes, for a least squares solution with $\hat{\beta} < 0$.  With the least squares penalty $\lambda\hat{\beta}^2$, however, the derivative becomes:$-2y^Tx +2x^Tx\hat{\beta} + 2\lambda\hat{\beta}$which evidently has solution $\hat{\beta} = y^Tx/(x^Tx + \lambda)$.  Obviously no increase in $\lambda$ will drive this all the way to zero.  So the L2 penalty can't act as a variable selection tool without some mild ad-hockery such as ""set the parameter estimate equal to zero if it is less than $\epsilon$"".  Obviously things can change when you move to multivariate models, for example, moving one parameter estimate around might force another one to change sign, but the general principle is the same: the L2 penalty function can't get you all the way to zero, because, writing very heuristically, it in effect adds to the ""denominator"" of the expression for $\hat{\beta}$, but the L1 penalty function can, because it in effect adds to the ""numerator"".  "
"At each step of a limiting infinite process, put 10 balls in an urn and remove one at random. How many balls are left?","
The question (slightly modified) goes as follows and if you have never encountered it before you can check it in example 6a, chapter 2, of Sheldon Ross' A First Course in Probability:

Suppose that we possess an infinitely large urn and an infinite
  collection of balls labeled ball number 1, number 2, number 3, and so
  on. Consider an experiment performed as follows: At 1 minute to 12
  P.M., balls numbered 1 through 10 are placed in the urn and one ball
  removed at random. (Assume that the withdrawal takes no time.) At 1/2
  minute to 12 P.M., balls numbered 11 through 20 are placed in the  urn
  and another ball removed at random. At 1/4 minute to 12P.M., balls
  numbered 21 through 30 are placed in the urn and another ball removed
  at random... and so on. The question of interest is, How many balls
  are in the urn at 12 P.M.?

This question, as it's posed, forces basically everyone to get it wrong --- usually the intuition is to say there will be infinitely many balls at 12 P.M. The  answer provided by Ross, however, is that with probability one the urn will be empty at 12 P.M.
When teaching probability theory this problem is one of those for which is very hard to give a good intuitive explanation.  
On the one hand, you could try to explain it like this: ""think of the probability of any ball i being on the urn at 12 P.M. During the infinite random draws, it will eventually be removed. Since this holds for all balls, none of them can be there at the end"". 
However, students will correctly argue with you: ""but I'm putting 10 balls and removing 1 ball at each time. It's impossible there will be zero balls at the end"".
What's the best explanation we can give to them to solve these conflicting intuitions?
I'm also open to the argument the question is ill-posed and that if we formulate it better the ""paradox"" disappears or to the argument that the paradox is ""purely mathematical""  (but please try to be precise about it).
","['probability', 'paradox']","Ross describes three versions of this ""paradox"" in the Example 6a in his textbook. In each version, 10 balls are added to the urn and 1 ball is removed at each step of the procedure.In the first version, $10n$-th ball is removed at the $n$-th step. There are infinitely many balls left after midnight because all balls with numbers not ending in zero are still in there.In the second version, $n$-th ball is removed at the $n$-th step. There are zero balls left after midnight because each ball is eventually going to be removed at the corresponding step.In the third version, balls are removed uniformly at random. Ross computes the probability of each ball to be removed by step $n$ and finds that it converges to $1$ as $n\to\infty$ (note that this is not evident! one actually has to perform the computation). This means, by Boole's inequality, that the probability of having zero balls in the end is also $1$.You are saying that this last conclusion is not intuitive and hard to explain; this is wonderfully supported by many confused answers and comments in this very thread. However, the conclusion of the second version is exactly as un-intuitive! And it has absolutely nothing to do with probability or statistics. I think that after one accepts the second version, there is nothing particularly surprising about the third version anymore.So whereas the ""probabilistic"" discussion must be about the third version [see very insightful answers by @paw88789, @Paul, and @ekvall], the ""philosophical"" discussion should rather focus on the second version which is much easier and is similar in spirit to the Hilbert's hotel.The second version is known as the Ross-Littlewood paradox. I link to the Wikipedia page, but the discussion there is horribly confusing and I do not recommend reading it at all. Instead, take a look at this MathOverflow thread from years ago. It is closed by now but contains several very perceptive answers. A short summary of the answers that I find most crucial is as follows.We can define a set $S_n$ of the balls present in the urn after step $n$. We have that $S_1=\{2,\ldots 10\}$, $S_2=\{3,\ldots 20\}$, etc. There is a mathematically well-defined notion of the limit of a sequence of sets and one can rigorously prove that the limit of this sequence exists and is the empty set $\varnothing$. Indeed, what balls can be in the limit set? Only the ones that are never removed. But every ball is eventually removed. So the limit is empty. We can write $S_n \to \varnothing$.At the same time, the number $|S_n|$ of the balls in the set $S_n$, also known as the cardinality of this set, is equal to $10n-n=9n$. The sequence $9n$ is obviously diverging, meaning that the cardinality converges to the cardinality of $\mathbb N$, also known as aleph-zero $\aleph_0$. So we can write that $|S_n|\to \aleph_0$.The ""paradox"" now is that these two statements seem to contradict each other:
\begin{align}
S_n &\to \varnothing \\
|S_n| &\to \aleph_0 \ne 0
\end{align}But of course there is no real paradox and no contradiction. Nobody said that taking cardinality is a ""continuous"" operation on sets, so we cannot exchange it with the limit:$$\lim |S_n| \ne |\lim S_n|.$$ In other words, from the fact that $|S_n|=9n$ for all integer $n\in \mathbb N$ we cannot conclude that $|S_\omega|$ (the value at the first ordinal) is equal to $\infty$. Instead, $|S_\omega|$ has to be computed directly and turns out to be zero. So I think what we get out of this really is the conclusion that taking cardinalities is a discontinous operation... [@HarryAltman]So I think this paradox is just the human tendency to assume that ""simple"" operations are continuous. [@NateEldredge]This is easier to understand with functions instead of sets. Consider a characteristic (aka indicator) function $f_n(x)$ of set $S_n$ which is defined to be equal to one on the $[n, 10n]$ interval and zero elsewhere. The first ten functions look like that (compare the ASCII art from @Hurkyl's answer):$\quad\quad\quad$Everybody will agree that for each point $a\in\mathbb R$, we have $\lim f_n(a) = 0$. This by definition means that functions $f_n(x)$ converge to the function $g(x)=0$. Again, everybody will agree to that. However, observe that the integrals of these functions $\int_0^\infty f(x)dx = 9n$ get larger and larger and the sequence of integrals diverges. In other words,$$\lim\int f_n(x)dx \ne \int \lim f_n(x) dx.$$This is a completely standard and familiar analysis result. But it is an exact reformulation of our paradox!A good way to formalize the problem is to describe the state of the jug not as a set (a subset of $\mathbb N$), because those are hard to take limits of, but as its characteristic function. The first ""paradox"" is that pointwise limits are not the same as uniform limits. [@TheoJohnson-Freyd]The crucial point is that ""at midnight noon"" the whole infinite sequence has already passed, i.e. we made a ""trasfinite jump"" and arrived to the transfinite state $f_\omega = \lim f_n(x)$. The value of the integral ""at midnight noon"" has to be the value of the integral of $\lim f_n$, not the other way around.Please note that some of the answers in this thread are misleading despite being highly upvoted.In particular, @cmaster computes $\lim_{n\to\infty} \operatorname{ballCount}(S_n)$ which is indeed infinite, but this is not what the paradox asks about. The paradox asks about what happens after the whole infinite sequence of steps; this is a transfinite construction and so we need to be computing $\operatorname{ballCount}(S_\omega)$ which is equal to zero as explained above."
Is it possible to train a neural network without backpropagation?,"
Many neural network books and tutorials spend a lot of time on the backpropagation algorithm, which is essentially a tool to compute the gradient.
Let's assume we are building a model with ~10K parameters / weights. Is it possible to run the optimization using some gradient free optimization algorithms? 
I think computing the numerical gradient would be too slow, but how about other methods such as Nelder-Mead, Simulated Annealing or a Genetic Algorithm?
All the algorithms would suffer from local minima, why obsessed with gradient?
","['machine-learning', 'neural-networks', 'optimization', 'backpropagation']","The first two algorithms you mention (Nelder-Mead and Simulated Annealing) are generally considered pretty much obsolete in optimization circles, as there are much better alternatives which are both more reliable and less costly. Genetic algorithms covers a wide range, and some of these can be reasonable.However, in the broader class of derivative-free optimization (DFO) algorithms, there are many which are significantly better than these ""classics"", as this has been an active area of research in recent decades. So, might some of these newer approaches be reasonable for deep learning?A relatively recent paper comparing the state of the art is the following:Rios, L. M., & Sahinidis, N. V. (2013) Derivative-free optimization: a review of algorithms and comparison of software implementations. Journal of Global Optimization.This is a nice paper which has many interesting insights into recent techniques. For example, the results clearly show that the best local optimizers are all ""model-based"", using different forms of sequential quadratic programming (SQP). However, as noted in their abstract ""We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size."" To give an idea of the numbers, for all problems the solvers were given a budget of 2500 function evaluations, and problem sizes were a maximum of ~300 parameters to optimize. Beyond O[10] parameters, very few of these optimizers performed very well, and even the best ones showed a noticable decay in performance as problem size was increased.So for very high dimensional problems, DFO algorithms just are not competitive with derivative based ones. To give some perspective, PDE (partial differential equation)-based optimization is another area with very high dimensional problems (e.g. several parameter for each cell of a large 3D finite element grid). In this realm, the ""adjoint method"" is one of the most used methods. This is also a gradient-descent optimizer based on automatic differentiation of a forward model code.The closest to a high-dimensional DFO optimizer is perhaps the Ensemble Kalman Filter, used for assimilating data into complex PDE simulations, e.g. weather models. Interestingly, this is essentially an SQP approach, but with a Bayesian-Gaussian interpretation (so the quadratic model is positive definite, i.e. no saddle points). But I do not think that the number of parameters or observations in these applications is comparable to what is seen in deep learning.Side note (local minima): From the little I have read on deep learning, I think the consensus is that it is saddle points rather than local minima, which are most problematic for high dimensional NN-parameter spaces.For example, the recent review in Nature says ""Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general. Instead, the landscape is packed with a combinatorially large number of saddle points where the gradient is zero, and the surface curves up in most dimensions and curves down in the remainder.""A related concern is about local vs. global optimization (for example this question pointed out in the comments). While I do not do deep learning, in my experience overfitting is definitely a valid concern. In my opinion, global optimization methods are most suited for engineering design problems that do not strongly depend on ""natural"" data. In data assimilation problems, any current global minima could easily change upon addition of new data (caveat: My experience is concentrated in geoscience problems, where data is generally ""sparse"" relative to model capacity).An interesting perspective is perhapsO. Bousquet & L. Bottou (2008) The tradeoffs of large scale learning. NIPS.which provides semi-theoretical arguments on why and when approximate optimization may be preferable in practice.End note (meta-optimization): While gradient based techniques seem likely to be dominant for training networks, there may be a role for DFO in associated meta-optimization tasks.One example would be hyper-parameter tuning. (Interestingly, the successful model-based DFO optimizers from Rios & Sahinidis could be seen as essentially solving a sequence of design-of-experiments/response-surface problems.)Another example might be designing architectures, in terms of the set-up of layers (e.g. number, type, sequence, nodes/layer). In this discrete-optimization context genetic-style algorithms may be more appropriate. Note that here I am thinking of the case where connectivity is determined implicitly by these factors (e.g. fully-connected layers, convolutional layers, etc.). In other words the $\mathrm{O}[N^2]$ connectivity is $not$ meta-optimized explicitly. (The connection strength would fall under training, where e.g. sparsity could be promoted by $L_1$ regularization and/or ReLU activations ... these choices could be meta-optimized however.)"
Assessing approximate distribution of data based on a histogram,"
Suppose I want to see whether my data is exponential based on a histogram (i.e. skewed to the right). 
Depending on how I group or bin the data, I can get wildly different histograms. 
One set of histograms will make is seem that the data is exponential. Another set will make it seem that data are not exponential. How do I make determining distributions from histograms well defined?
","['distributions', 'data-visualization', 'histogram', 'binning']",
What is an ablation study? And is there a systematic way to perform it?,"
What is an ablation study? And is there a systematic way to perform it? For example, I have $n$ predictors in a linear regression which I will call as my model. 
How will I perform an ablation study to this? What metrics should I use?
A comprehensive source or textbook would be appreciated. 
","['regression', 'machine-learning', 'neural-networks']",
Why isn't Logistic Regression called Logistic Classification?,"
Since Logistic Regression is a statistical classification model dealing with categorical dependent variables, why isn't it called Logistic Classification? Shouldn't the ""Regression"" name be reserved to models dealing with continuous dependent variables?
","['regression', 'machine-learning', 'logistic', 'classification', 'terminology']","Logistic regression is emphatically not a classification algorithm on its own. It is only a classification algorithm in combination with a decision rule that makes dichotomous the predicted probabilities of the outcome. Logistic regression is a regression model because it estimates the probability of class membership as a (transformation of a) multilinear function of the features.Frank Harrell has posted a number of answers on this website enumerating the pitfalls of regarding logistic regression as a classification algorithm. Among them:If I recall correctly, he once pointed me to his book on regression strategies for more elaboration on these (and more!) points, but I can't seem to find that particular post."
Including the interaction but not the main effects in a model,"
Is it ever valid to include a two-way interaction in a model without including the main effects?  What if your hypothesis is only about the interaction, do you still need to include the main effects?
","['regression', 'modeling', 'interaction', 'regression-coefficients']","In my experience, not only is it necessary to have all lower order effects in the model when they are connected to higher order effects, but it is also important to properly model (e.g., allowing to be nonlinear) main effects that are seemingly unrelated to the factors in the interactions of interest.  That's because interactions between $x_1$ and $x_2$ can be stand-ins for main effects of $x_3$ and $x_4$.  Interactions sometimes seem to be needed because they are collinear with omitted variables or omitted nonlinear (e.g., spline) terms."
ASA discusses limitations of $p$-values - what are the alternatives?,"
We already have multiple threads tagged as p-values that reveal lots of misunderstandings about them. Ten months ago we had a thread about psychological journal that ""banned"" $p$-values, now American Statistical Association (2016) says that with our analysis we ""should not end with the calculation of a $p$-value"". 

American Statistical Association (ASA) believes that the scientific 
  community could benefit from a formal statement clarifying several
  widely agreed upon principles underlying the proper use and
  interpretation of the $p$-value.

The committee lists other approaches as possible alternatives or supplements to $p$-values: 

In view of the prevalent misuses of and misconceptions concerning
  $p$-values, some statisticians prefer to supplement or even replace
  $p$-values with other approaches. These include methods that emphasize
  estimation over testing, such as confidence, credibility, or
  prediction intervals; Bayesian methods; alternative measures of
  evidence, such as likelihood ratios or Bayes Factors; and other
  approaches such as decision-theoretic modeling and false discovery
  rates. All these measures and approaches rely on further assumptions,
  but they may more directly address the size of an effect (and its
  associated uncertainty) or whether the hypothesis is correct.

So let's imagine post-$p$-values reality. ASA lists some methods that can be used in place of $p$-values, but why are they better? Which of them can be real-life replacement for a researcher that used $p$-values for all his life? I imagine that this kind of questions will appear in post-$p$-values reality, so maybe let's try to be one step ahead of them. What is the reasonable alternative that can be applied out-of-the-box? Why this approach should convince your lead researcher, editor, or readers?
As this follow-up blog entry suggests, $p$-values are unbeatable in their simplicity:

The p-value requires only a statistical model for the behavior of a
  statistic under the null hypothesis to hold. Even if a model of an
  alternative hypothesis is used for choosing a “good” statistic (which
  would be used for constructing the p-value), this alternative model
  does not have to be correct in order for the p-value to be valid and
  useful (i.e.: control type I error at the desired level while offering
  some power to detect a real effect). In contrast, other (wonderful and
  useful) statistical methods such as Likelihood ratios, effect size
  estimation, confidence intervals, or Bayesian methods all need the
  assumed models to hold over a wider range of situations, not merely
  under the tested null.

Are they, or maybe it is not true and we can easily replace them?
I know, this is broad, but the main question is simple: what is the best (and why), real-life alternative to $p$-values that can be used as a replacement?

ASA (2016). ASA Statement on Statistical Significance and $P$-values. The American Statistician. (in press)
","['hypothesis-testing', 'bayesian', 'p-value', 'frequentist']","I will focus this answer on the specific question of what are the alternatives to $p$-values.There are 21 discussion papers published along with the ASA statement (as Supplemental Materials): by Naomi Altman, Douglas Altman,
Daniel J. Benjamin, Yoav Benjamini, Jim Berger, Don Berry, John Carlin, George Cobb, Andrew Gelman, Steve Goodman, Sander Greenland, John Ioannidis, Joseph Horowitz, Valen
Johnson, Michael Lavine, Michael Lew, Rod Little, Deborah Mayo, Michele Millar, Charles
Poole, Ken Rothman, Stephen Senn, Dalene Stangl, Philip Stark and Steve Ziliak (some of them wrote together; I list all for future searches). These people probably cover all existing opinions about $p$-values and statistical inference.I have looked through all 21 papers.Unfortunately, most of them do not discuss any real alternatives, even though the majority are about the limitations, misunderstandings, and various other problems with $p$-values (for a defense of $p$-values, see Benjamini, Mayo, and Senn). This already suggests that alternatives, if any, are not easy to find and/or to defend.So let us look at the list of ""other approaches"" given in the ASA statement itself (as quoted in your question):[Other approaches] include methods that
emphasize estimation over testing, such as confidence, credibility, or prediction intervals;
Bayesian methods; alternative measures of evidence, such as likelihood ratios or Bayes Factors;
and other approaches such as decision-theoretic modeling and false discovery rates.Confidence intervals are a frequentist tool that goes hand-in-hand with $p$-values; reporting a confidence interval (or some equivalent, e.g., mean $\pm$ standard error of the mean) together with the $p$-value is almost always a good idea.Some people (not among the ASA disputants) suggest that confidence intervals should replace the $p$-values. One of the most outspoken proponents of this approach is Geoff Cumming who calls it new statistics (a name that I find appalling). See e.g. this blog post by Ulrich Schimmack for a detailed critique: A Critical Review of Cumming’s (2014) New Statistics: Reselling Old Statistics as New Statistics. See also We cannot afford to study effect size in the lab blog post by Uri Simonsohn for a related point.See also this thread (and my answer therein) about the similiar suggestion by Norm Matloff where I argue that when reporting CIs one would still like to have the $p$-values reported as well: What is a good, convincing example in which p-values are useful?Some other people (not among the ASA disputants either), however, argue that confidence intervals, being a frequentist tool, are as misguided as $p$-values and should also be disposed of. See, e.g., Morey et al. 2015, The Fallacy of Placing Confidence in Confidence Intervals linked by @Tim here in the comments. This is a very old debate.(I don't like how the ASA statement formulates the list. Credible intervals and Bayes factors are listed separately from ""Bayesian methods"", but they are obviously Bayesian tools. So I count them together here.)There is a huge and very opinionated literature on the Bayesian vs. frequentist debate. See, e.g., this recent thread for some thoughts: When (if ever) is a frequentist approach substantively better than a Bayesian? Bayesian analysis makes total sense if one has good informative priors, and everybody would be only happy to compute and report $p(\theta|\text{data})$ or $p(H_0:\theta=0|\text{data})$ instead of $p(\text{data at least as extreme}|H_0)$—but alas, people usually do not have good priors. An experimenter records 20 rats doing something in one condition and 20 rats doing the same thing in another condition; the prediction is that the performance of the former rats will exceed the performance of the latter rats, but nobody would be willing or indeed able to state a clear prior over the performance differences. (But see  @FrankHarrell's answer where he advocates using ""skeptical priors"".)Die-hard Bayesians suggest to use Bayesian methods even if one does not have any informative priors. One recent example is Krushke, 2012, Bayesian estimation supersedes the $t$-test, humbly abbreviated as BEST. The idea is to use a Bayesian model with weak uninformative priors to compute the posterior for the effect of interest (such as, e.g., a group difference). The practical difference with frequentist reasoning seems usually to be minor, and as far as I can see this approach remains unpopular. See What is an ""uninformative prior""? Can we ever have one with truly no information? for the discussion of what is ""uninformative"" (answer: there is no such thing, hence the controversy).An alternative approach, going back to Harold Jeffreys, is based on Bayesian testing (as opposed to Bayesian estimation) and uses Bayes factors. One of the more eloquent and prolific proponents is Eric-Jan Wagenmakers, who has published a lot on this topic in recent years. Two features of this approach are worth emphasizing here. First, see Wetzels et al., 2012, A Default Bayesian Hypothesis Test for ANOVA Designs for an illustration of just how strongly the outcome of such a Bayesian test can depend on the specific choice of the alternative hypothesis $H_1$ and the parameter distribution (""prior"") it posits. Second, once a ""reasonable"" prior is chosen (Wagenmakers advertises Jeffreys' so called ""default"" priors), resulting Bayes factors often turn out to be quite consistent with the standard $p$-values, see e.g. this figure from this preprint by Marsman & Wagenmakers:So while Wagenmakers et al. keep insisting that $p$-values are deeply flawed and Bayes factors are the way to go, one cannot but wonder... (To be fair, the point of Wetzels et al. 2011 is that for $p$-values close to $0.05$ Bayes factors only indicate very weak evidence against the null; but note that this can be easily dealt with in a frequentist paradigm simply by using a more stringent $\alpha$, something that a lot of people are advocating anyway.)One of the more popular papers by Wagenmakers et al. in the defense of Bayes factors is 2011, Why psychologists must change the way they analyze their data: The case of psi where he argues that infamous Bem's paper on predicting the future would not have reached their faulty conclusions if  only they had used Bayes factors instead of $p$-values. See this thoughtful blog post by Ulrich Schimmack for a detailed (and IMHO convincing) counter-argument: Why Psychologists Should Not Change The Way They Analyze Their Data: The Devil is in the Default Prior.See also The Default Bayesian Test is Prejudiced Against Small Effects blog post by Uri Simonsohn.For completeness, I mention that Wagenmakers 2007, A practical solution to the pervasive
problems of $p$-values suggested to use BIC as an approximation to Bayes factor to replace the $p$-values. BIC does not depend on the prior and hence, despite its name, is not really Bayesian; I am not sure what to think about this proposal. It seems that more recently Wagenmakers is more in favour of Bayesian tests with uninformative Jeffreys' priors, see above.For further discussion of Bayes estimation vs. Bayesian testing, see Bayesian parameter estimation or Bayesian hypothesis testing? and links therein.Among the ASA disputants, this is explicitly suggested by Benjamin & Berger and by Valen Johnson (the only two papers that are all about suggesting a concrete alternative). Their specific suggestions are a bit different but they are similar in spirit.The ideas of Berger go back to the Berger & Sellke 1987 and there is a number of papers by Berger, Sellke, and collaborators up until last year elaborating on this work. The idea is that under a spike and slab prior where point null $\mu=0$ hypothesis gets probability $0.5$ and all other values of $\mu$ get probability $0.5$ spread symmetrically around $0$ (""local alternative""), then the minimal posterior $p(H_0)$ over all local alternatives, i.e. the minimal Bayes factor, is much higher than the $p$-value. This is the basis of the (much contested) claim that $p$-values ""overstate the evidence"" against the null. The suggestion is to use a lower bound on Bayes factor in favour of the null instead of the $p$-value; under some broad assumptions this lower bound turns out to be given by $-ep\log(p)$, i.e., the $p$-value is effectively multiplied by $-e\log(p)$ which is a factor of around $10$ to $20$ for the common range of $p$-values. This approach has been endorsed by Steven Goodman too.Later update: See a nice cartoon explaining these ideas in a simple way.Even later update: See Held & Ott, 2018, On $p$-Values and Bayes Factors for a comprehensive review and further analysis of converting $p$-values to minimum Bayes factors. Here is one table from there:Valen Johnson suggested something similar in his PNAS 2013 paper; his suggestion approximately boils down to multiplying $p$-values by $\sqrt{-4\pi\log(p)}$ which is around $5$ to $10$.For a brief critique of Johnson's paper, see Andrew Gelman's and @Xi'an's reply in PNAS. For the counter-argument to Berger & Sellke 1987, see Casella & Berger 1987 (different Berger!). Among the APA discussion papers, Stephen Senn argues explicitly against any of these approaches:Error probabilities are not posterior probabilities. Certainly, there is much more to statistical analysis than $P$-values but they should be left alone rather than being deformed in some way to become second class Bayesian posterior probabilities.See also references in Senn's paper, including the ones to Mayo's blog.The ""other approaches"" section ignores the fact that the assumptions of
some of those methods are identical to those of $p$-values. Indeed, some of
the methods use $p$-values as input (e.g., the False Discovery Rate).I am highly skeptical that there is anything that can replace $p$-values in actual scientific practice such that the problems that are often associated with $p$-values (replication crisis, $p$-hacking, etc.) would go away. Any fixed decision procedure, e.g. a Bayesian one, can probably be ""hacked"" in the same way as $p$-values can be $p$-hacked (for some discussion and demonstration of this see this 2014 blog post by Uri Simonsohn).To quote from Andrew Gelman's discussion paper:In summary, I agree with most of the ASA’s statement on $p$-values but I feel that the problems
are deeper, and that the solution is not to reform $p$-values or to replace them with some other
statistical summary or threshold, but rather to move toward a greater acceptance of uncertainty
and embracing of variation.And from Stephen Senn:In short, the problem is less with $P$-values per se but with making an idol of them. Substituting another false god will not help.And here is how Cohen put it into his well-known and highly-cited (3.5k citations) 1994 paper The Earth is round ($p<0.05$) where he argued very strongly against $p$-values:[...] don't look for a magic alternative to NHST, some other objective mechanical ritual to replace it. It doesn't exist."
What skills are required to perform large scale statistical analyses?,"
Many statistical jobs ask for experience with large scale data. What are the sorts of statistical and computational skills that would be need for working with large data sets. For example, how about building regression models given a data set with 10 million samples?
","['regression', 'machine-learning', 'multivariate-analysis', 'large-data']","Good answers have already appeared.  I will therefore just share some thoughts based on personal experience: adapt the relevant ones to your own situation as needed.For background and context--so you can account for any personal biases that might creep in to this message--much of my work has been in helping people make important decisions based on relatively small datasets.  They are small because the data can be expensive to collect (10K dollars for the first sample of a groundwater monitoring well, for instance, or several thousand dollars for analyses of unusual chemicals).  I'm used to getting as much as possible out of any data that are available, to exploring them to death, and to inventing new methods to analyze them if necessary.  However, in the last few years I have been engaged to work on some fairly large databases, such as one of socioeconomic and engineering data covering the entire US at the Census block level (8.5 million records, 300 fields) and various large GIS databases (which nowadays can run from gigabytes to hundreds of gigabytes in size).With very large datasets one's entire approach and mindset change.  There are now too much data to analyze.  Some of the immediate (and, in retrospect) obvious implications (with emphasis on regression modeling) includeAny analysis you think about doing can take a lot of time and computation.  You will need to develop methods of subsampling and working on partial datasets so you can plan your workflow when computing with the entire dataset.  (Subsampling can be complicated, because you need a representative subset of the data that is as rich as the entire dataset.  And don't forget about cross-validating your models with the held-out data.)Because of this, you will spend more time documenting what you do and scripting everything (so that it can be repeated).  As @dsimcha has just noted, good programming skills are useful.  Actually, you don't need much in the way of experience with programming environments, but you need a willingness to program, the ability to recognize when programming will help (at just about every step, really) and a good understanding of basic elements of computer science, such as design of appropriate data structures and how to analyze computational complexity of algorithms.  That's useful for knowing in advance whether code you plan to write will scale up to the full dataset.Some datasets are large because they have many variables (thousands or tens of thousands, all of them different).  Expect to spend a great deal of time just summarizing and understanding the data.  A codebook or data dictionary, and other forms of metadata, become essential.Much of your time is spent simply moving data around and reformatting them.  You need skills with processing large databases and skills with summarizing and graphing large amounts of data.  (Tufte's Small Multiple comes to the fore here.)Some of your favorite software tools will fail.  Forget spreadsheets, for instance.  A lot of open source and academic software will just not be up to handling large datasets: the processing will take forever or the software will crash.  Expect this and make sure you have multiple ways to accomplish your key tasks.Almost any statistical test you run will be so powerful that it's almost sure to identify a ""significant"" effect.  You have to focus much more on statistical importance, such as effect size, rather than significance.Similarly, model selection is troublesome because almost any variable and any interaction you might contemplate is going to look significant.  You have to focus more on the meaningfulness of the variables you choose to analyze.There will be more than enough information to identify appropriate nonlinear transformations of the variables.  Know how to do this.You will have enough data to detect nonlinear relationships, changes in trends, nonstationarity, heteroscedasticity, etc.  You will never be finished.  There are so much data you could study them forever.  It's important, therefore, to establish your analytical objectives at the outset and constantly keep them in mind.I'll end with a short anecdote which illustrates one unexpected difference between regression modeling with a large dataset compared to a smaller one.  At the end of that project with the Census data, a regression model I had developed needed to be implemented in the client's computing system, which meant writing SQL code in a relational database.  This is a routine step but the code generated by the database programmers involved thousands of lines of SQL.  This made it almost impossible to guarantee it was bug free--although we could detect the bugs (it gave different results on test data), finding them was another matter.  (All you need is one typographical error in a coefficient...)  Part of the solution was to write a program that generated the SQL commands directly from the model estimates.  This assured that what came out of the statistics package was exactly what went into the RDBMS.  As a bonus, a few hours spent on writing this script replaced possibly several weeks of SQL coding and testing.  This is a small part of what it means for the statistician to be able to communicate their results."
How do you calculate precision and recall for multiclass classification using confusion matrix?,"
I wonder how to compute precision and recall using a confusion matrix for a multi-class classification problem. Specifically, an observation can only be assigned to its most probable class / label. I would like to compute: 

Precision = TP / (TP+FP)  
Recall = TP / (TP+FN)

for each class, and then compute the micro-averaged F-measure.
","['machine-learning', 'classification', 'precision-recall', 'multi-class']","In a 2-hypothesis case, the confusion matrix is usually:where I've used something similar to your notation:From the raw data, the values in the table would typically be the counts for each occurrence over the test data.  From this, you should be able to compute the quantities you need.EditThe generalization to multi-class problems is to sum over rows / columns of the confusion matrix.  Given that the matrix is oriented as above, i.e., that
a given row of the matrix corresponds to specific value for the ""truth"", we have:$\text{Precision}_{~i} = \cfrac{M_{ii}}{\sum_j M_{ji}}$$\text{Recall}_{~i} = \cfrac{M_{ii}}{\sum_j M_{ij}}$That is, precision is the fraction of events where we correctly declared $i$
out of all instances where the algorithm declared $i$.  Conversely, recall is the fraction of events where we correctly declared $i$ out of all of the cases where the true of state of the world is $i$."
"If 900 out of 1000 people say a car is blue, what is the probability that it is blue?","
This initially arose in connection some work we are doing to a model to classify natural text, but I've simplified it... Perhaps too much.
You have a blue car (by some objective scientific measure - it is blue).
You show it to 1000 people.
900 say it is blue. 100 do not.
You give this information to someone who cannot see the car. All they know is that 900 people said it was blue, and 100 did not. You know nothing more about these people (the 1000).
Based on this, you ask the person, ""What is the probability that the car is blue?""
This has caused a huge divergence of opinion amongst those I have asked! What is the right answer, if there is one?
",['probability'],"TL;DR: Unless you assume people are unreasonably bad at judging car color, or that blue cars are unreasonably rare, the large number of people in your example means the probability that the car is blue is basically 100%.Matthew Drury already gave the right answer but I'd just like to add to that with some numerical examples, because you chose your numbers such that you actually get pretty similar answers for a wide range of different parameter settings. For example, let's assume, as you said in one of your comments, that the probability that people judge the color of a car correctly is 0.9. That is: 
$$p(\text{say it's blue}|\text{car is blue})=0.9=1-p(\text{say it isn't blue}|\text{car is blue})$$
and also
$$p(\text{say it isn't blue}|\text{car isn't blue})=0.9=1-p(\text{say it is blue}|\text{car isn't blue})$$Having defined that, the remaining thing we have to decide is: what is the prior probability that the car is blue? Let's pick a very low probability just to see what happens, and say that $p(\text{car is blue})=0.001$, i.e. only 0.1% of all cars are blue. Then the posterior probability that the car is blue can be calculated as:\begin{align*}
&p(\text{car is blue}|\text{answers})\\
&=\frac{p(\text{answers}|\text{car is blue})\,p(\text{car is blue})}{p(\text{answers}|\text{car is blue})\,p(\text{car is blue})+p(\text{answers}|\text{car isn't blue})\,p(\text{car isn't blue})}\\
&=\frac{0.9^{900}\times 0.1^{100}\times0.001}{0.9^{900}\times 0.1^{100}\times0.001+0.1^{900}\times0.9^{100}\times0.999}
\end{align*}If you look at the denominator, it's pretty clear that the second term in that sum will be negligible, since the relative size of the terms in the sum is dominated by the ratio of $0.9^{900}$ to $0.1^{900}$, which is on the order of $10^{58}$. And indeed, if you do this calculation on a computer (taking care to avoid numerical underflow issues) you get an answer that is equal to 1 (within machine precision). The reason the prior probabilities don't really matter much here is because you have so much evidence for one possibility (the car is blue) versus another. This can be quantified by the likelihood ratio, which we can calculate as:
$$
\frac{p(\text{answers}|\text{car is blue})}{p(\text{answers}|\text{car isn't blue})}=\frac{0.9^{900}\times 0.1^{100}}{0.1^{900}\times 0.9^{100}}\approx 10^{763}
$$So before even considering the prior probabilities, the evidence suggests that one option is already astronomically more likely than the other, and for the prior to make any difference, blue cars would have to be unreasonably, stupidly rare (so rare that we would expect to find 0 blue cars on earth).So what if we change how accurate people are in their descriptions of car color? Of course, we could push this to the extreme and say they get it right only 50% of the time, which is no better than flipping a coin. In this case, the posterior probability that the car is blue is simply equal to the prior probability, because the people's answers told us nothing. But surely people do at least a little better than that, and even if we say that people are accurate only 51% of the time, the likelihood ratio still works out such that it is roughly $10^{13}$ times more likely for the car to be blue. This is all a result of the rather large numbers you chose in your example. If it had been 9/10 people saying the car was blue, it would have been a very different story, even though the same ratio of people were in one camp vs. the other. Because statistical evidence doesn't depend on this ratio, but rather on the numerical difference between the opposing factions. In fact, in the likelihood ratio (which quantifies the evidence), the 100 people who say the car isn't blue exactly cancel 100 of the 900 people who say it is blue, so it's the same as if you had 800 people all agreeing it was blue. And that's obviously pretty clear evidence.(Edit: As Silverfish pointed out, the assumptions I made here actually implied that whenever a person describes a non-blue car incorrectly, they will default to saying it's blue. This isn't realistic of course, because they could really say any color, and will say blue only some of the time. This makes no difference to the conclusions though, since the less likely people are to mistake a non-blue car for a blue one, the stronger the evidence that it is blue when they say it is. So if anything, the numbers given above are actually only a lower bound on the pro-blue evidence.)"
"When should linear regression be called ""machine learning""?","
In a recent colloquium, the speaker's abstract claimed they were using machine learning. During the talk, the only thing related to machine learning was that they perform linear regression on their data. After calculating the best-fit coefficients in 5D parameter space, they compared these coefficients in one system to the best-fit coefficients of other systems. 
When is linear regression machine learning, as opposed to simply finding a best-fit line? (Was the researcher's abstract misleading?)
With all the attention machine learning has been garnering recently, it seems important to make such distinctions. 
My question is like this one, except that that question asks for the definition of ""linear regression"", whereas mine asks when linear regression (which has a broad number of applications) may appropriately be called ""machine learning"". 
Clarifications
I'm not asking when linear regression is the same as machine learning. As some have pointed out, a single algorithm does not constitute a field of study. I'm asking when it's correct to say that one is doing machine learning when the algorithm one is using is simply a linear regression. 
All jokes aside (see comments), one of the reasons I ask this is because it is unethical to say that one is doing machine learning to add a few gold stars to your name if they aren't really doing machine learning. (Many scientists calculate some type of best-fit line for their work, but this does not mean that they are doing machine learning.) On the other hand, there are clearly situations when linear regression is being used as part of machine learning. I'm looking for experts to help me classify these situations. ;-) 
","['regression', 'machine-learning', 'multiple-regression', 'terminology', 'definition']","Answering your question with a question: what exactly is machine learning? Trevor Hastie, Robert Tibshirani and Jerome Friedman in The Elements of
Statistical Learning, Kevin P. Murphy in Machine Learning A Probabilistic Perspective, Christopher Bishop in Pattern Recognition and Machine Learning,  Ian Goodfellow, Yoshua Bengio and Aaron Courville in Deep Learning and a number of other machine learning ""bibles"" mention linear regression as one of the machine learning ""algorithms"". Machine learning is partly a buzzword for applied statistics and the distinction between statistics and machine learning is often blurry."
"What's a real-world example of ""overfitting""?","
I kind of understand what ""overfitting"" means, but I need help as to how to come up with a real-world example that applies to overfitting. 
",['overfitting'],
How does the reparameterization trick for VAEs work and why is it important?,"
How does the reparameterization trick for variational autoencoders (VAE) work? Is there an intuitive and easy explanation without simplifying the underlying math? And why do we need the 'trick'? 
","['mathematical-statistics', 'autoencoders', 'variational-bayes', 'generative-models']",
T-test for non normal when N>50?,"
Long ago I learnt that normal distribution was necessary to use a two sample T-test. Today a colleague told me that she learnt that for N>50 normal distribution was not necessary. Is that true?
If true is that because of the central limit theorem?
","['hypothesis-testing', 'normal-distribution', 't-test', 'inference', 'central-limit-theorem']","Normality assumption of a t-testConsider a large population from which you could take many different samples of a particular size. (In a particular study, you generally collect just one of these samples.)The t-test assumes that the means of the different samples are normally distributed; it does not assume that the population is normally distributed.By the central limit theorem, means of samples from a population with finite variance approach a normal distribution regardless of the distribution of the population. Rules of thumb say that the sample means are basically normally distributed as long as the sample size is at least 20 or 30. For a t-test to be valid on a sample of smaller size, the population distribution would have to be approximately normal.The t-test is invalid for small samples from non-normal distributions, but it is valid for large samples from non-normal distributions.Small samples from non-normal distributionsAs Michael notes below, sample size needed for the distribution of means to approximate normality depends on the degree of non-normality of the population. For approximately normal distributions, you won't need as large sample as a very non-normal distribution.Here are some simulations you can run in R to get a feel for this. First, here are a couple of population distributions.Next are some simulations of samples from the population distributions. In each of these lines, ""10"" is the sample size, ""100"" is the number of samples and the function after that specifies the population distribution. They produce histograms of the sample means.For a t-test to be valid, these histograms should be normal.Utility of a t-testI have to note that all of the knowledge I just imparted is somewhat obsolete; now that we have computers, we can do better than t-tests. As Frank notes, you probably want to use Wilcoxon tests anywhere you were taught to run a t-test."
What is the difference between zero-inflated and hurdle models?,"
I wonder if there is a clear-cut difference between the so-called zero-inflated distributions (models) and so-called hurdle-at-zero distributions (models)? The terms occur quite often in the literature and I suspect they are not the same, but would you please explain me the difference in simple terms?  
",['zero-inflation'],"Thank you for the interesting question!Difference: One  limitation  of standard  count models  is  that  the zeros and  the nonzeros (positives) are assumed  to  come from  the same data-generating process.  With hurdle models, these two processes are not constrained  to  be  the  same. The basic  idea is  that  a Bernoulli probability  governs the binary outcome  of  whether  a  count  variate  has a  zero  or positive realization.  If the  realization  is positive, the  hurdle is  crossed,  and  the  conditional  distribution  of the positives is governed  by a  truncated-at-zero  count  data  model. With zero-inflated models, the response variable is modelled as a mixture of a Bernoulli distribution (or call it a point mass at zero) and a Poisson distribution (or any other count distribution supported on
non-negative integers). For more detail and formulae, see, for example, Gurmu and Trivedi (2011) and Dalrymple, Hudson, and Ford (2003).Example: Hurdle models can be motivated by sequential decision-making processes confronted by individuals. You first decide if you need to buy something, and then you decide on the quantity of that something (which must be positive). When you are allowed to (or can potentially) buy nothing after your decision to buy something is an example of a situation where zero-inflated model is appropriate. Zeros may come from two sources: a) no decision to buy; b) wanted to buy but ended up buying nothing (e.g. out of stock).Beta: The hurdle model is a special case of the two-part model described in Chapter
16 of Frees (2011). There, we will see that for two-part models, the amount of health care utilized may be a continuous as well as a count variable. So what has been somewhat confusingly termed ""zero-inflated beta distribution"" in the literature is in fact belongs in the class of two-part distributions and models (so common in actuarial science), which is consistent with the above definition of a hurdle model. This excellent book discussed zero-inflated models in section 12.4.1 and hurdle models in section 12.4.2, with formulas and examples from actuarial applications.History: zero-inflated Poisson (ZIP) models without covariates have a long history (see e.g., Johnson and Kotz, 1969). The general form of ZIP regression models incorporating covariates is due to Lambert (1992). Hurdle models were first proposed by a Canadian statistician Cragg (1971), and later developped further by Mullahy (1986). You may also consider Croston (1972), where positive geometric counts are used together with Bernoulli process to describe an integer-valued process dominated by zeros.R: Finally, if you use R, there is package pscl for ""Classes and Methods for R developed in the Political Science Computational Laboratory"" by Simon Jackman, containing hurdle() and zeroinfl() functions by Achim Zeileis.The following references have been consulted to produce the above: "
What is the role of the logarithm in Shannon's entropy?,"
Shannon's entropy is the negative of the sum of the probabilities of each outcome multiplied by the logarithm of probabilities for each outcome. What purpose does the logarithm serve in this equation?
An intuitive or visual answer (as opposed to a deeply mathematical answer) will be given bonus points!
","['intuition', 'entropy', 'information-theory', 'sequence-analysis', 'diversity']",
What misused statistical terms are worth correcting?,"
Statistics is everywhere; common usage of statistical terms is, however, often unclear.
The terms probability and odds are used interchangeable in lay English despite their well-defined and different mathematical expressions.
Not separating the term likelihood from probability routinely confuses physicians trying to quantify the probability of breast cancer given a positive mammography, “Oh, what nonsense. I can’t do this. You should test my daughter; she is studying medicine.” 
Equally spread is the use of correlation instead of association. Or correlation implying causation. 
In Al Gore's famous documentary An Inconvenient Truth, a slide illustrates the correlation of ice core $\small \text{CO}_2$ and temperatures, leaving the more technical work to prove causation out of the discussion:


QUESTION: Which statistical terms pose interpretation problems when used without mathematical rigor, and are, therefore, worth correcting?

",['terminology'],"It can be futile to fight against shifts in language. Butparameter does not mean variableIn classical statistics, which in this case starts precisely with R.A. Fisher who first used the term with this meaning, a parameter is an unknown constant to be estimated, say a population mean or correlation. In mathematics, there are related but not identical meanings, as when a curve is given parametrically. In many sciences, parameter is just another word for a measure (itself a term dense with mathematical meaning), property or variable, say length or conductivity or porosity or virtue, as the case may be. Naturally, an individual's length or virtue is unknown before it is measured. but statistically minded people can be bemused by its use for a set of such measurements. In ordinary or vulgar parlance, parameters (almost always plural) often mean the limits of something, say a personal relationship or a political policy, perhaps stemming from some original confusion with perimeter. With high prior probability it is to be presumed that Bayesians will speak for themselves on their own usages (grateful nod to @conjugateprior).skewed does not mean biasedFor a century or more, skewness has had a specific statistical sense of referring to asymmetry of distributions, whether assessed graphically, measured numerically, or presumed theoretically as a matter of faith or hope. For much longer, or so it may be guessed, bias has meant being wrong on average, which -- so long as we know the truth, meaning a true or correct value -- can be quantified as systematic error. Skewed in ordinary language has a common sense of being warped or distorted, and thus of being incorrect, wrong and so also biased too. That sense (so far as I have noticed, only quite recently) has begun filtering back into statistical discussions, so that the original meaning of skewness is in some danger of being blurred or submerged.correlation does not mean agreementCorrelation has attracted several precise senses in statistics, which have in common an idea of a bivariate relationship perfect in some precise sense: the leading cases are linear and monotone relationship. It is often diluted, even in statistical discussions, to mean almost any kind of relationship or association. What correlation does not mean, necessarily, is agreement: thus $y = a + bx$ implies Pearson correlation of  $1$ or $-1$ so long as $b \ne 0$, but agreement $y = x$ requires the very strict condition $a =0, b= 1$.unique does not mean distinctIt is quite common to talk about the distinct values of data as unique, but unique is still ideally better preserved as meaning occurring once only. My own guess is that some of the blame stems from the Unix [sic] utility uniq and its imitators, which reduce possibly repeated values to a set in which each value really is unique. The usage, on this guess, conflates input and output of a program. (Conversely, if we talk of duplicates in data, we rarely restrict ourselves to doubletons that occur precisely twice. The term replicates would make more sense linguistically but has been
pre-empted for deliberate replication of  controls in experiments; the resulting response values are usually not at all identical, which is much of the point.)samples are rarely repeatedIn statistics, a sample includes several values, and repeated sampling is a high theoretical virtue, but one rarely practised, except by simulation, which is our customary term for any kind of faking in silico. In many sciences, a sample is a single object, consisting of a lump, chunk or dollop of water, soil, sediment, rock, blood, tissue, or other substances varying from attractive through benign to disgusting; far from being exceptional, taking many samples may be essential for any serious analysis. Here every field's terminology makes perfect sense to its people, but translation is sometimes needed.error does not usually mean mistake; as Harold Jeffreys pointed out, the primary sense is erratic, not erroneous.Nevertheless, we should be wary of our own sins or quirks of terminology:expected values or expectations (for means over the possible outcomes) may not be what you expect at all, and could even be impossible: in tossing a die fairly with outcomes 1 to 6, the expected value is 3.5regression is not going backwardsstationary does not mean immobile or fixedconfidence has nothing to do with anyone's mental or psychological statesignificance has only sometimes its everyday meaningexact is often an honorific term, referring to a conveniently tractable solution or calculation rather than one appropriate to the problemright-skewed distributions to many look skewed left, and vice versa (and the terminology of right and left for skewness assumes that you are looking at something like a conventional histogram, with horizontal magnitude axis)the lognormal is so called because it's an exponentiated normalbut the lognormal is more normal than the normalthe Gaussian was discovered by De MoivrePoisson didn't discover the Poisson, let alone Poisson regressionthe bootstrap won't help you with your footwearthe jackknife doesn't cutkurtosis is not a medical conditionstem-and-leaf plots don't refer to plantsa dummy variable is useful, not pointless or stupidwho on Earth (or anywhere else) thinks that heteroscedasticity is really a preferable term over  unequal variability?robust now has at least two major technical meanings for different groups, neither of which inhibits its frequent use, even in technical discussions, to mean merely something like ""asserted to behave well""IV now has at least two major meanings for different groupsfactor now has at least two major meanings for different groupsnormalize and standardize have uncountably many meanings (we really need to standardize there)versus describing a graph means vertical variable versus horizontal variable, unless it means the oppositeand (last but not least, to coin a phrase) statistics has at least three major meanings.Notes:Despite any appearances to the contrary, I think this is a good, serious question.Fashions shift. Well into the twentieth century, it seems that many people (no names, no pack-drill, but Karl Pearson could be mentioned) could only invent terms by reaching for their Greek and Latin dictionaries. (It would be unfair not to give him credit for scatter plot.) But R.A. Fisher did hijack many pre-existing English words, including variance, sufficiency, efficiency and likelihood. More recently, J.W. Tukey was a master in using homely terms, but few should feel distress that sploms and badmandments did not catch on.One comment is based on recollection of ""Life is [...] Multiplicative rather than additive: the log normal distribution is more normal than the normal."" Anon. 1962. Bloggins's working rules. In Good, I.J. (Ed.) The scientist speculates: an anthology of partly-baked ideas. London: Heinemann, 212-213 (quotation on p.213)."
"What is rank deficiency, and how to deal with it?","
Fitting a logistic regression using lme4 ends with 
Error in mer_finalize(ans) : Downdated X'X is not positive definite. 

A likely cause of this error is apparently rank deficiency. What is rank deficiency, and how should I address it?
","['r', 'logistic', 'lme4-nlme']","Rank deficiency in this context says there is insufficient information contained in your data to estimate the model you desire. It stems from many origins. I'll talk here about modeling in a fairly general context, rather than explicitly logistic regression, but everything still applies to the specific context.The deficiency may stem from simply too little data. In general, you cannot uniquely estimate n parameters with less than n data points. That does not mean that all you need are n points, as if there is any noise in the process, you would get rather poor results. You need more data to help the algorithm to choose a solution that will represent all of the data, in a minimum error sense. This is why we use least squares tools. How much data do you need? I was always asked that question in a past life, and the answer was more than you have, or as much as you can get. :)Sometimes you may have more data than you need, but some (too many) points are replicates. Replication is GOOD in the sense that it helps to reduce the noise, but it does not help to increase numerical rank. Thus, suppose you have only two data points. You cannot estimate a unique quadratic model through the points. A million replicates of each point will still not allow you to fit more than a straight line, through what are still only effectively a pair of points. Essentially, replication does not add information content. All it does is decrease noise at locations where you already have information.Sometimes you have information in the wrong places. For example, you cannot fit a two dimensional quadratic model if all you have are points that all lie in a straight line in two dimensions. That is, suppose you have points scattered only along the line x = y in the plane, and you wish to fit a model for the surface z(x,y). Even with zillions of points (not even replicates) will you have sufficient information to intelligently estimate more than a constant model. Amazingly, this is a common problem that I've seen in sampled data. The user wonders why they cannot build a good model. The problem is built into the very data they have sampled.Sometimes it is simply choice of model. This can be viewed as ""not enough data"", but from the other side. You wish to estimate a complicated model, but have provided insufficient data to do so.In all of the above instances the answer is to get more data, sampled intelligently from places that will provide information about the process that you currently lack. Design of experiments is a good place to start.However, even good data is sometimes inadequate, at least numerically so. (Why do bad things happen to good data?) The problem here may be model related. It may lie in nothing more than a poor choice of units. It may stem from the computer programming done to solve the problem. (Ugh! Where to start?)First, lets talk about units and scaling. Suppose I try to solve a problem where one variable is MANY orders of magnitude larger than another. For example, suppose I have a problem that involves my height and my shoe size. I'll measure my height in nanometers. So my height would be roughly 1.78 billion (1.78e9) nanometers. Of course, I'll choose to measure my shoe size in kilo-parsecs, so 9.14e-21 kilo-parsecs. When you do regression modeling, linear regression is all about linear algebra, which involves linear combinations of variables. The problem here is these numbers are different by hugely many orders of magnitude (and not even the same units.) The mathematics will fail when a computer program tries to add and subtract numbers that vary by so many orders of magnitude (for a double precision number, that absolute limit is roughly 16 powers of 10.)The trick is usually to use common units, but on some problems even that is an issue when variables vary by too many orders of magnitude. More important is to scale your numbers to be similar in magnitude.Next, you may see problems with big numbers and small variation in those numbers. Thus, suppose you try to build a moderately high order polynomial model with data where your inputs all lie in the interval [1,2]. Squaring, cubing, etc., numbers that are on the order of 1 or 2 will cause no problems when working in double precision arithmetic. Alternatively, add 1e12 to every number. In theory, the mathematics will allow this. All it does is shift any polynomial model we build on the x-axis. It would have exactly the same shape, but be translated by 1e12 to the right. In practice, the linear algebra will fail miserably due to rank deficiency problems. You have done nothing but translate the data, but suddenly you start to see singular matrices popping up.Usually the comment made will be a suggestion to ""center and scale your data"". Effectively this says to shift and scale the data so that it has a mean near zero and a standard deviation that is roughly 1. That will greatly improve the conditioning of most polynomial models, reducing the rank deficiency issues.Other reasons for rank deficiency exist. In some cases it is built directly into the model. For example, suppose I provide the derivative of a function, can I uniquely infer the function itself? Of course not, as integration involves a constant of integration, an unknown parameter that is generally inferred by knowledge of the value of the function at some point. In fact, this sometimes arises in estimation problems too, where the singularity of a system is derived from the fundamental nature of the system under study.I surely left out a few of the many reasons for rank deficiency in a linear system, and I've prattled along for too long now. Hopefully I managed to explain those I covered in simple terms, and a way to alleviate the problem."
What is the relation between k-means clustering and PCA?,"
It is a common practice to apply PCA (principal component analysis) before a clustering algorithm (such as k-means). It is believed that it improves the clustering results in practice (noise reduction). 
However I am interested in a comparative and in-depth study of the relationship between PCA and k-means. For example, Chris Ding and Xiaofeng He, 2004, K-means Clustering via Principal Component Analysis showed that ""principal components are the continuous
solutions to the discrete cluster membership indicators for K-means clustering"". However, I have hard time understanding this paper, and Wikipedia actually claims that it is wrong.
Also, the results of the two methods are somewhat different in the sense that PCA helps to reduce the number of ""features"" while preserving the variance, whereas clustering reduces the number of ""data-points"" by summarizing several points by their expectations/means (in the case of k-means).
So if the dataset consists in $N$ points with $T$ features each, PCA aims at compressing the $T$ features whereas clustering aims at compressing the $N$ data-points.
I am looking for a layman explanation of the relations between these two techniques + some more technical papers relating the two techniques.
","['clustering', 'pca', 'k-means']","It is true that K-means clustering and PCA appear to have very different goals and at first sight do not seem to be related. However, as explained in the Ding & He 2004 paper K-means Clustering via Principal Component Analysis, there is a deep connection between them.The intuition is that PCA seeks to represent all $n$ data vectors as linear combinations of a small number of eigenvectors, and does it to minimize the mean-squared reconstruction error. In contrast, K-means seeks to represent all $n$ data vectors via small number of cluster centroids, i.e. to represent them as linear combinations of a small number of cluster centroid vectors where linear combination weights must be all zero except for the  single $1$. This is also done to minimize the mean-squared reconstruction error.So K-means can be seen as a super-sparse PCA.Ding & He paper makes this connection more precise.Unfortunately, the Ding & He paper contains some sloppy formulations (at best) and can easily be misunderstood. E.g. it might seem  that Ding & He claim to have proved that cluster centroids of K-means clustering solution lie in the $(K-1)$-dimensional PCA subspace:Theorem 3.3. Cluster centroid subspace is spanned by the first
$K-1$ principal directions [...].For $K=2$ this would imply that projections on PC1 axis will necessarily be negative for one cluster and positive for another cluster, i.e. PC2 axis will separate clusters perfectly.This is either a mistake or some sloppy writing; in any case, taken literally, this particular claim is false.Let's start with looking at some toy examples in 2D for $K=2$. I generated some samples from the two normal distributions with the same covariance matrix but varying means. I then ran both K-means and PCA. The following figure shows the scatter plot of the data above, and the same data colored according to the K-means solution below. I also show the first principal direction as a black line and class centroids found by K-means with black crosses. PC2 axis is shown with the dashed black line. K-means was repeated $100$ times with random seeds to ensure convergence to the global optimum.One can clearly see that even though the class centroids tend to be pretty close to the first PC direction, they do not fall on it exactly. Moreover, even though PC2 axis separates clusters perfectly in subplots 1 and 4, there is a couple of points on the wrong side of it in subplots 2 and 3.So the agreement between K-means and PCA is quite good, but it is not exact.So what did Ding & He prove? For simplicity, I will consider only $K=2$ case. Let the number of points assigned to each cluster be $n_1$ and $n_2$  and the total number of points $n=n_1+n_2$. Following Ding & He, let's define cluster indicator vector $\mathbf q\in\mathbb R^n$  as follows: $q_i = \sqrt{n_2/nn_1}$ if $i$-th points belongs to cluster 1 and $q_i = -\sqrt{n_1/nn_2}$ if it belongs to cluster 2. Cluster indicator vector has unit length $\|\mathbf q\| = 1$ and is ""centered"", i.e. its elements sum to zero $\sum q_i = 0$.Ding & He show that K-means loss function $\sum_k \sum_i (\mathbf x_i^{(k)} - \boldsymbol \mu_k)^2$ (that K-means algorithm minimizes), where $x_i^{(k)}$ is the $i$-th element in cluster $k$, can be equivalently rewritten as $-\mathbf q^\top \mathbf G \mathbf q$, where $\mathbf G$ is the $n\times n$ Gram matrix of scalar products between all points: $\mathbf G = \mathbf X_c \mathbf X_c^\top$, where $\mathbf X$ is the $n\times 2$ data matrix and $\mathbf X_c$ is the centered data matrix.(Note: I am using notation and terminology that slightly differs from their paper but that I find clearer).So the K-means solution $\mathbf q$ is a centered unit vector maximizing $\mathbf q^\top \mathbf G \mathbf q$. It is easy to show that the first principal component (when normalized to have unit sum of squares) is the leading eigenvector of the Gram matrix, i.e. it is also a centered unit vector $\mathbf p$ maximizing $\mathbf p^\top \mathbf G \mathbf p$. The only difference is that $\mathbf q$ is additionally constrained to have only two different values whereas $\mathbf p$ does not have this constraint.In other words, K-means and PCA maximize the same objective function, with the only difference being that K-means has additional ""categorical"" constraint.It stands to reason that most of the times the K-means (constrained) and PCA (unconstrained) solutions will be pretty to close to each other, as we saw above in the simulation, but one should not expect them to be identical. Taking $\mathbf p$ and setting all its negative elements to be equal to $-\sqrt{n_1/nn_2}$ and all its positive elements to $\sqrt{n_2/nn_1}$ will generally not give exactly  $\mathbf q$.Ding & He seem to understand this well because they formulate their theorem as follows:Theorem 2.2. For K-means clustering where $K= 2$, the continuous solution of the cluster indicator vector is the [first] principal componentNote that words ""continuous solution"". After proving this theorem they additionally comment that PCA can be used to initialize K-means iterations which makes total sense given that we expect $\mathbf q$ to be close to $\mathbf p$. But one still needs to perform the iterations, because they are not identical.However, Ding & He then go on to develop a more general treatment for $K>2$ and end up formulating Theorem 3.3 asTheorem 3.3. Cluster centroid subspace is spanned by the first
$K-1$ principal directions [...].I did not go through the math of Section 3, but I believe that this theorem in fact also refers to the ""continuous solution"" of K-means, i.e. its statement should read ""cluster centroid space of the continuous solution of K-means is spanned [...]"".Ding & He, however, do not make this important qualification, and moreover write in their abstract thatHere we prove
that principal components are the continuous
solutions to the discrete cluster membership
indicators for
K-means clustering.  Equivalently, we show that the subspace spanned
by the cluster centroids are given by spectral expansion of the data covariance matrix truncated at $K-1$ terms.The first sentence is absolutely correct, but the second one is not. It is not clear to me if this is a (very) sloppy writing or a genuine mistake. I have very politely emailed both authors asking for clarification. (Update two months later: I have never heard back from them.)"
Mean absolute error OR root mean squared error?,"
Why use Root Mean Squared Error (RMSE) instead of Mean Absolute Error (MAE)??
I've been investigating the error generated in a calculation - I initially calculated the error as a Root Mean Normalised Squared Error.
Looking a little closer, I see the effects of squaring the error gives more weight to larger errors than smaller ones, skewing the error estimate towards the odd outlier. This is quite obvious in retrospect.
In what instance would the Root Mean Squared Error be a more appropriate measure of error than the Mean Absolute Error? The latter seems more appropriate to me or am I missing something?
To illustrate this I have attached an example below:

The scatter plot shows two variables with a good correlation,

the two histograms to the right chart the error between Y(observed )
and Y(predicted) using normalised RMSE (top) and MAE (bottom).



There are no significant outliers in this data and MAE gives a lower error than RMSE. Is there any rational, other than MAE being preferable, for using one measure of error over the other?
","['least-squares', 'mean', 'rms', 'mae']","This depends on your loss function. In many circumstances it makes sense to give more weight to points further away from the mean--that is, being off by 10 is more than twice as bad as being off by 5. In such cases RMSE is a more appropriate measure of error.If being off by ten is just twice as bad as being off by 5, then MAE is more appropriate.In any case, it doesn't make sense to compare RMSE and MAE to each other as you do in your second-to-last sentence (""MAE gives a lower error than RMSE""). MAE will never be higher than RMSE because of the way they are calculated. They only make sense in comparison to the same measure of error: you can compare RMSE for Method 1 to RMSE for Method 2, or MAE for Method 1 to MAE for Method 2, but you can't say MAE is better than RMSE for Method 1 because it's smaller."
What is an embedding layer in a neural network?,"
In many neural network libraries, there are 'embedding layers', like in Keras or Lasagne.
I am not sure I understand its function, despite reading the documentation.  For example, in the Keras documentation it says:

Turn positive integers (indexes) into denses vectors of fixed size, eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]

Could a knowledgeable person explain what it does, and when you would use it?

EDIT:
Regarding pasting in documentation, there is not much to paste from the documentation, hence my question. I don't understand the transformation it does, nor why it should be used. 
Anyway, this is how it's explained in Keras:

Embedding
keras.layers.embeddings.Embedding(input_dim, output_dim,
  init='uniform', input_length=None, weights=None, W_regularizer=None,
  W_constraint=None, mask_zero=False) Turn positive integers (indexes)
  into denses vectors of fixed size, eg. [[4], [20]] -> [[0.25, 0.1],
  [0.6, -0.2]]
Input shape: 2D tensor with shape: (nb_samples, sequence_length).
  Output shape: 3D tensor with shape: (nb_samples, sequence_length,
  output_dim). Arguments:
input_dim: int >= 0. Size of the vocabulary, ie. 1+maximum integer
  index occurring in the input data. output_dim: int >= 0. Dimension of
  the dense embedding

And here it's how it's explained in Lasagne:

A layer for word embeddings. The input should be an integer type
  Tensor variable.
Parameters:    incoming : a Layer instance or a tuple
The layer feeding into this layer, or the expected input shape.
input_size: int
The Number of different embeddings. The last embedding will have index
  input_size - 1.
output_size : int
The size of each embedding.
W : Theano shared variable, expression, numpy array or callable
Initial value, expression or initializer for the embedding matrix.
  This should be a matrix with shape (input_size, output_size). See
  lasagne.utils.create_param() for more information.
Examples
>>> from lasagne.layers import EmbeddingLayer, InputLayer, get_output
>>> import theano
>>> x = T.imatrix()
>>> l_in = InputLayer((3, ))
>>> W = np.arange(3*5).reshape((3, 5)).astype('float32')
>>> l1 = EmbeddingLayer(l_in, input_size=3, output_size=5, W=W)
>>> output = get_output(l1, x)
>>> f = theano.function([x], output)
>>> x_test = np.array([[0, 2], [1, 2]]).astype('int32')
>>> f(x_test) array([[[  0.,   1.,   2.,   3.,   4.],
                      [ 10.,  11.,  12.,  13.,  14.]],
                     [[  5.,   6.,   7.,   8.,   9.],
                      [ 10.,  11.,  12.,  13.,  14.]]], dtype=float32)


","['machine-learning', 'neural-networks', 'python', 'word-embeddings']","==========================================
(source: netdna-ssl.com)I believe it's related to the recent Word2Vec innovation in natural language processing. Roughly, Word2Vec means our vocabulary is discrete and we will learn an map which will embed each word into a continuous vector space. Using this vector space representation will allow us to have a continuous, distributed representation of our vocabulary words. If for example our dataset consists of n-grams, we may now use our continuous word features to create a distributed representation of our n-grams. In the process of training a language model we will learn this word embedding map. The hope is that by using a continuous representation, our embedding will map similar words to similar regions. For example in the landmark paper Distributed Representations of Words and Phrases
and their Compositionality, observe in Tables 6 and 7 that certain phrases have very good nearest neighbour phrases from a semantic point of view. Transforming into this continuous space allows us to use continuous metric notions of similarity to evaluate the semantic quality of our embedding.Let's break down the Lasagne code snippet:x is a matrix of integers. Okay, no problem. Each word in the vocabulary can be represented an integer, or a 1-hot sparse encoding. So if x is 2x2, we have two datapoints, each being a 2-gram.The input layer. The 3 represents the size of our vocabulary. So we have words $w_0, w_1, w_2$ for example.This is our word embedding matrix. It is a 3 row by 5 column matrix with entries 0 to 14.Up until now we have the following interpretation. Our vocabulary has 3 words and we will embed our words into a 5 dimensional vector space. For example, we may represent one word $w_0 = (1,0,0)$, and another word $w_1 = (0, 1, 0)$ and the other word $w_2 = (0, 0, 1)$, e.g. as hot sparse encodings.  We can view the $W$ matrix as embedding these words via matrix multiplication. Therefore the first word $w_0 \rightarrow w_0W = [0, 1, 2, 3, 4].$ Simmilarly $w_1 \rightarrow w_1W = [5, 6, 7, 8, 9]$.It should be noted, due to the one-hot sparse encoding we are using, you also see this referred to as table lookups.The embedding layerSymbolic Theano expression for the embedding.Theano function which computes the embedding.It's worth pausing here to discuss what exactly x_test means. First notice that all of x_test entries are in {0, 1, 2}, i.e. range(3). x_test has 2 datapoints. The first datapoint [0, 2] represents the 2-gram $(w_0, w_2)$ and the second datapoint represents the 2-gram $(w_1, w_2)$.We wish to embed our 2-grams using our word embedding layer now. Before we do that, let's make sure we're clear about what should be returned by our embedding function f. The 2 gram $(w_0, w_2)$ is equivalent to a [[1, 0, 0], [0, 0, 1]] matrix. Applying our embedding matrix W to this sparse matrix should yield: [[0, 1, 2, 3, 4], [10, 11, 12, 13, 14]]. Note in order to have the matrix multiplication work out, we have to apply the word embedding matrix $W$ via right multiplication to the sparse matrix representation of our 2-gram.returns:To convince you that the 3 does indeed represent the vocabulary size, try inputting a matrix x_test = [[5, 0], [1, 2]]. You will see that it raises a matrix mis-match error."
Validation Error less than training error?,"
I found two questions here and here about this issue but there is no obvious answer or explanation yet.I enforce the same problem where the validation error is less than training error in my Convolution Neural Network. What does that mean?
","['machine-learning', 'mathematical-statistics', 'neural-networks', 'cross-validation']","It is difficult to be certain without knowing your actual methodology (e.g. cross-validation method, performance metric, data splitting method, etc.).Generally speaking though, training error will almost always underestimate your validation error.  However it is possible for the validation error to be less than the training.  You can think of it two ways:That is why it is important that you really evaluate your model training methodology.  If you don't split your data for training properly your results will lead to confusing, if not simply incorrect, conclusions.I think of model evaluation in four different categories:Underfitting – Validation and training error highOverfitting – Validation error is high, training error lowGood fit – Validation error low, slightly higher than the training errorUnknown fit - Validation error low, training error 'high'I say 'unknown' fit because the result is counter intuitive to how machine learning works.  The essence of ML is to predict the unknown.  If you are better at predicting the unknown than what you have 'learned', AFAIK the data between training and validation must be different in some way.  This could mean you either need to reevaluate your data splitting method, adding more data, or possibly changing your performance metric (are you actually measuring the performance you want?).EDITTo address the OP's reference to a previous python lasagne question.This suggests that you have sufficient data to not require cross-validation and simply have your training, validation, and testing data subsets.  Now, if you look at the lasagne tutorial you can see that the same behavior is seen at the top of the page.  I would find it hard to believe the authors would post such results if it was strange but instead of just assuming they are correct let's look further.  The section of most interest to us here is in the training loop section, just above the bottom you will see how the loss parameters are calculated.The training loss is calculated over the entire training dataset.  Likewise, the validation loss is calculated over the entire validation dataset.  The training set is typically at least 4 times as large as the validation (80-20).  Given that the error is calculated over all samples, you could expect up to approximately 4X the loss measure of the validation set.  You will notice, however, that the training loss and validation loss are approaching one another as training continues.  This is intentional as if your training error begins to get lower than your validation error you would be beginning to overfit your model!!!I hope this clarifies these errors.  "
"""Best"" series of colors to use for differentiating series in publication-quality plots","
Has any study been done on what are the best set of colors to use for showing multiple series on the same plot?  I've just been using the defaults in matplotlib, and they look a little childish since they're all bright, primary colors.
",['data-visualization'],
Difference between standard error and standard deviation,"
I'm struggling to understand the difference between the standard error and the standard deviation. How are they different and why do you need to measure the standard error?
","['mean', 'standard-deviation', 'standard-error', 'intuition']","To complete the answer to the question, Ocram nicely addressed standard error but did not contrast it to standard deviation and did not mention the dependence on sample size.  As a special case for the estimator consider the sample mean.  The standard error for the mean is $\sigma \, / \, \sqrt{n}$ where $\sigma$ is the population standard deviation.  So in this example we see explicitly how the standard error decreases with increasing sample size. The standard deviation is most often used to refer to the individual observations. So standard deviation describes the variability of the individual observations while standard error shows the variability of the estimator. Good estimators are consistent which means that they converge to the true parameter value. When their standard error decreases to 0 as the sample size increases the estimators are consistent which in most cases happens because the standard error goes to 0 as we see explicitly with the sample mean."
Why do we need to normalize data before principal component analysis (PCA)? [duplicate],"







This question already has answers here:
                                
                            




PCA on correlation or covariance?

                                (7 answers)
                            

Closed 9 years ago.



I'm doing principal component analysis on my dataset and my professor told me that I should normalize the data before doing the analysis. Why?

What would happen If I did PCA without normalization? 
Why do we normalize data in general? 
Could someone give clear and intuitive example which would demonstrate the consequences of not normalizing the data before analysis?

","['pca', 'normalization', 'dimensionality-reduction']","Normalization is important in PCA since it is a variance maximizing exercise. It projects your original data onto directions which maximize the variance. The first plot below shows the amount of total variance explained in the different principal components wher we have not normalized the data. As you can see, it seems like component one explains most of the variance in the data. If you look at the second picture, we have normalized the data first. Here it is clear that the other components contribute as well. The reason for this is because PCA seeks to maximize the variance of each component. And since the covariance matrix of this particular dataset is:From this structure, the PCA will select to project as much as possible in the direction of Assault since that variance is much greater. So for finding features usable for any kind of model, a PCA without normalization would perform worse than one with normalization."
What is the difference between a multiclass and a multilabel problem?,"
What is the difference between a multiclass problem and a multilabel problem?
","['classification', 'clustering', 'terminology', 'multi-class', 'multilabel']","I suspect the difference is that in multi-class problems the classes are mutually exclusive, whereas for multi-label problems each label represents a different classification task, but the tasks are somehow related (so there is a benefit in tackling them together rather than separately).  For example, in the famous leptograspus crabs dataset there are examples of males and females of two colour forms of crab.  You could approach this as a multi-class problem with four classes (male-blue, female-blue, male-orange, female-orange) or as a multi-label problem, where one label would be male/female and the other blue/orange.  Essentially in multi-label problems a pattern can belong to more than one class."
"Are unbalanced datasets problematic, and (how) does oversampling (purport to) help?","
TL;DR
See title.

Motivation
I am hoping for a canonical answer along the lines of ""(1) No, (2) Not applicable, because (1)"", which we can use to close many wrong questions about unbalanced datasets and oversampling. I would be quite as happy to be proven wrong in my preconceptions. Fabulous Bounties await the intrepid answerer.

My argument
I am baffled by the many questions we get in the unbalanced-classes tag. Unbalanced classes seem to be self-evidently bad. And oversampling the minority class(es) is quite as self-evidently seen as helping to address the self-evident problems. Many questions that carry both tags proceed to ask how to perform oversampling in some specific situation.
I understand neither what problem unbalanced classes pose, nor how oversampling is supposed to address these problems.
In my opinion, unbalanced data do not pose a problem at all. One should model class membership probabilities, and these may be small. As long as they are correct, there is no problem. One should, of course, not use accuracy as a KPI to be maximized in a classification problem. Or calculate classification thresholds. Instead, one should assess the quality of the entire predictive distribution using proper scoring-rules. Tetlock's Superforecasting serves as a wonderful and very readable introduction to predicting unbalanced classes, even if this is nowhere explicitly mentioned in the book.

Related
The discussion in the comments has brought up a number of related threads.

What problem does oversampling, undersampling, and SMOTE solve? IMO, this question does not have a satisfactory answer. (Per my suspicion, this may be because there is no problem.)
When is unbalanced data really a problem in Machine Learning? The consensus appears to be ""it isn't"". I'll probably vote to close this question as a duplicate of that one.

IcannotFixThis' answer, seems to presume (1) that the KPI we attempt to maximize is accuracy, and (2) that accuracy is an appropriate KPI for classification model evaluation. It isn't. This may be one key to the entire discussion.
AdamO's answer focuses on the low precision of estimates from unbalanced factors. This is of course a valid concern and probably the answer to my titular question. But oversampling does not help here, any more than we can get more precise estimates in any run-of-the-mill regression by simply duplicating each observation ten times.

What is the root cause of the class imbalance problem? Some of the comments here echo my suspicion that there is no problem. The single answer again implicitly presumes that we use accuracy as a KPI, which I find unsatisfactory.

Are there Imbalanced learning problems where re-balancing/re-weighting demonstrably improves accuracy? is related, but presupposes accuracy as an evaluation measure. (Which I argue is not a good choice.)



Summary
The threads above can apparently be summarized as follows.

Rare classes (both in the outcome and in predictors) are a problem, because parameter estimates and predictions have high variance/low precision. This cannot be addressed through oversampling. (In the sense that it is always better to get more data that is representative of the population, and selective sampling will induce bias per my and others' simulations.)
Rare classes are a ""problem"" if we assess our model by accuracy. But accuracy is not a good measure for assessing classification models. (I did think about including accuracy in my simulations, but then I would have needed to set a classification threshold, which is a closely related wrong question, and the question is long enough as it is.)


An example
Let's simulate for an illustration. Specifically, we will simulate ten predictors, only a single one of which actually has an impact on a rare outcome. We will look at two algorithms that can be used for probabilistic classification: logistic-regression and random-forests.
In each case, we will apply the model either to the full dataset, or to an oversampled balanced one, which contains all the instances of the rare class and the same number of samples from the majority class (so the oversampled dataset is smaller than the full dataset).
For the logistic regression, we will assess whether each model actually recovers the original coefficients used to generate the data. In addition, for both methods, we will calculate probabilistic class membership predictions and assess these on holdout data generated using the same data generating process as the original training data. Whether the predictions actually match the outcomes will be assessed using the Brier score, one of the most common proper scoring rules.
We will run 100 simulations. (Cranking this up only makes the beanplots more cramped and makes the simulation run longer than one cup of coffee.) Each simulation contains $n=10,000$ samples. The predictors form a $10,000\times 10$ matrix with entries uniformly distributed in $[0,1]$. Only the first predictor actually has an impact; the true DGP is
$$ \text{logit}(p_i) = -7+5x_{i1}. $$
This makes for simulated incidences for the minority TRUE class between 2 and 3%:

Let's run the simulations. Feeding the full dataset into a logistic regression, we (unsurprisingly) get unbiased parameter estimates (the true parameter values are indicated by the red diamonds):

However, if we feed the oversampled dataset to the logistic regression, the intercept parameter is heavily biased:

Let's compare the Brier scores between models fitted to the ""raw"" and the oversampled datasets, for both the logistic regression and the Random Forest. Remember that smaller is better:


In each case, the predictive distributions derived from the full dataset are much better than those derived from an oversampled one.
I conclude that unbalanced classes are not a problem, and that oversampling does not alleviate this non-problem, but gratuitously introduces bias and worse predictions.
Where is my error?

A caveat
I'll happily concede that oversampling has one application: if

we are dealing with a rare outcome, and
assessing the outcome is easy or cheap, but
assessing the predictors is hard or expensive

A prime example would be genome-wide association studies (GWAS) of rare diseases. Testing whether one suffers from a particular disease can be far easier than genotyping their blood. (I have been involved with a few GWAS of PTSD.) If budgets are limited, it may make sense to screen based on the outcome and ensure that there are ""enough"" of the rarer cases in the sample.
However, then one needs to balance the monetary savings against the losses illustrated above - and my point is that the questions on unbalanced datasets at CV do not mention such a tradeoff, but treat unbalanced classes as a self-evident evil, completely apart from any costs of sample collection.

R code
    library(randomForest)
    library(beanplot)
    
    nn_train <- nn_test <- 1e4
    n_sims <- 1e2
    
    true_coefficients <- c(-7, 5, rep(0, 9))
    
    incidence_train <- rep(NA, n_sims)
    model_logistic_coefficients <- 
         model_logistic_oversampled_coefficients <- 
         matrix(NA, nrow=n_sims, ncol=length(true_coefficients))
    
    brier_score_logistic <- brier_score_logistic_oversampled <- 
      brier_score_randomForest <- 
    brier_score_randomForest_oversampled <- 
      rep(NA, n_sims)
    
    pb <- winProgressBar(max=n_sims)
    for ( ii in 1:n_sims ) {
        setWinProgressBar(pb,ii,paste(ii,""of"",n_sims))
        set.seed(ii)
        while ( TRUE ) {    # make sure we even have the minority 
                            # class
            predictors_train <- matrix(
              runif(nn_train*(length(true_coefficients) - 1)), 
                  nrow=nn_train)
            logit_train <- 
             cbind(1, predictors_train)%*%true_coefficients
            probability_train <- 1/(1+exp(-logit_train))
            outcome_train <- factor(runif(nn_train) <= 
                     probability_train)
            if ( sum(incidence_train[ii] <- 
               sum(outcome_train==TRUE))>0 ) break
        }
        dataset_train <- data.frame(outcome=outcome_train, 
                          predictors_train)
        
        index <- c(which(outcome_train==TRUE),  
          sample(which(outcome_train==FALSE),   
                sum(outcome_train==TRUE)))
        
        model_logistic <- glm(outcome~., dataset_train, 
                    family=""binomial"")
        model_logistic_oversampled <- glm(outcome~., 
              dataset_train[index, ], family=""binomial"")
        
        model_logistic_coefficients[ii, ] <- 
               coefficients(model_logistic)
        model_logistic_oversampled_coefficients[ii, ] <- 
          coefficients(model_logistic_oversampled)
        
        model_randomForest <- randomForest(outcome~., dataset_train)
        model_randomForest_oversampled <- 
          randomForest(outcome~., dataset_train, subset=index)
        
        predictors_test <- matrix(runif(nn_test * 
            (length(true_coefficients) - 1)), nrow=nn_test)
        logit_test <- cbind(1, predictors_test)%*%true_coefficients
        probability_test <- 1/(1+exp(-logit_test))
        outcome_test <- factor(runif(nn_test)<=probability_test)
        dataset_test <- data.frame(outcome=outcome_test, 
                         predictors_test)
    
        prediction_logistic <- predict(model_logistic, dataset_test, 
                                        type=""response"")
        brier_score_logistic[ii] <- mean((prediction_logistic - 
               (outcome_test==TRUE))^2)
    
        prediction_logistic_oversampled <-      
               predict(model_logistic_oversampled, dataset_test, 
                        type=""response"")
        brier_score_logistic_oversampled[ii] <- 
          mean((prediction_logistic_oversampled - 
                (outcome_test==TRUE))^2)
        
        prediction_randomForest <- predict(model_randomForest, 
            dataset_test, type=""prob"")
        brier_score_randomForest[ii] <-
          mean((prediction_randomForest[,2]-(outcome_test==TRUE))^2)
    
        prediction_randomForest_oversampled <-   
                         predict(model_randomForest_oversampled, 
                                  dataset_test, type=""prob"")
        brier_score_randomForest_oversampled[ii] <- 
          mean((prediction_randomForest_oversampled[, 2] - 
                (outcome_test==TRUE))^2)
    }
    close(pb)
    
    hist(incidence_train, breaks=seq(min(incidence_train)-.5, 
            max(incidence_train) + .5),
      col=""lightgray"",
      main=paste(""Minority class incidence out of"", 
                    nn_train,""training samples""), xlab="""")
    
    ylim <- range(c(model_logistic_coefficients, 
                   model_logistic_oversampled_coefficients))
    beanplot(data.frame(model_logistic_coefficients),
      what=c(0,1,0,0), col=""lightgray"", xaxt=""n"", ylim=ylim,
      main=""Logistic regression: estimated coefficients"")
    axis(1, at=seq_along(true_coefficients),
      c(""Intercept"", paste(""Predictor"", 1:(length(true_coefficients) 
             - 1))), las=3)
    points(true_coefficients, pch=23, bg=""red"")
    
    beanplot(data.frame(model_logistic_oversampled_coefficients),
      what=c(0, 1, 0, 0), col=""lightgray"", xaxt=""n"", ylim=ylim,
      main=""Logistic regression (oversampled): estimated 
              coefficients"")
    axis(1, at=seq_along(true_coefficients),
      c(""Intercept"", paste(""Predictor"", 1:(length(true_coefficients) 
             - 1))), las=3)
    points(true_coefficients, pch=23, bg=""red"")
    
    beanplot(data.frame(Raw=brier_score_logistic, 
            Oversampled=brier_score_logistic_oversampled),
      what=c(0,1,0,0), col=""lightgray"", main=""Logistic regression: 
             Brier scores"")
    beanplot(data.frame(Raw=brier_score_randomForest, 
      Oversampled=brier_score_randomForest_oversampled),
      what=c(0,1,0,0), col=""lightgray"", 
              main=""Random Forest: Brier scores"")

","['unbalanced-classes', 'oversampling', 'faq']","I'd like to start by seconding a statement in the question:... my point is that the questions on unbalanced datasets at CV do not
mention such a tradeoff, but treat unbalanced classes as a
self-evident evil, completely apart from any costs of sample
collection.I also have the same concern, my questions here and here are intended to invite counter-evidence that it is a ""self-evident evil"" the lack of answers (even with a bounty) suggests it isn't.  A lot of blog posts and academic papers don't make this clear either.  Classifiers can have a problem with imbalanced datasets, but only where the dataset is very small, so my answer is concerned with exceptional cases, and does not justify resampling the dataset in general.There is a class imbalance problem, but it is not caused by the imbalance per se, but because there are too few examples of the minority class to adequately describe it's statistical distribution.  As mentioned in the question, this means that the parameter estimates can have high variance, which is true, but that can give rise to a bias in favour of the majority class (rather than affecting both classes equally).  In the case of logistic regression, this is discussed by King and Zeng,3 Gary King and Langche Zeng. 2001. “Logistic Regression in Rare Events Data.” Political Analysis, 9, Pp. 137–163. https://j.mp/2oSEnmf[In my experiments I have found that sometimes there can be a bias in favour of the minority class, but that is caused by wild over-fitting where the class-overlap dissapears due to random sampling, so that doesn't really count and (Bayesian) regularisation ought to fix that]The good thing is that MLE is asymptotically unbiased, so we can expect this bias against the minority class to go away as the overall size of the dataset increases, regardless of the imbalance.As this is an estimation problem, anything that makes estimation more difficult (e.g. high dimensionality) seems likely to make the class imbalance problem worse.Note that probabilistic classifiers (such as logistic regression) and proper scoring rules will not solve this problem as ""popular statistical procedures, such as logistic regression, can sharply underestimate the probability of rare events"" 3.  This means that your probability estimates will not be well calibrated, so you will have to do things like adjust the threshold (which is equivalent to re-sampling or re-weighting the data).So if we look at a logistic regression model with 10,000 samples, we should not expect to see an imbalance problem as adding more data tends to fix most estimation problems.So an imbalance might be problematic, if you have an extreme imbalance and the dataset is small (and/or high dimensional etc.), but in that case it may be difficult to do much about it (as you don't have enough data to estimate how big a correction to the sampling is needed to correct the bias).  If you have lots of data, the only reason to resample is because operational class frequencies are different to those in the training set or different misclassification costs etc.  (if either are unknown or variable, your really ought to use a probabilistic classifier).This is mostly a stub, I hope to be able to add more to it later."
What is covariance in plain language?,"
What is covariance in plain language and how is it linked to the terms dependence, correlation and variance-covariance structure with respect to repeated-measures designs?
","['correlation', 'repeated-measures', 'terminology', 'covariance', 'independence']","Covariance is a measure of how changes in one variable are associated with changes in a second variable. Specifically, covariance measures the degree to which two variables are linearly associated. However, it is also often used informally as a general measure of how monotonically related two variables are. There are many useful intuitive explanations of covariance here. Regarding how covariance is related to each of the terms you mentioned: (1) Correlation is a scaled version of covariance that takes on values in $[-1,1]$ with a correlation of $\pm 1$ indicating perfect linear association and $0$ indicating no linear relationship. This scaling makes correlation invariant to changes in scale of the original variables, (which Akavall points out and gives an example of, +1). The scaling constant is the product of the standard deviations of the two variables.  (2) If two variables are independent, their covariance is $0$. But, having a covariance of $0$ does not imply the variables are independent. This figure (from Wikipedia) $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $ shows several example plots of data that are not independent, but their covariances are $0$. One important special case is that if two variables are jointly normally distributed, then they are independent if and only if they are uncorrelated. Another special case is that pairs of bernoulli variables are uncorrelated if and only if they are independent (thanks @cardinal).(3) The variance/covariance structure (often called simply the covariance structure) in repeated measures designs refers to the structure used to model the fact that repeated measurements on individuals are potentially correlated (and therefore are dependent) - this is done by modeling the entries in the covariance matrix of the repeated measurements. One example is the exchangeable correlation structure with constant variance which specifies that each repeated measurement has the same variance, and all pairs of measurements are equally correlated. A better choice may be to specify a covariance structure that requires two measurements taken farther apart in time to be less correlated (e.g. an autoregressive model). Note that the term covariance structure arises more generally in many kinds of multivariate analyses where observations are allowed to be correlated. "
Detecting a given face in a database of facial images,"
I'm working on a little project involving the faces of twitter users via their profile pictures.
A problem I've encountered is that after I filter out all but the images that are clear portrait photos, a small but significant percentage of twitter users use a picture of Justin Bieber as their profile picture.
In order to filter them out, how can I tell programmatically whether a picture is that of Justin Bieber?
","['machine-learning', 'clustering', 'image-processing']",
"Explain ""Curse of dimensionality"" to a child","
I heard many times about curse of dimensionality, but somehow I'm still unable to grasp the idea, it's all foggy.
Can anyone explain this in the most intuitive way, as you would explain it to a child, so that I (and the others confused as I am) could understand this ones for good?

EDIT:
Now, let's say that the child somehow heard about clustering (for example, they know how to cluster their toys :) ). How would the increase of dimensionality make the job of clustering their toys harder?
For example, they used to consider only the shape of the toy and the color of the toy (one-color toys), but now need to consider the size and the weight of toys also. 
Why is it more difficult for the child to find similar toys?

EDIT 2
For the sake of discussion I need to clarify that by - ""Why is it more difficult for the child to find similar toys"" - I also mean why is the notion of distance lost in high-dimensional spaces?
","['machine-learning', 'dimensionality-reduction', 'high-dimensional']",
Diagnostic plots for count regression,"
What diagnostic plots (and perhaps formal tests) do you find most informative for regressions where the outcome is a count variable?
I'm especially interested in Poisson and negative binomial models, as well as zero-inflated and hurdle counterparts of each. Most of the sources I've found simply plot the residuals vs. fitted values without discussion of what these plots ""should"" look like.
Wisdom and references greatly appreciated. The back story on why I'm asking this, if it's relevant, is my other question.
Related discussions:

Interpreting residual diagnostic plots for glm models?
Assumptions of generalized linear models
GLMs - Diagnostics and Which Family

","['generalized-linear-model', 'residuals', 'negative-binomial-distribution', 'zero-inflation', 'poisson-regression']","Here is what I usually like doing (for illustration I use the overdispersed and not very easily modelled quine data of pupil's days absent from school from MASS):    Test and graph the original count data by plotting observed frequencies and fitted frequencies (see chapter 2 in Friendly) which is supported by the vcd package in R in large parts. For example, with goodfit and a rootogram: or with Ord plots which help in identifying which count data model is underlying (e.g., here the slope is positive and the intercept is positive which speaks for a negative binomial distribution):  or with the ""XXXXXXness"" plots where XXXXX is the distribution of choice, say Poissoness plot (which speaks against Poisson, try also type=""nbinom""):Inspect usual goodness-of-fit measures (such as likelihood ratio statistics vs. a null model or similar):  Check for over / underdispersion by looking at residual deviance/df or at a formal test statistic (e.g., see this answer). Here we have clearly overdispersion: Check for influential and leverage points, e.g., with the influencePlot in the car package. Of course here many points are highly influential because Poisson is a bad model:Check for zero inflation by fitting a count data model and its zeroinflated / hurdle counterpart and compare them (usually with AIC). Here a zero inflated model would fit better than the simple Poisson (again probably due to overdispersion):Plot the residuals (raw, deviance or scaled) on the y-axis vs. the (log) predicted values (or the linear predictor) on the x-axis. Here we see some very large residuals and a substantial deviance of the deviance residuals from the normal (speaking against the Poisson; Edit: @FlorianHartig's answer suggests that normality of these residuals is not to be expected so this is not a conclusive clue):If interested, plot a half normal probability plot of residuals by plotting ordered absolute residuals vs. expected normal values Atkinson (1981). A special feature would be to simulate a reference ‘line’ and envelope with simulated / bootstrapped confidence intervals (not shown though): Diagnostic plots for log linear models for count data (see chapters 7.2 and 7.7 in Friendly's book). Plot predicted vs. observed values perhaps with some interval estimate (I did just for the age groups--here we see again that we are pretty far off with our estimates due to the overdispersion apart, perhaps, in group F3. The pink points are the point prediction $\pm$ one standard error): This should give you much of the useful information about your analysis and most steps work for all standard count data distributions (e.g., Poisson, Negative Binomial, COM Poisson, Power Laws).  "
What is the benefit of breaking up a continuous predictor variable?,"
I'm wondering what the value is in taking a continuous predictor variable and breaking it up (e.g., into quintiles), before using it in a model. 
It seems to me that by binning the variable we lose information.

Is this just so we can model non-linear effects? 
If we kept the variable continuous and it wasn't really a straight linear relationship would we need to come up with some kind of curve to best fit the data?

","['regression', 'continuous-data', 'regression-strategies', 'binning', 'faq']","You're right on both counts. See Frank Harrell's page here for a long list of problems with binning continuous variables. If you use a few bins you throw away a lot of information in the predictors; if you use many you tend to fit wiggles in what should be a smooth, if not linear, relationship, & use up a lot of degrees of freedom. Generally better to use polynomials ($x + x^2 + \ldots$) or splines (piecewise polynomials that join smoothly) for the predictors. Binning's really only a good idea when you'd expect a discontinuity in the response at the cut-points—say the temperature something boils at, or the legal age for driving–, & when the response is flat between them..The value?—well, it's a quick & easy way to take curvature into account without having to think about it, & the model may well be good enough for what you're using it for. It tends to work all right when you've lots of data compared to the number of predictors, each predictor is split into plenty of categories; in this case within each predictor band the range of response is small & the average response is precisely determined.[Edit in response to comments:Sometimes there are standard cut-offs used within a field for a continuous variable: e.g. in medicine blood pressure measurements may be categorized as low, medium or high. There may be many good reasons for using such cut-offs when you present or apply a model. In particular, decision rules are often based on less information than goes into a model, & may need to be simple to apply. But it doesn't follow that these cut-offs are appropriate for binning the predictors when you fit the model.Suppose some response varies continuously with blood pressure. If you define a high blood pressure group as a predictor in your study, the effect you're estimating is the average response over the particular blood-pressures of the individuals in that group. It's not an estimate of the average response of people with high blood pressure in the general population, or of people in the high blood pressure group in another study, unless you take specific measures to make it so. If the distribution of blood pressure in the general population is known, as I imagine it is, you'll do better to calculate the average response of people with high blood pressure in the general population based on predictions from the model with blood pressure as a continuous variable. Crude binning makes your model only approximately generalizable.In general, if you have questions about the behaviour of the response between cut-offs, fit the best model you can first, & then use it to answer them.][With regard to presentation; I think this is a red herring:(1) Ease of presentation doesn't justify bad modelling decisions. (And in the cases where binning is a good modelling decision, it doesn't need additional justification.) Surely this is self-evident. No-one ever recommends taking an important interaction out of a model because it's hard to present.(2) Whatever kind of model you fit, you can still present its results in terms of categories if you think it will aid interpretation. Though ...(3) You have to be careful to make sure it doesn't aid mis-interpretation, for the reasons given above.(4) It's not in fact difficult to present non-linear responses. Personal opinion, clearly, & audiences differ; but I've never seen a graph of fitted response values versus predictor values puzzle someone just because it's curved. Interactions, logits, random effects, multicollinearity, ...—these are all much harder to explain.][An additional point brought up by @Roland is the exactness of the measurement of the predictors; he's suggesting, I think, that categorization may be appropriate when they're not especially precise. Common sense might suggest that you don't improve matters by re-stating them even less precisely, & common sense would be right:
MacCallum et al (2002), ""On the Practice of Dichotomization of Quantitative Variables"", Psychological Methods, 7, 1, pp17–19.]"
How to select kernel for SVM?,"
When using SVM, we need to select a kernel.
I wonder how to select a kernel. Any criteria on kernel selection?
","['machine-learning', 'svm', 'kernel-trick']","The kernel is effectively a similarity measure, so choosing a kernel according to prior knowledge of invariances as suggested by Robin (+1) is a good idea.In the absence of expert knowledge, the Radial Basis Function kernel makes a good default kernel (once you have established it is a problem requiring a non-linear model).The choice of the kernel and kernel/regularisation parameters can be automated by optimising a cross-valdiation based model selection (or use the radius-margin or span bounds).  The simplest thing to do is to minimise a continuous model selection criterion using the Nelder-Mead simplex method, which doesn't require gradient calculation and works well for sensible numbers of hyper-parameters.  If you have more than a few hyper-parameters to tune, automated model selection is likely to result in severe over-fitting, due to the variance of the model selection criterion.  It is possible to use gradient based optimization, but the performance gain is not usually worth the effort of coding it up).Automated choice of kernels and kernel/regularization parameters is a tricky issue, as it is very easy to overfit the model selection criterion (typically cross-validation based), and you can end up with a worse model than you started with.  Automated model selection also can bias performance evaluation, so make sure your performance evaluation evaluates the whole process of fitting the model (training and model selection), for details, see G. C. Cawley and N. L. C. Talbot, Preventing over-fitting in model selection via Bayesian regularisation of the hyper-parameters, Journal of Machine Learning Research, volume 8, pages 841-861, April 2007. (pdf)andG. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, vol. 11, pp. 2079-2107, July 2010.(pdf)"
Loadings vs eigenvectors in PCA: when to use one or another?,"
In principal component analysis (PCA), we get eigenvectors (unit vectors) and eigenvalues. Now, let us define loadings as $$\text{Loadings} = \text{Eigenvectors} \cdot \sqrt{\text{Eigenvalues}}.$$
I know that eigenvectors are just directions and loadings (as defined above) also include variance along these directions. But for my better understanding, I would like to know where I should use loadings instead of eigenvectors? An example would be perfect! 
I have generally only seen people using eigenvectors but every once in a while they use loadings (as defined above) and then I am left feeling that I do not really understand the difference.
",['pca'],"In PCA, you split covariance (or correlation) matrix into scale part (eigenvalues) and direction part (eigenvectors). You may then endow eigenvectors with the scale: loadings. So, loadings are thus become comparable by magnitude with the covariances/correlations observed between the variables, - because what had been drawn out from the variables' covariation now returns back - in the form of the covariation between the variables and the principal components. Actually, loadings are the covariances/correlations between the original variables and the unit-scaled components. This answer shows geometrically what loadings are and what are coefficients associating components with variables in PCA or factor analysis.Loadings:Help you interpret principal components or factors; Because they are the linear combination weights (coefficients) whereby unit-scaled components or factors define or ""load"" a variable.(Eigenvector is just a coefficient of orthogonal transformation or projection, it is devoid of ""load"" within its value. ""Load"" is (information of the amount of) variance, magnitude. PCs are extracted to explain variance of the variables. Eigenvalues are the variances of (= explained by) PCs. When we multiply eigenvector by sq.root of the eivenvalue we ""load"" the bare coefficient by the amount of variance. By that virtue we make the coefficient to be the measure of association, co-variability.)Loadings sometimes are ""rotated"" (e.g. varimax) afterwards to facilitate
interpretability (see also);It is loadings which ""restore"" the original covariance/correlation matrix (see also this thread discussing nuances of PCA and FA in that respect);While in PCA you can
compute values of components both from eigenvectors and loadings, in
factor analysis you compute factor scores out of loadings.And, above all, loading matrix is informative: its vertical sums of
squares are the eigenvalues, components' variances, and its
horizontal sums of squares are portions of the variables' variances
being ""explained"" by the components.Rescaled or standardized loading is the loading divided by the variable's st. deviation; it is the correlation. (If your PCA is correlation-based PCA, loading is equal to the rescaled one, because correlation-based PCA is the PCA on standardized variables.) Rescaled loading squared has the meaning of the contribution of a pr. component into a variable; if it is high (close to 1) the variable is well defined by that component alone.An example of computations done in PCA and FA for you to see.Eigenvectors are unit-scaled loadings; and they are the coefficients (the cosines) of orthogonal transformation (rotation) of variables into principal components or back. Therefore it is easy to compute the components' values (not standardized) with them. Besides that their usage is limited. Eigenvector value squared has the meaning of the contribution of a variable into a pr. component; if it is high (close to 1) the component is well defined by that variable alone.Although eigenvectors and loadings are simply two different ways to normalize coordinates of the same points representing columns (variables) of the data on a biplot, it is not a good idea to mix the two terms. This answer explained why. See also."
Conditional inference trees vs traditional decision trees,"
Can anyone explain the primary differences between conditional inference trees (ctree from party package in R) compared to the more traditional decision tree algorithms (such as rpart in R)?

What makes CI trees different?
Strengths and weaknesses?

Update: I have looked at the paper by Horthorn et al that Chi refers to in the comments. I was not able to follow it completely - can anyone explain how variables are selected using permutations (e.g. what is an influence function)?
","['r', 'machine-learning', 'cart']","For what it's worth:both rpart and ctree recursively perform univariate splits of the dependent variable based on values on a set of covariates. rpart and related algorithms usually employ information measures (such as the Gini coefficient) for selecting the current covariate.ctree, according to its authors (see chl's comments) avoids the following variable selection bias of rpart (and related methods): They tend to select variables that have many possible splits or many missing values. Unlike the others, ctree uses a significance test procedure in order to select variables instead of selecting the variable that maximizes an information measure (e.g. Gini coefficient).The significance test, or better: the multiple significance tests computed at each start of the algorithm (select covariate - choose split - recurse) are permutation tests, that is, the ""the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points."" (from the wikipedia article).Now for the test statistic: it is computed from transformations (including identity, that is, no transform) of the dependent variable and the covariates. You can choose any of a number of transformations for both variables. For the DV (Dependant Variable), the transformation is called the influence function you were asking about.Examples (taken from the paper):small example for a permutation test in R:Now suppose you have a set of covariates, not only one as above. Then calculate p-values for each of the covariates like in the above scheme, and select the one with the smallest p-value. You want to calculate p-values instead of the correlations directly, because you could have covariates of different kinds (e.g. numeric and categorical).Once you have selected a covariate, now explore all possible splits (or often a somehow restricted number of all possible splits, e.g. by requiring a minimal number of elements of the DV before splitting) again evaluating a permutation-based test.ctree comes with a number of possible transformations for both DV and covariates (see the help for Transformations in the party package).so generally the main difference seems to be that ctree uses a covariate selection scheme that is based on statistical theory (i.e. selection by permutation-based significance tests) and thereby avoids a potential bias in rpart, otherwise they seem similar; e.g. conditional inference trees can be used as base learners for Random Forests.This is about as far as I can get. For more information, you really need to read the papers. Note that I strongly recommend that you really know what you're doing when you want to apply any kind of statistical analysis."
Does the variance of a sum equal the sum of the variances?,"
Is it (always) true that
$$\mathrm{Var}\left(\sum\limits_{i=1}^m{X_i}\right) = \sum\limits_{i=1}^m{\mathrm{Var}(X_i)} \>?$$
","['variance', 'faq']","The answer to your question is ""Sometimes, but not in general"". To see this let $X_1, ..., X_n$ be random variables (with finite variances). Then, $$ {\rm var} \left( \sum_{i=1}^{n} X_i \right) = E \left( \left[ \sum_{i=1}^{n} X_i \right]^2 \right) - \left[ E\left( \sum_{i=1}^{n} X_i \right) \right]^2$$Now note that $(\sum_{i=1}^{n} a_i)^2 = \sum_{i=1}^{n} \sum_{j=1}^{n} a_i a_j $, which is clear if you think about what you're doing when you calculate $(a_1+...+a_n) \cdot (a_1+...+a_n)$ by hand. Therefore,$$ E \left( \left[ \sum_{i=1}^{n} X_i \right]^2 \right) = E \left( \sum_{i=1}^{n} \sum_{j=1}^{n} X_i X_j \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} E(X_i X_j) $$similarly, $$ \left[ E\left( \sum_{i=1}^{n} X_i \right) \right]^2 = \left[ \sum_{i=1}^{n} E(X_i) \right]^2 = \sum_{i=1}^{n} \sum_{j=1}^{n} E(X_i) E(X_j)$$so $$ {\rm var} \left( \sum_{i=1}^{n} X_i \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} \big( E(X_i X_j)-E(X_i) E(X_j) \big) = \sum_{i=1}^{n} \sum_{j=1}^{n} {\rm cov}(X_i, X_j)$$by the definition of covariance. Now regarding Does the variance of a sum equal the sum of the variances?: If the variables are uncorrelated, yes: that is, ${\rm cov}(X_i,X_j)=0$ for $i\neq j$, then $$ {\rm var} \left( \sum_{i=1}^{n} X_i \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} {\rm cov}(X_i, X_j) = \sum_{i=1}^{n} {\rm cov}(X_i, X_i) = \sum_{i=1}^{n} {\rm var}(X_i) $$ If the variables are correlated, no, not in general: For example,  suppose $X_1, X_2$ are two random variables each with variance $\sigma^2$ and ${\rm cov}(X_1,X_2)=\rho$ where $0 <  \rho <\sigma^2$. Then ${\rm var}(X_1 + X_2) = 2(\sigma^2 + \rho) \neq 2\sigma^2$, so the identity fails. but it is possible for certain examples: Suppose $X_1, X_2, X_3$ have covariance matrix $$ \left( \begin{array}{ccc} 1 & 0.4 &-0.6 \\
0.4 & 1 & 0.2 \\ 
-0.6 & 0.2 & 1 \\
\end{array} \right) $$ then ${\rm var}(X_1+X_2+X_3) = 3 = {\rm var}(X_1) + {\rm var}(X_2) + {\rm var}(X_3)$Therefore if the variables are uncorrelated then the variance of the sum is the sum of the variances, but converse is not true in general. "
What book would you recommend for non-statistician scientists?,"
What book would you recommend for scientists who are not statisticians?
Clear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.
",['references'],
Correlation between a nominal (IV) and a continuous (DV) variable,"
I have a nominal variable (different topics of conversation, coded as topic0=0 etc) and a number of scale variables (DV) such as the length of a conversation. 
How can I derive correlations between the nominal and scale variables?
","['correlation', 'continuous-data', 'categorical-data']",
Simple algorithm for online outlier detection of a generic time series,"
I am working with a large amount of time series. These time series are basically network measurements coming every 10 minutes, and some of them are periodic (i.e. the bandwidth), while some other aren't (i.e. the amount of routing traffic).
I would like a simple algorithm for doing an online ""outlier detection"". Basically, I want to keep in memory (or on disk) the whole historical data for each time series, and I want to detect any outlier in a live scenario (each time a new sample is captured). What is the best way to achieve these results?
I'm currently using a moving average in order to remove some noise, but then what next? Simple things like standard deviation, mad, ... against the whole data set doesn't work well (I can't assume the time series are stationary), and I would like something more ""accurate"", ideally a black box like:
double outlier_detection(double* vector, double value);
where vector is the array of double containing the historical data, and the return value is the anomaly score for the new sample ""value"" .
","['time-series', 'outliers', 'mathematical-statistics', 'real-time']",
US Election results 2016: What went wrong with prediction models?,"
First it was Brexit, now the US election. Many model predictions were off by a wide margin, and are there lessons to be learned here? As late as 4 pm PST yesterday, the betting markets were still favoring Hillary 4 to 1.
I take it that the betting markets, with real money on the line, should act as an ensemble of all the available prediction models out there. So it's not far-fetched to say these models didn't do a very good job.
I saw one explanation was voters were unwilling to identify themselves as Trump supporters. How could a model incorporate effects like that?
One macro explanation I read is the rise of populism. The question then is how could a statistical model capture a macro trend like that?
Are these prediction models out there putting too much weight on data from polls and sentiment, not enough from where the country is standing in a 100 year view? I am quoting a friend's comments.
","['predictive-models', 'ensemble-learning', 'confounding']","In short, polling is not always easy. This election may have been the hardest. Any time we are trying to do statistical inference, a fundamental question is whether our sample is a good representation of the population of interest. A typical assumption that is required for many types of statistical inference is that of having our sample being a completely random sample from the population of interest (and often, we also need samples to be independent). If these assumptions hold true, we typically have good measures of our uncertainty based on statistical theory. But we definitively do not have these assumptions holding true with polls! We have exactly 0 samples from our population of interest: actual votes cast at election day. In this case, we cannot make any sort of valid inference without further, untestable assumptions about the data. Or at least, untestable until after election day. Do we completely give up and say ""50%-50%!""? Typically, no. We can try to make what we believe are reasonable assumptions about how the votes will be cast. For example, maybe we want to believe that polls are unbiased estimates for the election day votes, plus some certain unbiased temporal noise (i.e., evolving public opinion as time passes). I'm not an expert on polling methods, but I believe this is the type of model 538 uses. And in 2012, it worked pretty well. So those assumptions were probably pretty reasonable. Unfortunately, there's no real way of evaluating those assumptions, outside strictly qualitative reasoning. For more discussion on a similar topic, see the topic of Non-Ignorable missingness. My theory for why polls did so poorly in 2016: the polls were not unbiased estimates of voter day behavior. That is, I would guess that Trump supporters (and likely Brexit supporters as well) were much more distrustful of pollsters. Remember that Mr. Trump actively denounced polls. As such, I think Trump supporters were less likely to report their voting intentions to pollsters than supporters of his opponents. I would speculate that this caused an unforeseen heavy bias in the polls. How could analysts have accounted for this when using the poll data? Based on the poll data alone, there is no real way to do this in a quantitative way. The poll data does not tell you anything about those who did not participate. However, one may be able to improve the polls in a qualitative way, by choosing more reasonable (but untestable) assumptions about the relation between polling data and election day behavior. This is non-trivial and the truly difficult part of being a good pollster (note: I am not a pollster). Also note that the results were very surprising to the pundits as well, so it's not like there were obvious signs that the assumptions were wildly off this time.Polling can be hard. "
How does the correlation coefficient differ from regression slope?,"
I would have expected the correlation coefficient to be the same as a regression slope (beta), however having just compared the two, they are different. How do they differ - what different information do they give?
","['regression', 'correlation']","Assuming you're talking about a simple regression model $$Y_i = \alpha + \beta X_i + \varepsilon_i$$ estimated by least squares, we know from wikipedia that $$ \hat {\beta} = {\rm cor}(Y_i, X_i) \cdot \frac{ {\rm SD}(Y_i) }{ {\rm SD}(X_i) } $$ Therefore the two only coincide when ${\rm SD}(Y_i) = {\rm SD}(X_i)$. That is, they only coincide when the two variables are on the same scale, in some sense. The most common way of achieving this is through standardization, as indicated by @gung.  The two, in some sense give you the same information - they each tell you the strength of the linear relationship between $X_i$ and $Y_i$. But, they do each give you distinct information (except, of course, when they are exactly the same): The correlation gives you a bounded measurement that can be interpreted independently of the scale of the two variables. The closer the estimated correlation is to $\pm 1$, the closer the two are to a perfect linear relationship. The regression slope, in isolation, does not tell you that piece of information. The regression slope gives a useful quantity interpreted as the estimated change in the expected value of $Y_i$ for a given value of $X_i$. Specifically, $\hat \beta$ tells you the change in the expected value of $Y_i$ corresponding to a 1-unit increase in $X_i$. This information can not be deduced from the correlation coefficient alone. "
Feature selection and cross-validation,"
I have recently been reading a lot on this site (@Aniko, @Dikran Marsupial, @Erik) and elsewhere about the problem of overfitting occuring with cross validation - (Smialowski et al 2010 Bioinformatics, Hastie, Elements of statistical learning). 
The suggestion is that any supervised feature selection (using correlation with class labels) performed outside of the model performance estimation using cross validation (or other model estimating method such as bootstrapping) may result in overfitting. 
This seems unintuitive to me - surely if you select a feature set and then evaluate your model using only the selected features using cross validation, then you are getting an unbiased estimate of generalized model performance on those features (this assumes the sample under study are representive of the populatation)?
With this procedure one cannot of course claim an optimal feature set but can one report the performance of the selected feature set on unseen data as valid?
I accept that selecting features based on the entire data set may resuts in some data leakage between test and train sets. But if the feature set is static after initial selection, and no other tuning is being done, surely it is valid to report the cross-validated performance metrics? 
In my case I have 56 features and 259 cases and so #cases > #features. The features are derived from sensor data.
Apologies if my question seems derivative but this seems an important point to clarify.
Edit:
On implementing feature selection within cross validation on the data set detailed above (thanks to the answers below), I can confirm that selecting features prior to cross-validation in this data set introduced a significant bias. This bias/overfitting was greatest when doing so for a 3-class formulation, compared to as 2-class formulation. 
I think the fact that I used stepwise regression for feature selection increased this overfitting; for comparison purposes, on a different but related data set I compared a sequential forward feature selection routine performed prior to cross-validation against results I had previously obtained with feature selection within CV. The results between both methods did not differ dramatically. This may mean that stepwise regression is more prone to overfitting than sequential FS or may be a quirk of this data set.
","['cross-validation', 'feature-selection']","If you perform feature selection on all of the data and then cross-validate, then the test data in each fold of the cross-validation procedure was also used to choose the features and this is what biases the performance analysis.Consider this example.  We generate some target data by flipping a coin 10 times and recording whether it comes down as heads or tails.  Next, we generate 20 features by flipping the coin 10 times for each feature and write down what we get.  We then perform feature selection by picking the feature that matches the target data as closely as possible and use that as our prediction.  If we then cross-validate, we will get an expected error rate slightly lower than 0.5.  This is because we have chosen the feature on the basis of a correlation over both the training set and the test set in every fold of the cross-validation procedure. However, the true error rate is going to be 0.5 as the target data is simply random.  If you perform feature selection independently within each fold of the cross-validation, the expected value of the error rate is 0.5 (which is correct).The key idea is that cross-validation is a way of estimating the generalization performance of a process for building a model, so you need to repeat the whole process in each fold.  Otherwise, you will end up with a biased estimate, or an under-estimate of the variance of the estimate (or both).HTHHere is some MATLAB code that performs a Monte-Carlo simulation of this setup, with 56 features and 259 cases, to match your example, the output it gives is:Biased estimator: erate = 0.429210 (0.397683 - 0.451737)Unbiased estimator: erate = 0.499689 (0.397683 - 0.590734)The biased estimator is the one where feature selection is performed prior to cross-validation, the unbiased estimator is the one where feature selection is performed independently in each fold of the cross-validation.  This suggests that the bias can be quite severe in this case, depending on the nature of the learning task."
Is there an intuitive explanation why multicollinearity is a problem in linear regression?,"
The wiki discusses the problems that arise when multicollinearity is an issue in linear regression. The basic problem is multicollinearity results in unstable parameter estimates which makes it very difficult to assess the effect of independent variables on dependent variables.
I understand the technical reasons behind the problems (may not be able to invert $X' X$, ill-conditioned $X' X$ etc) but I am searching for a more intuitive (perhaps geometric?) explanation for this issue.
Is there a geometric or perhaps some other form of easily understandable explanation as to why multicollinearity is problematic in the context of linear regression?   
","['regression', 'multicollinearity', 'intuition', 'faq']","Consider the simplest case where $Y$ is regressed against $X$ and $Z$ and where $X$ and $Z$ are highly positively correlated. Then the effect of $X$ on $Y$ is hard to distinguish from the effect of $Z$ on $Y$ because any increase in $X$ tends to be associated with an increase in $Z$. Another way to look at this is to consider the equation. If we write $Y = b_0 + b_1X + b_2Z + e$, then the coefficient $b_1$ is the increase in $Y$ for every unit increase in $X$ while holding $Z$ constant. But in practice, it is often impossible to hold $Z$ constant and the positive correlation between $X$ and $Z$ mean that a unit increase in $X$ is usually accompanied by some increase in $Z$ at the same time.A similar but more complicated explanation holds for other forms of multicollinearity."
How scared should we be about convergence warnings in lme4,"
If we a re fitting a glmer we may get a warning that tells us the model is finding a hard time to converge...e.g.
>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00389462 (tol = 0.001)

another way to check convergence discussed in this thread by @Ben Bolker is:
 relgrad <- with(model@optinfo$derivs,solve(Hessian,gradient))
 max(abs(relgrad))
 #[1] 1.152891e-05

if max(abs(relgrad)) is <0.001 then things might be ok... so in this case we have conflicting results? How should we choose between methods and feel safe with our model fits?
On the other hand when we get more extreme values like:
>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 35.5352 (tol = 0.001)

relgrad <- with(model@optinfo$derivs,solve(Hessian,gradient))
max(abs(relgrad))
#[1] 0.002776518

Does this mean we have to ignore the model results/estimates/p-values? Is 0.0027 far too large to proceed? 
When different optimisers give different results and centering of variables / removing parameters (stripping models down to the minimum) does not help but VIFs are low, models not overdispersed, and models results make logical sense based on a priori expectations, it seems hard to know what to do.
Advice on how to interpret the convergence problems, how extreme they need be to really get us worried and possible ways to try manage them beyond those mentioned would be very helpful.
Using:
    R version 3.1.0 (2014-04-10) and lme4_1.1-6
","['r', 'mixed-model', 'lme4-nlme']","Be afraid. Be very afraid. Last year, I interviewed John Nash, the author of optim and optimx, for an article on IBM's DeveloperWorks site. We talked about how optimizers work and why they fail when they fail. He seemed to take it for granted that they often do. That's why the diagnostics are included in the package. He also thought that you need to ""understand your problem"", and understand your data. All of which means that warnings should be taken seriously, and are an invitation to look at your data in other ways.Typically, an optimizer stops searching when it can no longer improve the loss function by a meaningful amount. It doesn't know where to go next, basically. If the gradient of the loss function is not zero at that point, you haven't reached an extremum of any kind. If the Hessian is not positive, but the gradient is zero, you haven't found a minimum, but possibly, you did find a maximum or saddle point. Depending on the optimizer, though, results about the Hessian might not be supplied. In Optimx, if you want the KKT conditions evaluated, you have to ask for them -- they are not evaluated by default. (These conditions look at the gradient and Hessian to see if you really have a minimum.)The problem with mixed models is that the variance estimates for the random effects are constrained to be positive, thus placing a boundary within the optimization region. But suppose a particular random effect is not really needed in your model -- i.e. the variance of the random effect is 0. Your optimizer will head into that boundary,be unable to proceed, and stop with a non-zero gradient. If removing that random effect improved convergence, you will know that was the problem. As an aside, note that asymptotic maximum likelihood theory assumes the MLE is found in an interior point (i.e. not on the boundary of licit parameter values) - so likelihood ratio tests for variance components may not work when indeed the null hypothesis of zero variance is true. Testing can be done using simulation tests, as implemented in package RLRsim.To me, I suspect that optimizers run into problems when there is too little data for the number of parameters, or the proposed model is really not suitable. Think glass slipper and ugly step-sister: you can't shoehorn your data into the model, no matter how hard you try, and something has to give.Even if the data happen to fit the model, they may not have the power to estimate all the parameters. A funny thing happened to me along those lines. I simulated some mixed models to answer a question about what happens if you don't allow the random effects to be correlated when fitting a mixed effects model. I simulated data with a strong correlation between the two random effects, then fit the model both ways with lmer: positing 0 correlations and free correlations. The correlation model fit better than the uncorrelated model, but interestingly, in 1000 simulations, I had 13 errors when fitting the true model and 0 errors when fitting the simpler model. I don't fully understand why this happened (and I repeated the sims to similar results). I suspect that the correlation parameter is fairly useless and the optimizer can't find the value (because it doesn't matter).You asked about what to do when different optimizers give different results. John and I discussed this point. Some optimizers, in his opinion, are just not that good! And all of them have points of weakness -- i.e., data sets that will cause them to fail. This is why he wrote optimx, which includes a variety of optimizers. You can run several on the same data set.If two optimizers give the same parameters, but different diagnostics -- and those parameters make real world sense -- then I would be inclined to trust the parameter values. The difficulty could lie with the diagnostics, which are not fool-proof. If you have not explicitly supplied the gradient function and/or Hessian matrix, the optimizer will need to estimate these from the loss function and the data, which is just something else that can go wrong.If you are getting different parameter values as well, then you might want to try different starting values and see what happens then. Some optimizers and some problems are very sensitive to the starting values. You want to be starting in the ball park."
On the importance of the i.i.d. assumption in statistical learning,"
In statistical learning, implicitly or explicitly, one always assumes that the training set $\mathcal{D} = \{ \bf {X}, \bf{y} \}$ is composed of $N$ input/response tuples $({\bf{X}}_i,y_i)$ that are independently drawn from the same joint distribution $\mathbb{P}({\bf{X}},y)$ with
$$ p({\bf{X}},y) = p( y \vert {\bf{X}}) p({\bf{X}}) $$
and $p( y \vert {\bf{X}})$ the relationship we are trying to capture through a particular learning algorithm. Mathematically, this i.i.d. assumption writes:
\begin{gather}
({\bf{X}}_i,y_i) \sim \mathbb{P}({\bf{X}},y), \forall i=1,...,N \\
({\bf{X}}_i,y_i) \text{ independent of } ({\bf{X}}_j,y_j), \forall i \ne j \in \{1,...,N\}
\end{gather}
I think we can all agree that this assumption is rarely satisfied in practice, see this related SE question and the wise comments of @Glen_b and @Luca.
My question is therefore: 

Where exactly does the i.i.d. assumption becomes critical in practice? 

[Context] 
I'm asking this because I can think of many situations where such a stringent assumption is not needed to train a certain model (e.g. linear regression methods), or at least one can work around the i.i.d. assumption and obtain robust results. Actually the results will usually stay the same, it is rather the inferences that one can draw that will change (e.g. heteroskedasticity and autocorrelation consistent HAC estimators in linear regression: the idea is to re-use the good old OLS regression weights but to adapt the finite-sample behaviour of the OLS estimator to account for the violation of the Gauss-Markov assumptions). 
My guess is therefore that the i.i.d. assumption is required not to be able to train a particular learning algorithm, but rather to guarantee that techniques such as cross-validation can indeed be used to infer a reliable measure of the model's capability of generalising well, which is the only thing we are interested in at the end of the day in statistical learning because it shows that we can indeed learn from the data. Intuitively, I can indeed understand that using cross-validation on dependent data could be optimistically biased (as illustrated/explained in this interesting example). 
For me i.i.d. has thus nothing to do with training a particular model but everything to do with that model's generalisability. This seems to agree with a paper I found by Huan Xu et al, see ""Robustness and Generalizability for Markovian Samples"" here. 
Would you agree with that?
[Example] 
If this can help the discussion, consider the problem of using the LASSO algorithm to perform a smart selection amongst $P$ features given $N$ training samples $({\bf{X}}_i,y_i)$ with $\forall i=1,...,N$
$$ {\bf{X}}_i=[X_{i1},...,X_{iP}] $$
We can further assume that:

The inputs ${\bf{X}}_i$ are dependent hence leading to a violation of the i.i.d. assumption (e.g. for each feature $j=1,..,P$ we observe a $N$ point time series, hence introducing temporal auto-correlation)
The conditional responses $y_i \vert {\bf{X}}_i$ are independent.
We have $P \gg N$. 

In what way(s) does the violation of the i.i.d. assumption can pose problem in that case assuming we plan to determine the LASSO penalisation coefficient $\lambda$ using a cross-validation approach (on the full data set) + use a nested cross-validation to get a feel for the generalisation error of this learning strategy (we can leave the discussion concerning the inherent pros/cons of the LASSO aside, except if it is useful).
","['machine-learning', 'cross-validation', 'non-independent', 'iid']","The i.i.d. assumption about the pairs $(\mathbf{X}_i, y_i)$, $i = 1, \ldots, N$, is often made in statistics and in machine learning. Sometimes for a good reason, sometimes out of convenience and sometimes just because we usually make this assumption. To satisfactorily answer if the assumption is really necessary, and what the consequences are of not making this assumption, I would easily end up writing a book (if you ever easily end up doing something like that). Here I will try to give a brief overview of what I find to be the most important aspects.Let's assume that we want to learn a probability model of $y$ given $\mathbf{X}$, which we call $p(y \mid \mathbf{X})$. We do not make any assumptions about this model a priory, but we will make the minimal assumption that such a model exists such thatWhat is worth noting about this assumption is that the conditional distribution of $y_i$ depends on $i$ only through $\mathbf{X}_i$. This is what makes the model useful, e.g. for prediction. The assumption holds as a consequence of the identically distributed part under the i.i.d. assumption, but it is weaker because we don't make any assumptions about the $\mathbf{X}_i$'s.In the following the focus will mostly be on the role of independence.There are two major approaches to learning a model of $y$ given $\mathbf{X}$. One approach is known as discriminative modelling and the other as generative modelling.For both modelling approaches the working modelling assumption is used to derive or propose learning methods (or estimators). That could be by maximising the (penalised) log-likelihood, minimising the empirical risk or by using Bayesian methods. Even if the working modelling assumption is wrong, the resulting method can still provide a sensible fit of $p(y \mid \mathbf{X})$.Some techniques used together with discriminative modelling, such as bagging (bootstrap aggregation), work by fitting many models to data sampled randomly from the dataset. Without the i.i.d. assumption (or exchangeability) the resampled datasets will not have a joint distribution similar to that of the original dataset. Any dependence structure has become ""messed up"" by the resampling. I have not thought deeply about this, but I don't see why that should necessarily break the method as a method for learning $p(y \mid \mathbf{X})$. At least not for methods based on the working independence assumptions. I am happy to be proved wrong here.A central question for all learning methods is whether they result in models close to $p(y \mid \mathbf{X})$. There is a vast theoretical literature in statistics and machine learning dealing with consistency and error bounds. A main goal of this literature is to prove that the learned model is close to $p(y \mid \mathbf{X})$ when $N$ is large. Consistency is a qualitative assurance, while error bounds provide (semi-) explicit quantitative control of the closeness and give rates of convergence.The theoretical results all rely on assumptions about the joint distribution of the observations in the dataset. Often the working modelling assumptions mentioned above are made (that is, conditional independence for discriminative modelling and i.i.d. for generative modelling). For discriminative modelling, consistency and error bounds will require that the $\mathbf{X}_i$'s fulfil certain conditions. In classical regression one such condition is that $\frac{1}{N} \mathbb{X}^T \mathbb{X} \to \Sigma$ for $N \to \infty$, where $\mathbb{X}$ denotes the design matrix with rows $\mathbf{X}_i^T$. Weaker conditions may be enough for consistency. In sparse learning another such condition is the restricted eigenvalue condition, see e.g. On the conditions used to prove oracle results for the Lasso. The i.i.d. assumption together with some technical distributional assumptions imply that some such sufficient conditions are fulfilled with large probability, and thus the i.i.d. assumption may prove to be a sufficient but not a necessary assumption to get consistency and error bounds for discriminative modelling.The working modelling assumption of independence may be wrong for either of the modelling approaches. As a rough rule-of-thumb one can still expect consistency if the data comes from an ergodic process, and one can still expect some error bounds if the process is sufficiently fast mixing. A precise mathematical definition of these concepts would take us too far away from the main question. It is enough to note that there exist dependence structures besides the i.i.d. assumption for which the learning methods can be proved to work as $N$ tends to infinity.If we have more detailed knowledge about the dependence structure, we may choose to replace the working independence assumption used for modelling with a model that captures the dependence structure as well. This is often done for time series. A better working model may result in a more efficient method.Rather than proving that the learning method gives a model close to $p(y \mid \mathbf{X})$ it is of great practical value to obtain a (relative) assessment of ""how good a learned model is"". Such assessment scores are comparable for two or more learned models, but they will not provide an absolute assessment of how close a learned model is to $p(y \mid \mathbf{X})$. Estimates of assessment scores are typically computed empirically based on splitting the dataset into a training and a test dataset or by using cross-validation.As with bagging, a random splitting of the dataset will ""mess up"" any dependence structure. However, for methods based on the working independence assumptions, ergodicity assumptions weaker than i.i.d. should be sufficient for the assessment estimates to be reasonable, though standard errors on these estimates will be very difficult to come up with.[Edit: Dependence among the variables will result in a distribution of the learned model that differs from the distribution under the i.i.d. assumption. The estimate produced by cross-validation is not obviously related to the generalization error. If the dependence is strong, it will most likely be a poor estimate.]All the above is under the assumption that there is a fixed conditional probability model, $p(y \mid \mathbf{X})$. Thus there cannot be trends or sudden changes in the conditional distribution not captured by $\mathbf{X}$.When learning a model of $y$ given $\mathbf{X}$, independence plays a role asTo understand precisely what alternatives to i.i.d. that are also sufficient is non-trivial and to some extent a research subject."
"What are examples where a ""naive bootstrap"" fails?","
Suppose I have a set of sample data from an unknown or complex distribution, and I want to perform some inference on a statistic $T$ of the data. My default inclination is to just generate a bunch of bootstrap samples with replacement, and calculate my statistic $T$ on each bootstrap sample to create an estimated distribution for $T$.
What are examples where this is a bad idea?
For example, one case where naively performing this bootstrap would fail is if I'm trying to use the bootstrap on time series data (say, to test whether I have significant autocorrelation). The naive bootstrap described above (generating the $i$th datapoint of the nth bootstrap sample series by sampling with replacement from my original series) would (I think) be ill-advised, since it ignores the structure in my original time series, and so we get fancier bootstrap techniques like the block bootstrap.
To put it another way, what is there to the bootstrap besides ""sampling with replacement""?
","['hypothesis-testing', 'confidence-interval', 'bootstrap']","If the quantity of interest, usually a functional of a distribution, is reasonably smooth and your data are i.i.d., you're usually in pretty safe territory. Of course, there are other circumstances when the bootstrap will work as well.What it means for the bootstrap to ""fail""Broadly speaking, the purpose of the bootstrap is to construct an approximate sampling distribution for the statistic of interest. It's not about actual estimation of the parameter. So, if the statistic of interest (under some rescaling and centering) is $\newcommand{\Xhat}{\hat{X}_n}\Xhat$ and $\Xhat \to X_\infty$ in distribution, we'd like our bootstrap distribution to converge to the distribution of $X_\infty$. If we don't have this, then we can't trust the inferences made.The canonical example of when the bootstrap can fail, even in an i.i.d. framework is when trying to approximate the sampling distribution of an extreme order statistic. Below is a brief discussion.Maximum order statistic of a random sample from a $\;\mathcal{U}[0,\theta]$ distributionLet $X_1, X_2, \ldots$ be a sequence of i.i.d. uniform random variables on $[0,\theta]$. Let $\newcommand{\Xmax}{X_{(n)}} \Xmax = \max_{1\leq k \leq n} X_k$. The distribution of $\Xmax$ is 
$$
\renewcommand{\Pr}{\mathbb{P}}\Pr(\Xmax \leq x) = (x/\theta)^n \>.
$$ 
(Note that by a very simple argument, this actually also shows that $\Xmax \to \theta$ in probability, and even, almost surely, if the random variables are all defined on the same space.)An elementary calculation yields
$$
\Pr( n(\theta - \Xmax) \leq x ) = 1 - \Big(1 - \frac{x}{\theta n}\Big)^n \to 1 - e^{-x/\theta} \>,
$$
or, in other words, $n(\theta - \Xmax)$ converges in distribution to an exponential random variable with mean $\theta$.Now, we form a (naive) bootstrap estimate of the distribution of $n(\theta - \Xmax)$ by resampling $X_1, \ldots, X_n$ with replacement to get $X_1^\star,\ldots,X_n^\star$ and using the distribution of $n(\Xmax - \Xmax^\star)$ conditional on $X_1,\ldots,X_n$.But, observe that $\Xmax^\star = \Xmax$ with probability $1 - (1-1/n)^n \to 1 - e^{-1}$, and so the bootstrap distribution has a point mass at zero even asymptotically despite the fact that the actual limiting distribution is continuous.More explicitly, though the true limiting distribution is exponential with mean $\theta$, the limiting bootstrap distribution places a point mass at zero of size $1−e^{-1} \approx 0.632$ independent of the actual value of $\theta$. By taking $\theta$ sufficiently large, we can make the probability of the true limiting distribution arbitrary small for any fixed interval $[0,\varepsilon)$, yet the bootstrap will (still!) report that there is at least probability 0.632 in this interval! From this it should be clear that the bootstrap can behave arbitrarily badly in this setting.In summary, the bootstrap fails (miserably) in this case. Things tend to go wrong when dealing with parameters at the edge of the parameter space.An example from a sample of normal random variablesThere are other similar examples of the failure of the bootstrap in surprisingly simple circumstances.Consider a sample $X_1, X_2, \ldots$ from $\mathcal{N}(\mu,1)$ where the parameter space for $\mu$ is restricted to $[0,\infty)$. The MLE in this case is $\newcommand{\Xbar}{\bar{X}}\Xhat = \max(\bar{X},0)$. Again, we use the bootstrap estimate $\Xhat^\star = \max(\Xbar^\star, 0)$. Again, it can be shown that the distribution of $\sqrt{n}(\Xhat^\star - \Xhat)$ (conditional on the observed sample) does not converge to the same limiting distribution as $\sqrt{n}(\Xhat - \mu)$.Exchangeable arraysPerhaps one of the most dramatic examples is for an exchangeable array. Let $\newcommand{\bm}[1]{\mathbf{#1}}\bm{Y} = (Y_{ij})$ be an array of random variables such that, for every pair of permutation matrices $\bm{P}$ and $\bm{Q}$, the arrays $\bm{Y}$ and $\bm{P} \bm{Y} \bm{Q}$ have the same joint distribution. That is, permuting rows and columns of $\bm{Y}$ keeps the distribution invariant. (You can think of a two-way random effects model with one observation per cell as an example, though the model is much more general.)Suppose we wish to estimate a confidence interval for the mean $\mu = \mathbb{E}(Y_{ij}) = \mathbb{E}(Y_{11})$ (due to the exchangeability assumption described above the means of all the cells must be the same). McCullagh (2000) considered two different natural (i.e., naive) ways of bootstrapping such an array. Neither of them get the asymptotic variance for the sample mean correct. He also considers some examples of a one-way exchangeable array and linear regression.ReferencesUnfortunately, the subject matter is nontrivial, so none of these are particularly easy reads.P. Bickel and D. Freedman, Some asymptotic theory for the bootstrap. Ann. Stat., vol. 9, no. 6 (1981), 1196–1217.D. W. K. Andrews, Inconsistency of the bootstrap when a parameter is on the boundary of the parameter space, Econometrica, vol. 68, no. 2 (2000), 399–405.P. McCullagh, Resampling and exchangeable arrays, Bernoulli, vol. 6, no. 2 (2000), 285–301.E. L. Lehmann and J. P. Romano, Testing Statistical Hypotheses, 3rd. ed., Springer (2005). [Chapter 15: General Large Sample Methods]"
How to annoy a statistical referee?,"
I recently asked a question regarding general principles around reviewing statistics in papers. What I would now like to ask, is what particularly irritates you when reviewing a paper, i.e. what's the best way to really annoy a statistical referee!
One example per answer, please.
","['references', 'referee']",
Relationship between poisson and exponential distribution,"
The waiting times for poisson distribution is an exponential distribution with parameter lambda. But I don't understand it. Poisson models the number of arrivals per unit of time for example. How is this related to exponential distribution? Lets say probability of k arrivals in a unit of time is P(k) (modeled by poisson) and probability of k+1 is P(k+1), how does exponential distribution model the waiting time between them?
","['distributions', 'poisson-distribution', 'exponential-distribution']","I will use the following notation to be as consistent as possible with the wiki (in case you want to go back and forth between my answer and the wiki definitions for the poisson and exponential.)$N_t$: the number of arrivals during time period $t$$X_t$: the time it takes for one additional arrival to arrive assuming that someone arrived at time $t$By definition, the following conditions are equivalent:$ (X_t > x) \equiv (N_t = N_{t+x})$The event on the left captures the event that no one has arrived in the time interval $[t,t+x]$ which implies that our count of the number of arrivals at time $t+x$ is identical to the count at time $t$ which is the event on the right.By the complement rule, we also have:$P(X_t \le x) = 1 - P(X_t > x)$Using the equivalence of the two events that we described above, we can re-write the above as:$P(X_t \le x) = 1 - P(N_{t+x} - N_t = 0)$But,$P(N_{t+x} - N_t = 0) = P(N_x = 0)$ Using the poisson pmf the above where $\lambda$ is the average number of arrivals per time unit and $x$ a quantity of time units, simplifies to:$P(N_{t+x} - N_t = 0) = \frac{(\lambda x)^0}{0!}e^{-\lambda x}$i.e.$P(N_{t+x} - N_t = 0) = e^{-\lambda x}$Substituting in our original eqn, we have:$P(X_t \le x) = 1 - e^{-\lambda x}$The above is the cdf of a exponential pdf."
What is the difference between Cross-entropy and KL divergence?,"
Both the cross-entropy and the KL divergence are tools to measure the distance between two probability distributions, but what is the difference between them?
$$ H(P,Q) = -\sum_x P(x)\log Q(x)  $$
$$ KL(P | Q) = \sum_{x} P(x)\log {\frac{P(x)}{Q(x)}} $$
Moreover, it turns out that the minimization of KL divergence is equivalent to the minimization of cross-entropy.
I want to know them instinctively.
","['entropy', 'kullback-leibler', 'cross-entropy']","You will need some conditions to claim the equivalence between minimizing cross entropy and minimizing KL divergence. I will put your question under the context of classification problems using cross entropy as loss functions.Let us first recall that entropy is used to measure the uncertainty of a system, which is defined as
\begin{equation}
	S(v)=-\sum_ip(v_i)\log p(v_i)\label{eq:entropy},
\end{equation}
for $p(v_i)$ as the probabilities of different states $v_i$ of the system. From an information theory point of view, $S(v)$ is the amount of information is needed for removing the uncertainty.For instance, the event $I$ I will die within 200 years is almost certain (we may solve the aging problem for the word almost), therefore it has low uncertainty which requires only the information of the aging problem cannot be solved to make it certain. However, the event $II$ I will die within 50 years is more uncertain than event $I$, thus it needs more information to remove the uncertainties. Here entropy can be used to quantify the uncertainty of the distribution When will I die?, which can be regarded as the expectation of uncertainties of individual events like $I$ and $II$.Now look at the definition of KL divergence between distributions A and B
\begin{equation}
	D_{KL}(A\parallel B) = \sum_ip_A(v_i)\log p_A(v_i) - p_A(v_i)\log p_B(v_i)\label{eq:kld}, 
\end{equation}
where the first term of the right hand side is the entropy of distribution A, the second term can be interpreted as the expectation of distribution B in terms of A. And the $D_{KL}$ describes how different B is from A from the perspective of A. It's worth of noting $A$ usually stands for the data, i.e. the measured distribution, and $B$ is the theoretical or hypothetical distribution. That means, you always start from what you observed.To relate cross entropy to entropy and KL divergence, we formalize the cross entropy in terms of distributions $A$ and $B$ as
\begin{equation}
	H(A, B) = -\sum_ip_A(v_i)\log p_B(v_i)\label{eq:crossentropy}. 
\end{equation}
From the definitions, we can easily see
\begin{equation}
	H(A, B) = D_{KL}(A\parallel B)+S_A\label{eq:entropyrelation}. 
\end{equation}
If $S_A$ is a constant, then minimizing $H(A, B)$ is equivalent to minimizing $D_{KL}(A\parallel B)$.A further question follows naturally as how the entropy can be a constant. In a machine learning task, we start with a dataset (denoted as $P(\mathcal D)$) which represent the problem to be solved, and the learning purpose is to make the model estimated distribution (denoted as $P(model)$) as close as possible to true distribution of the problem (denoted as $P(truth)$).
$P(truth)$ is unknown and represented by $P(\mathcal D)$. Therefore in an ideal world, we expect
\begin{equation}
	P(model)\approx P(\mathcal D) \approx P(truth)
\end{equation}
and minimize $D_{KL}(P(\mathcal D)\parallel P(model))$. And luckily, in practice $\mathcal D$ is given, which means its entropy $S(D)$ is fixed as a constant."
Generate a random variable with a defined correlation to an existing variable(s),"
For a simulation study I have to generate random variables that show a predefined (population) correlation to an existing variable $Y$.
I looked into the R packages copula and CDVine which can produce random multivariate distributions with a given dependency structure. It is, however, not possible to fix one of the resulting variables to an existing variable.
Any ideas and links to existing functions are appreciated!
 
Conclusion:
Two valid answers came up, with different solutions:

An R script by caracal, which calculates a random variable with an exact (sample) correlation to a predefined variable
An R function I found myself, which calculates a random variable with a defined population correlation to a predefined variable


[@ttnphns' addition: I took the liberty to expand the question title from single fixed variable case to arbitrary number of fixed variables; i.e. how to generate a variable having predefined corretation(s) with some fixed, existing variable(s)]
","['r', 'correlation', 'random-variable', 'random-generation']","Here's another one: for vectors with mean 0, their correlation equals the cosine of their angle. So one way to find a vector $x$ with exactly the desired correlation $r$, corresponding to an angle $\theta$:Here is the code:For the orthogonal projection $P$, I used the $QR$-decomposition to improve numerical stability, since then simply $P = Q Q'$."
Subscript notation in expectations,"
What is the exact meaning of the subscript notation $\mathbb{E}_X[f(X)]$ in conditional expectations in the framework of measure theory ? These subscripts do not appear in the definition of conditional expectation, but we may see for example in this page of wikipedia. (Note that it wasn't always the case, the same page few months ago).
What should be for example the meaning of $\mathbb{E}_X[X+Y]$ with $X\sim\mathcal{N}(0,1)$ and $Y=X+1$ ?
","['conditional-expectation', 'notation']","In an expression where more than one random variables are involved, the symbol $E$ alone does not clarify with respect to which random variable is the expected value ""taken"". For example$$E[h(X,Y)] =\text{?} \int_{-\infty}^{\infty} h(x,y) f_X(x)\,dx$$
or
$$E[h(X,Y)] = \text{?} \int_{-\infty}^\infty h(x,y) f_Y(y)\,dy$$Neither. When many random variables are involved, and there is no subscript in the $E$ symbol, the expected value is taken with respect to their joint distribution:$$E[h(X,Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty h(x,y) f_{XY}(x,y) \, dx \, dy$$When a subscript is present... in some cases it tells us on which variable we should condition. So$$E_X[h(X,Y)] = E[h(X,Y)\mid X] = \int_{-\infty}^\infty h(x,y) f_{h(X,Y)\mid X}(h(x,y)\mid x)\,dy  $$Here, we ""integrate out"" the $Y$ variable, and we are left with a function of $X$....But in other cases, it tells us which marginal density to use for the ""averaging""$$E_X[h(X,Y)] = \int_{-\infty}^\infty h(x,y) f_{X}(x) \, dx $$ Here, we ""average over"" the $X$ variable, and we are left with a function of $Y$.Rather confusing I would say, but who said that scientific notation is totally free of ambiguity or multiple use? You should look how each author defines the use of such symbols."
"What is meant by a ""random variable""?","
What do they mean when they say ""random variable""? 
","['mathematical-statistics', 'random-variable', 'intuition', 'definition']","A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as ""state"", and then the random variable is a function of the state.Example:  Suppose we have three dice rolls ($D_{1}$,$D_{2}$,$D_{3}$).  Then the state $S=(D_{1},D_{2},D_{3})$. $$ X=(D_{1}=5?)+(D_{2}=5?)+(D_{3}=5?)$$$$ Y=D_{1}+D_{2}+D_{3}  $$"
Under what conditions does correlation imply causation?,"
We all know the mantra ""correlation does not imply causation"" which is drummed into all first year statistics students. There are some nice examples here to illustrate the idea.
But sometimes correlation does imply causation. The following example is taking from this Wikipedia page

For example, one could run an experiment on identical twins who were known to consistently get the same grades on their tests. One twin is sent to study for six hours while the other is sent to the amusement park. If their test scores suddenly diverged by a large degree, this would be strong evidence that studying (or going to the amusement park) had a causal effect on test scores. In this case, correlation between studying and test scores would almost certainly imply causation.

Are there other situations where correlation implies causation?
","['correlation', 'causality']","Correlation is not sufficient for causation. One can get around the Wikipedia example by imagining that those twins always cheated in their tests by having a device that gives them the answers. The twin that goes to the amusement park loses the device, hence the low grade.A good way to get this stuff straight is to think of the structure of Bayesian network that may be generating the measured quantities, as done by Pearl in his book Causality. His basic point is to look for hidden variables. If there is a hidden variable that happens not to vary in the measured sample, then the correlation would not imply causation. Expose all hidden variables and you have causation."
"Understanding ""variance"" intuitively","
What is the cleanest, easiest way to explain someone the concept of variance? What does it intuitively mean? If one is to explain this to their child how would one go about it?
It's a concept that I have difficulty in articulating - especially when relating variance to risk. I understand it mathematically and can explain it that way too. But when explaining real world phenomena how do you make one understand variance and it's applicability in the 'real world', so to speak.
Let's say we are simulating an investment in a stock using random numbers (rolling a die or using an excel sheet, doesn't matter). We get some 'return on investment' by associating each instance of the random variable to 'some change' in the return. Eg.: 

Rolling a 1 implies a change of 0.8 per \$1 in investment, a 5 a change of 1.1 per \$1 and so on.

Now if this simulation is run for about 50 times (or 20 or 100) we will get some values and the final value of the investment. So what does 'variance' actually tell us if we were to calculate it from the above data set? What does one ""see"" - If the variance turns out to be 1.7654 or 0.88765 or 5.2342 what does this even mean? What did/can I observe about this investment?? What conclusions can I draw - in lay man terms.
Please feel free to augment the question with that for standard deviation too! Although I feel it's 'easier' to understand, but something that would contribute to making it also 'intuitively' clear would be greatly appreciated!
","['distributions', 'variance', 'standard-deviation', 'inference', 'intuition']","I would probably use a similar analogy to the one I've learned to give 'laypeople' when introducing the concept of bias and variance:  the dartboard analogy. See below:The particular image above is from Encyclopedia of Machine Learning, and the reference within the image is Moore and McCabe's ""Introduction to the Practice of Statistics"".EDIT:Here's an exercise that I believe is pretty intuitive:  Take a deck of cards (out of the box), and drop the deck from a height of about 1 foot.  Ask your child to pick up the cards and return them to you.  Then, instead of dropping the deck, toss it as high as you can and let the cards fall to the ground.  Ask your child to pick up the cards and return them to you.The relative fun they have during the two trials should give them an intuitive feel for variance :)"
Convergence in probability vs. almost sure convergence,"
I've never really grokked the difference between these two measures of convergence. (Or, in fact, any of the different types of convergence, but I mention these two in particular because of the Weak and Strong Laws of Large Numbers.)
Sure, I can quote the definition of each and give an example where they differ, but I still don't quite get it.
What's a good way to understand the difference? Why is the difference important? Is there a particularly memorable example where they differ?
","['probability', 'random-variable']","From my point of view the difference is important, but largely for philosophical reasons.  Assume you have some device, that improves with time.  So, every time you use the device the probability of it failing is less than before.  Convergence in probability says that the chance of failure goes to zero as the number of usages goes to infinity.  So, after using the device a large number of times, you can be very confident of it working correctly, it still might fail, it's just very unlikely.Convergence almost surely is a bit stronger.  It says that the total number of failures is finite. That is, if you count the number of failures as the number of usages goes to infinity, you will get a finite number.  The impact of this is as follows:  As you use the device more and more, you will, after some finite number of usages, exhaust all failures.  From then on the device will work perfectly. As Srikant points out, you don't actually know when you have exhausted all failures, so from a purely practical point of view, there is not much difference between the two modes of convergence.However, personally I am very glad that, for example, the strong law of large numbers exists, as opposed to just the weak law.  Because now, a scientific experiment to obtain, say, the speed of light, is justified in taking averages.  At least in theory, after obtaining enough data, you can get arbitrarily close to the true speed of light.  There wont be any failures (however improbable) in the averaging process.Let me clarify what I mean by ''failures (however improbable) in the averaging process''.  Choose some $\delta > 0$ arbitrarily small. You obtain $n$ estimates $X_1,X_2,\dots,X_n$ of the speed of light (or some other quantity) that has some `true' value, say $\mu$. You compute the average 
$$S_n = \frac{1}{n}\sum_{k=1}^n X_k.$$
As we obtain more data ($n$ increases) we can compute $S_n$ for each $n = 1,2,\dots$.  The weak law says (under some assumptions about the $X_n$) that the probability
$$P(|S_n - \mu| > \delta) \rightarrow 0$$
as $n$ goes to $\infty$. The strong law says that the number of times that $|S_n - \mu|$ is larger than $\delta$ is finite (with probability 1).  That is, if we define the indicator function $I(|S_n - \mu| > \delta)$ that returns one when $|S_n - \mu| > \delta$ and zero otherwise, then 
$$\sum_{n=1}^{\infty}I(|S_n - \mu| > \delta)$$
converges. This gives you considerable confidence in the value of $S_n$, because it guarantees (i.e. with probability 1) the existence of some finite $n_0$ such that $|S_n - \mu| < \delta$ for all $n > n_0$ (i.e. the average never fails for $n > n_0$).  Note that the weak law gives no such guarantee."
"What, precisely, is a confidence interval?","
I know roughly and informally what a confidence interval is.  However, I can't seem to wrap my head around one rather important detail:  According to Wikipedia:

A confidence interval does not predict that the true value of the parameter has a particular probability of being in the confidence interval given the data actually obtained.  

I've also seen similar points made in several places on this site.  A more correct definition, also from Wikipedia, is:

if confidence intervals are constructed across many separate data analyses of repeated (and possibly different) experiments, the proportion of such intervals that contain the true value of the parameter will approximately match the confidence level

Again, I've seen similar points made in several places on this site.  I don't get it.  If, under repeated experiments, the fraction of computed confidence intervals that contain the true parameter $\theta$ is $(1 - \alpha)$, then how can the probability that $\theta$ is in the confidence interval computed for the actual experiment be anything other than $(1 - \alpha)$?  I'm looking for the following in an answer:

Clarification of the distinction between the incorrect and correct definitions above.
A formal, precise definition of a confidence interval that clearly shows why the first definition is wrong.
A concrete example of a case where the first definition is spectacularly wrong, even if the underlying model is correct.

","['confidence-interval', 'definition']","I found this thought experiment helpful when thinking about confidence intervals. It also answers your question 3.Let $X\sim U(0,1)$ and $Y=X+a-\frac{1}{2}$. Consider two observations of $Y$ taking the values $y_1$ and $y_2$ corresponding to observations $x_1$ and $x_2$ of $X$, and let $y_l=\min(y_1,y_2)$ and $y_u=\max(y_1,y_2)$. Then $[y_l,y_u]$ is a 50% confidence interval for $a$ (since the interval includes $a$ if $x_1<\frac12<x_2$ or $x_1>\frac12>x_2$, each of which has probability $\frac14$).However, if $y_u-y_l>\frac12$ then we know that the probability that the interval contains $a$ is $1$, not $\frac12$. The subtlety is that a $z\%$ confidence interval for a parameter means that the endpoints of the interval (which are random variables) lie either side of the parameter with probability $z\%$ before you calculate the interval, not that the probability of the parameter lying within the interval is $z\%$ after you have calculated the interval."
Principled way of collapsing categorical variables with many levels?,"
What techniques are available for collapsing (or pooling) many categories to a few, for the purpose of using them as an input (predictor) in a statistical model?

Consider a variable like college student major (discipline chosen by an undergraduate student). It is unordered and categorical, but it can potentially have dozens of distinct levels. Let's say I want to use major as a predictor in a regression model.
Using these levels as-is for modeling leads to all sorts of issues because there are just so many. A lot of statistical precision would be thrown away to use them, and the results are hard to interpret. We're rarely interested in specific majors -- we're much more likely to be interested in broad categories (subgroups) of majors. But it isn't always clear how to divide up the levels into such higher-level categories, or even how many higher-level categories to use.
For typical data I would be happy to use factor analysis, matrix factorization, or a discrete latent modeling technique. But majors are mutually exclusive categories, so I'm hesitant to exploit their covariance for anything.
Furthermore I don't care about the major categories on their own. I care about producing higher-level categories that are coherent with respect to my regression outcome. In the binary outcome case, that suggests to me something like linear discriminant analysis (LDA) to generate higher-level categories that maximize discriminative performance. But LDA is a limited technique and that feels like dirty data dredging to me. Moreover any continuous solution will be hard to interpret.
Meanwhile something based on covariances, like multiple correspondence analysis (MCA), seems suspect to me in this case because of the inherent dependence among mutually exclusive dummy variables -- they're better suited for studying multiple categorical variables, rather than multiple categories of the same variable.
edit: to be clear, this is about collapsing categories (not selecting them), and the categories are predictors or independent variables. In hindsight, this problem seems like an appropriate time to ""regularize 'em all and let God sort 'em out"". Glad to see this question is interesting to so many people!
","['regression', 'categorical-data', 'feature-engineering', 'many-categories', 'faq']","If I understood correctly, you imagine a linear model where one of the predictors is categorical (e.g. college major); and you expect that for some subgroups of its levels (subgroups of categories) the coefficients might be exactly the same. So perhaps the regression coefficients for Maths and Physics are the same, but different from those for Chemistry and Biology.In a simplest case, you would have a ""one way ANOVA"" linear model with a single categorical predictor: $$y_{ij} = \mu + \alpha_i + \epsilon_{ij},$$ where $i$ encodes the level of the categorical variable (the category). But you might prefer a solution that collapses some levels (categories) together, e.g. $$\begin{cases}\alpha_1=\alpha_2, \\ \alpha_3=\alpha_4=\alpha_5.\end{cases}$$This suggests that one can try to use a regularization penalty that would penalize solutions with differing alphas. One penalty term that immediately comes to mind is $$L=\omega \sum_{i<j}|\alpha_i-\alpha_j|.$$ This resembles lasso and should enforce sparsity of the $\alpha_i-\alpha_j$ differences, which is exactly what you want: you want many of them to be zero. Regularization parameter $\omega$ should be selected with cross-validation.I have never dealt with models like that and the above is the first thing that came to my mind. Then I decided to see if there is something like that implemented. I made some google searches and soon realized that this is called fusion of categories; searching for lasso fusion categorical will give you a lot of references to read. Here are a few that I briefly looked at:Gerhard Tutz, Regression for Categorical Data, see pp. 175-175 in Google Books. Tutz mentions the following four papers:Land and Friedman, 1997, Variable fusion: a new adaptive signal regression methodBondell and Reich, 2009, Simultaneous factor selection and collapsing levels in ANOVAGertheiss and Tutz, 2010, Sparse modeling of categorial explanatory variablesTibshirani et al. 2005, Sparsity and smoothness via the fused lasso is somewhat relevant even if not exactly the same (it is about ordinal variables)Gertheiss and Tutz 2010, published in the Annals of Applied Statistics, looks like a recent and very readable paper that contains other references. Here is its abstract:Shrinking methods in regression analysis are usually designed for metric
  predictors. In this article, however, shrinkage methods for categorial predictors
  are proposed. As an application we consider data from the Munich rent
  standard, where, for example, urban districts are treated as a categorial predictor.
  If independent variables are categorial, some modifications to usual
  shrinking procedures are necessary. Two $L_1$-penalty based methods for factor
  selection and clustering of categories are presented and investigated. The
  first approach is designed for nominal scale levels, the second one for ordinal
  predictors. Besides applying them to the Munich rent standard, methods are
  illustrated and compared in simulation studies.I like their Lasso-like solution paths that show how levels of two categorical variables get merged together when regularization strength increases:"
Locating freely available data samples,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I've been working on a new method for analyzing and parsing datasets to identify and isolate subgroups of a population without foreknowledge of any subgroup's characteristics.  While the method works well enough with artificial data samples (i.e. datasets created specifically for the purpose of identifying and segregating subsets of the population), I'd like to try testing it with live data.
What I'm looking for is a freely available (i.e. non-confidential, non-proprietary) data source.  Preferably one containing bimodal or multimodal distributions or being obviously comprised of multiple subsets that cannot be easily pulled apart via traditional means.  Where would I go to find such information?
","['dataset', 'sample', 'population', 'teaching']",Also see the UCI machine learning Data Repository.http://archive.ics.uci.edu/ml/
What's the difference between correlation and simple linear regression?,"
In particular, I am referring to the Pearson product-moment correlation coefficient.
","['correlation', 'regression']","What's the difference between the correlation between $X$ and $Y$ and a linear regression predicting $Y$ from $X$?First, some similarities:Second, some differences:"
Is this really how p-values work? Can a million research papers per year be based on pure randomness?,"
I'm very new to statistics, and I'm just learning to understand the basics, including $p$-values. But there is a huge question mark in my mind right now, and I kind of hope my understanding is wrong. Here's my thought process:
Aren't all researches around the world somewhat like the monkeys in the ""infinite monkey theorem""? Consider that there are 23887 universities in the world. If each university has 1000 students, that's 23 million students each year.
Let's say that each year, each student does at least one piece of research, using hypothesis testing with $\alpha=0.05$.
Doesn't that mean that even if all the research samples were pulled from a random population, about 5% of them would ""reject the null hypothesis as invalid"". Wow. Think about that. That's about a million research papers per year getting published due to ""significant"" results.
If this is how it works, this is scary. It means that a lot of the ""scientific truth"" we take for granted is based on pure randomness.
A simple chunk of R code seems to support my understanding:
library(data.table)
dt <- data.table(p=sapply(1:100000,function(x) t.test(rnorm(10,0,1))$p.value))
dt[p<0.05,]

So does this article on successful $p$-fishing: I Fooled Millions Into Thinking Chocolate Helps Weight Loss. Here's How.
Is this really all there is to it? Is this how ""science"" is supposed to work?
","['hypothesis-testing', 'statistical-significance', 'p-value']",
What is the best way to identify outliers in multivariate data?,"
Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces.
I am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful.
One possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches?
","['multivariate-analysis', 'outliers']","I think Robin Girard's answer would work pretty well for 3 and possibly 4 dimensions, but the curse of dimensionality would prevent it working beyond that. However, his suggestion led me to a related approach which is to apply the cross-validated kernel density estimate to the first three principal component scores. Then a very high-dimensional data set can still be handled ok.In summary, for i=1 to nend forSort the Li (for i=1,..,n) and the outliers are those with likelihood below some threshold. I'm not sure what would be a good threshold -- I'll leave that for whoever writes the paper on this! One possibility is to do a boxplot of the log(Li) values and see what outliers are detected at the negative end."
Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?,"
ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression.
In light of their equivalence, is there any reason why ANOVA is used instead of linear regression?
Note: I am particularly interested in hearing about technical reasons for the use of ANOVA instead of linear regression.
Edit
Here is one example using one-way ANOVA. Suppose, you want to know if the average height of male and females is the same. To test for your hypothesis you would collect data from a random   sample of male and females (say 30 each) and perform the ANOVA analysis (i.e., sum of squares for sex and error) to decide whether an effect exists.
You could also use linear regression to test for this as follows:
Define:  $\text{Sex} = 1$ if respondent is a male and $0$ otherwise.
$$
\text{Height} = \text{Intercept} + \beta * \text{Sex} + \text{error}
$$
where: $\text{error}\sim\mathcal N(0,\sigma^2)$
Then a test of whether $\beta = 0$ is a an equivalent test for your hypothesis.
","['regression', 'anova']","As an economist, the analysis of variance (ANOVA) is taught and usually understood in relation to linear regression (e.g. in Arthur Goldberger's A Course in Econometrics). Economists/Econometricians typically view ANOVA as uninteresting and prefer to move straight to regression models. From the perspective of linear (or even generalised linear) models, ANOVA assigns coefficients into batches, with each batch corresponding to a ""source of variation"" in ANOVA terminology.Generally you can replicate the inferences you would obtain from ANOVA using regression but not always OLS regression. Multilevel models are needed for analysing hierarchical data structures such as ""split-plot designs,"" where between-group effects are compared to group-level errors, and within-group effects are compared to data-level errors. Gelman's paper [1] goes into great detail about this problem and effectively argues that ANOVA is an important statistical tool that should still be taught for it's own sake.In particular Gelman argues that ANOVA is a way of understanding and structuring multilevel models. Therefore ANOVA is not an alternative to regression but as a tool for summarizing complex high-dimensional inferences and for exploratory data analysis. Gelman is a well-respected statistician and some credence should be given to his view. However, almost all of the empirical work that I do would be equally well served by linear regression and so I firmly fall into the camp of viewing it as a little bit pointless. Some disciplines with complex study designs (e.g. psychology) may find ANOVA useful.[1] Gelman, A. (2005). Analysis of variance: why it is more important than ever (with discussion). Annals of Statistics 33, 1–53. doi:10.1214/009053604000001048"
Is there a way to remember the definitions of Type I and Type II Errors?,"
I'm not a statistician by education, I'm a software engineer. Yet statistics comes up a lot. In fact, questions specifically about Type I and Type II error are coming up a lot in the course of my studying for the Certified Software Development Associate exam (mathematics and statistics are 10% of the exam). I'm having trouble always coming up with the right definitions for Type I and Type II error - although I'm memorizing them now (and can remember them most of the time), I really don't want to freeze up on this exam trying to remember what the difference is.
I know that Type I Error is a false positive, or when you reject the null hypothesis and it's actually true and a Type II error is a false negative, or when you accept the null hypothesis and it's actually false.
Is there an easy way to remember what the difference is, such as a mnemonic? How do professional statisticians do it - is it just something that they know from using or discussing it often?
(Side Note: This question can probably use some better tags. One that I wanted to create was ""terminology"", but I don't have enough reputation to do it. If someone could add that, it would be great. Thanks.)
","['terminology', 'type-i-and-ii-errors']","Since type two means ""False negative"" or sort of ""false false"", I remember it as the number of falses. "
The Book of Why by Judea Pearl: Why is he bashing statistics?,"
I am reading The Book of Why by Judea Pearl, and it is getting under my skin1. Specifically, it appears to me that he is unconditionally bashing ""classical"" statistics by putting up a straw man argument that statistics is never, ever able to investigate causal relations, that it never is interested in causal relations, and that statistics ""became a model-blinded data-reduction enterprise"".  Statistics becomes an ugly s-word in his book.
For example:

Statisticians have been immensely confused about what variables should and should not be controlled for, so the default practice has been to control for everything one can measure. [...] It is a convenient, simple procedure to follow, but it is both wasteful and ridden with errors. A key achievement of the Causal Revolution has been to bring an end to this confusion.
At the same time, statisticians greatly underrate controlling in the sense that they are loath to talk about causality at all [...]

However, causal models have been in statistics like, forever. I mean, a regression model can be used essentially a causal model, since we are essentially assuming that one variable is the cause and another is the effect (hence correlation is different approach from regression modelling) and testing whether this causal relationship explains the observed patterns.
Another quote:

No wonder statisticians in particular found this puzzle [The Monty Hall problem] hard to comprehend. They are accustomed to, as R.A. Fisher (1922) put it, ""the reduction of data"" and ignoring the data-generating process.

This reminds me of the reply Andrew Gelman wrote to the famous xkcd cartoon on Bayesians and frequentists: ""Still, I think the cartoon as a whole is unfair in that it compares a sensible Bayesian to a frequentist statistician who blindly follows the advice of shallow textbooks.""
The amount of misrepresentation of s-word which, as I perceive it, exists in Judea Pearls book made me wonder whether causal inference (which hitherto I perceived as a useful and interesting way of organizing and testing a scientific hypothesis2) is questionable.
Questions: do you think that Judea Pearl is misrepresenting statistics, and if yes, why? Just to make causal inference sound bigger than it is? Do you think that causal inference is a Revolution with a big R which really changes all our thinking?
Edit:
The questions above are my main issue, but since they are, admittedly, opinionated, please answer these concrete questions (1) what is the meaning of the ""Causation Revolution""? (2) how is it different from ""orthodox"" statistics?

1. Also because he is such a modest guy.
2. I mean in the scientific, not statistical sense.

EDIT: Andrew Gelman wrote this blog post on Judea Pearls book and I think he did a much better job explaining my problems with this book than I did. Here are two quotes:

On page 66 of the book, Pearl and Mackenzie write that statistics “became a model-blind data reduction enterprise.” Hey! What the hell are you talking about?? I’m a statistician, I’ve been doing statistics for 30 years, working in areas ranging from politics to toxicology. “Model-blind data reduction”? That’s just bullshit. We use models all the time.

And another one:

Look. I know about the pluralist’s dilemma. On one hand, Pearl believes that his methods are better than everything that came before. Fine. For him, and for many others, they are the best tools out there for studying causal inference. At the same time, as a pluralist, or a student of scientific history, we realize that there are many ways to bake a cake. It’s challenging to show respect to approaches that you don’t really work for you, and at some point the only way to do it is to step back and realize that real people use these methods to solve real problems. For example, I think making decisions using p-values is a terrible and logically incoherent idea that’s led to lots of scientific disasters; at the same time, many scientists do manage to use p-values as tools for learning. I recognize that. Similarly, I’d recommend that Pearl recognize that the apparatus of statistics, hierarchical regression modeling, interactions, poststratification, machine learning, etc etc., solves real problems in causal inference. Our methods, like Pearl’s, can also mess up—GIGO!—and maybe Pearl’s right that we’d all be better off to switch to his approach. But I don’t think it’s helping when he gives out inaccurate statements about what we do.

",['causality'],"I fully agree that Pearl's tone is arrogant, and his characterisation of ""statisticians"" is simplistic and monolithic. Also, I don't find his writing particularly clear.However, I think he has a point. Causal reasoning was not part of my formal training (MSc): the closest I got to the topic was an elective course in experimental design, i.e. any causality claims required me to physically control the environment. Pearl's book Causality was my first exposure to a refutation of this idea. Obviously I can't speak for all statisticians and curricula, but from my own perspective I subscribe to Pearl's observation that causal reasoning is not a priority in statistics.It is true that statisticians sometimes control for more variables than is strictly necessary, but this rarely leads to error (at least in my experience).This is also a belief that I held after graduating with an MSc in statistics in 2010.However, it is deeply incorrect. When you control for a common effect (called ""collider"" in the book), you can introduce selection bias. This realization was quite astonishing to me, and really convinced me of the usefulness of representing my causal hypotheses as graphs.EDIT: I was asked to elaborate on selection bias. This topic is quite subtle, I highly recommend perusing the edX MOOC on Causal Diagrams, a very nice introduction to graphs which has a chapter dedicated to selection bias.  For a toy example, to paraphrase this paper cited in the book: Consider the variables A=attractiveness, B=beauty, C=competence. Suppose that B and C are causally unrelated in the general population (i.e., beauty does not cause competence, competence does not cause beauty, and beauty and competence do not share a common cause). Suppose also that any one of B or C is sufficient for being attractive, i.e. A is a collider. Conditioning on A creates a spurious association between B and C.A more serious example is the ""birth weight paradox"", according to which a mother's smoking (S) during pregnancy seems to decrease the mortality (M) of the baby, if the baby is underweight (U). The proposed explanation is that birth defects (D) also cause low birth weight, and also contribute to mortality. The corresponding causal diagram is { S -> U, D -> U, U -> M, S -> M, D -> M } in which U is a collider; conditioning on it introduces the spurious association. The intuition behind this is that if the mother is a smoker, the low birth weight is less likely to be due to a defect."
When is unbalanced data really a problem in Machine Learning?,"
We already had multiple questions about unbalanced data when using logistic regression, SVM, decision trees, bagging and a number of other similar questions, what makes it a very popular topic! Unfortunately, each of the questions seems to be algorithm-specific and I didn't find any general guidelines for dealing with unbalanced data.
Quoting one of the answers by Marc Claesen, dealing with unbalanced data

(...) heavily depends on the learning method. Most general purpose
approaches have one (or several) ways to deal with this.

But when exactly should we worry about unbalanced data? Which algorithms are mostly affected by it and which are able to deal with it? Which algorithms would need us to balance the data? I am aware that discussing each of the algorithms would be impossible on a Q&A site like this. I am rather looking for general guidelines on when it could be a problem.
","['machine-learning', 'classification', 'predictive-models', 'unbalanced-classes', 'faq']",
"What is the best way to remember the difference between sensitivity, specificity, precision, accuracy, and recall?","
Despite having seen these terms 502847894789 times, I cannot for the life of me remember the difference between sensitivity, specificity, precision, accuracy, and recall.  They're pretty simple concepts, but the names are highly unintuitive to me, so I keep getting them confused with each other.  What is a good way to think about these concepts so the names start making sense?
Put another way, why were these names chosen for these concepts, as opposed to some other names?
","['terminology', 'accuracy', 'sensitivity-specificity']",Personally I remember the difference between precision and recall (a.k.a. sensitivity) by thinking about information retrieval:
Solving for regression parameters in closed-form vs gradient descent,"
In Andrew Ng's machine learning course, he introduces linear regression and logistic regression, and shows how to fit the model parameters using gradient descent and Newton's method.
I know gradient descent can be useful in some applications of machine learning (e.g., backpropogation), but in the more general case is there any reason why you wouldn't solve for the parameters in closed form-- i.e., by taking the derivative of the cost function and solving via Calculus?
What is the advantage of using an iterative algorithm like gradient descent over a closed-form solution in general, when one is available?
","['regression', 'machine-learning', 'logistic', 'gradient-descent']","Unless the closed form solution is extremely expensive to compute, it generally is the way to go when it is available.  However,For most nonlinear regression problems there is no closed form solution.  Even in linear regression (one of the few cases where a closed form solution is available), it may be impractical to use the formula.  The following example shows one way in which this can happen.  For linear regression on a model of the form $y=X\beta$, where $X$ is a matrix with full column rank, the least squares solution, $\hat{\beta} = \arg \min \| X \beta -y \|_{2}$ is given by $\hat{\beta}=(X^{T}X)^{-1}X^{T}y$Now, imagine that $X$ is a very large but sparse matrix.  e.g. $X$ might have 100,000 columns and 1,000,000 rows, but only 0.001% of the entries in $X$ are nonzero.  There are specialized data structures for storing only the nonzero entries of such sparse matrices.  Also imagine that we're unlucky, and $X^{T}X$ is a fairly dense matrix with a much higher percentage of nonzero entries.  Storing a dense 100,000 by 100,000 element $X^{T}X$ matrix would then require $1 \times 10^{10}$ floating point numbers (at 8 bytes per number, this comes to 80 gigabytes.)  This would be impractical to store on anything but a supercomputer.  Furthermore, the inverse of this matrix (or more commonly a Cholesky factor) would also tend to have mostly nonzero entries.  However, there are iterative methods for solving the least squares problem that require no more storage than $X$, $y$, and $\hat{\beta}$ and never explicitly form the matrix product $X^{T}X$.  In this situation, using an iterative method is much more computationally efficient than using the closed form solution to the least squares problem.     This example might seem absurdly large.  However, large sparse least squares problems of this size are routinely solved by iterative methods on desktop computers in seismic tomography research.  "
What is the reason that a likelihood function is not a pdf?,"
What is the reason that a likelihood function is not a pdf (probability density function)?
","['likelihood', 'density-function']","We'll start with two definitions: A probability density function (pdf) is a non-negative function that integrates to $1$. The likelihood is defined as the joint density of the observed data as a function of the parameter. But, as pointed out by the reference to Lehmann made by @whuber in a comment below, the likelihood function is a function of the parameter only, with the data held as a fixed constant. So the fact that it is a density as a function of the data is irrelevant. Therefore, the likelihood function is not a pdf because its integral with respect to the parameter does not necessarily equal 1 (and may not be integrable at all, actually, as pointed out by another comment from @whuber). To see this, we'll use a simple example. Suppose you have a single observation, $x$, from a ${\rm Bernoulli}(\theta)$ distribution. Then the likelihood function is $$ L(\theta) = \theta^{x} (1 - \theta)^{1-x} $$ It is a fact that $\int_{0}^{1} L(\theta) d \theta = 1/2$. Specifically, if $x = 1$, then $L(\theta) = \theta$, so $$\int_{0}^{1} L(\theta) d \theta = \int_{0}^{1} \theta \  d \theta = 1/2$$ and a similar calculation applies when $x = 0$. Therefore, $L(\theta)$ cannot be a density function.Perhaps even more important than this technical example showing why the likelihood isn't a probability density is to point out that the likelihood is not the probability of the parameter value being correct or anything like that - it is the probability (density) of the data given the parameter value, which is a completely different thing. Therefore one should not expect the likelihood function to behave like a probability density. "
"If mean is so sensitive, why use it in the first place?","
It is a known fact that median is resistant to outliers. If that is the case, when and why would we use the mean in the first place? 
One thing I can think of perhaps is to understand the presence of outliers i.e. if the median is far from the mean, then the distribution is skewed and perhaps the data needs to be examined to decide what is to be done with the outliers. Are there any other uses?
","['mathematical-statistics', 'mean', 'median']","In a sense, the mean is used because it is sensitive to the data.  If the distribution happens to be symmetric and the tails are about like the normal distribution, the mean is a very efficient summary of central tendency.  The median, while being robust and well-defined for any continuous distribution, is only $\frac{2}{\pi}$ as efficient as the mean if the data happened to come from a normal distribution.  It is this relative inefficiency of the median that keeps us from using it even more than we do.  The relative inefficiency translates into a minor absolute inefficiency as the sample size gets large, so for large $n$ we can be more guilt-free about using the median.It is interesting to note that for a measure of variation (spread, dispersion), there is a very robust estimator that is 0.98 as efficient as the standard deviation, namely Gini's mean difference.  This is the mean absolute difference between any two observations. [You have to multiply the sample standard deviation by a constant to estimate the same quantity estimated by Gini's mean difference.]   An efficient measure of central tendency is the Hodges-Lehmann estimator, i.e., the median of all pairwise means.  We would use it more if its interpretation were simpler."
Interpreting plot.lm(),"
I had a question about interpreting the graphs generated by plot(lm) in R.  I was wondering if you guys could tell me how to interpret the scale-location and leverage-residual plots?  Any comments would be appreciated.  Assume basic knowledge of statistics, regression and econometrics.
","['r', 'regression', 'data-visualization', 'residuals', 'outliers']",
Are there any examples where Bayesian credible intervals are obviously inferior to frequentist confidence intervals,"
A recent question on the difference between confidence and credible intervals led me to start re-reading Edwin Jaynes' article on that topic:
Jaynes, E. T., 1976. `Confidence Intervals vs Bayesian Intervals,' in Foundations of Probability Theory, Statistical Inference, and Statistical Theories of Science, W. L. Harper and C. A. Hooker (eds.), D. Reidel, Dordrecht, p. 175; (pdf)
In the abstract, Jaynes writes:

...we exhibit the Bayesian and orthodox solutions to six common statistical problems involving confidence intervals (including significance tests based on the same reasoning).  In every case, we find the situation is exactly the opposite, i.e. the Bayesian method is easier to apply and yields the same or better results.  Indeed, the orthodox results are satisfactory only when they agree closely (or exactly) with the Bayesian results. No contrary example has yet been produced.

(emphasis mine)
The paper was published in 1976, so perhaps things have moved on. My question is, are there examples where the frequentist confidence interval is clearly superior to the Bayesian credible interval (as per the challenge implicitly made by Jaynes)?
Examples based on incorrect prior assumptions are not acceptable as they say nothing about the internal consistency of the different approaches.
","['bayesian', 'confidence-interval']",
"How to calculate Area Under the Curve (AUC), or the c-statistic, by hand","
I am interested in calculating area under the curve (AUC), or the c-statistic, by hand for a binary logistic regression model.
For example, in the validation dataset, I have the true value for the dependent variable, retention (1 = retained; 0 = not retained), as well as a predicted retention status for each observation generated by my regression analysis using a model that was built using the training set (this will range from 0 to 1).
My initial thoughts were to identify the ""correct"" number of model classifications and simply divide the number of ""correct"" observations by the number of total observations to calculate the c-statistic. By ""correct"", if the true retention status of an observation = 1 and the predicted retention status is > 0.5 then that is a ""correct"" classification. Additionally, if the true retention status of an observation = 0 and the predicted retention status is < 0.5 then that is also a ""correct"" classification. I assume a ""tie"" would occur when the predicted value = 0.5, but that phenomenon does not occur in my validation dataset. On the other hand, ""incorrect"" classifications would be if the true retention status of an observation = 1 and the predicted retention status is < 0.5 or if the true retention status for an outcome = 0 and the predicted retention status is > 0.5. I am aware of TP, FP, FN, TN, but not aware of how to calculate the c-statistic given this information.
","['regression', 'logistic', 'classification', 'roc', 'auc']","I would recommend Hanley’s & McNeil’s 1982 paper ‘The meaning and use of the area under a receiver operating characteristic (ROC) curve’.They have the following table of disease status and test result (corresponding to, for example, the estimated risk from a logistic model). The first number on the right is the number of patients with true disease status ‘normal’ and the second number is the number of patients with true disease status ‘abnormal’:(1) Definitely normal: 33/3
(2) Probably normal: 6/2
(3) Questionable: 6/2
(4) Probably abnormal: 11/11
(5) Definitely abnormal: 2/33So there are in total 58 ‘normal’ patients and ‘51’ abnormal ones. We see that when the predictor is 1, ‘Definitely normal’, the patient is usually normal (true for 33 of the 36 patients), and when it is 5, ‘Definitely abnormal’ the patients is usually abnormal (true for 33 of the 35 patients), so the predictor makes sense. But how should we judge a patient with a score of 2, 3, or 4? What we set our cutoff for judging a patients as abnormal or normal to determines the sensitivity and specificity of the resulting test.We can calculate the estimated sensitivity and specificity for different cutoffs. (I’ll just write ‘sensitivity’ and ‘specificity’ from now on, letting the estimated nature of the values be implicit.)If we choose our cutoff so that we classify all the patients as abnormal, no matter what their test results says (i.e., we choose the cutoff 1+), we will get a sensitivity of 51/51 = 1. The specificity will be 0/58 = 0. Doesn’t sound so good.OK, so let’s choose a less strict cutoff. We only classify patients as abnormal if they have a test result of 2 or higher. We then miss 3 abnormal patients, and have a sensitivity of 48/51 = 0.94. But we have a much increased specificity, of 33/58 = 0.57.We can now continue this, choosing various cutoffs (3, 4, 5, >5). (In the last case, we won’t classify any patients as abnormal, even if they have the highest possible test score of 5.)If we do this for all possible cutoffs, and the plot the sensitivity against 1 minus the specificity, we get the ROC curve. We can use the following R code:  The output is:We can calculate various statistics:And using this, we can plot the (estimated) ROC curve:We can very easily calculate the area under the ROC curve, using the formula for the area of a trapezoid:The result is 0.8931711.The AUC can also be seen as a concordance measure. If we take all possible pairs of patients where one is normal and the other is abnormal, we can calculate how frequently it’s the abnormal one that has the highest (most ‘abnormal-looking’) test result (if they have the same value, we count that this as ‘half a victory’):The answer is again 0.8931711, the area under the ROC curve. This will always be the case.As pointed out by Harrell in his answer, this also has a graphical interpretation. Let’s plot test score (risk estimate) on the y-axis and true disease status on the x-axis (here with some jittering, to show overlapping points):Let us now draw a line between each point on the left (a ‘normal’ patient) and each point on the right (an ‘abnormal’ patient). The proportion of lines with a positive slope (i.e., the proportion of concordant pairs) is the concordance index (flat lines count as ‘50% concordance’). It’s a bit difficult to visualise the actual lines for this example, due to the number of ties (equal risk score), but with some jittering and transparency we can get a reasonable plot:We see that most of the lines slope upwards, so the concordance index will be high. We also see the contribution to the index from each type of observation pair. Most of it comes from normal patients with a risk score of 1 paired with abnormal patients with a risk score of 5 (1–5 pairs), but quite a lot also comes from 1–4 pairs and 4–5 pairs. And it’s very easy to calculate the actual concordance index based on the slope definition:The answer is again 0.8931711, i.e., the AUC.There is a close connection between the concordance measure and the Wilcoxon–Mann–Whitney test. Actually, the latter tests if the probability of concordance (i.e., that it’s the abnormal patient in a random normal–abnormal pair that will have the most ‘abnormal-looking’ test result) is exactly 0.5. And its test statistic is just a simple transformation of the estimated concordance probability:The test statistic (W = 2642) counts the number of concordant pairs. If we divide it by the number of possible pairs, we get a familar number:Yes, it’s 0.8931711, the area under the ROC curve.But let’s make life easier for ourselves. There are various packages that calculate the AUC for us automatically.The Epi package creates a nice ROC curve with various statistics (including the AUC) embedded: I also like the pROC package, since it can smooth the ROC estimate (and calculate an AUC estimate based on the smoothed ROC):(The red line is the original ROC, and the black line is the smoothed ROC. Also note the default 1:1 aspect ratio. It makes sense to use this, since both the sensitivity and specificity has a 0–1 range.)The estimated AUC from the smoothed ROC is 0.9107, similar to, but slightly larger than, the AUC from the unsmoothed ROC (if you look at the figure, you can easily see why it’s larger). (Though we really have too few possible distinct test result values to calculate a smooth AUC).Harrell’s rms package can calculate various related concordance statistics using the rcorr.cens() function. The C Index in its output is the AUC:Finally, we have the caTools package and its colAUC() function. It has a few advantages over other packages (mainly speed and the ability to work with multi-dimensional data – see ?colAUC) that can sometimes be helpful. But of course it gives the same answer as we have calculated over and over:Many people seem to think that the AUC tells us how ‘good’ a test is. And some people think that the AUC is the probability that the test will correctly classify a patient. It is not. As you can see from the above example and calculations, the AUC tells us something about a family of tests, one test for each possible cutoff.And the AUC is calculated based on cutoffs one would never use in practice. Why should we care about the sensitivity and specificity of ‘nonsensical’ cutoff values? Still, that’s what the AUC is (partially) based on. (Of course, if the AUC is very close to 1, almost every possible test will have great discriminatory power, and we would all be very happy.)The ‘random normal–abnormal’ pair interpretation of the AUC is nice (and can be extended, for instance to survival models, where we see if its the person with the highest (relative) hazard that dies the earliest). But one would never use it in practice. It’s a rare case where one knows one has one healthy and one ill person, doesn’t know which person is the ill one, and must decide which of them to treat. (In any case, the decision is easy; treat the one with the highest estimated risk.)So I think studying the actual ROC curve will be more useful than just looking at the AUC summary measure. And if you use the ROC together with (estimates of the) costs of false positives and false negatives, along with base rates of what you’re studying, you can get somewhere.Also note that the AUC only measures discrimination, not calibration. That is, it measures whether you can discriminate between two persons (one ill and one healthy), based on the risk score. For this, it only looks at relative risk values (or ranks, if you will, cf. the Wilcoxon–Mann–Whitney test interpretation), not the absolute ones, which you should be interested in. For example, if you divide each risk estimate from your logistic model by 2, you will get exactly the same AUC (and ROC).When evaluating a risk model, calibration is also very important. To examine this, you will look at all patients with a risk score of around, e.g., 0.7, and see if approximately 70% of these actually were ill. Do this for each possible risk score (possibly using some sort of smoothing / local regression). Plot the results, and you’ll get a graphical measure of calibration.If have a model with both good calibration and good discrimination, then you start to have good model. :)"
"How much do we know about p-hacking ""in the wild""?","
The phrase p-hacking (also: ""data dredging"", ""snooping"" or ""fishing"") refers to various kinds of statistical malpractice in which results become artificially statistically significant. There are many ways to procure a ""more significant"" result, including but by no means limited to:

only analysing an ""interesting"" subset of the data, in which a pattern was found;
failing to adjust properly for multiple testing, particularly post-hoc testing and failing to report tests carried out that were not significant;
trying different tests of the same hypothesis, e.g. both a parametric and a non-parametric test (there's some discussion of that in this thread), but only reporting the most significant;  
experimenting with inclusion/exclusion of data points, until the desired result is obtained. One opportunity comes when ""data-cleaning outliers"", but also when applying an ambiguous definition (e.g. in an econometric study of ""developed countries"", different definitions yield different sets of countries), or qualitative inclusion criteria (e.g. in a meta-analysis, it may be a finely balanced argument whether a particular study's methodology is sufficient robust to include);
the previous example is related to optional stopping, i.e., analyzing a dataset and deciding on whether to collect more data or not depending on the data collected so far (""this is almost significant, let's measure three more students!"") without accounting for this in the analysis;
experimentation during model-fitting, particularly covariates to include, but also  regarding data transformations/functional form.

So we know p-hacking can be done. It is often listed as one of the ""dangers of the p-value"" and was mentioned in the ASA report on statistical significance, discussed here on Cross Validated, so we also know it's a Bad Thing. Although some dubious motivations and (particularly in the competition for academic publication) counterproductive incentives are obvious, I suspect it's hard to figure out quite why it's done, whether deliberate malpractice or simple ignorance. Someone reporting p-values from a stepwise regression (because they find stepwise procedures ""produce good models"", but aren't aware the purported p-values are invalidated) is in the latter camp, but the effect is still p-hacking under the last of my bullet points above.
There's certainly evidence that p-hacking is ""out there"", e.g. Head et al (2015) looks for tell-tale signs of it infecting the scientific literature, but what is the current state of our evidence base about it? I'm aware that the approach taken by Head et al was not without controversy, so the current state of the literature, or general thinking in the academic community, would be interesting. For instance do we have any idea about:

Just how prevalent is it, and to what extent can we differentiate its occurrence from publication bias? (Is this distinction even meaningful?)
Is the effect particularly acute at the $p \approx 0.05$ boundary? Are similar effects seen at $p \approx 0.01$, for instance, or do we see whole ranges of p-values affected?
Do patterns in p-hacking vary between academic fields?
Do we have any idea which of the mechanisms of p-hacking (some of which are listed in the bullet points above) are most common? Have some forms proven harder to detect than others because they are ""better disguised""?

References
Head, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. (2015). The extent and consequences of p-hacking in science. PLoS Biol, 13(3), e1002106.
","['hypothesis-testing', 'statistical-significance', 'p-value', 'model-selection', 'reproducible-research']","EXECUTIVE SUMMARY: if ""p-hacking"" is to be understood broadly a la Gelman's forking paths, the answer to how prevalent it is, is that it is almost universal.Andrew Gelman likes to write about this topic and has been posting extensively about it lately on his blog. I don't always agree with him but I like his perspective on $p$-hacking.  Here is an excerpt from the Introduction to his Garden of Forking Paths paper (Gelman & Loken 2013; a version appeared in American Scientist 2014; see also Gelman's brief comment on the ASA's statement), emphasis mine:This problem is sometimes called “p-hacking” or “researcher degrees of freedom” (Simmons, Nelson,
  and Simonsohn, 2011). In a recent article, we spoke of “fishing expeditions [...]”. But we are starting to feel that the term “fishing” was unfortunate, in that it invokes an image
  of a researcher trying out comparison after comparison, throwing the line into the lake repeatedly
  until a fish is snagged. We have no reason to think that researchers regularly do that. We think
  the real story is that researchers can perform a reasonable analysis given their assumptions and
  their data, but had the data turned out differently, they could have done other analyses that were
  just as reasonable in those circumstances.We regret the spread of the terms “fishing” and “p-hacking” (and even “researcher degrees of
  freedom”) for two reasons: first, because when such terms are used to describe a study, there is
  the misleading implication that researchers were consciously trying out many different analyses
  on a single data set; and, second, because it can lead researchers who know they did not try
  out many different analyses to mistakenly think they are not so strongly subject to problems of
  researcher degrees of freedom. [...]
  Our key point here is that it is possible to have multiple potential comparisons, in the sense of
  a data analysis whose details are highly contingent on data, without the researcher performing any
  conscious procedure of fishing or examining multiple p-values.So: Gelman does not like the term p-hacking because it implies that the researches were actively cheating. Whereas the problems can occur simply because the researchers choose what test to perform/report after looking at the data, i.e. after doing some exploratory analysis.With some experience of working in biology, I can safely say that everybody does that. Everybody (myself included) collects some data with only vague a priori hypotheses, does extensive exploratory analysis, runs various significance tests, collects some more data, runs and re-runs the tests, and finally reports some $p$-values in the final manuscript. All of this is happening without actively cheating, doing dumb xkcd-jelly-beans-style cherry-picking, or consciously hacking anything.So if ""p-hacking"" is to be understood broadly a la Gelman's forking paths, the answer to how prevalent it is, is that it is almost universal.The only exceptions that come to mind are fully pre-registered replication studies in psychology or fully pre-registered medical trials.Amusingly, some people polled researchers to find that many admit doing some sort of hacking (John et al. 2012, Measuring the Prevalence of Questionable
Research Practices With Incentives for
Truth Telling):Apart from that, everybody heard about the so called ""replication crisis"" in psychology: more than one half of the recent studies published in the top psychology journals do not replicate (Nosek et al. 2015, Estimating the reproducibility of psychological science). (This study has recently been all over the blogs again, because the March 2016 issue of Science published a Comment attempting to refute Nosek et al. and also a reply by Nosek et al. The discussion continued elsewhere, see post by Andrew Gelman  and the RetractionWatch post that he links to. To put it politely, the critique is unconvincing.)Update Nov 2018: Kaplan and Irvin, 2017, Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time show that the fraction of clinical trials reporting null results increased from 43% to 92% after pre-registration became required: I have not heard about Head et al. study before, but have now spent some time looking through the surrounding literature. I have also taken a brief look at their raw data.Head et al. downloaded all Open Access papers from PubMed and extracted all p-values reported in the text, getting 2.7 mln p-values. Out of these, 1.1 mln was reported as $p=a$ and not as $p<a$. Out of these, Head et al. randomly took one p-value per paper but this does not seem to change the distribution, so here is how the distribution of all 1.1 mln values looks like (between $0$ and $0.06$):I used $0.0001$ bin width, and one can clearly see a lot of predictable rounding in the reported $p$-values. Now, Head et al. do the following: they compare the number of $p$-values in the $(0.045, 0.5)$ interval and in the $(0.04, 0.045)$ interval; the former number turns out to be (significantly) larger and they take it as an evidence of $p$-hacking. If one squints, one can see it on my figure.I find this hugely unconvincing for one simple reason. Who wants to report their findings with $p=0.05$? Actually, many people seem to be doing exactly that, but still it appears natural to try to avoid this unsatisfactory border-line value and  rather to report another significant digit, e.g. $p=0.048$ (unless of course it's $p=0.052$). So some excess of $p$-values close but not equal to $0.05$ can be explained by researcher's rounding preferences.And apart from that, the effect is tiny.(The only strong effect that I can see on this figure is a pronounced drop of the $p$-value density right after $0.05$. This is clearly due to the publication bias.)Unless I missed something, Head et al. do not even discuss this potential alternative explanation. They do not present any histogram of the $p$-values either.There is a bunch of papers criticizing Head et al. In this unpublished manuscript Hartgerink argues that Head et al. should have included $p=0.04$ and $p=0.05$ in their comparison (and if they had, they would not have found their effect). I am not sure about that; it does not sound very convincing. It would be much better if we could somehow inspect the distribution of the ""raw"" $p$-values without any rounding.In this 2016 PeerJ paper (preprint posted in 2015) the same Hartgerink et al. extract p-values from lots of papers in top psychology journals and do exactly that: they recompute exact $p$-values from the reported $t$-, $F$-, $\chi^2$- etc. statistic values; this distribution is free from any rounding artifacts and does not exhibit any increase towards 0.05 whatsoever (Figure 4):$\hspace{5em}$A very similar approach is taken by Krawczyk 2015 in PLoS One, who extracts 135k $p$-values from the top experimental psychology journals. Here is how the distribution looks for the reported (left) and recomputed (right) $p$-values:The difference is striking. The left histogram shows some weird stuff going on around $p=0.05$, but on the right one it is gone. This means that this weird stuff is due to people's preferences of reporting values around $p\approx 0.05$ and not due to $p$-hacking.It seems that the first to observe the alleged excess of $p$-values just below 0.05 were Masicampo & Lalande 2012, looking at three top journals in psychology: This does look impressive, but Lakens 2015 (preprint) in a published Comment argues that this only appears impressive thanks to the misleading exponential fit. See also Lakens 2015, On the challenges of drawing conclusions from p-values just below 0.05 and references therein.Brodeur et al. 2016 (the link goes to the 2013 preprint) do the same thing for economics literature. The look at the three economics journals, extract 50k test results, convert all of them into $z$-scores (using reported coefficients and standard errors whenever possible and using $p$-values if only they were reported), and get the following:This is a bit confusing because small $p$-values are on the right and large $p$-values are on the left. As authors write in the abstract, ""The distribution of p-values exhibits a camel shape with abundant p-values above .25"" and ""a valley between .25 and .10"". They argue that this valley is a sign of something fishy, but this is only an indirect evidence. Also, it might be simply due to selective reporting, when large p-values above .25 are reported as some evidence of a lack of effect but p-values between .1 and .25 are felt to be neither here nor there and tend to be omitted. (I am not sure if this effect is present in biological literature or not because the plots above focus on $p<0.05$ interval.)Based on all of the above, my conclusion is that I don't see any strong evidence of $p$-hacking in $p$-value distributions across biological/psychological literature as a whole. There is plenty of evidence of selective reporting, publication bias, rounding $p$-values down to $0.05$ and other funny rounding effects, but I disagree with conclusions of Head et al.: there is no suspicious bump below $0.05$.Uri Simonsohn argues that this is ""falsely reassuring"". Well, actually he cites these papers un-critically but then remarks that ""most p-values are way smaller"" than 0.05. Then he says: ""That’s reassuring, but falsely reassuring"". And here is why:If we want to know if researchers p-hack their results, we need to examine the p-values associated with their results, those they may want to p-hack in the first place. Samples, to be unbiased, must only include observations from the population of interest.Most p-values reported in most papers are irrelevant for the strategic behavior of interest. Covariates, manipulation checks, main effects in studies testing interactions, etc. Including them we underestimate p-hacking and we overestimate the evidential value of data.  Analyzing all p-values asks a different question, a less sensible one. Instead of “Do researchers p-hack what they study?” we ask “Do researchers p-hack everything?”This makes total sense. Looking at all reported $p$-values is way too noisy. Uri's $p$-curve paper (Simonsohn et al. 2013) nicely demonstrates what one can see if one looks at carefully selected $p$-values. They selected 20 psychology papers based on some suspicious keywords (namely, authors of these papers reported tests controlling for a covariate and did not report what happens without controlling for it) and then took only $p$-values that are testing the main findings. Here is how the distribution looks like (left):Strong left skew suggests strong $p$-hacking.I would say that we know that there must be a lot of $p$-hacking going on, mostly of the Forking-Paths type that Gelman describes; probably to the extent that published $p$-values cannot really be taken at face value and should be ""discounted"" by the reader by some substantial fraction. However, this attitude seems to produce much more subtle effects than simply a bump in the overall $p$-values distribution just below $0.05$ and cannot really be detected by such a blunt analysis. "
Can someone explain Gibbs sampling in very simple words? [duplicate],"







This question already has an answer here:
                                
                            




Explanation regarding Gibbs Sampling

                                (1 answer)
                            

Closed 5 years ago.



I'm doing some reading on topic modeling (with Latent Dirichlet Allocation) which makes use of Gibbs sampling. As a newbie in statistics―well, I know things like binomials, multinomials, priors, etc.―,I find it difficult to grasp how Gibbs sampling works. Can someone please explain it in simple English and/or using simple examples? (If you are not familiar with topic modeling, any examples will do.) 
","['modeling', 'sampling', 'conditional-probability', 'gibbs']","You are a dungeonmaster hosting Dungeons & Dragons and a player casts 'Spell of 
Eldritch Chaotic Weather (SECW).  You've never heard of this spell before, but it turns out it is quite involved.  The player hands you a dense book and says, 'the effect of this spell is that one of the events in this book occurs.'  The book contains a whopping 1000 different effects, and what's more, the events have different 'relative probabilities.'  The book tells you that the most likely event is 'fireball'; all the probabilities of the other events are described relative to the probability of 'fireball'; for example: on page 155, it says that 'duck storm' is half as likely as 'fireball.'How are you, the Dungeon Master, to sample a random event from this book?  Here's how you can do it: The accept-reject algorithm:1) Roll a d1000 to decide a 'candidate' event.2) Suppose the candidate event is 44% as likely as the most likely event, 'fireball'.  Then accept the candidate with probability 44%.  (Roll a d100, and accept if the roll is 44 or lower.  Otherwise, go back to step 1 until you accept an event.)3) The accepted event is your random sample.The accept-reject algorithm is guaranteed to sample from the distribution with the specified relative probabilities.After much dice rolling you finally end up accepting a candidate: 'summon frog'.  You breathe a sigh of relief as you now you can get back to the (routine in comparison) business of handling the battle between the troll-orcs and dragon-elves.However, not to be outdone, another player decides to cast 'Level. 2 arcane cyber-effect storm.' For this spell, two different random effects occur: a randomly generated attack, and a randomly generated character buff.  The manual for this spell is so huge that it can only fit on a CD.  The player boots you up and shows you a page.  Your jaw drops: the entry for each attack is about as large a the manual for the previous spell, because it lists a relative probability for each possible accompanying buff'Cupric Blade'The most likely buff accompanying this attack is 'Hotelling aura''Jackal Vision' is 33% as likely to accompany this attack as 'Hotelling aura''Toaster Ears' is 20% as likely to accompany this attack as 'Hotelling aura'...Similarly, the probability of a particular attack spell occurring depends on the probability of the buff occurring.It would be justified to wonder if a proper probability distribution can even be defined given this information.  Well, it turns out that if there is one, it is uniquely specified by the conditional probabilities given in the manual.  But how to sample from it?Luckily for you, the CD comes with an automated Gibbs' sampler, because you would have to spend an eternity doing the following by hand.Gibbs' sampler algorithm1) Choose an attack spell randomly2) Use the accept-reject algorithm to choose the buff conditional on the attack3) Forget the attack spell you chose in step 1.
Choose a new attack spell using the accept-reject algorithm conditional on the buff in step 24) Go to step 2, repeat forever (though usually 10000 iterations will be enough)5) Whatever your algorithm has at the last iteration, is your sample.You see, in general, MCMC samplers are only asymptotically guaranteed to generate samples from a distribution with the specified conditional probabilities.  But in many cases, MCMC samplers are the only practical solution available."
How to choose nlme or lme4 R library for mixed effects models?,"
I have fit a few mixed effects models (particularly longitudinal models) using lme4 in R but would like to really master the models and the code that goes with them.
However, before diving in with both feet (and buying some books) I want to be sure that I am learning the right library. I have used lme4 up to now because I just found it easier than nlme, but if nlme is better for my purposes then I feel I should use that.
I'm sure neither is ""better"" in a simplistic way, but I would value some opinions or thoughts. My main criteria are:

Easy to use (I'm a psychologist by training, and not particularly versed in statistics or coding, but I'm learning)
Good features for fitting longitudinal data (if there is a difference here- but this is what I mainly use them for)
Good (easy to interpret) graphical summaries, again not sure if there is a difference here but I often produce graphs for people even less technical than I, so nice clear plots are always good (I'm very fond of the xyplot function in lattice() for this reason).

","['r', 'mixed-model', 'lme4-nlme']","Both packages use Lattice as the backend, but nlme has some nice features like groupedData() and lmList() that are lacking in lme4 (IMO). From a practical perspective, the two most important criteria seem, however, thatNow, lme4 can easily handle very huge number of random effects (hence, number of individuals in a given study) thanks to its C part and the use of sparse matrices. The nlme package has somewhat been superseded by lme4 so I won't expect people spending much time developing add-ons on top of nlme. Personally, when I have a continuous response in my model, I tend to use both packages, but I'm now versed to the lme4 way for fitting GLMM.Rather than buying a book, take a look first at the Doug Bates' draft book on R-forge: lme4: Mixed-effects Modeling with R."
What correlation makes a matrix singular and what are implications of singularity or near-singularity?,"
I am doing some calculations on different matrices (mainly in logistic regression) and I commonly get the error ""Matrix is singular"", where I have to go back and remove the correlated variables. My question here is what would you consider a ""highly"" correlated matrix? Is there a threshold value of correlation to represent this word? Like if a variable was 0.97 correlated to another one, is this a ""high"" enough to make a matrix singular?
Apologies if the question is very basic, I wasn't able to find any references talking about this issue (a hint towards any reference would be a big plus!).
","['regression', 'correlation', 'matrix', 'multicollinearity', 'singular-matrix']","A square matrix is singular, that is, its determinant is zero, if it contains rows or columns which are proportionally interrelated; in other words, one or more of its rows (columns) is exactly expressible as a linear combination of all or some other its rows (columns), the combination being without a constant term.Imagine, for example, a $3 \times 3$ matrix $A$ - symmetric, like correlaton matrix, or asymmetric. If in terms of its entries it appears that $\text {col}_3 = 2.15 \cdot \text {col}_1$ for example, then the matrix $A$ is singular. If, as another example, its $\text{row}_2 = 1.6 \cdot \text{row}_1 - 4 \cdot \text{row}_3$, then $A$ is again singular. As a particular case, if any row contains just zeros, the matrix is also singular because any column then is a linear combination of the other columns. In general, if any row (column) of a square matrix is a weighted sum of the other rows (columns), then any of the latter is also a weighted sum of the other rows (columns).Singular or near-singular matrix is often referred to as ""ill-conditioned"" matrix because it delivers problems in many statistical data analyses.What must multivariate data look like in order for its correlation or covariance matrix to be a singular matrix as described above? It is when there is linear interdependances among the variables. If some variable is an exact linear combination of the other variables, with constant term allowed, the correlation and covariance matrces of the variables will be singular. The dependency observed in such matrix between its columns is actually that same dependency as the dependency between the variables in the data observed after the variables have been centered (their means brought to 0) or standardized (if we mean correlation rather than covariance matrix).Some frequent particular situations when the correlation/covariance matrix of variables is singular: (1) Number of variables is equal or greater than the number of cases; (2) Two or more variables sum up to a constant; (3) Two variables are identical or differ merely in mean (level) or variance (scale).Also, duplicating observations in a dataset will lead the matrix towards singularity. The more times you clone a case the closer is singularity. So, when doing some sort of imputation of missing values it is always beneficial (from both statistical and mathematical view) to add some noise to the imputed data.In geometrical viewpoint, singularity is (multi)collinearity (or ""complanarity""): variables displayed as vectors (arrows) in space lie in the space of dimentionality lesser than the number of variables - in a reduced space. (That dimensionality is known as the rank of the matrix; it is equal to the number of non-zero eigenvalues of the matrix.)In a more distant or ""transcendental"" geometrical view, singularity or zero-definiteness (presense of zero eigenvalue) is the bending point between positive definiteness and non-positive definiteness of a matrix. When some of the vectors-variables (which is the correlation/covariance matrix) ""go beyond"" lying even in the reduced euclidean space - so that they cannot ""converge in"" or ""perfectly span"" euclidean space anymore, non-positive definiteness appears, i.e. some eigenvalues of the correlation matrix become negative. (See about non-positive definite matrix, aka non-gramian here.) Non-positive definite matrix is also ""ill-conditioned"" for some kinds of statistical analysis.The first picture below shows a normal regression situation with two predictors (we'll speek of linear regression). The picture is copied from here where it is explained in more details. In short, moderately correlated (= having acute angle between them) predictors $X_1$ and $X_2$ span 2-dimesional space ""plane X"". The dependent variable $Y$ is projected onto it orthogonally, leaving the predicted variable $Y'$ and the residuals with st. deviation equal to the length of $e$. R-square of the regression is the angle between $Y$ and $Y'$, and the two regression coefficients are directly related to the skew coordinates $b_1$ and $b_2$, respectively.The picture below shows regression situation with completely collinear predictors. $X_1$ and $X_2$ correlate perfectly and therefore these two vectors coincide and form the line, a 1-dimensional space. This is a reduced space. Mathematically though, plane X must exist in order to solve regression with two predictors, - but the plane is not defined anymore, alas. Fortunately, if we drop any one of the two collinear predictors out of analysis the regression is then simply solved because one-predictor regression needs one-dimensional predictor space. We see prediction $Y'$ and error $e$ of that (one-predictor) regression, drawn on the picture. There exist other approaches as well, besides dropping variables, to get rid of collinearity.The final picture below displays a situation with nearly collinear predictors. This situation is different and a bit more complex and nasty. $X_1$ and $X_2$ (both shown again in blue) tightly correlate and thence almost coincide. But there is still a tiny angle between, and because of the non-zero angle, plane X is defined (this plane on the picture looks like the plane on the first picture). So, mathematically there is no problem to solve the regression. The problem which arises here is a statistical one.Usually we do regression to infer about the R-square and the coefficients in the population. From sample to sample, data varies a bit. So, if we took another sample, the juxtaposition of the two predictor vectors would change slightly, which is normal. Not ""normal"" is that under near collinearity it leads to devastating consequences. Imagine that $X_1$ deviated just a little down, beyond plane X - as shown by grey vector. Because the angle between the two predictors was so small, plane X which will come through $X_2$ and through that drifted $X_1$ will drastically diverge from old plane X. Thus, because $X_1$ and $X_2$ are so much correlated we expect very different plane X in different samples from the same population. As plane X is different, predictions, R-square, residuals, coefficients - everything become different, too. It is well seen on the picture, where plane X swung somewhere 40 degrees. In a situation like that, estimates (coefficients, R-square etc.) are very unreliable which fact is expressed by their huge standard errors. And in contrast, with predictors far from collinear, estimates are reliable because the space spanned by the predictors is robust to those sampling fluctuations of data.Even a high correlation between two variables, if it is below 1, doesn't necessarily make the whole correlation matrix singular; it depends on the rest correlations as well. For example this correlation matrix:has determinant .00950 which is yet enough different from 0 to be considered eligible in many statistical analyses. But this matrix:has determinant .00010, a degree closer to 0.Statistical data analyses, such as regressions, incorporate special indices and tools to detect collinearity strong enough to consider dropping some of the variables or cases from the analysis, or to undertake other healing means. Please search (including this site) for ""collinearity diagnostics"", ""multicollinearity"", ""singularity/collinearity tolerance"", ""condition indices"", ""variance decomposition proportions"", ""variance inflation factors (VIF)""."
What do you call an average that does not include outliers?,"
What do you call an average that does not include outliers?
For example if you have a set:
{90,89,92,91,5} avg = 73.4

but excluding the outlier (5) we have
{90,89,92,91(,5)} avg = 90.5

How do you describe this average in statistics?
","['terminology', 'outliers', 'types-of-averages']","It's called the trimmed mean.  Basically what you do is compute the mean of the middle 80% of your data, ignoring the top and bottom 10%. Of course, these numbers can vary, but that's the general idea."
Is there a minimum sample size required for the t-test to be valid?,"
I'm currently working on a quasi-experimental research paper. I only have a sample size of 15 due to low population within the chosen area and that only 15 fit my criteria. Is 15 the minimum sample size to compute for t-test and F-test? If so, where can I get an article or book to support this small sample size?
This paper was already defended last Monday and one of the panel asked to have a supporting reference because my sample size is too low.   He said it should've been at least 40 respondents.  
","['t-test', 'sample-size', 'assumptions', 'statistical-power']",
Who Are The Bayesians?,"
As one becomes interested in statistics, the dichotomy ""Frequentist"" vs. ""Bayesian"" soon becomes commonplace (and who hasn't read Nate Silver's The Signal and the Noise, anyway?). In talks and introductory courses, the point of view is overwhelmingly frequentist (MLE, $p$ values), but there tends to be a tiny fraction of time dedicated to admire Bayes formula and touch upon the idea of a prior distribution, usually tangentially. 
The tone employed to discuss Bayesian statistics oscillates between respect for its conceptual underpinnings, and a hint of skepticism regarding the chasm between lofty objectives, and arbitrariness in the selection of the prior distribution, or eventual use of frequentist maths after all.
Sentences such as ""if you are a hard-core Bayesian..."" abound.
The question is, Who are the Bayesians today? Are they some select academic institutions where you know that if you go there you will become a Bayesian? If so, are they specially sought after? Are we referring to just a few respected statisticians and mathematicians, and if so who are they?
Do they even exist as such, these pure ""Bayesians""? Would they happily accept the label? Is it always a flattering distinction? Are they mathematicians with peculiar slides in meetings, deprived of any $p$ values and confidence intervals, easily spotted on the brochure?
How much of a niche is being a ""Bayesian""? Are we referring to a minority of statisticians?
Or is current Bayesian-ism equated with machine learning applications?
... Or even more likely, is Bayesian statistics not so much a branch of statistics, but rather an epistemological movement that transcends the ambit of probability calculations into a philosophy of science? In this regard, all scientists would be Bayesian at heart... but there would be no such thing as a pure Bayesian statistician impermeable to frequentist techniques (or contradictions).
","['bayesian', 'mathematical-statistics', 'inference', 'bayes', 'frequentist']","I'm going to take your questions in order:The question is, Who are the Bayesians today?Anybody who does Bayesian data analysis and self-identifies as ""Bayesian"". Just like a programmer is someone who programs and self-identifies as a ""programmer"". A slight difference is that for historical reasons Bayesian has ideological connotations, because of the often heated argument between proponents of ""frequentist"" interpretations of probability and proponents of ""Bayesian"" interpretations of probability.Are they some select academic institutions, where you know that if you go there you will become a Bayesian?No, just like other parts of statistics you just need a good book (and perhaps a good teacher).If so, are they specially sought after?Bayesian data analysis is a very useful tool when doing statistical modeling, which I imagine is a pretty sought-after skill, (even if companies perhaps  aren't specifically looking for ""Bayesians"").Are we referring to just a few respected statisticians and mathematicians, and if so who are they?There are many respected statisticians that I believe would call themselves Bayesians, but those are not the Bayesians. Do they even exist as such, these pure ""Bayesians""?That's a bit like asking ""Do these pure programmers exist""? There is an amusing article called 46656 Varieties of Bayesians, and sure there is a healthy argument among ""Bayesians"" regarding many foundational issues. Just like programmers can argue over the merits of different programming techniques. (BTW, pure programmers program in Haskell).Would they happily accept the label?Some do, some don't. When I discovered Bayesian data analysis I thought it was the best since sliced bread (I still do) and I was happy to call myself a ""Bayesian"" (not least to irritate the p-value people at my department). Nowadays I don't like the term, I think it might alienate people as it makes Bayesian data analysis sound like some kind of cult, which it isn't, rather than a useful method to have in your statistical toolbox.Is it always a flattering distinction?Nope! As far as I know, the term ""Bayesian"" was introduced by the famous statistician Fisher as a derogatory term. Before that it was called ""inverse probability"" or just ""probability"".Are they mathematicians with peculiar slides in meetings, deprived of any p values and confidence intervals, easily spotted on the brochure?Well, there are conferences in Bayesian statistics, and I don't think they include that many p-values. Whether you'll find the slides peculiar will depend on your background...How much of a niche is being a ""Bayesian""? Are we referring to a minority of statisticians?I still think a minority of statisticians deal with Bayesian statistics, but I also think the proportion is growing.Or is current Bayesian-ism equated with machine learning applications?Nope, but Bayesian models are used a lot in machine learning. Here is a great machine learning book that presents machine learning from a Bayesian/probibalistic perspective: http://www.cs.ubc.ca/~murphyk/MLbook/ Hope that answered most of the questions :)Update:[C]ould you please consider adding a list of specific techniques or premises that distinguish Bayesian statistics?What distinguish Bayesian statistics is the use of Bayesian models :) Here is my spin on what a Bayesian model is:A Bayesian model is a statistical model where you use probability to represent all uncertainty within the model, both the uncertainty regarding the output but also the uncertainty regarding the input (aka parameters) to the model. The whole prior/posterior/Bayes theorem thing follows on this, but in my opinion, using probability for everything is what makes it Bayesian (and indeed a better word would perhaps just be something like probabilistic model).Now, Bayesian models can be tricky to fit, and there is a host of different computational techniques that are used for this. But these techniques are not Bayesian in themselves. To namedrop some computational techniques:Who was the famous statistician who introduced the term 'Bayesian' as derogatory?It was supposedly Ronald Fisher. The paper When did Bayesian inference become ""Bayesian""? gives the history of the term ""Bayesian""."
Mutual information versus correlation,"
Why and when we should use Mutual Information over statistical correlation measurements such as ""Pearson"", ""spearman"", or ""Kendall's tau"" ?
","['correlation', 'mathematical-statistics', 'mutual-information']","Let's consider one fundamental concept of (linear) correlation, covariance (which is Pearson's correlation coefficient ""un-standardized""). For two discrete random variables $X$ and $Y$ with probability mass functions $p(x)$, $p(y)$ and joint pmf $p(x,y)$ we have$$\operatorname{Cov}(X,Y) = E(XY) - E(X)E(Y) = \sum_{x,y}p(x,y)xy - \left(\sum_xp(x)x\right)\cdot \left(\sum_yp(y)y\right)$$$$\Rightarrow \operatorname{Cov}(X,Y) = \sum_{x,y}\left[p(x,y)-p(x)p(y)\right]xy$$The Mutual Information between the two is defined as$$I(X,Y) = E\left (\ln \frac{p(x,y)}{p(x)p(y)}\right)=\sum_{x,y}p(x,y)\left[\ln p(x,y)-\ln p(x)p(y)\right]$$Compare the two: each contains a point-wise ""measure"" of ""the distance of the two rv's from independence"" as it is expressed by the distance of the joint pmf from the product of the marginal pmf's: the $\operatorname{Cov}(X,Y)$ has it as difference of levels, while $I(X,Y)$ has it as difference of logarithms.  And what do these measures do? In $\operatorname{Cov}(X,Y)$ they create a weighted sum of the product of the two random variables. In $I(X,Y)$ they create a weighted sum of their joint probabilities.So with $\operatorname{Cov}(X,Y)$ we look at what non-independence does to their product, while in $I(X,Y)$ we look at what non-independence does to their joint probability distribution.  Reversely, $I(X,Y)$ is the average value of the logarithmic measure of distance from independence, while $\operatorname{Cov}(X,Y)$ is the weighted value of the levels-measure of distance from independence, weighted by the product of the two rv's. So the two are not antagonistic—they are complementary, describing different aspects of the association between two random variables. One could comment that Mutual Information ""is not concerned"" whether the association is linear or not, while Covariance may be zero and the variables may still be stochastically dependent. On the other hand, Covariance can be calculated directly from a data sample without the need to actually know the probability distributions involved (since it is an expression involving moments of the distribution), while Mutual Information requires knowledge of the distributions, whose estimation, if unknown, is a much more delicate and uncertain work compared to the estimation of Covariance."
Calculating the parameters of a Beta distribution using the mean and variance,"
How can I calculate the $\alpha$ and $\beta$ parameters for a Beta distribution if I know the mean and variance that I want the distribution to have? Examples of an R command to do this would be most helpful.
","['r', 'distributions', 'estimation', 'beta-distribution']","I set$$\mu=\frac{\alpha}{\alpha+\beta}$$and$$\sigma^2=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$$and solved for $\alpha$ and $\beta$. My results show that$$\alpha=\left(\frac{1-\mu}{\sigma^2}-\frac{1}{\mu}\right)\mu^2$$and$$\beta=\alpha\left(\frac{1}{\mu}-1\right)$$I've written up some R code to estimate the parameters of the Beta distribution from a given mean, mu, and variance, var:There's been some confusion around the bounds of $\mu$ and $\sigma^2$ for any given Beta distribution, so let's make that clear here."
What is a complete list of the usual assumptions for linear regression? ,"
What are the usual assumptions for linear regression? 
Do they include:

a linear relationship between the independent and dependent variable
independent errors
normal distribution of errors
homoscedasticity

Are there any others?
","['regression', 'assumptions']",
How to 'sum' a standard deviation?,"
I have a monthly average for a value and a standard deviation corresponding to that average. I am now computing the annual average as the sum of monthly averages, how can I represent the standard deviation for the summed average ?
For example considering output from a wind farm:
Month        MWh     StdDev
January      927     333 
February     1234    250
March        1032    301
April        876     204
May          865     165
June         750     263
July         780     280
August       690     98
September    730     76
October      821     240
November     803     178
December     850     250

We can say that in the average year the wind farm produces 10,358 MWh, but what is the standard deviation corresponding to this figure ?
","['standard-deviation', 'descriptive-statistics']","Short answer: You average the variances; then you can take square root to get the average standard deviation.ExampleAnd then the average standard deviation is sqrt(53,964) = 232From Sum of normally distributed random variables:If $X$ and $Y$ are independent random variables that are normally distributed (and therefore also jointly so), then their sum is also normally distributed...the sum of two independent normally distributed random variables is normal, with its mean being the sum of the two means, and its variance being the sum of the two variancesAnd from Wolfram Alpha's Normal Sum Distribution:Amazingly, the distribution of a sum of two normally distributed independent variates $X$ and $Y$ with means and variances $(\mu_X,\sigma_X^2)$ and $(\mu_Y,\sigma_Y^2)$, respectively is another normal distribution$$
P_{X+Y}(u) = \frac{1}{\sqrt{2\pi (\sigma_X^2 + \sigma_Y^2)}} 
e^{-[u-(\mu_X+\mu_Y)]^2/[2(\sigma_X^2 + \sigma_Y^2)]}
$$which has mean$$\mu_{X+Y} = \mu_X+\mu_Y$$and variance$$ \sigma_{X+Y}^2 = \sigma_X^2 + \sigma_Y^2$$For your data:So to answer your question:How to 'sum' a standard deviation?You sum them quadratically:Conceptually you sum the variances, then take the square root to get the standard deviation.Because i was curious, i wanted to know the average monthly mean power, and its standard deviation. Through induction, we need 12 normal distributions which:That would be 12 average monthly distributions of:We can check our monthly average distributions by adding them up 12 times, to see that they equal the yearly distribution:Note: i'll leave it to someone with a knowledge of the esoteric Latex math to convert my formula images, and formula code into stackexchange formatted formulas.Edit: I moved the short, to the point, answer up top. Because i needed to do this again today, but wanted to double-check that i average the variances."
Kendall Tau or Spearman's rho?,"
In which cases should one prefer the one over the other?
I found someone who claims an advantage for Kendall, for pedagogical reasons, are there other reasons?
","['correlation', 'nonparametric', 'spearman-rho', 'kendall-tau']","I found that Spearman correlation is mostly used in place of usual linear correlation when working with integer valued scores on a measurement scale, when it has a moderate number of possible scores or when we don't want to make rely on assumptions about the bivariate relationships. As compared to Pearson coefficient, the interpretation of Kendall's tau seems to me less direct than that of Spearman's rho, in the sense that it quantifies the difference between the % of concordant and discordant pairs among all possible pairwise events. In my understanding, Kendall's tau more closely resembles Goodman-Kruskal Gamma.I just browsed an article from Larry Winner in the J. Statistics Educ. (2006) which discusses the use of both measures, NASCAR Winston Cup Race Results for 1975-2003.I also found @onestop answer about Pearson's or Spearman's correlation with non-normal data interesting in this respect.Of note, Kendall's tau (the a version) has connection to Somers' D (and Harrell's C) used for predictive modelling (see e.g., Interpretation of Somers’ D under four simple models by RB Newson and reference 6 therein, and articles by Newson published in the Stata Journal 2006). An overview of rank-sum tests is provided in Efficient Calculation of Jackknife Confidence Intervals for Rank Statistics, that was published in the JSS (2006)."
When to use regularization methods for regression?,"
In what circumstances should one consider using regularization methods (ridge, lasso or least angles regression) instead of OLS?
In case this helps steer the discussion, my main interest is improving predictive accuracy.
","['regression', 'least-squares', 'lasso', 'ridge-regression', 'fused-lasso']","Short answer: Whenever you are facing one of these situations: Ridge regression generally yields better predictions than OLS solution, through a better compromise between bias and variance. Its main drawback is that all predictors are kept in the model, so it is not very interesting if you seek a parsimonious model or want to apply some kind of feature selection. To achieve sparsity, the lasso is more appropriate but it will not necessarily yield good results in presence of high collinearity (it has been observed that if predictors are highly correlated, the prediction performance of the lasso is dominated by ridge regression). The second problem with L1 penalty is that the lasso solution is not uniquely determined when the number of variables is greater than the number of subjects (this is not the case of ridge regression). The last drawback of lasso is that it tends to select only one variable among a group of predictors with high pairwise correlations. In this case, there are alternative solutions like the group (i.e., achieve shrinkage on block of covariates, that is some blocks of regression coefficients are exactly zero) or fused lasso. The Graphical Lasso also offers promising features for GGMs (see the R glasso package).But, definitely, the elasticnet criteria, which is a combination of L1 and L2 penalties achieve both shrinkage and automatic variable selection, and it allows to keep $m>p$ variables in the case where $n\ll p$. Following Zou and Hastie (2005), it is defined as the argument that minimizes (over $\beta$)$$
L(\lambda_1,\lambda_2,\mathbf{\beta}) = \|Y-X\beta\|^2 + \lambda_2\|\beta\|^2 + \lambda_1\|\beta\|_1
$$where $\|\beta\|^2=\sum_{j=1}^p\beta_j^2$ and $\|\beta\|^1=\sum_{j=1}^p|\beta_j |$.The lasso can be computed with an algorithm based on coordinate descent as described in the recent paper by Friedman and coll., Regularization Paths for Generalized Linear Models via Coordinate Descent (JSS, 2010) or the LARS algorithm. In R, the penalized, lars or biglars, and glmnet packages are useful packages; in Python, there's the scikit.learn toolkit, with extensive documentation on the algorithms used to apply all three kind of regularization schemes.As for general references, the Lasso page contains most of what is needed to get started with lasso regression and technical details about L1-penalty, and this related question features essential references, When should I use lasso vs ridge?"
How to plot ROC curves in multiclass classification?,"
In other words, instead of having a two class problem I am dealing with 4 classes and still would like to assess performance using AUC.
","['classification', 'roc']",
How to compute precision/recall for multiclass-multilabel classification?,"
I'm wondering how to calculate precision and recall measures for multiclass multilabel classification, i.e. classification where there are more than two labels, and where each instance can have multiple labels?
","['machine-learning', 'classification', 'precision-recall', 'multi-class']",
"What is an ""uninformative prior""? Can we ever have one with truly no information?","
Inspired by a comment from this question:
What do we consider ""uninformative"" in a prior - and what information is still contained in a supposedly uninformative prior?
I generally see the prior in an analysis where it's either a frequentist-type analysis trying to borrow some nice parts from Bayesian analysis (be it some easier interpretation all the way to 'its the hot thing to do'), the specified prior is a uniform distribution across the bounds of the effect measure, centered on 0. But even that asserts a shape to the prior - it just happens to be flat.
Is there a better uninformative prior to use?
","['bayesian', 'prior', 'uninformative-prior']","[Warning: as a card-carrying member of the Objective Bayes Section of ISBA, my views are not exactly representative of all Bayesian statisticians!, quite the opposite...]In summary, there is no such thing as a prior with ""truly no information"".Indeed, the concept of ""uninformative"" prior is sadly a misnomer. Any prior distribution contains some specification that is akin to some amount of information. Even (or especially) the uniform prior. For one thing, the uniform prior is only flat for one given parameterisation of the problem. If one changes to another parameterisation (even a bounded one), the Jacobian change of variable comes into the picture and the density and therefore the prior is no longer flat.As pointed out by Elvis, maximum entropy is one approach advocated to select so-called ""uninformative"" priors. It however requires (a) some degree of information on some moments $h(\theta)$ of the prior distribution $\pi(\cdot)$ to specify the constraints$$\int_{\Theta} h(\theta)\,\text{d}\pi(\theta) = \mathfrak{h}_0$$ that lead to the MaxEnt prior
$$\pi^*(\theta)\propto \exp\{ \lambda^\text{T}h(\theta) \}$$
and (b) the preliminary choice of a reference measure $\text{d}\mu(\theta)$ [in continuous settings], a choice that brings the debate back to its initial stage! (In addition, the parametrisation of the constraints (i.e., the choice of $h$) impacts the shape of the resulting MaxEnt prior.)José Bernardo has produced an original theory of reference priors where he chooses the prior in order to maximise the information brought by the data by maximising the Kullback distance between prior and posterior. In the simplest cases with no nuisance parameters, the solution is Jeffreys' prior. In more complex problems, (a) a choice of the parameters of interest (or even a ranking of their order of interest) must be made; (b) the computation of the prior is fairly involved and requires a sequence of embedded compact sets to avoid improperness issues. (See e.g. The Bayesian Choice for details.)In an interesting twist, some researchers outside the Bayesian perspective have been developing procedures called confidence distributions that are probability distributions on the parameter space, constructed by inversion from frequency-based procedures without an explicit prior structure or even a dominating measure on this parameter space. They argue that this absence of well-defined prior is a plus, although the result definitely depends on the choice of the initialising frequency-based procedureIn short, there is no ""best"" (or even ""better"") choice for ""the"" ""uninformative"" prior. And I consider this is how things should be because the very nature of Bayesian analysis implies that the choice of the prior distribution matters. And that there is no comparison of priors: one cannot be ""better"" than another. (At least before observing the data: once it is observed, comparison of priors becomes model choice.) The conclusion of José Bernardo, Jim Berger, Dongchu Sun, and many other ""objective"" Bayesians is that there are roughly equivalent reference priors one can use when being unsure about one's prior information or seeking a benchmark Bayesian inference, some of those priors being partly supported by information theory arguments, others by non-Bayesian frequentist properties (like matching priors), and all resulting in rather similar inferences."
"Given the power of computers these days, is there ever a reason to do a chi-squared test rather than Fisher's exact test?","
Given that software can do the Fisher's exact test calculation so easily nowadays, is there any circumstance where, theoretically or practically, the chi-squared test is actually preferable to Fisher's exact test?
Advantages of the Fisher's exact test include:

scaling to contingency tables larger than 2x2 (i.e any r x c table)
gives an exact p-value
not needing to have a minimum expected cell count to be valid

","['chi-squared-test', 'contingency-tables', 'fishers-exact-test']",
How to obtain the p-value (check significance) of an effect in a lme4 mixed model?,"
I use lme4 in R to fit the mixed model
lmer(value~status+(1|experiment)))

where value is continuous, status and experiment are factors, and I get
Linear mixed model fit by REML 
Formula: value ~ status + (1 | experiment) 
  AIC   BIC logLik deviance REMLdev
 29.1 46.98 -9.548    5.911    19.1
Random effects:
 Groups     Name        Variance Std.Dev.
 experiment (Intercept) 0.065526 0.25598 
 Residual               0.053029 0.23028 
Number of obs: 264, groups: experiment, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.78004    0.08448   32.91
statusD      0.20493    0.03389    6.05
statusR      0.88690    0.03583   24.76

Correlation of Fixed Effects:
        (Intr) statsD
statusD -0.204       
statusR -0.193  0.476

How can I know that the effect of status is significant? R reports only $t$-values and not $p$-values.
","['r', 'hypothesis-testing', 'mixed-model', 'p-value', 'lme4-nlme']","There is a lot of information on this topic at the GLMM FAQ. However, in your particular case, I would suggest usingbecause you don't need any of the stuff that lmer offers (higher speed, handling of crossed random effects, GLMMs ...). lme should give you exactly the same coefficient and variance estimates but will also compute df and p-values for you (which do make  sense in a ""classical"" design such as you appear to have). You may also want to consider the random term ~status|experiment (allowing for variation of status effects across blocks, or equivalently including a status-by-experiment interaction).  Posters above are also correct that your t statistics are so large that your p-value will definitely be <0.05, but I can imagine you would like ""real"" p-values."
When to use Fisher and Neyman-Pearson framework?,"
I've been reading a lot lately about the differences between Fisher's method of hypothesis testing and the Neyman-Pearson school of thought.
My question is, ignoring philosophical objections for a moment; when should we use the Fisher's approach of statistical modelling and when should be use the Neyman-Pearson method of significance levels et cetera? Is there a practical way of deciding which viewpoint to endorse in any given practical problem?
","['hypothesis-testing', 'p-value', 'methodology']","Let me start by defining the terms of the discussion as I see them.  A p-value is the probability of getting a sample statistic (say, a sample mean) as far as, or further from some reference value than your sample statistic, if the reference value were the true population parameter.  For example, a p-value answers the question: what is the probability of getting a sample mean IQ more than $|\bar x-100|$ points away from 100, if 100 is really the mean of the population from which your sample was drawn.  Now the issue is, how should that number be employed in making a statistical inference?  Fisher thought that the p-value could be interpreted as a continuous measure of evidence against the null hypothesis.  There is no particular fixed value at which the results become 'significant'.  The way I usually try to get this across to people is to point out that, for all intents and purposes, p=.049 and p=.051 constitute an identical amount of evidence against the null hypothesis (cf. @Henrik's answer here).  On the other hand, Neyman & Pearson thought you could use the p-value as part of a formalized decision making process.  At the end of your investigation, you have to either reject the null hypothesis, or fail to reject the null hypothesis.  In addition, the null hypothesis could be either true or not true.  Thus, there are four theoretical possibilities (although in any given situation, there are just two): you could make a correct decision (fail to reject a true--or reject a false--null hypothesis), or you could make a type I or type II error (by rejecting a true null, or failing to reject a false null hypothesis, respectively).  (Note that the p-value is not the same thing as the type I error rate, which I discuss here.)  The p-value allows the process of deciding whether or not to reject the null hypothesis to be formalized.  Within the Neyman-Pearson framework, the process would work like this: there is a null hypothesis that people will believe by default in the absence of sufficient evidence to the contrary, and an alternative hypothesis that you believe may be true instead.  There are some long-run error rates that you will be willing to live with (note that there is no reason these have to be 5% and 20%).  Given these things, you design your study to differentiate between those two hypotheses while maintaining, at most, those error rates, by conducting a power analysis and conducting your study accordingly.  (Typically, this means having sufficient data.)  After your study is completed, you compare your p-value to $\alpha$ and reject the null hypothesis if $p<\alpha$; if it's not, you fail to reject the null hypothesis.  Either way, your study is complete and you have made your decision.  The Fisherian and Neyman-Pearson approaches are not the same.  The central contention of the Neyman-Pearson framework is that at the end of your study, you have to make a decision and walk away.  Allegedly, a researcher once approached Fisher with 'non-significant' results, asking him what he should do, and Fisher said, 'go get more data'.  Personally, I find the elegant logic of the Neyman-Pearson approach very appealing.  But I don't think it's always appropriate.  To my mind, at least two conditions must be met before the Neyman-Pearson framework should be considered:  When these conditions aren't met, the p-value can still be interpreted in keeping with Fisher's ideas.  Moreover, it seems likely to me that most of the time these conditions are not met.  Here are some easy examples that come to mind, where tests are run, but the above conditions are not met:  "
Essential data checking tests,"
In my job role I often work with other people's datasets, non-experts bring me clinical data and I help them to summarise it and perform statistical tests.
The problem I am having is that the datasets I am brought are almost always riddled with typos, inconsistencies, and all sorts of other problems. I am interested to know if other people have standard tests which they do to try to check any datasets that come in.
I used to draw histograms of each variable just to have a look but I now realise there are lots of horrible errors that can survive this test. For example, I had a repeated measures dataset the other day where, for some individuals, the repeated measure was identical at Time 2 as it was at Time 1. This was subsequently proved to be incorrect, as you would expect. Another dataset had an individual who went from being very severely disordered (represented by a high score) to being problem-free, represented by 0's across the board. This is just impossible, although I couldn't prove it definitively.
So what basic tests can I run on each dataset to make sure that they don't have typos and they don't contain impossible values?
Thanks in advance! 
","['dataset', 'outliers', 'checking']","It helps to understand how the data were recorded.Let me share a story.  Once, long ago, many datasets were stored only in fading hardcopy.  In those dark days I contracted with an organization (of great pedigree and size; many of you probably own its stock) to computerize about 10^5 records of environmental monitoring data at one of its manufacturing plants.  To do this, I personally marked up a shelf of laboratory reports (to show where the data were), created data entry forms, and contracted with a temp agency for literate workers to type the data into the forms.  (Yes, you had to pay extra for people who could read.)  Due to the value and sensitivity of the data, I conducted this process in parallel with two workers at a time (who usually changed from day to day).  It took a couple of weeks.  I wrote software to compare the two sets of entries, systematically identifying and correcting all the errors that showed up.Boy were there errors!  What can go wrong?  A good way to describe and measure errors is at the level of the basic record, which in this situation was a description of a single analytical result (the concentration of some chemical, often) for a particular sample obtained at a given monitoring point on a given date.  In comparing the two datasets, I found:Errors of omission: one dataset would include a record, another would not.  This usually happened because either (a) a line or two would be overlooked at the bottom of a page or (b) an entire page would be skipped.Apparent errors of omission that were really data-entry mistakes.  A record is identified by a monitoring point name, a date, and the ""analyte"" (usually a chemical name).  If any of these has a typographical error, it will not be matched to the other records with which it is related.  In effect, the correct record disappears and an incorrect record appears.Fake duplication.  The same results can appear in multiple sources, be transcribed multiple times, and seem to be true repeated measures when they are not.  Duplicates are straightforward to detect, but deciding whether they are erroneous depends on knowing whether duplicates should even appear in the dataset.  Sometimes you just can't know.Frank data-entry errors.  The ""good"" ones are easy to catch because they change the type of the datum: using the letter ""O"" for the digit ""0"", for instance, turns a number into a non-number.  Other good errors change the value so much it can readily be detected with statistical tests.  (In one case, the leading digit in ""1,000,010 mg/Kg"" was cut off, leaving a value of 10.  That's an enormous change when you're talking about a pesticide concentration!) The bad errors are hard to catch because they change a value into one that fits (sort of) with the rest of the data, such as typing ""80"" for ""50"".  (This kind of mistake happens with OCR software all the time.)Transpositions.  The right values can be entered but associated with the wrong record keys.  This is insidious, because the global statistical characteristics of the dataset might remain unaltered, but spurious differences can be created between groups.  Probably only a mechanism like double-entry is even capable of detecting these errors.Once you are aware of these errors and know, or have a theory, of how they occur, you can write scripts to troll your datasets for the possible presence of such errors and flag them for further attention.  You cannot always resolve them, but at least you can include a ""comment"" or ""quality flag"" field to accompany the data throughout their later analysis.Since that time I have paid attention to data quality issues and have had many more opportunities to make comprehensive checks of large statistical datasets.  None is perfect; they all benefit from quality checks.  Some of the principles I have developed over the years for doing this includeWhenever possible, create redundancy in data entry and data transcription procedures: checksums, totals, repeated entries: anything to support automatic internal checks of consistency.If possible, create and exploit another database which describes what the data should look like: that is, computer-readable metadata.  For instance, in a drug experiment you might know in advance that every patient will be seen three times.  This enables you to create a database with all the correct records and their identifiers with the values just waiting to be filled in.  Fill them in with the data given you and then check for duplicates, omissions, and unexpected data.Always normalize your data (specifically, get them into at least fourth normal form), regardless of how you plan to format the dataset for analysis.  This forces you to create tables of every conceptually distinct entity you are modeling.  (In the environmental case, this would include tables of monitoring locations, samples, chemicals (properties, typical ranges, etc.), tests of those samples (a test usually covers a suite of chemicals), and the individual results of those tests.  In so doing you create many effective checks of data quality and consistency and identify many potentially missing or duplicate or inconsistent values.This effort (which requires good data processing skills but is straightforward) is astonishingly effective.  If you aspire to analyze large or complex datasets and do not have good working knowledge of relational databases and their theory, add that to your list of things to be learned as soon as possible.  It will pay dividends throughout your career.Always perform as many ""stupid"" checks as you possibly can.  These are automated verification of obvious things such that dates fall into their expected periods, the counts of patients (or chemicals or whatever) always add up correctly, that values are always reasonable (e.g., a pH must be between 0 and 14 and maybe in a much narrower range for, say, blood pH readings), etc.  This is where domain expertise can be the most help: the statistician can fearlessly ask stupid questions of the experts and exploit the answers to check the data.Much more can be said of course--the subject is worth a book--but this should be enough to stimulate ideas."
When to use an offset in a Poisson regression? [duplicate],"







This question already has answers here:
                                
                            




In a Poisson model, what is the difference between using time as a covariate or an offset?

                                (2 answers)
                            

Closed 2 years ago.



Does anybody know why offset in a Poisson regression is used? What do you achieve by this? 
","['poisson-regression', 'offset']","Here is an example of application.Poisson regression is typically used to model count data. But, sometimes, it is more relevant to model rates instead of counts. This is relevant when, e.g., individuals are not followed the same amount of time. For example, six cases over 1 year should not amount to the same as six cases over 10 years. So, instead of having$$\log \mu_x = \beta_0 + \beta_1 x$$(where $\mu_x$ is the expected count for those with covariate $x$), you have$$\log \tfrac{\mu_x}{t_x} = \beta'_0 + \beta'_1 x$$(where $t_x$ is the exposure time for those with covariate $x$). Now, the last equation could be rewritten$$\log \mu_x = \log t_x + \beta'_0 + \beta'_1 x$$and $\log t_x$ plays the role of an offset."
Why to optimize max log probability instead of probability,"
In most machine learning tasks where you can formulate some probability $p$ which should be maximised, we would actually optimize the log probability $\log p$ instead of the probability for some parameters $\theta$. E.g. in maximum likelihood training, it's usually the log-likelihood. When doing this with some gradient method, this involves a factor:
$$ \frac{\partial \log p}{\partial \theta} = \frac{1}{p} \cdot \frac{\partial p}{\partial \theta} $$
See here or here for some examples.
Of course, the optimization is equivalent, but the gradient will be different, so any gradient-based method will behave different (esp. stochastic gradient methods).
Is there any justification that the $\log p$ gradient works better than the $p$ gradient?
","['probability', 'optimization', 'likelihood']","Gradient methods generally work better optimizing $\log p(x)$ than $p(x)$ because the gradient of $\log p(x)$ is generally more well-scaled. That is, it has a size that consistently and helpfully reflects the objective function's geometry, making it easier to select an appropriate step size and get to the optimum in fewer steps.To see what I mean, compare the gradient optimization process for $p(x) = \exp(-x^2)$ and $f(x) = \log p(x) = -x^2$. At any point $x$, the gradient of $f(x)$ is $$f'(x) = -2x.$$ If we multiply that by $1/2$, we get the exact step size needed to get to the global optimum at the origin, no matter what $x$ is. This means that we don't have to work too hard to get a good step size (or ""learning rate"" in ML jargon). No matter where our initial point is, we just set our step to half the gradient and we'll be at the origin in one step. And if we don't know the exact factor that is needed, we can just pick a step size around 1, do a bit of line search, and we'll find a great step size very quickly, one that works well no matter where $x$ is. This property is robust to translation and scaling of $f(x)$. While scaling $f(x)$ will cause the optimal step scaling to differ from 1/2, at least the step scaling will be the same no matter what $x$ is, so we only have to find one parameter to get an efficient gradient-based optimization scheme.In contrast, the gradient of $p(x)$ has very poor global properties for optimization. We have $$p'(x) = f'(x) p(x)= -2x \exp(-x^2).$$ This multiplies the perfectly nice, well-behaved gradient $-2x$ with a factor $\exp(-x^2)$ which decays (faster than) exponentially as $x$ increases. At $x = 5$, we already have $\exp(-x^2) = 1.4 \cdot 10^{-11}$, so a step along the gradient vector is about $10^{-11}$ times too small. To get a reasonable step size toward the optimum, we'd have to scale the gradient by the reciprocal of that, an enormous constant $\sim 10^{11}$. Such a badly-scaled gradient is worse than useless for optimization purposes - we'd be better off just attempting a unit step in the uphill direction than setting our step by scaling against $p'(x)$! (In many variables $p'(x)$ becomes a bit more useful since we at least get directional information from the gradient, but the scaling issue remains.)In general there is no guarantee that $\log p(x)$ will have such great gradient scaling properties as this toy example, especially when we have more than one variable. However, for pretty much any nontrivial problem, $\log p(x)$ is going to be way, way better than $p(x)$. This is because the likelihood is a big product with a bunch of terms, and the log turns that product into a sum, as noted in several other answers. Provided the terms in the likelihood are well-behaved from an optimization standpoint, their log is generally well-behaved, and the sum of well-behaved functions is well-behaved. By well-behaved I mean $f''(x)$ doesn't change too much or too rapidly, leading to a nearly quadratic function that is easy to optimize by gradient methods. The sum of a derivative is the derivative of the sum, no matter what the derivative's order, which helps to ensure that that big pile of sum terms has a very reasonable second derivative!"
"What is ""restricted maximum likelihood"" and when should it be used?","
I have read in the abstract of this paper that:

""The maximum likelihood (ML) procedure of Hartley aud Rao is modified by adapting a transformation from Patterson and Thompson which partitions the likelihood render normality into two parts, one being free of the fixed effects. Maximizing this part yields what are called restricted maximum likelihood (REML) estimators.""

I also read in the abstract of this paper that REML:

""takes into account the loss in degrees of freedom resulting from estimating fixed effects.""

Sadly I don't have access to the full text of those papers (and probably would not understand if I did).
Also, what are the advantages of REML vs. ML? Under what circumstances may REML be preferred over ML (or vice versa) when fitting a mixed effects model?
Please give an explanation suitable for someone with a high-school (or just beyond) mathematics background!
","['mixed-model', 'maximum-likelihood', 'reml']","As per ocram's answer, ML is biased for the estimation of variance components. But observe that the bias gets smaller for larger sample sizes. Hence in answer to your questions ""...what are the advantages of REML vs ML ? Under what circumstances may REML be preferred over ML (or vice versa) when fitting a mixed effects model ?"", for small sample sizes REML is preferred. However, likelihood ratio tests for REML require exactly the same fixed effects specification in both models. So, to compare models with different fixed effects (a common scenario) with an LR test, ML must be used.REML takes account of the number of (fixed effects) parameters estimated, losing 1 degree of freedom for each. This is achieved by applying ML to the least squares residuals, which are independent of the fixed effects. "
How should outliers be dealt with in linear regression analysis?,"
Often times a statistical analyst is handed a set dataset and asked to fit a model using a technique such as linear regression.  Very frequently the dataset is accompanied with a disclaimer similar to ""Oh yeah, we messed up collecting some of these data points -- do what you can"".
This situation leads to regression fits that are heavily impacted by the presence of outliers that may be erroneous data. Given the following:

It is dangerous from both a scientific and moral standpoint to throw out data for no reason other than it ""makes the fit look bad"".
In real life, the people who collected the data are frequently not available to answer questions such as ""when generating this data set, which of the points did you mess up, exactly?""

What statistical tests or rules of thumb can be used as a basis for excluding outliers in linear regression analysis?
Are there any special considerations for multilinear regression?
","['regression', 'outliers']",
Why does k-means clustering algorithm use only Euclidean distance metric?,"
Is there a specific purpose in terms of efficiency or functionality why the k-means algorithm does not use for example cosine (dis)similarity as a distance metric, but can only use the Euclidean norm? In general, will K-means method comply and be correct when other distances than Euclidean are considered or used?
[Addition by @ttnphns. The question is two-fold. ""(Non)Euclidean distance"" may concern distance between two data points or distance between a data point and a cluster centre. Both ways have been attempted to address in the answers so far.]
","['clustering', 'k-means', 'distance-functions', 'euclidean']",
Shape of confidence interval for predicted values in linear regression,"
I have noticed that the confidence interval for predicted values in an linear regression tends to be narrow around the mean of the predictor and fat around the minimum and maximum values of the predictor. This can be seen in plots of these 4 linear regressions:

I initially thought this was because most values of the predictors were concentrated around the mean of the predictor. However, I then noticed that the narrow middle of the confidence interval would occur even if many values of were concentrated around the extremes of the predictor, as in the bottom left linear regression, which lots of values of the predictor are concentrated around the minimum of the predictor.
is anyone able to explain why confidence intervals for the predicted values in an linear regression tend to be narrow in the middle and fat at the extremes?
","['regression', 'confidence-interval', 'linear-model', 'standard-error']","I'll discuss it in intuitive terms.Both confidence intervals and prediction intervals in regression take account of the fact that the intercept and slope are uncertain - you estimate the values from the data, but the population values may be different (if you took a new sample, you'd get different estimated values).A regression line will pass through $(\bar x, \bar y)$, and it's best to center the discussion about changes to the fit around that point - that is to think about the line $y= a + b(x-\bar x)$ (in this formulation, $\hat a = \bar y$).If the line went through that $(\bar x, \bar y)$ point, but the slope were little higher or lower (i.e. if the height of the line at the mean was fixed but the slope was a little different), what would that look like?You'd see that the new line would move further away from the current line near the ends than near the middle, making a kind of slanted X that crossed at the mean (as each of the purple lines below do with respect to the red line; the purple lines represent the estimated slope $\pm$ two standard errors of the slope).If you drew a collection of such lines with the slope varying a little from its estimate, you'd see the distribution of predicted values near the ends 'fan out' (imagine the region between the two purple lines shaded in grey, for example, because we sampled again and drew many such slopes near the estimated one; We can get a sense of this by bootstrapping a line through the point ($\bar{x},\bar{y}$)). Here's an example using 2000 resamples with a parametric bootstrap:If instead you take account of the uncertainty in the constant (making the line pass close to but not quite through $(\bar x, \bar y)$), that moves the line up and down, so intervals for the mean at any $x$ will sit above and below the fitted line.(Here the purple lines are $\pm$ two standard errors of the constant term either side of the estimated line).When you do both at once (the line may be up or down a tiny bit, and the slope may be slightly steeper or shallower), then you get some amount of spread at the mean, $\bar x$, because of the uncertainty in the constant, and you get some additional fanning out due to the slope's uncertainty, between them producing the characteristic hyperbolic shape of your plots.That's the intuition.Now, if you like, we can consider a little algebra (but it's not essential):It's actually the square root of the sum of the squares of those two effects - you can see it in the confidence interval's formula. Let's build up the pieces:The $a$ standard error with $b$ known is $\sigma /\sqrt{n}$ (remember $a$ here is the expected value of $y$ at the mean of $x$, not the usual intercept; it's just a standard error of a mean). That's the standard error of the line's position at the mean ($\bar x$).The $b$ standard error with $a$ known is $\sigma/\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}$. The effect of uncertainty in slope at some value $x^*$ is multiplied by how far you are from the mean ($x^*-\bar x$) (because the change in level is the change in slope times the distance you move), giving $(x^*-\bar x)\cdot\sigma/\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}$.Now the overall effect is just the square root of the sum of the squares of those two things (why? because variances of uncorrelated things add, and if you write your line in the $y= a + b(x-\bar x)$  form, the estimates of $a$ and $b$ are uncorrelated. So the overall standard error is the square root of the overall variance, and the variance is the sum of the variances of the components - that is, we have$\sqrt{(\sigma /\sqrt{n})^2+ \left[(x^*-\bar x)\cdot\sigma/\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\right]^2 }$A little simple manipulation gives the usual term for the standard error of the estimate of the mean value at $x^*$:$\sigma\sqrt{\frac{1}{n}+ \frac{(x^*-\bar x)^2}{\sum_{i=1}^n (x_i-\bar{x})^2} }$If you draw that as a function of $x^*$, you'll see it forms a curve (looks like a smile) with a minimum at $\bar x$, that gets bigger as you move out. That's what gets added to / subtracted from the fitted line (well, a multiple of it is, in order to get a desired confidence level).[With prediction intervals, there's also the variation in position due to the process variability; this adds another term that shifts the limits up and down, making a much wider spread, and because that term usually dominates the sum under the square root, the curvature is much less pronounced.]"
"Feature selection for ""final"" model when performing cross-validation in machine learning","
I am getting a bit confused about feature selection and machine learning
and I was wondering if you could help me out.  I have a microarray dataset that is
classified into two groups and has 1000s of features.  My aim is to get a small number of genes (my features) (10-20) in a signature that I will in theory be able to apply to
other datasets to optimally classify those samples.  As I do not have that many samples (<100), I am not using a test and training set but using Leave-one-out cross-validation to help
determine the robustness.  I have read that one should perform feature selection for each split of the samples i.e.

Select one sample as the test set
On the remaining samples perform feature selection
Apply machine learning algorithm to remaining samples using the features selected
Test whether the test set is correctly classified
Go to 1.

If you do this, you might get different genes each time, so how do you
get your ""final"" optimal gene classifier? i.e. what is step 6.
What I mean by optimal is the collection of genes that any further studies
should use.  For example, say I have a cancer/normal dataset and I want
to find the top 10 genes that will classify the tumour type according to
an SVM.  I would like to know the set of genes plus SVM parameters that
could be used in further experiments to see if it could be used as a
diagnostic test.
","['machine-learning', 'classification', 'cross-validation', 'feature-selection', 'genetics']","Whether you use LOO or K-fold CV, you'll end up with different features since the cross-validation iteration must be the most outer loop, as you said. You can think of some kind of voting scheme which would rate the n-vectors of features you got from your LOO-CV (can't remember the paper but it is worth checking the work of Harald Binder or Antoine Cornuéjols). In the absence of a new test sample, what is usually done is to re-apply the ML algorithm to the whole sample once you have found its optimal cross-validated parameters. But proceeding this way, you cannot ensure that there is no overfitting (since the sample was already used for model optimization).Or, alternatively, you can use embedded methods which provide you with features ranking through a measure of variable importance, e.g. like in Random Forests (RF). As cross-validation is included in RFs, you don't have to worry about the $n\ll p$ case or curse of dimensionality. Here are nice papers of their applications in gene expression studies:Since you are talking of SVM, you can look for penalized SVM."
How to interpret an inverse covariance or precision matrix?,"
I was wondering whether anyone could point me to some references that discuss the interpretation of the elements of the inverse covariance matrix, also known as the concentration matrix or the precision matrix.
I have access to Cox and Wermuth's Multivariate Dependencies, but what I'm looking for is an interpretation of each element in the inverse matrix.  Wikipedia states: ""The elements of the precision matrix have an interpretation in terms of partial correlations and partial variances,"" which leads me to this page.  Is there an interpretation without using linear regression?  IE, in terms of covariances or geometry?
","['interpretation', 'covariance-matrix', 'precision-matrix']",
What do the residuals in a logistic regression mean?,"
In answering this question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to:  

Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman & Hall.  

I do not have a copy of that.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?
","['r', 'logistic', 'generalized-linear-model', 'residuals', 'aic']","The easiest residuals to understand are the deviance residuals as when squared these sum to -2 times the log-likelihood. In its simplest terms logistic regression can be understood in terms of fitting the function $p = \text{logit}^{-1}(X\beta)$ for known $X$ in such a way as to minimise the total deviance, which is the sum of squared deviance residuals of all the data points.The (squared) deviance of each data point is equal to (-2 times) the logarithm of the difference between its predicted probability $\text{logit}^{-1}(X\beta)$  and the complement of its actual value (1 for a control; a 0 for a case) in absolute terms. A perfect fit of a point (which never occurs) gives a deviance of zero as log(1) is zero. A poorly fitting point has a large residual deviance as -2 times the log of a very small value is a large number. Doing logistic regression is akin to finding a beta value such that the sum of squared deviance residuals is minimised. This can be illustrated with a plot, but I don't know how to upload one."
"Explain the difference between multiple regression and multivariate regression, with minimal use of symbols/math","
Are multiple and multivariate regression really different? What is a variate anyways?
","['regression', 'multiple-regression', 'terminology', 'multivariate-regression']","Very quickly, I would say: 'multiple' applies to the number of predictors that enter the model (or equivalently the design matrix) with a single outcome (Y response), while 'multivariate' refers to a matrix of response vectors. Cannot remember the author who starts its introductory section on multivariate modeling with that consideration, but I think it is Brian Everitt in his textbook An R and S-Plus Companion to Multivariate Analysis. For a thorough discussion about this, I would suggest to look at his latest book, Multivariable Modeling and Multivariate Analysis for the Behavioral Sciences.For 'variate', I would say this is a common way to refer to any random variable that follows a known or hypothesized distribution, e.g. we speak of gaussian variates $X_i$ as a series of observations drawn from a normal distribution (with parameters $\mu$ and $\sigma^2$). In probabilistic terms, we said that these are some random realizations of X, with mathematical expectation $\mu$, and about 95% of them are expected to lie on the range $[\mu-2\sigma;\mu+2\sigma]$ ."
Euclidean distance is usually not good for sparse data (and more general case)?,"
I have seen somewhere that classical distances (like Euclidean distance) become weakly discriminant when we have multidimensional and sparse data. Why? Do you have an example of two sparse data vectors where the Euclidean distance does not perform well? In this case which similarity should we use?
","['machine-learning', 'clustering', 'data-mining', 'sparse', 'euclidean']","Here is a simple toy example illustrating the effect of dimension in a discrimination problem e.g. the problem you face when you want to say if something is observed or if only random effect is observed (this problem is a classic in science). Heuristic.  The key issue here is that the Euclidian norm gives the same importance to any direction. This constitutes a lack of prior, and as you certainly know in high dimension there is no free lunch (i.e. if you have no prior idea of what you are searching for, then there is no reason why some noise would not look like what you are searching for, this is tautology ...). I would say that for any problem there is a limit of information that is necessary to find something else than noise. This limit is related somehow to the ""size"" of the area you are trying to explore with regard to the ""noise"" level (i.e. level of uninformative content). In high dimension if you have the prior that your signal is sparse then you can remove (i.e. penalize) non sparse vector with a metric that fills the space with sparse vector or by using a thresholding technique. Framework Assume that $\xi$ is a gaussian vector with mean $\nu$ and diagonal covariance $\sigma Id$ ($\sigma$ is known) and that you want to test the simple hypothesis $$H_0: \;\nu=0,\; Vs \; H_{\theta}: \; \nu=\theta $$
(for a given $\theta\in \mathbb{R}^n$) $\theta$ is not necessarily known in advance. Test statistic with energy. The intuition you certainly have is that it is a good idea to evaluate the norm/energy $\mathcal{E}_n=\frac{1}{n}\sum_{i=1}^n\xi_i^2$ of you observation $\xi$ to build a test statistic.  Actually you can construct a standardized centered (under $H_0$) version $T_n$ of the energy $T_n=\frac{\sum_i\xi_i^2-\sigma^2}{\sqrt{2n\sigma^4}}$. That makes a critical region at level $\alpha$ of the form $\{T_n\geq v_{1-\alpha}\}$ for a well chosen $v_{1-\alpha}$Power of the test and dimension. In this case it is an easy probability exercise to show the following formula for the power of your test: $$P_{\theta}(T\leq v_{1-\alpha})=P\left (Z\leq \frac{v_{1-\alpha}}{\sqrt{1+2\|\theta\|_2^2/(n\sigma^2)}}-\frac{\|\theta\|^2_2}{\sqrt{2n\sigma^4+2\sigma^2\|\theta\|_2^2/(n\sigma^2)}}\right )$$
  with $Z$ a sum of $n$ iid random variables with $\mathbb{E}[Z]=0$ and $Var(Z)=1$. This means that the power of your test is increased by the energy of your signal $\|\theta\|^2_2$ and decreased by $n$. Practically speaking this means that when you increase the size $n$ of your problem if it does not increase the strength of the signal at the same time then you are adding uninformative information to your observation (or you are reducing the proportion of useful information in the information you have): this is like adding noise and reduces the power of the test (i.e. it is more likely that you are gonna say nothing is observed while there is actually something). Toward a test with a threshold statistic. If you do not have much energy in your signal but if you know a linear transformation that can help you to have this energy concentrated in a small part of your signal, then you can build a test statistic that will only evaluate the energy for the small part of your signal. If you known in advance where it is concentrated (for example you known there cannot be high frequencies in your signal) then you can obtain a power in the preceding test with $n$ replaced by a small number and $\|\theta\|^2_2$ almost the same... If you do not know it in advance you have to estimate it this leads to well known thresholding tests. Note that this argument is exactly at the root many papers such as "
"Why is ridge regression called ""ridge"", why is it needed, and what happens when $\lambda$ goes to infinity?","
Ridge regression coefficient estimate $\hat{\beta}^R$ are the values that minimize the 
$$ \text{RSS} + \lambda \sum_{j=1}^p\beta_j^2. $$
My questions are:

If $\lambda = 0$, then we see that the expression above reduces to the usual RSS. What if $\lambda \to \infty$?  I do not understand the textbook explanation of the behaviour of the coefficients.
To aid in understanding the concept behind a particular term, why is the term called RIDGE Regression? (Why ridge?) And what could have been wrong with the usual/common regression that there is a need to introduce a new concept called ridge regression? 

Your insights would be great.  
","['machine-learning', 'ridge-regression', 'history', 'etymology']","Since you ask for insights, I'm going to take a fairly intuitive approach rather than a more mathematical tack:Following the concepts in my answer here, we can formulate a ridge regression as a regression with dummy data by adding $p$ (in your formulation) observations, where $y_{n+j}=0$, $x_{j,n+j}=\sqrt{\lambda}$ and $x_{i,n+j}=0$ for $i\neq j$. If you write out the new RSS for this expanded data set, you'll see the additional observations each add a term of the form $(0-\sqrt{\lambda}\beta_j)^2=\lambda\beta_j^2$, so the new RSS is the original $\text{RSS} + \lambda \sum_{j=1}^p\beta_j^2$ -- and minimizing the RSS on this new, expanded data set is the same as minimizing the ridge regression criterion.So what can we see here? As $\lambda$ increases, the additional $x$-rows  each have one component that increases, and so the influence of these points also increases. They pull the fitted hyperplane toward themselves. Then as $\lambda$ and the corresponding components of the $x$'s go off to infinity, all the involved coefficients ""flatten out"" to $0$.That is, as $\lambda\to\infty$, the penalty will dominate the minimization, so the $\beta$s will go to zero. If the intercept is not penalized (the usual case) then the model shrinks more and more toward the mean of the response.I'll give an intuitive sense of why we're talking about ridges first (which also suggests why it's needed), then tackle a little history. The first is adapted from my answer here:If there's multicollinearity, you get a ""ridge"" in the likelihood function (likelihood is a function of the $\beta$'s). This in turn yields a long ""valley"" in the RSS (since RSS=$-2\log\mathcal{L}$). Ridge regression ""fixes"" the ridge - it adds a penalty that turns the ridge into a nice peak in likelihood space, equivalently a nice depression in the criterion we're minimizing:
[Clearer image]The actual story behind the name is a little more complicated. In 1959 A.E. Hoerl [1] introduced ridge analysis for response surface methodology, and it very soon [2] became adapted to dealing with multicollinearity in regression ('ridge regression'). See for example, the discussion by R.W. Hoerl in [3], where it  describes Hoerl's (A.E. not R.W.) use of contour plots of the response surface* in the identification of where to head to find local optima (where one 'heads up the ridge'). In ill-conditioned problems, the issue of a very long ridge arises, and insights and methodology from ridge analysis are adapted to the related issue with the likelihood/RSS in regression, producing ridge regression.* examples of response surface contour plots (in the case of quadratic response) can be seen here (Fig 3.9-3.12).That is, ""ridge"" actually refers to the characteristics of the function we were attempting to optimize, rather than to adding a ""ridge"" (+ve diagonal) to the $X^TX$ matrix (so while ridge regression does add to the diagonal, that's not why we call it 'ridge' regression).For some additional information on the need for ridge regression, see the first link under list item 2. above.References:[1]: Hoerl, A.E. (1959). Optimum solution of many variables equations. Chemical Engineering Progress, 
55 (11) 69-78.[2]: Hoerl, A.E. (1962). Applications of ridge analysis to regression problems. Chemical Engineering Progress, 
58 (3) 54-59.[3] Hoerl, R.W. (1985). Ridge Analysis 25 Years Later.
American Statistician, 39 (3), 186-192 "
"Resampling / simulation methods: monte carlo, bootstrapping, jackknifing, cross-validation, randomization tests, and permutation tests","
I am trying to understand difference between different resampling methods (Monte Carlo simulation, parametric bootstrapping, non-parametric bootstrapping, jackknifing, cross-validation, randomization tests, and permutation tests) and their implementation in my own context using R.
Say I have the following situation – I want to perform ANOVA with a Y variable (Yvar) and X variable (Xvar). Xvar is categorical. I am interested in the following things:
(1) Significance of p-values – false discovery rate
(2) effect size of Xvar levels  
Yvar <- c(8,9,10,13,12, 14,18,12,8,9,   1,3,2,3,4)
Xvar <- c(rep(""A"", 5),  rep(""B"", 5),    rep(""C"", 5))
mydf <- data.frame (Yvar, Xvar)

Could you gel me to explain the  sampling differences with explicit worked examples how these resampling method work?
Edits:
Here are my attempts:
Bootstrap 
10 bootstrap samples, sample number of samples with replacement, means that samples can be repeated   
boot.samples <- list()
for(i in 1:10) {
   t.xvar <- Xvar[ sample(length(Xvar), length(Xvar), replace=TRUE) ]
   t.yvar <- Yvar[ sample(length(Yvar), length(Yvar), replace=TRUE) ]
   b.df <- data.frame (t.xvar, t.yvar) 
   boot.samples[[i]] <- b.df 
}
str(boot.samples)
 boot.samples[1]

Permutation:
10 permutation samples, sample number of samples without replacement
 permt.samples <- list()
    for(i in 1:10) {
       t.xvar <- Xvar[ sample(length(Xvar), length(Xvar), replace=FALSE) ]
       t.yvar <- Yvar[ sample(length(Yvar), length(Yvar), replace=FALSE) ]
       b.df <- data.frame (t.xvar, t.yvar) 
       permt.samples[[i]] <- b.df 
    }
    str(permt.samples)
    permt.samples[1]

Monte Caro Simulation 
Although the term ""resampling"" is often used to refer to any repeated random or pseudorandom sampling simulation, when the ""resampling"" is done from a known theoretical distribution, the correct term is ""Monte Carlo"" simulation.
I am not sure about all above terms and whether my above edits are correct. I did find some information on jacknife but I could not tame it to my situation. 
","['r', 'bootstrap', 'resampling', 'jackknife', 'permutation-test']","We can find different Resampling methods, or loosely called ""simulation"" methods, that depend upon resampling or shuffling of the samples. There might be differences in opinions with respect to proper terminology, but the following discussion tries to generalize and simplify what is available in the appropriate literature:Resampling methods are used in (1) estimating precision / accuracy of sample statistics through using subset of data (e.g. Jackknifing) or drawing randomly with replacement from a set of data points (e.g. bootstrapping) (2)  Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests) (3) Validating models by using random subsets (bootstrapping, cross validation) (see wikipedia: resampling methods)BOOTSTRAPING""Bootstrapping is a statistical method for estimating the sampling distribution of an estimator by sampling with replacement from the original sample"". The method assigns measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates.The basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modeled by resampling the sample data and performing inference on (resample → sample). As the population is unknown, the true error in a sample statistic against its population value is unknowable. In bootstrap-resamples, the 'population' is in fact the sample, and this is known; hence the quality of inference from resample data → 'true' sample is measurable."" see wikipediaIn univariate problems, it is usually acceptable to resample the individual observations with replacement (""case resampling""). Here we resample the data with replacement, and the size of the resample must be equal to the size of the original data set.In regression problems,   case resampling refers to the simple scheme of resampling individual cases - often rows of a data set in regression problems, the explanatory variables are often fixed, or at least observed with more control than the response variable. Also, the range of the explanatory variables defines the information available from them. Therefore, to resample cases means that each bootstrap sample will lose some information (see Wikipedia). So it will be logical to sample rows of the data rather just Yvar.You can see some cases are repeated as we are sampling with replacement.""Parametric bootstrap -  a parametric model is fitted to the data, often by maximum likelihood, and samples of random numbers are drawn from this fitted model. Usually the sample drawn has the same sample size as the original data. Then the quantity, or estimate, of interest is calculated from these data. This sampling process is repeated many times as for other bootstrap methods. The use of a parametric model at the sampling stage of the bootstrap methodology leads to procedures which are different from those obtained by applying basic statistical theory to inference for the same model.""(see Wikipedia). The following is parametric bootstrap with normal distribution assumption with mean and standard deviation parameters.There are other variants of bootstrap, please consult the wikipedia page or any good statical book on resampling.JACKNIFE""The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size N, the jackknife estimate is found by aggregating the estimates of each N − 1 estimate in the sample."" see: wikipedia The following shows how to jackknife the Yvar.""the regular bootstrap and the jackknife, estimate the variability of a statistic from the variability of that statistic between subsamples, rather than from parametric assumptions. For the more general jackknife, the delete-m observations jackknife, the bootstrap can be seen as a random approximation of it. Both yield similar numerical results, which is why each can be seen as approximation to the other."" See this question on Bootstrap vs Jacknife.RANDOMIZATION TESTS""In parametric tests we randomly sample from one or more populations. We make certain assumptions about those populations, most commonly that they are normally distributed with equal variances. We establish a null hypothesis that is framed in terms of parameters, often of the form m1  -m2 = 0 . We use our sample statistics as estimates of the corresponding population parameters, and calculate a test statistic (such as a t test). For example: in  Student's t - test for differences in means when variances are unknown, but are considered to be equal.  The hypothesis of interest is that  H0: m1 = m2. One of alternative hypothesis would be stated as : HA: m1 < m2.
Given two samples drawn from populations 1 and 2, assuming that these are normally distributed populations with equal variances, and that the samples were drawn independently and at random from each population, then a statistic whose distribution is known can be elaborated to test H0.One way to avoid these distributional assumptions has been the approach now called non - parametric, rank - order, rank - like, and distribution - free statistics. These distribution - free statistics are usually criticized for being less ""efficient"" than the analogous test based on assuming the populations to be normally distributed.Another alternative approach is randomization approach - ""process of randomly assigning ranks to observations independent of one's knowledge of which sample an observation is a member.  A randomization test makes use of such a procedure, but does so by operating on the observations rather than the joint ranking of the observations.  For this reason, the distribution of an analogous statistic (the sum of the observations in one sample) cannot be easily tabulated, although it is theoretically possible to enumerate such a distribution"" (see)Randomization tests differ from parametric tests in almost every respect. (1) There is no requirement that we have random samples from one or more populations—in fact we usually have not sampled randomly. (2) We rarely think in terms of the populations from which the data came, and there is no need to assume anything about normality or homoscedasticity (3) Our null hypothesis has nothing to do with parameters, but is phrased rather vaguely, as, for example, the hypothesis that the treatment has no effect on the how participants perform.(4)  Because we are not concerned with populations, we are not concerned with estimating (or even testing) characteristics of those populations (5) We do calculate some sort of test statistic, however we do not compare that statistic to tabled distributions. Instead, we compare it to the results we obtain when we repeatedly randomize the data across the groups, and calculate the corresponding statistic for each randomization. (6) Even more than parametric tests, randomization tests emphasize the importance of random assignment of participants to treatments."" see.The type of randomization test that is very popular is permutation test. If our sample size is 12 and 5, the total permutation possible is C(12,5) = 792. If our sample sizes been 10 and 15 then over 3.2 million arrangements would have been possible. This is computing challenge: What then?  Sample.  When the universe of possible arrangements is too large to enumerate why not sample arrangements from this universe independently and at random?  The distribution of the test statistic over this series of samples can then be tabulated, its' mean and variance computed, and the error rate associated with an hypothesis test estimated.PERMUTATION TESTAccording to wikipedia ""A permutation test (also called a randomization test, re-randomization test, or an exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points. Permutation tests exist for any test statistic, regardless of whether or not its distribution is known. Thus one is always free to choose the statistic which best discriminates between hypothesis and alternative and which minimizes losses.""The difference between permutation and bootstrap is that bootstraps sample with replacement, and permutations sample without replacement. In either case, the time order of the observations is lost and hence volatility clustering is lost — thus assuring that the samples are under the null hypothesis of no volatility clustering.The permutations always have all of the same observations, so they are more like the original data than bootstrap samples. The expectation is that the permutation test should be more sensitive than a bootstrap test. The permutations destroy volatility clustering but do not add any other variability.See the question on permutation vs bootstrapping - ""The permutation test is best for testing hypotheses and bootstrapping is best for estimating confidence intervals"".So to perform permutation in this case we can just change replace = FALSE in the above bootstrap example.In case of more than one variable, just picking of the rows and reshuffling the order will not make any difference as the data will remain same. So we reshuffle the y variable. Something what you have done, but I do not think we need double reshuffling of both x and y variables (as you have done).MONTE CARLO METHODS""Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results; typically one runs simulations many times over in order to obtain the distribution of an unknown probabilistic entity. The name comes from the resemblance of the technique to the act of playing and recording results in a real gambling casino. "" see Wikipedia""In applied statistics, Monte Carlo methods are generally used for two purposes:(1) To compare competing statistics for small samples under realistic data conditions. Although Type I error and power properties of statistics can be calculated for data drawn from classical theoretical distributions (e.g., normal curve, Cauchy distribution) for asymptotic conditions (i. e, infinite sample size and infinitesimally small treatment effect), real data often do not have such distributions.(2) To provide implementations of hypothesis tests that are more efficient than exact tests such as permutation tests (which are often impossible to compute) while being more accurate than critical values for asymptotic distributions.Monte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice – or more frequently—for the efficiency of not having to track which permutations have already been selected).""Both MC and Permutation test are sometime collectively called randomization tests. The difference is in MC we sample the permutation samples, rather using all possible combinations [see] 21.CROSS VALIDATIONThe idea beyond cross validation is that models should be tested with data that were not used to fit the model. Cross validation is perhaps most often used in the context of prediction.""Cross-validation is a statistical method for validating a predictive model. Subsets of the data are held out for use as validating sets; a model is fit to the remaining data (a training set) and used to predict for the validation set. Averaging the quality of the predictions across the validation sets yields an overall measure of prediction accuracy.One form of cross-validation leaves out a single observation at a time; this is similar to the jackknife. Another, K-fold cross-validation, splits the data into K subsets; each is held out in turn as the validation set."" see Wikipedia . Cross validation is usually done with quantitative data. You can convert your qualitative (factor data) to quantitative someway to fit a linear model and test this model. The following is simple hold-out strategy where 50% of data is used for model prediction while rest is used for testing. Lets assume Xvar is also quantitative variable.Unlike bootstrap and permutation tests the cross-validation dataset for training and testing is different. The following figure shows a summary of resampling in different methods.Hope this helps a bit."
How to generate uniformly distributed points on the surface of the 3-d unit sphere?,"
I am wondering how to generate uniformly distributed points on the surface of the 3-d unit sphere? Also after generating those points, what is the best way to visualize and check whether they are truly uniform on the surface $x^2+y^2+z^2=1$?
",['random-generation'],"A standard method is to generate three standard normals and construct a unit vector from them.  That is, when $X_i \sim N(0,1)$ and $\lambda^2 = X_1^2 + X_2^2 + X_3^2$, then $(X_1/\lambda, X_2/\lambda, X_3/\lambda)$ is uniformly distributed on the sphere.  This method works well for $d$-dimensional spheres, too.In 3D you can use rejection sampling: draw $X_i$ from a uniform$[-1,1]$ distribution until the length of $(X_1, X_2, X_3)$ is less than or equal to 1, then--just as with the preceding method--normalize the vector to unit length.  The expected number of trials per spherical point equals $2^3/(4 \pi / 3)$ = 1.91.  In higher dimensions the expected number of trials gets so large this rapidly becomes impracticable.There are many ways to check uniformity.  A neat way, although somewhat computationally intensive, is with Ripley's K function.  The expected number of points within (3D Euclidean) distance $\rho$ of any location on the sphere is proportional to the area of the sphere within distance $\rho$, which equals $\pi\rho^2$.  By computing all interpoint distances you can compare the data to this ideal.General principles of constructing statistical graphics suggest a good way to make the comparison is to plot variance-stabilized residuals $e_i(d_{[i]} - e_i)$ against $i = 1, 2, \ldots, n(n-1)/2=m$ where $d_{[i]}$ is the $i^\text{th}$ smallest of the mutual distances and $e_i = 2\sqrt{i/m}$.  The plot should be close to zero.  (This approach is unconventional.)Here is a picture of 100 independent draws from a uniform spherical distribution obtained with the first method:Here is the diagnostic plot of the distances:The y scale suggests these values are all close to zero.Here is the accumulation of 100 such plots to suggest what size deviations might actually be significant indicators of non-uniformity:(These plots look an awful lot like Brownian bridges...there may be some interesting theoretical discoveries lurking here.)Finally, here is the diagnostic plot for a set of 100 uniform random points plus another 41 points uniformly distributed in the upper hemisphere only:Relative to the uniform distribution, it shows a significant decrease in average interpoint distances out to a range of one hemisphere.  That in itself is meaningless, but the useful information here is that something is non-uniform on the scale of one hemisphere.  In effect, this plot readily detects that one hemisphere has a different density than the other.  (A simpler chi-square test would do this with more power if you knew in advance which hemisphere to test out of the infinitely many possible ones.)"
"What is the difference between a ""link function"" and a ""canonical link function"" for GLM","
What's the difference between terms 'link function' and 'canonical link function'? Also, are there any (theoretical) advantages of using one over the other?
For example, a binary response variable can be modeled using many link functions such as logit, probit, etc. But, logit here is considered the ""canonical"" link function.
","['logistic', 'generalized-linear-model', 'link-function']","The above answers are more intuitive, so I try more rigor.What is a GLM?Let $Y=(y,\mathbf{x})$ denote a set of a response $y$ and $p$-dimensional covariate vector $\mathbf{x}=(x_1,\dots,x_p)$ with expected value $E(y)=\mu$. For $i=1,\dots,n$ independent observations, the distribution of each $y_i$ is an exponential family with density
$$
f(y_i;\theta_i,\phi)=\exp\left(\frac{y_i\theta_i-\gamma(\theta_i)}{\phi}+\tau(y_i,\phi)\right) = \alpha(y_i, \phi)\exp\left(\frac{y_i\theta_i-\gamma(\theta_i)}{\phi}\right)
$$
Here, the parameter of interest (natural or canonical parameter) is $\theta_i$, $\phi$ is a scale parameter (known or seen as a nuisance) and $\gamma$ and $\tau$ are known functions. The $n$-dimensional vectors of fixed input values for the $p$ explanatory variables are denoted by $\mathbf{x}_1,\dots,\mathbf{x}_p$. We assume that the input vectors influence (1) only via a linear function, the linear predictor,
$$
\eta_i=\beta_0+\beta_1x_{i1}+\dots+\beta_px_{ip}
$$
upon which $\theta_i$ depends. As it can be shown that $\theta=(\gamma')^{-1}(\mu)$, this dependency is established by connecting the linear predictor $\eta$ and $\theta$ via the mean. More specifically, the mean $\mu$ is seen as an invertible and smooth function of the linear predictor, i.e.
$$
g(\mu)=\eta\ \textrm{or}\ \mu=g^{-1}(\eta)
$$
Now to answer your question:The function $g(\cdot)$ is called the link function. If the function connects $\mu$, $\eta$ and $\theta$ such that $\eta \equiv\theta$, then this link is called canonical and has the form $g=(\gamma')^{-1}$.That's it. Then there are a number of desirable statistical properties of using the canonical link, e.g., the sufficient statistic is $X'y$ with
components $\sum_i x_{ij} y_i$ for $j = 1, \dots, p$, the Newton Method and Fisher scoring for finding the ML estimator coincide, these links simplify the derivation of the MLE, they ensure that some properties of linear regression (e.g., the sum of the residuals is 0) hold up or they ensure that $\mu$ stays within the range of the outcome variable.Hence they tend to be used by default. Note however, that there is no a priori reason why the effects in the model should be additive on the scale given by this or any other link."
How to efficiently manage a statistical analysis project?,"
We often hear of project management and design patterns in computer science, but less frequently in statistical analysis. However, it seems that a decisive step toward designing an effective and durable statistical project is to keep things organized. 
I often advocate the use of R and a consistent organization of files in separate folders (raw data file, transformed data file, R scripts, figures, notes, etc.). The main reason for this approach is that it may be easier to run your analysis later (when you forgot how you happened to produce a given plot, for instance).
What are the best practices for statistical project management, or the recommendations you would like to give from your own experience? Of course, this applies to any statistical software. (one answer per post, please)
",['project-management'],"I am compiling a quick series of guidelines I found on SO (as suggested by @Shane), Biostar (hereafter, BS), and this SE. I tried my best to acknowledge ownership for each item, and to select first or highly upvoted answer. I also added things of my own, and flagged items that are specific to the [R] environment.Data managementCodingAnalysisVersioningEditing/ReportingAs a side note, Hadley Wickham offers a comprehensive overview of R project management, including reproducible exemplification and an unified philosophy of data.Finally, in his R-oriented Workflow of statistical data analysis Oliver Kirchkamp offers a very detailed overview of why adopting and obeying a specific workflow will help statisticians collaborate with each other, while ensuring data integrity and reproducibility of results. It further includes some discussion of using a weaving and version control system. Stata users might find J. Scott Long's The Workflow of Data Analysis Using Stata useful too."
"If I have a 58% chance of winning a point, what's the chance of me winning a ping pong game to 21, win by 2?","
I have a bet with a co-worker that out of 50 ping pong games (first to win 21 points, win by 2), I will win all 50. So far we've played 15 games and on average I win 58% of the points, plus I've won all the games so far. So we're wondering if I have a 58% chance of winning a point and he has a 42% chance of winning a point, what's the percent chance that I would win the game? Is there a formula that we can plug in difference % chances?
We've googled all over and even asked the data scientists at our company but couldn't find a straight answer.
Edit: Wow, I am blown away by the thoroughness of responses. Thank you all so much!!!
In case people are curious, I have an update to how my bet is going: I've now won 18 out of 50 games, so I need to win 32 more games. I've won 58.7% of all points and my opponent has therefore won 41.3% of points. The standard deviation for my opponent is 3.52, his average score is 14.83, and his median score is 15.50. Below is a screenshot of the score of each game so far. I can keep updating as the bet goes on, if people are interested.
Edit #2: Unfortunately we've only been able to play a few more games, below are the results. I'm just going to keep replacing the picture so I don't have a bunch of screenshots of the score.
Final Update: I finally lost to my co-worker on game #28. He beat me 21-13. Thanks for all of your help!

","['probability', 'games']",
What are principal component scores?,"
What are principal component scores (PC scores, PCA scores)?
","['pca', 'definition']","First, let's define a score.John, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:In this case there are 12 scores in total. Each score represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.Now let's informally define a Principal Component.In the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables: Maths, Science, English, and Music), i.e.:But how would you plot 4 subjects?At the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as Multidimensional scaling.Principal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could look at the types of subjects each student is maybe more suited to.A principal component is therefore a combination of the original variables after a linear transformation. In R, this is:Which will give you something like this (first two Principal Components only for sake of simplicity):The first column here shows coefficients of linear combination that defines principal component #1, and the second column shows coefficients for principal component #2.So what is a Principal Component Score?It's a score from the table at the end of this post (see below).The above output from R means we can now plot each person's score across all subjects in a 2D graph as follows. First, we need to center the original variables by subtracting column means:And then to form linear combinations to get PC1 and PC2 scores:Which simplifies to:There are six principal component scores in the table above. You can now plot the scores in a 2D graph to get a sense of the type of subjects each student is perhaps more suited to.The same output can be obtained in R by typing prcomp(DF, scale = FALSE)$x.EDIT 1: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.EDIT 2: full credit to @drpaulbrewer for his comment in improving this answer."
What are the shortcomings of the Mean Absolute Percentage Error (MAPE)?,"
The Mean Absolute Percentage Error (mape) is a common accuracy or error measure for time series or other predictions,
$$ \text{MAPE} = \frac{100}{n}\sum_{t=1}^n\frac{|A_t-F_t|}{A_t}\%,$$
where $A_t$ are actuals and $F_t$ corresponding forecasts or predictions.
The MAPE is a percentage, so we can easily compare it between series, and people can easily understand and interpret percentages.
However, I hear that the MAPE has drawbacks. I'd like to understand these drawbacks better so I can make an informed decision about whether to use the MAPE or some alternative like the MSE (mse), the MAE (mae) or the MASE (mase).
","['accuracy', 'mape']",
Understanding the role of the discount factor in reinforcement learning,"
I'm teaching myself about reinforcement learning, and trying to understand the concept of discounted reward. So the reward is necessary to tell the system which state-action pairs are good, and which are bad. But what I don't understand is why the discounted reward is necessary. Why should it matter whether a good state is reached soon rather than later?
I do understand that this is relevant in some specific cases. For example, if you are using reinforcement learning to trade in the stock market, it is more beneficial to make profit sooner rather than later. This is because having that money now allows you to do things with that money now, which is more desirable than doing things with that money later.
But in most cases, I don't see why the discounting is useful. For example, let's say you wanted a robot to learn how to navigate around a room to reach the other side, where there are penalties if it collides with an obstacle. If there was no discount factor, then it would learn to reach the other side perfectly, without colliding with any obstacles. It may take a long time to get there, but it will get there eventually.
But if we give a discount to the reward, then the robot will be encouraged to reach the other side of the room quickly, even if it has to collide with objects along the way. This is clearly not a desirable outcome. Sure, you want the robot to get to the other side quickly, but not if this means that it has to collide with objects along the way.
So my intuition is that any form of discount factor, will actually lead to a sub-optimal solution. And the choice of the discount factor often seems arbitrary -- many methods I have seen simply set it to 0.9. This appears to be very naive to me, and seems to give an arbitrary trade-off between the optimum solution and the fastest solution, whereas in reality this trade-off is very important.
Please can somebody help me to understand all this? Thank you :)
","['machine-learning', 'reinforcement-learning']","The fact that the discount rate is bounded to be smaller than 1 is a mathematical trick to make an infinite sum finite. This helps proving the convergence of certain algorithms.In practice, the discount factor could be used to model the fact that the decision maker is uncertain about if in the next decision instant the world (e.g., environment / game / process ) is going to end.For example:If the decision maker is a robot, the discount factor could be the
probability that the robot is switched off in the next time instant
(the world ends in the previous terminology). That is the reason why the robot is
short sighted and does not optimize the sum reward but the
discounted sum reward.In order to answer more precisely, why the discount rate has to be smaller than one I will first introduce  the Markov Decision Processes (MDPs).Reinforcement learning techniques can be used to solve MDPs. An MDP provides a mathematical framework for modeling decision-making situations where outcomes are partly random and partly under the control of the decision maker.  An MDP is  defined via a state space  $\mathcal{S}$, an action space $\mathcal{A}$, a function of transition probabilities between states (conditioned to the action taken by the decision maker), and a reward function.In its basic setting, the decision maker takes and action, and gets a reward from the environment, and the environment changes its state. Then the decision maker senses the state of the environment, takes an action, gets a reward, and so on so forth. The state transitions are probabilistic and depend solely on the actual state and the action taken by the decision maker. The reward obtained by the decision maker depends on the action taken, and on both the original and the new state of the environment.A reward $R_{a_i}(s_j,s_k)$ is obtained when taking action $a_i$ in state $s_j$ and the environment/system changes to state $s_k$ after the decision maker takes action $a_i$. The decision maker follows a policy, $\pi$ $\pi(\cdot):\mathcal{S}\rightarrow\mathcal{A}$, that for each state $s_j \in \mathcal{S}$ takes an action $a_i \in \mathcal{A}$. So that the policy is what tells the decision maker which actions to take in each state. The policy $\pi$ may be randomized as well but it does not matter for now.The objective is to find a policy $\pi$ such that\begin{equation} \label{eq:1}
\max_{\pi:S(n)\rightarrow a_i}  \lim_{T\rightarrow \infty } E \left\{ \sum_{n=1}^T \beta^n  R_{x_i}(S(n),S(n+1)) \right\} (1),
\end{equation}
where $\beta$ is the discount factor and $\beta<1$.Note that the optimization problem above, has infinite time horizon ($T\rightarrow \infty $), and the objective is to maximize the sum $discounted$ reward (the reward $R$ is multiplied by $\beta^n$).
This is usually called an MDP problem with a infinite horizon discounted reward criteria.The problem is called discounted because $\beta<1$. If it was not a discounted problem $\beta=1$ the sum would not converge. All policies that have obtain on average a positive reward at each time instant would sum up to infinity. The would be a infinite horizon sum reward criteria, and is not a good optimization criteria.Here is a toy example to show you what I mean:Assume that there are only two possible actions $a={0,1}$ and that the reward function $R$ is equal to $1$ if $a=1$, and  $0$ if $a=0$ (reward does not depend on the state).It is clear the the policy that get more reward is to take always action $a=1$ and never action $a=0$.
I'll call this policy $\pi^*$. I'll compare $\pi^*$ to another policy $\pi'$ that takes action $a=1$ with small probability $\alpha << 1$, and action $a=0$ otherwise.In the infinite horizon discounted reward criteria  equation (1) becomes $\frac{1}{1-\beta}$ (the sum of a geometric series) for policy $\pi^*$ while for policy $\pi '$   equation (1) becomes $\frac{\alpha}{1-\beta}$. Since  $\frac{1}{1-\beta} > \frac{\alpha}{1-\beta}$, we say that $\pi^*$ is a better policy than $\pi '$. Actually $\pi^*$ is the optimal policy.In the infinite horizon sum reward criteria ($\beta=1$) equation (1) does not converge for any of the polices (it sums up to infinity). So whereas policy $\pi$ achieves higher rewards than $\pi'$ both policies are equal according to this criteria. That is one reason why the infinite horizon sum reward criteria is not useful.As I mentioned before, $\beta<1$ makes the trick of making the sum in  equation (1) converge.There are other optimality criteria that do not impose that $\beta<1$:The finite horizon criteria case the objective is to maximize the discounted reward until the time horizon $T$
\begin{equation} \label{eq:2}
\max_{\pi:S(n)\rightarrow a_i}  E \left\{ \sum_{n=1}^T \beta^n  R_{x_i}(S(n),S(n+1)) \right\},
\end{equation}for $\beta \leq 1$ and $T$ finite.In the infinite horizon average reward criteria the objective is
\begin{equation}
\max_{\pi:S(n)\rightarrow a_i}  \lim_{T\rightarrow \infty } E \left\{ \sum_{n=1}^T \frac{1}{T}  R_{x_i}(S(n),S(n+1)) \right\},
\end{equation}Depending on the optimality criteria one would use a different algorithm to find the optimal policy. For instances the optimal policies of the finite horizon problems would depend on both the state and the actual time instant. Most Reinforcement Learning algorithms (such as SARSA or Q-learning) converge  to the optimal policy only for the discounted reward infinite horizon criteria (the same happens for the Dynamic programming algorithms). For the average reward criteria there is no algorithm that has been shown to converge to the optimal policy, however one can use R-learning which have good performance albeit not good theoretical convergence."
How to apply Neural Network to time series forecasting?,"
I'm new to machine learning, and I have been trying to figure out how to apply neural network to time series forecasting. I have found resource related to my query, but I seem to still be a bit lost. I think a basic explanation without too much detail would help.
Let's say I have some price values for each month over a few years, and I want to predict new price values. I could get a list of prices for the last few months, and then try to find similar trends in the past using K-Nearest-Neighbor. I could them use the rate of change or some other property of the past trends to try and predict new prices. How I can apply neural network to this same problem is what I am trying to find out.
","['time-series', 'forecasting', 'neural-networks']","Here is a simple recipe that may help you get started writing code and testing ideas...Let's assume you have monthly data recorded over several years, so you have 36 values. Let's also assume that you only care about predicting one month (value) in advance.This recipe is obviously high level and you may scratch your head at first when trying to map your context into different software libraries/programs. But, hopefully this sketches out the main point: you need to create training patterns that reasonably contain the correlation structure of the series you are trying to forecast. And whether you do the forecasting with a neural network or an ARIMA model, the exploratory work to determine what that structure is is often the most time consuming and difficult part.In my experience, neural networks can provide great classification and forecasting functionality but setting them up can be time consuming. In the example above, you may find that 21 training patterns is not enough; different input data transformations lead to a better/worse forecasts; varying the number of hidden layers and hidden layer nodes greatly affects forecasts; etc.   I highly recommend looking at the neural_forecasting website, which contains tons of information on neural network forecasting competitions. The Motivations page is especially useful."
Why should I be Bayesian when my model is wrong?,"
Edits: I have added a simple example: inference of the mean of the $X_i$. I have also slightly clarified why the credible intervals not matching confidence intervals is bad.
I, a fairly devout Bayesian, am in the middle of a crisis of faith of sorts.
My problem is the following. Assume that I want to analyse some IID data $X_i$. What I would do is:

first, propose a conditional model:
$$ p(X|\theta) $$
Then, choose a prior on $\theta$:
$$ p(\theta) $$
Finally, apply Bayes' rule, compute the posterior: $p(\theta | X_1 \dots X_n )$ (or some approximation to it if it should be uncomputable) and answer all questions I have about $\theta$

This is a sensible approach: if the true model of the data $X_i$ is indeed ""inside"" of my conditional (it corresponds to some value $\theta_0$), then I can call upon statistical decision theory to say that my method is admissible (see Robert's ""The Bayesian choice"" for details; ""All of statistics"" also gives a clear account in the relevant chapter).
However, as everybody knows, assuming that my model is correct is fairly arrogant: why should nature fall neatly inside the box of the models which I have considered? It is much more realistic to assume that the real model of the data $p_{true}(X)$ differs from $p(X|\theta)$ for all values of $\theta$. This is usually called a ""misspecified"" model.
My problem is that, in this more realistic misspecified case, I don't have any good arguments for being Bayesian (i.e: computing the posterior distribution) versus simply computing the Maximum Likelihood Estimator (MLE):
$$ \hat \theta_{ML} = \arg \max_\theta [ p(X_1 \dots X_n |\theta) ] $$
Indeed, according to Kleijn, v.d Vaart (2012), in the misspecified case, the posterior distribution:

converges as $n\rightarrow \infty $ to a dirac distribution centered at a $\hat \theta_{ML} $
does not have the correct variance (unless two values just happen to be same) in order to ensure that credible intervals of the posterior match confidence intervals for $\theta$. (Note that, while confidence intervals are obviously something that Bayesians don't care about excessively, this qualitatively means that the posterior distribution is intrinsically wrong, as it implies that its credible intervals do not have correct coverage)

Thus, we are paying a computational premium (Bayesian inference, in general, is more expensive than MLE) for no additional properties
Thus, finally, my question: are there any arguments, whether theoretical or empirical, for using Bayesian inference over the simpler MLE alternative when the model is misspecified?
(Since I know that my questions are often unclear, please let me known if you don't understand something: I'll try to rephrase it)
Edit: let's consider a simple example: infering the mean of the $X_i$ under a Gaussian model (with known variance $\sigma$ to simplify even further).
We consider a Gaussian prior: we denote $\mu_0$ the prior mean, $\beta_0$ the inverse variance of the prior. Let $\bar X$ be the empirical mean of the $X_i$. Finally, note: $\mu = (\beta_0 \mu_0 + \frac{n}{\sigma^2} \bar X) / (\beta_0 + \frac{n}{\sigma^2} )$.
The posterior distribution is:
$$ p(\theta |X_1 \dots X_n)\; \propto\; \exp\!\Big( - (\beta_0 + \frac{n}{\sigma^2} ) (\theta - \mu)^2 / 2\Big) $$
In the correctly specified case (when the $X_i$ really have a Gaussian distribution), this posterior has the following nice properties

If the $X_i$ are generated from a hierarchical model in which their shared mean is picked from the prior distribution, then the posterior credible intervals have exact coverage. Conditional on the data, the probability of $\theta$ being in any interval is equal to the probability that the posterior ascribes to this interval
Even if the prior isn't correct, the credible intervals have correct coverage in the limit $n\rightarrow \infty$ in which the prior influence on the posterior vanishes
the posterior further has good frequentist properties: any Bayesian estimator constructed from the posterior is guaranteed to be admissible, the posterior mean is an efficient estimator (in the Cramer-Rao sense) of the mean, credible intervals are, asymptotically, confidence intervals.

In the misspecified case, most of these properties are not guaranteed by the theory. In order to fix ideas, let's assume that the real model for the $X_i$ is that they are instead Student distributions. The only property that we can guarantee (Kleijn et al) is that the posterior distribution concentrates on the real mean of the $X_i$ in the limit $n \rightarrow \infty$. In general, all the coverage properties would vanish. Worse, in general, we can guarantee that, in that limit, the coverage properties are fundamentally wrong: the posterior distribution ascribes the wrong probability to various regions of space.
","['bayesian', 'modeling', 'philosophical', 'misspecification']",
"Can bootstrap be seen as a ""cure"" for the small sample size?","
This question has been triggered by something I read in this graduate-level statistics textbook and also (independently) heard during this presentation at a statistical seminar. In both cases, the statement was along the lines of ""because the sample size is pretty small, we decided to perform estimation via bootstrap instead of (or along with) this parametric method $X$"". 
They didn't get into the details, but probably the reasoning was as follows: method $X$ assumes the data follow a certain parametric distribution $D$. In reality the distribution is not exactly $D$, but it's ok as long as the sample size is large enough. Since in this case the sample size is too small, let's switch to the (non-parametric) bootstrap that doesn't make any distributional assumptions. Problem solved!
In my opinion, that's not what bootstrap is for. Here is how I see it: bootstrap can give one an edge when it's more or less obvious that there are enough data, but there is no closed form solution to get standard errors, p-values and similar statistics. A classic example is obtaining a CI for the correlation coefficient given a sample from a bivariate normal distribution: the closed form solution exists, but it is so convoluted that bootstrapping is simpler. However, nothing implies that bootstrap can somehow help one to get away with a small sample size.
Is my perception right?
If you find this question interesting, there is another, more specific bootstrap question from me:
Bootstrap: the issue of overfitting
P.S. I can’t help sharing one egregious example of the “bootstrap approach”. I am not disclosing the author’s name, but he is one of the older generation “quants” who wrote a book on Quantitative Finance in 2004. The example is taken from there.
Consider the following problem: suppose you have 4 assets and 120 monthly return observations for each. The goal is to construct the joint 4-dimensional cdf of yearly returns. Even for a single asset, the task appears hardly attainable with only 10 yearly observations, let alone the estimation of 4-dimensional cdf. But not to worry, the “bootstrap” will help you out: take all of the available 4-dimensional observations, resample 12 with replacement and compound them to construct a single “bootstrapped” 4-dimensional vector of annual returns. Repeat that 1000 times and, lo and behold, you got yourself a “bootstrap sample” of 1000 annual returns. Use this as an i.i.d. sample of size 1000 for the purpose of cdf estimation, or any other inference that can be drawn from a thousand –year history.
","['bootstrap', 'small-sample']","I remember reading that using the percentile confidence interval for bootstrapping is equivalent to using a Z interval instead of a T interval and using $n$ instead of $n-1$ for the denominator.  Unfortunately I don't remember where I read this and could not find a reference in my quick searches.  These differences don't matter much when n is large (and the advantages of the bootstrap outweigh these minor problems when $n$ is large), but with small $n$ this can cause problems.  Here is some R code to simulate and compare:My results for one run are:So we can see that using the t-test and the z-test (with the true population standard deviation) both give a type I error rate that is essentially $\alpha$ as designed.  The improper z test (dividing by sample standard deviation, but using Z critical value instead of T) rejects the null more than twice as often as it should.  Now to the bootstrap, it is rejecting the null 3 times as often as it should (looking if 0, the true mean, is in the interval or not), so for this small sample size the simple bootstrap is not sized properly and therefore does not fix problems (and this is when the data is optimally normal).  The improved bootstrap intervals (BCa etc.) will probably do better, but this should raise some concern about using bootstrapping as a panacea for small sample sizes."
What is the single most influential book every statistician should read? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 6 months ago.


The community reviewed whether to reopen this question 6 months ago and left it closed:

Original close reason(s) were not resolved






                        Improve this question
                    



If you could go back in time and tell yourself to read a specific book at the beginning of your career as a statistician, which book would it be?
",['references'],
How to normalize data between -1 and 1?,"
I have seen the min-max normalization formula but that normalizes values between 0 and 1. How would I normalize my data between -1 and 1? I have both negative and positive values in my data matrix.
","['dataset', 'normalization']",
"What is the meaning of ""All models are wrong, but some are useful""","

""Essentially, all models are wrong, but some are useful.""
--- Box, George E. P.; Norman R. Draper (1987). Empirical Model-Building and Response Surfaces, p. 424, Wiley. ISBN 0471810339.

What exactly is the meaning of the above phrase?  
",['modeling'],
What're the differences between PCA and autoencoder?,"
Both PCA and autoencoder can do demension reduction, so what are the difference between them? In what situation I should use one over another?
","['machine-learning', 'pca', 'neural-networks', 'autoencoders']","PCA is restricted to a linear map, while auto encoders can have nonlinear enoder/decoders.A single layer auto encoder with linear transfer function is nearly equivalent to PCA, where nearly means that the $W$ found by AE and PCA won't necessarily be the same - but the subspace spanned by the respective $W$'s will."
Why is the L2 regularization equivalent to Gaussian prior?,"
I keep reading this and intuitively I can see this but how does one go from L2 regularization to saying that this is a Gaussian Prior analytically? Same goes for saying L1 is equivalent to a Laplacean prior. 
Any further references would be great. 
","['regression', 'references', 'regularization']","Let us imagine that you want to infer some parameter $\beta$ from some observed input-output pairs $(x_1,y_1)\dots,(x_N,y_N)$. Let us assume that the outputs are linearly related to the inputs via $\beta$ and that the data are corrupted by some noise $\epsilon$:$$y_n = \beta x_n + \epsilon,$$where $\epsilon$ is Gaussian noise with mean $0$ and variance $\sigma^2$.
This gives rise to a Gaussian likelihood:$$\prod_{n=1}^N \mathcal{N}(y_n|\beta x_n,\sigma^2).$$Let us regularise parameter $\beta$ by imposing the Gaussian prior $\mathcal{N}(\beta|0,\lambda^{-1}),$ where $\lambda$ is a strictly positive scalar ($\lambda$ quantifies of by how much we believe that $\beta$ should be close to zero, i.e. it controls the strength of the regularisation).
Hence, combining the likelihood and the prior we simply have:$$\prod_{n=1}^N \mathcal{N}(y_n|\beta x_n,\sigma^2) \mathcal{N}(\beta|0,\lambda^{-1}).$$Let us take the logarithm of the above expression. Dropping some constants we get:$$\sum_{n=1}^N -\frac{1}{\sigma^2}(y_n-\beta x_n)^2 - \lambda \beta^2 + \mbox{const}.$$If we maximise the above expression with respect to $\beta$, we get the so called maximum a-posteriori estimate for $\beta$, or MAP estimate for short. In this expression it becomes apparent why the Gaussian prior can be interpreted as a L2 regularisation term.The relationship between the L1 norm and the Laplace prior can be understood in the same fashion. Instead of a Gaussian prior, multiply your likelihood with a Laplace prior and then take the logarithm.A good reference (perhaps slightly advanced) detailing both issues is the paper ""Adaptive Sparseness for Supervised Learning"", which currently does not seem easy to find online. Alternatively look at ""Adaptive Sparseness using Jeffreys Prior"". Another good reference is ""On Bayesian classification with Laplace priors""."
An example: LASSO regression using glmnet for binary outcome,"
I am starting to dabble with the use of glmnet with LASSO Regression where my outcome of interest is dichotomous. I have created a small mock data frame below:
age     <- c(4, 8, 7, 12, 6, 9, 10, 14, 7) 
gender  <- c(1, 0, 1, 1, 1, 0, 1, 0, 0)
bmi_p   <- c(0.86, 0.45, 0.99, 0.84, 0.85, 0.67, 0.91, 0.29, 0.88)
m_edu   <- c(0, 1, 1, 2, 2, 3, 2, 0, 1)
p_edu   <- c(0, 2, 2, 2, 2, 3, 2, 0, 0)
f_color <- c(""blue"", ""blue"", ""yellow"", ""red"", ""red"", ""yellow"", ""yellow"", 
             ""red"", ""yellow"")
asthma  <- c(1, 1, 0, 1, 0, 0, 0, 1, 1)
# df is a data frame for further use!
df <- data.frame(age, gender, bmi_p, m_edu, p_edu, f_color, asthma)

The columns (variables) in the above dataset are as follows:

age (age of child in years) - continuous
gender - binary (1 = male; 0 = female)
bmi_p (BMI percentile) - continuous
m_edu (mother highest education level) - ordinal (0 = less than high school; 1 = high school diploma; 2 = bachelors degree; 3 = post-baccalaureate degree)
p_edu (father highest education level) - ordinal (same as m_edu)
f_color (favorite primary color) - nominal (""blue"", ""red"", or ""yellow"")
asthma (child asthma status) - binary (1 = asthma; 0 = no asthma)

The goal of this example is to make use of LASSO to create a model predicting child asthma status from the list of 6 potential predictor variables (age, gender, bmi_p, m_edu, p_edu, and f_color). Obviously the sample size is an issue here, but I am hoping to gain more insight into how to handle the different types of variables (i.e., continuous, ordinal, nominal, and binary) within the glmnet framework when the outcome is binary (1 = asthma; 0 = no asthma).
As such, would anyone being willing to provide a sample R script along with explanations for this mock example using LASSO with the above data to predict asthma status? Although very basic, I know I, and likely many others on CV, would greatly appreciate this! 
","['r', 'self-study', 'lasso']","Categorical variables are usually first transformed into factors,
then a dummy variable matrix of predictors is created and along with the continuous predictors, is passed to the model.
Keep in mind, glmnet uses both ridge and lasso penalties, but can be set to either alone.Some results:Coefficients can be extracted from the glmmod. Here shown with 3 variables selected.Lastly, cross validation can also be used to select lambda."
"What are modern, easily used alternatives to stepwise regression?","
I have a dataset with around 30 independent variables and would like to construct a generalized linear model (GLM) to explore the relationship between them and the dependent variable.
I am aware that the method I was taught for this situation, stepwise regression, is now considered a statistical sin.
What modern methods of model selection should be used in this situation?
","['regression', 'generalized-linear-model', 'model-selection', 'stepwise-regression']","There are several alternatives to Stepwise Regression. The most used I have seen are:Both PLS Regression and LASSO are implemented in R packages like PLS: http://cran.r-project.org/web/packages/pls/ and LARS: http://cran.r-project.org/web/packages/lars/index.htmlIf you only want to explore the relationship between your dependent variable and the independent variables (e.g. you do not need statistical significance tests), I would also recommend Machine Learning methods like Random Forests or Classification/Regression Trees. Random Forests can also approximate complex non-linear relationships between your dependent and independent variables, which might not have been revealed by linear techniques (like Linear Regression). A good starting point to Machine Learning might be the Machine Learning task view on CRAN: Machine Learning Task View: http://cran.r-project.org/web/views/MachineLearning.html"
Why not approach classification through regression?,"
Some material I've seen on machine learning said that it's a bad idea to approach a classification problem through regression. But I think it's always possible to do a continuous regression to fit the data and truncate the continuous prediction to yield discrete classifications. So why is it a bad idea?
","['regression', 'machine-learning', 'classification']",
How to produce a pretty plot of the results of k-means cluster analysis?,"
I'm using R to do K-means clustering. I'm using 14 variables to run K-means

What is a pretty way to plot the results of K-means? 
Are there any existing implementations?
Does having 14 variables complicate plotting the results? 

I found something called GGcluster which looks cool but it is still in development. 
I also read something about sammon mapping, but didn't understand it very well. Would this be a good option? 
","['data-visualization', 'classification', 'k-means', 'unsupervised-learning']","I'd push the silhouette plot for this, because it's unlikely that you'll get much actionable information from pair plots when the number of dimension is 14.This approach is highly cited and well known (see here for an explanation). Rousseeuw, P.J. (1987) Silhouettes: A graphical aid to the
 interpretation and validation of cluster analysis. J. Comput.
  Appl. Math., 20, 53-65."
"Rules of thumb for ""modern"" statistics","
I like G van Belle's book on Statistical Rules of Thumb, and to a lesser extent Common Errors in Statistics (and How to Avoid Them) from Phillip I Good and James W. Hardin. They address common pitfalls when interpreting results from experimental and observational studies and provide practical recommendations for statistical inference, or exploratory data analysis. But I feel that ""modern"" guidelines are somewhat lacking, especially with the ever growing use of computational and robust statistics in various fields, or the introduction of techniques from the machine learning community in, e.g. clinical biostatistics or genetic epidemiology.
Apart from computational tricks or common pitfalls in data visualization which could be addressed elsewhere, I would like to ask: What are the top rules of thumb you would recommend for efficient data analysis? (one rule per answer, please).
I am thinking of guidelines that you might provide to a colleague, a researcher without strong background in statistical modeling, or a student in intermediate to advanced course. This might pertain to various stages of data analysis, e.g. sampling strategies, feature selection or model building, model comparison, post-estimation, etc.
","['modeling', 'exploratory-data-analysis', 'rule-of-thumb']",
Why is it possible to get significant F statistic (p<.001) but non-significant regressor t-tests?,"
In a multiple linear regression, why is it possible to have a highly significant F statistic (p<.001) but have very high p-values on all the regressor's t tests?
In my model, there are 10 regressors. One has a p-value of 0.1 and the rest are above 0.9

For dealing with this problem see the follow-up question.
","['regression', 'hypothesis-testing', 't-test', 'multicollinearity', 'faq']","As Rob mentions, this occurs when you have highly correlated variables. The standard example I use is predicting weight from shoe size. You can predict weight equally well with the right or left shoe size. But together it doesn't work out.Brief simulation example"
Why haven't robust (and resistant) statistics replaced classical techniques?,"
When solving business problems using data, it's common that at least one key assumption that under-pins classical statistics is invalid. Most of the time, no one bothers to check those assumptions so you never actually know.
For instance, that so many of the common web metrics are ""long-tailed"" (relative to the normal distribution) is, by now, so well documented that we take it for granted. Another example, online communities--even in communities with thousands of members, it's well-documented that by far the largest share of contribution to/participation in many of these community is attributable to a minuscule group of 'super-contributors.' (E.g., a few months ago, just after the SO API was made available in beta, a StackOverflow member published a brief analysis from data he collected through the API; his conclusion--less than one percent of the SO members account for most of the activity on SO (presumably asking questions, and answering them), another 1-2% accounted for the rest, and the overwhelming majority of the members do nothing).
Distributions of that sort--again more often the rule rather than the exception--are often best modeled with a power law density function. For these type of distributions, even the central limit theorem is problematic to apply.
So given the abundance of populations like this of interest to analysts, and given that classical models perform demonstrably poorly on these data, and given that robust and resistant methods have been around for a while (at least 20 years, I believe)--why are they not used more often? (I am also wondering why I don't use them more often, but that's not really a question for CrossValidated.)
Yes I know that there are textbook chapters devoted entirely to robust statistics and I know there are (a few) R Packages (robustbase is the one I am familiar with and use), etc.
And yet given the obvious advantages of these techniques, they are often clearly the better tools for the job--why are they not used much more often? Shouldn't we expect to see robust (and resistant) statistics used far more often (perhaps even presumptively) compared with the classical analogs?
The only substantive (i.e., technical) explanation I have heard is that robust techniques (likewise for resistant methods) lack the power/sensitivity of classical techniques. I don't know if this is indeed true in some cases, but I do know it is not true in many cases.
A final word of preemption: yes I know this question does not have a single demonstrably correct answer; very few questions on this Site do. Moreover, this question is a genuine inquiry; it's not a pretext to advance a point of view--I don't have a point of view here, just a question for which i am hoping for some insightful answers.
","['model-selection', 'nonparametric', 'outliers', 'robust', 'philosophical']","Researchers want small p-values, and you can get smaller p-values if you use methods that make stronger distributional assumptions. In other words, non-robust methods let you publish more papers. Of course more of these papers may be false positives, but a publication is a publication.  That's a cynical explanation, but it's sometimes valid."
ImageNet: what is top-1 and top-5 error rate?,"
In ImageNet classification papers top-1 and top-5 error rates are important units for measuring the success of some solutions, but what are those error rates?
In ImageNet Classification with Deep Convolutional
Neural Networks
 by Krizhevsky et al. every solution based on one single CNN (page 7) has no top-5 error rates while the ones with 5 and 7 CNNs have (and also the error rate for 7 CNNs are better than for 5 CNNs).
Does this mean top-1 error rate is the best single error rate for one single CNN?
Is the top-5 error rate simply the accumulated error rate of five CNNs?
","['classification', 'neural-networks', 'error', 'measurement-error', 'image-processing']","[...] where the top-5 error rate is the fraction of test images for which
the correct label is not among the five labels considered most
probable by the mode.First, you make a prediction using the CNN and obtain the predicted class multinomial distribution ($\sum p_{class} = 1$).Now, in the case of the top-1 score, you check if the top class (the one with the highest probability) is the same as the target label.In the case of the top-5 score, you check if the target label is one of your top 5 predictions (the 5 ones with the highest probabilities).In both cases, the top score is computed as the number of times a predicted label matched the target label, divided by the number of data points evaluated.Finally, when 5-CNNs are used, you first average their predictions and follow the same procedure for calculating the top-1 and top-5 scores."
What are disadvantages of using the lasso for variable selection for regression?,"
From what I know, using lasso for variable selection handles the problem of correlated inputs. Also, since it is equivalent to Least Angle Regression, it is not slow computationally. However, many people (for example people I know doing bio-statistics) still seem to favour stepwise or stagewise variable selection. Are there any practical disadvantages of using the lasso that makes it unfavourable?
","['regression', 'feature-selection', 'lasso']","There is NO reason to do stepwise selection.  It's just wrong.LASSO/LAR are the best automatic methods.  But they are automatic methods.  They let the analyst not think.In many analyses, some variables should be in the model REGARDLESS of any measure of significance.  Sometimes they are necessary control variables.  Other times, finding a small effect can be substantively important."
What is the intuition behind SVD?,"
I have read about singular value decomposition (SVD). In almost all textbooks it is mentioned that it factorizes the matrix into three matrices with given specification. 
But what is the intuition behind splitting the matrix in such form? PCA and other algorithms for dimensionality reduction are intuitive in the sense that algorithm has nice visualization property but with SVD it is not the case. 
","['pca', 'matrix', 'intuition', 'linear-algebra', 'svd']","Write the SVD of matrix $X$ (real, $n\times p$) as
$$
   X = U D V^T
$$
where $U$ is $n\times p$, $D$ is diagonal $p\times p$ and $V^T$ is $p\times p$. In terms of the columns of the matrices $U$ and $V$ we can write
$X=\sum_{i=1}^p d_i u_i v_i^T$. That shows $X$ written as a sum of $p$ rank-1 matrices. What does a rank-1 matrix look like? Let's see:
$$
\begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} \begin{pmatrix} 4 & 5 & 6 \end{pmatrix} = \begin{pmatrix} 4 & 5 & 6 \\ 8 & 10 & 12 \\ 12 & 15 & 18 \end{pmatrix}
$$ The rows are proportional, and the columns are proportional.Think now about $X$ as containing the grayscale values of a black-and-white image, each entry in the matrix representing one pixel.  For instance the following picture of a baboon:Then read this image into R and get the matrix part of the resulting structure, maybe using the library pixmap.If you want a step-by-step guide as to how to reproduce the results, you can find the code here.Calculate the SVD:How can we think about this?  We get the $512 \times 512$ baboon image represented as a sum of $512$ simple images, with each one only showing vertical and horizontal structure, i.e. it is an image of vertical and horizontal stripes! So, the SVD of the baboon represents the baboon image as a superposition of $512$ simple images, each one only showing horizontal/vertical stripes.  Let us calculate a low-rank reconstruction of the image with $1$ and with $20$ components:resulting in the following two images:On the left we can easily see the vertical/horizontal stripes in the rank-1 image.Let us finally look at the ""residual image"", the image reconstructed (as above, code not shown) from the $20$ rank-one images with the lowest singular values. Here it is:Which is quite interesting: we see the parts of the original image that are difficult to represent as superposition of vertical/horizontal lines,  mostly diagonal nose hair and some texture, and the eyes!"
What is the lasso in regression analysis?,"
I'm looking for a non-technical definition of the lasso and what it is used for.
","['regression', 'lasso', 'regularization']",
"How to tell if data is ""clustered"" enough for clustering algorithms to produce meaningful results?","
How would you know if your (high dimensional) data exhibits enough clustering so that results from kmeans or other clustering algorithm is actually meaningful?
For k-means algorithm in particular, how much of a reduction in within-cluster variance should there be for the actual clustering results to be meaningful (and not spurious)?
Should clustering be apparent when a dimensionally-reduced form of the data is plotted, and are the results from kmeans (or other methods) meaningless if the clustering cannot be visualized?
","['clustering', 'k-means']","About k-means specifically, you can use the Gap statistics. Basically, the idea is to compute a goodness of clustering measure based on average dispersion compared to a reference distribution for an increasing number of clusters.
More information can be found in the original paper:Tibshirani, R., Walther, G., and
  Hastie, T. (2001). Estimating the
  numbers of clusters in a data set via
  the gap statistic. J. R. Statist.
  Soc. B, 63(2): 411-423.The answer that I provided to a related question highlights other general validity indices that might be used to check whether a given dataset exhibits some kind of a structure. When you don't have any idea of what you would expect to find if there was noise only, a good approach is to use resampling and study clusters stability. In other words, resample your data (via bootstrap or by adding small noise to it) and compute the ""closeness"" of the resulting partitions, as measured by Jaccard similarities. In short, it allows to estimate the frequency with which similar clusters were recovered in the data. This method is readily available in the fpc R package as clusterboot().
It takes as input either raw data or a distance matrix, and allows to apply a wide range of clustering methods (hierarchical, k-means, fuzzy methods). The method is discussed in the linked references:Hennig, C. (2007) Cluster-wise
  assessment of cluster stability.
  Computational Statistics and Data Analysis, 52, 258-271.Hennig, C. (2008) Dissolution point
  and isolation robustness: robustness
  criteria for general cluster analysis
  methods. Journal of Multivariate
  Analysis, 99, 1154-1176.Below is a small demonstration with the k-means algorithm.The results are quite positive in this artificial (and well structured) dataset since none of the three clusters (krange) were dissolved across the samples, and the average clusterwise Jaccard similarity is > 0.95 for all clusters.Below are the results on the 20 bootstrap samples. As can be seen, statistical units tend to stay grouped into the same cluster, with few exceptions for those observations lying in between.You can extend this idea to any validity index, of course: choose a new series of observations by bootstrap (with replacement), compute your statistic (e.g., silhouette width, cophenetic correlation, Hubert's gamma, within sum of squares) for a range of cluster numbers (e.g., 2 to 10), repeat 100 or 500 times, and look at the boxplot of your statistic as a function of the number of cluster. Here is what I get with the same simulated dataset, but using Ward's hierarchical clustering and considering the cophenetic correlation (which assess how well distance information are reproduced in the resulting partitions) and silhouette width (a combination measure assessing intra-cluster homogeneity and inter-cluster separation).The cophenetic correlation ranges from 0.6267 to 0.7511 with a median value of 0.7031 (500 bootstrap samples). Silhouette width appears to be maximal when we consider 3 clusters (median 0.8408, range 0.7371-0.8769)."
Basic question about Fisher Information matrix and relationship to Hessian and standard errors,"
Ok, this is a quite basic question, but I am a little bit confused. In my thesis I write:
The standard errors can be found by calculating the inverse of the square root of the diagonal elements of the (observed) Fisher Information matrix:
\begin{align*}
s_{\hat{\mu},\hat{\sigma}^2}=\frac{1}{\sqrt{\mathbf{I}(\hat{\mu},\hat{\sigma}^2)}}
\end{align*}
Since the optimization command in R minimizes $-\log\mathcal{L}$ the (observed) Fisher Information matrix can be found by calculating the inverse of the Hessian:
\begin{align*}
\mathbf{I}(\hat{\mu},\hat{\sigma}^2)=\mathbf{H}^{-1}
\end{align*}
My main question: Is this correct what I am saying?
I am a little bit confused, because in this source on page 7 it says: 

the Information matrix is the negative of the expected value of the
  Hessian matrix

(So no inverse of the Hessian.)
Whereas in this source on page 7 (footnote 5) it says:

The observed Fisher information is equal to $(-H)^{-1}$.

(So here is the inverse.)
I am aware of the minus sign and when to use it and when not, but why is there a difference in taking the inverse or not?
","['maximum-likelihood', 'fisher-information']","Yudi Pawitan writes in his book In All Likelihood that the second derivative of the log-likelihood evaluated at the maximum likelihood estimates (MLE) is the observed Fisher information (see also this document, page 1). This is exactly what most optimization algorithms like optim in R return: the Hessian evaluated at the MLE. When the negative log-likelihood is minimized, the negative Hessian is returned. As you correctly point out, the estimated standard errors of the MLE are the square roots of the diagonal elements of the inverse of the observed Fisher information matrix. In other words: The square roots of the diagonal elements of the inverse of the Hessian (or the negative Hessian) are the estimated standard errors.SummaryFormallyLet $l(\theta)$ be a log-likelihood function. The Fisher information matrix $\mathbf{I}(\theta)$ is a symmetrical $(p\times p)$ matrix containing the entries:
$$
\mathbf{I}(\theta)=-\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}l(\theta),~~~~ 1\leq i, j\leq p
$$
The observed Fisher information matrix is simply $\mathbf{I}(\hat{\theta}_{\mathrm{ML}})$, the information matrix evaluated at the maximum likelihood estimates (MLE). The Hessian is defined as:
$$
\mathbf{H}(\theta)=\frac{\partial^{2}}{\partial\theta_{i}\partial\theta_{j}}l(\theta),~~~~ 1\leq i, j\leq p
$$
It is nothing else but the matrix of second derivatives of the likelihood function with respect to the parameters. It follows that if you minimize the negative log-likelihood, the returned Hessian is the equivalent of the observed Fisher information matrix whereas in the case that you maximize the log-likelihood, then the negative Hessian is the observed information matrix.Further, the inverse of the Fisher information matrix is an estimator of the asymptotic covariance matrix:
$$
\mathrm{Var}(\hat{\theta}_{\mathrm{ML}})=[\mathbf{I}(\hat{\theta}_{\mathrm{ML}})]^{-1}
$$
The standard errors are then the square roots of the diagonal elements of the covariance matrix.
For the asymptotic distribution of a maximum likelihood estimate, we can write
$$
\hat{\theta}_{\mathrm{ML}}\stackrel{a}{\sim}\mathcal{N}\left(\theta_{0}, [\mathbf{I}(\hat{\theta}_{\mathrm{ML}})]^{-1}\right)
$$
where $\theta_{0}$ denotes the true parameter value. Hence, the estimated standard error of the maximum likelihood estimates is given by:
$$
\mathrm{SE}(\hat{\theta}_{\mathrm{ML}})=\frac{1}{\sqrt{\mathbf{I}(\hat{\theta}_{\mathrm{ML}})}}
$$"
Cross-Validation in plain english?,"
How would you describe cross-validation to someone without a data analysis background?
","['cross-validation', 'intuition']","Consider the following situation: I want to catch the subway to go to my office. My plan is to take my car, park at the subway and then take the train to go to my office. My goal is to catch the train at 8.15 am every day so that I can reach my office on time. I need to decide the following: (a) the time at which I need to leave from my home and (b) the route I will take to drive to the station.In the above example, I have two parameters (i.e., time of departure from home and route to take to the station) and I need to choose these parameters such that I reach the station by 8.15 am. In order to solve the above problem I may try out different sets of 'parameters' (i.e., different combination of times of departure and route) on Mondays, Wednesdays, and Fridays, to see which combination is the 'best' one. The idea is that once I have identified the best combination I can use it every day so that I achieve my objective.Problem of OverfittingThe problem with the above approach is that I may overfit which essentially means that the best combination I identify may in some sense may be unique to Mon, Wed and Fridays and that combination may not work for Tue and Thu. Overfitting may happen if in my search for the best combination of times and routes I exploit some aspect of the traffic situation on Mon/Wed/Fri which does not occur on Tue and Thu.One Solution to Overfitting: Cross-ValidationCross-validation is one solution to overfitting. The idea is that once we have identified our best combination of parameters (in our case time and route) we test the performance of that set of parameters in a different context. Therefore, we may want to test on Tue and Thu as well to ensure that our choices work for those days as well.Extending the analogy to statisticsIn statistics, we have a similar issue. We often use a limited set of data to estimate the unknown parameters we do not know. If we overfit then our parameter estimates will work very well for the existing data but not as well for when we use them in another context. Thus, cross-validation helps in avoiding the above issue of overfitting by proving us some reassurance that the parameter estimates are not unique to the data we used to estimate them.Of course, cross validation is not perfect. Going back to our example of the subway, it can happen that even after cross-validation, our best choice of parameters may not work one month down the line because of various issues (e.g., construction, traffic volume changes over time etc)."
Line of best fit does not look like a good fit. Why?,"
Have a look at this Excel graph:

The 'common sense' line-of-best-fit would appear be an almost vertical line straight through the center of the points (edited by hand in red). However the linear trend line as decided by Excel is the diagonal black line shown.

Why has Excel produced something that (to the human eye) appears to be wrong?
How can I produce a best fit line that looks a little more intuitive (i.e. something like the red line)?


Update 1. An Excel spreadsheet with data and graph is available here:
example data, CSV in Pastebin. Are the type1 and type2 regression techniques available as excel functions?
Update 2. The data represent a paraglider climbing in a thermal whilst drifting with the wind. The final objective is to investigate how wind strength and direction varies with height. I'm an engineer, NOT a mathematician or statistician, so the information in these responses has given me a lot more areas for research.

This is an interesting thread and It would be a shame for the data to be lost and someone in future unable to reproduce the examples, so I'm adding it as a comment here (which is the data from the following link).
""lon"",""lat""
-0.713917,53.9351
-0.712917,53.93505
-0.712617,53.934983
-0.712333,53.9349
-0.7122,53.93475
-0.71215,53.934567
-0.712233,53.9344
-0.712483,53.934233
-0.712817,53.934167
-0.713217,53.934167
-0.713617,53.934267
-0.7141,53.934733
-0.714133,53.935
-0.71395,53.935283
-0.713617,53.9355
-0.713233,53.935617
-0.712767,53.935617
-0.712383,53.9355
-0.712183,53.9353
-0.712367,53.934883
-0.712717,53.934767
-0.713133,53.9348
-0.713583,53.934917
-0.713867,53.93515
-0.714017,53.935433
-0.7139,53.935717
-0.7136,53.935933
-0.71325,53.936067
-0.712833,53.936133
-0.7124,53.936117
-0.712083,53.935983
-0.7119,53.935767
-0.711917,53.935567
-0.7121,53.935383
-0.7124,53.935283
-0.712733,53.93525
-0.713117,53.935267
-0.7135,53.93535
-0.713817,53.935517
-0.71405,53.935733
-0.71415,53.935983
-0.7141,53.93625
-0.7139,53.9365
-0.713567,53.936667
-0.713183,53.936767
-0.712767,53.9368
-0.7124,53.9367
-0.712133,53.93655
-0.712033,53.936333
-0.712167,53.936167
-0.712383,53.936017
-0.712733,53.935917
-0.7132,53.93595
-0.713567,53.936067
-0.713867,53.936267
-0.714067,53.9365
-0.71415,53.936767
-0.714033,53.937033
-0.71375,53.937233
-0.7134,53.9374
-0.712967,53.93745
-0.71255,53.937433
-0.7122,53.937267
-0.712067,53.937033
-0.712117,53.9368
-0.712367,53.936617
-0.712733,53.936533
-0.713133,53.93655
-0.713467,53.93665
-0.71375,53.93685
-0.713933,53.937083
-0.71395,53.937367
-0.713767,53.937633
-0.713433,53.937833
-0.713033,53.937967
-0.712567,53.937967
-0.71215,53.937867
-0.711883,53.93765
-0.711817,53.937433
-0.711983,53.937233
-0.71265,53.937033
-0.713067,53.9371
-0.713683,53.93745
-0.713817,53.937983
-0.713633,53.938233
-0.7133,53.938433
-0.71285,53.938533
-0.71205,53.938333
-0.71185,53.938117
-0.711867,53.937867
-0.712067,53.9377
-0.712417,53.937583
-0.712833,53.937567
-0.713233,53.937667
-0.713567,53.937883
-0.7137,53.938417
-0.713467,53.93865
-0.713117,53.938817
-0.712683,53.938917000000004
-0.71225,53.938867
-0.711917,53.938717
-0.711767,53.938483
-0.711883,53.938267
-0.712133,53.9381
-0.712483,53.938017
-0.713283,53.93815
-0.713567,53.938333
-0.7138,53.938567
-0.713683,53.9391
-0.713417,53.9393
-0.71305,53.939433
-0.7126,53.939483
-0.7122,53.9394
-0.711917,53.93925
-0.711783,53.93905
-0.7118,53.938817
-0.711967,53.938667
-0.712217,53.938533
-0.712567,53.938433
-0.712933,53.93845
-0.7133,53.938567
-0.713583,53.93875
-0.71375,53.939

","['regression', 'excel', 'intuition']",
"Regarding p-values, why 1% and 5%? Why not 6% or 10%?","
Regarding p-values, I am wondering why $1$% and $5$% seem to be the gold standard for ""statistical significance"". Why not other values, like $6$% or $10$%?
Is there a fundamental mathematical reason for this, or is this just a widely held convention?
","['hypothesis-testing', 'statistical-significance', 'p-value', 'history']","If you check the references below you'll find quite a bit of variation in the background, though there are some common elements.Those numbers are at least partly based on some comments from Fisher, where he said  (while discussing a level of 1/20)  It is convenient to take this point as a limit in judging
  whether a deviation is to be considered significant or not.
  Deviations exceeding twice the standard deviation are
  thus formally regarded as significant$\quad$ Fisher, R.A. (1925) Statistical
Methods for Research Workers, p. 47On the other hand, he was sometimes more broad:If one in twenty does not seem high enough odds, we
  may, if we prefer it, draw the line at one in fifty (the
  2 per cent point), or one in a hundred (the 1 per cent
  point). Personally, the writer prefers to set a low standard
  of significance at the 5 per cent point, and ignore entirely
  all results which fail to reach this level. A scientific fact
  should be regarded as experimentally established only
  if a properly designed experiment rarely fails to give
  this level of significance.$\quad$ Fisher, R.A. (1926)
The arrangement of field experiments.
$\quad$ Journal
of the Ministry of Agriculture, p. 504Fisher also used 5% for one of his book's tables - but most of his other tables had a larger variety of significance levelsSome of his comments have suggested more or less strict (i.e. lower or higher alpha levels) approaches in different situations.That sort of discussion above led to a tendency to produce tables focusing 5% and 1% significance levels (and sometimes with others, like 10%, 2% and 0.5%) for want of any other 'standard' values to use.However, in this paper, Cowles and Davis suggest that the use of 5% - or something close to it at least - goes back further than Fisher's comment.In short, our use of 5% (and to a lesser extent 1%) is pretty much arbitrary convention, though clearly a lot of people seem to feel that for many problems they're in the right kind of ballpark. There's no reason either particular value should be used in general.Further references:Dallal, Gerard E. (2012). The Little Handbook of Statistical Practice.  - 
Why 0.05?Stigler, Stephen (December 2008). ""Fisher and the 5% level"". Chance 21 (4): 12.
available here(Between them, you get a fair bit of background - it does look like between them there's a good case for thinking significance levels at least in the general ballpark of 5% - say between 2% and 10% - had been more or less in the air for a while.)"
Please explain the waiting paradox,"
A few years ago I designed a radiation detector that works by measuring the interval between events rather than counting them.  My assumption was, that when measuring non-contiguous samples, on average I would measure half of the actual interval. However when I tested the circuit with a calibrated source the reading was a factor of two too high which meant I had been measuring the full interval.  
In an old book on probability and statistics I found a section about something called ""The Waiting Paradox"".  It presented an example in which a bus arrives at the bus stop every 15 minutes and a passenger arrives at random, it stated that the passenger would on average wait the full 15 minutes.  I have never been able to understand the math presented with the example and continue to look for an explanation. If someone can explain why it is so that the passenger waits the full interval I will sleep better.  
","['poisson-process', 'paradox']","As Glen_b pointed out, if the buses arrive every $15$ minutes without any uncertainty whatsoever, we know that the maximum possible waiting time is $15$ minutes. If from our part we arrive ""at random"", we feel that ""on average"" we will wait half the maximum possible waiting time. And the maximum possible waiting time is here equal to the maximum possible length between two consecutive arrivals. Denote our waiting time $W$ and the maximum length between two consecutive bus arrivals $R$, and we argue that$$ E(W) = \frac 12 R = \frac {15}{2} = 7.5 \tag{1}$$and we are right.But suddenly certainty is taken away from us and we are told that $15$ minutes is now the average length between two bus arrivals. And we fall into the ""intuitive thinking trap"" and think: ""we only need to replace $R$ with its expected value"", and we argue$$ E(W) = \frac 12 E(R) = \frac {15}{2} = 7.5\;\;\; \text{WRONG} \tag{2}$$A first indication that we are wrong, is that $R$ is not ""length between any two consecutive bus-arrivals"", it is ""maximum length etc"". So in any case, we have that $E(R) \neq 15$.How did we arrive at equation $(1)$? We thought:""waiting time can be from $0$ to $15$ maximum. I arrive with equal probability at any instance, so I ""choose"" randomly and with equal probability all possible waiting times. Hence half the maximum length between two consecutive bus arrivals is my average waiting time"". And we are right. But by mistakenly inserting the value $15$ in equation $(2)$, it no longer reflects our behavior. With $15$ in place of $E(R)$, equation $(2)$ says ""I choose randomly and with equal probability all possible waiting times that are smaller or equal to the average length between two consecutive bus-arrivals"" -and here is where our intuitive mistake lies, because, our behavior has not change - so, by arriving randomly uniformly, we in reality still ""choose randomly and with equal probability"" all possible waiting times - but ""all possible waiting times"" is not captured by $15$ - we have forgotten the right tail of the distribution of lengths between two consecutive bus-arrivals.  So perhaps, we should calculate the expected value of the maximum length between any two consecutive bus arrivals, is this the correct solution?  Yes it could be, but : the specific ""paradox"" goes hand-in-hand with a specific stochastic assumption: that bus-arrivals are modeled by the benchmark Poisson process, which means that as a consequence we assume that the time-length between any two consecutive bus-arrivals follows an Exponential distribution. Denote $\ell$ that length, and we have that$$f_{\ell}(\ell) = \lambda e^{-\lambda \ell},\;\; \lambda = 1/15,\;\; E(\ell) = 15$$This is approximate of course, since the Exponential distribution has unbounded support from the right, meaning that strictly speaking ""all possible waiting times"" include, under this modeling assumption, larger and large magnitudes up to and ""including"" infinity, but with vanishing probability.  But wait, the Exponential is memoryless: no matter at what point in time we will arrive, we face the same random variable, irrespective of what has gone before.  Given this stochastic/distributional assumption, any point in time is part of an ""interval between two consecutive bus-arrivals"" whose length is described by the same probability distribution with expected value (not maximum value) $15$: ""I am here, I am surrounded by an interval between two bus-arrivals. Some of its length lies in the past and some in the future but I have no way of knowing how much and how much, so the best I can do is  ask What is its expected length -which will be my average waiting time?"" - And the answer is always ""$15$"", alas.  "
Mathematician wants the equivalent knowledge to a quality stats degree,"
I know people love to close duplicates so I am not asking for a reference to start learning statistics (as here). 
I have a doctorate in mathematics but never learned statistics. What is the shortest route to the equivalent knowledge to a top notch BS statistics degree and how do I measure when I have achieved that. 
If a list of books would suffice (assuming I do the exercises lets say), that's terrific. Yes, I expect working out problems to be an implicit part of learning it but I want to fast track as much as realistically possible. I am not looking for an insanely rigorous treatment unless that is part of what statistical majors generally learn.
","['references', 'careers']",
What are the 'big problems' in statistics?,"
Mathematics has its famous Millennium Problems (and, historically, Hilbert's 23), questions that helped to shape the direction of the field.
I have little idea, though, what the Riemann Hypotheses and P vs. NP's of statistics would be.
So, what are the overarching open questions in statistics?
Edited to add:
As an example of the general spirit (if not quite specificity) of answer I'm looking for, I found a ""Hilbert's 23""-inspired lecture by David Donoho at a ""Math Challenges of the 21st Century"" conference: High-Dimensional Data Analysis: The Curses and Blessings of Dimensionality
So a potential answer could talk about big data and why it's important, the types of statistical challenges high-dimensional data poses, and methods that need to be developed or questions that need to be answered in order to help solve the problem.
",['history'],"A big question should involve key issues of statistical methodology or, because statistics is entirely about applications, it should concern how statistics is used with problems important to society.This characterization suggests the following should be included in any consideration of big problems:How best to conduct drug trials.  Currently, classical hypothesis testing requires many formal phases of study.  In later (confirmatory) phases, the economic and ethical issues loom large.  Can we do better?  Do we have to put hundreds or thousands of sick people into control groups and keep them there until the end of a study, for example, or can we find better ways to identify treatments that really work and deliver them to members of the trial (and others) sooner?Coping with scientific publication bias.  Negative results are published much less simply because they just don't attain a magic p-value.  All branches of science need to find better ways to bring scientifically important, not just statistically significant, results to light.  (The multiple comparisons problem and coping with high-dimensional data are subcategories of this problem.)Probing the limits of statistical methods and their interfaces with machine learning and machine cognition.  Inevitable advances in computing technology will make true AI accessible in our lifetimes.  How are we going to program artificial brains?  What role might statistical thinking and statistical learning have in creating these advances?  How can statisticians help in thinking about artificial cognition, artificial learning, in exploring their limitations, and making advances?Developing better ways to analyze geospatial data.  It is often claimed that the majority, or vast majority, of databases contain locational references.  Soon many people and devices will be located in real time with GPS and cell phone technologies.  Statistical methods to analyze and exploit spatial data are really just in their infancy (and seem to be relegated to GIS and spatial software which is typically used by non-statisticians)."
Rules of thumb for minimum sample size for multiple regression,"
Within the context of a research proposal in the social sciences, I was asked the following question:

I have always gone by 100 + m (where m
is the number of predictors) when
determining minimum sample size for
multiple regression. Is this
appropriate?

I get similar questions a lot, often with different rules of thumb.
I've also read such rules of thumb quite a lot in various textbooks.
I sometimes wonder whether popularity of a rule in terms of citations is based on how low the standard is set.
However, I'm also aware of the value of good heuristics in simplifying decision making.
Questions:

What is the utility of simple rules of thumb for minimum sample sizes within the context of applied researchers designing research studies?
Would you suggest an alternative rule of thumb for minimum sample size for multiple regression?
Alternatively, what alternative strategies would you suggest for determining minimum sample size for multiple regression? In particular, it would be good if value is assigned to the degree to which any strategy can readily be applied by a non-statistician.

","['regression', 'sample-size', 'statistical-power', 'rule-of-thumb']",
Likelihood ratio vs Bayes Factor,"
I'm rather evangelistic with regards to the use of likelihood ratios for representing the objective evidence for/against a given phenomenon. However, I recently learned that the Bayes factor serves a similar function in the context of Bayesian methods (i.e. the subjective prior is combined with the objective Bayes factor to yield an objectively updated subjective state of belief). I'm now trying to understand the computational and philosophical differences between a likelihood ratio and a Bayes factor. 
At the computational level, I understand that while the likelihood ratio is usually computed using the likelihoods that represent the maximum likelihood for each model's respective parameterization (either estimated by cross validation or penalized according to model complexity using AIC), apparently the Bayes factor somehow uses likelihoods that represent the likelihood of each model integrated over it's entire parameter space (i.e. not just at the MLE). How is this integration actually achieved typically? Does one really just try to calculate the likelihood at each of thousands (millions?) of random samples from the parameter space, or are there analytic methods to integrating the likelihood across the parameter space? Additionally, when computing the Bayes factor, does one apply correction for complexity (automatically via cross-validated estimation of likelihood or analytically via AIC) as one does with the likelihood ratio?
Also, what are the philosophical differences between the likelihood ratio and the Bayes factor (n.b. I'm not asking about the philosophical differences between the likelihood ratio and Bayesian methods in general, but the Bayes factor as a representation of the objective evidence specifically). How would one go about characterizing the meaning of the Bayes factor as compared to the likelihood ratio?
","['likelihood-ratio', 'bayes-factors']","apparently the Bayes factor somehow uses likelihoods that represent the likelihood of each model integrated over it's entire parameter space (i.e. not just at the MLE). How is this integration actually achieved typically? Does one really just try to calculate the likelihood at each of thousands (millions?) of random samples from the parameter space, or are there analytic methods to integrating the likelihood across the parameter space?First, any situation where you consider a term such as $P(D|M)$ for data $D$ and model $M$ is considered a likelihood model. This is often the bread and butter of any statistical analysis, frequentist or Bayesian, and this is the portion that your analysis is meant to suggest is either a good fit or a bad fit. So Bayes factors are not doing anything fundamentally different than likelihood ratios.It's important to put Bayes factors in their right setting. When you have two models, say, and you convert from probabilities to odds, then Bayes factors act like an operator on prior beliefs:$$ Posterior Odds = Bayes Factor * Prior Odds $$
$$ \frac{P(M_{1}|D)}{P(M_{2}|D)} = B.F. \times \frac{P(M_{1})}{P(M_{2})} $$The real difference is that likelihood ratios are cheaper to compute and generally conceptually easier to specify. The likelihood at the MLE is just a point estimate of the Bayes factor numerator and denominator, respectively. Like most frequentist constructions, it can be viewed as a special case of Bayesian analysis with a contrived prior that's hard to get at. But mostly it arose because it's analytically tractable and easier to compute (in the era before approximate Bayesian computational approaches arose).To the point on computation, yes: you will evaluate the different likelihood integrals in the Bayesian setting with a large-scale Monte Carlo procedure in almost any case of practical interest. There are some specialized simulators, such as GHK, that work if you assume certain distributions, and if you make these assumptions, sometimes you can find analytically tractable problems for which fully analytic Bayes factors exist.But no one uses these; there is no reason to. With optimized Metropolis/Gibbs samplers and other MCMC methods, it's totally tractable to approach these problems in a fully data driven way and compute your integrals numerically. In fact, one will often do this hierarchically and further integrate the results over meta-priors that relate to data collection mechanisms, non-ignorable experimental designs, etc.I recommend the book Bayesian Data Analysis for more on this. Although, the author, Andrew Gelman, seems not to care too much for Bayes factors. As an aside, I agree with Gelman. If you're going to go Bayesian, then exploit the full posterior. Doing model selection with Bayesian methods is like handicapping them, because model selection is a weak and mostly useless form of inference. I'd rather know distributions over model choices if I can... who cares about quantizing it down to ""model A is better than model B"" sorts of statements when you do not have to?Additionally, when computing the Bayes factor, does one apply correction for complexity (automatically via cross-validated estimation of likelihood or analytically via AIC) as one does with the likelihood ratio?This is one of the nice things about Bayesian methods. Bayes factors automatically account for model complexity in a technical sense. You can set up a simple scenario with two models, $M_{1}$ and $M_{2}$ with assumed model complexities $d_{1}$ and $d_{2}$, respectively, with $d_{1} < d_{2}$ and a sample size $N$.Then if $B_{1,2}$ is the Bayes factor with $M_{1}$ in the numerator, under the assumption that $M_{1}$ is true one can prove that as $N\to\infty$, $B_{1,2}$ approaches $\infty$ at a rate that depends on the difference in model complexity, and that the Bayes factor favors the simpler model. More specifically, you can show that under all of the above assumptions, $$ B_{1,2} = \mathcal{O}(N^{\frac{1}{2}(d_{2}-d_{1})}) $$I'm familiar with this derivation and the discussion from the book Finite Mixture and Markov Switching Models by Sylvia Frühwirth-Schnatter, but there are likely more directly statistical accounts that dive more into the epistemology underlying it.I don't know the details well enough to give them here, but I believe there are some fairly deep theoretical connections between this and the derivation of AIC. The Information Theory book by Cover and Thomas hinted at this at least.Also, what are the philosophical differences between the likelihood ratio and the Bayes factor (n.b. I'm not asking about the philosophical differences between the likelihood ratio and Bayesian methods in general, but the Bayes factor as a representation of the objective evidence specifically). How would one go about characterizing the meaning of the Bayes factor as compared to the likelihood ratio?The Wikipedia article's section on ""Interpretation"" does a good job of discussing this (especially the chart showing Jeffreys' strength of evidence scale). Like usual, there's not too much philosophical stuff beyond the basic differences between Bayesian methods and frequentist methods (which you seem already familiar with).The main thing is that the likelihood ratio is not coherent in a Dutch book sense. You can concoct scenarios where the model selection inference from likelihood ratios will lead one to accept losing bets. The Bayesian method is coherent, but operates on a prior which could be extremely poor and has to be chosen subjectively. Tradeoffs.. tradeoffs...FWIW, I think this kind of heavily parameterized model selection is not very good inference. I prefer Bayesian methods and I prefer to organize them more hierarchically, and I want the inference to center on the full posterior distribution if it is at all computationally feasible to do so. I think Bayes factors have some neat mathematical properties, but as a Bayesian myself, I am not impressed by them. They conceal the really useful part of Bayesian analysis, which is that it forces you to deal with your priors out in the open instead of sweeping them under the rug, and allows you to do inference on full posteriors."
Intuition on the Kullback–Leibler (KL) Divergence,"
I have learned about the intuition behind the KL Divergence as how much a model distribution function differs from the theoretical/true distribution of the data. The source I am reading goes on to say that the intuitive understanding of 'distance' between these two distributions is helpful, but should not be taken literally because for two distributions $P$ and $Q$, the KL Divergence is not symmetric in $P$ and $Q$. 
I am not sure how to understand the last statement, or is this where the intuition of 'distance' breaks down? 
I would appreciate a simple, but insightful example.
","['distributions', 'distance', 'intuition', 'kullback-leibler']","A (metric) distance $D$ must be symmetric, i.e. $D(P,Q) = D(Q,P)$.
But, from definition, $KL$ is not.Example: $\Omega = \{A,B\}$, $P(A) = 0.2, P(B) = 0.8$, $Q(A) = Q(B) = 0.5$.We have:$$KL(P,Q) = P(A)\log \frac{P(A)}{Q(A)} + P(B) \log \frac{P(B)}{Q(B)} \approx 0.19$$and$$KL(Q,P) = Q(A)\log \frac{Q(A)}{P(A)} + Q(B) \log \frac{Q(B)}{P(B)} \approx 0.22$$thus $KL(P,Q) \neq KL(Q,P)$ and therefore $KL$ is not a (metric) distance."
Central limit theorem for sample medians,"
If I calculate the median of a sufficiently large number of observations drawn from the same distribution, does the central limit theorem state that the distribution of medians will approximate a normal distribution? My understanding is that this is true with the means of a large number of samples, but is it also true with medians?
If not, what is the underlying distribution of sample medians?
","['normal-distribution', 'mathematical-statistics', 'sampling', 'median', 'central-limit-theorem']","If you work in terms of indicator variables (i.e. $Z_i = 1$ if $X_i \leq x$ and $0$ otherwise), you can directly apply the Central limit theorem to a mean of $Z$'s, and by using the Delta method, turn that into an asymptotic normal distribution for $F_X^{-1}(\bar{Z})$, which in turn means that you get asymptotic normality for fixed quantiles of $X$.So not just the median, but quartiles, 90th percentiles, ... etc.Loosely, if we're talking about the $q$th  sample quantile in sufficiently large samples, we get that it will approximately have a normal distribution with mean the $q$th population quantile $x_q$ and variance $q(1-q)/(nf_X(x_q)^2)$. Hence for the median ($q = 1/2$), the variance in sufficiently large samples will be approximately $1/(4nf_X(\tilde{\mu})^2)$.You need all the conditions along the way to hold, of course, so it doesn't work in all situations, but for continuous distributions where the density at the population quantile is positive and differentiable, etc, ...Further, it doesn't hold for extreme quantiles, because the CLT doesn't kick in there (the average of Z's won't be asymptotically normal). You need different theory for extreme values.Edit: whuber's critique is correct; this would work if $x$ were a population median rather than a sample median. The argument needs to be modified to actually work properly."
How exactly did statisticians agree to using (n-1) as the unbiased estimator for population variance without simulation?,"
The formula for computing variance has $(n-1)$ in the denominator:
$s^2 = \frac{\sum_{i=1}^N (x_i - \bar{x})^2}{n-1}$
I've always wondered why. However, reading and watching a few good videos about ""why"" it is, it seems, $(n-1)$ is a good unbiased estimator of the population variance. Whereas $n$ underestimates and $(n-2)$ overestimates the population variance.
What I'm curious to know, is that in the era of no computers how exactly was this choice made?  Is there an actual mathematical proof proving this or was this purely empirical and statisticians made A LOT of calculations by hand to come up with the ""best explanation"" at the time? 
Just how did statisticians come up with this formula in the early 19th century with the aid of computers? Manual or there is more to it than meets the eye?
","['variance', 'unbiased-estimator', 'proof', 'history']","The correction is called Bessel's correction and it has a mathematical proof. Personally, I was taught it the easy way: using $n-1$ is how you correct the bias of $E[\frac{1}{n}\sum_1^n(x_i - \bar x)^2]$ (see here).You can also explain the correction based on the concept of degrees of freedom, simulation isn't strictly needed."
When (if ever) is a frequentist approach substantively better than a Bayesian?,"
Background: I do not have an formal training in Bayesian statistics (though I am very interested in learning more), but I know enough--I think--to get the gist of why many feel as though they are preferable to Frequentist statistics. Even the undergraduates in the introductory statistics (in social sciences) class I am teaching find the Bayesian approach appealing--""Why are we interested in calculating the probability of the data, given the null? Why can't we just quantify the probability of the null hypothesis? Or the alternative hypothesis? And I've also read threads like these, which attest to the empirical benefits of Bayesian statistics as well. But then I came across this quote by Blasco (2001; emphasis added):

If the animal breeder is not interested in the philosophical problems associated with induction, but in tools to solve problems, both Bayesian and frequentist schools of inference are well established and it is not necessary to justify why one or the other school is preferred. Neither of them now has operational difficulties, with the exception of some complex cases...To choose one school or the other should be related to whether there are solutions in one school that the other does not offer, to how easily the problems are solved, and to how comfortable the scientist feels with the particular way of expression results. 

The Question: The Blasco quote seems to suggest that there might be times when a Frequentist approach is actually preferable to a Bayesian one. And so I am curious: when would a frequentist approach be preferable over a Bayesian approach? I'm interested in answers that tackle the question both conceptually (i.e., when is knowing the probability of the data conditioned on the null hypothesis especially useful?) and empirically (i.e., under what conditions do Frequentist methods excel vs. Bayesian?). 
It would also be preferable if answers were conveyed as accessibly as possible--it would be nice to take some responses back to my class to share with my students (though I understand some level of technicality is required). 
Finally, despite being a regular user of Frequentist statistics, I am actually open to the possibility that Bayesian just wins across the board.  
","['bayesian', 'frequentist', 'philosophical']","Here's five reasons why frequentists methods may be preferred: Faster. Given that Bayesian statistics often give nearly identical answers to frequentist answers (and when they don't, it's not 100% clear that Bayesian is always the way to go), the fact that frequentist statistics can be obtained often several orders of magnitude faster is a strong argument. Likewise, frequentist methods do not require as much memory to store the results. While these things may seem somewhat trivial, especially with smaller datasets, the fact that Bayesian and Frequentist typically agree  in results (especially if you have lots of informative data) means that if you are going to care, you may start caring about the less important things. And of course, if you live in the big data world, these are not trivial at all. Non-parametric statistics. I recognize that Bayesian statistics does have non-parametric statistics, but I would argue that the frequentist side of the field has some truly undeniably practical tools, such as the Empirical Distribution Function. No method in the world will ever replace the EDF, nor the Kaplan Meier curves, etc. (although clearly that's not to say those methods are the end of an analysis). Less diagnostics. MCMC methods, the most common method for fitting Bayesian models, typically require more work by the user than their frequentist counter part. Usually, the diagnostic for an MLE estimate is so simple that any good algorithm implementation will do it automatically (although that's not to say every available implementation is good...). As such, frequentist algorithmic diagnostics is typically ""make sure there's no red text when fitting the model"". Given that all statisticians have limited bandwidth, this frees up more time to ask questions like ""is my data really approximately normal?"" or ""are these hazards really proportional?"", etc. Valid inference under model misspecification. We've all heard that ""All models are wrong but some are useful"", but different areas of research take this more or less seriously. The Frequentist literature is full of methods for fixing up inference when the model is misspecified: bootstrap estimator, cross-validation, sandwich estimator (link also discusses general MLE inference under model misspecification), generalized estimation equations (GEE's), quasi-likelihood methods, etc. As far as I know, there is very little in the Bayesian literature about inference under model misspecification (although there's a lot of discussion of model checking, i.e., posterior predictive checks). I don't think this just by chance: evaluating how an estimator behaves over repeated trials does not require the estimator to be based on a ""true"" model, but using Bayes theorem does! Freedom from the prior (this is probably the most common reason for why people don't use Bayesian methods for everything). The strength of the Bayesian standpoint is often touted as the use of priors. However, in all of the applied fields I have worked in, the idea of an informative prior in the analysis is not considered. Reading literature on how to elicit priors from non-statistical experts gives good reasoning for this; I've read papers that say things like (cruel straw-man like paraphrasing my own) ""Ask the researcher who hired you because they have trouble understanding statistics to give a range that they are 90% certain the effect size they have trouble imagining will be in. This range will typically be too narrow, so arbitrarily try to get them to widen it a little. Ask them if their belief looks like a gamma distribution. You will probably have to draw a gamma distribution for them, and show how it can have heavy tails if the shape parameter is small. This will also involve explaining what a PDF is to them.""(note: I don't think even statisticians are really able to accurately say a priori whether they are 90% or 95% certain whether the effect size lies in a range, and this difference can have a substantial effect on the analysis!). Truth be told, I'm being quite unkind and there may be situations where eliciting a prior may be a little more straightforward.  But you can see how this is a can of worms. Even if you switch to non-informative priors, it can still be a problem; when transforming parameters, what are easily mistaken for non-informative priors suddenly can be seen as very informative! Another example of this is that I've talked with several researchers who adamantly do not want to hear what another expert's interpretation of the data is because empirically, the other experts tend to be over confident. They'd rather just know what can be inferred from the other expert's data and then come to their  own conclusion. I can't recall where I heard it, but somewhere I read the phrase ""if you're a Bayesian, you want everyone to be a Frequentist"". I interpret that to mean that theoretically, if you're a Bayesian and someone describes their analysis results, you should first try to remove the influence of their prior and then figure out what the impact would be if you had used your own. This little exercise would be simplified if they had given you a confidence interval rather than a credible interval!Of course, if you abandon informative priors, there is still utility in Bayesian analyses. Personally, this where I believe their highest utility lies; there are some problems that are extremely hard to get any answer from in using MLE methods but can be solved quite easily with MCMC. But my view on this being Bayesian's highest utility is due to strong priors on my part, so take it with a grain of salt."
Bayes regression: how is it done in comparison to standard regression?,"
I got some questions about the Bayesian regression:

Given a standard regression as $y = \beta_0 + \beta_1 x + \varepsilon$.
If I want to change this into a Bayesian regression, do I need prior distributions both for $\beta_0$ and $\beta_1$ (or doesn't it work this way)? 
In standard regression one would try to minimize the residuals to get  single values for $\beta_0$ and $\beta_1$. 
How is this done in Bayes regression? 


I really struggle a lot here: 
$$ \text{posterior} = \text{prior} \times \text{likelihood} $$
Likelihood comes from the current dataset (so it's my regression parameter but not as a single value but as a likelihood distribution, right?). Prior comes from a previous research (let's say). So I got this equation:
$$ y = \beta_1 x + \varepsilon $$
with $\beta_1$ being my likelihood or posterior (or is this just totally wrong)?  
I simply can't understand how the standard regression transforms into a Bayes one.
","['regression', 'bayesian']","The simple linear regression model$$ y_i = \alpha + \beta x_i + \varepsilon $$can be written in terms of the probabilistic model behind it$$
\mu_i =  \alpha + \beta x_i \\
y_i \sim \mathcal{N}(\mu_i, \sigma)
$$i.e. dependent variable $Y$ follows normal distribution parametrized by mean $\mu_i$, that is a linear function of $X$ parametrized by $\alpha,\beta$, and by standard deviation $\sigma$. If you estimate such a model using ordinary least squares, you do not have to bother about the probabilistic formulation, because you are searching for optimal values of $\alpha,\beta$ parameters by minimizing the squared errors of fitted values to predicted values. On another hand, you could estimate such model using maximum likelihood estimation, where you would be looking for optimal values of parameters by maximizing the likelihood function$$ \DeclareMathOperator*{\argmax}{arg\,max} \argmax_{\alpha,\,\beta,\,\sigma} \prod_{i=1}^n \mathcal{N}(y_i; \alpha + \beta x_i, \sigma)  $$where $\mathcal{N}$ is a density function of normal distribution evaluated at $y_i$ points, parametrized by means $\alpha + \beta x_i$ and standard deviation $\sigma$.In the Bayesian approach instead of maximizing the likelihood function alone, we would assume prior distributions for the parameters and use the Bayes theorem$$ \text{posterior} \propto \text{likelihood} \times \text{prior} $$The likelihood function is the same as above, but what changes is that you assume some prior distributions for the estimated parameters $\alpha,\beta,\sigma$ and include them into the equation$$ \underbrace{f(\alpha,\beta,\sigma\mid Y,X)}_{\text{posterior}} \propto \underbrace{\prod_{i=1}^n \mathcal{N}(y_i\mid \alpha + \beta x_i, \sigma)}_{\text{likelihood}} \; \underbrace{f_{\alpha}(\alpha) \, f_{\beta}(\beta) \, f_{\sigma}(\sigma)}_{\text{priors}} $$""What distributions?"" is a different question, since there is an unlimited number of choices. For $\alpha,\beta$ parameters you could, for example, assume normal distributions parametrized by some hyperparameters, or $t$-distribution if you want to assume heavier tails, or uniform distribution if you do not want to make many assumptions, but you want to assume that the parameters can be a priori ""anything in the given range"", etc. For $\sigma$ you need to assume some prior distribution that is bounded to be greater than zero since standard deviation needs to be positive. This may lead to the model formulation as illustrated below by John K. Kruschke.(source: http://www.indiana.edu/~kruschke/BMLR/)While in the maximum likelihood you were looking for a single optimal value for each of the parameters, in the Bayesian approach by applying the Bayes theorem you obtain the posterior distribution of the parameters. The final estimate will depend on the information that comes from your data and from your priors, but the more information is contained in your data, the less influential are priors.Notice that when using uniform priors, they take form $f(\theta) \propto 1$ after dropping the normalizing constants. This makes Bayes theorem proportional to the likelihood function alone, so the posterior distribution will reach its maximum at exactly the same point as the maximum likelihood estimate. What follows, the estimate under uniform priors will be the same as by using ordinary least squares since minimizing the squared errors corresponds to maximizing the normal likelihood.To estimate a model in the Bayesian approach in some cases you can use conjugate priors, so the posterior distribution is directly available (see example here). However, in the vast majority of cases, posterior distribution will not be directly available and you will have to use Markov Chain Monte Carlo methods for estimating the model (check this example of using Metropolis-Hastings algorithm to estimate parameters of linear regression). Finally, if you are only interested in point estimates of parameters, you could use maximum a posteriori estimation, i.e.$$ \argmax_{\alpha,\,\beta,\,\sigma} f(\alpha,\beta,\sigma\mid Y,X) $$For a more detailed description of logistic regression, you can check the Bayesian logit model - intuitive explanation? thread.For learning more you could check the following books:Kruschke, J. (2014). Doing Bayesian Data Analysis: A Tutorial with R,
JAGS, and Stan. Academic Press.Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (2004).
Bayesian data analysis. Chapman & Hall/CRC."
Cross Entropy vs. Sparse Cross Entropy: When to use one over the other,"
I am playing with convolutional neural networks using Keras+Tensorflow to classify categorical data. I have a choice of two loss functions: categorial_crossentropy and sparse_categorial_crossentropy.
I have a good intuition about the categorial_crossentropy loss function, which is defined as follows:
$$
J(\textbf{w}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \text{log}(\hat{y}_i) + (1-y_i) \text{log}(1-\hat{y}_i) \right]
$$ 
where,

$\textbf{w}$ refer to the model parameters, e.g. weights of the neural network
$y_i$ is the true label
$\hat{y_i}$ is the predicted label

Both labels use the one-hot encoded scheme. 
Questions:

How does the above loss function change in sparse_categorial_crossentropy?
What is the mathematical intuition behind it?
When to use one over the other?

","['machine-learning', 'conv-neural-network', 'loss-functions', 'information-theory', 'cross-entropy']","Both, categorical cross entropy and sparse categorical cross entropy have the same loss function which you have mentioned above.
The only difference is the format in which you mention $Y_i$ (i,e true labels).If your $Y_i$'s are one-hot encoded, use categorical_crossentropy.
Examples (for a 3-class classification):  [1,0,0] , [0,1,0], [0,0,1]But if your $Y_i$'s are integers, use sparse_categorical_crossentropy.
Examples for above 3-class classification problem: 
[1] , [2], [3]The usage entirely depends on how you load your dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector."
What's so 'moment' about 'moments' of a probability distribution?,"
I KNOW what moments are and how to calculate them and how to use the moment generating function for getting higher order moments. Yes, I know the math.
Now that I need to get my statistics knowledge lubricated for work, I thought I might as well ask this question – it's been nagging me for about a few years and back in college no professor knew the answer or would just dismiss the question (honestly).
So what does the word ""moment"" mean in this case? Why this choice of word? It doesn't sound intuitive to me (or I never heard it that way back in college :) Come to think of it I am equally curious with its usage in ""moment of inertia"" ;) but let's not focus on that for now.
So what does a ""moment"" of a distribution mean and what does it seek to do and why THAT word! :) Why does any one care about moments? At this moment I am feeling otherwise about that moment ;)
PS: Yes, I've probably asked a similar question on variance but I do value intuitive understanding over 'look in the book to find out' :)
","['distributions', 'terminology', 'moments', 'intuition']","According to the paper ""First (?) Occurrence of Common Terms in Mathematical Statistics"" by H.A. David, the first use of the word 'moment' in this situation was in a 1893 letter to Nature by Karl Pearson entitled ""Asymmetrical Frequency Curves"".Neyman's 1938 Biometrika paper ""A Historical Note on Karl Pearson's Deduction of the Moments of the Binomial"" gives a good synopsis of the letter and Pearson's subsequent work on moments of the binomial distribution and the method of moments.  It's a really good read.  Hopefully you have access JSTOR for I don't have the time now to give a good summary of the paper (though I will this weekend).  Though I will mention one piece that may give insight as to why the term 'moment' was used.  From Neyman's paper:It [Pearson's memoir] deals primarily with methods of approximating 
      continuous frequency curves by means of some processes involving the 
      calculation of easy formulae.  One of these formulae considered was the 
      ""point-binomial"" or the ""binomial with loaded ordinates"".  The formula
      differs from what to-day we call a binomial, viz. (4), only by a factor 
      $\alpha$, representing the area under the continuous curve which it is desired
      to fit.This is what eventually led to the 'method of moments.' Neyman goes over the Pearson's derivation of the binomial moments in the above paper.And from Pearson's letter:We shall now proceed to find the first four moments of the system of 
      rectangles round GN. If the inertia of each rectangle might be considered 
      as concentrated along its mid vertical, we should have for the $s^{\text{th}}$ moment
      round NG, writing $d = c(1 + nq)$.This hints at the fact that Pearson used the term 'moment' as an allusion to 'moment of inertia,' a term common in physics.Here's a scan of most of Pearson's Nature letter:You can view the entire article on page 615 here."
"What is a ""kernel"" in plain English?","
There are several distinct usages:

kernel density estimation  
kernel trick  
kernel smoothing

Please explain what the ""kernel"" in them means, in plain English, in your own words.
","['kernel-trick', 'kernel-smoothing']","In both statistics (kernel density estimation or kernel smoothing) and machine learning (kernel methods) literature, kernel is used as a measure of similarity. In particular, the kernel function $k(x,.)$ defines the distribution of similarities of points around a given point $x$. $k(x,y)$ denotes the similarity of point $x$ with another given point $y$."
When to use generalized estimating equations vs. mixed effects models?,"
I have been quite happily using mixed effects models for a while now with longitudinal data. I wish I could fit AR relationships in lmer (I think I'm right that I can't do this?) but I don't think it's desperately important so I don't worry too much.
I've just come across generalized estimating equations (GEE), and they seem to offer a lot more flexibility than ME models.
At the risk of asking an over-general question, is there any advice as to which is better for different tasks? I've seen some papers comparing them, and they tend to be of the form:
""In this highly specialised area, don't use GEEs for X, don't use ME models for Y"".
I haven't found any more general advice. Can anyone enlighten me?
Thank you!
","['mixed-model', 'generalized-estimating-equations']","Use GEE when you're interested in uncovering the population average effect of a covariate vs. the individual specific effect. These two things are only equivalent in linear models, but not in non-linear (e.g. logistic). To see this, take, for example the random effects logistic model of the $j$'th observation of the $i$'th subject, $Y_{ij}$; $$ \log \left( \frac{p_{ij}}{1-p_{ij}} \right) 
= \mu + \eta_{i} $$ where $\eta_{i} \sim N(0,\sigma^{2})$ is a random effect for subject $i$ and $p_{ij} = P(Y_{ij} = 1|\eta_{i})$. If you used a random effects model on these data, then you would get an estimate of $\mu$ that accounts for the fact that a mean zero normally distributed perturbation was applied to each individual, making it individual specific. If you used GEE on these data, you would estimate the population average log odds. In this case that would be $$ \nu = \log \left( \frac{ E_{\eta} \left( \frac{1}{1 + e^{-\mu-\eta_{i}}} \right)}{ 
1-E_{\eta} \left( \frac{1}{1 + e^{-\mu-\eta_{i}}} \right)} \right) $$ $\nu \neq \mu$, in general. For example, if $\mu = 1$ and $\sigma^{2} = 1$, then $\nu \approx .83$. Although the random effects have mean zero on the transformed (or linked) scale, their effect is not mean zero on the original scale of the data. Try simulating some
data from a mixed effects logistic regression model and comparing the population level average with the inverse-logit of the intercept and you will see that they are not equal, as in this example. This difference in the interpretation of the coefficients is the fundamental difference between GEE and random effects models. Edit: In general, a mixed effects model with no predictors can be written as $$ \psi \big( E(Y_{ij}|\eta_{i}) \big) = \mu + \eta_{i} $$ where $\psi$ is a link function. Whenever $$ \psi \Big( E_{\eta} \Big( \psi^{-1} \big( E(Y_{ij}|\eta_{i}) \big) \Big) \Big) \neq E_{\eta} \big( E(Y_{ij}|\eta_{i}) \big) $$ there will be a difference between the population average coefficients (GEE) and the individual specific coefficients (random effects models). That is, the averages change by transforming the data, integrating out the random effects on the transformed scale, and then transformating back. Note that in the linear model, (that is, $\psi(x) = x$), the equality does hold, so they are equivalent. Edit 2: It is also worth noting that the ""robust"" sandwich-type standard errors produced by a GEE model provide valid asymptotic confidence intervals (e.g. they actually cover 95% of the time) even if the correlation structure specified in the model is not correct. Edit 3: If your interest is in understanding the association structure in the data, the GEE estimates of associations are notoriously inefficient (and sometimes inconsistent). I've seen a reference for this but can't place it right now. "
"What are the major philosophical, methodological, and terminological differences between econometrics and other statistical fields?","
Econometrics has substantial overlap with traditional statistics, but often uses its own jargon about a variety of topics (""identification,"" ""exogenous,"" etc.).  I once heard an applied statistics professor in another field comment that frequently the terminology is different but the concepts are the same.  Yet it also has its own methods and philosophical distinctions (Heckman's famous essay comes to mind).
What terminology differences exist between econometrics and mainstream statistics, and where do the fields diverge to become different in more than just terminology?
","['econometrics', 'terminology', 'philosophical']","There are some terminological differences where the same thing is called different names in different disciplines:There are terminological differences where the same term is used to mean different things in different disciplines:I view the unique contributions of econometrics to beOverall, economists tend to look for strong interpretation of coefficients in their models. Statisticians would take a logistic model as a way to get to the probability of the positive outcome, often as a simple predictive device, and may also note the GLM interpretation with nice exponential family properties that it possesses, as well as connections with discriminant analysis. Economists would think about the utility interpretation of the logit model, and be concerned that only $\beta/\sigma$ is identified in this model, and that heteroskedasticity can throw it off. (Statisticians will be wondering what $\sigma$ are the economists talking about, of course.) Of course, a utility that is linear in its inputs is a very funny thing from the perspective of Microeconomics 101, although some generalizations to semi-concave functions are probably done in Mas-Collel.What economists generally tend to miss, but, IMHO, would benefit from, are aspects of multivariate analysis (including latent variable models as a way to deal with measurement errors and multiple proxies... statisticians are oblivious to these models, though, too), regression diagnostics (all these Cook's distances, Mallows' $C_p$, DFBETA, etc.), analysis of missing data (Manski's partial identification is surely fancy, but the mainstream MCAR/MAR/NMAR breakdown and multiple imputation are more useful), and survey statistics. A lot of other contributions from the mainstream statistics have been entertained by econometrics and either adopted as a standard methodology, or passed by as a short term fashion: ARMA models of the 1960s are probably better known in econometrics than in statistics, as some graduate programs in statistics may fail to offer a time series course these days; shrinkage estimators/ridge regression of the 1970s have come and gone; the bootstrap of the 1980s is a knee-jerk reaction for any complicated situations, although economists need to be better aware of the limitations of the bootstrap; the empirical likelihood of the 1990s has seen more methodology development from theoretical econometricians than from theoretical statisticians; computational Bayesian methods of the 2000s are being entertained in econometrics, but my feeling is that are just too parametric, too heavily model-based, to be compatible with the robustness paradigm I mentioned earlier. (EDIT: that was the view on the scene in 2012; by 2020, Bayesian models have become standard in empirical macro where people probably care a little less about robustness, and are making their presence heard in empirical micro, as well. They are just too easy to run these days to pass by.) Whether economists will find any use of the statistical learning/bioinformatics or spatio-temporal stuff that is extremely hot in modern statistics is an open call."
"Is there any good reason to use PCA instead of EFA? Also, can PCA be a substitute for factor analysis?","
In some disciplines, PCA (principal component analysis) is systematically used without any justification, and PCA and EFA (exploratory factor analysis) are considered as synonyms.
I therefore recently used PCA to analyse the results of a scale validation study (21 items on 7-points Likert scale, assumed to compose 3 factors of 7 items each) and a reviewer asks me why I chose PCA instead of EFA. I read about the differences between both techniques, and it seems that EFA is favored against PCA in a majority of your answers here. 
Do you have any good reasons for why PCA would be a better choice? What benefits it could provide and why it could be a wise choice in my case?
","['pca', 'factor-analysis', 'exploratory-data-analysis']",
"Probability of a single real-life future event: What does it mean when they say that ""Hillary has a 75% chance of winning""?","
As the election is a one time event, it is not an experiment that can be repeated. So exactly what does the statement ""Hillary has a 75% chance of winning"" technically mean? I am seeking a statistically correct definition not an intuitive or conceptual one.
I am an amateur statistics fan who is trying to respond to this question that came up in a discussion. I am pretty sure there's a good objective response to it but I can't come up with it myself...
","['probability', 'prediction', 'politics']","All the answers so far provided are helpful, but they aren't very statistically precise, so I'll take a shot at that. At the same time, I'm going to give a general answer rather than focusing on this election.The first thing to keep in mind when we're trying to answer questions about real-world events like Clinton winning the election, as opposed to made-up math problems like taking balls of various colors out of an urn, is that there isn't a unique reasonable way to answer the question, and hence not a unique reasonable answer. If somebody just says ""Hillary has a 75% chance of winning"" and doesn't go on to describe their model of the election, the data they used to make their estimates, the results of their model validation, their background assumptions, whether they're referring to the popular vote or the electoral vote, etc., then they haven't really told you what they mean, much less provided enough information for you to evaluate whether their prediction is any good. Besides, it isn't beneath some people to do no data analysis at all and simply draw a precise-sounding number out of thin air.So, what are some procedures a statistician might use to estimate Clinton's chances? Indeed, how might they frame the problem? At a high level, there are various notions of probability itself, two of the most important of which are frequentist and Bayesian.In a frequentist view, a probability represents the limiting frequency of an event over many independent trials of the same experiment, as in the law of large numbers (strong or weak). Even though any particular election is a unique event, its outcome can be seen as a draw from an infinite population of events both historical and hypothetical, which could comprise all American presidential elections, or all elections worldwide in 2016, or something else. A 75% chance of a Clinton victory means that if $X_1, X_2, …$ is a sequence of outcomes (0 or 1) of independent elections that are entirely equivalent to this election so far as our model is concerned, then the sample mean of $X_1, X_2, …, X_n$ converges in probability to .75 as $n$ goes to infinity.In a Bayesian view, a probability represents a degree of believability or credibility (which may or may not be actual belief, depending on whether you're a subjectivist Bayesian). A 75% chance of a Clinton victory means that it is 75% credible she will win. Credibilities, in turn, can be chosen freely (based on a model's or analyst's preexisting beliefs) within the constraints of basic laws of probability (like Bayes's theorem, and the fact that the probability of a joint event cannot exceed the marginal probability of either of the component events). One way to summarize these laws is that if you take bets on the outcome of an event, offering odds to gamblers according to your credibilities, then no gambler can construct a Dutch book against you, that is, a set of bets that guarantees you will lose money no matter how the event actually works out.Whether you take a frequentist or Bayesian view on probability, there are still a lot of decisions to be made about how to analyze the data and estimate the probability. Possibly the most popular method is based on parametric regression models, such as linear regression. In this setting, the analyst chooses a parametric family of distributions (that is, probability measures) that is indexed by a vector of numbers called parameters. Each outcome is an independent random variable drawn from this distribution, transformed according to the covariates, which are known values (such as the unemployment rate) that the analyst wants to use to predict the outcome. The analyst chooses estimates of the parameter values using the data and a criterion of model fit such as least squares or maximum likelihood. Using these estimates, the model can produce a prediction of the outcome (possibly just a single value, possibly an interval or other set of values) for any given value of the covariates. In particular, it can predict the outcome of an election. Besides parametric models, there are nonparametric models (that is, models defined by a family of distributions that is indexed with an infinitely long parameter vector), and also methods of deciding on predicted values that use no model by which the data was generated at all, such as nearest-neighbor classifiers and random forests.Coming up with predictions is one thing, but how do you know whether they're any good? After all, sufficiently inaccurate predictions are worse than useless. Testing predictions is part of the larger practice of model validation, that is, quantifying how good a given model is for a given purpose. Two popular methods for validating predictions are cross-validation and splitting the data into training and testing subsets before fitting any models. To the degree that the elections included in the data are representative of the 2016 US presidential election, the estimates of predictive accuracy we get from validating predictions will inform us how accurate our prediction will be of the 2016 US presidential election."
Does no correlation imply no causality?,"
I know that correlation does not imply causality but does an absence of correlation imply absence of causality?
","['correlation', 'causality']","does an absence of correlation imply absence of causality?No. Any controlled system is a counterexample.Without causal relationships control is clearly impossible, but successful control means - roughly speaking - that some quantity is being maintained constant, which implies it won't be correlated with anything, including whatever things are causing it to be constant.So in this situation, concluding no causal relationship from lack of correlation would be a mistake.Here's a somewhat topical example."
How to use Pearson correlation correctly with time series,"
I have 2 time-series (both smooth) that I would like to cross-correlate to see how correlated they are.
I intend to use the Pearson correlation coefficient. Is this appropriate?
My second question is that I can choose to sample the 2 time-series as well as I like. i.e. I can choose how many data points I will us. Will this affect the correlation coefficient that is output? Do I need to account for this?
For illustration purposes
option(i)

[1,    4,    7,    10] & [6,    9,    6,    9,    6]

option(ii)

[1,2,3,4,5,6,7,8,9,10] & [6,7,8,9,8,7,6,7,8,9,8,7,6]  

","['time-series', 'correlation', 'pearson-r', 'smoothing']","Pearson correlation is used to look at correlation between series ... but being time series the correlation is looked at across different lags -- the cross-correlation function.The cross-correlation is impacted by dependence within-series, so in many cases the within-series dependence should be removed first. So to use this correlation, rather than smoothing the series, it's actually more common (because it's meaningful) to look at dependence between residuals - the rough part that's left over after a suitable model is found for the variables.You probably want to begin with some basic resources on time series models before delving into trying to figure out whether a Pearson correlation across (presumably) non-stationary, smoothed series is interpretable.In particular, you'll probably want to look into the phenomenon here.[Edit -- the Wikipedia landscape keeps changing; the above para. should probably be revised to reflect what's there now.]e.g. see some discussionsWhy Do We Sometimes Get Nonsense-Correlations between Time Series? A Study in Sampling and the Nature of Time Series  (the opening quote of Yule, in a paper presented in 1925 but published the following year, summarizes the problem quite well)Christos Agiakloglou and Apostolos Tsimpanos, Spurious Correlations for Stationary AR(1) Processes http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.611.5055&rep=rep1&type=pdf  (this shows that you can even get the problem between stationary series; hence the tendency to pre-whiten)The classic reference of Yule, (1926) 1 mentioned above.You may also find the discussion here useful, as well as the discussion here--Using Pearson correlation in a meaningful way between time series is difficult and sometimes surprisingly subtle.I looked up spurious correlation, but I don't care if my A series is the cause of my B series or vice versa. I only want to know if you can learn something about series A by looking at what series B is doing (or vice versa). In other words - do they have an correlation.Take note of my previous comment about the narrow use of the term spurious correlation in the Wikipedia article.The point about spurious correlation is that series can appear correlated, but the correlation itself is not meaningful. Consider two people tossing two distinct coins counting number of heads so far minus number of tails so far as the value of their series.(So if person 1 tosses $\text{HTHH...}$ they have 3-1 = 2 for the value at the 4th time step, and their series goes $1, 0, 1, 2,...$.)Obviously there's no connection whatever between the two series. Clearly neither can tell you the first thing about the other!But look at the sort of correlations you get between pairs of coins:If I didn't tell you what those were, and you took any pair of those series by themselves, those would be impressive correlations would they not?But they're all meaningless. Utterly spurious. None of the three pairs are really any more positively or negatively related to each other than any of the others -- its just cumulated noise (and for people thinking an edit would improve this, yes, I really do mean to write, cumulated, as in the inverse of differencing, not accumulated, which would be the total, please desist from 'fixing' this). The spuriousness isn't just about prediction, the whole notion of considering association between series without taking account of the within-series dependence is misplaced.All you have here is within-series dependence. There's no actual cross-series relation whatever (and yes, I really do mean to type ""whatever"" rather than ""whatsoever"").Once you deal properly with the issue that makes these series auto-dependent - they're all integrated (Bernoulli random walks), so you need to difference (yes, it's definitely difference NOT differentiate, I do not understand why people keep replacing a perfectly correct word with a very-much-incorrect word in my various time series posts, and other people blithely approve it) them - the ""apparent"" association disappears (the largest absolute cross-series correlation of the three is 0.048).What that tells you is the truth -- the apparent association is a mere illusion caused by the dependence within-series.Your question asked ""how to use Pearson correlation correctly with time series"" -- so please understand: if there's within-series dependence and you don't deal with it first, you won't be using it correctly.Further, smoothing won't reduce the problem of serial dependence; quite the opposite -- it makes it even worse! Here are the correlations after smoothing (default loess smooth - of series vs index - performed in R):They all got further from 0. They're all still nothing but meaningless noise, though now it's smoothed, accumulated noise. (By smoothing, we reduce the variability in the series we put into the correlation calculation, so that may be why the correlation goes up.)1: Yule, G.U. (1926) ""Why do we Sometimes get Nonsense-Correlations between Time-Series?"" J.Roy.Stat.Soc., 89, 1, pp. 1-63"
How to visualize what canonical correlation analysis does (in comparison to what principal component analysis does)?,"
Canonical correlation analysis (CCA) is a technique related to principal component analysis (PCA). While it is easy to teach PCA or linear regression using a scatter plot (see a few thousand examples on google image search), I have not seen a similar intuitive two-dimensional example for CCA. How to explain visually what linear CCA does?
","['regression', 'data-visualization', 'pca', 'canonical-correlation', 'geometry']","Well, I think it is really difficult to present a visual explanation of Canonical correlation analysis (CCA) vis-a-vis Principal components analysis (PCA) or Linear regression. The latter two are often explained and compared by means of a 2D or 3D data scatterplots, but I doubt if that is possible with CCA. Below I've drawn pictures which might explain the essence and the differences in the three procedures, but even with these pictures - which are vector representations in the ""subject space"" - there are problems with capturing CCA adequately. (For algebra/algorithm of canonical correlation analysis look in here.)Drawing individuals as points in a space where the axes are variables, a usual scatterplot, is a variable space. If you draw the opposite way - variables as points and individuals as axes - that will be a subject space. Drawing the many axes is actually needless because the space has the number of non-redundant dimensions equal to the number of non-collinear variables. Variable points are connected with the origin and form vectors, arrows, spanning the subject space; so here we are (see also). In a subject space, if variables have been centered, the cosine of the angle between their vectors is Pearson correlation between them, and the vectors' lengths squared are their variances. On the pictures below the variables displayed are centered (no need for a constant arises).Variables $X_1$ and $X_2$ positively correlate: they have acute angle between them. Principal components $P_1$ and $P_2$ lie in the same space ""plane X"" spanned by the two variables. The components are variables too, only mutually orthogonal (uncorrelated). The direction of $P_1$ is such as to maximize the sum of the two squared loadings of this component; and $P_2$, the remaining component, goes orthogonally to $P_1$ in plane X. The squared lengths of all the four vectors are their variances (the variance of a component is the aforementioned sum of its squared loadings). Component loadings are the coordinates of variables onto the components - $a$'s shown on the left pic. Each variable is the error-free linear combination of the two components, with the corresponding loadings being the regression coefficients. And vice versa, each component is the error-free linear combination of the two variables; the regression coefficients in this combination are given by the skew coordinates of the components onto the variables - $b$'s shown on the right pic. The actual regression coefficient magnitude will be $b$ divided by the product of lengths (standard deviations) of the predicted component and the predictor variable, e.g. $b_{12}/(|P_1|*|X_2|)$. [Footnote: The components' values appearing in the mentioned above two linear combinations are standardized values, st. dev. = 1. This because the information about their variances is captured by the loadings. To speak in terms of unstandardized component values, $a$'s on the pic above should be eigenvectors' values, the rest of the reasoning being the same.]Whereas in PCA everything lies in plane X, in multiple regression there appears a dependent variable $Y$ which usually doesn't belong to plane X, the space of the predictors $X_1$, $X_2$. But $Y$ is perpendicularly projected onto plane X, and the projection $Y'$, the $Y$'s shade, is the prediction by or linear combination of the two $X$'s. On the picture, the squared length of $e$ is the error variance. The cosine between $Y$ and $Y'$ is the multiple correlation coefficient. Like it was with PCA, the regression coefficients are given by the skew coordinates of the prediction ($Y'$) onto the variables - $b$'s. The actual regression coefficient magnitude will be $b$ divided by the length (standard deviation) of the predictor variable, e.g. $b_{2}/|X_2|$.In PCA, a set of variables predict themselves: they model principal components which in turn model back the variables, you don't leave the space of the predictors and (if you use all the components) the prediction is error-free. In multiple regression, a set of variables predict one extraneous variable and so there is some prediction error. In CCA, the situation is similar to that in regression, but (1) the extraneous variables are multiple, forming a set of their own; (2) the two sets predict each other simultaneously (hence correlation rather than regression); (3) what they predict in each other is rather an extract, a latent variable, than the observed predictand of a regression (see also).Let's involve the second set of variables $Y_1$ and $Y_2$ to correlate canonically with our $X$'s set. We have spaces - here, planes - X and Y. It should be notified that in order the situation to be nontrivial - like that was above with regression where $Y$ stands out of plane X - planes X and Y must intersect only in one point, the origin. Unfortunately it is impossible to draw on paper because 4D presentation is necessary. Anyway, the grey arrow indicates that the two origins are one point and the only one shared by the two planes. If that is taken, the rest of the picture resembles what was with regression. $V_x$ and $V_y$ are the pair of canonical variates. Each canonical variate is the linear combination of the respective variables, like $Y'$ was. $Y'$ was the orthogonal projection of $Y$ onto plane X. Here $V_x$ is a projection of $V_y$ on plane X and simultaneously $V_y$ is a projection of $V_x$ on plane Y, but they are not orthogonal projections. Instead, they are found (extracted) so as to minimize the angle $\phi$ between them. Cosine of that angle is the canonical correlation. Since projections need not be orthogonal, lengths (hence variances) of the canonical variates are not automatically determined by the fitting algorithm and are subject to conventions/constraints which may differ in different implementations. The number of pairs of canonical variates (and hence the number of canonical correlations) is min(number of $X$s, number of $Y$s). And here comes the time when CCA resembles PCA. In PCA, you skim mutually orthogonal principal components (as if) recursively until all the multivariate variability is exhausted. Similarly, in CCA mutually orthogonal pairs of maximally correlated variates are extracted until all the multivariate variability that can be predicted in the lesser space (lesser set) is up. In our example with $X_1$ $X_2$ vs $Y_1$ $Y_2$ there remains the second and weaker correlated canonical pair $V_{x(2)}$ (orthogonal to $V_x$) and $V_{y(2)}$ (orthogonal to $V_y$).For the difference between CCA and PCA+regression see also Doing CCA vs. building a dependent variable with PCA and then doing regression.What is the benefit of canonical correlation over individual Pearson correlations of pairs of variables from the two sets? (my answer's in comments)."
What is global max pooling layer and what is its advantage over maxpooling layer?,"
Can somebody explain what is a global max pooling layer and why and when do we use it for training a neural network. Do they have any advantage over ordinary max pooling layer?
","['neural-networks', 'conv-neural-network', 'pooling']","Global max pooling  =  ordinary max pooling layer with pool size equals to the size of the input (minus filter size + 1, to be precise). You can see that MaxPooling1D takes a pool_length argument, whereas GlobalMaxPooling1D does not.For example, if the input of the max pooling layer  is $0,1,2,2,5,1,2$, global max pooling outputs $5$, whereas  ordinary max pooling layer with pool size equals to 3 outputs $2,2,5,5,5$ (assuming stride=1).This can be seen in the code:In some domains, such as natural language processing, it is common to use global max pooling. In some other domains, such as computer vision, it is common to use a max pooling that isn't global."
Help me understand Support Vector Machines,"
I understand the basics of what a Support Vector Machines' aim is in terms of classifying an input set into several different classes, but what I don't understand is some of the nitty-gritty details. For starters, I'm a bit confused by the use of Slack Variables. What is their purpose?
I'm doing a classification problem where I've captured pressure readings from sensors I've placed on the insole of a shoe. A subject will sit, stand, and walk for a couple of minutes while pressure data is recorded. I want to train a classifier to be able to determine whether a person is sitting, standing or walking and be able to do that for any future test data. What classifier type do I need to try? What is the best way for me to train a classifier from the data I've captured? I have 1000 entries for sitting, standing and walking (3x1000=3000 total), and they all have the following feature vector form. (pressurefromsensor1, pressurefromsensor2, pressurefromsensor3, pressurefromsensor4)
","['machine-learning', 'classification', 'svm']","I think you are trying to start from a bad end. What one should know about SVM to use it is just that this algorithm is finding a hyperplane in hyperspace of attributes that separates two classes best, where best means with biggest margin between classes (the knowledge how it is done is your enemy here, because it blurs the overall picture), as illustrated by a famous picture like this:
Now, there are some problems left.
First of all, what to with those nasty outliers laying shamelessly in a center of cloud of points of a different class?

To this end we allow the optimizer to leave certain samples mislabelled, yet punish each of such examples. To avoid multiobjective opimization, penalties for mislabelled cases are merged with margin size with an use of additional parameter C which controls the balance among those aims.
Next, sometimes the problem is just not linear and no good hyperplane can be found. Here, we introduce kernel trick -- we just project the original, nonlinear space to a higher dimensional one with some nonlinear transformation, of course defined by a bunch of additional parameters, hoping that in the resulting space the problem will be suitable for  a plain SVM:Yet again, with some math and we can see that this whole transformation procedure can be elegantly hidden by modifying objective function by replacing dot product of objects with so-called kernel function.
Finally, this all works for 2 classes, and you have 3; what to do with it? Here we create 3 2-class classifiers (sitting -- no sitting, standing -- no standing, walking -- no walking) and in classification combine those with voting. Ok, so problems seems solved, but we have to select kernel (here we consult with our intuition and pick RBF) and fit at least few parameters (C+kernel). And we must have overfit-safe objective function for it, for instance error approximation from cross-validation. So we leave computer working on that, go for a coffee, come back and see that there are some optimal parameters. Great! Now we just start nested cross-validation to have error approximation and voila. This brief workflow is of course too simplified to be fully correct, but shows reasons why I think you should first try with random forest, which is almost parameter-independent, natively multiclass, provides unbiased error estimate and perform almost as good as well fitted SVMs."
What is wrong with extrapolation?,"
I remember sitting in stats courses as an undergrad hearing about why extrapolation was a bad idea. Furthermore, there are a variety of sources online which comment on this. There's also a mention of it here.
Can anyone help me understand why extrapolation is a bad idea?
If it is, how is it that forecasting techniques aren't statistically invalid?
","['regression', 'time-series', 'forecasting', 'extrapolation']",
Examples for teaching: Correlation does not mean causation,"
There is an old saying: ""Correlation does not mean causation"". When I teach, I tend to use the following standard examples to illustrate this point:

number of storks and birth rate in Denmark;
number of priests in America and alcoholism;
in the start of the 20th century it was noted that there was a strong correlation between 'Number of radios' and 'Number of people in Insane Asylums'
and my favorite: pirates cause global warming.

However, I do not have any references for these examples and whilst amusing, they are obviously false.
Does anyone have any other good examples?
","['correlation', 'teaching']",
What is the advantages of Wasserstein metric compared to Kullback-Leibler divergence?,"
What is the practical difference between Wasserstein metric and Kullback-Leibler divergence? Wasserstein metric is also referred to as Earth mover's distance.
From Wikipedia:

Wasserstein (or Vaserstein) metric is a distance function defined between probability distributions on a given metric space M.

and

Kullback–Leibler divergence is a measure of how one probability distribution diverges from a second expected probability distribution.

I've seen KL been used in machine learning implementations, but I recently came across the Wasserstein metric. Is there a good guideline on when to use one or the other?
(I have insufficient reputation to create a new tag with Wasserstein or Earth mover's distance.)
","['distributions', 'kullback-leibler', 'metric', 'wasserstein']","When considering the advantages of Wasserstein metric compared to KL divergence, then the most obvious one is that W is a metric whereas KL divergence is not, since KL is not symmetric (i.e. $D_{KL}(P||Q) \neq D_{KL}(Q||P)$ in general) and does not satisfy the triangle inequality (i.e. $D_{KL}(R||P) \leq D_{KL}(Q||P) + D_{KL}(R||Q)$ does not hold in general).As what comes to practical difference, then one of the most important is that unlike KL (and many other measures) Wasserstein takes into account the metric space and what this means in less abstract terms is perhaps best explained by an example (feel free to skip to the figure, code just for producing it):
Here the measures between red and blue distributions are the same for KL divergence whereas Wasserstein distance measures the work required to transport the probability mass from the red state to the blue state using x-axis as a “road”. This measure is obviously the larger the further away the probability mass is (hence the alias earth mover's distance). So which one you want to use depends on your application area and what you want to measure. As a note, instead of KL divergence there are also other options like Jensen-Shannon distance that are proper metrics."
XKCD's modified Bayes theorem: actually kinda reasonable?,"
I know this is from a comic famous for taking advantage of certain analytical tendencies, but it actually looks kind of reasonable after a few minutes of staring. Can anyone outline for me what this ""modified Bayes theorem"" is doing?

","['bayesian', 'hierarchical-bayesian']","Well by distributing the $P(H)$ term, we obtain
$$
P(H|X) = \frac{P(X|H)P(H)}{P(X)} P(C) + P(H) [1 - P(C)],
$$
which we can interpret as the Law of Total Probability applied to the event $C =$ ""you are using Bayesian statistics correctly."" So if you are using Bayesian statistics correctly, then you recover Bayes' law (the left fraction above) and if you aren't, then you ignore the data and just use your prior on $H$.I suppose this is a rejoinder against the criticism that in principle Bayesians can adjust the prior to support whatever conclusion they want, whereas Bayesians would argue that this is not how Bayesian statistics actually works.(And yes, you did successfully nerd-snipe me. I'm neither a mathematician nor a physicist though, so I'm not sure how many points I'm worth.)"
What is translation invariance in computer vision and convolutional neural network?,"
I don't have computer vision background, yet when I read some image processing  and convolutional neural networks related articles and papers, I constantly face the term, translation invariance, or translation invariant.
Or I read alot that the convolution operation provides translation invariance?!! what does this mean?
I myself always translated it to myself as if it means if we change an image in any shape, the actual concept of the image doesn't change.
For example if I rotate an image of a lets say tree, it's again a tree no matter what I do to that picture.
And I myself consider all operations that can happen to an image and transform it in a way (crop it, resize it, gray-scale it,color it etc...) to be this way. I have no idea  if this is true so I would be grateful if anyone could explain this to me . 
","['machine-learning', 'conv-neural-network', 'convolution', 'computer-vision']","You're on the right track.Invariance means that you can recognize an object as an object, even when its appearance varies in some way. This is generally a good thing, because it preserves the object's identity, category, (etc) across changes in the specifics of the visual input, like relative positions of the viewer/camera and the object. The image below contains many views of the same statue. You (and well-trained neural networks) can recognize that the same object appears in every picture, even though the actual pixel values are quite different. Note that translation here has a specific meaning in vision, borrowed from geometry. It does not refer to any type of conversion, unlike say, a translation from French to English or between file formats. Instead, it means that each point/pixel in the image has been moved the same amount in the same direction. Alternately, you can think of the origin as having been shifted an equal amount in the opposite direction. For example, we can generate the 2nd and 3rd images in the first row from the first by moving each pixel 50 or 100 pixels to the right. One approach to translation-invariant object recognition is to take a ""template"" of the object and convolve it with every possible location of the object in the image. If you get a large response at a location, it suggests that an object resembling the template is located at that location. This approach is often called template-matching.Santanu_Pattanayak's answer (here) points out that there is a difference between translation invariance and translation equivariance. Translation invariance means that the system produces exactly the same response, regardless of how its input is shifted. For example, a face-detector might report ""FACE FOUND"" for all three images in the top row. Equivariance means that the system works equally well across positions, but its response shifts with the position of the target. For example, a heat map of ""face-iness"" would have similar bumps at the left, center, and right when it processes the first row of images. This is is sometimes an important distinction, but many people call both phenomena ""invariance"", especially since it is usually trivial to convert an equivariant response into an invariant one--just disregard all the position information). "
What are good RMSE values? [duplicate],"







This question already has answers here:
                                
                            




Is my model any good, based on the diagnostic metric ($R^2$/ AUC/ accuracy/ RMSE etc.) value?

                                (3 answers)
                            

Closed 3 years ago.



Suppose I have some dataset. I perform some regression on it. I have a separate test dataset. I test the regression on this set. Find the RMSE on the test data. How should I conclude that my learning algorithm has done well, I mean what properties of the data I should look at to conclude that the RMSE I have got is good for the data?
","['regression', 'error']",
Choosing a clustering method,"
When using cluster analysis on a data set to group similar cases, one needs to choose among a large number of clustering methods and measures of distance.  Sometimes, one choice might influence the other, but there are many possible combinations of methods.  
Does anyone have any recommendations on how to choose among the various clustering algorithms / methods and distance measures?  How is this related to the nature of the variables (e.g., categorical or numerical) and the clustering problem? Is there an optimal technique?
","['clustering', 'distance-functions', 'methodology']","There is no definitive answer to your question, as even within the same method the choice of the distance to represent individuals (dis)similarity may yield different result, e.g. when using euclidean vs. squared euclidean in hierarchical clustering. As an other example, for binary data, you can choose the Jaccard index as a measure of similarity and proceed with classical hierarchical clustering; but there are alternative approaches, like the Mona (Monothetic Analysis) algorithm which only considers one variable at a time, while other hierarchical approaches (e.g. classical HC, Agnes, Diana) use all variables at each step. The k-means approach has been extended in various way, including partitioning around medoids (PAM) or representative objects rather than centroids (Kaufman and Rousseuw, 1990), or fuzzy clustering (Chung and Lee, 1992). For instance, the main difference between the k-means and PAM is that PAM minimizes a sum of dissimilarities rather than a sum of squared euclidean distances; fuzzy clustering allows to consider ""partial membership"" (we associate to each observation a weight reflecting class membership). And for methods relying on a probabilistic framework, or so-called model-based clustering (or latent profile analysis for the psychometricians), there is a great package: Mclust. So definitively, you need to consider how to define the resemblance of individuals as well as the method for linking individuals together (recursive or iterative clustering, strict or fuzzy class membership, unsupervised or semi-supervised approach, etc.).Usually, to assess cluster stability, it is interesting to compare several algorithm which basically ""share"" some similarity (e.g. k-means and hierarchical clustering, because euclidean distance work for both). For assessing the concordance between two cluster solutions, some pointers were suggested in response to this question, Where to cut a dendrogram? (see also the cross-references for other link on this website). If you are using R, you will see that several packages are already available in Task View on Cluster Analysis, and several packages include vignettes that explain specific methods or provide case studies. Cluster Analysis: Basic Concepts and Algorithms provides a good overview of several techniques used in Cluster Analysis.
As for a good recent book with R illustrations, I would recommend chapter 12 of Izenman, Modern Multivariate Statistical Techniques (Springer, 2008). A couple of other standard references is given below:"
Reduce Classification Probability Threshold,"
I have a question regarding classification in general. Let $f$ be a classifier, which outputs a set of probabilities given some data D. Normally, one would say: well, if $P(c|D) > 0.5$, we will assign a class 1, otherwise 0 (let this be a binary classification). 
My question is, what if I find out, that if I classify the class as 1 also when the probabilities are larger than, for instance 0.2, and the classifier performs better. Is it legitimate to then use this new threshold when doing classification?
I would interpret the necessity for lower classification bound in the context of the data emitting a smaller signal; yet still significant for the classification problem.
I realize this is one way to do it. However, if this is not correct thinking of reducing the threshold, what would be some data transformations, which emphasize individual features in a similar manner, so that the threshold can remain at 0.5?
","['machine-learning', 'classification', 'binary-data', 'threshold']","Frank Harrell has written about this on his blog: Classification vs. Prediction, which I agree with wholeheartedly.Essentially, his argument is that the statistical component of your exercise ends when you output a probability for each class of your new sample. Choosing a threshold beyond which you classify a new observation as 1 vs. 0 is not part of the statistics any more. It is part of the decision component. And here, you need the probabilistic output of your model - but also considerations like:So, to answer your question: talk to the end consumer of your classification, and get answers to the questions above. Or explain your probabilistic output to her or him, and let her or him walk through the next steps.Here is another way of looking at this. You ask:what if I find out, that if I classify the class as 1 also when the probabilities are larger than, for instance 0.2, and the classifier performs better.They key word in this question is ""better"". What does it mean that your classifier performs ""better""? This of course depends on your evaluation metric, and depending on your metric, a ""better"" performing classifier may look very different. In a numerical prediction framework, I have written a short paper on this (Kolassa, 2020), but the exact same thing happens for classification.Importantly, this is the case even if we have perfect probabilistic classifications. That is, they are calibrated: if an instance is predicted to have a probability $\hat{p}$ to belong to the target class, then that is indeed its true probability to be of that class.As an illustration, suppose you have applied your probabilistic classifier to a new set of instances. Some of them have a high predicted probability to belong to the target class, more not. Perhaps the distribution of these predicted probabilities looks like this:Now suppose you need to make hard 0-1 classifications. For that, you need to decide on a threshold such that you will classify each instance into the target class if its predicted probability exceeds that threshold. What is the optimal threshold to use?Based on my paragraph above, it should not come as a surprise that this optimal threshold (where the classifier performs ""best"") depends on the evaluation measure. In this case, we can simulate: we draw $10^7$ samples for the predicted probability as above, then for each sample $\hat{p}$ assign it to the target class with probability $\hat{p}$, as the ground truth. In parallel, we can compare the probabilities to all possible thresholds $0\leq t\leq 1$ and evaluate common error measures for such thresholded hard classifications:These plots are unsurprising. Using a threshold of $t=0$ (assigning everything to the target class) yields a perfect recall of $1$. Precision is undefined for high thresholds where there are no instances whose predicted probabilities exceed that threshold, and it is unstable just below that high threshold, depending on whether the highest-scoring instances are in the target class or not. Finally, since we have an unbalanced dataset with more negatives than positives, assigning everything to the non-target class (i.e., using a threshold of $t=1$) maximizes accuracy.So, these three measures elicit classifications that are probably not very useful. In practice, people often use combinations of precision and recall. One very common such combination is the F1 score, which will indeed elicit an ""optimal"" threshold that is not $0$ or $1$, but in between. Sounds better, right?However, note that this again depends on the particular weight between precision and recall we want. The F1 score uses equal weighting, but it is just one member of an entire family of evaluation metrics parameterized by the relative weights of precision and recall. And, again unsurprisingly, the ""optimal"" threshold depends on which F$\beta$ score we use, i.e., on which weight we use, and we are back to square one: in order to find the ""optimal"" classifier, we need to tailor our evaluation metric to the business problem at hand.R code:"
What is the difference between ZCA whitening and PCA whitening?,"
I am confused about ZCA whitening and normal whitening (which is obtained by dividing principal components by the square roots of PCA eigenvalues). As far as I know,
$$\mathbf x_\mathrm{ZCAwhite} = \mathbf U \mathbf x_\mathrm{PCAwhite},$$ where $\mathbf U$ are PCA eigenvectors.
What are the uses of ZCA whitening? What are the differences between normal whitening and ZCA whitening?
","['pca', 'dimensionality-reduction', 'image-processing']","Let your (centered) data be stored in a $n\times d$ matrix $\mathbf X$ with $d$ features (variables) in columns and $n$ data points in rows. Let the covariance matrix $\mathbf C=\mathbf X^\top \mathbf X/n$ have eigenvectors in columns of $\mathbf E$ and eigenvalues on the diagonal of $\mathbf D$, so that $\mathbf C = \mathbf E \mathbf D \mathbf E^\top$.Then what you call ""normal"" PCA whitening transformation is given by $\mathbf W_\mathrm{PCA} = \mathbf D^{-1/2} \mathbf E^\top$, see e.g. my answer in How to whiten the data using principal component analysis?However, this whitening transformation is not unique. Indeed, whitened data will stay whitened after any rotation, which means that any $\mathbf W = \mathbf R \mathbf W_\mathrm{PCA}$ with orthogonal matrix $\mathbf R$ will also be a whitening transformation. In what is called ZCA whitening, we take $\mathbf E$ (stacked together eigenvectors of the covariance matrix) as this orthogonal matrix, i.e. $$\mathbf W_\mathrm{ZCA} = \mathbf E \mathbf D^{-1/2} \mathbf E^\top = \mathbf C^{-1/2}.$$One defining property of ZCA transformation (sometimes also called ""Mahalanobis transformation"") is that it results in whitened data that is as close as possible to the original data (in the least squares sense). In other words, if you want to minimize $\|\mathbf X -  \mathbf X \mathbf A^\top\|^2$ subject to $ \mathbf X \mathbf A^\top$ being whitened, then you should take $\mathbf A = \mathbf W_\mathrm{ZCA}$. Here is a 2D illustration:Left subplot shows the data and its principal axes. Note the dark shading in the upper-right corner of the distribution: it marks its orientation. Rows of $\mathbf W_\mathrm{PCA}$ are shown on the second subplot: these are the vectors the data is projected on. After whitening (below) the distribution looks round, but notice that it also looks rotated --- dark corner is now on the East side, not on the North-East side. Rows of $\mathbf W_\mathrm{ZCA}$ are shown on the third subplot (note that they are not orthogonal!). After whitening (below) the distribution looks round and it's oriented in the same way as originally. Of course, one can get from PCA whitened data to ZCA whitened data by rotating with $\mathbf E$.The term ""ZCA"" seems to have been introduced in Bell and Sejnowski 1996 in the context of independent component analysis, and stands for ""zero-phase component analysis"". See there for more details. Most probably, you came across this term in the context of image processing. It turns out, that when applied to a bunch of natural images (pixels as features, each image as a data point), principal axes look like Fourier components of increasing frequencies, see first column of their Figure 1 below. So they are very ""global"". On the other hand, rows of ZCA transformation look very ""local"", see the second column. This is precisely because ZCA tries to transform the data as little as possible, and so each row should better be close to one the original basis functions (which would be images with only one active pixel). And this is possible to achieve, because correlations in natural images are mostly very local (so de-correlation filters can also be local).UpdateMore examples of ZCA filters and of images transformed with ZCA are given in Krizhevsky, 2009, Learning Multiple Layers of Features from Tiny Images, see also examples in @bayerj's answer (+1).I think these examples give an idea as to when ZCA whitening might be preferable to the PCA one. Namely, ZCA-whitened images still resemble normal images, whereas PCA-whitened ones look nothing like normal images. This is probably important for algorithms like convolutional neural networks (as e.g. used in Krizhevsky's paper), which treat neighbouring pixels together and so greatly rely on the local properties of natural images. For most other machine learning algorithms it should be absolutely irrelevant whether the data is whitened with PCA or ZCA."
Using principal component analysis (PCA) for feature selection,"
I'm new to feature selection and I was wondering how you would use PCA to perform feature selection. Does PCA compute a relative score for each input variable that you can use to filter out noninformative input variables? Basically, I want to be able to order the original features in the data by variance or amount of information contained. 
","['pca', 'feature-selection']","The basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings). You may recall that PCA seeks to replace $p$ (more or less correlated) variables by $k<p$ uncorrelated linear combinations (projections) of the original variables. Let us ignore how to choose an optimal $k$ for the problem at hand. Those $k$ principal components are ranked by importance through their explained variance, and each variable contributes with varying degree to each component. Using the largest variance criteria would be akin to feature extraction, where principal component are used as new features, instead of the original variables. However, we can decide to keep only the first component and select the $j<p$ variables that have the highest absolute coefficient; the number $j$ might be based on the proportion of the number of variables (e.g., keep only the top 10% of the $p$ variables), or a fixed cutoff (e.g., considering a threshold on the normalized coefficients). This approach bears some resemblance with the Lasso operator in penalized regression (or PLS regression). Neither the value of $j$, nor the number of components to retain are obvious choices, though.The problem with using PCA is that (1) measurements from all of the original variables are used in the projection to the lower dimensional space, (2) only linear relationships are considered, and (3) PCA or SVD-based methods, as well as univariate screening methods (t-test, correlation, etc.), do not take into account the potential multivariate nature of the data structure (e.g., higher order interaction between variables).About point 1, some more elaborate screening methods have been proposed, for example principal feature analysis or stepwise method, like the one used for 'gene shaving' in gene expression studies. Also, sparse PCA might be used to perform dimension reduction and variable selection based on the resulting variable loadings. About point 2, it is possible to use kernel PCA (using the kernel trick) if one needs to embed nonlinear relationships into a lower dimensional space. Decision trees, or better the random forest algorithm, are probably better able to solve Point 3. The latter allows to derive Gini- or permutation-based measures of variable importance.A last point: If you intend to perform feature selection before applying a classification or regression model, be sure to cross-validate the whole process (see §7.10.2 of the Elements of Statistical Learning, or Ambroise and McLachlan, 2002).As you seem to be interested in R solution, I would recommend taking a look at the caret package which includes a lot of handy functions for data preprocessing and variable selection in a classification or regression context."
Why is it that natural log changes are percentage changes? What is about logs that makes this so?,"
Can somebody explain how the properties of logs make it so you can do log linear regressions where the coefficients are interpreted as percentage changes?
","['regression', 'logarithm', 'mathematical-statistics']",
What is regularization in plain english?,"
Unlike other articles, I found the wikipedia entry for this subject unreadable for a non-math person (like me).
I understood the basic idea, that you favor models with fewer rules. What I don't get is how do you get from a set of rules to a 'regularization score' which you can use to sort the models from least to most overfit.
Can you describe a simple regularization method?
I'm interested in the context of analyzing statistical trading systems. It would be great if you could describe if/how I can apply regularization to analyze the following two predictive models:
Model 1 - price going up when:

exp_moving_avg(price, period=50) > exp_moving_avg(price, period=200)

Model 2 - price going up when:

price[n] < price[n-1] 10 times in a row
exp_moving_avg(price, period=200) going up

But I'm more interested in getting a feeling for how you do regularization. So if you know better models for explaining it please do.
",['regularization'],"In simple terms, regularization is tuning or selecting the preferred level of model complexity so your models are better at predicting (generalizing). If you don't do this your models may be too complex and overfit or too simple and underfit, either way giving poor predictions.If you least-squares fit a complex model to a small set of training data you will probably overfit, this is the most common situation. The optimal complexity of the model depends on the sort of process you are modeling and the quality of the data, so there is no a-priori correct complexity of a model.To regularize you need 2 things:Note that the optimized regularization error will not be an accurate estimate of the overall prediction error so after regularization you will finally have to use an additional validation dataset or perform some additional statistical analysis to get an unbiased prediction error.An alternative to using (cross-)validation testing is to use Bayesian Priors or other methods to penalize complexity or non-smoothness, but these require more statistical sophistication and knowledge of the problem and model features."
What is an intuitive explanation for how PCA turns from a geometric problem (with distances) to a linear algebra problem (with eigenvectors)?,"
I've read a lot about PCA, including various tutorials and questions (such as this one, this one, this one, and this one). 
The geometric problem that PCA is trying to optimize is clear to me: PCA tries to find the first principal component by minimizing the reconstruction (projection) error, which simultaneously maximizes the variance of the projected data.

When I first read that, I immediately thought of something like linear regression; maybe you can solve it using gradient descent if needed.
However, then my mind was blown when I read that the optimization problem is solved by using linear algebra and finding eigenvectors and eigenvalues. I simply do not understand how this use of linear algebra comes into play.
So my question is: How can PCA turn from a geometric optimization problem to a linear algebra problem? Can someone provide an intuitive explanation?
I am not looking for an answer like this one that says ""When you solve the mathematical problem of PCA, it ends up being equivalent to finding the eigenvalues and eigenvectors of the covariance matrix."" Please explain why eigenvectors come out to be the principal components and why the eigenvalues come out to be variance of the data projected onto them
I am a software engineer and not a mathematician, by the way.
Note: the figure above was taken and modified from this PCA tutorial.
","['pca', 'optimization', 'linear-algebra', 'intuition']","The geometric problem that PCA is trying to optimize is clear to me: PCA tries to find the first principal component by minimizing the reconstruction (projection) error, which simultaneously maximizes the variance of the projected data.That's right. I explain the connection between these two formulations in my answer here (without math) or here (with math).Let's take the second formulation: PCA is trying the find the direction such that the projection of the data on it has the highest possible variance. This direction is, by definition, called the first principal direction. We can formalize it as follows: given the covariance matrix $\mathbf C$, we are looking for a vector $\mathbf w$ having unit length, $\|\mathbf w\|=1$, such that $\mathbf w^\top \mathbf{Cw}$ is maximal.(Just in case this is not clear: if $\mathbf X$ is the centered data matrix, then the projection is given by $\mathbf{Xw}$ and its variance is $\frac{1}{n-1}(\mathbf{Xw})^\top \cdot \mathbf{Xw} = \mathbf w^\top\cdot (\frac{1}{n-1}\mathbf X^\top\mathbf X)\cdot \mathbf w = \mathbf w^\top \mathbf{Cw}$.)On the other hand, an eigenvector of $\mathbf C$ is, by definition, any vector $\mathbf v$ such that $\mathbf{Cv}=\lambda \mathbf v$.It turns out that the first principal direction is given by the eigenvector with the largest eigenvalue. This is a nontrivial and  surprising statement.If one opens any book or tutorial on PCA, one can find there the following almost one-line proof of the statement above. We want to maximize $\mathbf w^\top \mathbf{Cw}$ under the constraint that $\|\mathbf w\|=\mathbf w^\top \mathbf w=1$; this can be done introducing a Lagrange multiplier and maximizing $\mathbf w^\top \mathbf{Cw}-\lambda(\mathbf w^\top \mathbf w-1)$; differentiating, we obtain $\mathbf{Cw}-\lambda\mathbf w=0$, which is the eigenvector equation. We see that $\lambda$ has in fact to be the largest eigenvalue by substituting this solution into the objective function, which gives $\mathbf w^\top \mathbf{Cw}-\lambda(\mathbf w^\top \mathbf w-1) = \mathbf w^\top \mathbf{Cw} = \lambda\mathbf w^\top \mathbf{w} = \lambda$. By virtue of the fact that this objective function should be maximized, $\lambda$ must be the largest eigenvalue, QED.This tends to be not very intuitive for most people.A better proof (see e.g. this neat answer by @cardinal) says that because $\mathbf C$ is symmetric matrix, it is diagonal in its eigenvector basis. (This is actually called spectral theorem.) So we can choose an orthogonal basis, namely the one given by the eigenvectors, where $\mathbf C$ is diagonal and has eigenvalues $\lambda_i$ on the diagonal. In that basis, $\mathbf w^\top \mathbf{C w}$ simplifies to $\sum \lambda_i w_i^2$, or in other words the variance is given by the weighted sum of the eigenvalues. It is almost immediate that to maximize this expression one should simply take $\mathbf w = (1,0,0,\ldots, 0)$, i.e. the first eigenvector, yielding variance $\lambda_1$ (indeed, deviating from this solution and ""trading"" parts of the largest eigenvalue for the parts of smaller ones will only lead to smaller overall variance). Note that the value of $\mathbf w^\top \mathbf{C w}$ does not depend on the basis! Changing to the eigenvector basis amounts to a rotation, so in 2D one can imagine simply rotating a piece of paper with the scatterplot; obviously this cannot change any variances.I think this is a very intuitive and a very useful argument, but it relies on the spectral theorem. So the real issue here I think is: what is the intuition behind the spectral theorem?Take a symmetric matrix $\mathbf C$. Take its eigenvector $\mathbf w_1$ with the largest eigenvalue $\lambda_1$. Make this eigenvector the first basis vector and choose other basis vectors randomly (such that all of them are orthonormal). How will $\mathbf C$ look in this basis?It will have $\lambda_1$ in the top-left corner, because $\mathbf w_1=(1,0,0\ldots 0)$ in this basis and  $\mathbf {Cw}_1=(C_{11}, C_{21}, \ldots C_{p1})$ has to be equal to $\lambda_1\mathbf w_1 =  (\lambda_1,0,0 \ldots 0)$.By the same argument it will have zeros in the first column under the $\lambda_1$.But because it is symmetric, it will have zeros in the first row after $\lambda_1$ as well. So it will look like that:$$\mathbf C=\begin{pmatrix}\lambda_1 & 0 & \ldots & 0 \\ 0 &  &  & \\ \vdots & & & \\ 0 & & & \end{pmatrix},$$where empty space means that there is a block of some elements there. Because the matrix is symmetric, this block will be symmetric too. So we can apply exactly the same argument to it, effectively using the second eigenvector as the second basis vector, and getting $\lambda_1$ and $\lambda_2$ on the diagonal. This can continue until $\mathbf C$ is diagonal. That is essentially the spectral theorem. (Note how it works only because $\mathbf C$ is symmetric.)Here is a more abstract reformulation of exactly the same argument.We know that $\mathbf{Cw}_1 = \lambda_1 \mathbf w_1$, so the first eigenvector defines a 1-dimensional subspace where $\mathbf C$ acts as a scalar multiplication. Let us now take any vector $\mathbf v$ orthogonal to $\mathbf w_1$. Then it is almost immediate that $\mathbf {Cv}$ is also orthogonal to $\mathbf w_1$. Indeed:$$ \mathbf w_1^\top \mathbf{Cv} = (\mathbf w_1^\top \mathbf{Cv})^\top = \mathbf v^\top \mathbf C^\top \mathbf w_1 = \mathbf v^\top \mathbf {Cw}_1=\lambda_1 \mathbf v^\top \mathbf w_1 = \lambda_1\cdot 0 = 0.$$This means that $\mathbf C$ acts on the whole remaining subspace orthogonal to $\mathbf w_1$ such that it stays separate from $\mathbf w_1$. This is the crucial property of symmetric matrices. So we can find the largest eigenvector there, $\mathbf w_2$, and proceed in the same manner, eventually constructing an orthonormal basis of eigenvectors."
What is the difference between R functions prcomp and princomp?,"
I compared ?prcomp and ?princomp and found something about Q-mode and R-mode principal component analysis (PCA). But honestly – I don't understand it. Can anybody explain the difference and maybe even explain when to apply which?
","['r', 'pca']",
Variable selection for predictive modeling really needed in 2016?,"
This question has been asked on CV some yrs ago, it seems worth a repost in light of 1) order of magnitude better computing technology (e.g. parallel computing, HPC etc) and 2) newer techniques, e.g. [3].
First, some context. Let's assume the goal is not hypothesis testing, not effect estimation, but prediction on un-seen test set. So, no weight is given to any interpretable benefit. Second, let's say you cannot rule out the relevance of any predictor on subject matter consideration, ie. they all seem plausible individually or in combination with other predictors. Third, you're confront with (hundreds of) millions of predictors. Fourth, let's say you have access to AWS with an unlimited budget, so computing power is not a constraint.  
The usual reaons for variable selection are 1) efficiency; faster to fit a smaller model and cheaper to collect fewer predictors, 2) interpretation; knowing the ""important"" variables gives insight into the underlying process [1].
It's now widely known that many variable selection methods are ineffective and often outright dangerous (e.g. forward stepwise regression) [2].
Secondly, if the selected model is any good, one shouldn't need to cut down on the list of predictors at all. The model should do it for you. A good example is lasso, which assigns a zero coefficient to all the irrelevant variables. 
I'm aware that some people advocate using an ""elephant"" model, ie. toss every conceivable predictors into the fit and run with it [2].
Is there any fundamental reason to do variable selection if the goal is predictive accuracy?
[1] Reunanen, J. (2003). Overfitting in making comparisons between variable selection methods. The Journal of Machine Learning Research, 3, 1371-1382.
[2] Harrell, F. (2015). Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis. Springer.
[3] Taylor, J., & Tibshirani, R. J. (2015). Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25), 7629-7634.
[4] Zhou, J., Foster, D., Stine, R., & Ungar, L. (2005, August). Streaming feature selection using alpha-investing. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining (pp. 384-393). ACM.
","['machine-learning', 'modeling', 'feature-selection', 'model-selection', 'prediction']",
How to interpret coefficients in a Poisson regression?,"
How can I interpret the main effects (coefficients for dummy-coded factor) in a Poisson regression?
Assume the following example:
treatment     <- factor(rep(c(1, 2), c(43, 41)), 
                        levels = c(1, 2),
                        labels = c(""placebo"", ""treated""))
improved      <- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                        levels = c(1, 2, 3),
                        labels = c(""none"", ""some"", ""marked""))    
numberofdrugs <- rpois(84, 10) + 1    
healthvalue   <- rpois(84, 5)   
y             <- data.frame(healthvalue, numberofdrugs, treatment, improved)
test          <- glm(healthvalue~numberofdrugs+treatment+improved, y, family=poisson)
summary(test)

The output is:
Coefficients:
                 Estimate Std. Error z value Pr(>|z|)    
(Intercept)       1.88955    0.19243   9.819   <2e-16 ***
numberofdrugs    -0.02303    0.01624  -1.418    0.156    
treatmenttreated -0.01271    0.10861  -0.117    0.907   MAIN EFFECT  
improvedsome     -0.13541    0.14674  -0.923    0.356   MAIN EFFECT 
improvedmarke    -0.10839    0.12212  -0.888    0.375   MAIN EFFECT 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

I know that the incident rate for numberofdrugs is exp(-0.023)=0.977. But how do I interpret the main effects for the dummy variables? 
","['r', 'generalized-linear-model', 'interpretation', 'poisson-distribution', 'regression-coefficients']",
Skills hard to find in machine learners?,"
It seems that data mining and machine learning became so popular that now almost every CS student knows about classifiers, clustering, statistical NLP ... etc. So it seems that finding data miners is not a hard thing nowadays. 
My question is: 
What are the skills that a data miner could learn that would make him different than the others? To make him a not-so-easy-to-find-someone-like-him kind of person.
","['machine-learning', 'data-mining']",
Famous statistical wins and horror stories for teaching purposes,"
I am designing a one year program in data analysis with a local community college. The program aims to prepare students to handle basic tasks in data analysis, visualization and summarization, advanced Excel skills and R programming. 
I would like to prepare a set of short, real world examples that illustrate where ordinary intuition fails and statistical analysis is necessary. I'm also interested in ""famous statistical fails"", but more interested in the wins. The data involved should be freely available.
A perfect example of what I'm looking for is the Berkeley discrimination case, which illustrates Simpson's paradox. The data for that is memorialized in R's datasets. 
Historical cases are also interesting. John Snow's analysis of the Broad Street pump data is a good example of the power of visualization.
There are a lot of fails in the collection of data (selection bias), etc. and the literature in medical statistics is full of them. 
A lot of ""statistical wins"" occur in the area of variable selection and sampling design. I'm interested in paradoxes that occur in other areas -- like the analysis as such. 
","['mathematical-statistics', 'data-visualization', 'experiment-design', 'teaching']","Described here. Digits do not appear with uniform frequency in front of numbers, but rather follow a specific pattern: digit 1 is the most likely to be the first digit, with 30% chance, followed by 2 (17.6% chance), and so on. The following picture (from Wikipedia) shows the frequency of each digit at the beginning of each number, in some naturally-occurring datasets:There are certain conditions under which the law holds (e.g., the data should span across several scales, so stuff like peoples' heights are not eligible), but it is quite generic. Perhaps the most surprising application is in fraud detection. This is based on the assumption that people who try to fabricate figures tend to distribute the digits uniformly, thus violating Benford's law. I recall once I was explaining this to a class, and in the break one of the students came up with an accounting spreadsheet from his company, in which he had tried to validate my claims. It worked :)Described here: The frequency of a word in a corpus is inversely proportional to its rank. What is surprising is that this relationship holds for any corpus, even for ancient languages that have not yet been translated. An interesting video explaining more about why this pattern may hold is here. The following picture shows rank (horizontal) vs frequency (vertical) in a log-log scale for the first 10 million words in 30 Wikipedias (source). Note that the law would predict a straight line:
These two laws are powerful and counter-intuitive, and in the sense that they enhance one's understanding of the world via statistics, they could be called ""statistical wins"". "
What are i.i.d. random variables?,"
How would you go about explaining i.i.d (independent and identically distributed) to non-technical people?
","['random-variable', 'intuition', 'iid']","It means ""Independent and identically distributed"".A good example is a succession of throws of a fair coin: The coin has no memory, so all the throws are ""independent"".And every throw is 50:50 (heads:tails), so the coin is and stays fair - the distribution from which every throw is drawn, so to speak, is and stays the same: ""identically distributed"".A good starting point would be the Wikipedia page.::EDIT::Follow this link to further explore the concept."
Is it meaningful to calculate Pearson or Spearman correlation between two Boolean vectors?,"
There are two Boolean vectors, which contain 0 and 1 only. If I calculate the Pearson or Spearman correlation, are they meaningful or reasonable?
","['correlation', 'binary-data', 'pearson-r', 'spearman-rho']","The Pearson and Spearman correlation are defined as long as you have some $0$s and some $1$s for both of two binary variables, say $y$ and $x$. It is easy to get a good qualitative idea of what they mean by thinking of a scatter plot of the two variables. Clearly, there are only four possibilities $(0,0), (0,1), (1, 0), (1,1)$ (so that jittering to shake identical points apart for visualization is a good idea). For example, in any situation where the two vectors are identical, subject to having some 0s and some 1s in each, then by definition $y = x$ and the correlation is necessarily $1$. Similarly, it is possible that $y = 1 -x$ and then the correlation is $-1$. For this set-up, there is no scope for monotonic relations that are not linear. When taking ranks of $0$s and $1$s under the usual midrank convention the ranks are just a linear transformation of the original $0$s and $1$s and the Spearman correlation is necessarily identical to the Pearson correlation. Hence there is no reason to consider Spearman correlation separately here, or indeed at all. Correlations arise naturally for some problems involving $0$s and $1$s, e.g. in the study of binary processes in time or space. On the whole, however, there will be better ways of thinking about such data, depending largely on the main motive for such a study. For example, the fact that correlations make much sense does not mean that linear regression is a good way to model a binary response. If one of the binary variables is a response, then most statistical people would start by considering a logit model. "
Why do neural network researchers care about epochs?,"
An epoch in stochastic gradient descent is defined as a single pass through the data. For each SGD minibatch, $k$ samples are drawn, the gradient computed and parameters are updated. In the epoch setting, the samples are drawn without replacement.
But this seems unnecessary. Why not draw each SGD minibatch as $k$ random draws from the whole data set at each iteration? Over a large number of epochs, the small deviations of which samples are seen more or less often would seem to be unimportant.
","['neural-networks', 'gradient-descent']","In addition to Franck's answer about practicalities, and David's answer about looking at small subgroups – both of which are important points – there are in fact some theoretical reasons to prefer sampling without replacement. The reason is perhaps related to David's point (which is essentially the coupon collector's problem).In 2009, Léon Bottou compared the convergence performance on a particular text classification problem ($n = 781,265$).Bottou (2009). Curiously Fast Convergence of some
  Stochastic Gradient Descent Algorithms. Proceedings of the
  symposium on learning and data science. (author's pdf)He trained a support vector machine via SGD with three approaches:He empirically examined the convergence $\mathbb E[ C(\theta_t) - \min_\theta C(\theta) ]$, where $C$ is the cost function, $\theta_t$ the parameters at step $t$ of optimization, and the expectation is over the shuffling of assigned batches.This is his Figure 1 illustrating that:
This was later theoretically confirmed by the paper:Gürbüzbalaban, Ozdaglar, and Parrilo (2015). Why Random Reshuffling Beats Stochastic Gradient Descent. arXiv:1510.08560. (video of invited talk at NIPS 2015)Their proof only applies to the case where the loss function is strongly convex, i.e. not to neural networks. It's reasonable to expect, though, that similar reasoning might apply to the neural network case (which is much harder to analyze)."
How does a simple logistic regression model achieve a 92% classification accuracy on MNIST?,"
Even though all the images in the MNIST dataset are centered, with a similar scale, and face up with no rotations, they have a significant handwriting variation that puzzles me how a linear model achieves such a high classification accuracy.
As far as I am able to visualize, given the significant handwriting variation, the digits should be linearly inseparable in a 784 dimensional space, i.e., there should be a little complex (though not very complex) non-linear boundary that separates the different digits, similar to the well-cited $XOR$ example where positive and negative classes can not be separated by any linear classifier. It seems baffling to me how multi-class logistic regression produces such a high accuracy with entirely linear features (no polynomial features).
As an example, given any pixel in the image, different handwritten variations of the digits $2$ and $3$ can make that pixel illuminated or not. Therefore, with a set of learned weights, each pixel can make a digit look as a $2$ as well as a $3$. Only with a combination of pixel values should it be possible to say whether a digit is a $2$ or a $3$. This is true for most of the digit pairs. So, how is logistic regression, which blindly bases its decision independently on all pixel values (without considering any inter-pixel dependencies at all), able to achieve such high accuracies.
I know that I am wrong somewhere or am just over-estimating the variation in the images. However, it would be great if someone could help me with an intuition on how the digits are 'almost' linearly separable.
","['logistic', 'image-processing']","tl;dr Even though this is an image classification dataset, it remains a very easy task, for which one can easily find a direct mapping from inputs to predictions.Answer:This is a very interesting question and thanks to the simplicity of logistic regression you can actually find out the answer. What logistic regression does is for each image accept $784$ inputs and multiply them with weights to generate its prediction. The interesting thing is that due to the direct mapping between input and output (i.e. no hidden layer), the value of each weight corresponds to how much each one of the $784$ inputs are taken into account when computing the probability of each class. Now, by taking the weights for each class and reshaping them into $28 \times 28$ (i.e. the image resolution), we can tell what pixels are most important for the computation of each class.Note, again, that these are the weights.Now take a look at the above image and focus on the first two digits (i.e. zero and one). Blue weights mean that this pixel's intensity contributes a lot for that class and red values mean that it contributes negatively. Now imagine, how does a person draw a $0$? He draws a circular shape that's empty in between. That's exactly what the weights picked up on. In fact if someone draws the middle of the image, it counts negatively as a zero. So to recognize zeros you don't need some sophisticated filters and high-level features. You can just look at the drawn pixel locations and judge according to this.Same thing for the $1$. It always has a straight vertical line in the middle of the image. All else counts negatively.The rest of the digits are a bit more complicated, but with little imaginations you can see the $2$, the $3$, the $7$ and the $8$. The rest of the numbers are a bit more difficult, which is what actually limits the logistic regression from reaching the high-90s.Through this you can see that logistic regression has a very good chance of getting a lot of images right and that's why it scores so high.The code to reproduce the above figure is a bit dated, but here you go:"
Best way to present a random forest in a publication?,"
I am using the random forest algorithm as a robust classifier of two groups in a microarray study with 1000s of features.  

What is the best way to present the random forest so that there is enough information to make it
reproducible in a paper?
Is there a plot method in R to actually plot the tree, if there are a small number of features?
Is the OOB estimate of error rate the best statistic to quote?

","['r', 'machine-learning', 'classification', 'random-forest', 'microarray']","Regarding making it reproducible, the best way is to provide reproducible research (i.e. code and data) along with the paper.  Make it available on your website, or on a hosting site (like github).Regarding visualization, Leo Breiman has done some interesting work on this (see his homepage, in particular the section on graphics).But if you're using R, then the randomForest package has some useful functions:And I'm not aware of a simple way to actually plot a tree, but you can use the getTree function to retrieve the tree and plot that separately.The Strobl/Zeileis presentation on ""Why and how to use random forest variable importance measures (and how you shouldn’t)"" has examples of trees which must have been produced in this way.  This blog post on tree models has some nice examples of CART tree plots which you can use for example.As @chl commented, a single tree isn't especially meaningful in this context, so short of using it to explain what a random forest is, I wouldn't include this in a paper."
What algorithm should I use to detect anomalies on time-series?,"
Background
I'm working in Network Operations Center, we monitor computer systems and their performance. One of the key metrics to monitor is a number of visitors\customers currently connected to our servers. To make it visible we (Ops team) collect such metrics as time-series data and draw graphs. Graphite allows us to do it, it has a pretty rich API which I use to build alerting system to notify our team if sudden drops (mostly) and other changes occur. For now I've set a static threshold based on avg value but it doesn't work very well (there are a lot of false-positives) due to different load during the day and week (seasonality factor).
It looks something like this:

The actual data (an example for one metric, 15 min time range; the first number is a number of users, the second - time stamp ):
[{""target"": ""metric_name"", ""datapoints"": [[175562.0, 1431803460], [176125.0, 1431803520], [176125.0, 1431803580], [175710.0, 1431803640], [175710.0, 1431803700], [175733.0, 1431803760], [175733.0, 1431803820], [175839.0, 1431803880], [175839.0, 1431803940], [175245.0, 1431804000], [175217.0, 1431804060], [175629.0, 1431804120], [175104.0, 1431804180], [175104.0, 1431804240], [175505.0, 1431804300]]}]

What I'm trying to accomplish
I've created a Python script which receives recent datapoints, compares them with historical average and alerts if there is a sudden change or drop.
Due to seasonality ""static"" threshold doesn't work well and script generates false-positives alerts. I want to improve an alerting algorithm to be more precise and make it work without constant tuning the alerting threshold.
What advise I need and things I discovered
By googling I figured that I'm looking for machine learning algorithms for anomaly detection (unsupervised ones). Further investigation showed that there are tons of them and it's very difficult to understand which one is applicable in my case.
Due to my limited math knowledge I can't read sophisticated scholar papers and I'm looking for something simple to a beginner in the field. 
I like Python and familiar with R a bit, thus I'll be happy to see examples for these languages.
Please recommend a good book or article which will help me to solve my problem.
Thank you for your time and excuse me for such long description
Useful links
Similar questions:

Time series and anomaly detection
Time Series Anomaly Detection with Python
Time series anomalies
Algorithms for Time Series Anomaly Detection
Application of wavelets to time-series-based anomaly detection algorithms
Which algorithm should I use?

External resources:

Machine Learning - Anomaly Detection Playlist on YouTube
Detection Algorithms for Biosurveillance: A tutorial

","['machine-learning', 'time-series', 'python', 'computational-statistics', 'anomaly-detection']",
Why is it possible to get significant F statistic (p<.001) but non-significant regressor t-tests?,"
In a multiple linear regression, why is it possible to have a highly significant F statistic (p<.001) but have very high p-values on all the regressor's t tests?
In my model, there are 10 regressors. One has a p-value of 0.1 and the rest are above 0.9

For dealing with this problem see the follow-up question.
","['regression', 'hypothesis-testing', 't-test', 'multicollinearity', 'faq']","As Rob mentions, this occurs when you have highly correlated variables. The standard example I use is predicting weight from shoe size. You can predict weight equally well with the right or left shoe size. But together it doesn't work out.Brief simulation example"
Understanding ROC curve,"
I'm having trouble understanding the ROC curve.
Is there any advantage / improvement in area under the ROC curve if I build different models from each unique subset of the training set and use it to produce a probability?
For example, if $y$ has values of $\{a, a, a, a, b, b, b, b\}$, and I build model $A$ by using $a$ from 1st-4th values of $y$ and 8th-9th values of $y$ and build model $B$ by using remained train data. Finally, generate probability. Any thoughts / comments will be much appreciated. 
Here is r code for better explanation for my question:
Y    = factor(0,0,0,0,1,1,1,1)
X    = matirx(rnorm(16,8,2))
ind  = c(1,4,8,9)
ind2 = -ind

mod_A    = rpart(Y[ind]~X[ind,])
mod_B    = rpart(Y[-ind]~X[-ind,])
mod_full = rpart(Y~X)

pred = numeric(8)
pred_combine[ind]  = predict(mod_A,type='prob')
pred_combine[-ind] = predict(mod_B,type='prob')
pred_full          = predict(mod_full, type='prob')

So my question is, area under ROC curve of pred_combine vs pred_full.
","['r', 'roc']","I'm not sure I got the question, but since the title asks for explaining ROC curves, I'll try. ROC Curves are used to see how well your classifier can separate positive and negative examples and to identify the best threshold for separating them. To be able to use the ROC curve, your classifier has to be ranking - that is, it should be able to rank examples such that the ones with higher rank are more likely to be positive. For example, Logistic Regression outputs probabilities, which is a score you can use for ranking.Given a data set and a ranking classifier: where $\text{pos}$ and $\text{neg}$ are the fractions of positive and negative examples respectively. This nice gif-animated picture should illustrate this process clearer On this graph, the $y$-axis is true positive rate, and the $x$-axis is false positive rate. 
Note the diagonal line - this is the baseline, that can be obtained with a random classifier. The further our ROC curve is above the line, the better. The area under the ROC Curve (shaded) naturally shows how far the curve from the base line. For the baseline it's 0.5, and for the perfect classifier it's 1. You can read more about AUC ROC in this question: What does AUC stand for and what is it?I'll outline briefly the process of selecting the best threshold, and more details can be found in the reference. To select the best threshold you see each point of your ROC curve as a separate classifier. This mini-classifiers uses the score the point got as a boundary between + and - (i.e. it classifies as + all points above the current one)Depending on the pos/neg fraction in our data set - parallel to the baseline in case of 50%/50% - you build ISO Accuracy Lines and take the one with the best accuracy. Here's a picture that illustrates that and for details I again invite you to the reference "
"Do not vote, one vote will not reverse election results. What is wrong with this reasoning?","

Do not vote, one vote will not reverse the election result. What's
  more, the probability of injury in a traffic collision on the way to the
  ballot box is much higher than your vote reversing the election
  result. What is even more, the probability that you would win grand
  prize of lottery game is higher than that you would reverse election
  result.

What is wrong with this reasoning, if anything? Is it possible to statistically prove that one vote matters?
I know that there are some arguments like ""if everybody thought like that, it would change the election result"". But everybody will not think like that. Even if 20% of electorate copy you, always a great number of people will go, and the margin of victory of winning candidate will be counted in hundreds of thousands. Your vote would count only in case of a tie.
Judging it with game theory gains and costs, it seems that more optimal strategy for Sunday is horse race gambling than going to the ballot box. 
Update, March 3.
I am grateful for providing me with so much material and for keeping the answers related to statistical part of the question. Not attempting to solve the stated problem but rather to share and validate my thinking path I posted an answer. I have formulated there few assumptions. 

two candidates 
unknown number of voters 
each voter can cast a random vote on either candidate

I have showed there a solution for 6 voters (could be a case in choosing a captain on a fishing boat). I would be interested in knowing what are the odds for each additional milion of voters. 
Update, March 5.
I would like to make it clear that I am interested in more or less realistic assumptions to calculating the probability of a decisive vote. More or less because I do not want to sacrifice simplicity for precision. I have just understood that my update of March 3 formulated unrealistic assumptions. These assumptions probably formulate the highest possible probability of a decisive vote but I would be grateful if you could confirm it. 
Yet still unknown for me thing is what is meant by the number of voters in the provided formulas. Is it a maximum pool of voters or exact number of voters. Say we have 1 milion voters, so is the probability calculated for all the cases from 1 to milion voters taking part in election?   
Adding more fuel to the discussion heat
In the USA, because president is elected indirectly, your vote would be decisive if only one vote, your vote, were to reverse the electors of your state, and then, owing to the votes of your electors, there was a tie at Electoral College. Of course, breaking this double tie condition hampers the chances that a single vote may reverse election result, even more than discussed here so far. I have opened a separate thread about that here.
","['probability', 'game-theory', 'elections', 'voting-system']",
How to interpret F-measure values?,"
I would like to know how to interpret a difference of f-measure values. I know that f-measure is a balanced mean between precision and recall, but I am asking about the practical meaning of a difference in F-measures.
For example, if a classifier C1 has an accuracy of 0.4  and another classifier C2 an accuracy of 0.8, then we can say that C2 has correctly classified the double of test examples compared to C1. However, if a classifier C1 has an F-measure of 0.4 for a certain class and another classifier C2 an F-measure of 0.8, what can we state about the difference in performance of the 2 classifiers ? Can we say that C2 has classified X more instances correctly that C1 ?    
","['classification', 'precision-recall']","I cannot think of an intuitive meaning of the F measure, because it's just a combined metric.  What's more intuitive than F-mesure, of course, is precision and recall.But using two values, we often cannot determine if one algorithm is superior to another. For example, if one algorithm has higher precision but lower recall than other, how can you tell which algorithm is better?If you have a specific goal in your mind like 'Precision is the king. I don't care much about recall', then there's no problem. Higher precision is better. But if you don't have such a strong goal, you will want a combined metric. That's F-measure. By using it, you will compare some of precision and some of recall.The ROC curve is often drawn stating the F-measure. You may find this article interesting as it contains explanation on several measures including ROC curves:
http://binf.gmu.edu/mmasso/ROC101.pdf"
Is there a name for the phenomenon of false positives counterintuitively outstripping true positives,"
It seems very counter intuitive to many people that a given diagnostic test with very high accuracy (say 99%) can generate massively more false positives than true positives in some situations, namely where the population of true positives is very small compared to whole population.
I see people making this mistake often e.g. when arguing for wider public health screenings, or wider anti-crime surveillance measures etc but I am at a loss for how to succinctly describe the mistake people are making.
Does this phenomenon / statistical fallacy have a name? Failing that has anyone got a good, terse, jargon free intuition/example that would help me explain it to a lay person.
Apologies if this is the wrong forum to ask this. If so please direct me to a more appropriate one.
","['probability', 'terminology', 'intuition']",Yes there is. Generally it is termed base rate fallacy or more specific false positive paradox. There is even a wikipedia article about it: see here
Optimization when Cost Function Slow to Evaluate,"
Gradient descent and many other methods are useful for finding local minima in cost functions. They can be efficient when the cost function can be evaluated quickly at each point, whether numerically or analytically.  
I have what appears to me to be an unusual situation. Each evaluation of my cost function is expensive. I am attempting to find a set of parameters that minimize a 3D surface against ground truth surfaces. Whenever I change a parameter, I need to run the algorithm against the entire sample cohort to measure its effect.  In order to calculate a gradient, I need to change all 15 parameters independently, meaning I have to regenerate all the surfaces and compare against the sample cohort way too many times per gradient, and definitely way too many times over the course of optimization.
I have developed a method to circumvent this problem and am currently evaluating it, but I am surprised that I have not found much in the literature regarding expensive cost function evaluations.  This makes me wonder if I am making the problem harder than it is and that there might be a better way already available.
So my questions are basically this: Does anyone know of methods for optimizing cost functions, convex or not, when evaluation is slow?  Or, am I doing something silly in the first place by rerunning the algorithm and comparing against the sample cohort so many times?
","['gradient-descent', 'optimization', 'bayesian-optimization']","I recommend using LIPO. It is provably correct and provably better than pure random search (PRS). It is also extremely simple to implement, and has no hyperparameters. I have not conducted an analysis that compares LIPO to BO, but my expectation is that the simplicity and efficiency of LIPO imply that it will out-perform BO.(See also: What are some of the disavantage of bayesian hyper parameter optimization?)This is an exciting arrival which, if it is not new, is certainly new to me. It proceeds by alternating between placing informed bounds on the function, and sampling from the best bound, and using quadratic approximations. I'm still working through all the details, but I think this is very promising. This is a nice blog write-up, and the paper is Cédric Malherbe and Nicolas Vayatis ""Global optimization of Lipschitz functions.""LIPO is most useful when the number of hyper-parameters that you are searching over is small.Bayesian Optimization-type methods build Gaussian process surrogate models to explore the parameter space. The main idea is that parameter tuples that are closer together will have similar function values, so the assumption of a co-variance structure among points allows the algorithm to make educated guesses about what best parameter tuple is most worthwhile to try next. This strategy helps to reduce the number of function evaluations; in fact, the motivation of BO methods is to keep the number of function evaluations as low as possible while ""using the whole buffalo"" to make good guesses about what point to test next. There are different figures of merit (expected improvement, expected quantile improvement, probability of improvement...) which are used to compare points to visit next.Contrast this to something like a grid search, which will never use any information from its previous function evaluations to inform where to go next.Incidentally, this is also a powerful global optimization technique, and as such makes no assumptions about the convexity of the surface. Additionally, if the function is stochastic (say, evaluations have some inherent random noise), this can be directly accounted for in the GP model.On the other hand, you'll have to fit at least one GP at every iteration (or several, picking the ""best"", or averaging over alternatives, or fully Bayesian methods). Then, the model is used to make (probably thousands) of predictions, usually in the form of multistart local optimization, with the observation that it's much cheaper to evaluate the GP prediction function than the function under optimization. But even with this computational overhead, it tends to be the case that even nonconvex functions can be optimized with a relatively small number of function calls.A downside to GP is that the number of iterations to get a good result tends to grow with the number of hyper-parameters to search over.A widely-cited paper on the topic is Jones et al (1998), ""Efficient Global Optimization of Expensive Black-Box Functions."" But there are many variations on this idea.Even when the cost function is expensive to evaluate, random search can still be useful. Random search is dirt-simple to implement. The only choice for a researcher to make is setting the the probability $p$ that you want your results to lie in some quantile $q$; the rest proceeds automatically using results from basic probability.Suppose your quantile is $q = 0.95$ and you want a $p=0.95$ probability that the model results are in top $100\times (1-q)=5$ percent of all hyperparameter tuples. The probability that all $n$ attempted tuples are not in that window is $q^n = 0.95^n$ (because they are chosen independently at random from the same distribution), so the probability that at least one tuple is in that region is $1 - 0.95^n$. Putting it all together, we have$$
1 - q^n \ge p \implies n \ge \frac{\log(1 - p)}{\log(q)}
$$which in our specific case yields $n \ge 59$.This result is why most people recommend $n=60$ attempted tuples for random search. It's worth noting that $n=60$ is comparable to the number of experiments required to get good results with Gaussian Process-based methods when there are a moderate number of parameters.Unlike Gaussian Processes, for random search, the number of queried tuples does not grow with the number of hyper-parameters to search over. Indee,d the dimension of the problem does not appear in the expression that recommends attempting $n=60$ random values.  However, this does not mean that random search is ""immune"" to curse of dimensionality. Increasing the dimension of the hyperparameter search space can mean that the average result drawn from among the ""best 5% of values"" is still very poor. More information: The ""Amazing Hidden Power"" of Random Search? The intuition is that if we increase the volume of the search space, then we are naturally also increasing the volume of 5% of the search space.Since you have a probabilistic characterization of how good the results are, this  result can be a persuasive tool to convince your boss that running additional experiments will yield diminishing marginal returns."
How and why do normalization and feature scaling work?,"
I see that lots of machine learning algorithms work better with mean cancellation and covariance equalization. For example, Neural Networks tend to converge faster, and K-Means generally gives better clustering with pre-processed features. I do not see the intuition behind these pre-processing steps lead to improved performance. Can someone explain this me?
","['machine-learning', 'neural-networks', 'covariance', 'normalization']","It's simply a case of getting all your data on the same scale: if the scales for different features are wildly different, this can have a knock-on effect on your ability to learn (depending on what methods you're using to do it). Ensuring standardised feature values implicitly weights all features equally in their representation."
What method can be used to detect seasonality in data?,"
I want to detect seasonality in data that I receive. There are some methods that I have found like the seasonal subseries plot and the autocorrelation plot but the thing is I don't understand how to read the graph, could anyone help?  The other thing is, are there other methods to detect seasonality with or without the final result in graph?
","['time-series', 'seasonality']",
Multivariate multiple regression in R,"
I have 2 dependent variables (DVs) each of whose score may be influenced by the set of 7 independent variables (IVs). DVs are continuous, while the set of IVs consists of a mix of continuous and binary coded variables. (In code below continuous variables are written in upper case letters and binary variables in lower case letters.)
The aim of the study is to uncover how these DVs are influenced by IVs variables. I proposed the following multivariate multiple regression (MMR) model:
my.model <- lm(cbind(A, B) ~ c + d + e + f + g + H + I)

To interpret the results I call two statements:

summary(manova(my.model))
Manova(my.model)

Outputs from both calls are pasted below and are significantly different. Can somebody please explain which statement among the two should be picked to properly summarize the results of MMR, and why? Any suggestion would be greatly appreciated.
Output using summary(manova(my.model)) statement:
> summary(manova(my.model))
           Df   Pillai approx F num Df den Df    Pr(>F)    
c           1 0.105295   5.8255      2     99  0.004057 ** 
d           1 0.085131   4.6061      2     99  0.012225 *  
e           1 0.007886   0.3935      2     99  0.675773    
f           1 0.036121   1.8550      2     99  0.161854    
g           1 0.002103   0.1043      2     99  0.901049    
H           1 0.228766  14.6828      2     99 2.605e-06 ***
I           1 0.011752   0.5887      2     99  0.556999    
Residuals 100                                              
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Output using Manova(my.model) statement:
> library(car)
> Manova(my.model)

Type II MANOVA Tests: Pillai test statistic
  Df test stat approx F num Df den Df    Pr(>F)    
c  1  0.030928   1.5798      2     99   0.21117    
d  1  0.079422   4.2706      2     99   0.01663 *  
e  1  0.003067   0.1523      2     99   0.85893    
f  1  0.029812   1.5210      2     99   0.22355    
g  1  0.004331   0.2153      2     99   0.80668    
H  1  0.229303  14.7276      2     99 2.516e-06 ***
I  1  0.011752   0.5887      2     99   0.55700    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

","['r', 'multivariate-analysis', 'manova', 'multiple-regression', 'multivariate-regression']","Briefly stated, this is because base-R's manova(lm()) uses sequential model comparisons for so-called Type I sum of squares, whereas car's Manova() by default uses model comparisons for Type II sum of squares.I assume you're familiar with the model-comparison approach to ANOVA or regression analysis. This approach defines these tests by comparing a restricted model (corresponding to a null hypothesis) to an unrestricted model (corresponding to the alternative hypothesis). If you're not familiar with this idea, I recommend Maxwell & Delaney's excellent ""Designing experiments and analyzing data"" (2004).For type I SS, the restricted model in a regression analysis for your first predictor c is the null-model which only uses the absolute term: lm(Y ~ 1), where Y in your case would be the multivariate DV defined by cbind(A, B). The unrestricted model then adds predictor c, i.e. lm(Y ~ c + 1).For type II SS, the unrestricted model in a regression analysis for your first predictor c is the full model which includes all predictors except for their interactions, i.e., lm(Y ~ c + d + e + f + g + H + I). The restricted model removes predictor c from the unrestricted model, i.e., lm(Y ~ d + e + f + g + H + I).Since both functions rely on different model comparisons, they lead to different results. The question which one is preferable is hard to answer - it really depends on your hypotheses.What follows assumes you're familiar with how multivariate test statistics like the Pillai-Bartlett Trace are calculated based on the null-model, the full model, and the pair of restricted-unrestricted models. For brevity, I only consider predictors c and H, and only test for c.For comparison, the result from car's Manova() function using SS type II.Now manually verify both results. Build the design matrix $X$ first and compare to R's design matrix.Now define the orthogonal projection for the full model ($P_{f} = X (X'X)^{-1} X'$, using all predictors). This gives us the matrix $W = Y' (I-P_{f}) Y$.Restricted and unrestricted models for SS type I plus their projections $P_{rI}$ and $P_{uI}$, leading to matrix $B_{I} = Y' (P_{uI} - P_{PrI}) Y$.Restricted and unrestricted models for SS type II plus their projections $P_{rI}$ and $P_{uII}$, leading to matrix $B_{II} = Y' (P_{uII} - P_{PrII}) Y$.Pillai-Bartlett trace for both types of SS: trace of $(B + W)^{-1} B$.Note that the calculations for the orthogonal projections mimic the mathematical formula, but are a bad idea numerically. One should really use QR-decompositions or SVD in combination with crossprod() instead."
Diagnostics for logistic regression?,"
For linear regression, we can check the diagnostic plots (residuals plots, Normal QQ plots, etc) to check if the assumptions of linear regression are violated.
For logistic regression, I am having trouble finding resources that explain how to diagnose the logistic regression model fit. Digging up some course notes for GLM, it simply states that checking the residuals is not helpful for performing diagnosis for a logistic regression fit.
Looking around the internet, there also seems to be various ""diagnosis"" procedures, such as checking the model deviance and performing chi-squared tests, but other sources state that this is inappropriate, and that you should perform a Hosmer-Lemeshow goodness of fit test. Then I find other sources that state that this test may be highly dependent on the actual groupings and cut-off values (may not be reliable).
So how should one diagnose the logistic regression fit?
","['regression', 'logistic', 'diagnostic']","A few newer techniques I have come across for assessing the fit of logistic regression models come from political science journals:Both of these techniques purport to replace Goodness-of-Fit tests (like Hosmer & Lemeshow) and identify potential mis-specification (in particular non-linearity in included variables in the equation). These are particularly useful as typical R-square measures of fit are frequently criticized.Both of the above papers above utilize predicted probabilities vs. observed outcomes in plots - somewhat avoiding the unclear issue of what is a residual in such models. Examples of residuals could be contribution to the log-likelihood or Pearson residuals (I believe there are many more though). Another measure that is often of interest (although not a residual) are DFBeta's (the amount a coefficient estimate changes when an observation is excluded from the model). See examples in Stata for this UCLA page on Logistic Regression Diagnostics along with other potential diagnostic procedures.I don't have it handy, but I believe J. Scott Long's Regression Models for Categorical and Limited Dependent Variables goes in to sufficient detail on all of these different diagnostic measures in a simple manner."
Performance metrics to evaluate unsupervised learning,"
With respect to the unsupervised learning (like clustering), are there any metrics to evaluate performance?
","['machine-learning', 'clustering', 'data-mining', 'unsupervised-learning']","In some sense I think this question is unanswerable. I say this because how well a particular unsupervised method performs will largely depend on why one is doing unsupervised learning in the first place, i.e., does the method perform well in the context of your end goal? Obviously this isn't completely true, people work on these problems and publish results which include some sort of evaluation. I'll outline a few of the approaches I'm familiar with below.A good resource (with references) for clustering is sklearn's documentation page, Clustering Performance Evaluation. This covers several method, but all but one, the Silhouette Coefficient, assumes ground truth labels are available. This method is also mentioned in the question Evaluation measure of clustering, linked in the comments for this question.If your unsupervised learning method is probabilistic, another option is to evaluate some probability measure (log-likelihood, perplexity, etc) on held out data. The motivation here is that if your unsupervised learning method assigns high probability to similar data that wasn't used to fit parameters, then it has probably done a good job of capturing the distribution of interest. A domain where this type of evaluation is commonly used is language modeling.The last option I'll mention is using a supervised learner on a related auxiliary task. If you're unsupervised method produces latent variables, you can think of these latent variables as being a representation of the input. Thus, it is sensible to use these latent variables as input for a supervised classifier performing some task related to the domain the data is from. The performance of the supervised method can then serve as a surrogate for the performance of the unsupervised learner. This is essentially the setup you see in most work on representation learning. This description is probably a little nebulous, so I'll give a concrete example. Nearly all of the work on word representation learning uses the following approach for evaluation: For an example of this approach in action see the paper Training Restricted Boltzmann Machines on Word Observations by Dahl et al."
Variance of product of multiple independent random variables,"
We know the answer for two independent variables:
$$ {\rm Var}(XY) = E(X^2Y^2) − (E(XY))^2={\rm Var}(X){\rm Var}(Y)+{\rm Var}(X)(E(Y))^2+{\rm Var}(Y)(E(X))^2$$
However, if we take the product of more than two variables, ${\rm Var}(X_1X_2 \cdots X_n)$, what would the answer be in terms of variances and expected values of each variable?
","['variance', 'random-variable', 'independence']",
One-hot vs dummy encoding in Scikit-learn,"
There are two different ways to encoding categorical variables. Say, one categorical variable has n values. One-hot encoding converts it into n variables, while dummy encoding converts it into n-1 variables. If we have k categorical variables, each of which has n values. One hot encoding ends up with kn variables, while dummy encoding ends up with kn-k variables.
I hear that for one-hot encoding, intercept can lead to collinearity problem, which makes the model not sound. Someone call it ""dummy variable trap"".
My questions:

Scikit-learn's linear regression model allows users to disable intercept. So for one-hot encoding, should I always set fit_intercept=False? For dummy encoding, fit_intercept should always be set to True? I do not see any ""warning"" on the website.
Since one-hot encoding generates more variables, does it have more degree of freedom than dummy encoding?

","['regression', 'categorical-data', 'data-transformation', 'scikit-learn', 'data-preprocessing']","Scikit-learn's linear regression model allows users to disable intercept. So for one-hot encoding, should I always set fit_intercept=False? For dummy encoding, fit_intercept should always be set to True? I do not see any ""warning"" on the website.For an unregularized linear model with one-hot encoding, yes, you need to set the intercept to be false or else incur perfect collinearity.  sklearn also allows for a ridge shrinkage penalty, and in that case it is not necessary, and in fact you should include both the intercept and all the levels.  For dummy encoding you should include an intercept, unless you have standardized all your variables, in which case the intercept is zero.Since one-hot encoding generates more variables, does it have more degree of freedom than dummy encoding?The intercept is an additional degree of freedom, so in a well specified model it all equals out.For the second one, what if there are k categorical variables? k variables are removed in dummy encoding. Is the degree of freedom still the same?You could not fit a model in which you used all the levels of both categorical variables, intercept or not.  For, as soon as you have one-hot-encoded all the levels in one variable in the model, say with binary variables $x_1, x_2, \ldots, x_n$, then you have a linear combination of predictors equal to the constant vector$$ x_1 + x_2 + \cdots + x_n = 1 $$If you then try to enter all the levels of another categorical $x'$ into the model, you end up with a distinct linear combination equal to a constant vector$$ x_1' + x_2' + \cdots + x_k' = 1 $$and so you have created a linear dependency$$ x_1 + x_2 + \cdots x_n - x_1' - x_2' - \cdots - x_k' = 0$$So you must leave out a level in the second variable, and everything lines up properly.Say, I have 3 categorical variables, each of which has 4 levels. In dummy encoding, 3*4-3=9 variables are built with one intercept. In one-hot encoding, 3*4=12 variables are built without an intercept. Am I correct?The second thing does not actually work.  The $3  \times 4 = 12$ column design matrix you create will be singular.  You need to remove three columns, one from each of three distinct categorical encodings, to recover non-singularity of your design."
How is the minimum of a set of IID random variables distributed?,"
If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\min(X_1, ..., X_n)$ in general?
","['distributions', 'random-variable', 'extreme-value']","If the cdf of $X_i$ is denoted by $F(x)$, then the cdf of the minimum is given by $1-[1-F(x)]^n$."
F1/Dice-Score vs IoU,"
I was confused about the differences between the F1 score, Dice score and IoU (intersection over union). By now I found out that F1 and Dice mean the same thing (right?) and IoU has a very similar formula to the other two.

F1 / Dice: $$\frac{2TP}{2TP+FP+FN}$$
IoU / Jaccard: $$\frac{TP}{TP+FP+FN}$$

Are there any practical differences or other things worth noting except that F1 weights the true-positives higher? Is there a situation where I'd use one but not the other?
","['terminology', 'accuracy', 'precision-recall']","You're on the right track.So a few things right off the bat. From the definition of the two metrics, we have that IoU and F score are always within a factor of 2 of each other:
$$ F/2 \leq IoU \leq F $$
and also that they meet at the extremes of one and zero under the conditions that you would expect (perfect match and completely disjoint).Note also that the ratio between them can be related explicitly to the IoU:
$$ IoU/F = 1/2 + IoU/2 $$
so that the ratio approaches 1/2 as both metrics approach zero.But there's a stronger statement that can be made for the typical application of classification a la machine learning. For any fixed ""ground truth"", the two metrics are always positively correlated. That is to say that if classifier A is better than B under one metric, it is also better than classifier B under the other metric.It is tempting then to conclude that the two metrics are functionally equivalent so the choice between them is arbitrary, but not so fast! The problem comes when taking the average score over a set of inferences. Then the difference emerges when quantifying how much worse classifier B is than A for any given case.In general, the IoU metric tends to penalize single instances of bad classification more than the F score quantitatively even when they can both agree that this one instance is bad. Similarly to how L2 can penalize the largest mistakes more than L1, the IoU metric tends to have a ""squaring"" effect on the errors relative to the F score. So the F score tends to measure something closer to average performance, while the IoU score measures something closer to the worst case performance.Suppose for example that the vast majority of the inferences are moderately better with classifier A than B, but some of them of them are significantly worse using classifier A. It may be the case then that the F metric favors classifier A while the IoU metric favors classifier B.To be sure, both of these metrics are much more alike than they are different. But both of them suffer from another disadvantage from the standpoint of taking averages of these scores over many inferences: they both overstate the importance of sets with little-to-no actual ground truth positive sets. In the common example of image segmentation, if an image only has a single pixel of some detectable class, and the classifier detects that pixel and one other pixel, its F score is a lowly 2/3 and the IoU is even worse at 1/2. Trivial mistakes like these can seriously dominate the average score taken over a set of images. In short, it weights each pixel error inversely proportionally to the size of the selected/relevant set rather than treating them equally.There is a far simpler metric that avoids this problem. Simply use the total error: FN + FP (e.g. 5% of the image's pixels were miscategorized). In the case where one is more important than the other, a weighted average may be used: $c_0$FP + $c_1$FN."
What are good initial weights in a neural network?,"
I have just heard, that it's a good idea to choose initial weights of a neural network from the range $(\frac{-1}{\sqrt d} , \frac{1}{\sqrt d})$, where $d$ is the number of inputs to a given neuron. It is assumed, that the sets are normalized - mean 0, variance 1 (don't know if this matters).
Why is this a good idea?
","['neural-networks', 'normalization']","I assume you are using logistic neurons, and that you are training by gradient descent/back-propagation. The logistic function is close to flat for large positive or negative inputs. The derivative at an input of $2$ is about $1/10$, but at $10$ the derivative is about $1/22000$ . This means that if the input of a logistic neuron is $10$ then, for a given training signal, the neuron will learn about $2200$ times slower that if the input was $2$. If you want the neuron to learn quickly, you either need to produce a huge training signal (such as with a cross-entropy loss function) or you want the derivative to be large. To make the derivative large, you set the initial weights so that you often get inputs in the range $[-4,4]$. The initial weights you give might or might not work. It depends on how the inputs are normalized. If the inputs are normalized to have mean $0$ and standard deviation $1$, then a random sum of $d$ terms with weights uniform on $(\frac{-1}{\sqrt{d}},\frac{1}{\sqrt{d}})$ will have mean $0$ and variance $\frac{1}{3}$, independent of $d$. The probability that you get a sum outside of $[-4,4]$ is small. That means as you increase $d$, you are not causing the neurons to start out saturated so that they don't learn. With inputs which are not normalized, those weights may not be effective at avoiding saturation. "
Free resources for learning R,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I'm interested in learning R on the cheap. What's the best free resource/book/tutorial for learning R?
","['r', 'references']","Intro:with a focus on economics:Graphics: plots, maps, etc.:GUIs:Time series & finance:a good beginner’s tutorial for Time Series http://www.stat.pitt.edu/stoffer/tsa2/index.htmlInteresting time series packages in R http://robjhyndman.com/softwareadvanced time series in R http://www.wise.xmu.edu.cn/2007summerworkshop/download/Advanced%20Topics%20in%20Time%20Series%20Econometrics%20Using%20R1_ZongwuCAI.pdfprovides a great analysis and visualization framework for quantitative trading http://www.quantmod.com/Guide to Credit Scoring using R http://cran.r-project.org/doc/contrib/Sharma-CreditScoring.pdfan Open Source framework for Financial Analysis http://www.rmetrics.org/Data / text mining:Other statistical techniques:Editors:Interfacing w/ other languages / software:Blogs, newsletters, etc.:Other / uncategorized: (as of yet)"
Practical thoughts on explanatory vs. predictive modeling,"
Back in April, I attended a talk at the UMD (University of Maryland) Math Department Statistics group seminar series called ""To Explain or To Predict?"". The talk was given by Prof. Galit Shmueli who teaches at UMD's Smith Business School. Her talk was based on research she did for a paper titled ""Predictive vs. Explanatory Modeling in IS Research"", and a follow up working paper titled ""To Explain or To Predict?"".
Dr. Shmueli's argument is that the terms predictive and explanatory in a statistical modeling context have become conflated, and that statistical literature lacks a a thorough discussion of the differences. In the paper, she contrasts both and talks about their practical implications. I encourage you to read the papers.
The questions I'd like to pose to the practitioner community are:

How do you define a predictive exercise vs an explanatory/descriptive
one? It would be useful if you could talk about the specific
application.
Have you ever fallen into the trap of using one when meaning to use the other? I certainly have. How do you know which one to use?

",['predictive-models'],
Covariance and independence?,"
I read from my textbook that $\text{cov}(X,Y)=0$ does not guarantee X and Y are independent. But if they are independent, their covariance must be 0. I could not think of any proper example yet; could someone provide one?
","['independence', 'covariance']","Easy example:  Let $X$ be a random variable that is $-1$ or $+1$ with probability 0.5.  Then let $Y$ be a random variable such that $Y=0$ if $X=-1$, and $Y$ is randomly $-1$ or $+1$ with probability 0.5 if $X=1$.Clearly $X$ and $Y$ are highly dependent (since knowing $Y$ allows me to perfectly know $X$), but their covariance is zero:  They both have zero mean, and $$\eqalign{
\mathbb{E}[XY] &=&(-1) &\cdot &0 &\cdot &P(X=-1) \\
 
&+& 1 &\cdot &1 &\cdot &P(X=1,Y=1) \\

&+& 1 &\cdot &(-1)&\cdot &P(X=1,Y=-1) \\

&=&0.
}$$Or more generally, take any distribution $P(X)$ and any $P(Y|X)$ such that $P(Y=a|X) = P(Y=-a|X)$ for all $X$ (i.e., a joint distribution that is symmetric around the $x$ axis), and you will always have zero covariance.  But you will have non-independence whenever $P(Y|X) \neq P(Y)$; i.e., the conditionals are not all equal to the marginal. Or ditto for symmetry around the $y$ axis. "
How can adding a 2nd IV make the 1st IV significant?,"
I have what is probably a simple question, but it is baffling me right now, so I am hoping you can help me out.
I have a least squares regression model, with one independent variable and one dependent variable. The relationship is not significant. Now I add a second independent variable. Now the relationship between the first independent variable and the dependent variable becomes significant.
How does this work? This is probably demonstrating some issue with my understanding, but to me, but I do not see how adding this second independent variable can make the first significant.
","['regression', 'multiple-regression', 'statistical-power', 'suppressor']","Although collinearity (of predictor variables) is a possible explanation, I would like to suggest it is not an illuminating explanation because we know collinearity is related to ""common information"" among the predictors, so there is nothing mysterious or counter-intuitive about the side effect of introducing a second correlated predictor into the model.Let us then consider the case of two predictors that are truly orthogonal: there is absolutely no collinearity among them.  A remarkable change in significance can still happen.Designate the predictor variables $X_1$ and $X_2$ and let $Y$ name the predictand.  The regression of $Y$ against $X_1$ will fail to be significant when the variation in $Y$ around its mean is not appreciably reduced when $X_1$ is used as the independent variable.  When that variation is strongly associated with a second variable $X_2$, however, the situation changes.  Recall that multiple regression of $Y$ against $X_1$ and $X_2$ is equivalent toSeparately regress $Y$ and $X_1$ against $X_2$.Regress the $Y$ residuals against the $X_1$ residuals.The residuals from the first step have removed the effect of $X_2$.  When $X_2$ is closely correlated with $Y$, this can expose a relatively small amount of variation that had previously been masked.  If this variation is associated with $X_1$, we obtain a significant result.All this might perhaps be clarified with a concrete example.  To begin, let's use R to generate two orthogonal independent variables along with some independent random error $\varepsilon$:(The svd step assures the two columns of matrix x (representing $X_1$ and $X_2$) are orthogonal, ruling out collinearity as a possible explanation of any subsequent results.)Next, create $Y$ as a linear combination of the $X$'s and the error.  I have adjusted the coefficients to produce the counter-intuitive behavior:This is a realization of the model $Y \sim_{iid} N(0.05 X_1 + 1.00 X_2, 0.01^2)$ with $n=32$ cases.Look at the two regressions in question.  First, regress $Y$ against $X_1$ only:The high p-value of 0.710 shows that $X_1$ is completely non-significant.Next, regress $Y$ against $X_1$ and $X_2$:Suddenly, in the presence of $X_2$, $X_1$ is strongly significant, as indicated by the near-zero p-values for both variables.We can visualize this behavior by means of a scatterplot matrix of the variables $X_1$, $X_2$, and $Y$ along with the residuals used in the two-step characterization of multiple regression above.  Because $X_1$ and $X_2$ are orthogonal, the $X_1$ residuals will be the same as $X_1$ and therefore need not be redrawn.  We will include the residuals of $Y$ against $X_2$ in the scatterplot matrix, giving this figure:Here is a rendering of it (with a little prettification):This matrix of graphics has four rows and four columns, which I will count down from the top and from left to right.Notice:The $(X_1, X_2)$ scatterplot in the second row and first column confirms the orthogonality of these predictors: the least squares line is horizontal and correlation is zero.The $(X_1, Y)$ scatterplot in the third row and first column exhibits the slight but completely insignificant relationship reported by the first regression of $Y$ against $X_1$.  (The correlation coefficient, $\rho$, is only $0.07$).The $(X_2, Y)$ scatterplot in the third row and second column shows the strong relationship between $Y$ and the second independent variable.  (The correlation coefficient is $0.996$).The fourth row examines the relationships between the residuals of $Y$ (regressed against $X_2$) and other variables:The vertical scale shows that the residuals are (relatively) quite small: we couldn't easily see them in the scatterplot of $Y$ against $X_2$.The residuals are strongly correlated with $X_1$ ($\rho = 0.80$).  The regression against $X_2$ has unmasked this previously hidden behavior.By construction, there is no remaining correlation between the residuals and $X_2$.There is little correlation between $Y$ and these residuals ($\rho = 0.09$).  This shows how the residuals can behave entirely differently than $Y$ itself.  That's how $X_1$ can suddenly be revealed as a significant contributor to the regression.Finally, it is worth remarking that the two estimates of the $X_1$ coefficient (both equal to $0.06895$, not far from the intended value of $0.05$) agree only because $X_1$ and $X_2$ are orthogonal.  Except in designed experiments, it is rare for orthogonality to hold exactly.  A departure from orthogonality usually causes the coefficient estimates to change."
Why does including latitude and longitude in a GAM account for spatial autocorrelation?,"
I have produced generalized additive models for deforestation. To account for spatial-autocorrelation, I have included latitude and longitude as a smoothed, interaction term (i.e. s(x,y)).
I've based this on reading many papers where the authors say 'to account for spatial autocorrelation, coordinates of points were included as smoothed terms' but these have never explained why this actually accounts for it. It's quite frustrating. I've read all the books I can find on GAMs in the hope of finding an answer, but most (e.g. Generalized Additive Models, an Introduction with R, S.N. Wood) just touch on the subject without explaining.
I'd really appreciate it if someone could explain WHY the inclusion of latitude and longitude accounts for spatial autocorrelation, and what 'accounting' for it really means - is it simply enough to include it in the model, or should you compare a model with s(x,y) in and a model without? And does the deviance explained by the term indicate the extent of spatial autocorrelation?
","['r', 'modeling', 'spatial', 'autocorrelation', 'generalized-additive-model']","The main issue in any statistical model is the assumptions that underlay any inference procedure. In the sort of model you describe, the residuals are assumed independent. If they have some spatial dependence and this is not modelled in the sytematic part of the model, the residuals from that model will also exhibit spatial dependence, or in other words they will be spatially autocorrelated. Such dependence would invalidate the theory that produces p-values from test statistics in the GAM for example; you can't trust the p-values because they were computed assuming independence.You have two main options for handling such data; i) model the spatial dependence in the systematic part of the model, or ii) relax the assumption of independence and estimate the correlation between residuals.i) is what is being attempted by including a smooth of the spatial locations in the model. ii) requires estimation of the correlation matrix of the residuals often during model fitting using a procedure like generalised least squares. How well either of these approaches deal with the spatial dependence will depend upon the nature & complexity of the spatial dependence and how easily it can be modelled.In summary, if you can model the spatial dependence between observations then the residuals are more likely to be independent random variables and therefore not violate the assumptions of any inferential procedure."
Where to cut a dendrogram?,"
Hierarchical clustering can be represented by a dendrogram. Cutting a dendrogram at a certain level gives a set of clusters. Cutting at another level gives another set of clusters. How would you pick where to cut the dendrogram? Is there something we could consider an optimal point? If I look at a dendrogram across time as it changes, should I cut at the same point?
","['clustering', 'dendrogram']","There is no definitive answer since cluster analysis is essentially an exploratory approach; the interpretation of the resulting hierarchical structure is context-dependent and often several solutions are equally good from a theoretical point of view.Several clues were given in a related question, What stop-criteria for agglomerative hierarchical clustering are used in practice? I generally use visual criteria, e.g. silhouette plots, and some kind of numerical criteria, like Dunn’s validity index, Hubert's gamma, G2/G3 coefficient, or the corrected Rand index. Basically, we want to know how well the original distance matrix is approximated in the cluster space, so a measure of the cophenetic correlation is also useful. I also use k-means, with several starting values, and the gap statistic (mirror) to determine the number of clusters that minimize the within-SS. The concordance with Ward hierarchical clustering gives an idea of the stability of the cluster solution (You can use matchClasses() in the e1071 package for that).You will find useful resources in the CRAN Task View Cluster, including pvclust, fpc, clv, among others. Also worth to give a try is the clValid package (described in the Journal of Statistical Software).Now, if your clusters change over time, this is a bit more tricky; why choosing the first cluster-solution rather than another? Do you expect that some individuals move from one cluster to another as a result of an underlying process evolving with time?There are some measure that try to match clusters that have a maximum absolute or relative overlap, as was suggested to you in your preceding question. Look at Comparing Clusterings - An Overview from Wagner and Wagner."
"Unified view on shrinkage: what is the relation (if any) between Stein's paradox, ridge regression, and random effects in mixed models?","
Consider the following three phenomena.

Stein's paradox: given some data from multivariate normal distribution in $\mathbb R^n, \: n\ge 3$, sample mean is not a very good estimator of the true mean. One can obtain an estimation with lower mean squared error if one shrinks all the coordinates of the sample mean towards zero [or towards their mean, or actually towards any value, if I understand correctly].
NB: usually Stein's paradox is formulated via considering only one single data point from $\mathbb R^n$; please correct me if this is crucial and my formulation above is not correct.
Ridge regression: given some dependent variable $\mathbf y$ and some independent variables $\mathbf X$, the standard regression $\beta = (\mathbf X^\top \mathbf X)^{-1} \mathbf X^\top \mathbf y$ tends to overfit the data and lead to poor out-of-sample performance. One can often reduce overfitting by shrinking $\beta$ towards zero: $\beta = (\mathbf X^\top \mathbf X + \lambda \mathbf I)^{-1} \mathbf X^\top \mathbf y$.
Random effects in multilevel/mixed models: given some dependent variable $y$ (e.g. student's height) that depends on some categorical predictors (e.g. school id and student's gender), one is often advised to treat some predictors as 'random', i.e. to suppose that the mean student's height in each school comes from some underlying normal distribution. This results in shrinking the estimations of mean height per school towards the global mean.

I have a feeling that all of this are various aspects of the same ""shrinkage"" phenomenon, but I am not sure and certainly lack a good intuition about it. So my main question is: is there indeed a deep similarity between these three things, or is it only a superficial semblance? What is the common theme here? What is the correct intuition about it?
In addition, here are some pieces of this puzzle that don't really fit together for me:

In ridge regression, $\beta$ is not shrunk uniformly; ridge shrinkage is actually related to singular value decomposition of $\mathbf X$, with low-variance directions being shrunk more (see e.g. The Elements of Statistical Learning 3.4.1). But James-Stein estimator simply takes the sample mean and multiplies it by one scaling factor. How does that fit together?
Update: see James-Stein Estimator with unequal variances and e.g. here regarding variances of $\beta$ coefficients.
Sample mean is optimal in dimensions below 3. Does it mean that when there are only one or two predictors in the regression model, ridge regression will always be worse than ordinary least squares? Actually, come to think of it, I cannot imagine a situation in 1D (i.e. simple, non-multiple regression) where ridge shrinkage would be beneficial...
Update: No. See Under exactly what conditions is ridge regression able to provide an improvement over ordinary least squares regression?
On the other hand, sample mean is always suboptimal in dimensions above 3. Does it mean that with more than 3 predictors ridge regression is always better than OLS, even if all the predictors are uncorrelated (orthogonal)? Usually  ridge regression is motivated by multicollinearity and the need to ""stabilize"" the $(\mathbf X^\top \mathbf X)^{-1}$ term. 
Update: Yes! See the same thread as above.
There are often some heated discussion about whether various factors in ANOVA should be included as fixed or random effects. Shouldn't we, by the same logic, always treat a factor as random if it has more than two levels (or if there are more than two factors? now I am confused)?
Update: ?


Update: I got some excellent answers, but none provides enough of a big picture, so I will let the question ""open"". I can promise to award a bounty of at least 100 points to a new answer that will surpass the existing ones. I am mostly looking for a unifying view that could explain how the general phenomenon of shrinkage manifests itself in these various contexts and point out the principal differences between them.
","['regression', 'mixed-model', 'ridge-regression', 'regularization', 'steins-phenomenon']","Let $\mathbf y$ be a vector of observation of $\boldsymbol \theta$ of length $m$, ${\mathbf y} \sim N({\boldsymbol \theta}, \sigma^2 I)$, the James-Stein estimator is,
$$\widehat{\boldsymbol \theta}_{JS} = 
\left( 1 - \frac{(m-2) \sigma^2}{\|{\mathbf y}\|^2} \right) {\mathbf y}.$$
In terms of ridge regression, we can estimate $\boldsymbol \theta$ via $\min_{\boldsymbol{\theta}} \|\mathbf{y}-\boldsymbol{\theta}\|^2 + \lambda\|\boldsymbol{\theta}\|^2 ,$
where the solution is $$\widehat{\boldsymbol \theta}_{\mathrm{ridge}} = \frac{1}{1+\lambda}\mathbf y.$$
It is easy to see that the two estimators are in the same form, but we need to estimate $\sigma^2$ in James-Stein estimator, and determine $\lambda$ in ridge regression via cross-validation.Let us discuss the mixed/random effects models in genetics first. The model is $$\mathbf {y}=\mathbf {X}\boldsymbol{\beta} + \boldsymbol{Z\theta}+\mathbf {e},
\boldsymbol{\theta}\sim N(\mathbf{0},\sigma^2_{\theta} I),
\textbf{e}\sim N(\mathbf{0},\sigma^2 I).$$
If there is no fixed effects and $\mathbf {Z}=I$, the model becomes 
$$\mathbf {y}=\boldsymbol{\theta}+\mathbf {e},
\boldsymbol{\theta}\sim N(\mathbf{0},\sigma^2_{\theta} I),
\textbf{e}\sim N(\mathbf{0},\sigma^2 I),$$
which is equivalent to the setting of James-Stein estimator, with some Bayesian idea.If we focus on the random effects models above,
$$\mathbf {y}=\mathbf {Z\theta}+\mathbf {e},
\boldsymbol{\theta}\sim N(\mathbf{0},\sigma^2_{\theta} I),
\textbf{e}\sim N(\mathbf{0},\sigma^2 I).$$
The estimation is equivalent to solve the problem
$$\min_{\boldsymbol{\theta}} \|\mathbf{y}-\mathbf {Z\theta}\|^2 + \lambda\|\boldsymbol{\theta}\|^2$$
when $\lambda=\sigma^2/\sigma_{\theta}^2$. The proof can be found in Chapter 3 of Pattern recognition and machine learning.In the random effects model above, the dimension of $\mathbf y$ is $m\times 1,$ and that of $\mathbf Z$ is $m \times p$. If we vectorize $\mathbf Z$ as $(mp)\times 1,$ and repeat $\mathbf y$ correspondingly, then we have the hierarchical/clustered structure, $p$ clusters and each with $m$ units. If we regress $\mathrm{vec}(\mathbf Z)$ on repeated $\mathbf y$, then we can obtain the random effect of $Z$ on $y$ for each cluster, though it is kind of like reverse regression.
Acknowledgement: the first three points are largely learned from these two Chinese articles, 1, 2."
How should tiny $p$-values be reported? (and why does R put a minimum on 2.22e-16?),"
For some tests in R, there is a lower limit on the p-value calculations of $2.22 \cdot 10^{-16}$. I'm not sure why it's this number, if there is a good reason for it or if it's just arbitrary. A lot of other stats packages just go to 0.0001, so this is a much higher level of precision. But I haven't seen too many papers reporting $p < 2.22\cdot 10^{-16}$ or $p = 2.22\cdot 10^{-16}$. 
Is it a common/best practice to report this computed value or is it more typical to report something else (like p < 0.000000000000001)?
","['r', 'p-value', 'reporting', 'precision']","There's a good reason for it.The value can be found via noquote(unlist(format(.Machine)))If you look at the help, (?"".Machine""):It's essentially a value below which you can be quite confident the value will be pretty numerically meaningless - in that any smaller value isn't likely to be an accurate calculation of the value we were attempting to compute. (Having studied a little numerical analysis, depending on what computations were performed by the specific procedure, there's a good chance numerical meaninglessness comes in a fair way above that.)But statistical meaning will have been lost far earlier. Note that p-values depend on assumptions, and the further out into the extreme tail you go the more heavily the true p-value (rather than the nominal value we calculate) will be affected by the mistaken assumptions, in some cases even when they're only a little bit wrong. Since the assumptions are simply not going to be all exactly satisfied, middling p-values may be reasonably accurate (in terms of relative accuracy, perhaps only out by a modest fraction), but extremely tiny p-values may be out by many orders of magnitude.Which is to say that usual practice  (something like the ""<0.0001"" that's you say is common in packages, or the APA rule that Jaap mentions in his answer) is probably not so far from sensible practice, but the approximate point at which things lose meaning beyond saying 'it's very very small' will of course vary quite a lot depending on circumstances.This is one reason why I can't suggest a general rule - there can't be a single rule that's even remotely suitable for everyone in all circumstances - change the circumstances a little and the broad grey line marking the change from somewhat meaningful to relatively meaningless will change, sometimes by a long way.If you were to specify sufficient information about the exact circumstances (e.g. it's a regression, with this much nonlinearity, that amount of variation in this independent variable, this kind and amount of dependence in the error term, that kind of and amount of heteroskedasticity, this shape of error distribution), I could simulate 'true' p-values for you to compare with the nominal p-values, so you could see when they were too different for the nominal value to carry any meaning.But that leads us to the second reason why - even if you specified enough information to simulate the true p-values - I still couldn't responsibly state a cut-off for even those circumstances.What you report depends on people's preferences - yours, and your audience. Imagine you told me enough about the circumstances for me to decide that I wanted to draw the line at a nominal $p$ of $10^{-6}$.All well and good, we might think - except your own preference function (what looks right to you, were you to look at the difference between nominal p-values given by stats packages and the the ones resulting from simulation when you suppose a particular set of failures of assumptions) might put it at $10^{-5}$ and the editors of the journal you want to submit to might put have their blanket rule to cut off at $10^{-4}$, while the next journal might put it at $10^{-3}$ and the next may have no general rule and the specific editor you got might accept even lower values than I gave ... but one of the referees may then have a specific cut off!In the absence of knowledge of their preference functions and rules, and the absence of knowledge of your own utilities, how do I responsibly suggest any general choice of what actions to take?I can at least tell you the sorts of things that I do (and I don't suggest this is a good choice for you at all):There are few circumstances (outside of simulating p-values) in which I would make much of a p less than $10^{-6}$ (I may or may not mention the value reported by the package, but I wouldn't make anything of it other than it was very small, I would usually emphasize the meaningless of the exact number). Sometimes I take a value somewhere in the region of $10^{-5}$ to $10^{-4}$ and say that p was much less than that. On occasion I do actually do as suggested above - perform some simulations to see how sensitive the p-value is in the far tail to various violations of the assumptions, particularly if there's a specific kind of violation I am worried about.That's certainly helpful in informing a choice - but I am as likely to discuss the results of the simulation as to use them to choose a cut-off-value, giving others a chance to choose their own.An alternative to simulation is to look at some procedures that are more robust* to the various potential failures of assumption and see how much difference to the p-value that might make. Their p-values will also not be particularly meaningful, but they do at least give some sense of how much impact there might be. If some are very different from the nominal one, it also gives more of an idea which violations of assumptions to investigate the impact of. Even if you don't report any of those alternatives, it gives a better picture of how meaningful your small p-value is.* Note that here we don't really need procedures that are robust to gross violations of some assumption; ones that are less affected by relatively mild deviations of the relevant assumption should be fine for this exercise.I will say that when/if you do come to do such simulations, even with quite mild violations, in some cases it can be surprising at how far even not-that-small p-values can be wrong. That has done more to change the way I personally interpret a p-value than it has shifted the specific cut-offs I might use.When submitting the results of an actual hypothesis test to a journal, I try to find out if they have any rule. If they don't, I tend to please myself, and then wait for the referees to complain."
Why is multicollinearity not checked in modern statistics/machine learning,"
In traditional statistics, while building a model, we check for multicollinearity using methods such as estimates of the variance inflation factor (VIF), but in machine learning, we instead use regularization for feature selection and don't seem to check whether features are correlated at all. Why do we do that?
","['regression', 'machine-learning', 'multicollinearity', 'regularization', 'variance-inflation-factor']",
Understanding stratified cross-validation,"
I read in Wikipedia: 

In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In
  the case of a dichotomous classification, this means that each fold
  contains roughly the same proportions of the two types of class
  labels.


Say we are using CV for estimating the performance of a predictor or estimator. What would mean response value (MRV) mean in this context? Just the average value of the predictor / estimator? 
In what scenarios  would ""achieving approximately the same MRV"" in all folds be actually important? In other words, what are the consequences of not doing so?

","['cross-validation', 'stratification']","Stratification seeks to ensure that each fold is representative of all strata of the data. Generally this is done in a supervised way for classification and aims to ensure each class is (approximately) equally represented across each test fold (which are of course combined in a complementary way to form training folds).The intuition behind this relates to the bias of most classification algorithms. They tend to weight each instance equally which means overrepresented classes get too much weight (e.g. optimizing F-measure, Accuracy or a complementary form of error).  Stratification is not so important for an algorithm that weights each class equally (e.g. optimizing Kappa, Informedness or ROC AUC) or according to a cost matrix (e.g. that is giving a value to each class correctly weighted and/or a cost to each way of misclassifying). See, e.g.
D. M. W. Powers (2014), What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes. http://arxiv.org/pdf/1503.06410One specific issue that is important across even unbiased or balanced algorithms, is that they tend not to be able to learn or test a class that isn't represented at all in a fold, and furthermore even the case where only one of a class is represented in a fold doesn't allow generalization to performed resp. evaluated. However even this consideration isn't universal and for example doesn't apply so much to one-class learning, which tries to determine what is normal for an individual class, and effectively identifies outliers as being a different class, given that cross-validation is about determining statistics not generating a specific classifier.On the other hand, supervised stratification compromises the technical purity of the evaluation as the labels of the test data shouldn't affect training, but in stratification are used in the selection of the training instances. Unsupervised stratification is also possible based on spreading similar data around looking only at the attributes of the data, not the true class. See, e.g.
https://doi.org/10.1016/S0004-3702(99)00094-6
N. A. Diamantidis, D. Karlis, E. A. Giakoumakis (1997),
Unsupervised stratification of cross-validation for accuracy estimation.Stratification can also be applied to regression rather than classification, in which case like the unsupervised stratification, similarity rather than identity is used, but the supervised version uses the known true function value.Further complications are rare classes and multilabel classification, where classifications are being done on multiple (independent) dimensions.  Here tuples of the true labels across all dimensions can be treated as classes for the purpose of cross-validation. However, not all combinations necessarily occur, and some combinations may be rare. Rare classes and rare combinations are a problem in that a class/combination that occurs at least once but less than K times (in K-CV) cannot be represented in all test folds. In such cases, one could instead consider a form of stratified boostrapping (sampling with replacement to generate a full size training fold with repetitions expected and 36.8% expected unselected for testing, with one instance of each class selected initially without replacement for the test fold).Another approach to multilabel stratification is to try to stratify or bootstrap each class dimension separately without seeking to ensure representative selection of combinations. With L labels and N instances and Kkl instances of class k for label l, we can randomly choose (without replacement) from the corresponding set of labeled instances Dkl approximately N/LKkl instances. This does not ensure optimal balance but rather seeks balance heuristically. This can be improved by barring selection of labels at or over quota unless there is no choice (as some combinations do not occur or are rare).  Problems tend to mean either that there is too little data or that the dimensions are not independent."
"How can an artificial neural network ANN, be used for unsupervised clustering?","
I understand how an artificial neural network (ANN), can be trained in a supervised manner using backpropogation to improve the fitting by decreasing the error in the predictions. I have heard that an ANN can be used for unsupervised learning but how can this be done without a cost function of some sort to guide the optimization stages? With k-means or the EM algorithm there is a function for which each iteration searches to increase. 

How can we do clustering with an ANN and what mechanism does it use
to group data points in the same locality?

(and what extra capabilities are brought with adding more layers to it?)
","['clustering', 'neural-networks', 'unsupervised-learning', 'self-organizing-maps']","Neural networks are widely used in unsupervised learning in order to learn better representations of the input data. For example, given a set of text documents, NN can learn a mapping from document to real-valued vector in such a way that resulting vectors are similar for documents with similar content, i.e. distance preserving. This can be achieved using, for example, auto-encoders - a model that is trained to reconstruct the original vector from a smaller representation (hidden layer activations) with reconstruction error (distance from the ID function) as cost function. This process doesn't give you clusters, but  it creates meaningful representations that can be used for clustering. You could, for instance, run a clustering algorithm on the hidden layer's activations.Clustering: There are a number of different NN architectures specifically designed for clustering. The most widely known is probably self organizing maps. A SOM is a NN that has a set of neurons connected to form a topological grid (usually rectangular). When some pattern is presented to an SOM, the neuron with closest weight vector is considered a winner and its weights are adapted to the pattern, as well as the weights of its neighbourhood. In this way an SOM naturally finds data clusters. A somewhat related algorithm is growing neural gas (it is not limited to predefined number of neurons). Another approach is Adaptive Resonance Theory where we have two layers: ""comparison field"" and ""recognition field"". Recognition field also determines the best match (neuron) to the vector transferred from the comparison field and also have lateral inhibitory connections. Implementation details and exact equations can readily found by googling the names of these models, so I won't put them here."
"If A and B are correlated with C, why are A and B not necessarily correlated?","
I know empirically that is the case.  I have just developed models that run into this conundrum.  I also suspect it is not necessarily a yes/no answer. I mean by that if both A and B are correlated with C, this may have some implication regarding the correlation between A and B.  But, this implication may be weak.  It may be just a sign direction and nothing else.  
Here is what I mean...  Let's say A and B both have a 0.5 correlation with C.  Given that, the correlation between A and B could well be 1.0.  I think it also could be 0.5 or even lower.  But, I think it is unlikely that it would be negative.  Do you agree with that?  
Also, is there an implication if you are considering the standard Pearson Correlation Coefficient or instead the Spearman (rank) Correlation Coefficient?  My recent empirical observations were associated with the Spearman Correlation Coefficient.   
","['correlation', 'cross-correlation']","Because correlation is a mathematical property of multivariate distributions, some insight can be had purely through calculations, regardless of the statistical genesis of those distributions.For the Pearson correlations, consider multinormal variables $X$, $Y$, $Z$.  These are useful to work with because any non-negative definite matrix actually is the covariance matrix of some multinormal distributions, thereby resolving the existence question.  If we stick to matrices with $1$ on the diagonal, the off-diagonal entries of the covariance matrix will be their correlations.  Writing the correlation of $X$ and $Y$ as $\rho$, the correlation of $Y$ and $Z$ as $\tau$, and the correlation of $X$ and $Z$ as $\sigma$, we compute that$1 + 2 \rho \sigma \tau - \left(\rho^2 + \sigma^2 + \tau^2\right) \ge 0$ (because this is the determinant of the correlation matrix and it cannot be negative).When $\sigma = 0$ this implies that $\rho^2 + \tau^2 \le 1$.  To put it another way: when both $\rho$ and $\tau$ are large in magnitude, $X$ and $Z$ must have nonzero correlation.If $\rho^2 = \tau^2 = 1/2$, then any non-negative value of $\sigma$ (between $0$ and $1$ of course) is possible.When $\rho^2 + \tau^2 \lt 1$, negative values of $\sigma$ are allowable.   For example, when $\rho = \tau = 1/2$, $\sigma$ can be anywhere between $-1/2$ and $1$.These considerations imply there are indeed some constraints on the mutual correlations.  The constraints (which depend only on the non-negative definiteness of the correlation matrix, not on the actual distributions of the variables) can be tightened depending on assumptions about the univariate distributions.  For instance, it's easy to see (and to prove) that when the distributions of $X$ and $Y$ are not in the same location-scale family, their correlations must be strictly less than $1$ in size.  (Proof: a correlation of $\pm 1$ implies $X$ and $Y$ are linearly related a.s.)As far as Spearman rank correlations go, consider three trivariate observations $(1,1,2)$, $(2,3,1)$, and $(3,2,3)$ of $(X, Y, Z)$.  Their mutual rank correlations are $1/2$, $1/2$, and $-1/2$.  Thus even the sign of the rank correlation of $Y$ and $Z$ can be the reverse of the signs of the correlations of $X$ and $Y$ and $X$ and $Z$."
Why would parametric statistics ever be preferred over nonparametric?,"
Can someone explain to me why would anyone choose a parametric over a nonparametric statistical method for hypothesis testing or regression analysis? 
In my mind, it's like going for rafting and choosing a non-water resistant watch, because you may not get it wet. Why not use the tool that works on every occasion? 
","['regression', 'hypothesis-testing', 'mathematical-statistics', 'estimation', 'nonparametric']","Rarely if ever a parametric test and a non-parametric test actually have the same null. The parametric $t$-test is testing the mean of the distribution, assuming the first two moments exist. The Wilcoxon rank sum test does not assume any moments, and tests equality of distributions instead. Its implied parameter is a weird functional of distributions, the probability that the observation from one sample is lower than the observation from the other. You can sort of talk about comparisons between the two tests under the completely specified null of identical distributions... but you have to recognize that the two tests are testing different hypotheses.The information that parametric tests bring in along with their assumption helps improving the power of the tests. Of course that information better be right, but there are few if any domains of human knowledge these days where such preliminary information does not exist. An interesting exception that explicitly says ""I don't want to assume anything"" is the courtroom where non-parametric methods continue to be widely popular -- and it makes perfect sense for the application. There's probably a good reason, pun intended, that Phillip Good authored good books on both non-parametric statistics and courtroom statistics.There are also testing situations where you don't have access to the microdata necessary for the nonparametric test. Suppose you were asked to compare two groups of people to gauge whether one is more obese than the other. In an ideal world, you will have height and weight measurements for everybody, and you could form a permutation test stratifying by height. In a less than ideal (i.e., real) world, you may only have the mean height and mean weight in each group (or may be some ranges or variances of these characteristics on top of the sample means). Your best bet is then to compute the mean BMI for each group and compare them if you only have the means; or assume a bivariate normal for height and weight if you have means and variances (you'd probably have to take a correlation from some external data if it did not come with your samples), form some sort of regression lines of weight on height within each group, and check whether one line is above the other."
Impractical question: is it possible to find the regression line using a ruler and compass?,"
The ancient greeks famously sought to construct geometrical relationships using only a ruler and a compass. Given a set of points in a two dimensional plane, is it possible to find the OLS line using only such instruments?
This question has absolutely no practical application that I can think of.
","['regression', 'geometry', 'blue']","Loosely speaking, it's apparently possible to compute any quantity which can be expressed ""using only the integers 0 and 1 and the operations for addition, subtraction, multiplication, division, and square roots"" with only a compass and ruler -- the wikipedia article on constructible numbers has more details. Since the slope of the OLS line definitely has such a closed form, we could deduce it's possible to construct the line.As someone who isn't an expert in compass and ruler constructions, I found this a bit unbelievable, so I gave it a try myself: the green line is the OLS fit for the three blue points, not fitting an intercept for simplicity.You can play around with it here for yourself and drag around the blue points a bit.Here's roughly how the construction went: it turns out you can multiply two numbers by constructing similar triangles. So for each of the three (x,y) points, I computed x^2 on the x-axis and xy on the y-axis (shown in red). Then I simply added up all the x^2's and xy's to get the green point in the top right which defines the OLS line."
KL divergence between two multivariate Gaussians,"
I'm having trouble deriving the KL divergence formula assuming two multivariate normal distributions. I've done the univariate case fairly easily. However, it's been quite a while since I took math stats, so I'm having some trouble extending it to the multivariate case. I'm sure I'm just missing something simple.
Here's what I have...
Suppose both $p$ and $q$ are the pdfs of normal distributions with means $\mu_1$ and $\mu_2$ and variances $\Sigma_1$ and $\Sigma_2$, respectively.
The Kullback-Leibler distance from $q$ to $p$ is:
$\int \left[\log( p(x)) - \log( q(x)) \right]\ p(x)\ dx$, 
which for two multivariate normals is:
$\frac{1}{2}\left[\log\frac{|\Sigma_2|}{|\Sigma_1|} - d + Tr(\Sigma_2^{-1}\Sigma_1) + (\mu_2 - \mu_1)^T \Sigma_2^{-1}(\mu_2 - \mu_1)\right]$
Following the same logic as this proof, I get to about here before I get stuck:
$=\int \left[ \frac{d}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} + \frac{1}{2} \left((x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) - (x-\mu_1)^T\Sigma_2^{-1}(x-\mu_1) \right) \right] \times p(x) dx$
$=\mathbb{E} \left[ \frac{d}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} + \frac{1}{2} \left((x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) - (x-\mu_1)^T\Sigma_2^{-1}(x-\mu_1) \right) \right]$
I think I have to implement the trace trick, but I'm just not sure what to do after that. Any helpful hints to put me back on the right track would be appreciated!
","['mathematical-statistics', 'normal-distribution', 'multivariate-normal-distribution', 'kullback-leibler']","Starting with where you began with some slight corrections, we can write$$
\begin{aligned}
KL &= \int \left[ \frac{1}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} (x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + \frac{1}{2} (x-\mu_2)^T\Sigma_2^{-1}(x-\mu_2) \right] \times p(x) dx \\
&= \frac{1}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} \text{tr}\ \left\{E[(x - \mu_1)(x - \mu_1)^T] \ \Sigma_1^{-1} \right\} + \frac{1}{2} E[(x - \mu_2)^T \Sigma_2^{-1} (x - \mu_2)] \\
&= \frac{1}{2} \log\frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} \text{tr}\ \{I_d \} + \frac{1}{2} (\mu_1 - \mu_2)^T \Sigma_2^{-1} (\mu_1 - \mu_2) + \frac{1}{2} \text{tr} \{ \Sigma_2^{-1} \Sigma_1 \} \\
&= \frac{1}{2}\left[\log\frac{|\Sigma_2|}{|\Sigma_1|} - d + \text{tr} \{ \Sigma_2^{-1}\Sigma_1 \} + (\mu_2 - \mu_1)^T \Sigma_2^{-1}(\mu_2 - \mu_1)\right].
\end{aligned}
$$Note that I have used a couple of properties from Section 8.2 of the Matrix Cookbook."
Practical questions on tuning Random Forests,"
My questions are about Random Forests. The concept of this beautiful classifier is clear to me, but still there are a lot of practical usage questions. Unfortunately, I failed to find any practical guide to RF (I've been searching for something like ""A Practical Guide for Training Restricted Boltzman Machines"" by Geoffrey Hinton, but for Random Forests!
How can one tune RF in practice?
Is it true that bigger number of trees is always better? Is there a reasonable limit (except comp. capacity of course) on increasing number of trees and how to estimate it for given dataset?
What about depth of the trees? How to choose the reasonable one? Is there a sense in experimenting with trees of different length in one forest and what is the guidance for that?
Are there any other parameters worth looking at when training RF? Algos for building individual trees may be?
When they say RF are resistant to overfitting, how true is that?
I'll appreciate any answers and/or links to guides or articles that I might have missed while my search.
","['references', 'random-forest', 'cart']",
Is there any *mathematical* basis for the Bayesian vs frequentist debate?,"
It says on Wikipedia that:

the mathematics [of probability] is largely independent of any interpretation of probability.

Question: Then if we want to be mathematically correct, shouldn't we disallow any interpretation of probability? I.e., are both Bayesian and frequentism mathematically incorrect?
I don't like philosophy, but I do like math, and I want to work exclusively within the framework of Kolmogorov's axioms. If this is my goal, should it follow from what it says on Wikipedia that I should reject both Bayesianism and frequentism? If the concepts are purely philosophical and not at all mathematical, then why do they appear in statistics in the first place?
Background/Context:
This blog post doesn't quite say the same thing, but it does argue that attempting to classify techniques as ""Bayesian"" or ""frequentist"" is counter-productive from a pragmatic perspective.
If the quote from Wikipedia is true, then it seems like from a philosophical perspective attempting to classify statistical methods is also counter-productive -- if a method is mathematically correct, then it is valid to use the method when the assumptions of the underlying mathematics hold, otherwise, if it is not mathematically correct or if the assumptions do not hold, then it is invalid to use it.
On the other hand, a lot of people seem to identify ""Bayesian inference"" with probability theory (i.e. Kolmogorov's axioms), although I'm not quite sure why. Some examples are Jaynes's treatise on Bayesian inference called ""Probability"", as well as James Stone's book ""Bayes' Rule"". So if I took these claims at face value, that means I should prefer Bayesianism.
However, Casella and Berger's book seems like it is frequentist because it discusses maximum likelihood estimators but ignores maximum a posteriori estimators, but it also seems like everything therein is mathematically correct.
So then wouldn't it follow that the only mathematically correct version of statistics is that which refuses to be anything but entirely agnostic with respect to Bayesianism and frequentism? If methods with both classifications are mathematically correct, then isn't it improper practice to prefer some over the others, because that would be prioritizing vague, ill-defined philosophy over precise, well-defined mathematics?
Summary: In short, I don't understand what the mathematical basis is for the Bayesian versus frequentist debate, and if there is no mathematical basis for the debate (which is what Wikipedia claims), I don't understand why it is tolerated at all in academic discourse.
","['probability', 'bayesian', 'frequentist', 'philosophical', 'kolmogorov-axioms']","A probability space $\mathcal{P}$ is by definition a tripple $(\Omega, \mathcal{F}, \mathbb{P} )$ where $\Omega$ is a set of outcomes, $\mathcal{F}$ is a $\sigma$-algebra on the subsets of $\Omega$ and $\mathbb{P}$ is a probability-measure that fulfills the axioms of Kolmogorov, i.e. $\mathbb{P}$ is a function from $\mathcal{F}$ to $[0,1]$ such that $\mathbb{P}(\Omega)=1$ and for disjoint $E_1, E_2, \dots$ in $\mathcal{F}$ it holds that $P \left( \cup_{j=1}^\infty E_j \right)=\sum_{j=1}^\infty \mathbb{P}(E_j)$. Within such a probability space one can, for two events $E_1, E_2$ in $\mathcal{F}$ define the conditional probability as $\mathbb{P}(E_1|_{E_2})\stackrel{def}{=}\frac{\mathbb{P}(E_1 \cap E_2)}{\mathbb{P}(E_2)}$Note that: More detail can be found in this linkFrom the definition of conditional probability it also holds that $\mathbb{P}(E_2|_{E_1})=\frac{\mathbb{P}(E_2 \cap E_1)}{\mathbb{P}(E_1)}$. And from the two latter equations we find Bayes' rule.  So Bayes' rule holds (by definition of conditional probabilty) in any probability space (to show it, derive $\mathbb{P}(E_1 \cap E_2)$ and $\mathbb{P}(E_2 \cap E_1)$ from each equation and equate them (they are equal because intersection is commutative)).  As Bayes rule is the basis for Bayesian inference, one can do Bayesian analysis in any valid (i.e. fulfilling all conditions, a.o. Kolmogorov's axioms) probability space.  The above holds ''in general'', i.e. we have no specific $\Omega$, $\mathcal{F}$, $\mathbb{P}$ in mind as long as $\mathcal{F}$ is a $\sigma$-algebra on subsets of $\Omega$ and $\mathbb{P}$ fulfills Kolmogorov's axioms. We will now show that a ''frequentist'' definition of $\mathbb{P}$ fulfills Kolomogorov's axioms.  If that is the case then ''frequentist'' probabilities are only a special case of Kolmogorov's general and abstract probability.  Let's take an example and roll the dice. Then the set of all possible outcomes $\Omega$ is $\Omega=\{1,2,3,4,5,6\}$. We also need a $\sigma$-algebra on this set $\Omega$ and we take $\mathcal{F}$ the set of all subsets of $\Omega$, i.e. $\mathcal{F}=2^\Omega$. We still have to define the probability measure $\mathbb{P}$ in a frequentist way. Therefore we define $\mathbb{P}(\{1\})$ as $\mathbb{P}(\{1\}) \stackrel{def}{=} \lim_{n \to +\infty} \frac{n_1}{n}$ where $n_1$ is the number of $1$'s obtained in $n$ rolls of the dice. Similar for $\mathbb{P}(\{2\})$, ... $\mathbb{P}(\{6\})$. In this way $\mathbb{P}$ is defined for all singletons in $\mathcal{F}$.  For any other set in $\mathcal{F}$, e.g. $\{1,2\}$ we define $\mathbb{P}(\{1,2\})$ in a frequentist way i.e. 
$\mathbb{P}(\{1,2\}) \stackrel{def}{=} \lim_{n \to +\infty} \frac{n_1+n_2}{n}$, but by the linearity of the 'lim', this is equal to $\mathbb{P}(\{1\})+\mathbb{P}(\{2\})$, which implies that Kolmogorov's axioms hold. So the frequentist definition of probability is only a special case of Kolomogorov's general and abstract definition of a probability measure.  Note that there are other ways to define a probability measure that fulfills Kolmogorov's axioms, so the frequentist definition is not the only possible one. The probability in Kolmogorov's axiomatic system is ''abstract'', it has no real meaning, it only has to fulfill conditions called ''axioms''.  Using only these axioms Kolmogorov was able to derive a very rich set of theorems.  The frequentist definition of probability fullfills the axioms and therefore replacing the abstract, ''meaningless'' $\mathbb{P}$ by a probability defined in a frequentist way, all these theorems are valid because the ''frequentist probability'' is only a special case of Kolmogorov's abstract probability (i.e. it fulfills the axioms). One of the properties that can be derived in Kolmogorov's general framework is Bayes rule.  As it holds in the general and abstract framework, it will also hold (cfr supra) in the specific case that the probabilities are defined in a frequentist way (because the frequentist definition fulfills the axioms and these axioms were the only thing that is needed to derive all theorems).  So one can do Bayesian analysis with a frequentist definition of probability. Defining $\mathbb{P}$ in a frequentist way is not the only possibility, there are other ways to define it such that it fulfills the abstract axioms of Kolmogorov.  Bayes' rule will also hold in these ''specific cases''. So one can also do Bayesian analysis with a non-frequentist definition of probability.@mpiktas reaction to your comment: As I said, the sets $\Omega, \mathcal{F}$ and the probability measure $\mathbb{P}$ have no particular meaning in the axiomatic system, they are abstract.  In order to apply this theory you have to give further definitions (so what you say in your comment ""no need to muddle it further with some bizarre definitions'' is wrong, you need additional definitions). Let's apply it to the case of tossing a fair coin.  The set $\Omega$ in Kolmogorov's theory has no particular meaning, it just has to be ''a set''.  So we must specify what this set is in case of the fair coin, i.e. we must define the set $\Omega$.  If we represent head as H and tail as T, then the set $\Omega$ is by definition $\Omega\stackrel{def}{=}\{H,T\}$. We also have to define the events, i.e. the $\sigma$-algebra $\mathcal{F}$. We define is as $\mathcal{F} \stackrel{def}{=} \{\emptyset, \{H\},\{T\},\{H,T\} \}$. It is easy to verify that $\mathcal{F}$ is a $\sigma$-algebra. Next we must define for every event in $E \in \mathcal{F}$ its measure.  So we need to define a map from $\mathcal{F}$ in $[0,1]$.  I will define it in the frequentist way, for a fair coin, if I toss it a huge number of times, then the fraction of heads will be 0.5, so I define $\mathbb{P}(\{H\})\stackrel{def}{=}0.5$.  Similarly I define $\mathbb{P}(\{T\})\stackrel{def}{=}0.5$, $\mathbb{P}(\{H,T\})\stackrel{def}{=}1$ and $\mathbb{P}(\emptyset)\stackrel{def}{=}0$. Note that $\mathbb{P}$ is a map from $\mathcal{F}$ in $[0,1]$ and that it fulfills Kolmogorov's axioms. For a reference with the frequentist definition of probability see this link (at the end of the section 'definition') and this link."
Why is sample standard deviation a biased estimator of $\sigma$?,"
According to the Wikipedia article on unbiased estimation of standard deviation the sample SD 
$$s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2}$$
is a biased estimator of the SD of the population. It states that $E(\sqrt{s^2}) \neq \sqrt{E(s^2)}$. 
NB. Random variables are independent and each $x_{i} \sim N(\mu,\sigma^{2})$ 
My question is two-fold:

What is the proof of the biasedness?
How does one compute the expectation of the sample standard deviation 

My knowledge of maths/stats is only intermediate.
","['estimation', 'standard-deviation']",
What are some of the most common misconceptions about linear regression?,"
I'm curious, for those of you who have extensive experience collaborating with other researchers, what are some of the most common misconceptions about linear regression that you encounter? 
I think can be a useful exercise to think about common misconceptions ahead of time in order to 


Anticipate people's mistakes and be able to successful articulate why some misconception is incorrect
Realize if I am harboring some misconceptions myself!


A couple of basic ones I can think of:

Independent/Dependent variables must be normally distributed
Variables must be standardized for accurate interpretation

Any others? 
All responses are welcome.
","['regression', 'multiple-regression']",
Is standardization needed before fitting logistic regression?,"
My question is do we need to standardize the data set to make sure all variables have the same scale, between [0,1], before fitting logistic regression. The formula is:  
$$\frac{x_i-\min(x_i)}{\max(x_i)-\min(x_i)}$$
My data set has 2 variables, they describe the same thing for two channels, but the volume is different. Say it's the number of customer visits in two stores, y here is whether a customer purchases. Because a customer can visit both stores, or twice first store, once second store before he makes a purchase. but the total number of customer visits for 1st store is 10 times larger than the second store. When I fit this logistic regression, without standardization,  coef(store1)=37, coef(store2)=13; if I standardize the data, then coef(store1)=133, coef(store2)=11. Something like this. Which approach makes more sense?
What if I am fitting a decision tree model? I know tree structure models don't need standardization since the model itself will adjust it somehow. But checking with all of you. 
","['regression', 'logistic', 'standardization']",
Random Forest - How to handle overfitting,"
I have a computer science background but am trying to teach myself data science by solving problems on the internet. 
I have been working on this problem for the last couple of weeks (approx 900 rows and 10 features). I was initially using logistic regression but now I have switched to random forests. When I run my random forest model on my training data I get really high values for auc (> 99%). However when I run the same model on the test data the results are not so good (Accuracy of approx 77%). This leads me to believe that I am over fitting the training data. 
What are the best practices regarding preventing over fitting in random forests? 
I am using r and rstudio as my development environment. I am using the randomForest package and have accepted defaults for all parameters
","['random-forest', 'overfitting']","To avoid over-fitting in random forest, the main thing you need to do is optimize a tuning parameter that governs the number of features that are randomly chosen to grow each tree from the bootstrapped data. Typically, you do this via $k$-fold cross-validation, where $k \in \{5, 10\}$, and choose the tuning parameter that minimizes test sample prediction error. In addition, growing a larger forest will improve predictive accuracy, although there are usually diminishing returns once you get up to several hundreds of trees."
Do the predictions of a Random Forest model have a prediction interval?,"
If I run a randomForest model, I can then make predictions based on the model.  Is there a way to get a prediction interval of each of the predictions such that I know how ""sure"" the model is of its answer.  If this is possible is it simply based on the variability of the dependent variable for the whole model or will it have wider and narrower intervals depending on the particular decision tree that was followed for a particular prediction?   
","['r', 'confidence-interval', 'random-forest']","This is partly a response to @Sashikanth Dareddy (since it will not fit in a comment) and partly a response to the original post.Remember what a prediction interval is, it is an interval or set of values where we predict that future observations will lie.  Generally the prediction interval has 2 main pieces that determine its width, a piece representing the uncertainty about the predicted mean (or other parameter) this is the confidence interval part, and a piece representing the variability of the individual observations around that mean.  The confidence interval is fairy robust due to the Central Limit Theorem and in the case of a random forest, the bootstrapping helps as well.  But the prediction interval is completely dependent on the assumptions about how the data is distributed given the predictor variables, CLT and bootstrapping have no effect on that part.The prediction interval should be wider where the corresponding confidence interval would also be wider.  Other things that would affect the width of the prediction interval are assumptions about equal variance or not, this has to come from the knowledge of the researcher, not the random forest model.A prediction interval does not make sense for a categorical outcome (you could do a prediction set rather than an interval, but most of the time it would probably not be very informative).  We can see some of the issues around prediction intervals by simulating data where we know the exact truth.  Consider the following data:This particular data follows the assumptions for a linear regression and is fairly straight forward for a random forest fit.  We know from the ""true"" model that when both predictors are 0 that the mean is 10, we also know that the individual points follow a normal distribution with standard deviation of 1.  This means that the 95% prediction interval based on perfect knowledge for these points would be from 8 to 12 (well actually 8.04 to 11.96, but rounding keeps it simpler).  Any estimated prediction interval should be wider than this (not having perfect information adds width to compensate) and include this range.Let's look at the intervals from regression:We can see there is some uncertainty in the estimated means (confidence interval) and that gives us a prediction interval that is wider (but includes) the 8 to 12 range.Now let's look at the interval based on the individual predictions of individual trees (we should expect these to be wider since the random forest does not benefit from the assumptions (which we know to be true for this data) that the linear regression does):The intervals are wider than the regression prediction intervals, but they don't cover the entire range.  They do include the true values and therefore may be legitimate as confidence intervals, but they are only predicting where the mean (predicted value) is, no the added piece for the distribution around that mean.  For the first case where x1 and x2 are both 0 the intervals don't go below 9.7, this is very different from the true prediction interval that goes down to 8.  If we generate new data points then there will be several points (much more than 5%) that are in the true and regression intervals, but don't fall in the random forest intervals.To generate a prediction interval you will need to make some strong assumptions about the distribution of the individual points around the predicted means, then you could take the predictions from the individual trees (the bootstrapped confidence interval piece) then generate a random value from the assumed distribution with that center.  The quantiles for those generated pieces may form the prediction interval (but I would still test it, you may need to repeat the process several more times and combine).Here is an example of doing this by adding normal (since we know the original data used a normal) deviations to the predictions with the standard deviation based on the estimated MSE from that tree:These intervals contain those based on perfect knowledge, so look reasonable.  But, they will depend greatly on the assumptions made (the assumptions are valid here because we used the knowledge of how the data was simulated, they may not be as valid in real data cases).  I would still repeat the simulations several times for data that looks more like your real data (but simulated so you know the truth) several times before fully trusting this method."
"What makes the Gaussian kernel so magical for PCA, and also in general?","
I was reading about kernel PCA (1, 2, 3) with Gaussian and polynomial kernels. 

How does the Gaussian kernel separate seemingly any sort of nonlinear data exceptionally well? Please give an intuitive analysis, as well as a mathematically involved one if possible.
What is a property of the Gaussian kernel (with ideal $\sigma$) that other kernels don't have? Neural networks, SVMs, and RBF networks come to mind. 
Why don't we put the norm through, say, a Cauchy PDF and expect the same results?

","['machine-learning', 'pca', 'svm', 'kernel-trick']",
How to statistically compare two time series?,"
I have two time series, shown in the plot below:

The plot is showing the full detail of both time series, but I can easily reduce it to just the coincident observations if needed.
My question is: What statistical methods can I use to assess the differences between the time series?
I know this is a fairly broad and vague question, but I can't seem to find much introductory material on this anywhere. As I can see it, there are two distinct things to assess:
1. Are the values the same?
2. Are the trends the same?
What sort of statistical tests would you suggest looking at to assess these questions? For question 1 I can obviously assess the means of the different datasets and look for significant differences in distributions, but is there a way of doing this that takes into account the time-series nature of the data?
For question 2 - is there something like the Mann-Kendall tests that looks for the similarity between two trends? I could do the Mann-Kendall test for both datasets and compare, but I don't know if that is a valid way to do things, or whether there is a better way?
I'm doing all of this in R, so if tests you suggest have a R package then please let me know.
","['r', 'time-series']","As others have stated, you need to have a common frequency of measurement (i.e. the time between observations). With that in place I would identify a common model that would reasonably describe each series separately. This might be an ARIMA model or a multiply-trended  Regression Model with possible Level Shifts or a composite model integrating both memory (ARIMA) and dummy variables. This common model could be estimated globally and separately for each of the two series and then one could construct an F test to test the hypothesis of a common set of parameters."
Maximum likelihood method vs. least squares method,"
What is the main difference between maximum likelihood estimation (MLE) vs. least squares estimaton (LSE) ?
Why can't we use MLE for predicting $y$ values in linear regression and vice versa?
Any help on this topic will be greatly appreciated.
","['regression', 'estimation', 'maximum-likelihood', 'least-squares']",
Model for predicting number of Youtube views of Gangnam Style,"
PSY's music video ""Gangnam style"" is popular, after a little more than 2 months it has about 540 million viewers. I learned this from my preteen children at dinner last week and soon the discussion went in the direction of if it was possible to do some kind of prediction of how many viewers there will be in 10-12 days and when(/if) the song will pass 800 million viewers or 1 billion viewers.  
Here is the picture from number of viewers since it was posted: 

Here are the picture from number of viewers of the No1 ""Justin Biever-Baby""and No2 ""Eminem - Love the way you lie"" music videos that both have been around for a much longer time


My first attempt to reason about the model was that is should be a S-curve but this doesn't seem to fit the the No1 and No2 songs and also doesn't fit that there are no limit on how many views that the music video can have, only a slower growth. 
So my question is: what kind of model should I use to predict number of viewers of the music video? 
","['modeling', 'internet']","Aha, excellent question!!I would also have naively proposed an S-shaped logisitic curve, but this is obviously a poor fit. As far as I know, the constant increase is an approximation because YouTube counts the unique views (one per IP address), so there cannot be more views than computers.We could use an epidemiological model where people have different susceptibility. To make it simple, we could divide it in the high risk group (say the children) and the low risk group (say the adults). Let's call $x(t)$ the proportion of ""infected"" children and $y(t)$ the proportion of ""infected"" adults at time $t$. I will call $X$ the (unknown) number of individuals in the high risk group and $Y$ the (also unknown) number of individuals in the low risk group.$$\dot{x}(t) = r_1(x(t)+y(t))(X-x(t))$$
$$\dot{y}(t) = r_2(x(t)+y(t))(Y-y(t)),$$where $r_1 > r_2$. I don't know how to solve that system (maybe @EpiGrad would), but looking at your graphs, we could make a couple of simplifying assumptions. Because the growth does not saturate, we can assume that $Y$ is very large and $y$ is small, or$$\dot{x}(t) = r_1x(t)(X-x(t))$$
$$\dot{y}(t) = r_2x(t),$$which predicts linear growth once the high risk group is completely infected. Note that with this model there is no reason to assume $r_1 > r_2$, quite the contrary because the large term $Y-y(t)$ is now subsumed in $r_2$.This system solves to$$x(t) = X \frac{C_1e^{Xr_1t}}{1 + C_1e^{Xr_1t}}$$
$$y(t) = r_2 \int x(t)dt + C_2 = \frac{r_2}{r_1} \log(1+C_1e^{Xr_1t})+C_2,$$where $C_1$ and $C_2$ are integration constants. The total ""infected"" population is then
$x(t) + y(t)$, which has 3 parameters and 2 integration constants (initial conditions). I don't know how easy it would be to fit...Update: playing around with the parameters, I could not reproduce the shape of the top curve with this model, the transition from $0$ to $600,000,000$ is always sharper than above. Continuing with the same idea, we could again assume that there are two kinds of Internet users: the ""sharers"" $x(t)$ and the ""loners"" $y(t)$. The sharers infect each other, the loners bump into the video by chance. The model is$$\dot{x}(t) = r_1x(t)(X-x(t))$$
$$\dot{y}(t) = r_2,$$and solves to $$x(t) = X \frac{C_1e^{Xr_1t}}{1 + C_1e^{Xr_1t}}$$
$$y(t) = r_2 t+C_2.$$We could assume that $x(0) = 1$, i.e. that there is only patient 0 at $t=0$, which yields $C_1 = \frac{1}{X-1} \approx \frac{1}{X}$ because $X$ is a large number. $C_2 = y(0)$ so we can assume that $C_2 = 0$. Now only the 3 parameters $X$, $r_1$ and $r_2$ determine the dynamics.Even with this model, it seems that the inflection is very sharp, it is not a good fit so the model must be wrong. That makes the problem very interesting actually. As an example, the figure below was built with $X = 600,000,000$, $r_1 = 3.667 \cdot 10^{-10}$ and $r_2 = 1,000,000$.Update: From the comments I gathered that Youtube counts views (in its secret way) and not unique IPs, which makes a big difference. Back to the drawing board.To keep it simple, let's assume that the viewers are ""infected"" by the video. They come back to watch it regularly, until they clear the infection. One of the simplest models is the SIR (Susceptible-Infected-Resistant) which is the following:$$\dot{S}(t) = -\alpha S(t)I(t)$$
$$\dot{I}(t) = \alpha S(t)I(t) - \beta I(t)$$
$$\dot{R}(t) = \beta I(t)$$where $\alpha$ is the rate of infection and $\beta$ is the rate of clearance. The total view count $x(t)$ is such that $\dot{x}(t) = kI(t)$, where $k$ is the average views per day per infected individual.In this model, the view count starts increasing abruptly some time after the onset of the infection, which is not the case in the original data, perhaps because videos also spread in a non viral (or meme) way. I am no expert in estimating the parameters of the SIR model. Just playing with different values, here is what I came up with (in R).The model is obviously not perfect, and could be complemented in many sound ways. This very rough sketch predicts a billion views somewhere around March 2013, let's see..."
Which activation function for output layer?,"
While the choice of activation functions for the hidden layer is quite clear (mostly sigmoid or tanh), I wonder how to decide on the activation function for the output layer. Common choices are linear functions, sigmoid functions and softmax functions. However, when should I use which one?
",['neural-networks'],"Use simple sigmoid only if your output admits multiple ""true"" answers, for instance, a network that checks for the presence of various objects in an image. In other words, the output is not a probability distribution (does not need to sum to 1)."
Hold-out validation vs. cross-validation,"
To me, it seems that hold-out validation is useless. That is, splitting the original dataset into two-parts (training and testing) and using the testing score as a generalization measure, is somewhat useless. 
K-fold cross-validation seems to give better approximations of generalization (as it trains and tests on every point). So, why would we use the standard hold-out validation? Or even talk about it?
","['machine-learning', 'cross-validation', 'validation']","NOTE: This answer is old, incomplete, and thoroughly out of date. Its was only debatably correct when it was posted in 2014, and I'm not really sure how it got so many upvotes or how it became the accepted answer. I recommend this answer instead, written by an expert in the field (and with significantly more upvotes). I am leaving my answer here for historical/archival purposes only.My only guess is that you can Hold-Out with three hours of programming experience; the other takes a week in principle and six months in practice.In principle it's simple, but writing code is tedious and time-consuming. As Linus Torvalds famously said, ""Bad programmers worry about the code. Good programmers worry about data structures and their relationships."" Many of the people doing statistics are bad programmers, through no fault of their own. Doing k-fold cross validation efficiently (and by that I mean, in a way that isn't horribly frustrating to debug and use more than once) in R requires a vague understanding of data structures, but data structures are generally skipped in ""intro to statistical programming"" tutorials. It's like the old person using the Internet for the first time. It's really not hard, it just takes an extra half hour or so to figure out the first time, but it's brand new and that makes it confusing, so it's easy to ignore.You have questions like this: How to implement a hold-out validation in R. No offense intended, whatsoever, to the asker. But many people just are not code-literate. The fact that people are doing cross-validation at all is enough to make me happy.It sounds silly and trivial, but this comes from personal experience, having been that guy and having worked with many people who were that guy."
Standard error for the mean of a sample of binomial random variables,"
Suppose I'm running an experiment that can have 2 outcomes, and I'm assuming that the underlying ""true"" distribution of the 2 outcomes is a binomial distribution with parameters $n$ and $p$: ${\rm Binomial}(n, p)$. 
I can compute the standard error, $SE_X = \frac{\sigma_X}{\sqrt{n}}$,  from the form of the variance of  ${\rm Binomial}(n, p)$: $$ \sigma^{2}_{X} = npq$$ 
where $q = 1-p$. So, $\sigma_X=\sqrt{npq}$. For the standard error I get: $SE_X=\sqrt{pq}$, but I've seen somewhere that $SE_X = \sqrt{\frac{pq}{n}}$. What did I do wrong?
","['binomial-distribution', 'standard-error']","It seems like you're using $n$ twice in two different ways - both as the sample size and as the number of bernoulli trials that comprise the Binomial random variable; to eliminate any ambiguity, I'm going to use $k$ to refer to the latter. If you have $n$ independent samples from a ${\rm Binomial}(k,p)$ distribution, the variance of their sample mean is$$ {\rm var} \left( \frac{1}{n} \sum_{i=1}^{n} X_{i} \right) = \frac{1}{n^2} \sum_{i=1}^{n} {\rm var}( X_{i} ) = \frac{ n {\rm var}(X_{i}) }{ n^2 } = \frac{ {\rm var}(X_{i})}{n} = \frac{ k pq }{n} $$where $q=1-p$ and $\overline{X}$ is the same mean. This follows since (1) ${\rm var}(cX) = c^2 {\rm var}(X)$, for any random variable, $X$, and any constant $c$. (2) the variance of a sum of independent random variables equals the sum of the variances. The standard error of $\overline{X}$is the square root of the variance: $\sqrt{\frac{ k pq }{n}}$. Therefore, When $k = n$, you get the formula you pointed out: $\sqrt{pq}$When $k = 1$, and the Binomial variables are just bernoulli trials, you get the formula you've seen elsewhere: $\sqrt{\frac{pq }{n}}$"
Why is ANOVA equivalent to linear regression?,"
I read that ANOVA and linear regression are the same thing. How can that be, considering that the output of ANOVA is some $F$ value and some $p$-value based on which you conclude if the sample means across the different samples are same or different.
But assuming the means are not equal (reject null hypothesis), ANOVA tells you nothing about the coefficients of the linear model. So how is linear regression the same as ANOVA?
","['regression', 'anova']","ANOVA and linear regression are equivalent when the two models test against the same hypotheses and use an identical encoding. The models differ in their basic aim: ANOVA is mostly concerned to present differences between categories' means in the data while linear regression is mostly concern to estimate a sample mean response and an associated $\sigma^2$.Somewhat aphoristically one can describe ANOVA as a regression with dummy variables. We can easily see that this is the case in the simple regression with categorical variables. A categorical variable will be encoded as a indicator matrix (a matrix of 0/1 depending on whether a subject is part of a given group or not) and then used directly for the solution of the linear system described by a linear regression.
Let's see an example with 5 groups. For the sake of argument I will assume that the mean of group1 equals 1,  the mean of group2 equals 2, ... and the mean of group5 equals 5. (I use MATLAB, but the exact same thing is equivalent in R.)As it can be seen in this scenario the results where exactly the same. The minute numerical difference is due to the design not being perfectly balanced as well as the underlaying estimation procedure; the ANOVA accumulates numerical errors a bit more aggressively. To that respect we fit an intercept, LRbetas(1); we could fit an intercept-free model but that would not be a ""standard"" linear regression. (The results would be even closer to ANOVA in that case though.)The $F$-statistic (a ratio of the means) in the case of the ANOVA and in the case of linear regression will be also be the same for the above example: This is because procedures test the same hypothesis but with different wordings: ANOVA will qualitatively check if ""the ratio is high enough to suggest that no grouping is implausible"" while linear regression will qualitatively check if ""the ratio is high enough to suggest an intercept only model is possibly inadequate"".
(This is a somewhat free interpretation of the ""possibility to see a value equal or greater than the one observed under the null hypothesis"" and it is not meant to be a text-book definition.)Coming back to the final part of your question about ""ANOVA tell(ing) you nothing about the coefficients of the linear model (assuming the means are not equal"") I hope you can now see that the ANOVA, in the case that your design is simple/balanced enough, tells you everything that a linear model would. The confidence intervals for group means will be the same you have for your $\beta$, etc. Clearly when ones starts adding multiple covariate in his regression model, a simple one-way ANOVA does not have a direct equivalence. In that case one augments the information used to calculate the linear regression's mean response with information that are not directly available for a one way ANOVA. I believe that one can re-express things in ANOVA terms once more but it is mostly an academic exercise.An interesting paper on the matter is Gelman's 2005 paper titled: Analysis of Variance - Why it is more important than ever. Some important points raised; I am not fully supportive of the paper (I think I personally align much more with McCullach's view) but it can be a constructive read.As a final note: The plot thickens when you have mixed effects models. There you have different concepts about what can be considered a nuisance or actual information regarding the grouping of your data. These issues are outside the scope of this question but I think they are worthy of a nod. "
What's the difference between feed-forward and recurrent neural networks?,"
What is the difference between a feed-forward and recurrent neural network?  
Why would you use one over the other? 
Do other network topologies exist?  
","['machine-learning', 'neural-networks', 'terminology', 'recurrent-neural-network', 'topologies']","Feed-forward ANNs allow signals to travel one way only: from input to output. There are no feedback (loops); i.e., the output of any layer does not affect that same layer. Feed-forward ANNs tend to be straightforward networks that associate inputs with outputs. They are extensively used in pattern recognition. This type of organisation is also referred to as bottom-up or top-down.Feedback (or recurrent or interactive) networks can have signals traveling in both directions by introducing loops in the network. Feedback networks are powerful and can get extremely complicated. Computations derived from earlier input are fed back into the network, which gives them a kind of memory. Feedback networks are dynamic; their 'state' is changing continuously until they reach an equilibrium point. They remain at the equilibrium point until the input changes and a new equilibrium needs to be found.Feedforward neural networks are ideally suitable for modeling relationships between a set of predictor or input variables and one or more response or output variables. In other words, they are appropriate for any functional mapping problem where we want to know how a number of input variables affect the output variable. The multilayer feedforward neural networks, also called multi-layer perceptrons (MLP), are the most widely studied and used neural network model in practice.As an example of feedback network, I can recall Hopfield’s network. The main use of Hopfield’s network is as associative memory. An associative memory is a device which accepts an input pattern and generates an output as the stored pattern which is most closely associated with the input. The function of the associate memory is to recall the corresponding stored pattern, and then produce a clear version of the pattern at the output. Hopfield networks are typically used for those problems with binary pattern vectors and the input pattern may be a noisy version of one of the stored patterns. In the Hopfield network, the stored patterns are encoded as the weights of the network.Kohonen’s self-organizing maps (SOM) represent another neural network type that is markedly different from the feedforward multilayer networks. Unlike training in the feedforward MLP, the SOM training or learning is often called unsupervised because there are no known target outputs associated with each input pattern in SOM and during the training process, the SOM processes the input patterns and learns to cluster or segment the data through adjustment of weights (that makes it an important neural network model for dimension reduction and data clustering). A two-dimensional map is typically created in such a way that the orders of the interrelationships among inputs are preserved. The number and composition of clusters can be visually determined based on the output distribution generated by the training process. With only input variables in the training sample, SOM aims to learn or discover the underlying structure of the data.(The diagrams are from Dana Vrajitoru's C463 / B551 Artificial Intelligence web site.)"
A psychology journal banned p-values and confidence intervals; is it indeed wise to stop using them?,"
On 25 February 2015, the journal Basic and Applied Social Psychology issued an editorial banning $p$-values and confidence intervals from all future papers.
Specifically, they say (formatting and emphasis are mine):


[...] prior to publication, authors will have to remove all
  vestiges of the NHSTP [null hypothesis significance testing procedure] ($p$-values, $t$-values, $F$-values, statements about ‘‘significant’’ differences or lack thereof, and so on).
Analogous to how the NHSTP fails to provide the probability of the null hypothesis, which is needed to provide a strong case for rejecting it, confidence intervals do not
  provide a strong case for concluding that the population
  parameter of interest is likely to be within the stated
  interval. Therefore, confidence intervals also are banned
  from BASP.
[...] with respect to Bayesian procedures, we reserve the right to make case-by-case
  judgments, and thus Bayesian procedures are neither
  required nor banned from BASP.
[...] Are any inferential statistical procedures
  required? -- No [...] However, BASP will require strong
  descriptive statistics, including effect sizes.


Let us not discuss problems with and misuse of $p$-values here; there already are plenty of excellent discussions on CV that can be found by browsing the p-value tag. The critique of $p$-values often goes together with an advice to report confidence intervals for parameters of interest. For example, in this very well-argued answer @gung suggests to report effect sizes with confidence intervals around them. But this journal bans confidence intervals as well. 
What are the advantages and disadvantages of such an approach to presenting data and experimental results as opposed to the ""traditional"" approach with $p$-values, confidence intervals, and significant/insignificant dichotomy? The reaction to this ban seems to be mostly negative; so what are the disadvantages then? American Statistical Association has even posted a brief discouraging comment on this ban, saying that ""this policy may have its own negative consequences"". What could these negative consequences be? 
Or as @whuber suggested to put it, should this approach be advocated generally as a paradigm of quantitative research? And if not, why not?
PS. Note that my question is not about the ban itself; it is about the suggested approach. I am not asking about frequentist vs. Bayesian inference either. The Editorial is pretty negative about Bayesian methods too; so it is essentially about using statistics vs. not using statistics at all.

Other discussions: reddit, Gelman.
","['hypothesis-testing', 'confidence-interval', 'p-value', 'effect-size', 'psychology']",
"How to split the dataset for cross validation, learning curve, and final evaluation?","
What is an appropriate strategy for splitting the dataset?
I ask for feedback on the following approach (not on the individual parameters like test_size or n_iter, but if I used X, y, X_train, y_train, X_test, and y_test appropriately and if the sequence makes sense):
(extending this example from the scikit-learn documentation)
1. Load the dataset
from sklearn.datasets import load_digits
digits = load_digits()
X, y = digits.data, digits.target

2. Split into training and test set (e.g., 80/20)
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

3. Choose estimator
from sklearn.svm import SVC
estimator = SVC(kernel='linear')

4. Choose cross-validation iterator
from sklearn.cross_validation import ShuffleSplit
cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2, random_state=0)

5. Tune the hyperparameters
applying the cross-validation iterator on the training set
from sklearn.grid_search import GridSearchCV
import numpy as np
gammas = np.logspace(-6, -1, 10)
classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=dict(gamma=gammas))
classifier.fit(X_train, y_train)

6. Debug algorithm with learning curve
X_train is randomly split into a training and a test set 10 times (n_iter=10). Each point on the training-score curve is the average of 10 scores where the model was trained and evaluated on the first i training examples. Each point on the cross-validation score curve is the average of 10 scores where the model was trained on the first i training examples and evaluated on all examples of the test set. 
from sklearn.learning_curve import learning_curve
title = 'Learning Curves (SVM, linear kernel, $\gamma=%.6f$)' %classifier.best_estimator_.gamma
estimator = SVC(kernel='linear', gamma=classifier.best_estimator_.gamma)
plot_learning_curve(estimator, title, X_train, y_train, cv=cv)
plt.show()


plot_learning_curve() can be found in the current dev version of scikit-learn (0.15-git).
7. Final evaluation on the test set
classifier.score(X_test, y_test)

7a. Test over-fitting in model selection with nested cross-validation (using the whole dataset)
from sklearn.cross_validation import cross_val_score
cross_val_score(classifier, X, y)

Additional question: Does it make sense to replace step 7 by nested cross-validation? Or should nested cv be seen as complementary to step 7
(the code seems to work with k-fold cross validation in scikit-learn, but not with shuffle & split. So cv needs to be changed above to make the code work)
8. Train final model on whole dataset
classifier.fit(X, y)

EDIT: I now agree with cbeleites that step 7a doesn't make much sense in this sequence. So I wouldn't adopt that.
","['machine-learning', 'cross-validation', 'python', 'scikit-learn']","I'm not sure what you want to do in step 7a. As I understand it right now, it doesn't make sense to me. Here's how I understand your description: in step 7, you want to compare the hold-out performance with the results of a cross validation embracing steps 4 - 6. (so yes, that would be a nested setup). The main points why I don't think this comparison makes much sense are:This comparison cannot detect two of the main sources of overoptimistic validation results I encounter in practice:data leaks (dependence) between training and test data which is caused by a hierarchical (aka clustered) data structure, and which is not accounted for in the splitting. In my field, we have typically multiple (sometimes thousands) of readings (= rows in the data matrix) of the same patient or biological replicate of an experiment. These are not independent, so the validation splitting needs to be done at patient level. However, such a data leak occurs, you'll have it both in the splitting for the hold out set and in the cross validation splitting. Hold-out wold then be just as optimistically biased as cross validation. Preprocessing of the data done on the whole data matrix, where the calculations are not independent for each row but many/all rows are used to calculation parameters for the preprocessing. Typical examples would be e.g. a PCA projection before the ""actual"" classification.
Again, that would affect both your hold-out and the outer cross validation, so you cannot detect it. For the data I work with, both errors can easily cause the fraction of misclassifications to be underestimated by an order of magnitude!If you are restricted to this counted fraction of test cases type of performance, model comparisons need either extremely large numbers of test cases or ridiculously large differences in true performance. Comparing 2 classifiers with unlimited training data may be a good start for further reading.However, comparing the model quality the inner cross validation claims for the ""optimal"" model and the outer cross validation or hold out validation does make sense: if the discrepancy is high, it is questionable whether your grid search optimization did work (you may have skimmed variance due to the high variance of the performance measure). This comparison is easier in that you can spot trouble if you have the inner estimate being ridiculously good compared to the other - if it isn't, you don't need to worry that much about your optimization. But in any case, if your outer (7) measurement of the performance is honest and sound, you at least have a useful estimate of the obtained model, whether it is optimal or not.IMHO measuring the learning curve is yet a different problem. I'd probably deal with that separately, and I think you need to define more clearly what you need the learning curve for (do you need the learning curve for a data set of the given problem, data, and classification method or the learning curve for this data set of the given problem, data, and classification mehtod), and a bunch of further decisions (e.g. how to deal with the model complexity as function of the training sample size? Optimize all over again, use fixed hyperparameters, decide on function to fix hyperparameters depending on training set size?) (My data usually has so few independent cases to get the measurement of the learning curve sufficiently precise to use it in practice - but you may be better of if your 1200 rows are actually independent)First of all, nothing is wrong with nested cross validation here. Nested validation is of utmost importance for data-driven optimization, and cross validation is a very powerful approaches (particularly if iterated/repeated).Then, whether anything is wrong at all depends on your point of view: as long as you do an honest nested validation (keeping the outer test data strictly independent), the outer validation is a proper measure of the ""optimal"" model's performance. Nothing wrong with that. But several things can and do go wrong with grid search of these proportion-type performance measures for hyperparameter tuning of SVM. Basically they mean that you may (probably?) cannont rely on the optimization. Nevertheless, as long as your outer split was done properly, even if the model is not the best possible, you have an honest estimate of the performance of the model you got. I'll try to give intuitive explanations why the optimization may be in trouble:Mathematically/statisticaly speaking, the problem with the proportions is that measured proportions $\hat p$ are subject to a huge variance due to finite test sample size $n$ (depending also on the true performance of the model, $p$):
$Var (\hat p) = \frac{p (1 - p)}{n}$You need ridiculously huge numbers of cases (at least compared to the numbers of cases I can usually have) in order to achieve the needed precision (bias/variance sense) for estimating recall, precision (machine learning performance sense). This of course applies also to ratios you calculate from such proportions. Have a look at the confidence intervals for binomial proportions. They are shockingly large! Often larger than the true improvement in performance over the hyperparameter grid. 
And statistically speaking, grid search is a massive multiple comparison problem: the more points of the grid you evaluate, the higher the risk of finding some combination of hyperparameters that accidentally looks very good for the train/test split you are evaluating. This is what I mean with skimming variance. The well known optimistic bias of the inner (optimization) validation is just a symptom of this variance skimming. Intuitively, consider a hypothetical change of a hyperparameter, that slowly causes the model to deteriorate: one test case moves towards the decision boundary. The 'hard' proportion performance measures do not detect this until the case crosses the border and is on the wrong side. Then, however, they immediately assign a full error for an infinitely small change in the hyperparameter.
In order to do numerical optimization, you need the performance measure to be well behaved. That means: neither the jumpy (not continously differentiable) part of the proportion-type performance measure nor the fact that other than that jump, actually occuring changes are not detected are suitable for the optimization.
Proper scoring rules are defined in a way that is particularly suitable for optimization. They have their global maximum when the predicted probabilities match the true probabilities for each case to belong to the class in question. For SVMs you have the additional problem that not only the performance measures but also the model reacts in this jumpy fashion: small changes of the hyperparameter will not change anything. The model changes only when the hyperparameters are changes enough to cause some case to either stop being support vector or to become support vector. Again, such models are hard to optimize.  Literature:  Gneiting, T. & Raftery, A. E.: Strictly Proper Scoring Rules, Prediction, and Estimation, Journal of the American Statistical Association, 102, 359-378 (2007). DOI: 10.1198/016214506000001437 Brereton, R.: Chemometrics for pattern recognition, Wiley, (2009).
points out the jumpy behaviour of the SVM as function of the hyperparameters.what you can afford in terms of model comparison obviously depends on the number of independent cases. 
Let's make some quick and dirty simulation about the risk of skimming variance here: scikit.learn says that they have 1797 are in the digits data. i.e., all models have the same true performance of, say, 97 % (typical performance for the digits data set).Run $10^4$ simulations of ""testing these models"" with sample size = 1797 rows in the digits data setHere's the distribution for the best observed performance:The red line marks the true performance of all our hypothetical models. On average, we observe only 2/3 of the true error rate for the seemingly best of the 100 compared models (for the simulation we know that they all perform equally with 97% correct predictions). This simulation is obviously very much simplified:In general, however, both low number of independent test cases and high number of compared models increase the bias. Also, the Cawley and Talbot paper gives empirical observed behaviour. "
Good GUI for R suitable for a beginner wanting to learn  programming in R?,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






Is there any GUI for R that makes it easier for a beginner to start learning and programming in that language?
",['r'],"I would second @Shane's recommendation for Deducer, and would also recommend the R Commander by John Fox.  The CRAN package is here.  It's called the R ""Commander"" because it returns the R commands associated with the point-and-click menu selections, which can be saved and run later from the command prompt.  In this way, if you don't know how to do something then you can find it in the menus and get an immediate response for the proper way to do something with R code. It looks like Deducer operates similarly, though I haven't played with Deducer for a while.The base R Commander is designed for beginner-minded tasks, but there are plugins available for some more sophisticated analyses (Deducer has plugins, too). Bear in mind, however, that no GUI can do everything, and at some point the user will need to wean him/herself from pointing-and-clicking.  Some people (myself included) think that is a good thing."
Having a job in data-mining without a PhD,"
I've been very interested in data-mining and machine-learning for a while, partly because I majored in that area at school, but also because I am truly much more excited trying to solve problems that require a bit more thought than just programming knowledge and whose solution can have multiple forms. I don't have a researcher/scientist background, I come from a computer science background with an emphasis on data analysis, I have a Master's degree and not a PhD. I currently have a position related to data analysis, even if that is not the primary focus of what I'm doing, but I have at least some good exposure to it.
As I was interviewing some time ago for a job with several companies, and got to talk with a few recruiters, I found a common pattern that people seem to think that you need to have a PhD to do machine learning, even if I may be generalizing a bit too much (some companies were not really looking especially for PhDs).
While I think it's good to have a PhD in that area, I don't think this is absolutely necessary. I have some pretty decent knowledge of most real-world machine learning algorithms, have implemented most of them myself (either at school or on personal projects), and feel pretty confident when approaching problems involving machine-learning / data-mining and statistics in general. And I have some friends with a similar profile who seem very knowledgeable about this also, but also feel that in general companies are pretty shy about hiring in data-mining if you're not a PhD.
I'd like to get some feedback, do you think a PhD is absolutely necessary to have a job very focused in that area?
(I hesitated a bit before posting this question here, but since it seems to be an acceptable topic on meta, I've decided to post this question on which I've been thinking for a while.)
","['machine-learning', 'data-mining', 'careers', 'phd']","I believe actually the opposite of your conclusion is true. In The Disposable Academic, several pointers are given about the low wage premium in applied math, math, and computer science for PhD holders over master's degree holders. In part, this is because companies are realizing that master's degree holders usually have just as much theoretical depth, better programming skills, and are more pliable and can be trained for their company's specific tasks. It's not easy to get an SVM disciple, for instance, to appreciate your company's infrastructure that relies on decision trees, say. Often, when someone has dedicated tons of time to a particular machine learning paradigm, they have a hard time generalizing their productivity to other domains.Another problem is that a lot of machine learning jobs these days are all about getting things done, and not so much about writing papers or developing new methods. You can take a high risk approach to developing new mathematical tools, studying VC-dimensional aspects of your method, its underlying complexity theory, etc. But in the end, you might not get something that practitioners will care about. Meanwhile, look at something like poselets. Basically no new math arises from poselets at all. It's entirely unelegant, clunky, and lacks any mathematical sophistication. But it scales up to large data sets amazingly well and it's looking like it will be a staple in pose recognition (especially in computer vision) for some time to come. Those researchers did a great job and their work is to be applauded, but it's not something most people associate with a machine learning PhD.With a question like this, you'll get tons of different opinions, so by all means consider them all. I am currently a PhD student in computer vision, but I've decided to leave my program early with a master's degree, and I'll be working for an asset management company doing natural language machine learning, computational statistics, etc. I also considered ad-based data mining jobs at several large TV companies, and a few robotics jobs. In all of these domains, there are plenty of jobs for someone with mathematical maturity and a knack for solving problems in multiple programming languages. Having a master's degree is just fine. And, according to that Economist article, you'll be paid basically just as well as someone with a PhD. And if you work outside of academia, bonuses and getting to promotions faster than someone who spends extra years on a PhD can often mean your overall lifetime earnings are higher.As Peter Thiel once said, ""Graduate school is like hitting the snooze button on the alarm clock of life..."""
Should I use a categorical cross-entropy or binary cross-entropy loss for binary predictions?,"
First of all, I realized if I need to perform binary predictions, I have to create at least two classes through performing a one-hot-encoding. Is this correct? However, is binary cross-entropy only for predictions with only one class? If I were to use a categorical cross-entropy loss, which is typically found in most libraries (like TensorFlow), would there be a significant difference?
In fact, what are the exact differences between a categorical and binary cross-entropy? I have never seen an implementation of binary cross-entropy in TensorFlow, so I thought perhaps the categorical one works just as fine.
","['machine-learning', 'neural-networks', 'loss-functions', 'tensorflow', 'cross-entropy']","Bernoulli$^*$ cross-entropy loss is a special case of categorical cross-entropy loss for $m=2$.$$
\begin{align}
\mathcal{L}(\theta)
&= -\frac{1}{n}\sum_{i=1}^n\sum_{j=1}^m y_{ij}\log(p_{ij}) \\
&= -\frac{1}{n}\sum_{i=1}^n \left[y_i \log(p_i) + (1-y_i) \log(1-p_i)\right] 
\end{align}
$$Where $i$ indexes samples/observations and $j$ indexes classes, and $y$ is the sample label (binary for LSH, one-hot vector on the RHS) and $p_{ij}\in(0,1):\sum_{j} p_{ij} =1\forall i,j$ is the prediction for a sample.I write ""Bernoulli cross-entropy"" because this loss arises from a Bernoulli probability model. There is not a ""binary distribution."" A ""binary cross-entropy"" doesn't tell us if the thing that is binary is the one-hot vector of $k \ge 2$ labels, or if the author is using binary encoding for each trial (success or failure). This isn't a general convention, but it makes clear that these formulae arise from particular probability models. Conventional jargon is not clear in that way."
Do all interactions terms need their individual terms in regression model?,"
I am actually reviewing a manuscript where the authors compare 5-6 logit regression models with AIC. However, some of the models have interaction terms without including the individual covariate terms. Does it ever make sense to do this?
For example (not specific to logit models):
M1: Y = X1 + X2 + X1*X2
M2: Y = X1 + X2
M3: Y = X1 + X1*X2 (missing X2)
M4: Y = X2 + X1*X2 (missing X1)
M5: Y = X1*X2 (missing X1 & X2)

I've always been under the impression that if you have the interaction term X1*X2 you also need X1 + X2. Therefore, models 1 and 2 would be fine but models 3-5 would be problematic (even if AIC is lower). Is this correct? Is it a rule or more of a guideline? Does anyone have a good reference that explains the reasoning behind this? I just want to make sure I don't miscommunicate anything important in the review.
","['regression', 'modeling', 'interaction', 'aic']","Most of the time this is a bad idea - the main reason is that it no longer makes the model invariant to location shifts. For example, suppose you have a single outcome $y_i$ and two predictors $x_i$ and $z_i$ and  specify the model: $$ y_i = \beta_0 + \beta_1 x_{i} z_i + \varepsilon $$ If you were to center the predictors by their means, $x_i z_i$ becomes $$ (x_i - \overline{x})(z_i - \overline{z}) = x_i z_i - x_{i} \overline{z} - z_{i} \overline{x} + \overline{x} \overline{z}$$So, you can see that the main effects have been reintroduced into the model. I've given a heuristic argument here, but this does present a practical issue. As noted in Faraway(2005) on page 114, an additive change in scale changes the model inference when the main effects are left out of the model, whereas this does not happen when the lower order terms are included. It is normally undesirable to have arbitrary things like a location shift cause a fundamental change in the statistical inference (and therefore the conclusions of your inquiry), as can happen when you include polynomial terms or interactions in a model without the lower order effects. Note: There may be special circumstances where you would only want to include the interaction, if the $x_i z_i$ has some particular substantive meaning or if you only observe the product and not the individual variables $x_i, z_i$. But, in that case, one may as well think of the predictor $a_i = x_i z_i$ and proceed with the model $$ y_i = \alpha_0 + \alpha_1 a_i + \varepsilon_i $$ rather than thinking of $a_i$ as an interaction term."
How to actually plot a sample tree from randomForest::getTree()? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question appears to be off-topic because EITHER it is not about statistics, machine learning, data analysis, data mining, or data visualization, OR it focuses on programming, debugging, or performing routine operations within a statistical computing platform. If the latter, you could try the support links we maintain.


Closed 5 years ago.







                        Improve this question
                    



Anyone got library or code suggestions on how to actually plot a couple of sample trees from: 
getTree(rfobj, k, labelVar=TRUE)

(Yes I know you're not supposed to do this operationally, RF is a blackbox, etc etc. I want to visually sanity-check a tree to see if any variables are behaving counterintuitively, need tweaking/combining/discretization/transformation, check how well my encoded factors are working, etc.)

Prior questions without a decent answer: 

How to make Random Forests more interpretable?
Also Obtaining knowledge from a random forest

I actually want to plot a sample tree. So don't argue with me about that, already. I'm not asking about varImpPlot(Variable Importance Plot) or partialPlot or MDSPlot, or these other plots, I already have those, but they're not a substitute for seeing a sample tree.
Yes I can visually inspect the output of getTree(...,labelVar=TRUE).
(I guess a plot.rf.tree() contribution would be very-well-received.)
","['r', 'data-visualization', 'random-forest', 'cart']","First (and easiest) solution: If you are not keen to stick with classical RF, as implemented in Andy Liaw's randomForest, you can try the party package which provides a different implementation of the original RF algorithm (use of conditional trees and aggregation scheme based on units weight average). Then, as reported on this R-help post, you can plot a single member of the list of trees. It seems to run smoothly, as far as I can tell. Below is a plot of one tree generated by cforest(Species ~ ., data=iris, controls=cforest_control(mtry=2, mincriterion=0)).Second (almost as easy) solution: Most of tree-based techniques in R (tree, rpart, TWIX, etc.) offers a tree-like structure for printing/plotting a single tree. The idea would be to convert the output of randomForest::getTree to such an R object, even if it is nonsensical from a statistical point of view. Basically,  it is easy to access the tree structure from a tree object, as shown below. Please note that it will slightly differ depending of the type of task--regression vs. classification--where in the later case it will add class-specific probabilities as the last column of the obj$frame (which is a data.frame).Then, there are methods for pretty printing and plotting those objects. The key functions are a generic tree:::plot.tree method (I put a triple : which allows you to view the code in R directly) relying on tree:::treepl (graphical display) and tree:::treeco (compute nodes coordinates). These functions expect the obj$frame representation of the tree. Other subtle issues: (1) the argument type = c(""proportional"", ""uniform"") in the default plotting method, tree:::plot.tree, help to manage vertical distance between nodes (proportional means it is proportional to deviance, uniform mean it is fixed); (2) you need to complement plot(tr) by a call to text(tr) to add text labels to nodes and splits, which in this case means that you will also have to take a look at tree:::text.tree.The getTree method from randomForest returns a different structure, which is documented in the online help. A typical output is shown below, with terminal nodes indicated by status code (-1). (Again, output will differ depending on the type of task, but only on the status and prediction columns.)If you can manage to convert the above table to the one generated by tree, you will probably be able to customize tree:::treepl, tree:::treeco and tree:::text.tree to suit your needs, though I do not have an example of this approach. In particular, you probably want to get rid of the use of deviance, class probabilities, etc. which are not meaningful in RF. All you want is to set up nodes coordinates and split values. You could use fixInNamespace() for that, but, to be honest, I'm not sure this is the right way to go.Third (and certainly clever) solution: Write a true as.tree helper
function which will alleviates all of the above ""patches"". You could then use R's plotting methods or, probably better, Klimt (directly from R) to display individual trees."
What's the difference between momentum based gradient descent and Nesterov's accelerated gradient descent?,"
So momentum based gradient descent works as follows:
$v=\beta m-\eta g$
where $m$ is the previous weight update, and $g$ is the current gradient with respect to the parameters $p$, $\eta$ is the learning rate, and $\beta$ is a constant.
$p_{new} = p + v = p + \beta  m - \eta  g$
and Nesterov's accelerated gradient descent works as follows:
$p_{new} = p + \beta  v - \eta  g$
which is equivalent to:
$p_{new} = p + \beta  (\beta  m - \eta  g ) - \eta  g$
or
$p_{new} = p + \beta^2  m - (1 + \beta)  \eta  g$
source: https://github.com/fchollet/keras/blob/master/keras/optimizers.py
So to me it seems Nesterov's accelerated gradient descent just gives more weight to the $\eta  g$ term over the pervious weight change term m (compared to plain old momentum). Is this interpretation correct?
","['optimization', 'gradient-descent']",
What is the difference between discrete data and continuous data?,"
What is the difference between discrete data and continuous data?
","['continuous-data', 'discrete-data']","Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad.Continuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values, there may be an infinite number of others. Continuous data are always essentially numeric.It sometimes makes sense to treat discrete data as continuous and the other way around:For example, something like height is continuous, but often we
don't really care too much about tiny differences and instead group
heights into a number of discrete bins -- i.e. only measuring
centimetres --.Conversely, if we're counting large amounts of some discrete entity
-- i.e. grains of rice, or termites, or pennies in the economy -- we may choose not to think of 2,000,006 and 2,000,008 as crucially
different values but instead as nearby points on an approximate
continuum.It can also sometimes be useful to treat numeric data as categorical, eg: underweight, normal, obese. This is usually just another kind of binning.It seldom makes sense to consider categorical data as continuous."
Complete substantive examples of reproducible research using R,"
The Question: Are there any good examples of reproducible research using R that are freely available online?
Ideal Example: 
Specifically, ideal examples would provide:

The raw data (and ideally meta data explaining the data),
All R code including data import, processing, analyses, and output generation,
Sweave or some other approach for linking the final output to the final document,
All in a format that is easily downloadable and compilable on a reader's computer.

Ideally, the example would be a journal article or a thesis where the emphasis is on an actual applied topic as opposed to a statistical teaching example.
Reasons for interest:
I'm particularly interested in applied topics in journal articles and theses, because in these situations, several additional issues arise:

Issues arise related to data cleaning and processing,
Issues arise related to managing metadata,
Journals and theses often have style guide expectations regarding the appearance and formatting of tables and figures,
Many journals and theses often have a wide range of analyses which raise issues regarding workflow (i.e., how to sequence analyses) and processing time (e.g., issues of caching analyses, etc.).

Seeing complete working examples could provide good instructional material for researchers starting out with reproducible research.
","['r', 'references', 'reproducible-research']",
"What are the differences between 'epoch', 'batch', and 'minibatch'?","
As far as I know, when adopting Stochastic Gradient Descent as learning algorithm,
someone use 'epoch' for full dataset, and 'batch' for data used in a single update step, while another use 'batch' and 'minibatch' respectively, and the others use 'epoch' and 'minibatch'. This brings much confusion while discussing.
So what is the correct saying? Or they are just dialects which are all acceptable?
","['machine-learning', 'terminology']",
Criticism of Pearl's theory of causality,"
In the year 2000, Judea Pearl published Causality.  What controversies surround this work?  What are its major criticisms?
",['causality'],"Some authors dislike Pearl's focus on the directed acyclic graph (DAG) as the way in which to view causality.  Pearl essentially argues that any causal system can be considered as a non-parametric structural equation model (NPSEM), in which the value of each node is taken as a function of its parents and some individual error term; the error terms between different nodes may in general be correlated, to represent common causes.Cartwright's book Hunting Causes and Using Them, for example, gives an example involving a car engine, which she claims cannot be modelled in the NPSEM framework.  Pearl disputes this in his review of Cartwright's book.Others caution that the use of DAGs can be misleading, in that the arrows lend an apparent authority to a chosen model as having causal implications, when this may not be the case at all.  See Dawid's Beware of the DAG.  For example, the three DAGs $A \rightarrow B \rightarrow C$, $A \leftarrow B \rightarrow C$ and $A \leftarrow B \leftarrow C$ all induce the same probabilistic model under Pearl's d-separation criterion, which is that A is independent of C given B.  They are therefore indistinguishable based upon observational data.However they have quite different causal interpretations, so if we wish to learn about the causal relationships here we would need more than simply observational data, whether that be the results of interventional experiments, prior information about the system, or something else."
How small a quantity should be added to x to avoid taking the log of zero?,"
I have analysed my data as they are. Now I want to look at my analyses after taking the log of all variables. Many variables contain many zeros. Therefore I add a small quantity to avoid taking the log of zero. 
So far I've added 10^-10, without any rationale really, just because I felt like adding a very small quantity would be advisable to minimize the effect of my arbitrarily chosen quantity. But some variables contain mostly zeros, and therefore when logged mostly -23.02. The range of the ranges of my variables is 1.33-8819.21, and the frequency of zeroes also varies dramatically. Therefore my personal choice of ""small quantity"" affects the variables very differently.
It is clear now that 10^-10 is a completely unacceptable choice, as most of the variance in all the variables then comes from this arbitrary ""small quantity"".
I wonder what would be a more correct way of doing this.
Maybe it's better to derive the quantity from each variables individual distribution? Are there any guidelines about how big this ""small quantity"" should be?
My analyses are mostly simple cox models with each variable and age/sex as IVs. The variables are the concentrations of various blood lipids, with often considerable coefficients of variation.
Edit: Adding the smallest non-zero value of the variable seems practical for my data. But maybe there is a general solution?
Edit 2: As the zeros merely indicate concentrations below the detection limit, maybe setting them to (detection limit)/2 would be appropriate?
","['data-transformation', 'chemometrics', 'faq']","As the zeros merely indicate concentrations below the detection limit, maybe setting them to (detection limit)/2 would be appropriateI was just typing that the thing that comes to my mind where log does (frequently) make sense and 0 may occur are concentrations when you did the 2nd edit. As you say, for measured concentrations the 0 just means ""I couldn't measure that low concentrations"".Side note: do you mean LOQ instead of LOD?Whether setting the 0 to $\frac{1}{2}$LOQ is a good idea or not depends:from the point of view that $\frac{1}{2}\mathrm{LOQ}$ is your ""guess"" expressing that c is anywhere between 0 and LOQ, it does make sense.
But consider the corresponding calibration function:

On the left, the calibration function yields c = 0 below the LOQ. On the right, $\frac{1}{2}\mathrm{LOQ}$ is used instead of 0.However, if the original measured value is available, that may provide a better guess. After all, LOQ usually just means that the relative error is 10%. Below that the measurement still carries information, but the relative error becomes huge.

(blue: LOD, red: LOQ)An alternative would be to exclude these measurements. That can be reasonable, too
e.g. think of a calibration curve. In practice you often observe a sigmoid shape: for low c, signal ≈ constant, intermediate linear behaviour, then detector saturation. 

In that situation you may want to restrict yourself to statements about concentrations that are clearly in the linear range as both below and above other processes heavily influence the result.
Make sure you explain that the data was selected that way and why.  edit: What is sensible or acceptable, depends of course on the problem. Hopefully, we're talking here about a small part of the data that does not influence the analyis.Maybe a quick and dirty check is: run your data analysis with and without excluding the data (or whatever treatment you propose) and see whether anything changes substantially.If you see changes, then of course you're in trouble. However, from the analytical chemistry point of view, I'd say your trouble does not primarily lie in which method you use to deal with the data, but the underlying problem is that the analytical method (or its working range) was not appropriate for the problem at hand. There is of course a zone where the better statistical approach can save your day, but in the end the approximation ""garbage in, garbage out"" usually holds also for the more fancy methods.Quotations for the topic:A statistician once told me:  The problem with you (chemists/spectroscopists) is that your problems are either so hard that they cannot be solved or so easy that there is no fun in solving them.Fisher about the statistical post-mortem of experiments"
Which pseudo-$R^2$ measure is the one to report for logistic regression (Cox & Snell or Nagelkerke)?,"
I have SPSS output for a logistic regression model. The output reports two measures for the model fit, Cox & Snell and Nagelkerke.
So as a rule of thumb, which of these $R^²$ measures would you report as the model fit?
Or, which of these fit indices is the one that is usually reported in journals?

Some Background: The regression tries to predict the presence or absence of a bird (capercaillie) from some environmental variables (e.g., steepness, vegetation cover, ...). Unfortunately, the bird did not appear very often (35 hits to 468 misses) so the regression performs rather poorly. Cox & Snell is .09, Nagelkerke, .23.  
The subject is environmental sciences or ecology.
","['logistic', 'goodness-of-fit', 'r-squared', 'pseudo-r-squared']","Normally I wouldn't report $R^2$ at all.  Hosmer and Lemeshow, in their textbook Applied Logistic Regression (2nd Ed.), explain why:In general, [$R^2$ measures] are based on various comparisons of the predicted values from the fitted model to those from [the base model], the no data or intercept only model and, as a result, do not assess goodness-of-fit.  We think that a true measure of fit is one based strictly on a comparison of observed to predicted values from the fitted model.[At p. 164.]Concerning various ML versions of $R^2$, the ""pseudo $R^2$"" stat, they mention that it is not ""recommended for routine use, as it is not as intuitively easy to explain,"" but they feel obliged to describe it because various software packages report it.They conclude this discussion by writing,...low $R^2$ values in logistic regression are the norm and this presents a problem when reporting their values to an audience accustomed to seeing linear regression values. ... Thus [arguing by reference to running examples in the text] we do not recommend routine publishing of $R^2$ values with results from fitted logistic models.  However, they may be helpful in the model building state as a statistic to evaluate competing models.[At p. 167.]My experience with some large logistic models (100k to 300k records, 100 - 300 explanatory variables) has been exactly as H & L describe.  I could achieve relatively high $R^2$ with my data, up to about 0.40.  These corresponded to classification error rates between 3% and 15% (false negatives and false positives, balanced, as confirmed using 50% hold-out datasets).  As H & L hinted, I had to spend a lot of time disabusing the client (a sophisticated consultant himself, who was familiar with $R^2$) concerning $R^2$ and getting him to focus on what mattered in the analysis (the classification error rates).  I can warmly recommend describing the results of your analysis without reference to $R^2$, which is more likely to mislead than not."
How to tune hyperparameters of xgboost trees?,"
I have a class imbalanced data & I want to tune the hyperparameters of the boosted tress using xgboost. 
Questions

Is there an equivalent of gridsearchcv or randomsearchcv for
xgboost?
If not what is the recommended approach to tune the
parameters of xgboost?

","['machine-learning', 'cross-validation', 'boosting']","Caret package have incorporated xgboost.Sample outputOne drawback i see is that other parameters of xgboost like subsample etc are not supported by caret currently.EditGamma, colsample_bytree, min_child_weight and subsample etc can now (June 2017) be tuned directly using Caret. Just add them in the grid portion of the above code to make it work. Thanks  usεr11852 for highliting it in the comment."
Why do we minimize the negative likelihood if it is equivalent to maximization of the likelihood?,"
This question has puzzled me for a long time. I understand the use of 'log' in maximizing the likelihood so I am not asking about 'log'.
My question is, since maximizing log likelihood is equivalent to minimizing ""negative log likelihood"" (NLL), why did we invent this NLL? Why don't we use the ""positive likelihood"" all the time? In what circumstances is NLL favored?
I found a little explanation here. https://quantivity.wordpress.com/2011/05/23/why-minimize-negative-log-likelihood/, and it seems to explain the obvious equivalence in depth, but does not solve my confusion.
Any explanation will be appreciated. 
","['maximum-likelihood', 'likelihood']","This is an alternative answer: optimizers in statistical packages usually work by minimizing the result of a function. If your function gives the likelihood value first it's more convenient to use logarithm in order to decrease the value returned by likelihood function. Then, since the log likelihood and likelihood function have the same increasing or decreasing trend, you can minimize the negative log likelihood in order to actually perform the maximum likelihood estimate of the function you are testing. See for example the nlminb function in R here"
Why is the Jeffreys prior useful?,"
I understand that the Jeffreys prior is invariant under re-parameterization. However, what I don't understand is why this property is desired.
Why wouldn't you want the prior to change under a change of variables?
","['bayesian', 'prior']","Let me complete Zen's answer. I don't very like the notion of ""representing ignorance"". The important thing is not the Jeffreys prior but the Jeffreys posterior. This posterior aims to reflect as best as possible the information about the parameters brought by the data. The invariance property is naturally required for the two following points. Consider for instance the binomial model with unknown proportion parameter $\theta$ and odds parameter $\psi=\frac{\theta}{1-\theta}$.The Jeffreys posterior on $\theta$ reflects as best as possible the information about $\theta$ brought by the data. There is a one-to-one correspondence between $\theta$ and $\psi$. Then, transforming the Jeffreys posterior on $\theta$ into a posterior on $\psi$ (via the usual change-of-variables formula) should yield a distribution reflecting as best as possible the information about $\psi$. Thus this distribution should be the Jeffreys posterior about $\psi$. This is the invariance property.An important point when drawing conclusions of a statistical analysis is scientific communication. Imagine you give the Jeffreys posterior on $\theta$ to a scientific colleague. But he/she is interested in $\psi$ rather than $\theta$. Then this is not a problem with the invariance property: he/she just has to apply the change-of-variables formula."
Look and you shall find (a correlation),"
I have several hundred measurements. Now, I am considering utilizing some kind of software to correlate every measure with every measure. This means that there are thousands of correlations. Among these there should (statistically) be a high correlation, even if the data is completely random (each measure has only about 100 datapoints).
When I find a correlation, how do I include the information about how hard I looked for a correlation, into it? 
I am not at a high level in statistics, so please bear with me.
","['correlation', 'multiple-comparisons', 'permutation-test']","This is an excellent question, worthy of someone who is a clear statistical thinker, because it recognizes a subtle but important aspect of multiple testing.There are standard methods to adjust the p-values of multiple correlation coefficients (or, equivalently, to broaden their confidence intervals), such as the Bonferroni and Sidak methods (q.v.).  However, these are far too conservative with large correlation matrices due to the inherent mathematical relationships that must hold among correlation coefficients in general.  (For some examples of such relationships see the recent question and the ensuing thread.)  One of the best approaches for dealing with this situation is to conduct a permutation (or resampling) test.  It's easy to do this with correlations: in each iteration of the test, just randomly scramble the order of values of each of the fields (thereby destroying any inherent correlation) and recompute the full correlation matrix.  Do this for several thousand iterations (or more), then summarize the distributions of the entries of the correlation matrix by, for instance, giving their 97.5 and 2.5 percentiles: these would serve as mutual symmetric two-sided 95% confidence intervals under the null hypothesis of no correlation.  (The first time you do this with a large number of variables you will be astonished at how high some of the correlation coefficients can be even when there is no inherent correlation.)When reporting the results, no matter what computations you do, you should include the following:The size of the correlation matrix (i.e., how many variables you have looked at).How you determined the p-values or ""significance"" of any of the correlation coefficients (e.g., left them as-is, applied a Bonferroni correction, did a permutation test, or whatever).Whether you looked at alternative measures of correlation, such as Spearman rank correlation.  If you did, also indicate why you chose the method you are actually reporting on and using."
Why is the validation accuracy fluctuating?,"
I have a four layer CNN to predict response to cancer using MRI data. I use ReLU activations to introduce nonlinearities. The train accuracy and loss monotonically increase and decrease respectively. But, my test accuracy starts to fluctuate wildly. I have tried changing the learning rate, reduce the number of layers. But, it doesn't stop the fluctuations. I even read this answer and tried following the directions in that answer, but not luck again. Could anyone help me figure out where I am going wrong? 

","['machine-learning', 'python', 'deep-learning']",
Assumptions regarding bootstrap estimates of uncertainty,"
I appreciate the usefulness of the bootstrap in obtaining uncertainty estimates, but one thing that's always bothered me about it is that the distribution corresponding to those estimates is the distribution defined by the sample. In general, it seems like a bad idea to believe that our sample frequencies look exactly like the underlying distribution, so why is it sound/acceptable to derive uncertainty estimates based on a distribution where the sample frequencies define the underlying distribution?
On the other hand, this may be no worse (possibly better) than other distributional assumptions we typically make, but I'd still like to understand the justification a bit better.
","['bootstrap', 'uncertainty']","There are several ways that one can conceivably apply the bootstrap. The two most basic approaches are what are deemed the ""nonparametric"" and ""parametric"" bootstrap. The second one assumes that the model you're using is (essentially) correct.Let's focus on the first one. We'll assume that you have a random sample $X_1, X_2, \ldots, X_n$ distributed according the the distribution function $F$. (Assuming otherwise requires modified approaches.) Let $\hat{F}_n(x) = n^{-1} \sum_{i=1}^n \mathbf{1}(X_i \leq x)$ be the empirical cumulative distribution function. Much of the motivation for the bootstrap comes from a couple of facts.Dvoretzky–Kiefer–Wolfowitz inequality$$
\renewcommand{\Pr}{\mathbb{P}}
\Pr\big( \textstyle\sup_{x \in \mathbb{R}} \,|\hat{F}_n(x) - F(x)| > \varepsilon \big) \leq 2 e^{-2n \varepsilon^2} \> .
$$What this shows is that the empirical distribution function converges uniformly to the true distribution function exponentially fast in probability. Indeed, this inequality coupled with the Borel–Cantelli lemma shows immediately that $\sup_{x \in \mathbb{R}} \,|\hat{F}_n(x) - F(x)| \to 0$ almost surely.There are no additional conditions on the form of $F$ in order to guarantee this convergence.Heuristically, then, if we are interested in some functional $T(F)$ of the distribution function that is smooth, then we expect $T(\hat{F}_n)$ to be close to $T(F)$.(Pointwise) Unbiasedness of $\hat{F}_n(x)$By simple linearity of expectation and the definition of $\hat{F}_n(x)$, for each $x \in \mathbb{R}$,$$
\newcommand{\e}{\mathbb{E}}
\e_F \hat{F}_n(x) = F(x) \>.
$$Suppose we are interested in the mean $\mu = T(F)$. Then the unbiasedness of the empirical measure extends to the unbiasedness of linear functionals of the empirical measure. So,
$$
\e_F T(\hat{F}_n) = \e_F \bar{X}_n = \mu = T(F) \> .
$$So $T(\hat{F}_n)$ is correct on average and since $\hat{F_n}$ is rapidly approaching $F$, then (heuristically), $T(\hat{F}_n)$ rapidly approaches $T(F)$.To construct a confidence interval (which is, essentially, what the bootstrap is all about), we can use the central limit theorem, the consistency of empirical quantiles and the delta method as tools to move from simple linear functionals to more complicated statistics of interest.Good references are"
How to simulate data that satisfy specific constraints such as having specific mean and standard deviation?,"
This question is motivated by my question on meta-analysis. But I imagine that it would also be useful in teaching contexts where you want to create a dataset that exactly mirrors an existing published dataset.
I know how to generate random data from a given distribution. So for example, if I read about the results of a study that had:

a mean of 102,
a standard deviation of 5.2 , and 
a sample size of 72.

I could generate similar data using rnorm in R. For example, 
set.seed(1234)
x <- rnorm(n=72, mean=102, sd=5.2)

Of course the mean and SD would not be exactly equal to 102 and 5.2 respectively:
round(c(n=length(x), mean=mean(x), sd=sd(x)), 2)
##     n   mean     sd 
## 72.00 100.58   5.25 

In general I'm interested in how to simulate data that satisfies a set of constraints. In the above case, the constaints are sample size, mean, and standard deviation. In other cases, there might be additional constraints. For example, 

a minimum and a maximum in either the data or the underlying variable might be known.
the variable might be known to take on only integer values or only non-negative values.
the data might include multiple variables with known inter-correlations.

Questions

In general, how can I simulate data that exactly satisfies a set of constraints?
Are there articles written about this? Are there any programs in R that do this?
For the sake of example, how could and should I simulate a variable so that it has a specific mean and sd?

","['r', 'dataset', 'simulation', 'random-generation']",
Clustering with a distance matrix,"
I have a (symmetric) matrix M that represents the distance between each pair of nodes. For example,

    A   B   C   D   E   F   G   H   I   J   K   L
A   0  20  20  20  40  60  60  60 100 120 120 120
B  20   0  20  20  60  80  80  80 120 140 140 140
C  20  20   0  20  60  80  80  80 120 140 140 140
D  20  20  20   0  60  80  80  80 120 140 140 140
E  40  60  60  60   0  20  20  20  60  80  80  80
F  60  80  80  80  20   0  20  20  40  60  60  60
G  60  80  80  80  20  20   0  20  60  80  80  80
H  60  80  80  80  20  20  20   0  60  80  80  80
I 100 120 120 120  60  40  60  60   0  20  20  20
J 120 140 140 140  80  60  80  80  20   0  20  20
K 120 140 140 140  80  60  80  80  20  20   0  20
L 120 140 140 140  80  60  80  80  20  20  20   0

Is there any method to extract clusters from M (if needed, the number of clusters can be fixed), such that each cluster contains nodes with small distances between them. In the example, the clusters would be (A, B, C, D), (E, F, G, H) and (I, J, K, L).
I've already tried UPGMA and k-means but the resulting clusters are very bad.
The distances are the average steps a random walker would take to go from node A to node B (!= A) and go back to node A. It's guaranteed that M^1/2 is a metric. To run k-means, I don't use the centroid. I define the distance between node n cluster c as the average distance between n and all nodes in c.
Thanks a lot :)
","['clustering', 'distance-functions']","There are a number of options.First, you could try partitioning around medoids (pam) instead of using k-means clustering. This one is more robust, and could give better results. Van der Laan reworked the algorithm. If you're going to implement it yourself, his article is worth a read.There is a specific k-medoids clustering algorithm for large datasets. The algorithm is called Clara in R, and is described in chapter 3 of  Finding Groups in Data: An Introduction to Cluster Analysis. by Kaufman, L and Rousseeuw, PJ (1990).Instead of UPGMA, you could try some other hierarchical clustering options. First of all, when you use hierarchical clustering, be sure you define the partitioning method properly. This partitioning method is essentially how the distances between observations and clusters are calculated. I mostly use Ward's method or complete linkage, but other options might be the choice for you.Don't know if you tried it yet, but the single linkage method or neighbour joining is often preferred above UPGMA in phylogenetic applications. If you didn't try it yet, you could give it a shot as well, as it often gives remarkably good results.In R you can take a look at the package cluster. All described algorithms are implemented there. See ?pam, ?clara, ?hclust, ... Check also the different implementation of the algorithm in ?kmeans. Sometimes chosing another algorithm can improve the clustering substantially.EDIT : Just thought about something: If you work with graphs and nodes and the likes, you should take a look at the markov clustering algorithm as well. That one is used for example in grouping sequences based on blast similarities, and performs incredibly well. It can do the clustering for you, or give you some ideas on how to solve the research problem you're focusing on. Without knowing anything about it in fact, I guess his results are definitely worth looking at. If I may say so, I still consider this method of Stijn van Dongen one of the nicest results in clustering I've ever encountered.http://www.micans.org/mcl/"
Why bother with the dual problem when fitting SVM?,"
Given the data points $x_1, \ldots, x_n \in \mathbb{R}^d$ and labels $y_1, \ldots, y_n \in \left \{-1, 1 \right\}$, the hard margin SVM primal problem is
$$ \text{minimize}_{w, w_0} \quad \frac{1}{2} w^T w $$
$$ \text{s.t.} \quad  \forall i: y_i (w^T x_i + w_0) \ge 1$$
which is a quadratic program with $d+1$ variables to be optimized for and $i$ constraints. The dual
$$ \text{maximize}_{\alpha} \quad \sum_{i=1}^{n}{\alpha_i} - \frac{1}{2}\sum_{i=1}^{n}{\sum_{j=1}^{n}{y_i y_j \alpha_i \alpha_j x_i^T x_j}}$$
$$ \text{s.t.} \quad \forall i: \alpha_i \ge 0 \land \sum_{i=1}^{n}{y_i \alpha_i} = 0$$
is a quadratic program with $n + 1$ variables to be optimized for and $n$ inequality and $n$ equality constraints.
When implementing a hard margin SVM, why would I solve the dual problem instead of the primal problem? The primal problem looks more 'intuitive' to me, and I don't need to concern myself with the duality gap, the Kuhn-Tucker condition etc.
It would make sense to me to solve the dual problem if $d \gg n$, but I suspect there are better reasons. Is this the case?
",['svm'],"Based on the lecture notes referenced in @user765195's answer (thanks!), the most apparent reasons seem to be:Solving the primal problem, we obtain the optimal $w$, but know nothing about the $\alpha_i$. In order to classify a query point $x$ we need to explicitly compute the scalar product $w^Tx$, which may be expensive if $d$ is large.Solving the dual problem, we obtain the $\alpha_i$ (where $\alpha_i = 0$ for all but a few points - the support vectors). In order to classify a query point $x$, we calculate$$ w^Tx + w_0 = \left(\sum_{i=1}^{n}{\alpha_i y_i x_i} \right)^T x + w_0 = \sum_{i=1}^{n}{\alpha_i y_i \langle x_i, x \rangle} + w_0 $$This term is very efficiently calculated if there are only few support vectors. Further, since we now have a scalar product only involving data vectors, we may apply the kernel trick."
Intuitive explanation of Fisher Information and Cramer-Rao bound,"
I am not comfortable with Fisher information, what it measures and how is it helpful. Also it's relationship with the Cramer-Rao bound is not apparent to me.
Can someone please give an intuitive explanation of these concepts?
","['estimation', 'intuition', 'fisher-information']",
Is this the solution to the p-value problem?,"
In February 2016, the American Statistical Association released a formal statement on statistical significance and p-values.  Our thread about it discusses these issues extensively.  However, no authority has come forth to offer a universally recognized effective alternative--until now.  The American Statistical Society (ASS) has published its response, p-values: What’s next?
""The p-value isn't good for much.""

We think the ASA did not go far enough. It is time to admit that the era of p-values is over. Statisticians have successfully used them to baffle undergraduates, trick scientists, and fool editors everywhere, but the world is starting to see through this ruse. We need to abandon this early 20th century attempt by statisticians to control decision making. We need to return to what actually works.

The official ASS proposal is this:

In place of p-values, the ASS advocates the STOP (SeaT-Of-Pants procedure). This time-honored and -tested method was used by the ancient Greeks, renaissance men, and all scientists until Ronald Fisher came along and ruined things. The STOP is simple, direct, data-driven, and authoritative. To carry it out, an authority figure (an older male, by preference) reviews the data and decides whether they agree with his opinion. When he decides they do, the result is “significant.” Otherwise it is not and everybody is required to forget about the whole thing.

Principles
The response addresses each of the ASA's six principles.

The STOP can indicate how incompatible the data are with a specified statistical model.

We like this phrase because it’s such a fancy way of saying the STOP will answer any question yes or no. Unlike p-values or other statistical procedures, it leaves no doubt. It’s the perfect response to those who say “we don’t need no stinkin’ null hypothesis! What the *?!@ is that, anyway? Nobody ever could figure out what it was supposed to be.”

The STOP doesn’t measure the probability that a hypothesis is true: it actually decides whether it’s true or not.

Everybody is confused by probabilities. By taking probability out of the picture, the STOP eliminates the need for years of undergraduate and graduate study. Now anybody (who is sufficiently old and male) can perform statistical analysis without the pain and torture of listening to even a single statistical lecture or running arcane software that spews unintelligible output.

Scientific conclusions and business or policy decisions can be based on common sense and real authority figures.

Important decisions always have been made by authorities, anyway, so let’s just admit it and cut out the middlemen. Using the STOP will free statisticians to do what they are best suited for: using numbers to obfuscate the truth and sanctifying the preferences of those in power.

Proper inference requires full reporting and transparency.

The STOP is the most transparent and self-evident statistical procedure ever invented: you look at the data and you decide. It eliminates all those confusing z-tests, t-tests, chi-squared tests, and alphabet soup procedures (ANOVA! GLM! MLE!) used by people to hide the fact they have no clue what the data mean.

The STOP measures the importance of the result.

This is self-evident: if a person in authority employs the STOP, then the result must be important.

By itself, the STOP provides a good measure of evidence regarding a model or hypothesis.

We wouldn’t want to challenge an authority, would we? Researchers and decision makers will recognize that the STOP provides all the information they need to know. For these reasons, data analysis can end with the STOP; there is no need for alternative approaches, like p-values, machine learning, or astrology.


Other approaches

Some statisticians prefer so-called “Bayesian” methods, in which an obscure theorem posthumously published by an 18th century cleric is applied mindlessly to solve every problem. Its most noted advocates freely admit these methods are “subjective.” If we’re going to use subjective methods, then obviously the more authoritative and knowledgeable the decision maker is, the better the result will be. The STOP thereby emerges as the logical limit of all Bayes methods. Why go to the effort of working those awful calculations, and tying up so much computer time, when you can just show the data to the guy in charge and ask him what his opinion is? End of story.
Another community has recently arisen to challenge the priesthood of statisticians. They call themselves “machine learners” and “data scientists,” but they’re really just hackers looking for higher status. It’s the official position of the ASS that these guys should go form their own professional organization if they want people to take them seriously.


The question
Is this the answer to the problems the ASA identified with p-values and null hypothesis testing?  Can it really unite the Bayesian and Frequentist paradigms (as implicitly claimed in the response)?
","['hypothesis-testing', 'statistical-significance', 'p-value']",
What are some valuable Statistical Analysis open source projects?,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






What are some valuable Statistical Analysis open source projects available right now?
Edit: as pointed out by Sharpie, valuable could mean helping you get things done faster or more cheaply.
","['software', 'open-source']","The R-projecthttp://www.r-project.org/R is valuable and significant because it was the first widely-accepted Open-Source alternative to big-box packages.  It's mature, well supported, and a standard within many scientific communities."
Removing duplicated rows data frame in R [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            



Closed 10 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






How can I remove duplicate rows from this example data frame?
A   1
A   1
A   2
B   4  
B   1
B   1
C   2
C   2

I would like to remove the duplicates based on both the columns:
A   1
A   2
B   4
B   1
C   2

Order is not important.
",['r'],"unique() indeed answers your question, but another related and interesting function to achieve the same end is duplicated().It gives you the possibility to look up which rows are duplicated."
Why does ridge estimate become better than OLS by adding a constant to the diagonal?,"
I understand that the ridge regression estimate is the $\beta$ that minimizes residual sum of square and a penalty on the size of $\beta$
$$\beta_\mathrm{ridge} = (\lambda I_D + X'X)^{-1}X'y = \operatorname{argmin}\big[ \text{RSS} + \lambda \|\beta\|^2_2\big]$$
However, I don't fully understand the significance of the fact that $\beta_\text{ridge}$ differs from $\beta_\text{OLS}$ by only adding a small constant to the diagonal of $X'X$. Indeed,
$$\beta_\text{OLS} = (X'X)^{-1}X'y$$

My book mentions that this makes the estimate more stable numerically -- why? 
Is numerical stability related to the shrinkage towards 0 of the ridge estimate, or it's just a coincidence?

","['regression', 'least-squares', 'ridge-regression', 'regularization']","In an unpenalized regression, you can often get a ridge* in parameter space, where many different values along the ridge all do as well or nearly as well on the least squares criterion. *  (at least, it's a ridge in the likelihood function -- they're actually valleys$ in the RSS criterion, but I'll continue to call it a ridge, as this seems to be conventional -- or even, as Alexis points out in comments, I could call that a thalweg, being the valley's counterpart of a ridge)In the presence of a ridge in the least squares criterion in parameter space, the penalty you get with ridge regression gets rid of those ridges by pushing the criterion up as the parameters head away from the origin:
[Clearer image]In the first plot, a large change in parameter values (along the ridge) produces a miniscule change in the RSS criterion. This can cause numerical instability; it's very sensitive to small changes (e.g. a tiny change in a data value, even truncation or rounding error). The parameter estimates are almost perfectly correlated. You may get parameter estimates that are very large in magnitude.By contrast, by lifting up the thing that ridge regression minimizes (by adding the $L_2$ penalty) when the parameters are far from 0, small changes in conditions (such as a little rounding or truncation error) can't produce gigantic changes in the resulting estimates. The penalty term results in shrinkage toward 0 (resulting in some bias). A small amount of bias can buy a substantial improvement in the variance (by eliminating that ridge).The uncertainty of the estimates are reduced (the standard errors are inversely related to the second derivative, which is made larger by the penalty). Correlation in parameter estimates is reduced. You now won't get parameter estimates that are very large in magnitude if the RSS for small parameters would not be much worse."
Is PCA followed by a rotation (such as varimax) still PCA?,"
I have tried to reproduce some research (using PCA) from SPSS in R. In my experience, principal() function from package psych was the only function that came close (or if my memory serves me right, dead on) to match the output. To match the same results as in SPSS, I had to use parameter principal(..., rotate = ""varimax""). I have seen papers talk about how they did PCA, but based on the output of SPSS and use of rotation, it sounds more like Factor analysis.
Question: Is PCA, even after rotation (using varimax), still PCA? I was under the impression that this might be in fact Factor analysis... In case it's not, what details am I missing?
","['r', 'spss', 'pca', 'factor-analysis', 'factor-rotation']","This question is largely about definitions of PCA/FA, so opinions might differ. My opinion is that PCA+varimax should not be called either PCA or FA, bur rather explicitly referred to e.g. as ""varimax-rotated PCA"".I should add that this is quite a confusing topic. In this answer I want to explain what a rotation actually is; this will require some mathematics. A casual reader can skip directly to the illustration. Only then we can discuss whether PCA+rotation should or should not be called ""PCA"".One reference is Jolliffe's book ""Principal Component Analysis"", section 11.1 ""Rotation of Principal Components"", but I find it could be clearer.Let $\mathbf X$ be a $n \times p$ data matrix which we assume is centered. PCA amounts (see my answer here) to a singular-value decomposition: $\mathbf X=\mathbf{USV}^\top$. There are two equivalent but complimentary views on this decomposition: a more PCA-style ""projection"" view and a more FA-style ""latent variables"" view.According to the PCA-style view, we found a bunch of orthogonal directions $\mathbf V$ (these are eigenvectors of the covariance matrix, also called ""principal directions"" or ""axes""), and ""principal components"" $\mathbf{US}$ (also called principal component ""scores"") are projections of the data on these directions. Principal components are uncorrelated, the first one has maximally possible variance, etc. We can write: $$\mathbf X = \mathbf{US}\cdot \mathbf V^\top = \text{Scores} \cdot \text{Principal directions}.$$According to the FA-style view, we found some uncorrelated unit-variance ""latent factors"" that give rise to the observed variables via ""loadings"". Indeed, $\widetilde{\mathbf U}=\sqrt{n-1}\mathbf{U}$ are standardized principal components (uncorrelated and with unit variance), and if we define loadings as $\mathbf L = \mathbf{VS}/\sqrt{n-1}$, then  $$\mathbf X= \sqrt{n-1}\mathbf{U}\cdot (\mathbf{VS}/\sqrt{n-1})^\top =\widetilde{\mathbf U}\cdot \mathbf L^\top = \text{Standardized scores} \cdot \text{Loadings}.$$ (Note that $\mathbf{S}^\top=\mathbf{S}$.) Both views are equivalent. Note that loadings are eigenvectors scaled by the respective eigenvalues ($\mathbf{S}/\sqrt{n-1}$ are eigenvalues of the covariance matrix).(I should add in brackets that PCA$\ne$FA; FA explicitly aims at finding latent factors that are linearly mapped to the observed variables via loadings; it is more flexible than PCA and  yields different loadings. That is why I prefer to call the above ""FA-style view on PCA"" and not FA, even though some people take it to be one of FA methods.) Now, what does a rotation do? E.g. an orthogonal rotation, such as varimax. First, it considers only $k<p$ components, i.e.: $$\mathbf X \approx \mathbf U_k \mathbf S_k \mathbf V_k^\top = \widetilde{\mathbf U}_k \mathbf L^\top_k.$$ Then it takes a square orthogonal $k \times k$ matrix $\mathbf T$, and plugs $\mathbf T\mathbf T^\top=\mathbf I$ into this decomposition: $$\mathbf X \approx \mathbf U_k \mathbf S_k \mathbf V_k^\top = \mathbf U_k \mathbf T \mathbf T^\top \mathbf S_k \mathbf V_k^\top = \widetilde{\mathbf U}_\mathrm{rot} \mathbf L^\top_\mathrm{rot},$$ where rotated loadings are given by $\mathbf L_\mathrm{rot} = \mathbf L_k \mathbf T$, and rotated standardized scores are given by $\widetilde{\mathbf U}_\mathrm{rot} = \widetilde{\mathbf U}_k \mathbf T$. (The purpose of this is to find $\mathbf T$ such that $\mathbf L_\mathrm{rot}$ became as close to being sparse as possible, to facilitate its interpretation.)Note that what is rotated are: (1) standardized scores, (2) loadings. But not the raw scores and not the principal directions! So the rotation happens in the latent space, not in the original space. This is absolutely crucial.From the FA-style point of view, nothing much happened. (A) The latent factors are still uncorrelated and standardized. (B) They are still mapped to the observed variables via (rotated) loadings. (C) The amount of variance captured by each component/factor is given by the sum of squared values of the corresponding loadings column in $\mathbf L_\mathrm{rot}$. (D) Geometrically, loadings still span the same $k$-dimensional subspace in $\mathbb R^p$ (the subspace spanned by the first $k$ PCA eigenvectors). (E) The approximation to $\mathbf X$ and the reconstruction error did not change at all. (F) The covariance matrix is still approximated equally well:$$\boldsymbol \Sigma \approx \mathbf L_k\mathbf L_k^\top = \mathbf L_\mathrm{rot}\mathbf L_\mathrm{rot}^\top.$$But the PCA-style point of view has practically collapsed. Rotated loadings do not correspond to orthogonal directions/axes in $\mathbb R^p$ anymore, i.e. columns of $\mathbf L_\mathrm{rot}$ are not orthogonal! Worse, if you [orthogonally] project the data onto the directions given by the rotated loadings, you will get correlated (!) projections and will not be able to recover the scores. [Instead, to compute the standardized scores after rotation, one needs to multiply the data matrix with the pseudo-inverse of loadings $\widetilde{\mathbf U}_\mathrm{rot} = \mathbf X (\mathbf L_\mathrm{rot}^+)^\top$. Alternatively, one can simply rotate the original standardized scores with the rotation matrix: $\widetilde{\mathbf U}_\mathrm{rot} = \widetilde{\mathbf U} \mathbf T$.] Also, the rotated components do not successively capture the maximal amount of variance: the variance gets redistributed among the components (even though all $k$ rotated components capture exactly as much variance as all $k$ original principal components).Here is an illustration. The data is a 2D ellipse stretched along the main diagonal. First principal direction is the main diagonal, the second one is orthogonal to it. PCA loading vectors (eigenvectors scaled by the eigenvalues) are shown in red -- pointing in both directions and also stretched by a constant factor for visibility. Then I applied an orthogonal rotation by $30^\circ$ to the loadings. Resulting loading vectors are shown in magenta. Note how they are not orthogonal (!).An FA-style intuition here is as follows: imagine a ""latent space"" where points fill a small circle (come from a 2D Gaussian with unit variances). These distribution of points is then stretched along the PCA loadings (red) to become the data ellipse that we see on this figure. However, the same distribution of points can be rotated and then stretched along the rotated PCA loadings (magenta) to become the same data ellipse.[To actually see that an orthogonal rotation of loadings is a rotation, one needs to look at a PCA biplot; there the vectors/rays corresponding to original variables will simply rotate.] Let us summarize. After an orthogonal rotation (such as varimax), the ""rotated-principal"" axes are not orthogonal, and orthogonal projections on them do not make sense. So one should rather drop this whole axes/projections point of view. It would be weird to still call it PCA (which is all about projections with maximal variance etc.).From FA-style point of view, we simply rotated our (standardized and uncorrelated) latent factors, which is a valid operation. There are no ""projections"" in FA; instead, latent factors generate the observed variables via loadings. This logic is still preserved. However, we started with principal components, which are not actually factors (as PCA is not the same as FA). So it would be weird to call it FA as well.Instead of debating whether one ""should"" rather call it PCA or FA, I would suggest to be meticulous in specifying the exact used procedure: ""PCA followed by a varimax rotation"".Postscriptum. It is possible to consider an alternative rotation procedure, where $\mathbf{TT}^\top$ is inserted between $\mathbf{US}$ and $\mathbf V^\top$. This would rotate raw scores and eigenvectors (instead of standardized scores and loadings). The biggest problem with this approach is that after such a ""rotation"", scores will not be uncorrelated anymore, which is pretty fatal for PCA. One can do it, but it is not how rotations are usually being understood and applied."
"What is a ""saturated"" model?","
What is meant when we say we have a saturated model?
","['modeling', 'regression']","A saturated model is one in which there are as many estimated parameters as data points. By definition, this will lead to a perfect fit, but will be of little use statistically, as you have no data left to estimate variance.For example, if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term)."
Linear model with log-transformed response vs. generalized linear model with log link,"
In this paper titled ""CHOOSING AMONG GENERALIZED LINEAR MODELS APPLIED TO MEDICAL DATA"" the authors write:

In a generalized linear model, the mean is transformed, by the link
  function, instead of transforming the response itself. The two methods
  of transformation can lead to quite different results; for example,
  the mean of log-transformed responses is not the same as the logarithm
  of the mean response. In general, the former cannot easily be
  transformed to a mean response. Thus, transforming the mean often
  allows the results to be more easily interpreted, especially in that
  mean parameters remain on the same scale as the measured responses.

It appears they advise the fitting of a generalized linear model (GLM) with log link instead of a linear model (LM) with log-transformed response. I do not grasp the advantages of this approach, and it seems quite unusual to me.
My response variable looks log-normally distributed. I get similar results in terms of the coefficients and their standard errors with either approach.
Still I wonder: If a variable has a log-normal distribution, isn't the mean of the log-transformed variable preferable over the log of the mean untransformed variable, as the mean is the natural summary of a normal distribution, and the log-transformed variable is normally distributed, whereas the variable itself is not?
","['generalized-linear-model', 'model-selection', 'lognormal-distribution']","Although it may appear that the mean of the log-transformed variables is preferable (since this is how log-normal is typically parameterised), from a practical point of view, the log of the mean is typically much more useful.This is particularly true when your model is not exactly correct, and to quote George Box: ""All models are wrong, some are useful""Suppose some quantity is log normally distributed, blood pressure say (I'm not a medic!), and we have two populations, men and women.  One might hypothesise that the average blood pressure is higher in women than in men.  This exactly corresponds to asking whether log of average blood pressure is higher in women than in men.  It is not the same as asking whether the average of log blood pressure is higher in women that man.Don't get confused by the text book parameterisation of a distribution - it doesn't have any ""real"" meaning.  The log-normal distribution is parameterised by the mean of the log ($\mu_{\ln}$) because of mathematical convenience, but equally we could choose to parameterise it by its actual mean and variance$\mu = e^{\mu_{\ln} + \sigma_{\ln}^2/2}$$\sigma^2 = (e^{\sigma^2_{\ln}} -1)e^{2 \mu_{\ln} + \sigma_{\ln}^2}$Obviously, doing so makes the algebra horribly complicated, but it still works and means the same thing.Looking at the above formula, we can see an important difference between transforming the variables and transforming the mean.  The log of the mean, $\ln(\mu)$, increases as $\sigma^2_{\ln}$ increases, while the mean of the log, $\mu_{\ln}$ doesn't.This means that women could, on average, have higher blood pressure that men, even though the mean paramater of the log normal distribution ($\mu_{\ln}$) is the same, simply because the variance parameter is larger.  This fact would get missed by a test that used log(Blood Pressure).So far, we have assumed that blood pressure genuinly is log-normal.  If the true distributions are not quite log normal, then transforming the data will (typically) make things even worse than above - since we won't quite know what our ""mean"" parameter actually means.  I.e. we won't know those two equations for mean and variance I gave above are correct.  Using those to transform back and forth will then introduce additional errors."
"Difference between ""kernel"" and ""filter"" in CNN","
What is the difference between the terms ""kernel"" and ""filter"" in the context of convolutional neural networks?
","['neural-networks', 'terminology', 'deep-learning', 'conv-neural-network']",
Why is logistic regression a linear classifier?,"
Since we are using the logistic function to transform a linear combination of the input into a non-linear output, how can logistic regression be considered a linear classifier?
Linear regression is just like a neural network without the hidden layer, so why are neural networks considered non-linear classifiers and logistic regression is linear?
","['logistic', 'classification', 'neural-networks']","Logistic regression is linear in the sense that the predictions can be written as
$$ \hat{p} = \frac{1}{1 + e^{-\hat{\mu}}}, \text{ where } \hat{\mu} = \hat{\theta} \cdot x. $$
Thus, the prediction can be written in terms of $\hat{\mu}$, which is a linear function of $x$. (More precisely, the predicted log-odds is a linear function of $x$.)Conversely, there is no way to summarize the output of a neural network in terms of a linear function of $x$, and that is why neural networks are called non-linear.Also, for logistic regression, the decision boundary $\{x:\hat{p} = 0.5\}$ is linear: it's the solution to $\hat{\theta} \cdot x = 0$. The decision boundary of a neural network is in general not linear."
What problem do shrinkage methods solve?,"
The holiday season has given me the opportunity to curl up next to the fire with The Elements of Statistical Learning. Coming from a (frequentist) econometrics perspective, I'm having trouble grasping the uses of shrinkage methods like ridge regression, lasso, and least angle regression (LAR). Typically, I'm interested in the parameter estimates themselves and in achieving unbiasedness or at least consistency. Shrinkage methods don't do that. 
It seems to me that these methods are used when the statistician is worried that the regression function becomes too responsive to the predictors, that it considers the predictors to be more important (measured by the magnitude of the coefficients) than they actually are. In other words, overfitting.
But, OLS typically provides unbiased and consistent estimates.(footnote) I've always viewed the problem of overfitting not of giving estimates that are too big, but rather confidence intervals that are too small because the selection process isn't taken into account (ESL mentions this latter point). 
Unbiased/consistent coefficient estimates lead to unbiased/consistent predictions of the outcome. Shrinkage methods push predictions closer to the mean outcome than OLS would, seemingly leaving information on the table.
To reiterate, I don't see what problem the shrinkage methods are trying to solve. Am I missing something?
Footnote: We need the full column rank condition for identification of the coefficients. The exogeneity/zero conditional mean assumption for the errors and the linear conditional expectation assumption determine the interpretation that we can give to the coefficients, but we get an unbiased or consistent estimate of something even if these assumptions aren't true.
","['lasso', 'ridge-regression', 'regularization', 'lars']","I suspect you want a deeper answer, and I'll have to let someone else provide that, but I can give you some thoughts on ridge regression from a loose, conceptual perspective.  OLS regression yields parameter estimates that are unbiased (i.e., if such samples are gathered and parameters are estimated indefinitely, the sampling distribution of parameter estimates will be centered on the true value).  Moreover, the sampling distribution will have the lowest variance of all possible unbiased estimates (this means that, on average, an OLS parameter estimate will be closer to the true value than an estimate from some other unbiased estimation procedure will be).  This is old news (and I apologize, I know you know this well), however, the fact that the variance is lower does not mean that it is terribly low.  Under some circumstances, the variance of the sampling distribution can be so large as to make the OLS estimator essentially worthless.  (One situation where this could occur is when there is a high degree of multicollinearity.)What is one to do in such a situation?  Well, a different estimator could be found that has lower variance (although, obviously, it must be biased, given what was stipulated above).  That is, we are trading off unbiasedness for lower variance.  For example, we get parameter estimates that are likely to be substantially closer to the true value, albeit probably a little below the true value.  Whether this tradeoff is worthwhile is a judgment the analyst must make when confronted with this situation.  At any rate, ridge regression is just such a technique.  The following (completely fabricated) figure is intended to illustrate these ideas.This provides a short, simple, conceptual introduction to ridge regression.  I know less about lasso and LAR, but I believe the same ideas could be applied.  More information about the lasso and least angle regression can be found here, the ""simple explanation..."" link is especially helpful.  This provides much more information about shrinkage methods.  I hope this is of some value.  "
What does standard deviation tell us in non-normal distribution,"
In a normal distribution, the 68-95-99.7 rule imparts standard deviation a lot of meaning, but what would standard deviation mean in a non-normal distribution (multimodal or skewed)? Would all data values still fall within 3 standard deviations? Do we have rules like the 68-95-99.7 one for non-normal distributions?
","['normal-distribution', 'standard-deviation', 'skewness']",
"How to interpret type I, type II, and type III ANOVA and MANOVA?","
My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? 
My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:
lm1 <- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)

Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:
Response: y1
          Df  Sum Sq Mean Sq F value  Pr(>F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 

Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. 
For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:
> lm2 <- lm(y1 ~ density + inorganicN, data = Ena)
> anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(>F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 

Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I & II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.
library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(>F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  

My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?
In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:
Y <- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 <- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(>F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 < 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 <- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(>F)    
density    1 0.99950     4762      5     12 < 2e-16 ***
nitrate    1 0.99995    46248      5     12 < 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           

","['r', 'hypothesis-testing', 'anova', 'manova', 'sums-of-squares']","What you are calling type II SS, I would call type III SS.  Lets imagine that there are just two factors A and B (and we'll throw in the A*B interaction later to distinguish type II SS).  Further, lets imagine that there are different $n$s in the four cells (e.g., $n_{11}$=11, $n_{12}$=9, $n_{21}$=9, and $n_{22}$=11).  Now your two factors are correlated with each other.  (Try this yourself, make 2 columns of 1's and 0's and correlate them, $r=.1$; n.b. it doesn't matter if $r$ is 'significant', this is the whole population that you care about).  The problem with your factors being correlated is that there are sums of squares that are associated with both A and B.  When computing an ANOVA (or any other linear regression), we want to partition the sums of squares.  A partition puts all sums of squares into one and only one of several subsets.  (For example, we might want to divide the SS up into A, B and error.)  However, since your factors (still only A and B here) are not orthogonal there is no unique partition of these SS.  In fact, there can be very many partitions, and if you are willing to slice your SS up into fractions (e.g., ""I'll put .5 into this bin and .5 into that one""), there are infinite partitions.  A way to visualize this is to imagine the MasterCard symbol:  The rectangle represents the total SS, and each of the circles represents the SS that are attributable to that factor, but notice the overlap between the circles in the center, those SS could be given to either circle.  The question is:  How are we to choose the 'right' partition out of all of these possibilities?  Let's bring the interaction back in and discuss some possibilities:Notice how these different possibilities work.  Only type I SS actually uses those SS in the overlapping portion between the circles in the MasterCard symbol.  That is, the SS that could be attributed to either A or B, are actually attributed to one of them when you use type I SS (specifically, the one you entered into the model first).  In both of the other approaches, the overlapping SS are not used at all.  Thus, type I SS gives to A all the SS attributable to A (including those that could also have been attributed elsewhere), then gives to B all of the remaining SS that are attributable to B, then gives to the A*B interaction all of the remaining SS that are attributable to A*B, and leaves the left-overs that couldn't be attributed to anything to the error term.  Type III SS only gives A those SS that are uniquely attributable to A, likewise it only gives to B and the interaction those SS that are uniquely attributable to them.  The error term only gets those SS that couldn't be attributed to any of the factors.  Thus, those 'ambiguous' SS that could be attributed to 2 or more possibilities are not used.  If you sum the type III SS in an ANOVA table, you will notice that they do not equal the total SS.  In other words, this analysis must be wrong, but errs in a kind of epistemically conservative way.  Many statisticians find this approach egregious, however government funding agencies (I believe the FDA) requires their use.  The type II approach is intended to capture what might be worthwhile about the idea behind type III, but mitigate against its excesses.  Specifically, it only adjusts the SS for A and B for each other, not the interaction.  However, in practice type II SS is essentially never used.  You would need to know about all of this and be savvy enough with your software to get these estimates, and the analysts who are typically think this is bunk.  There are more types of SS (I believe IV and V).  They were suggested in the late 60's to deal with certain situations, but it was later shown that they do not do what was thought.  Thus, at this point they are just a historical footnote.  As for what questions these are answering, you basically have that right already in your question:  "
What are good basic statistics to use for ordinal data?,"
I have some ordinal data gained from survey questions.  In my case they are Likert style responses (Strongly Disagree-Disagree-Neutral-Agree-Strongly Agree).  In my data they are coded as 1-5.
I  don't think means would mean much here, so what basic summary statistics are considered usefull?
","['descriptive-statistics', 'ordinal-data', 'likert', 'faq']","A frequency table is a good place to start. You can do the count, and relative frequency for each level. Also, the total count, and number of missing values may be of use. You can also use a contingency table to compare two variables at once. Can display using a mosaic plot too."
What is the relationship between independent component analysis and factor analysis?,"
I am new to Independent Component Analysis (ICA) and have just a rudimentary understanding of the the method. It seems to me that ICA is similar to Factor Analysis (FA) with one exception: ICA assumes that the observed random variables are a linear combination of independent components/factors that are non-gaussian whereas the classical FA model assumes that the observed random variables are a linear combination of correlated, gaussian components/factors.
Is the above accurate?
","['multivariate-analysis', 'factor-analysis', 'independent-component-analysis']",
Is ridge regression useless in high dimensions ($n \ll p$)? How can OLS fail to overfit?,"
Consider a good old regression problem with $p$ predictors and sample size $n$. The usual wisdom is that OLS estimator will overfit and will generally be outperformed by the ridge regression estimator: $$\hat\beta = (X^\top X + \lambda I)^{-1}X^\top y.$$ It is standard to use cross-validation to find an optimal regularization parameter $\lambda$. Here I use 10-fold CV. Clarification update: when $n<p$, by ""OLS estimator"" I understand ""minimum-norm OLS estimator"" given by $$\hat\beta_\text{OLS} = (X^\top X)^+X^\top y = X^+ y.$$
I have a dataset with $n=80$ and $p>1000$. All predictors are standardized, and there are quite a few that (alone) can do a good job in predicting $y$. If I randomly select a small-ish, say $p=50<n$, number of predictors, I get a reasonable CV curve: large values of $\lambda$ yield zero R-squared, small values of $\lambda$ yield negative R-squared (because of overfitting) and there is some maximum in between. For $p=100>n$ the curve looks similar. However, for $p$ much larger than that, e.g. $p=1000$, I do not get any maximum at all: the curve plateaus, meaning that OLS with $\lambda\to 0$ performs as good as ridge regression with optimal $\lambda$.

How is it possible and what does it say about my dataset? Am I missing something obvious or is it indeed counter-intuitive? How can there be any qualitative difference between $p=100$ and $p=1000$ given that both are larger than $n$?
Under what conditions does minimal-norm OLS solution for $n<p$ not overfit?

Update: There was some disbelief in the comments, so here is a reproducible example using glmnet. I use Python but R users will easily adapt the code. 
%matplotlib notebook

import numpy as np
import pylab as plt
import seaborn as sns; sns.set()

import glmnet_python    # from https://web.stanford.edu/~hastie/glmnet_python/
from cvglmnet import cvglmnet; from cvglmnetPlot import cvglmnetPlot

# 80x1112 data table; first column is y, rest is X. All variables are standardized
mydata = np.loadtxt('../q328630.txt')   # file is here https://pastebin.com/raw/p1cCCYBR
y = mydata[:,:1]
X = mydata[:,1:]

# select p here (try 1000 and 100)
p = 1000

# randomly selecting p variables out of 1111
np.random.seed(42)
X = X[:, np.random.permutation(X.shape[1])[:p]]

fit = cvglmnet(x = X.copy(), y = y.copy(), alpha = 0, standardize = False, intr = False, 
               lambdau=np.array([.0001, .001, .01, .1, 1, 10, 100, 1000, 10000, 100000]))
cvglmnetPlot(fit)
plt.gcf().set_size_inches(6,3)
plt.tight_layout()



","['cross-validation', 'overfitting', 'ridge-regression', 'regularization']","A natural regularization happens because of the presence of many small components in the theoretical PCA of $x$. These small components are implicitly used to fit the noise using small coefficients. When using minimum norm OLS, you fit the noise with many small independent components and this has a regularizing effect equivalent to Ridge regularization. This regularization is often too strong, and it is possible to compensate it using ""anti-regularization"" know as negative Ridge. In that case, you will see the minimum of the MSE curve appears for negative values of $\lambda$.By theoretical PCA, I mean:Let $x\sim N(0,\Sigma)$ a multivariate normal distribution. There is a
  linear isometry $f$ such as $u=f(x)\sim N(0,D)$ where $D$ is diagonal:
  the components of $u$ are independent. $D$ is simply obtained by diagonalizing $\Sigma$.Now the model $y=\beta.x+\epsilon$ can be written
  $y=f(\beta).f(x)+\epsilon$ (a linear isometry preserves dot product).
  If you write $\gamma=f(\beta)$, the model can be written
  $y=\gamma.u+\epsilon$. Furthermore $\|\beta\|=\|\gamma\|$ hence
  fitting methods like Ridge or minimum norm OLS are perfectly
  isomorphic: the estimator of $y=\gamma.u+\epsilon$  is the image by $f$
  of the estimator of $y=\beta.x+\epsilon$.Theoretical PCA transforms non independent predictors into independent predictors. It is only loosely related to empirical PCA where you use the empirical covariance matrix (that differs a lot from the theoretical one with small sample size). Theoretical PCA is not practically computable but is only used here to interpret the model in an orthogonal predictor space.Let's see what happens when we append many small variance independent predictors to a model:TheoremRidge regularization with coefficient $\lambda$ is equivalent (when $p\rightarrow\infty$) to:(sketch of) ProofWe are going to prove that the cost functions are asymptotically
  equal. Let's split the model into real and fake predictors: $y=\beta x+\beta'x'+\epsilon$. The cost function of Ridge (for the true
  predictors) can be written:$$\mathrm{cost}_\lambda=\|\beta\|^2+\frac{1}{\lambda}\|y-X\beta\|^2$$When using minimum norm OLS, the response is fitted perfectly: the
  error term is 0. The cost function is only about the norm of the
  parameters. It can be split into the true parameters and the fake
  ones:$$\mathrm{cost}_{\lambda,p}=\|\beta\|^2+\inf\{\|\beta'\|^2 \mid X'\beta'=y-X\beta\}$$In the right expression, the minimum norm solution is given by:$$\beta'=X'^+(y-X\beta )$$Now using SVD for $X'$:$$X'=U\Sigma V$$$$X'^{+}=V^\top\Sigma^{+} U^\top$$We see that the norm of $\beta'$ essentially depends on the singular
  values of $X'^+$ that are the reciprocals of the singular values of
  $X'$. The normalized version of $X'$ is $\sqrt{p/\lambda} X'$. I've
  looked at literature and singular values of large random matrices are
  well known. For $p$ and $n$ large enough, minimum $s_\min$ and maximum
  $s_\max$ singular values are approximated by (see theorem 1.1):$$s_\min(\sqrt{p/\lambda}X')\approx \sqrt p\left(1-\sqrt{n/p}\right)$$ 
  $$s_\max(\sqrt{p/\lambda}X')\approx \sqrt p \left(1+\sqrt{n/p}\right)$$Since, for large $p$, $\sqrt{n/p}$ tends towards 0, we can just say
  that all singular values are approximated by $\sqrt p$. Thus:$$\|\beta'\|\approx\frac{1}{\sqrt\lambda}\|y-X\beta\|$$Finally:$$\mathrm{cost}_{\lambda,p}\approx\|\beta\|^2+\frac{1}{\lambda}\|y-X\beta\|^2=\mathrm{cost}_\lambda$$Note: it does not matter if you keep the coefficients of the fake
  predictors in your model. The variance introduced by $\beta'x'$ is
  $\frac{\lambda}{p}\|\beta'\|^2\approx\frac{1}{p}\|y-X\beta\|^2\approx\frac{n}{p}MSE(\beta)$.
  Thus you increase your MSE by a factor $1+n/p$ only which tends
  towards 1 anyway. Somehow you don't need to treat the
  fake predictors differently than the real ones.Now, back to @amoeba's data. After applying theoretical PCA to $x$ (assumed to be normal), $x$ is transformed by a linear isometry into a variable $u$ whose components are independent and sorted in decreasing variance order. The problem $y=\beta x+\epsilon$ is equivalent the transformed problem $y=\gamma u+\epsilon$.Now imagine the variance of the components look like:Consider many $p$ of the last components, call the sum of their variance $\lambda$. They each have a variance approximatively equal to $\lambda/p$ and are independent. They play the role of the fake predictors in the theorem.This fact is clearer in @jonny's model: only the first component of theoretical PCA  is correlated to $y$ (it is proportional $\overline{x}$) and has huge variance. All the other components (proportional to $x_i-\overline{x}$) have comparatively very small variance (write the covariance matrix and diagonalize it to see this) and play the role of fake predictors. I calculated that the regularization here corresponds (approx.) to prior $N(0,\frac{1}{p^2})$ on $\gamma_1$ while the true $\gamma_1^2=\frac{1}{p}$. This definitely over-shrinks. This is visible by the fact that the final MSE is much larger than the ideal MSE. The regularization effect is too strong. It is sometimes possible to improve this natural regularization by Ridge. First you sometimes need $p$ in the theorem really big (1000, 10000...) to seriously rival Ridge and the finiteness of $p$ is like an imprecision. But it also shows that Ridge is an additional regularization over a naturally existing implicit regularization and can thus have only a very small effect. Sometimes this natural regularization is already too strong and Ridge may not even be an improvement. More than this, it is better to use anti-regularization: Ridge with negative coefficient. This shows MSE for @jonny's model ($p=1000$), using $\lambda\in\mathbb{R}$:"
Test if two binomial distributions are statistically different from each other,"
I have three groups of data, each with a binomial distribution (i.e. each group has elements that are either success or failure).  I do not have a predicted probability of success, but instead can only rely on the success rate of each as an approximation for the true success rate. I have only found this question, which is close but does not seem to exactly deal with the this scenario.
To simplify down the test, let's just say that I have 2 groups (3 can be extended from this base case).




Group
Trials $n_i$
Successes $k_i$
Percentage $p_i$




Group 1
2455
1556
63.4%


Group 2
2730
1671
61.2%




I don't have an expected success probability, only what I know from the samples.
The success rate of each of the sample is fairly close. However my sample sizes are also quite large. If I check the CDF of the binomial distribution to see how different it is from the first (where I'm assuming the first is the null test) I get a very small probability that the second could be achieved.
In Excel:

1-BINOM.DIST(1556,2455,61.2%,TRUE) = 0.012

However, this does not take into account any variance of the first result, it just assumes the first result is the test probability.
Is there a better way to test if these two samples of data are actually statistically different from one another?
","['statistical-significance', 'binomial-distribution', 'bernoulli-distribution']","The solution is a simple google away: http://en.wikipedia.org/wiki/Statistical_hypothesis_testingSo you would like to test the following null hypothesis against the given alternative$H_0:p_1=p_2$ versus $H_A:p_1\neq p_2$So you just need to calculate the test statistic which is$$z=\frac{\hat p_1-\hat p_2}{\sqrt{\hat p(1-\hat p)\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}$$where $\hat p=\frac{n_1\hat p_1+n_2\hat p_2}{n_1+n_2}$.So now, in your problem, $\hat p_1=.634$, $\hat p_2=.612$, $n_1=2455$ and $n_2=2730.$Once you calculate the test statistic, you just need to calculate the corresponding critical region value to compare your test statistic too.  For example, if you are testing this hypothesis at the 95% confidence level then you need to compare the absolute value of your test statistic against the critical region value of $z_{\alpha/2}=1.96$ (for this two tailed test).Now, if $|z|>z_{\alpha/2}$ then you may reject the null hypothesis, otherwise you must fail to reject the null hypothesis.Well this solution works for the case when you are comparing two groups, but it does not generalize to the case where you want to compare 3 groups.You could however use a Chi Squared test to test if all three groups have equal proportions as suggested by @Eric in his comment above: "" Does this question help? stats.stackexchange.com/questions/25299/ … –  Eric"""
Testing equality of coefficients from two different regressions,"
This seems to be a basic issue, but I just realized that I actually don't know how to test equality of coefficients from two different regressions. Can anyone shed some light on this? 
More formally, suppose I ran the following two regressions: 
$$
y_1 = X_1\beta_1 + \epsilon_1
$$
and 
$$
y_2 = X_2\beta_2 + \epsilon_2
$$
where $X_i$ refers to the design matrix of regression $i$, and $\beta_i$ to the vector of coefficients in regression $i$. Note that $X_1$ and $X_2$ are potentially very different, with different dimensions etc. I am interested in for instance whether or not $\hat\beta_{11} \neq \hat\beta_{21}$. 
If these came from the same regression, this would be trivial. But since they come from different ones, I am not quite sure how to do it. Does anyone have an idea or can give me some pointers? 
My problem in detail: My first intuition was to look at the confidence intervals, and if they overlap, then I would say they are essentially the same. This procedure does not come with the correct size of the test, though (i.e. each individual confidence interval has $\alpha=0.05$, say, but looking at them jointly will not have the same probability). My ""second"" intuition was to conduct a normal t-test. That is, take 
$$
\frac{\beta_{11}-\beta_{21}}{sd(\beta_{11})}
$$
where $\beta_{21}$ is taken as the value of my null hypothesis. This does not take into account the estimation uncertainty of $\beta_{21}$, though, and the answer may depend on the order of the regressions (which one I call 1 and 2). 
My third idea was to do it as in a standard test for equality of two coefficients from the same regression, that is take 
$$
\frac{\beta_{11}-\beta_{21}}{sd(\beta_{11}-\beta_{21})}
$$ 
The complication arises due to the fact that both come from different regressions. Note that 
$$
Var(\beta_{11}-\beta_{21}) = Var(\beta_{11}) + Var(\beta_{21}) -2 Cov(\beta_{11},\beta_{21})
$$
but since they are from different regressions, how would I get $Cov(\beta_{11},\beta_{21})$?
This led me to ask this question here. This must be a standard procedure / standard test, but I cound not find anything that was sufficiently similar to this problem. So, if anyone can point me to the correct procedure, I would be very grateful!
","['hypothesis-testing', 'inference']","Although this isn't a common analysis, it really is one of interest.  The accepted answer fits the way you asked your question, but I'm going to provide another reasonably well accepted technique that may or may not be equivalent (I'll leave it to better minds to comment on that).This approach is to use the following Z test:$Z = \frac{\beta_1-\beta_2}{\sqrt{(SE\beta_1)^2+(SE\beta_2)^2}}$Where $SE\beta$ is the standard error of $\beta$.This equation is provided by Clogg, C. C., Petkova, E., & Haritou, A. (1995). Statistical methods for comparing regression coefficients between models. American Journal of Sociology, 100(5), 1261-1293. and is cited by Paternoster, R., Brame, R., Mazerolle, P., & Piquero, A. (1998). Using the correct statistical test for equality of regression coefficients. Criminology, 36(4), 859-866. equation 4, which is available free of a paywall.  I've adapted Peternoster's formula to use $\beta$ rather than $b$ because it is possible that you might be interested in different DVs for some awful reason and my memory of Clogg et al. was that their formula used $\beta$.  I also remember cross checking this formula against Cohen, Cohen, West, and Aiken, and the root of the same thinking can be found there in the confidence interval of differences between coefficients, equation 2.8.6, pg 46-47."
What is the difference between a neural network and a deep belief network?,"
I am getting the impression that when people are referring to a 'deep belief' network that this is basically a neural network but very large. Is this correct or does a deep belief network also imply that the algorithm itself is different (ie, no feed forward neural net but perhaps something with feedback loops)? 
","['machine-learning', 'neural-networks', 'deep-learning', 'deep-belief-networks']","""Neural networks"" is a term usually used to refer to feedforward neural networks. Deep Neural Networks are feedforward Neural Networks with many layers. A Deep belief network is not the same as a Deep Neural Network.As you have pointed out a deep belief network has undirected connections between some layers. This means that the topology of the DNN and DBN is different by definition.The undirected layers in the DBN are called Restricted Boltzmann Machines. This layers can be trained using an unsupervised learning algorithm (Contrastive Divergence) that is very fast (Here's a link! with details).Some more comments:The solutions obtained with deeper neural networks correspond to solutions that perform worse than the solutions obtained for networks with 1 or 2 hidden layers. As the architecture gets deeper, it becomes more difficult to obtain good generalization using a Deep NN.In 2006 Hinton  discovered that much better results could be achieved in deeper architectures when each layer (RBM) is pre-trained with an unsupervised learning algorithm (Contrastive Divergence). Then the Network can be trained in a supervised way using backpropagation in order to ""fine-tune"" the weights."
Proper way of using recurrent neural network for time series analysis,"
Recurrent neural networks differ from ""regular"" ones by the fact that they have a ""memory"" layer. Due to this layer, recurrent NN's are supposed to be useful in time series modelling. However, I'm not sure I understand correctly how to use them.
Let's say I have the following time series (from left to right): [0, 1, 2, 3, 4, 5, 6, 7], my goal is to predict i-th point using points i-1 and i-2 as an input (for each i>2). In a ""regular"", non-recurring ANN I would do process the data as follows:

 target| input
      2| 1 0
      3| 2 1
      4| 3 2
      5| 4 3
      6| 5 4
      7| 6 5 


I would then create a net with two input and one output node and train it with the data above.
How does one need to alter this process (if at all) in the case of recurrent networks? 
","['time-series', 'machine-learning', 'neural-networks']","What you describe is in fact a ""sliding time window"" approach and is different to recurrent networks. You can use this technique with any regression algorithm. There is a huge limitation to this approach: events in the inputs can only be correlatd with other inputs/outputs which lie at most t timesteps apart, where t is the size of the window.E.g. you can think of a Markov chain of order t. RNNs don't suffer from this in theory, however in practice learning is difficult.It is best to illustrate an RNN in contrast to a feedfoward network. Consider the (very) simple feedforward network $y = Wx$ where $y$ is the output, $W$ is the weight matrix, and $x$ is the input.Now, we use a recurrent network. Now we have a sequence of inputs, so we will denote the inputs by $x^{i}$ for the ith input. The corresponding ith output is then calculated via $y^{i} = Wx^i + W_ry^{i-1}$.Thus, we have another weight matrix $W_r$ which incorporates the output at the previous step linearly into the current output.This is of course a simple architecture. Most common is an architecture where you have a hidden layer which is recurrently connected to itself. Let $h^i$ denote the hidden layer at timestep i. The formulas are then:$$h^0 = 0$$
$$h^i = \sigma(W_1x^i + W_rh^{i-1})$$
$$y^i = W_2h^i$$Where $\sigma$ is a suitable non-linearity/transfer function like the sigmoid. $W_1$ and $W_2$ are the connecting weights between the input and the hidden and the hidden and the output layer. $W_r$ represents the recurrent weights.Here is a diagram of the structure: "
Regression with multiple dependent variables?,"
Is it possible to have a (multiple) regression equation with two or more dependent variables?  Sure, you could run two separate regression equations, one for each DV, but that doesn't seem like it would capture any relationship between the two DVs?
",['regression'],"Yes, it is possible.  What you're interested is is called ""Multivariate Multiple Regression"" or just ""Multivariate Regression"".  I don't know what software you are using, but you can do this in R.Here's a link that provides examples."
What are the worst (commonly adopted) ideas/principles in statistics?,"
In my statistical teaching, I encounter some stubborn ideas/principles relating to statistics that have become popularised, yet seem to me to be misleading, or in some cases utterly without merit.  I would like to solicit the views of others on this forum to see what are the worst (commonly adopted) ideas/principles in statistical analysis/inference.  I am mostly interested in ideas that are not just novice errors; i.e., ideas that are accepted and practiced by some actual statisticians/data analysts.  To allow efficient voting on these, please give only one bad principle per answer, but feel free to give multiple answers.
","['inference', 'teaching', 'philosophical']",
Taleb and the Black Swan,"
Taleb's book ""The Black Swan"" was a New York Times best seller when it came out several years ago.  The book is now in its second edition.  After meeting with statisticians at a JSM (an annual statistical conference), Taleb toned down his criticism of statistics somewhat.  But the thrust of the book is that statistics is not very useful because it relies on the normal distribution and very rare events: ""Black Swans"" don't have normal distributions.  
Do you think this is valid criticism? Is Taleb missing some important aspects of statistical modeling?  Can rare events be predicted at least in the sense that probabilities of occurrences can be estimated?
","['extreme-value', 'rare-events']","I read the Black Swan a couple of years ago.  The Black Swan idea is good and the attack on the ludic fallacy (seeing things as though they are dice games, with knowable probabilities) is good but statistics is outrageously misrepresented, with the central problem being the wrong claim that all statistics falls apart if variables are not normally distributed.  I was sufficiently annoyed by this aspect to write Taleb the letter below:Dear Dr TalebI recently read ""The Black Swan"".  Like you, I am a fan of Karl Popper, and I found myself agreeing with much that is in it.  I think your exposition of the ludic fallacy is basically sound, and draws attention to a real and common problem.  However, I think that much of Part III lets your overall argument down badly, even to the point of possibly discrediting the rest of the book.  This is a shame, as I think the arguments with regard to Black Swans and ""unknown unknowns"" stand on their merits without relying on some of the errors in Part III.The main issue I wish to point out - and seek your response on, particularly if I have misunderstood issues - is your misrepresentation of the field of applied statistics.  In my judgement, chapters 14, 15 and 16 depend largely upon a straw man argument, misrepresenting statistics and econometrics.  The field of econometrics that you describe is not the one that I was taught when I studied applied statistics, econometrics, and actuarial risk theory (at the Australian National University, but using texts that seemed pretty standard).  The issues that you raise (such as the limitations of Gaussian distributions) are well and truly understood and taught, even at the undergraduate level.For example, you go to some lengths to show how income distribution does not follow a normal distribution, and present this as an argument against statistical practice in general.  No competent statistician would ever claim that it does, and ways of dealing with this issue are well established.  Just using techniques from the very most basic ""first year econometrics"" level, for example, transforming the variable by taking its logarithm would make your numerical examples look much less convincing.  Such a transformation would in fact invalidate much of what you say, because then the variance of the original variable does increase as its mean increases.I am sure there are some incompetent econometricians who do OLS regressions etc with an untransformed response variable the way you say, but that just makes them incompetent and using techniques which are well established to be inappropriate.  They would certainly have been failed even in undergraduate courses, which spend much time looking for more appropriate ways of modelling variables such as income, reflecting the actual observed (non-Gaussian) distribution.The family of Generalized Linear Models is one set of techniques developed in part to get around the problems you raise.  Many of the exponential family of distributions (eg Gamma, Exponential, and Poisson distributions) are asymmetrical and have variance that increases as the centre of the distribution increases, getting around the problem you point out with using the Gaussian distribution.  If this is still too limiting, it is possible to drop a pre-existing ""shape"" altogether and simply specify a relationship between the mean of a distribution and its variance (eg allowing the variance to increase proportionately to the square of the mean), using the ""quasi-likelihood"" method of estimation.Of course, you could argue that this form of modelling is still too simplistic and an intellectual trap that lulls us into thinking the future will be like the past.  You may be correct, and I think the strength of your book is to make people like me consider this.  But you need different arguments to those that you use in chapters 14-16.  The great weight you place on the fact that the variance of the Gaussian distribution is constant regardless of its mean (which causes problems with scalability), for instance, is invalid.  So is your emphasis on the fact that real-life distributions tend to be asymmetric rather than bell-curves.Basically, you have taken one over-simplification of the most basic approach to statistics (naïve modelling of raw variables as having Gaussian distributions) and shown, at great length, (correctly) the shortcomings of such an oversimplified approach.  You then use this to make the gap to discredit the whole field.  This is either a serious lapse in logic, or a propaganda technique.  It is unfortunate because it detracts from your overall argument, much of which (as I said) I found valid and persuasive.I would be interested to hear what you say in response.  I doubt I am the first to have raised this issue.Yours sincerelyPE"
Statistics interview questions,"
I am looking for some statistics (and probability, I guess) interview questions, from the most basic through the more advanced. Answers are not necessary (although links to specific questions on this site would do well).
","['intuition', 'careers']","Not sure what the job is, but I think ""Explain x to a novice"" would probably be good- a) because they will probably need to do this in the jobb) it's a good test of understanding, I reckon."
What is so cool about de Finetti's representation theorem?,"
From Theory of Statistics by Mark J. Schervish (page 12):

Although DeFinetti's representation theorem 1.49 is central to motivating parametric models, it is not actually used in their implementation.

How is the theorem central to parametric models?
","['probability', 'mathematical-statistics', 'modeling', 'exchangeability']","De Finetti's Representation Theorem gives in a single take, within the subjectivistic interpretation of probabilities, the raison d'être of statistical models and the meaning of parameters and their prior distributions. Suppose that the random variables $X_1,\dots,X_n$ represent the results of successive tosses of a coin, with values $1$ and $0$ corresponding to the results ""Heads"" and ""Tails"", respectively. Analyzing, within the context of a subjectivistic interpretation of the probability calculus, the meaning of the usual frequentist model under which the $X_i$'s are independent and identically distributed, De Finetti observed that the condition of independence would imply, for example, that
$$  
  P\{X_n=x_n\mid X_1=x_1,\dots,X_{n-1}=x_{n-1}\} = P\{X_n=x_n\} \, ,
$$
and, therefore, the results of the first $n-1$ tosses would not change my uncertainty about the result of $n$-th toss. For example, if I believe $\textit{a priori}$ that this is a balanced coin, then, after getting the information that the first $999$ tosses turned out to be ""Heads"", I would still believe, conditionally on that information, that the probability of getting ""Heads"" on toss 1000 is equal to  $1/2$. Effectively, the hypothesis of independence of the $X_i$'s would imply that it is impossible to learn anything about the coin by observing the results of its tosses. This observation led De Finetti to the introduction of a condition weaker than independence that resolves this apparent contradiction. The key to De Finetti's solution is a kind of distributional symmetry known as exchangeability.$\textbf{Definition.}$ For a given finite set $\{X_i\}_{i=1}^n$ of random objects, let $\mu_{X_1,\dots,X_n}$ denote their joint distribution. This finite set is exchangeable if $\mu_{X_1,\dots,X_n} = \mu_{X_{\pi(1)},\dots,X_{\pi(n)}}$, for every permutation $\pi:\{1,\dots,n\}\to\{1,\dots,n\}$. A sequence $\{X_i\}_{i=1}^\infty$ of random objects is exchangeable if each of its finite subsets are exchangeable.Supposing only that the sequence of random variables $\{X_i\}_{i=1}^\infty$ is exchangeable, De Finetti proved a notable theorem that sheds light on the meaning of commonly used statistical models. In the particular case when the $X_i$'s take the values $0$ and $1$, De Finetti's Representation Theorem says that $\{X_i\}_{i=1}^\infty$ is exchangeable if and only if there is a random variable $\Theta:\Omega\to[0,1]$, with distribution $\mu_\Theta$, such that 
$$
  P\{X_1=x_1,\dots,X_n=x_n\} = \int_{[0,1]} \theta^s(1-\theta)^{n-s}\,d\mu_\Theta(\theta) \, ,
$$
in which $s=\sum_{i=1}^n x_i$. Moreover, we have that
$$
  \bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \xrightarrow[n\to\infty]{} \Theta \qquad \textrm{almost surely},
$$
which is known as De Finetti's Strong Law of Large Numbers.This Representation Theorem shows how statistical models emerge in a Bayesian context: under the hypothesis of exchangeability of the observables $\{X_i\}_{i=1}^\infty$, $\textbf{there is}$ a $\textit{parameter}$ $\Theta$ such that, given the value of $\Theta$, the observables are $\textit{conditionally}$ independent and identically distributed. Moreover, De Finetti's Strong law shows that our prior opinion about the unobservable $\Theta$, represented by the distribution $\mu_\Theta$, is the opinion about the limit of $\bar{X}_n$, before we have information about the values of the realizations of any of the $X_i$'s. The parameter $\Theta$ plays the role of a useful subsidiary construction, which allows us to obtain conditional probabilities involving only observables through relations like
$$
  P\{X_n=1\mid X_1=x_1,\dots,X_{n-1}=x_{n-1}\} = \mathrm{E}\left[\Theta\mid X_1=x_1,\dots,X_{n-1}=x_{n-1}\right] \, .
$$ "
Are there cases where PCA is more suitable than t-SNE?,"
I want to see how 7 measures of text correction behaviour (time spent correcting the text, number of keystrokes, etc.) relate to each other. The measures are correlated. I ran a PCA to see how the measures projected onto PC1 and PC2, which avoided the overlap of running separate two-way correlation tests between the measures.
I was asked why not using t-SNE, since the relationship between some of the measures might be non-linear.
I can see how allowing for non-linearity would improve this, but I wonder if there is any good reason to use PCA in this case and not t-SNE? I'm not interested in clustering the texts according to their relationship to the measures, but rather in the relationship between the measures themselves.
(I guess EFA could also a better/another approach, but that's a different discussion.)
Compared to other methods, there are few posts on here about t-SNE, so the question seems worth asking.
","['pca', 'tsne']",
How do you calculate the probability density function of the maximum of a sample of IID uniform random variables?,"
Given the random variable
$$Y = \max(X_1, X_2, \ldots, X_n)$$
where $X_i$ are IID uniform variables, how do I calculate the PDF of $Y$?
","['density-function', 'extreme-value']","It is possible that this question is homework but I felt this classical elementary probability question was still lacking a complete answer after several months, so I'll give one here.From the problem statement, we want the distribution of$$Y = \max \{ X_1, ..., X_n \}$$where $X_1, ..., X_n$ are iid ${\rm Uniform}(a,b)$. We know that $Y < x$ if and only if every element of the sample is less than $x$. Then this, as indicated in @varty's hint, combined with the fact that the $X_i$'s are independent, allows us to deduce$$ P(Y \leq x) = P(X_1 \leq x, ..., X_n \leq x) = \prod_{i=1}^{n} P(X_i \leq x) = F_{X}(x)^n$$where $F_{X}(x)$ is the CDF of the uniform distribution that is $\frac{y-a}{b-a}$. Therefore the CDF of $Y$ is
$$F_{Y}(y) = P(Y \leq y) = \begin{cases} 
0 & y \leq a \\ 
\phantom{} \left( \frac{y-a}{b-a} \right)^n & y\in(a,b) \\
1 & y \geq b \\ 
\end{cases}$$Since $Y$ has an absolutely continuous distribution we can derive its density by differentiating the CDF. Therefore the density of $Y$ is$$ p_{Y}(y) = \frac{n(y-a)^{n-1}}{(b-a)^{n}}$$In the special case where $a=0,b=1$, we have that $p_{Y}(y)=ny^{n-1}$, which is the density of a Beta distribution with $\alpha=n$ and $\beta=1$, since ${\rm Beta}(n,1) = \frac{\Gamma(n+1)}{\Gamma(n)\Gamma(1)}=\frac{n!}{(n-1)!} = n$.As a note, the sequence you get if you were to sort your sample in increasing order - $X_{(1)}, ..., X_{(n)}$ - are called the order statistics. A generalization of this answer is that all order statistics of a ${\rm Uniform}(0,1)$ distributed sample have a Beta distribution, as noted in @bnaul's answer."
Does it make sense to add a quadratic term but not the linear term to a model?,"
I have a (mixed) model in which one of my predictors should a priori only be quadratically related to the predictor (due to the experimental manipulation). Hence, I would like to add only the quadratic term to the model. Two things keep me from doing so:

I think I read somehwere that you should always include the lower order polynomial when fitting higher order polynomials. I forgot where I found it and in the literature I looked at (e.g., Faraway, 2002; Fox, 2002) I cannot find a good explanation.
When I add both, the linear and quadratic term, both are significant. When I add only one of them, they are not significant. However, a linear relation of predictor and data is not interpretable.

The context of my question is specifically a mixed-model using lme4, but I would like to get answers that could explain why it is or why it is not okay to inlcude a higher order polynomial and not the lower order polynomial.
If necessary I can provide the data.
","['regression', 'polynomial']","It is illuminating to notice that a quadratic relationship can be written in two ways:$$y = a_0 + a_1 x + a_2 x^2 = a_2(x - b)^2 + c$$(where, equating coefficients, we find $-2a_2 b = a_1$ and $a_2 b^2 + c = a_0$).  The value $x=b$ corresponds to a global extremum of the relationship (geometrically, it locates the vertex of a parabola).If you do not include the linear term $a_1 x$, the possibilities are reduced to$$y = a_0 + a_2 x^2 = a_2(x - 0)^2 + c$$(where now, obviously, $c = a_0$ and it is assumed the model contains a constant term $a_0$).  That is, you force $b=0$.In light of this, question #1 comes down to whether you are certain that the global extremum must occur at $x=0$.  If you are, then you may safely omit the linear term $a_1 x$.  Otherwise, you must include it.This is discussed in great detail in a related thread at https://stats.stackexchange.com/a/28493.In the present case, the significance of $a_2$ indicates there is curvature in the relationship and the significance of $a_1$ indicates that $b$ is nonzero: it sounds like you need to include both terms (as well as the constant, of course)."
When are Log scales appropriate?,"
I've read that using log scales when charting/graphing is appropriate in certain circumstances, like the y-axis in a time series chart.  However, I've not been able to find a definitive explanation as to why that's the case, or when else it would be appropriate.
Please keep in mind, I'm not a statistician so I may be missing the point altogether and if that's the case I'd appreciate direction to remedial resources.
","['data-visualization', 'data-transformation']","This is a very interesting question, and one that too few people think about. There are several different ways that a log scale can be appropriate. The first and most well known is that mentioned by Macro in his comment: log scales allow a large range to be displayed without small values being compressed down into bottom of the graph.A different reason for preferring a log scaling is in circumstances where the data are more naturally expressed geometrically. An example is when the data represent concentration of a biological mediator. Concentrations cannot be negative and the variability almost invariably scales with the mean (i.e. there is heteroscedastic variance). Using a logarithmic scale or, equivalently, using the log concentration as the primary measure both 'fixe' the uneven variability and gives a scale that is unbounded on both ends. The concentrations are probably log-normally distributed and so a log scaling gives us a very convenient result that is arguably 'natural'. In pharmacology we use a logarithmic scale for drug concentrations far more often than not, and in many cases linear scales are only the product of non-pharmacologists dabbling with drugs ;-)Another good reason for a log scale, probably the one that you are interested in for time-series data, comes from the ability of a log scale to make fractional changes equivalent. Imagine a display of the long-term performance of your retirement investments. It (should) be growing roughly exponentially because tomorrow's interest depends on today's investment (roughly speaking). Thus even if the performance in percentage terms has been fairly constant a graph of the funds will appear to have grown most rapidly at the right hand end. With a logarithmic scale a constant percentage change is seen as a constant vertical distance so a constant growth rate is seen as a straight line. That is often a substantial advantage.Another slightly more esoteric reason for choosing a log scale comes in circumstances where values can be reasonably expressed either as x or 1/x. An example from my own research is vascular resistance which can also be sensibly expressed as the reciprocal, vascular conductance. (It is also sensible in some circumstances to think of the diameter of the blood vessels which scale as a power of resistance or conductance.) Neither of those measures has any more reality than the other and both can be found in research papers. If they are scaled logarithmically then they are simply the negative of each other and the choice of one or the other makes no susbstantive difference. (Vascular diameter will differ from resistance and conductance by a constant multiplier when they are all log scaled.)"
Do we need a global test before post hoc tests?,"
I often hear that post hoc tests after an ANOVA can only be used if the ANOVA itself was significant. 

However, post hoc tests adjust $p$-values to keep the global type I error rate at 5%, don't they? 
So why do we need the global test first? 
If we don't need a global test is the terminology ""post hoc"" correct?
Or are there multiple kinds of post hoc tests, some assuming a significant global test result and others without that assumption? 

","['anova', 'statistical-significance', 'post-hoc']",
How can I help ensure testing data does not leak into training data?,"
Suppose we have someone building a predictive model, but that someone is not necessarily well-versed in proper statistical or machine learning principles.  Maybe we are helping that person as they are learning, or maybe that person is using some sort of software package that requires minimal knowledge to use.
Now this person might very well recognize that the real test comes from accuracy (or whatever other metric) on out-of-sample data.  However, my concern is that there are a lot of subtleties there to worry about.  In the simple case, they build their model and evaluate it on training data and evaluate it on held-out testing data.  Unfortunately it can sometimes be all too easy at that point to go back and tweak some modeling parameter and check the results on that same ""testing"" data.  At this point that data is no longer true out-of-sample data though, and overfitting can become a problem.
One potential way to resolve this problem would be to suggest creating many out-of-sample datasets such that each testing dataset can be discarded after use and not reused at all.  This requires a lot of data management though, especially that the splitting must be done before the analysis (so you would need to know how many splits beforehand).
Perhaps a more conventional approach is k-fold cross validation.  However, in some sense that loses the distinction between a ""training"" and ""testing"" dataset that I think can be useful, especially to those still learning.  Also I'm not convinced this makes sense for all types of predictive models.
Is there some way that I've overlooked to help overcome the problem of overfitting and testing leakage while still remaining somewhat clear to an inexperienced user?
","['machine-learning', 'classification', 'predictive-models', 'cross-validation', 'out-of-sample']","You are right, this is a significant problem in machine learning/statistical modelling.  Essentially the only way to really solve this problem is to retain an independent test set and keep it held out until the study is complete and use it for final validation.However, inevitably people will look at the results on the test set and then change their model accordingly; however this won't necessarily result in an improvement in generalisation performance as the difference in performance of different models may be largely due to the particular sample of test data that we have.  In this case, in making a choice we are effectively over-fitting the test error.  The way to limit this is to make the variance of the test error as small as possible (i.e. the variability in test error we would see if we used different samples of data as the test set, drawn from the same underlying distribution).  This is most easily achieved using a large test set if that is possible, or e.g. bootstrapping or cross-validation if there isn't much data available.I have found that this sort of over-fitting in model selection is a lot more troublesome than is generally appreciated, especially with regard to performance estimation, seeG. C. Cawley and N. L. C. Talbot, Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, 2010. Research, vol. 11, pp. 2079-2107, July 2010 (www)This sort of problem especially affects the use of benchmark datasets, which have been used in many studies, and each new study is implicitly affected by the results of earlier studies, so the observed performance is likely to be an over-optimistic estimate of the true performance of the method.  The way I try to get around this is to look at many datasets (so the method isn't tuned to one specific dataset) and also use multiple random test/training splits for performance estimation (to reduce the variance of the estimate).  However the results still need the caveat that these benchmarks have been over-fit.Another example where this does occur is in machine learning competitions with a leader-board based on a validation set.  Inevitably some competitors keep tinkering with their model to get further up the leader board, but then end up towards the bottom of the final rankings.  The reason for this is that their multiple choices have over-fitted the validation set (effectively learning the random variations in the small validation set).If you can't keep a statistically pure test set, then I'm afraid the two best options are (i) collect some new data to make a new statistically pure test set or (ii) make the caveat that the new model was based on a choice made after observing the test set error, so the performance estimate is likely to have an optimistic bias."
Is it true that the percentile bootstrap should never be used?,"
In the MIT OpenCourseWare notes for 18.05 Introduction to Probability and Statistics, Spring 2014 (currently available here), it states:

The bootstrap percentile method is appealing due to its simplicity. However it depends on
  the bootstrap distribution of $\bar{x}^{*}$ based on a particular sample being a good approximation to
  the true distribution of $\bar{x}$. Rice says of the percentile method, ""Although this direct equation
  of quantiles of the bootstrap sampling distribution with confidence limits may seem initially
  appealing, it’s rationale is somewhat obscure.""[2] In short, don’t use the bootstrap
  percentile method. Use the empirical bootstrap instead (we have explained both in the
  hopes that you won’t confuse the empirical bootstrap for the percentile bootstrap).
[2] John Rice, Mathematical Statistics and Data Analysis, 2nd edition, p. 272

After a bit of searching online, this is the only quote I've found which outright states that the percentile bootstrap should not be used. 
What I recall reading from the text Principles and Theory for Data Mining and Machine Learning by Clarke et al. is that the main justification for bootstrapping is the fact that 
$$\dfrac{1}{n}\sum_{i=1}^{n}\hat{F}_n(x) \overset{p}{\to} F(x)$$
where $\hat{F}_n$ is the empirical CDF. (I don't recall details beyond this.)
Is it true that the percentile bootstrap method should not be used? If so, what alternatives are there for when $F$ isn't necessarily known (i.e., not enough information is available to do a parametric bootstrap)?

Update
Because clarification has been requested, the ""empirical bootstrap"" from these MIT notes refers to the following procedure: they compute $\delta_1 = (\hat{\theta}^{*}-\hat{\theta})_{\alpha/2}$ and $\delta_2 =  (\hat{\theta}^{*}-\hat{\theta})_{1-\alpha/2}$ with $\hat{\theta}^{*}$ the bootstrapped estimates of $\theta$ and $\hat{\theta}$ the full-sample estimate of $\theta$, and the resulting estimated confidence interval would be $[\hat{\theta}-\delta_2, \hat{\theta} - \delta_1]$. 
In essence, the main idea is this: empirical bootstrapping estimates an amount proportional to the difference between the point estimate and the actual parameter, i.e., $\hat{\theta}-\theta$, and uses this difference to come up with the lower and upper CI bounds.
The ""percentile bootstrap"" refers to the following: use $[\hat{\theta}^*_{\alpha/2}, \hat{\theta}^*_{1-\alpha/2}]$ as the confidence interval for $\theta$. In this situation, we use bootstrapping to compute estimates of the parameter of interest and take the percentiles of these estimates for the confidence interval.
","['confidence-interval', 'bootstrap']","There are some difficulties that are common to all nonparametric bootstrapping estimates of confidence intervals (CI), some that are more of an issue with both the ""empirical"" (called ""basic"" in the boot.ci() function of the R boot package and in Ref. 1) and the ""percentile"" CI estimates (as described in Ref. 2), and some that can be exacerbated with percentile CIs.TL;DR: In some cases percentile bootstrap CI estimates might work adequately, but if certain assumptions don't hold then the percentile CI might be the worst choice, with the empirical/basic bootstrap the next worst. Other bootstrap CI estimates can be more reliable, with better coverage. All can be problematic. Looking at diagnostic plots, as always, helps avoid potential errors incurred by just accepting the output of a software routine.Bootstrap setupGenerally following the terminology and arguments of Ref. 1, we have a sample of data $y_1, ..., y_n$ drawn from independent and identically distributed random variables $Y_i$ sharing a cumulative distribution function $F$. The empirical distribution function (EDF) constructed from the data sample is $\hat F$. We are interested in a characteristic $\theta$ of the population, estimated by a statistic $T$ whose value in the sample is $t$. We would like to know how well $T$ estimates $\theta$, for example, the distribution of $(T - \theta)$.Nonparametric bootstrap uses sampling from the EDF $\hat F$ to mimic sampling from $F$, taking $R$ samples each of size $n$ with replacement from the $y_i$. Values calculated from the bootstrap samples are denoted with ""*"". For example, the statistic $T$ calculated on bootstrap sample j provides a value $T_j^*$.Empirical/basic versus percentile bootstrap CIsThe empirical/basic bootstrap uses the distribution of $(T^*-t)$ among the $R$ bootstrap samples from $\hat F$ to estimate the distribution of $(T-\theta)$ within the population described by $F$ itself. Its CI estimates are thus based on the distribution of $(T^*-t)$, where $t$ is the value of the statistic in the original sample.This approach is based on the fundamental principle of bootstrapping (Ref. 3):The population is to the sample as the sample is to the bootstrap samples.The percentile bootstrap instead uses quantiles of the $T_j^*$ values themselves to determine the CI. These estimates can be quite different if there is skew or bias in the distribution of $(T-\theta)$.Say that there is an observed bias $B$ such that:
$$\bar T^*=t+B,$$where $\bar T^*$ is the mean of the $T_j^*$. For concreteness, say that the 5th and 95th percentiles of the $T_j^*$ are expressed as $\bar T^*-\delta_1$ and $\bar T^*+\delta_2$, where $\bar T^*$ is the mean over the bootstrap samples and $\delta_1,\delta_2$ are each positive and potentially different to allow for skew. The 5th and 95th CI percentile-based estimates would directly be given respectively by:$$\bar T^*-\delta_1=t+B-\delta_1; \bar T^*+\delta_2=t+B+\delta_2.$$The 5th and 95th percentile CI estimates by the empirical/basic bootstrap method would be respectively (Ref. 1, eq. 5.6, page 194):$$2t-(\bar T^*+\delta_2) = t-B-\delta_2; 2t-(\bar T^*-\delta_1) = t-B+\delta_1.$$So percentile-based CIs both get the bias wrong and flip the directions of the potentially asymmetric positions of the confidence limits around a doubly-biased center. The percentile CIs from bootstrapping in such a case do not represent the distribution of $(T-\theta)$.This behavior is nicely illustrated on this page, for bootstrapping a statistic so negatively biased that the original sample estimate is below the 95% CIs based on the empirical/basic method (which directly includes appropriate bias correction). The 95% CIs based on the percentile method, arranged around a doubly-negatively biased center, are actually both below even the negatively biased point estimate from the original sample!Should the percentile bootstrap never be used?That might be an overstatement or an understatement, depending on your perspective. If you can document minimal bias and skew, for example by visualizing the distribution of $(T^*-t)$ with histograms or density plots, the percentile bootstrap should provide essentially the same CI as the empirical/basic CI. These are probably both better than the simple normal approximation to the CI.Neither approach, however, provides the accuracy in coverage that can be provided by other bootstrap approaches. Efron from the beginning recognized potential limitations of percentile CIs but said: ""Mostly we will be content to let the varying degrees of success of the examples speak for themselves."" (Ref. 2, page 3)Subsequent work, summarized for example by DiCiccio and Efron (Ref. 4), developed methods that ""improve by an order of magnitude upon the accuracy of the standard intervals"" provided by the empirical/basic or percentile methods. Thus one might argue that neither the empirical/basic nor the percentile methods should be used, if you care about accuracy of the intervals.In extreme cases, for example sampling directly from a lognormal distribution without transformation, no bootstrapped CI estimates might be reliable, as Frank Harrell has noted.What limits the reliability of these and other bootstrapped CIs?Several issues can tend to make bootstrapped CIs unreliable. Some apply to all approaches, others can be alleviated by approaches other than the empirical/basic or percentile methods.The first, general, issue is how well the empirical distribution $\hat F$ represents the population distribution $F$. If it doesn't, then no bootstrapping method will be reliable. In particular, bootstrapping to determine anything close to extreme values of a distribution can be unreliable. This issue is discussed elsewhere on this site, for example here and here. The few, discrete, values available in the tails of $\hat F$ for any particular sample might not represent the tails of a continuous $F$ very well. An extreme but illustrative case is trying to use bootstrapping to estimate the maximum order statistic of a random sample from a uniform $\;\mathcal{U}[0,\theta]$ distribution, as explained nicely here. Note that bootstrapped 95% or 99% CI are themselves at tails of a distribution and thus could suffer from such a problem, particularly with small sample sizes.Second, there is no assurance that sampling of any quantity from $\hat F$ will have the same distribution as sampling it from $F$. Yet that assumption underlies the fundamental principle of bootstrapping. Quantities with that desirable property are called pivotal. As AdamO explains:This means that if the underlying parameter changes, the shape of the distribution is only shifted by a constant, and the scale does not necessarily change. This is a strong assumption!For example, if there is bias it's important to know that sampling from $F$ around $\theta$ is the same as sampling from $\hat F$ around $t$. And this is a particular problem in nonparametric sampling; as Ref. 1 puts it on page 33:In nonparametric problems the situation is more complicated. It is now unlikely (but not strictly impossible) that any quantity can be exactly pivotal.So the best that's typically possible is an approximation. This problem, however, can often be addressed adequately. It's possible to estimate how closely a sampled quantity is to pivotal, for example with pivot plots as recommended by Canty et al. These can display how distributions of bootstrapped estimates $(T^*-t)$ vary with $t$, or how well a transformation $h$ provides a quantity $(h(T^*)-h(t))$ that is pivotal. Methods for improved bootstrapped CIs can try to find a transformation $h$ such that $(h(T^*)-h(t))$ is closer to pivotal for estimating CIs in the transformed scale, then transform back to the original scale.The boot.ci() function provides studentized bootstrap CIs (called ""bootstrap-t"" by DiCiccio and Efron) and $BC_a$ CIs (bias corrected and accelerated, where the ""acceleration"" deals with skew) that are ""second-order accurate"" in that the difference between the desired and achieved coverage $\alpha$ (e.g., 95% CI) is on the order of $n^{-1}$, versus only first-order accurate (order of $n^{-0.5}$) for the empirical/basic and percentile methods (Ref 1, pp. 212-3; Ref. 4). These methods, however, require keeping track of the variances within each of the bootstrapped samples, not just the individual values of the $T_j^*$ used by those simpler methods.In extreme cases, one might need to resort to bootstrapping within the bootstrapped samples themselves to provide adequate adjustment of confidence intervals. This ""Double Bootstrap"" is described in Section 5.6 of Ref. 1, with other chapters in that book suggesting ways to minimize its extreme computational demands.Davison, A. C.  and Hinkley, D. V. Bootstrap Methods and their Application, Cambridge University Press, 1997.Efron, B. Bootstrap Methods: Another look at the jacknife, Ann. Statist. 7: 1-26, 1979.Fox, J. and Weisberg, S. Bootstrapping regression models in R. An Appendix to An R Companion to Applied Regression, Third Edition (Sage, 2019). Revision as of 21 September 2018.DiCiccio, T. J. and Efron, B. Bootstrap confidence intervals. Stat. Sci. 11: 189-228, 1996.Canty, A. J., Davison, A. C., Hinkley, D. V., and Ventura, V. Bootstrap diagnostics and remedies. Can. J. Stat. 34: 5-27, 2006."
Data normalization and standardization in neural networks,"
I am trying to predict the outcome of a complex system using neural networks (ANN's). The outcome (dependent) values range between 0 and 10,000. The different input variables have different ranges. All the variables have roughly normal distributions. 
I consider different options to scale the data before training. One option is to scale the input (independent) and output (dependent) variables to [0, 1] by computing cumulative distribution function using the mean and standard deviation values of each variable, independently. The problem with this method is that if I use the sigmoid activation function at the output, I will very likely miss extreme data, especially those not seen in the training set
Another option is to use a z-score. In that case I don't have the extreme data problem; however, I'm limited to a linear activation function at the output. 
What are other accepted normalization techniques that are in use with ANN's? I tried to look for reviews on this topic, but failed to find anything useful.
","['machine-learning', 'neural-networks', 'normalization', 'standardization']","A standard approach is to scale the inputs to have mean 0 and a variance of 1. Also linear decorrelation/whitening/pca helps a lot.If you are interested in the tricks of the trade, I can recommend LeCun's efficient backprop paper."
"What is a good, convincing example in which p-values are useful?","
My question in the title is self explanatory, but I would like to give it some context.
The ASA released a statement earlier this week “on p-values: context, process, and purpose”, outlining various common misconceptions of the p-value, and urging caution in not using it without context and thought (which could be said just about any statistical method, really).
In response to the ASA, professor Matloff wrote a blog post titled: After 150 Years, the ASA Says No to p-values. Then professor Benjamini (and I) wrote a response post titled It’s not the p-values’ fault – reflections on the recent ASA statement. In response to it professor Matloff asked in a followup post:

What I would like to see [... is] — a good, convincing example
  in which p-values are useful. That really has to be the bottom line.

To quote his two major arguments against the usefulness of the $p$-value:


With large samples, significance tests pounce on tiny, unimportant departures from the null hypothesis.


Almost no null hypotheses are true in the real world, so performing a significance test on them is absurd and bizarre.


I am very interested in what other crossvalidated community members think of this question/arguments, and of what may constitute a good response to it.
","['hypothesis-testing', 'bayesian', 'p-value', 'inference', 'frequentist']","I will consider both Matloff's points:With large samples, significance tests pounce on tiny, unimportant departures from the null hypothesis.The logic here is that if somebody reports highly significant $p=0.0001$, then from this number alone we cannot say if the effect is large and important or irrelevantly tiny (as can happen with large $n$). I find this argument strange and cannot connect to it at all, because I have never seen a study that would report a $p$-value without reporting [some equivalent of] effect size. Studies that I read would e.g. say (and usually show on a figure) that group A had such and such mean, group B had such and such mean and they were significantly different with such and such $p$-value. I can obviously judge for myself if the difference between A and B is large or small.(In the comments, @RobinEkman pointed me to several highly-cited studies by Ziliak & McCloskey (1996, 2004) who observed that the majority of the economics papers trumpet ""statistical significance"" of some effects without paying much attention to the effect size and its ""practical significance"" (which, Z&MS argue, can often be minuscule). This is clearly bad practice. However, as @MatteoS explained below, the effect sizes (regression estimates) are always reported, so my argument stands.)Almost no null hypotheses are true in the real world, so performing a significance test on them is absurd and bizarre.This concern is also often voiced, but here again I cannot really connect to it. It is important to realize that researchers do not increase their $n$ ad infinitum. In the branch of neuroscience that I am familiar with, people will do experiments with $n=20$ or maybe $n=50$, say, rats. If there is no effect to be seen then the conclusion is that the effect is not large enough to be interesting. Nobody I know would go on breeding, training, recording, and sacrificing $n=5000$ rats to show that there is some statistically significant but tiny effect. And whereas it might be true that almost no real effects are exactly zero, it is certainly true that many many real effects are small enough to be detected with reasonable sample sizes that reasonable researchers are actually using, exercising their good judgment. (There is a valid concern that sample sizes are often not big enough and that many studies are underpowered. So perhaps researchers in many fields should rather aim at, say, $n=100$ instead of $n=20$. Still, whatever the sample size is, it puts a limit on the effect size that the study has power to detect.)In addition, I do not think I agree that almost no null hypotheses are true, at least not in the experimental randomized studies (as opposed to observational ones). Two reasons:Very often there is a directionality to the prediction that is being tested; researcher aims to demonstrate that some effect is positive $\delta>0$. By convention this is usually done with a two-sided test assuming a point null $H_0: \delta=0$ but in fact this is rather a one-sided test trying to reject $H_0: \delta<0$. (@CliffAB's answer, +1, makes a related point.) And this can certainly be true. Even talking about the point ""nil"" null $H_0: \delta=0$, I do not see why they are never true. Some things are just not causally related to other things. Look at the psychology studies that are failing to replicate in the last years: people feeling the future; women dressing in red when ovulating; priming with old-age-related words affecting walking speed; etc. It might very well be that there are no causal links here at all and so the true effects are exactly zero.Himself, Norm Matloff suggests to use confidence intervals instead of $p$-values because they show the effect size. Confidence intervals are good, but notice one disadvantage of a confidence interval as compared to the $p$-value: confidence interval is reported for one particular coverage value, e.g. $95\%$. Seeing a $95\%$ confidence interval does not tell me how broad a $99\%$ confidence interval would be. But one single $p$-value can be compared with any $\alpha$ and different readers can have different alphas in mind.In other words, I think that for somebody who likes to use confidence intervals, a $p$-value is a useful and meaningful additional statistic to report.I would like to give a long quote about the practical usefulness of $p$-values from my favorite blogger Scott Alexander; he is not a statistician (he is a psychiatrist) but has lots of experience with reading psychological/medical literature and scrutinizing the statistics therein. The quote is from his blog post on the fake chocolate study which I highly recommend. Emphasis mine.[...] But suppose we're not allowed to do $p$-values. All I do is tell you ""Yeah, there was a study with fifteen people that found chocolate helped with insulin resistance"" and you laugh in my face. Effect size is supposed to help with that. But suppose I tell you ""There was a study with fifteen people that found chocolate helped with insulin resistance. The effect size was $0.6$."" I don't have any intuition at all for whether or not that's consistent with random noise. Do you? Okay, then they say we’re supposed to report confidence intervals. The effect size was $0.6$, with $95\%$ confidence interval of $[0.2, 1.0]$. Okay. So I check the lower bound of the confidence interval, I see it’s different from zero. But now I’m not transcending the $p$-value. I’m just using the p-value by doing a sort of kludgy calculation of it myself – “$95\%$ confidence interval does not include zero” is the same as “$p$-value is less than $0.05$”.(Imagine that, although I know the $95\%$ confidence interval doesn’t include zero, I start wondering if the $99\%$ confidence interval does. If only there were some statistic that would give me this information!)But wouldn’t getting rid of $p$-values prevent “$p$-hacking”? Maybe, but it would just give way to “d-hacking”. You don’t think you could test for twenty different metabolic parameters and only report the one with the highest effect size? The only difference would be that p-hacking is completely transparent – if you do twenty tests and report a $p$ of $0.05$, I know you’re an idiot – but d-hacking would be inscrutable. If you do twenty tests and report that one of them got a $d = 0.6$, is that impressive? [...]But wouldn’t switching from $p$-values to effect sizes prevent people from making a big deal about tiny effects that are nevertheless statistically significant? Yes, but sometimes we want to make a big deal about tiny effects that are nevertheless statistically significant! Suppose that Coca-Cola is testing a new product additive, and finds in large epidemiological studies that it causes one extra death per hundred thousand people per year. That’s an effect size of approximately zero, but it might still be statistically significant. And since about a billion people worldwide drink Coke each year, that’s a ten thousand deaths. If Coke said “Nope, effect size too small, not worth thinking about”, they would kill almost two milli-Hitlers worth of people.For some further discussion of various alternatives to $p$-values (including Bayesian ones), see my answer in ASA discusses limitations of $p$-values - what are the alternatives?"
Derivation of closed form lasso solution,"
For the lasso problem
$\min_\beta (Y-X\beta)^T(Y-X\beta)$ such that $\|\beta\|_1 \leq t$. I often see the soft-thresholding result
$$ \beta_j^{\text{lasso}}= \mathrm{sgn}(\beta^{\text{LS}}_j)(|\beta_j^{\text{LS}}|-\gamma)^+ $$ 
for the orthonormal $X$ case. It is claimed that the solution can be ""easily shown"" to be such, but I've never seen a worked solution. Has anyone seen one or perhaps has done the derivation? 
",['lasso'],
What is the proper usage of scale_pos_weight in xgboost for imbalanced datasets?,"
I have a very imbalanced dataset. I'm trying to follow the tuning advice and use scale_pos_weight but not sure how should I tune it. 
I can see that RegLossObj.GetGradient does:
if (info.labels[i] == 1.0f) w *= param_.scale_pos_weight

so a gradient of a positive sample would be more influential. However, according to the xgboost paper, the gradient statistic is always used locally = within the instances of a specific node in a specific tree:

within the context of a node, to evaluate the loss reduction of a candidate split
within the context of a leaf node, to optimize the weight given to that node

So there's no way of knowing in advance what would be a good scale_pos_weight - it is a very different number for a node that ends up with 1:100 ratio between positive and negative instances, and for a node with a 1:2 ratio.
Any hints? 
","['unbalanced-classes', 'boosting']","Generally, scale_pos_weight is the ratio of number of negative class to the positive class.Suppose, the dataset has 90 observations of negative class and 10 observations of positive class, then ideal value of scale_pos_weight should be 9.See the doc:
http://xgboost.readthedocs.io/en/latest/parameter.html "
Why is tanh almost always better than sigmoid as an activation function?,"
In Andrew Ng's Neural Networks and Deep Learning course on Coursera he says that using $tanh$ is almost always preferable to using $sigmoid$.
The reason he gives is that the outputs using $tanh$ centre around 0 rather than $sigmoid$'s 0.5, and this ""makes learning for the next layer a little bit easier"".

Why does centring the activation's output speed learning? I assume he's referring to the previous layer as learning happens during backprop?
Are there any other features that make $tanh$ preferable? Would the steeper gradient delay vanishing gradients?
Are there any situations where $sigmoid$ would be preferable?

Math-light, intuitive  answers preferred.
","['machine-learning', 'neural-networks', 'backpropagation', 'sigmoid-curve']","Yan LeCun and others argue in Efficient BackProp thatConvergence is usually faster if the average of each input variable over the training set is close to zero. To see this, consider the extreme case where all the inputs are positive. Weights to a particular node in the first weight layer are updated by an amount proportional to $\delta x$ where $\delta$ is the (scalar) error at that node and $x$ is the input vector (see equations (5) and (10)). When all of the components of an input vector are positive, all of the updates of weights that feed into a node will have the same sign (i.e. sign($\delta$)). As a result, these weights can only all decrease or all increase together for a given input pattern. Thus, if a weight vector must change direction it can only do so by zigzagging which is inefficient and thus very slow.This is why you should normalize your inputs so that the average is zero.The same logic applies to middle layers:This heuristic should be applied at all layers which means that we want the average of the outputs of a node to be close to zero because these outputs are the inputs to the next layer.Postscript @craq makes the point that this quote doesn't make sense for ReLU(x)=max(0,x) which has become a widely popular activation function. While ReLU does avoid the first zigzag problem mentioned by LeCun, it doesn't solve this second point by LeCun who says it is important to push the average to zero. I would love to know what LeCun has to say about this. In any case, there is a paper called Batch Normalization, which builds on top of the work of LeCun and offers a way to address this issue:It has been long known (LeCun et al., 1998b; Wiesler & Ney, 2011) that the network training converges faster if its inputs are whitened – i.e., linearly transformed to have zero means and unit variances, and decorrelated. As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer.By the way, this video by Siraj explains a lot about activation functions in 10 fun minutes.@elkout says ""The real reason that tanh is preferred compared to sigmoid (...) is that the derivatives of the tanh are larger than the derivatives of the sigmoid.""I think this is a non-issue. I never seen this being a problem in the literature. If it bothers you that one derivative is smaller than another, you can just scale it.The logistic function has the shape $\sigma(x)=\frac{1}{1+e^{-kx}}$. Usually, we use $k=1$, but nothing forbids you from using another value for $k$ to make your derivatives wider, if that was your problem.Nitpick: tanh is also a sigmoid function. Any function with a S shape is a sigmoid. What you guys are calling sigmoid is the logistic function. The reason why the logistic function is more popular is historical reasons. It has been used for a longer time by statisticians. Besides, some feel that it is more biologically plausible."
Neural Network: For Binary Classification use 1 or 2 output neurons?,"
Assume I want to do binary classification (something belongs to class A or class B). There are some possibilities to do this in the output layer of a neural network:

Use 1 output node. Output 0 (<0.5) is considered class A and 1 (>=0.5) is considered class B (in case of sigmoid)
Use 2 output nodes. The input belongs to the class of the node with the highest value/probability (argmax). 

Are there any papers written which (also) discuss this? What are specific keywords to search on?
This question is already asked before on this site e.g. see this link with no real answers. I need to make a choice (Master Thesis), so I want to get insight in the pro/cons/limitations of each solution. 
","['machine-learning', 'classification', 'neural-networks']",
Interpreting Residual and Null Deviance in GLM R,"
How to interpret the Null and Residual Deviance in GLM in R? Like, we say that smaller AIC is better. Is there any similar and quick interpretation for the deviances also?
Null deviance: 1146.1  on 1077  degrees of freedom
Residual deviance:  4589.4  on 1099  degrees of freedom
AIC: 11089
","['generalized-linear-model', 'deviance']",
How to visualize what ANOVA does?,"
What way (ways?) is there to visually explain what is ANOVA?
Any references, link(s) (R packages?) will be welcomed.   
","['data-visualization', 'anova', 'teaching']","Personally, I like introducing linear regression and ANOVA by showing that it is all the same and that linear models amount to partition the total variance: We have some kind of variance in the outcome that can be explained by the factors of interest, plus the unexplained part (called the 'residual'). I generally use the following illustration (gray line for total variability, black lines for group or individual specific variability) :I also like the heplots R package, from Michael Friendly and John Fox, but see also Visual Hypothesis Tests in Multivariate Linear Models: The heplots Package for R.Standard ways to explain what ANOVA actually does, especially in the Linear Model framework, are really well explained in Plane answers to complex questions, by Christensen, but there are very few illustrations. Saville and Wood's Statistical methods: The geometric approach has some examples, but mainly on regression. In Montgomery's Design and Analysis of Experiments, which mostly focused on DoE, there are illustrations that I like, but see below(these are mine :-)But I think you have to look for textbooks on Linear Models if you want to see how sum of squares, errors, etc. translates into a vector space, as shown on Wikipedia. Estimation and Inference in Econometrics, by Davidson and MacKinnon, seems to have nice illustrations (the 1st chapter actually covers OLS geometry) but I only browse the French translation (available here). The Geometry of Linear Regression has also some good illustrations. Edit:Ah, and I just remember this article by Robert Pruzek, A new graphic for one-way ANOVA.Edit 2And now, the granova package (mentioned by @gd047 and associated to the above paper) has been ported to ggplot, see granovaGG with an illustration for one-way ANOVA below."
Questions about how random effects are specified in lmer,"
I recently measured how the meaning of a new word is acquired over repeated exposures (practice: day 1 to day 10) by measuring ERPs (EEGs) when the word was viewed in different contexts. I also controlled properties of the context, for instance, its usefulness for the discovery of new word meaning (high vs. low). I am particularly interested in the effect of practice (days). Because individual ERP recordings are noisy, ERP component values are  obtained by averaging over the trials of a particular condition. With the lmer function, I applied the following formula:
lmer(ERPindex ~ practice*context + (1|participants), data=base) 

and 
lmer(ERPindex ~ practice*context + (1+practice|participants), data=base) 

I've also seen the equivalent of the following random effects in the literature:
lmer(ERPindex ~ practice*context + (practice|participants) + 
                (practice|participants:context), data=base) 

What is accomplished by using a random factor of the form participants:context? Is there a good source that would allow someone with just cursory knowledge of matrix algebra understand precisely what random factors do in linear mixed models and how they should be selected?
","['r', 'mixed-model', 'lme4-nlme', 'random-effects-model']",
Standard errors for lasso prediction using R,"
I'm trying to use a LASSO model for prediction, and I need to estimate standard errors. Surely someone has already written a  package to do this. But as far as I can see, none of the packages on CRAN that do predictions using a LASSO will return standard errors for those predictions.
So my question is: Is there a package or some R code available to compute standard errors for LASSO predictions?
","['r', 'standard-error', 'prediction', 'lasso']","Kyung et al. (2010), ""Penalized regression, standard errors, & Bayesian lassos"", Bayesian Analysis , 5, 2, suggest that there might not be a consensus on a statistically valid method of calculating standard errors for the lasso predictions. Tibshirani seems to agree (slide 43) that standard errors are still an unresolved issue."
Is there a difference between 'controlling for' and 'ignoring' other variables in multiple regression?,"
The coefficient of an explanatory variable in a multiple regression tells us the relationship of that explanatory variable with the dependent variable. All this, while 'controlling' for the other explanatory variables.
How I have viewed it so far:
While each coefficient is being calculated, the other variables are not taken into account, so I consider them to be ignored. 
So am I right when I think that the terms 'controlled' and 'ignored' can be used interchangeably?
","['regression', 'multiple-regression']","Controlling for something and ignoring something are not the same thing.  Let's consider a universe in which only 3 variables exist: $Y$, $X_1$, and $X_2$.  We want to build a regression model that predicts $Y$, and we are especially interested in its relationship with $X_1$.  There are two basic possibilities.  we could assess the relationship between $X_1$ and $Y$ while ignoring $X_2$:  $$
 Y = \beta_0 + \beta_1X_1
 $$Granted, these are very simple models, but they constitute different ways of looking at how the relationship between $X_1$ and $Y$ manifests.  Often, the estimated $\hat\beta_1$s might be similar in both models, but they can be quite different.  What is most important in determining how different they are is the relationship (or lack thereof) between $X_1$ and $X_2$.  Consider this figure:  In this scenario, $X_1$ is correlated with $X_2$.  Since the plot is two-dimensional, it sort of ignores $X_2$ (perhaps ironically), so I have indicated the values of $X_2$ for each point with distinct symbols and colors (the pseudo-3D plot below provides another way to try to display the structure of the data).  If we fit a regression model that ignored $X_2$, we would get the solid black regression line.  If we fit a model that controlled for $X_2$, we would get a regression plane, which is again hard to plot, so I have plotted three slices through that plane where $X_2=1$, $X_2=2$, and $X_2=3$.  Thus, we have the lines that show the relationship between $X_1$ and $Y$ that hold when we control for $X_2$.  Of note, we see that controlling for $X_2$ does not yield a single line, but a set of lines.  Another way to think about the distinction between ignoring and controlling for another variable, is to consider the distinction between a marginal distribution and a conditional distribution.  Consider this figure:  (This is taken from my answer here: What is the intuition behind conditional Gaussian distributions?)If you look at the normal curve drawn to the left of the main figure, that is the marginal distribution of $Y$.  It is the distribution of $Y$ if we ignore its relationship with $X$.  Within the main figure, there are two normal curves representing conditional distributions of $Y$ when $X_1 = 25$ and $X_1 = 45$.  The conditional distributions control for the level of $X_1$, whereas the marginal distribution ignores it.  "
Why on average does each bootstrap sample contain roughly two thirds of observations?,"
I have run across the assertion that each bootstrap sample (or bagged tree) will contain on average approximately $2/3$ of the observations. 
I understand that the chance of not being selected in any of $n$ draws from $n$ samples with replacement is $(1- 1/n)^n$, which works out to approximately $1/3$ chance of not being selected. 
What is a mathematical explanation for why this formula always gives $\approx 1/3$ ?
",['bootstrap'],"Essentially, the issue is to show that $\lim_{n\to\infty}(1- 1/n)^n=e^{-1}$
(and of course, $e^{-1} =1/e \approx 1/3$, at least very roughly).It doesn't work at very small $n$ -- e.g. at $n=2$, $(1- 1/n)^n=\frac{1}{4}$. It passes $\frac{1}{3}$ at $n=6$, passes $0.35$ at $n=11$, and $0.366$ by $n=99$. Once you go beyond $n=11$, $\frac{1}{e}$ is a better approximation than $\frac{1}{3}$.The grey dashed line is at $\frac{1}{3}$; the red and grey line is at $\frac{1}{e}$.Rather than show a formal derivation (which can easily be found), I'm going to give an outline (that is an intuitive, handwavy argument) of why a (slightly) more general result holds:$$e^x = \lim_{n\to \infty} \left(1 + x/n \right)^n$$(Many people take this to be the definition of $\exp(x)$, but you can prove it from simpler results such as defining $e$ as $\lim_{n\to \infty} \left(1 + 1/n \right)^n$.)Fact 1: $\exp(x/n)^n=\exp(x)\quad$ This follows from basic results about powers and exponentiationFact 2: When $n$ is large, $\exp(x/n) \approx 1+x/n\quad$ This follows from the series expansion for $e^x$. (I can give fuller arguments for each of these but I assume you already know them)Substitute (2) in (1). Done. (For this to work as a more formal argument would take some work, because you'd have to show that the remaining terms in Fact 2 don't become large enough to cause a problem when taken to the power $n$. But this is intuition rather than formal proof.)[Alternatively, just take the Taylor series for $\exp(x/n)$ to first order. A second easy approach is to take the binomial expansion of $\left(1 + x/n \right) ^n$ and take the limit term-by-term, showing it gives the terms in the series for $\exp(x/n)$.]So if $e^x = \lim_{n\to \infty} \left(1 + x/n \right) ^n$, just substitute $x=-1$.Immediately, we have the result at the top of this answer, $\lim_{n\to\infty}(1- 1/n)^n=e^{-1}$  As gung points out in comments, the result in your question is the origin of the 632 bootstrap rulee.g. see Efron, B. and R. Tibshirani (1997),
""Improvements on Cross-Validation: The .632+ Bootstrap Method,""
Journal of the American Statistical Association Vol. 92, No. 438. (Jun), pp. 548-560"
Wald test for logistic regression,"
As far as I understand the Wald test in the context of logistic regression is used to determine whether a certain predictor variable $X$ is significant or not. It rejects the null hypothesis of the corresponding coefficient being zero. 
The test consists of dividing the value of the coefficient by standard error $\sigma$. 
What I am confused about is that $X/\sigma$ is also known as Z-score and indicates how likely it is that a given observation comes form the normal distribution (with mean zero).
","['logistic', 'z-statistic']","The estimates of the coefficients and the intercepts in logistic regression (and any GLM) are found via maximum-likelihood estimation (MLE). These estimates are denoted with a hat over the parameters, something like $\hat{\theta}$. Our parameter of interest is denoted $\theta_{0}$ and this is usually 0 as we want to test whether the coefficient differs from 0 or not. From asymptotic theory of MLE, we know that the difference between $\hat{\theta}$ and $\theta_{0}$ will be approximately normally distributed with mean 0 (details can be found in any mathematical statistics book such as Larry Wasserman's All of statistics). Recall that standard errors are nothing else than standard deviations of statistics (Sokal and Rohlf write in their book Biometry: ""a statistic is any one of many computed or estimated statistical quantities"", e.g. the mean, median, standard deviation, correlation coefficient, regression coefficient, ...). Dividing a normal distribution with mean 0 and standard deviation $\sigma$ by its standard deviation will yield the standard normal distribution with mean 0 and standard deviation 1. The Wald statistic is defined as (e.g. Wasserman (2006): All of Statistics, pages 153, 214-215):
$$
W=\frac{(\hat{\beta}-\beta_{0})}{\widehat{\operatorname{se}}(\hat{\beta})}\sim \mathcal{N}(0,1)
$$
or
$$
W^{2}=\frac{(\hat{\beta}-\beta_{0})^2}{\widehat{\operatorname{Var}}(\hat{\beta})}\sim \chi^{2}_{1}
$$
The second form arises from the fact that the square of a standard normal distribution is the $\chi^{2}_{1}$-distribution with 1 degree of freedom (the sum of two squared standard normal distributions would be a $\chi^{2}_{2}$-distribution with 2 degrees of freedom and so on).Because the parameter of interest is usually 0 (i.e. $\beta_{0}=0$), the Wald statistic simplifies to
$$
W=\frac{\hat{\beta}}{\widehat{\operatorname{se}}(\hat{\beta})}\sim \mathcal{N}(0,1)
$$
Which is what you described: The estimate of the coefficient divided by its standard error.When is a $z$ and when a $t$ value used?The choice between a $z$-value or a $t$-value depends on how the standard error of the coefficients has been calculated. Because the Wald statistic is asymptotically distributed as a standard normal distribution, we can use the $z$-score to calculate the $p$-value. When we, in addition to the coefficients, also have to estimate the residual variance, a $t$-value is used instead of the $z$-value. In ordinary least squares (OLS, normal linear regression), the variance-covariance matrix of the coefficients is $\operatorname{Var}[\hat{\beta}|X]=\sigma^2(X'X)^{-1}$ where $\sigma^2$ is the variance of the residuals (which is unknown and has to be estimated from the data) and $X$ is the design matrix. In OLS, the standard errors of the coefficients are the square roots of the diagonal elements of the variance-covariance matrix. Because we don't know $\sigma^2$, we have to replace it by its estimate $\hat{\sigma}^{2}=s^2$, so: $\widehat{\operatorname{se}}(\hat{\beta_{j}})=\sqrt{s^2(X'X)_{jj}^{-1}}$. Now that's the point: Because we have to estimate the variance of the residuals to calculate the standard error of the coefficients, we need to use a $t$-value and the $t$-distribution.In logistic (and poisson) regression, the variance of the residuals is related to the mean. If $Y\sim Bin(n, p)$, the mean is $E(Y)=np$ and the variance is $\operatorname{Var}(Y)=np(1-p)$ so the variance and the mean are related. In logistic and poisson regression but not in regression with gaussian errors, we know the expected variance and don't have to estimate it separately. The dispersion parameter $\phi$ indicates if we have more or less than the expected variance. If $\phi=1$ this means we observe the expected amount of variance, whereas $\phi<1$ means that we have less than the expected variance (called underdispersion) and $\phi>1$ means that we have extra variance beyond the expected (called overdispersion). The dispersion parameter in logistic and poisson regression is fixed at 1 which means that we can use the $z$-score. The dispersion parameter . In other regression types such as normal linear regression, we have to estimate the residual variance and thus, a $t$-value is used for calculating the $p$-values. In R, look at these two examples:Logistic regressionNote that the dispersion parameter is fixed at 1 and thus, we get $z$-values.Normal linear regression (OLS)Here, we have to estimate the residual variance (denoted as ""Residual standard error"") and hence, we use $t$-values instead of $z$-values. Of course, in large samples, the $t$-distribution approximates the normal distribution and the difference doesn't matter.Another related post can be found here."
Why is expectation the same as the arithmetic mean?,"
Today I came across a new topic called the Mathematical Expectation. The book I am following says, expectation is the arithmetic mean of random variable coming from any probability distribution. But, it defines expectation as the sum of product of some data and the probability of it. How can these two (average and expectation) be same? How can the sum of probability times the data be the average of whole distribution?
","['expected-value', 'faq']","Informally, a probability distribution defines the relative frequency of outcomes of a random variable - the expected value can be thought of as a weighted average of those outcomes (weighted by the relative frequency). Similarly, the expected value can be thought of as the arithmetic mean of a set of numbers generated in exact proportion to their probability of occurring (in the case of a continuous random variable this isn't exactly true since specific values have probability $0$). The connection between the expected value and the arithmetic mean is most clear with a discrete random variable, where the expected value is$$ E(X) = \sum_{S} x P(X=x) $$ where $S$ is the sample space. As an example, suppose you have a discrete random variable $X$ such that:$$ X = \begin{cases} 1 & \mbox{with probability  } 1/8 \\ 
2 & \mbox{with probability } 3/8 \\
3 & \mbox{with probability } 1/2
\end{cases} $$That is, the probability mass function is $P(X=1)=1/8$, $P(X=2)=3/8$, and $P(X=3)=1/2$. Using the formula above, the expected value is $$ E(X) = 1\cdot (1/8) + 2 \cdot (3/8) + 3 \cdot (1/2) = 2.375 $$Now consider numbers generated with frequencies exactly proportional to the probability mass function - for example, the set of numbers $\{1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3\}$ - two $1$s, six $2$s and eight $3$s. Now take the arithmetic mean of these numbers: $$ \frac{1+1+2+2+2+2+2+2+3+3+3+3+3+3+3+3}{16} = 2.375 $$and you can see it's exactly equal to the expected value. "
What is a contrast matrix?,"
What exactly is contrast matrix (a term, pertaining to an analysis with categorical predictors) and how exactly is contrast matrix specified? I.e. what are columns, what are rows, what are the constraints on that matrix and what does number in column j and row i mean? I tried to look into the docs and web but it seems that everyone uses it yet there's no definition anywhere. I could backward-engineer the available pre-defined contrasts, but I think the definition should be available without that.
    > contr.treatment(4)
      2 3 4
    1 0 0 0
    2 1 0 0
    3 0 1 0
    4 0 0 1
    > contr.sum(4)
      [,1] [,2] [,3]
    1    1    0    0
    2    0    1    0
    3    0    0    1
    4   -1   -1   -1
    > contr.helmert(4)
      [,1] [,2] [,3]
    1   -1   -1   -1
    2    1   -1   -1
    3    0    2   -1
    4    0    0    3
    > contr.SAS(4)
      1 2 3
    1 1 0 0
    2 0 1 0
    3 0 0 1
    4 0 0 0

","['regression', 'categorical-data', 'definition', 'contrasts', 'categorical-encoding']",
Advanced statistics books recommendation,"
There are several threads on this site for book recommendations on introductory statistics and machine learning but I am looking for a text on advanced statistics including, in order of priority: maximum likelihood, generalized linear models, principal component analysis, non-linear models. I've tried Statistical Models by A.C. Davison but frankly I had to put it down after 2 chapters. The text is encyclopedic in its coverage and mathematical treats but, as a practitioner, I like to approach subjects by understanding the intuition first, and then delve into the mathematical background. 
These are some texts that I consider outstanding for their pedagogical value. I would like to find an equivalent for the more advanced subjects I mentioned.

Statistics, D. Freedman, R. Pisani, R. Purves.
Forecasting: Methods and Applications, R. Hyndman et al.
Multiple Regression and Beyond, T. Z. Keith
Applying Contemporary Statistical Techniques, Rand R. Wilcox
An Introduction to Statistical Learning with Applications in R - (PDF Released Version), Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani
The Elements of Statistical Learning:Data Mining, Inference, and Prediction. - (PDF Released Version), Hastie, Tibshirani and Friedman (2009)

","['generalized-linear-model', 'pca', 'maximum-likelihood', 'references', 'saddlepoint-approximation']","Maximum likelihood: In all Likelihood (Pawitan). Moderately clear book and the most clear (IMO) with respect to books dealing with likelihood only. Also has R code.GLMs: Categorical Data Analysis (Agresti, 2002) is one of the best written stat books I have read (also has R code available). This text will also help with maximum likelihood. The third edition is coming out in a few months.Second on my list for the above two is Collett's Modelling Binary Data.PCA: I find Rencher's writing clear in Methods of multivariate analysis. This is a graduate level text, but it is introductory."
"What is the difference between a partial likelihood, profile likelihood and marginal likelihood?","
I see these terms being used and I keep getting them mixed up. Is there a simple explanation of the differences between them?
","['estimation', 'maximum-likelihood', 'likelihood', 'profile-likelihood']","The likelihood function usually depends on many parameters. Depending on the application, we are usually interested in only a subset of these parameters. For example, in linear regression, interest typically lies in the slope coefficients and not on the error variance. Denote the parameters we are interested in as $\beta$ and the parameters that are not of primary interest as $\theta$. The standard way to approach the estimation problem is to maximize the likelihood function so that we obtain estimates of $\beta$ and $\theta$. However, since the primary interest lies in $\beta$ partial, profile and marginal likelihood offer alternative ways to estimate $\beta$ without estimating $\theta$.In order to see the difference denote the standard likelihood by $L(\beta, \theta|\mathrm{data})$. Maximum LikelihoodFind $\beta$ and $\theta$ that maximizes $L(\beta, \theta|\mathrm{data})$.Partial LikelihoodIf we can write the likelihood function as:$$L(\beta, \theta|\mathrm{data}) = L_1(\beta|\mathrm{data}) L_2(\theta|\mathrm{data})$$Then we simply maximize $L_1(\beta|\mathrm{data})$.Profile LikelihoodIf we can express $\theta$ as a function of $\beta$ then we replace $\theta$ with the corresponding function. Say, $\theta = g(\beta)$. Then, we maximize:$$L(\beta, g(\beta)|\mathrm{data})$$Marginal LikelihoodWe integrate out $\theta$ from the likelihood equation by exploiting the fact that we can identify the probability distribution of $\theta$ conditional on $\beta$."
What does orthogonal mean in the context of statistics?,"
In other contexts, orthogonal means ""at right angles"" or ""perpendicular"".
What does orthogonal mean in a statistical context?
Thanks for any clarifications.
",['descriptive-statistics'],
"Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?","
In what circumstances would you want to, or not want to scale or standardize a variable prior to model fitting?  And what are the advantages / disadvantages of scaling a variable?
","['modeling', 'predictive-models', 'feature-selection', 'mathematical-statistics', 'standardization']","Standardization is all about the weights of different variables for the model. 
If you do the standardisation ""only"" for the sake of numerical stability, there may be transformations that yield very similar numerical properties but different physical meaning that could be much more appropriate for the interpretation. The same is true for centering, which is usually part of the standardization.Situations where you probably want to standardize:Situations where you may not want to standardize:You may do something ""in between"", and transform the variables or choose the unit so that the new variables still have physical meaning but the variation in the numerical value is not that different, e.g. Similar for centering:"
What is the relationship between a chi squared test and test of equal proportions?,"
Suppose that I have three populations with four, mutually exclusive characteristics.  I take random samples from each population and construct a crosstab or frequency table for the characteristics that I am measuring.  Am I correct in saying that:

If I wanted to test whether there is any relationship between the populations and the characteristics (e.g. whether one population has a higher frequency of one of the characteristics), I should run a chi-squared test and see whether the result is significant.
If the chi-squared test is significant, it only shows me that there is some relationship between the populations and characteristics, but not how they are related.
Furthermore, not all of the characteristics need to be related to the population.  For example, if the different populations have significantly different distributions of characteristics A and B, but not of C and D, then the chi-squared test may still come back as significant.
If I wanted to measure whether or not a specific characteristic is affected by the population, then I can run a test for equal proportions (I have seen this called a z-test, or as prop.test() in R) on just that characteristic.

In other words, is it appropriate to use the prop.test() to more accurately determine the nature of a relationship between two sets of categories when the chi-squared test says that there is a significant relationship?
","['chi-squared-test', 'proportion', 'contingency-tables', 'z-test']","Very short answer: The chi-Squared test (chisq.test() in R) compares the observed frequencies in each category of a contingency table with the expected frequencies (computed as the product of the marginal frequencies). It is used to determine whether the deviations between the observed and the expected counts are too large to be attributed to chance. Departure from independence is easily checked by inspecting residuals (try ?mosaicplot or ?assocplot, but also look at the vcd package). Use fisher.test() for an exact test (relying on the hypergeometric distribution).The prop.test() function in R allows to test whether proportions are comparable between groups or does not differ from theoretical probabilities. It is referred to as a $z$-test because the test statistic looks like this:$$
z=\frac{(f_1-f_2)}{\sqrt{\hat p \left(1-\hat p \right) \left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}
$$where $\hat p=(p_1+p_2)/(n_1+n_2)$, and the indices $(1,2)$ refer to the first and second line of your table.
In a two-way contingency table where $H_0:\; p_1=p_2$, this should yield comparable results to the ordinary $\chi^2$ test:For analysis of discrete data with R, I highly recommend R (and S-PLUS) Manual to Accompany Agresti’s Categorical Data Analysis (2002), from Laura Thompson."
"What is the difference between a ""nested"" and a ""non-nested"" model?","
In the literature on hierarchical/multilevel models I have often read about ""nested models"" and ""non-nested models"", but what does this mean? Could anyone maybe give me some examples or tell me about the mathematical implications of this phrasing?
","['hypothesis-testing', 'terminology', 'nested-models', 'nested-data']",
Find expected value using CDF,"
I'm going to start out by saying this is a homework problem straight out of the book. I have spent a couple hours looking up how to find expected values, and have determined I understand nothing.

Let $X$ have the CDF $F(x) = 1 - x^{-\alpha}, x\ge1$.
  Find $E(X)$ for those values of $\alpha$ for which $E(X)$ exists.

I have no idea how to even start this. How can I determine which values of $\alpha$ exist? I also don't know what to do with the CDF (I'm assuming this means Cumulative Distribution Function). There are formulas for finding the expected value when you have a frequency function or density function. Wikipedia says the CDF of $X$ can be defined in terms of the probability density function $f$ as follows:
$F(x) = \int_{-\infty}^x f(t)\,dt$
This is as far as I got. Where do I go from here?
EDIT: I meant to put $x\ge1$.
","['self-study', 'expected-value']","Edited for the comment from probabilityislogicNote that $F(1)=0$ in this case so the distribution has probability $0$ of being less than $1$, so $x \ge 1$, and you will also need $\alpha > 0$ for an increasing cdf. If you have the cdf then you want the anti-integral or derivative which with a continuous distribution like this $$f(x) = \frac{dF(x)}{dx}$$ and in reverse $F(x) = \int_{1}^x  f(t)\,dt$ for $x \ge 1$.Then to find the expectation you need to find $$E[X] = \int_{1}^{\infty}  x f(x)\,dx$$ providing that this exists.  I will leave the calculus to you."
A generalization of the Law of Iterated Expectations,"
I recently came across this identity:
$$E \left[ E \left(Y|X,Z \right) |X \right] =E \left[Y | X \right]$$
I am of course familiar with the simpler version of that rule, namely that $E \left[ E \left(Y|X \right) \right]=E \left(Y\right) $ but I was not able to find justification for its generalization. 
I would be grateful if someone could point me to a not-so-technical reference for that fact or, even better, if someone could lay out a simple proof for this important result.  
","['conditional-probability', 'conditional-expectation']","INFORMAL TREATMENT We should remember that the notation where we condition on random variables is inaccurate, although economical, as notation. In reality we condition on the sigma-algebra that these random variables generate. In other words $E[Y\mid X]$ is meant to mean $E[Y\mid \sigma(X)]$. This remark may seem out of place in an ""Informal Treatment"", but it reminds us that our conditioning entities are collections of sets (and when we condition on a single value, then this is a singleton set). And what do these sets contain? They contain the information with which the possible values of the random variable $X$ supply us about what may happen with the realization of $Y$.
Bringing in the concept of Information, permits us to think about (and use) the Law of Iterated Expectations (sometimes called the ""Tower Property"") in a very intuitive way:
The sigma-algebra generated by two random variables, is at least as large as that generated by one random variable: $\sigma (X) \subseteq \sigma(X,Z)$ in the proper set-theoretic meaning. So the information about $Y$ contained in $\sigma(X,Z)$ is at least as great as the corresponding information in $\sigma (X)$.
Now, as notational innuendo, set $\sigma (X) \equiv I_x$ and $\sigma(X,Z) \equiv I_{xz}$.
Then the LHS of the equation we are looking at, can be written$$E \left[ E \left(Y|I_{xz} \right) |I_{x} \right]$$
Describing verbally the above expression we have : ""what is the expectation of {the expected value of $Y$ given Information $I_{xz}$} given that we have available information $I_x$ only?""  Can we somehow ""take into account"" $I_{xz}$? No - we only know $I_x$. But if we use what we have (as we are obliged by the expression we want to resolve), then we are essentially saying things about $Y$ under the expectations operator, i.e. we say ""$E(Y\mid I_x)$"", no more -we have just exhausted our information.Hence
$$E \left[ E \left(Y|I_{xz} \right) |I_{x} \right] = E\left(Y|I_{x} \right)$$If somebody else doesn't, I will return for the formal treatment.A (bit more) FORMAL TREATMENT Let's see how two very important books of probability theory, P. Billingsley's Probability and Measure (3d ed.-1995) and D. Williams ""Probability with Martingales"" (1991), treat the matter of proving the ""Law Of Iterated Expectations"":
Billingsley devotes exactly three lines to the proof. Williams, and I quote, says ""(the Tower Property) is virtually immediate from the definition of
  conditional expectation"".That's one line of text. Billingsley's proof is not less opaque.They are of course right: this important and very intuitive property of conditional expectation derives essentially directly (and almost immediately) from its definition -the only problem is, I suspect that this definition is not usually taught, or at least not highlighted, outside probability or measure theoretic circles. But in order to show in (almost) three lines that the Law of Iterated Expectations holds, we need the definition of conditional expectation, or rather, its defining property.  Let a probability space $(\Omega, \mathcal F, \mathbf P)$, and an integrable random variable $Y$. Let $\mathcal G$ be a sub-$\sigma$-algebra of $\mathcal F$, $\mathcal G \subseteq \mathcal F$. Then there exists a function $W$ that is $\mathcal G$-measurable, is integrable and (this is the defining property)$$E(W\cdot\mathbb 1_{G}) = E(Y\cdot \mathbb 1_{G})\qquad \forall G \in \mathcal G \qquad [1]$$where $1_{G}$ is the indicator function of the set $G$. We say that $W$ is (""a version of"") the conditional expectation of $Y$ given $\mathcal G$, and we write
$W = E(Y\mid \mathcal G) \;a.s.$
The critical detail to note here is that the conditional expectation, has the same expected value as $Y$ does, not just over the whole $\mathcal G$, but in every subset $G$ of $\mathcal G$. (I will try now to present how the Tower property derives from the definition of conditional expectation).$W$ is a $\mathcal G$-measurable random variable. Consider then some  sub-$\sigma$-algebra, say $\mathcal H \subseteq \mathcal G$. Then $G\in \mathcal H \Rightarrow G\in \mathcal G$. So, in an analogous manner as previously, we have the conditional expectation of $W$ given $\mathcal H$, say $U=E(W\mid \mathcal H) \;a.s.$ that is characterized by  $$E(U\cdot\mathbb 1_{G}) = E(W\cdot \mathbb 1_{G})\qquad \forall G \in \mathcal H \qquad [2]$$Since $\mathcal H \subseteq \mathcal G$, equations $[1]$ and $[2]$ give us$$E(U\cdot\mathbb 1_{G}) = E(Y\cdot \mathbb 1_{G})\qquad \forall G \in \mathcal H \qquad [3]$$But this is the defining property of the conditional expectation of $Y$ given $\mathcal H$. So we are entitled to write $U=E(Y\mid \mathcal H)\; a.s.$
Since we have also by construction $U = E(W\mid \mathcal H) = E\big(E[Y\mid \mathcal G]\mid \mathcal H\big)$, we just proved the Tower property, or the general form of the Law of Iterated Expectations - in eight lines."
Where did the frequentist-Bayesian debate go?,"
The world of statistics was divided between frequentists and Bayesians. These days it seems everyone does a bit of both. How can this be? If the different approaches are suitable for different problems, why did the founding fathers of statistics did not see this? Alternatively, has the debate been won by Frequentists and the true subjective Bayesians moved to decision theory? 
","['bayesian', 'frequentist', 'history', 'philosophical']",
Rule of thumb for number of bootstrap samples,"
I wonder if someone knows any general rules of thumb regarding the number of bootstrap samples one should use, based on characteristics of the data (number of observations, etc.) and/or the variables included?
","['bootstrap', 'inference', 'monte-carlo']",
How does one interpret SVM feature weights?,"
I am trying to interpret the variable weights given by fitting a linear SVM.
(I'm using scikit-learn):
from sklearn import svm

svm = svm.SVC(kernel='linear')

svm.fit(features, labels)
svm.coef_

I cannot find anything in the documentation that specifically states how these weights are calculated or interpreted.
Does the sign of the weight have anything to do with class?
","['svm', 'feature-selection', 'python', 'scikit-learn']",
Why doesn't Random Forest handle missing values in predictors?,"
What are theoretical reasons to not handle missing values? Gradient boosting machines, regression trees handle missing values. Why doesn't Random Forest do that?
","['random-forest', 'missing-data', 'boosting']","Gradient Boosting Trees uses CART trees (in a standard setup, as it was proposed by its authors). CART trees are also used in Random Forests. What @user777 said is true, that RF trees handles missing values either by imputation with average, either by rough average/mode, either by an averaging/mode based on proximities. These methods were proposed by Breiman and Cutler and are used for RF. This is a reference from authors Missing values in training set. However, one can build a GBM or RF with other type of decision trees. The usual replacement for CART is C4.5 proposed by Quinlan. In C4.5 the missing values are not replaced on data set. Instead, the impurity function computed takes into account the missing values by penalizing the impurity score with the ration of missing values. On test set the evaluation in a node which has a test with missing value, the prediction is built for each child node and aggregated later (by weighting). Now, in many implementations C4.5 is used instead of CART. The main reason is to avoid expensive computation (CART has more rigorous statistical approaches, which require more computation), the results seems to be similar, the resulted trees are often smaller (since CART is binary and C4.5 not). I know that Weka uses this approach. I do not know other libraries, but I expect it to not be a singular situation. If that is the case with your GBM implementation, than this would be an answer."
Warning in R - Chi-squared approximation may be incorrect,"
I have data showing fire fighter entrance exam results. I am testing the hypothesis that exam results and ethnicity are not mutually independent. To test this, I ran a Pearson chi-square test in R. The results show what I expected, but it gave a warning that ""In chisq.test(a) : Chi-squared approximation may be incorrect.""
> a
       white black asian hispanic
pass       5     2     2        0
noShow     0     1     0        0
fail       0     2     3        4
> chisq.test(a)

    Pearson's Chi-squared test

data:  a
X-squared = 12.6667, df = 6, p-value = 0.04865

Warning message:
In chisq.test(a) : Chi-squared approximation may be incorrect

Does anyone know why it gave a warning? Is it because I am using a wrong method?
","['r', 'categorical-data', 'chi-squared-test', 'small-sample', 'error-message']","It gave the warning because many of the expected values will be very small and therefore the approximations of p may not be right.In R you can use chisq.test(a, simulate.p.value = TRUE) to use simulate p values. However, with such small cell sizes, all estimates will be poor. It might be good to just test pass vs. fail (deleting ""no show"") either with chi-square or logistic regression. Indeed, since it is pretty clear that the pass/fail grade is a dependent variable, logistic regression might be better. "
What is the difference in Bayesian estimate and maximum likelihood estimate?,"
Please explain to me the difference in Bayesian estimate and Maximum likelihood estimate?
","['bayesian', 'maximum-likelihood']","It is a very broad question and my answer here only begins to scratch the surface a bit. I will use the Bayes's rule to explain the concepts. Let’s assume that a set of probability distribution parameters,  $\theta$, best explains the dataset $D$. We may wish to estimate the parameters $\theta$ with the help of the Bayes’ Rule:$$p(\theta|D)=\frac{p(D|\theta) * p(\theta)}{p(D)}$$$$posterior = \frac{likelihood * prior}{evidence}$$The explanations follow:Maximum Likelihood EstimateWith MLE,we seek a point value for $\theta$ which maximizes the likelihood, $p(D|\theta)$, shown in the equation(s) above. We can denote this value as $\hat{\theta}$. In MLE, $\hat{\theta}$ is a point estimate, not a random variable.In other words, in the equation above, MLE treats the term $\frac{p(\theta)}{p(D)}$ as a constant and does NOT allow us to inject our prior beliefs, $p(\theta)$, about the likely values for $\theta$ in the estimation calculations.Bayesian EstimateBayesian estimation, by contrast, fully calculates (or at times approximates) the posterior distribution $p(\theta|D)$. Bayesian inference treats $\theta$ as a random variable. In Bayesian estimation, we put in probability density functions and get out probability density functions, rather than a single point as in MLE. Of all the $\theta$ values made possible by the output distribution $p(\theta|D)$, it is our job to select a value that we consider best in some sense. For example, we may choose the expected value of $\theta$ assuming its variance is small enough. The variance that we can calculate for the parameter $\theta$ from its posterior distribution allows us to express our confidence in any specific value we may use as an estimate. If the variance is too large, we may declare that there does not exist a good estimate for $\theta$.As a trade-off, Bayesian estimation is made complex by the fact that we now have to deal with the denominator in the Bayes' rule, i.e. $evidence$. Here evidence -or probability of evidence- is represented by:$$p(D) = \int_{\theta} p(D|\theta) * p(\theta) d\theta$$This leads to the concept of 'conjugate priors' in Bayesian estimation. For a given likelihood function, if we have a choice regarding how we express our prior beliefs, we must use that form which allows us to carry out the integration shown above. The idea of conjugate priors and how they are practically implemented are explained quite well in this post by COOlSerdash."
What is the difference between estimation and prediction?,"
For example, I have historical loss data and I am calculating extreme quantiles (Value-at-Risk or Probable Maximum Loss). The results obtained is for estimating the loss or predicting them? Where can one draw the line? I am confused. 
","['estimation', 'predictor', 'prediction-interval']",
"Why only three partitions? (training, validation, test)","
When you are trying to fit models to a large dataset, the common advice is to partition the data into three parts: the training, validation, and test dataset.
This is because the models usually have three ""levels"" of parameters: the first ""parameter"" is the model class (e.g. SVM, neural network, random forest), the second set of parameters are the ""regularization"" parameters or ""hyperparameters"" (e.g. lasso penalty coefficient, choice of kernel, neural network structure) and the third set are what are usually considered the ""parameters"" (e.g. coefficients for the covariates.)
Given a model class and a choice of hyperparameters, one selects the parameters by choosing the parameters which minimize error on the training set.  Given a model class, one tunes the hyperparameters by minimizing error on the validation set.  One selects the model class by performance on the test set.
But why not more partitions?  Often one can split the hyperparameters into two groups, and use a ""validation 1"" to fit the first and ""validation 2"" to fit the second.  Or one could even treat the size of the training data/validation data split as a hyperparameter to be tuned.
Is this already a common practice in some applications?  Is there any theoretical work on the optimal partitioning of data?
","['machine-learning', 'model-selection', 'data-mining']","First, I think you're mistaken about what the three partitions do. You don't make any choices based on the test data. Your algorithms adjust their parameters based on the training data. You then run them on the validation data to compare your algorithms (and their trained parameters) and decide on a winner. You then run the winner on your test data to give you a forecast of how well it will do in the real world.You don't validate on the training data because that would overfit your models. You don't stop at the validation step's winner's score because you've iteratively been adjusting things to get a winner in the validation step, and so you need an independent test (that you haven't specifically been adjusting towards) to give you an idea of how well you'll do outside of the current arena.Second, I would think that one limiting factor here is how much data you have. Most of the time, we don't even want to split the data into fixed partitions at all, hence CV."
"What is the definition of a ""feature map"" (aka ""activation map"") in a convolutional neural network?","
 Intro Background
Within a convolutional neural network, we usually have a general structure / flow that looks like this:

input image (i.e. a 2D vector x)


(1st Convolutional layer (Conv1) starts here...)

convolve a set of filters (w1) along the 2D image (i.e. do the z1 = w1*x + b1 dot product multiplications), where z1 is 3D, and b1 is biases.
apply an activation function (e.g. ReLu) to make z1 non-linear (e.g. a1 = ReLu(z1)), where a1 is 3D.


(2nd Convolutional layer  (Conv2) starts here...)

convolve a set of filters along the newly computed activations (i.e. do the z2 = w2*a1 + b2 dot product multiplications), where z2 is 3D, and and b2 is biases.
apply an activation function (e.g. ReLu) to make z2 non-linear (e.g. a2 = ReLu(z2)), where a2 is 3D.


 The Question
The definition of the term ""feature map"" seems to vary from literature to literature. Concretely:

For the 1st convolutional layer, does ""feature map"" corresponds to the input vector x, or the output dot product z1, or the output activations a1, or the ""process"" converting x to a1, or something else?
Similarly, for the 2nd convolutional layer, does ""feature map"" corresponds to the input activations a1, or the output dot product z2, or the output activation a2, or the ""process"" converting a1 to a2, or something else?

In addition, is it true that the term ""feature map"" is exactly the same as ""activation map""? (or do they actually mean two different thing?)
 Additional references:
Snippets from Neural Networks and Deep Learning - Chapter 6:

*The nomenclature is being used loosely here. In particular, I'm using ""feature map"" to mean not the function computed by the convolutional layer, but rather the activation of the hidden neurons output from the layer. This kind of mild abuse of nomenclature is pretty common in the research literature.


Snippets from Visualizing and Understanding
Convolutional Networks by Matt Zeiler:

In this paper we introduce a visualization technique that reveals the input stimuli that excite individual feature maps at
any layer in the model. [...] Our approach, by contrast, provides a non-parametric view of invariance, showing which patterns from the training set activate the feature map. [...] a
local contrast operation that normalizes the responses across feature maps.
[...] To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. [...] The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. [...] The convnet uses learned filters to convolve the feature maps from
the previous layer. [...] Fig. 6, these visualizations are accurate representations of the input pattern that stimulates the given feature map in the model [...] when the parts of the original input image corresponding to the pattern are occluded, we see a distinct drop in activity within the feature map. [...]

Remarks: also introduces the term ""feature map"" and ""rectified feature map"" in Fig 1.

Snippets from Stanford CS231n Chapter on CNN:

[...] One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates [...] Typical-looking activations on the first CONV layer (left), and the 5th CONV layer (right) of a trained AlexNet looking at a picture of a cat. Every box shows an activation map corresponding to some filter. Notice that the activations are sparse (most values are zero, in this visualization shown in black) and mostly local.


Snippets from A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks

[...] Every unique location on the input volume produces a number. After sliding the filter over all the locations, you will find out that what you’re left with is a 28 x 28 x 1 array of numbers, which we call an activation map or feature map.

","['neural-networks', 'deep-learning', 'conv-neural-network']","A feature map, or activation map, is the output activations for a given filter (a1 in your case) and the definition is the same regardless of what layer you are on. Feature map and activation map mean exactly the same thing. It is called an activation map because it is a mapping that corresponds to the activation of different parts of the image, and also a feature map because it is also a mapping of where a certain kind of feature is found in the image. A high activation means a certain feature was found.  A ""rectified feature map"" is just a feature map that was created using Relu. You could possibly see the term ""feature map"" used for the result of the dot products (z1) because this is also really a map of where certain features are in the image, but that is not common to see. "
Efficient online linear regression,"
I'm analysing some data where I would like to perform ordinary linear regression, however this is not possible as I am dealing with an on-line setting with a continuous stream of input data (which will quickly get too large for memory) and need to update parameter estimates while this is being consumed. i.e. I cannot just load it all into memory and perform linear regression on the entire data set.
I'm assuming a simple linear multivariate regression model, i.e.
$$\mathbf y = \mathbf A\mathbf x + \mathbf b + \mathbf e$$
What's the best algorithm for creating a continuously updating estimate of the linear regression parameters $\mathbf A$ and $\mathbf b$? 
Ideally:

I'd like an algorithm that is most $\mathcal O(N\cdot M)$ space and time complexity per update, where $N$ is the dimensionality of the independent variable ($\mathbf x$) and $M$ is the dimensionality of the dependent variable ($\mathbf y$).
I'd like to be able to
specify some parameter to determine
how much the parameters are updated
by each new sample, e.g. 0.000001
would mean that the next sample would
provide one millionth of the
parameter estimate. This would give
some kind of exponential decay for
the effect of samples in the distant
past.

","['regression', 'time-series', 'algorithms', 'online-algorithms', 'real-time']","Maindonald describes a sequential method based on Givens rotations.  (A Givens rotation is an orthogonal transformation of two vectors that zeros out a given entry in one of the vectors.)  At the previous step you have decomposed the design matrix $\mathbf{X}$ into a triangular matrix $\mathbf{T}$ via an orthogonal transformation $\mathbf{Q}$ so that $\mathbf{Q}\mathbf{X} = (\mathbf{T}, \mathbf{0})'$.  (It's fast and easy to get the regression results from a triangular matrix.)  Upon adjoining a new row $v$ below $\mathbf{X}$, you effectively extend $(\mathbf{T}, \mathbf{0})'$ by a nonzero row, too, say $t$.  The task is to zero out this row while keeping the entries in the position of $\mathbf{T}$ diagonal.  A sequence of Givens rotations does this: the rotation with the first row of $\mathbf{T}$ zeros the first element of $t$; then the rotation with the second row of $\mathbf{T}$ zeros the second element, and so on.  The effect is to premultiply $\mathbf{Q}$ by a series of rotations, which does not change its orthogonality.When the design matrix has $p+1$ columns (which is the case when regressing on $p$ variables plus a constant), the number of rotations needed does not exceed $p+1$ and each rotation changes two $p+1$-vectors.    The storage needed for $\mathbf{T}$ is $O((p+1)^2)$.  Thus this algorithm has a computational cost of $O((p+1)^2)$ in both time and space.A similar approach lets you determine the effect on regression of deleting a row.  Maindonald gives formulas; so do Belsley, Kuh, & Welsh.  Thus, if you are looking for a moving window for regression, you can retain data for the window within a circular buffer, adjoining the new datum and dropping the old one with each update.  This doubles the update time and requires additional $O(k (p+1))$ storage for a window of width $k$.  It appears that $1/k$ would be the analog of the influence parameter.For exponential decay, I think (speculatively) that you could adapt this approach to weighted least squares, giving each new value a weight greater than 1.  There shouldn't be any need to maintain a buffer of previous values or delete any old data.J. H. Maindonald, Statistical Computation.  J. Wiley & Sons, 1984.  Chapter 4.D. A. Belsley, E. Kuh, R. E. Welsch, Regression Diagnostics: Identifying Influential Data and Sources of Collinearity.  J. Wiley & Sons, 1980."
Perform feature normalization before or within model validation?,"
A common good practice in Machine Learning is to do feature normalization or data standardization of the predictor variables, that's it, center the data substracting the mean and normalize it dividing by the variance (or standard deviation too). For self containment and to my understanding we do this to achieve two main things:

Avoid extra small model weights for the purpose of numerical stability.
Ensure quick convergence of optimization algorithms like e.g. Conjugate Gradient so that the large magnitude of one predictor dimension w.r.t. the others doesn't lead to slow convergence.

We usually split the data into training, validation and testing sets. In the literature we usually see that to do feature normalization they take the mean and variance (or standard deviation) over the whole set of predictor variables. The big flaw I see here is that if you do that, you are in fact introducing future information into the training predictor variables namely the future information contained in the mean and variance.
Therefore, I do feature normalization over the training data and save the mean and variance. Then I apply feature normalization to the predictor variables of the validation and test data sets using the training mean and variances. Are there any fundamental flaws with this? can anyone recommend a better alternative? 
","['machine-learning', 'normalization', 'standardization']","Your approach is entirely correct. Although data transformations are often undervalued as ""preprocessing"", one cannot emphasize enough that transformations in order to optimize model performance can and should be treated as part of the model building process.Reasoning: A model shall be applied on unseen data which is in general not available at the time the model is built. The validation process (including data splitting) simulates this. So in order to get a good estimate of the model quality (and generalization power) one needs to restrict the calculation of the normalization parameters (mean and variance) to the training set.I can only guess why this is not always done in literature. One argument could be, that the calculation of mean and variance is not that sensitive to small data variations (but even this is only true if the basic sample size is large enough and the data is approximately normally distributed without extreme outliers)."
What is the effect of having correlated predictors in a multiple regression model?,"
I learned in my linear models class that if two predictors are correlated and both are included in a model, one will be insignificant. For example, assume the size of a house and the number of bedrooms are correlated. When predicting the cost of a house using these two predictors, one of them can be dropped because they are both providing a lot of the same information. Intuitively, this makes sense, but I have a some more technical questions:

How does this effect manifest itself in p-values of the regression coefficients when including only one or including both predictors in the model?
How does the variance of the regression coefficients get affected by including both predictors in the model or just having one?
How do I know which predictor the model will choose to be less significant?
How does including only one or including both predictors change the value/variance of my forecasted cost?

","['regression', 'multiple-regression', 'p-value', 'linear-model', 'multicollinearity']",
References containing arguments against null hypothesis significance testing?,"
In the last few years I've read a number of papers arguing against the use of null hypothesis significance testing in science, but didn't think to keep a persistent list. A colleague recently asked me for such a list, so I thought I'd ask everyone here to help build it. To start things off, here's what I have so far:

Johansson (2011) ""Hail the impossible: p-values, evidence, and likelihood.""
Haller & Kraus (2002) ""Misinterpretation of significance: A problem students share with their teachers.""
Wagenmakers (2007) ""A practical solution to the pervasive problem of p-values.""
Rodgers (2010) ""The epistemology of mathematical and statistical modeling: A quiet methodological revolution.""
Dixon (1998) ""Why scientists value p-values.""
Glover & Dixon (2004) ""Likelihood ratios: a simple and flexible statistic for empirical psychologists.""

","['hypothesis-testing', 'statistical-significance', 'references', 'p-value']",
Why is the square root transformation recommended for count data?,"
It is often recommended to take the square root when you have count data.  (For some examples on CV, see @HarveyMotulsky's answer here, or @whuber's answer here.)  On the other hand, when fitting a generalized linear model with a response variable distributed as Poisson, the log is the canonical link.  This is sort of like taking a log transformation of your response data (although more accurately it is taking a log transformation of $\lambda$, the parameter that governs the response distribution).  Thus, there is some tension between these two.  

How do you reconcile this (apparent) discrepancy? 
Why would the square root be better than the logarithm? 

","['generalized-linear-model', 'data-transformation', 'poisson-distribution', 'count-data', 'variance-stabilizing']","The square root is approximately variance-stabilizing for the Poisson. There are a number of variations on the square root that improve the properties, such as adding $\frac{3}{8}$ before taking the square root, or the Freeman-Tukey ($\sqrt{X}+\sqrt{X+1}$ - though it's often adjusted for the mean as well).In the plots below, we have a Poisson $Y$ vs a predictor $x$ (with mean of $Y$ a multiple of $x$), and then $\sqrt{Y}$ vs $\sqrt{x}$ and then $\sqrt{Y+\frac{3}{8}}$ vs $\sqrt{x}$.The square root transformation somewhat improves symmetry - though not as well as the $\frac{2}{3}$ power does [1]:If you particularly want near-normality (as long as the parameter of the Poisson is not really small) and don't care about/can adjust for heteroscedasticity, try $\frac{2}{3}$ power.The canonical link is not generally a particularly good transformation for Poisson data; log zero being a particular issue (another is heteroskedasticity; you can also get left-skewness even when you don't have 0's). If the smallest values are not too close to 0 it can be useful for linearizing the mean. It's a good 'transformation' for the conditional population mean of a Poisson in a number of contexts, but not always of Poisson data. However if you do want to transform, one common strategy is to add a constant $y^*=\log(y+c)$ which avoids the $0$ issue. In that case we should consider what constant to add. Without getting too far from the question at hand, values of $c$ between $0.4$ and $0.5$ work very well (e.g. in relation to bias in the slope estimate) across a range of $\mu$ values. I usually just use $\frac12$ since it's simple, with values around $0.43$ often doing just slightly better.As for why people choose one transformation over another (or none) -- that's really a matter of what they're doing it to achieve.[1]: Plots patterned after Henrik Bengtsson's plots in his handout ""Generalized Linear Models and Transformed
Residuals"" see here
(see first slide on p4). I added a little y-jitter and omitted the lines."
Real-life examples of moving average processes,"
Can you give some real-life examples of time series for which a moving average process of order $q$, i.e.
$$
y_t = \sum_{i=1}^q \theta_i \varepsilon_{t-i} + \varepsilon_t, \text{ where } \varepsilon_t \sim \mathcal{N}(0, \sigma^2)  
$$
has some a priori reason for being a good model? At least for me, autoregressive processes seem to be quite easy to understand intuitively, while MA processes do not seem as natural at first glance. Note that I am not interested in theoretical results here (such as Wold's Theorem or invertibility).
As an example of what I am looking for, suppose that you have daily stock returns $r_t \sim \text{IID}(0, \sigma^2)$. Then, average weekly stock returns will have an MA(4) structure as a purely statistical artifact.
","['time-series', 'arima', 'interpretation', 'moving-average']","One very common cause is mis-specification. For example, let $y$ be grocery sales and $\varepsilon$ be an unobserved (to the analyst) coupon campaign that varies in intensity over time. At any point in time, there may be several ""vintages"" of coupons circulating as people use them, throw them away, and receive new ones. Shocks can also have persistent (but gradually weakening) effects. Take natural disasters or simply bad weather. Battery sales go up before the storm, then fall during, and then jump again as people people realize that disaster kits may be a good idea for the future.Similarly, data manipulation (like smoothing or interpolation) can induce this effect. I also have ""inherently smooth behavior of time series data (inertia) can cause $MA(1)$"" in my notes, but that one no longer makes sense to me."
Why do neural networks need so many training examples to perform?,"
A human child at age 2 needs around 5 instances of a car to be able to identify it with reasonable accuracy regardless of color, make, etc. When my son was 2, he was able to identify trams and trains, even though he had seen just a few. Since he was usually confusing one with each other, apparently his neural network was not trained enough, but still.
What is it that artificial neural networks are missing that prevent them from being able to learn way quicker? Is transfer learning an answer?
","['neural-networks', 'neuroscience']","I caution against expecting strong resemblance between biological and artificial neural networks. I think the name ""neural networks"" is a bit dangerous, because it tricks people into expecting that neurological processes and machine learning should be the same. The differences between biological and artificial neural networks outweigh the similarities.As an example of how this can go awry, you can also turn the reasoning in the original post on its head. You can train a neural network to learn to recognize cars in an afternoon, provided you have a reasonably fast computer and some amount of training data. You can make this a binary task (car/not car) or a multi-class task (car/tram/bike/airplane/boat) and still be confident in a high level of success. By contrast, I wouldn't expect a child to be able to pick out a car the day - or even the week - after it's born, even after it has seen ""so many training examples."" Something is obviously different between a two-year-old and an infant that accounts for the difference in learning ability, whereas a vanilla image classification neural network is perfectly capable of picking up object classification immediately after ""birth."" I think that there are two important differences: (1) the relative volumes of training data available and (2) a self-teaching mechanism that develops over time because of abundant training data.The original post exposes two questions. The title and body of the question ask why neural networks need ""so many examples."" Relative to a child's experience, neural networks trained using common image benchmarks have comparatively little data.I will re-phrases the question in the title to For the sake of comparison I'll consider the CIFAR-10 data because it is a common image benchmark. The labeled portion is composed of 10 classes of images with 6000 images per class. Each image is 32x32 pixels. If you somehow stacked the labeled images from CIFAR-10 and made a standard 48 fps video, you'd have about 20 minutes of footage.A child of 2 years who observes the world for 12 hours daily has roughly 263000 minutes (more than 4000 hours) of direct observations of the world, including feedback from adults (labels). (These are just ballpark figures -- I don't know how many minutes a typical two-year-old has spent observing the world.) Moreover, the child will have exposure to many, many objects beyond the 10 classes that comprise CIFAR-10.So there are a few things at play. One is that the child has exposure to more data overall and a more diverse source of data than the CIFAR-10 model has. Data diversity and data volume are well-recognized as pre-requisites for robust models in general. In this light, it doesn't seem surprising that a neural network is worse at this task than the child, because a neural network trained on CIFAR-10 is positively starved for training data compared to the two-year-old. The image resolution available to a child is better than the 32x32 CIFAR-10 images, so the child is able to learn information about the fine details of objects.The CIFAR-10 to two-year-old comparison is not perfect because the CIFAR-10 model will likely be trained with multiple passes over the same static images, while the child will see, using binocular vision, how objects are arranged in a three-dimensional world while moving about and with different lighting conditions and perspectives on the same objects.The anecdote about OP's child implies a second question, A child is endowed with some talent for self-teaching, so that new categories of objects can be added over time without having to start over from scratch. OP's remark about transfer-learning names one kind of model adaptation in the machine learning context.In comments, other users have pointed out that one- and few-shot learning* is another machine learning research area.Additionally, reinforcement-learning addresses self-teaching models from a different perspective, essentially allowing robots to undertake trial-and-error experimentation to find optimal strategies for solving specific problems (e.g. playing chess).It's probably true that all three of these machine learning paradigms are germane to improving how machines adapt to new computer vision tasks. Quickly adapting machine learning models to new tasks is an active area of research. However, because the practical goals of these projects (identify new instances of malware, recognize imposters in passport photos, index the internet) and criteria for success differ from the goals of a child learning about the world, and the fact that one is done in a computer using math and the other is done in organic material using chemistry, direct comparisons between the two will remain fraught.As an aside, it would be interesting to study how to flip the CIFAR-10 problem around and train a neural network to recognize 6000 objects from 10 examples of each. But even this wouldn't be a fair comparison to 2-year-old, because there would still be a large discrepancy in the total volume, diversity and resolution of the training data.*We don't presently have a tags for one-shot learning or few-shot learning."
What is the most surprising characterization of the Gaussian (normal) distribution?,"
A standardized Gaussian distribution on $\mathbb{R}$ can be defined by giving explicitly its density: 
$$ \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$$
or its characteristic function. 
As recalled in this question it is also the only distribution for which the sample mean and variance are independent. 
What are other surprising alternative characterization of Gaussian measures that you know ? I will accept the most surprising answer 
","['probability', 'normal-distribution', 'mathematical-statistics', 'characteristic-function']","My personal most surprising is the one about the sample mean and variance, but here is another (maybe) surprising characterization: if $X$ and $Y$ are IID with finite variance with $X+Y$ and $X-Y$ independent, then $X$ and $Y$ are normal.Intuitively, we can usually identify when variables are not independent with a scatterplot.  So imagine a scatterplot of $(X,Y)$ pairs that looks independent.   Now rotate by 45 degrees and look again: if it still looks independent, then the $X$ and $Y$ coordinates individually must be normal (this is all speaking loosely, of course).To see why the intuitive bit works, take a look at$$
\left[
\begin{array}{cc}
\cos45^{\circ} & -\sin45^{\circ} \newline 
\sin45^{\circ} & \cos45^{\circ} 
\end{array}
\right]
\left[
\begin{array}{c}
x \newline
y
\end{array}
\right]= \frac{1}{\sqrt{2}}
\left[
\begin{array}{c}
x-y \newline
x+y
\end{array}
\right]
$$"
Is the R language reliable for the field of economics?,"
I am a graduate student in economics who recently converted to R from other very well-known statistical packages (I was using SPSS mainly). My little problem at the moment is that I am the only R user in my class. My classmates use Stata and Gauss and one of my professors even said that R is perfect for engineering, but not for economics. He said that many packages are built by people who know a lot about programming, but not much about economics and therefore are not reliable. He also mentioned the fact that since no money is actually involved in building an R package, there is therefore no incentive to do it correctly (unlike in Stata for example) and that he used R for a time and got some ""ridiculous"" results in his attempts to estimate some stuff. Moreover, he complained about he random number generator in R which he said was ""messy"".
I've been using R for just a little more than a month and I must say I have fallen in love with it. All this stuff I am hearing from my professor is just discouraging me.
So my question is: ""Is R reliable for the field of economics?"".
","['r', 'software', 'econometrics']","Let me share a contrasting view point. I'm an economist. I was trained in econometrics using SAS. I work in financial services and just tonight I updated R based models which we will use tomorrow to put millions of dollars at risk.Your professor is just plain wrong. But the mistake he's making is VERY common and is worth discussing. What your professor seems to be doing is commingling the idea of the R software (the GNU implementation of the S language) vs. packages (or other code) implemented in R. I can write crap implementations of a linear regression using SAS IML. As a matter of fact, I've done that very thing. Does that mean SAS is crap? Of course not. SAS is crap because their pricing is non-transparent, ridiculously expensive, and their in house consultants over promise, under deliver, and charge a premium for the pleasure. But I digress...The openness of R is a double edged sword: Openness allows any Tom, Dick, or Harry to write a crap implementation of any algorithm they think up while smoking pot in the basement of the economics building. The same openness allows practicing economists to share code openly and improve on each other's code. The licensing rules with R mean that I can write parallelization code for running R in parallel on Amazon's cloud and not have to worry about licensing fees for a 30 node cluster. This is a HUGE win for simulation based analysis which is a big part of what I do. Your professor's comment that ""many packages are built by people who know a lot about programming, but not much about economics"" is, no doubt, correct. But there are 3716 packages on CRAN. You can be damn sure many of them were not written by economists. In the same way that you can be sure many of the 105,089 modules in CPAN were not written by economists. Choose your software carefully. Make sure you understand and have tested the tools you're using. Also make sure you understand the true economics behind  which ever implementation you chose. Getting locked into a closed software solution is more costly than just the licensing fees. "
Why is the sum of two random variables a convolution?,"
For long time I did not understand why the ""sum"" of two random variables is their convolution, whereas a mixture density function sum of $f(x)$ and $g(x)$ is $p\,f(x)+(1-p)g(x)$; the arithmetic sum and not their convolution. The exact phrase ""the sum of two random variables"" appears in google 146,000 times, and is elliptical as follows. If one considers an RV to yield a single value, then that single value can be added to another RV single value, which has nothing to do with convolution, at least not directly, all that is is a sum of two numbers. An RV outcome in statistics is however a collection of values and thus a more exact phrase would be something like ""the set of coordinated sums of pairs of associated individual values from two RV's is their discrete convolution""...and can be approximated by the convolution of the density functions corresponding to those RV's. Even simpler language: 2 RV's of $n$-samples are in effect two n-dimensional vectors that add as their vector sum.         
Please show the details of how the sum of two random variables are a convolution and a sum. 
","['density-function', 'terminology', 'cumulative-distribution-function', 'mixture-distribution', 'convolution']","Convolution calculations associated with distributions of random variables are all mathematical manifestations of the Law of Total Probability.In the language of my post at What is meant by a “random variable”?,A pair of random variables $(X,Y)$ consists of a box of tickets on each of which are written two numbers, one designated $X$ and the other $Y$. The sum of these random variables is obtained by adding the two numbers found on each ticket.I posted a picture of such a box and its tickets at Clarifying the concept of sum of random variables.This computation literally is a task you could assign to a third-grade classroom. (I make this point to emphasize both the fundamental simplicity of the operation as well as showing how strongly it is connected with what everybody understands a ""sum"" to mean.)How the sum of random variables is expressed mathematically depends on how you represent the contents of the box:In terms of probability mass functions (pmf) or probability density functions (pdf), it is the operation of convolution.In terms of moment generating functions (mgf), it is the (elementwise) product.In terms of (cumulative) distribution functions (cdf), it is an operation closely related to the convolution.  (See the references.)In terms of characteristic functions (cf) it is the (elementwise) product.In terms of cumulant generating functions (cgf) it is the sum.The first two of these are special insofar as the box might not have a pmf, pdf, or mgf, but it always has a cdf, cf, and cgf.To see why convolution is the appropriate method to compute the pmf or pdf of a sum of random variables, consider the case where all three variables $X,$ $Y,$ and $X+Y$ have a pmf: by definition, the pmf for $X+Y$ at any number $z$ gives the proportion of tickets in the box where the sum $X+Y$ equals $z,$ written $\Pr(X+Y=z).$The pmf of the sum is found by breaking down the set of tickets according to the value of $X$ written on them, following the Law of Total Probability, which asserts proportions (of disjoint subsets) add.  More technically,The proportion of tickets found within a collection of disjoint subsets of the box is the sum of the proportions of the individual subsets.It is applied thus:The proportion of tickets where $X+Y=z$, written $\Pr(X+Y=z),$ must equal the sum over all possible values $x$ of the proportion of tickets where $X=x$ and $X+Y=z,$ written $\Pr(X=x, X+Y=z).$Because $X=x$ and $X+Y=z$ imply $Y=z-x,$ this expression can be rewritten directly in terms of the original variables $X$ and $Y$ as$$\Pr(X+Y=z) = \sum_x \Pr(X=x, Y=z-x).$$That's the convolution.Please note that although convolutions are associated with sums of random variables, the convolutions are not convolutions of the random variables themselves!Indeed, in most cases it is not possible to convolve two random variables.  For this to work, their domains have to have additional mathematical structure.  This structure is a continuous topological group.Without getting into details, suffice it to say that convolution of any two functions $X, Y:G \to H$ must abstractly look something like$$(X\star Y)(g) = \sum_{h,k\in G\mid h+k=g} X(h)Y(k).$$(The sum could be an integral and, if this is going to produce new random variables from existing ones, $X\star Y$ must be measurable whenever $X$ and $Y$ are; that's where some consideration of topology or measurability must come in.)This formula invokes two operations.  One is the multiplication on $H:$ it must make sense to multiply values $X(h)\in H$ and $Y(k)\in H.$  The other is the addition on $G:$ it must make sense to add elements of $G.$In most probability applications, $H$ is a set of numbers (real or complex) and multiplication is the usual one.  But $G,$ the sample space, often has no mathematical structure at all.  That's why the convolution of random variables is usually not even defined.  The objects involved in convolutions in this thread are mathematical representations of the distributions of random variables.  They are used to compute the distribution of a sum of random variables, given the joint distribution of those random variables.Stuart and Ord, Kendall's Advanced Theory of Statistics, Volume 1.  Fifth Edition, 1987, Chapters 1, 3, and 4 (Frequency Distributions, Moments and Cumulants, and Characteristic Functions)."
What should I do when my neural network doesn't generalize well?,"
I'm training a neural network and the training loss decreases, but the validation loss doesn't, or it decreases much less than what I would expect, based on references or experiments with very similar architectures and data. How can I fix this?

As for question

What should I do when my neural network doesn't learn?

to which this question is inspired, the question is intentionally left general so that other questions about how to reduce the generalization error of a neural network down to a level which has been proved to be attainable, can be closed as duplicates of this one.
See also dedicated thread on Meta:

Is there a generic question to which we can redirect questions of the type ""why does my neural network not generalize well?""

","['neural-networks', 'overfitting', 'faq']","First of all, let's mention what does ""my neural network doesn't generalize well"" mean and what's the difference with saying ""my neural network doesn't perform well"".When training a Neural Network, you are constantly evaluating it on a set of labelled data called the training set. If your model isn't working properly and doesn't appear to learn from the training set, you don't have a generalization issue yet, instead please refer to this post. However, if your model is achieving a satisfactory performance on the training set, but cannot perform well on previously unseen data (e.g. validation/test sets), then you do have a generalization problem.The most important part is understanding why your network doesn't generalize well. High-capacity Machine Learning models have the ability to memorize the training set, which can lead to overfitting.Overfitting is the state where an estimator has begun to learn the training set so well that it has started to model the noise in the training samples (besides all useful relationships).For example, in the image below we can see how the blue on the right line has clearly overfit.When attempting to evaluate our model on new, previously unseen data (i.e. validation/test set), the model's performance will be much worse than what we expect.In the beginning of the post I implied that the complexity of your model is what is actually causing the overfitting, as it is allowing the model to extract unnecessary relationships from the training set, that map its inherent noise. The easiest way to reduce overfitting is to essentially limit the capacity of your model. These techniques are called regularization techniques.Parameter norm penalties. These add an extra term to the weight update function of each model, that is dependent on the norm of the parameters. This term's purpose is to counter the actual update (i.e. limit how much each weight can be updated). This makes the models more robust to outliers and noise. Examples of such regularizations are L1 and L2 regularizations, which can be found on the Lasso, Ridge and Elastic Net regressors.
Since each (fully connected) layer in a neural network functions much like a simple linear regression, these are used in Neural Networks. The most common use is to regularize each layer individually.
keras implementation.Early stopping. This technique attempts to stop an estimator's training phase prematurely, at the point where it has learned to extract all meaningful relationships from the data, before beginning to model its noise. This is done by monitoring the validation loss (or a validation metric of your choosing) and terminating the training phase when this metric stops improving. This way we give the estimator enough time to learn the useful information but not enough to learn from the noise.
keras implementation.Another way of preventing overfitting, besides limiting the model's capacity, is by improving the quality of your data. The most obvious choice would be outlier/noise removal, however in practice their usefulness is limited. A more common way (especially in image-related tasks) is data augmentation. Here we attempt randomly transform the training examples so that while they appear to the model to be different, they convey the same semantic information (e.g. left-right flipping on images).
Data augmentation overview"
Effect of switching response and explanatory variable in simple linear regression,"
Let's say that there exists some ""true"" relationship between $y$ and $x$ such that $y = ax + b + \epsilon$, where $a$ and $b$ are constants and $\epsilon$ is i.i.d normal noise. When I randomly generate data from that R code: x <- 1:100; y <- ax + b + rnorm(length(x)) and then fit a model like y ~ x, I obviously get reasonably good estimates for $a$ and $b$.
If I switch the role of the variables as in (x ~ y), however, and then rewrite the result for $y$ to be a function of $x$, the resulting slope is always steeper (either more negative or more positive) than that estimated by the y ~ x regression. I'm trying to understand exactly why that is and would appreciate it if anyone could give me an intuition as to what's going on there.
","['regression', 'faq']","Given $n$ data points $(x_i,y_i), i = 1,2,\ldots n$, in the plane, 
let us draw a straight line
$y = ax+b$.  If we predict $ax_i+b$ as the value $\hat{y}_i$ of $y_i$, then
the error is $(y_i-\hat{y}_i) = (y_i-ax_i-b)$, the squared error is
$(y_i-ax_i-b)^2$, and the total squared error $\sum_{i=1}^n (y_i-ax_i-b)^2$.
We askWhat choice of $a$ and  $b$ minimizes 
  $S =\displaystyle\sum_{i=1}^n (y_i-ax_i-b)^2$?Since $(y_i-ax_i-b)$ is the vertical distance of $(x_i,y_i)$ from
the straight line, we are asking for the line such that the
sum of the squares of the vertical distances of the points from
the line is as small as possible.  Now $S$  is a
quadratic function of both $a$ and $b$ and attains its minimum
value when $a$ and $b$ are such that
$$\begin{align*}
\frac{\partial S}{\partial a} &= 2\sum_{i=1}^n (y_i-ax_i-b)(-x_i) &= 0\\
\frac{\partial S}{\partial b} &= 2\sum_{i=1}^n (y_i-ax_i-b)(-1) &= 0
\end{align*}$$
From the second equation, we get
$$b = \frac{1}{n}\sum_{i=1}^n (y_i - ax_i) = \mu_y - a\mu_x$$
where 
$\displaystyle \mu_y = \frac{1}{n}\sum_{i=1}^n y_i, ~
 \mu_x = \frac{1}{n}\sum_{i=1}^n x_i$ are the arithmetic average values
of the $y_i$'s and the $x_i$'s respectively.  Substituting into the
first equation, we get
$$
a = \frac{\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) -\mu_x\mu_y}{
\left( \frac{1}{n}\sum_{i=1}^n x_i^2\right)  -\mu_x^2}.
$$
Thus, the line that minimizes $S$ can be expressed as
$$y = ax+b = \mu_y +
\left(\frac{\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) -\mu_x\mu_y}{
\left( \frac{1}{n}\sum_{i=1}^n x_i^2\right)  -\mu_x^2}\right)
(x - \mu_x),
$$
and the minimum value of $S$ is
$$S_{\min} =  
\frac{\left[\left(\frac{1}{n}\sum_{i=1}^n y_i^2\right)  -\mu_y^2\right]
\left[\left(\frac{1}{n}\sum_{i=1}^n x_i^2\right)  -\mu_x^2\right]
-
\left[\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) 
-\mu_x\mu_y\right]^2}{\left(\frac{1}{n}\sum_{i=1}^n x_i^2\right)  -\mu_x^2}.$$If we interchange the roles of $x$ and $y$, draw a line
$x = \hat{a}y + \hat{b}$, and ask for the values of
$\hat{a}$ and $\hat{b}$ that minimize
$$T = \sum_{i=1}^n (x_i - \hat{a}y_i - \hat{b})^2,$$
that is, we want the line such that the
sum of the squares of the horizontal distances of the points from
the line is as small as possible, then we get$$x = \hat{a}y+\hat{b} = \mu_x +
\left(\frac{\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) -\mu_x\mu_y}{
\left( \frac{1}{n}\sum_{i=1}^n y_i^2\right)  -\mu_y^2}\right)
(y - \mu_y)
$$ 
and the minimum value of $T$ is
$$T_{\min} =  
\frac{\left[\left(\frac{1}{n}\sum_{i=1}^n y_i^2\right)  -\mu_y^2\right]
\left[\left(\frac{1}{n}\sum_{i=1}^n x_i^2\right)  -\mu_x^2\right]
-
\left[\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) 
-\mu_x\mu_y\right]^2}{\left(\frac{1}{n}\sum_{i=1}^n y_i^2\right)  -\mu_y^2}.$$Note that both lines pass through the point $(\mu_x,\mu_y)$
but the slopes are 
$$a = 
\frac{\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) -\mu_x\mu_y}{
\left( \frac{1}{n}\sum_{i=1}^n x_i^2\right)  -\mu_x^2},~~
\hat{a}^{-1} = \frac{
\left( \frac{1}{n}\sum_{i=1}^n y_i^2\right)  -\mu_y^2}{\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) -\mu_x\mu_y}$$ 
are different in general.  Indeed, as @whuber points out in a comment, the
slopes are the same when all the points $(x_i,y_i)$ lie on the same
straight line.  To see this, note that 
$$\hat{a}^{-1} - a = \frac{S_{\min}}{\left(\frac{1}{n}\sum_{i=1}^n x_iy_i\right) -\mu_x\mu_y} = 0 \Rightarrow S_{\min} = 0 \Rightarrow y_i=ax_i+b, i=1,2,\ldots, n.
$$"
Cost function of neural network is non-convex?,"
The cost function of neural network is $J(W,b)$, and it is claimed to be non-convex. I don't quite understand why it's that way, since as I see that it's quite similar to the cost function of logistic regression, right?
If it is non-convex, so the 2nd order derivative $\frac{\partial J}{\partial W} < 0$, right?
UPDATE
Thanks to the answers below as well as @gung's comment, I got your point, if there's no hidden layers at all, it's convex, just like logistic regression. But if there's hidden layers, by permuting the nodes in the hidden layers as well as the weights in subsequent connections, we could have multiple solutions of the weights resulting to the same loss. 
Now more questions, 
1) There're multiple local minima, and some of them should be of the same value, since they're corresponding to some nodes and weights permutations, right?
2) If the nodes and weights won't be permuted at all, then it's convex, right? And the minima will be the global minima. If so, the answer to 1) is, all those local minima will be of the same value, correct?
","['machine-learning', 'neural-networks', 'loss-functions']","The cost function of a neural network is in general neither convex nor concave. This means that the matrix of all second partial derivatives (the Hessian) is neither positive semidefinite, nor negative semidefinite. Since the second derivative is a matrix, it's possible that it's neither one or the other.To make this analogous to one-variable functions, one could say that the cost function is neither shaped like the graph of $x^2$ nor like the graph of $-x^2$. Another example of a non-convex, non-concave function is $\sin(x)$ on $\mathbb{R}$. One of the most striking differences is that $\pm x^2$ has only one extremum, whereas $\sin$ has infinitely many maxima and minima.How does this relate to our neural network? A cost function $J(W,b)$ has also a number of local maxima and minima, as you can see in this picture, for example.The fact that $J$ has multiple minima can also be interpreted in a nice way. In each layer, you use multiple nodes which are assigned different parameters to make the cost function small. Except for the values of the parameters, these nodes are the same. So you could exchange the parameters of the first node in one layer with those of the second node in the same layer, and accounting for this change in the subsequent layers. You'd end up with a different set of parameters, but the value of the cost function can't be distinguished by (basically you just moved a node, to another place, but kept all the inputs/outputs the same)."
Logistic Regression - Error Term and its Distribution,"
On whether an error term exists in logistic regression (and its assumed distribution), I have read in various places that:

no error term exists
the error term has a binomial distribution (in accordance with the distribution of the response variable)
the error term has a logistic distribution

Can someone please clarify?
","['logistic', 'binomial-distribution', 'bernoulli-distribution', 'logistic-distribution']","In linear regression observations are assumed to follow a Gaussian distribution with a mean parameter conditional on the predictor values. If you subtract the mean from the observations you get the error: a Gaussian distribution with mean zero, & independent of predictor values—that is errors at any set of predictor values follow the same distribution.In logistic regression observations $y\in\{0,1\}$ are assumed to follow a Bernoulli distribution† with a mean parameter (a probability) conditional on the predictor values. So for any given predictor values determining a mean $\pi$ there are only two possible errors: $1-\pi$ occurring with probability $\pi$, & $0-\pi$ occurring with probability $1-\pi$.  For other predictor values the errors will be $1-\pi'$ occurring with probability $\pi'$, & $0-\pi'$ occurring with probability $1-\pi'$. So there's no common error distribution independent of predictor values, which is why people say ""no error term exists"" (1).""The error term has a binomial distribution"" (2) is just sloppiness—""Gaussian models have Gaussian errors, ergo binomial models have binomial errors"". (Or, as @whuber points out, it could be taken to mean ""the difference between an observation and its expectation has a binomial distribution translated by the expectation"".)""The error term has a logistic distribution"" (3) arises from  the derivation of logistic regression from the model where you observe whether or not a latent variable with errors following a logistic distribution exceeds some threshold. So it's not the same error defined above. (It would seem an odd thing to say IMO outside that context, or without explicit reference to the latent variable.)† If you have $k$ observations with the same predictor values, giving the same probability $\pi$ for each, then their sum $\sum y$ follows a binomial distribution with probability $\pi$ and no. trials $k$. Considering $\sum y -k\pi$ as the error leads to the same conclusions."
Is chi-squared always a one-sided test?,"
A published article (pdf) contains these 2 sentences:

Moreover, misreporting may be caused by the application of incorrect rules or by a lack of knowledge of the statistical test. For example, the total df in an ANOVA may be taken to be the error df in the reporting of an $F$ test, or the researcher may divide the reported p value of a $\chi^2$ or $F$ test by two, in order to obtain a one-sided $p$ value, whereas the $p$ value of a $\chi^2$ or $F$ test is already a one-sided test. 

Why might they have said that?  The chi-squared test is a two-sided test.  (I have asked one of the authors, but gotten no response.)
Am I overlooking something?
","['hypothesis-testing', 'chi-squared-test']",
Are mean normalization and feature scaling needed for k-means clustering?,"
What are the best (recommended) pre-processing steps before performing k-means?
","['clustering', 'normalization', 'k-means']","If your variables are of incomparable units (e.g. height in cm and weight in kg) then you should standardize variables, of course. Even if variables are of the same units but show quite different variances it is still a good idea to standardize before K-means. You see, K-means clustering is ""isotropic"" in all directions of space and therefore tends to produce more or less round (rather than elongated) clusters. In this situation leaving variances unequal is equivalent to putting more weight on variables with smaller variance, so clusters will tend to be separated along variables with greater variance.A different thing also worth to remind is that K-means clustering results are potentially sensitive to the order of objects in the data set$^1$. A justified practice would be to run the analysis several times, randomizing objects order; then average the cluster centres of the correpondent/same clusters between those runs$^2$ and input the centres as initial ones for one final run of the analysis.Here is some general reasoning about the issue of standardizing features in cluster or other multivariate analysis.$^1$ Specifically, (1) some methods of centres initialization are sensitive to case order; (2) even when the initialization method isn't sensitive, results might depend sometimes on the order the initial centres are introduced to the program by (in particular, when there are tied, equal distances within data); (3) so-called running means version of k-means algorithm is naturaly sensitive to case order (in this version - which is not often used apart from maybe online clustering - recalculation of centroids take place after each individual case is re-asssigned to another cluster).$^2$ In practice, which clusters from different runs correspond - is often immediately seen by their relative closeness. When not easily seen, correspondence can be established by a hierarchical clustering done among the centres or by a matching algorithm such as Hungarian. But, to remark, if the correspondence is so vague that it almost vanishes, then the data either had no cluster structure detectable by K-means, or K is very wrong."
"How exactly does a ""random effects model"" in econometrics relate to mixed models outside of econometrics?","
I used to think that ""random effects model"" in econometrics corresponds to a ""mixed model with random intercept"" outside of econometrics, but now I am not sure. Does it?
Econometrics uses terms like ""fixed effects"" and ""random effects"" somewhat differently from the literature on mixed models, and this causes a notorious confusion. Let us consider a simple situation where $y$ linearly depends on $x$ but with a different intercept in different groups of measurements:
$$y_{it} = \beta x_{it} + u_i + \epsilon_{it}.$$
Here each unit/group $i$ is observed at different timepoints $t$. Econometricians call it ""panel data"". 

In mixed models terminology, we can treat $u_i$ as  a fixed effect or as a random effect (in this case, it's random intercept). Treating it as fixed means fitting $\hat \beta$ and $\hat u_i$ to minimize squared error (i.e. running OLS regression with dummy group variables). Treating it as random means that we additionally assume that $u_i\sim\mathcal N(u_0,\sigma^2_u)$ and use maximum likelihood to fit $u_0$ and $\sigma^2_u$ instead of fitting each $u_i$ on its own. This leads to the ""partial pooling"" effect, where the estimates $\hat u_i$ get shrunk toward their mean $\hat u_0$.
R formula when treating group as fixed:    y ~ x + group
R formula when treating group as random:   y ~ x + (1|group)

In econometrics terminology, we can treat this whole model as a fixed effects model or as a random effects model. The first option is equivalent to the fixed effect above (but econometrics has its own way of estimating $\beta$ in this case, called ""within"" estimator). I used to think that the second option is equivalent to the random effect above; e.g. @JiebiaoWang in his highly upvoted answer to What is a difference between random effects-, fixed effects- and marginal model? says that 


In econometrics, the random-effects model may only refer to random intercept model as in biostatistics


Okay --- let us test if this understanding is correct. Here is some random data generated by @ChristophHanck in his answer to What is the difference between fixed effect, random effect and mixed effect models? (I put the data here on pastebin for those who do not use R):

@Christoph does two fits using econometrics approaches:
fe <- plm(stackY~stackX, data = paneldata, model = ""within"")
re <- plm(stackY~stackX, data = paneldata, model = ""random"")

The first one yields the estimate of beta equal to -1.0451, the second one 0.77031 (yes, positive!). I tried to reproduce it with lm and lmer:
l1 = lm(stackY ~ stackX + as.factor(unit), data = paneldata)
l2 = lmer(stackY ~ stackX + (1|as.factor(unit)), data = paneldata)

The first one yields -1.045 in perfect agreement with the within estimator above. Cool. But the second yields -1.026, which is miles away from the random effects estimator. Heh? What is going on? In fact, what is  plm even doing, when called with model = ""random""?
Whatever it is doing, can one somehow understand it via the mixed models perspective?
And what is the intuition behind whatever it is doing? I read in a couple of econometrics places that random effects estimator is a weighted average between the fixed effects estimator and the ""between"" estimator which is more or less regression slope if we do not include group identity in the model at all (this estimate is strongly positive in this case, around 4.) E.g. @Andy writes here:

The random effects estimator then uses a matrix weighted average of the within and between variation of your data. [...] This makes random effects more efficient[.]

Why? Why would we want this weighted average? And in particular, why would we want it instead of running a mixed model?
","['mixed-model', 'econometrics', 'panel-data', 'lme4-nlme', 'plm']","Summary: the ""random-effects model"" in econometrics and a ""random intercept mixed model"" are indeed the same models, but they are estimated in different ways. The econometrics way is to use FGLS, and the mixed model way is to use ML. There are different algorithms of doing FGLS, and some of them (on this dataset) produce results that are very close to ML.I will answer with my testing on plm(..., model = ""random"") and lmer(), using the data generated by @ChristophHanck.According to the plm package manual, there are four options for random.method: the method of estimation for the variance components in the random effects model. @amoeba used the default one swar (Swamy and Arora, 1972).For random effects models, four estimators of the transformation
  parameter are available by setting random.method to one of ""swar""
  (Swamy and Arora (1972)) (default), ""amemiya"" (Amemiya (1971)),
  ""walhus"" (Wallace and Hussain (1969)), or ""nerlove"" (Nerlove (1971)).I tested all the four options using the same data, getting an error for amemiya, and three totally different coefficient estimates for the variable stackX. The ones from using random.method='nerlove' and 'amemiya' are nearly equivalent to that from lmer(), -1.029 and -1.025 vs -1.026. They are also not very different from that obtained in the ""fixed-effects"" model, -1.045.Unfortunately I do not have time right now, but interested readers can find the four references, to check their estimation procedures. It would be very helpful to figure out why they make such a difference. I expect that for some cases, the plm estimation procedure using the lm() on transformed data should be equivalent to the maximum likelihood procedure utilized in lmer().The authors of plm package did compare the two in Section 7 of their paper: Yves Croissant and Giovanni Millo, 2008, Panel Data Econometrics in R: The plm package.Econometrics deal mostly with non-experimental data. Great emphasis is put on specification procedures and misspecification testing. Model specifications tend therefore to be very simple, while great attention is put on the issues of endogeneity of the regressors, dependence
  structures in the errors and robustness of the estimators under deviations from normality.
  The preferred approach is often semi- or non-parametric, and heteroskedasticity-consistent
  techniques are becoming standard practice both in estimation and testing.For all these reasons, [...] panel model estimation in econometrics is mostly
  accomplished in the generalized least squares framework based on Aitken’s Theorem [...]. On the contrary, longitudinal data
  models in nlme and lme4 are estimated by (restricted or unrestricted) maximum likelihood. [...]The econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on
  the machine, the expressions for the estimators are usually rather simple. ML estimation of
  longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence
  criteria.I appreciate that @ChristophHanck provided a thorough introduction about the four random.method used in plm and explained why their estimates are so different. As requested by @amoeba, I will add some thoughts on the mixed models (likelihood-based) and its connection with GLS.The likelihood-based method usually assumes a distribution for both the random effect and the error term. A normal distribution assumption is commonly used, but there are also some studies assuming a non-normal distribution. I will follow @ChristophHanck's notations for a random intercept model, and allow unbalanced data, i.e., let $T=n_i$.The model is
\begin{equation}
y_{it}= \boldsymbol x_{it}^{'}\boldsymbol\beta + \eta_i + \epsilon_{it}\qquad i=1,\ldots,m,\quad t=1,\ldots,n_i
\end{equation}
with $\eta_i \sim N(0,\sigma^2_\eta), \epsilon_{it} \sim N(0,\sigma^2_\epsilon)$. For each $i$, $$\boldsymbol y_i \sim N(\boldsymbol X_{i}\boldsymbol\beta, \boldsymbol\Sigma_i), \qquad\boldsymbol\Sigma_i = \sigma^2_\eta \boldsymbol 1_{n_i} \boldsymbol 1_{n_i}^{'} + \sigma^2_\epsilon \boldsymbol I_{n_i}.$$
So the log-likelihood function is $$const -\frac{1}{2} \sum_i\mathrm{log}|\boldsymbol\Sigma_i| - \frac{1}{2} \sum_i(\boldsymbol y_i - \boldsymbol X_{i}\boldsymbol\beta)^{'}\boldsymbol\Sigma_i^{-1}(\boldsymbol y_i - \boldsymbol X_{i}\boldsymbol\beta).$$When all the variances are known, as shown in Laird and Ware (1982), the MLE is
$$\hat{\boldsymbol\beta} = \left(\sum_i\boldsymbol X_i^{'} \boldsymbol\Sigma_i^{-1} \boldsymbol X_i \right)^{-1} \left(\sum_i \boldsymbol X_i^{'} \boldsymbol\Sigma_i^{-1} \boldsymbol y_i \right),$$
which is equivalent to the GLS $\hat\beta_{RE}$ derived by @ChristophHanck. So the key difference is in the estimation for the variances. Given that there is no closed-form solution, there are several approaches:In summary, MLE has distribution assumptions, and it is estimated in an iterative algorithm. The key difference between MLE and GLS is in the estimation for the variances.Croissant and Millo (2008) pointed out that While under normality, homoskedasticity and no serial correlation of the errors OLS are also the maximum likelihood estimator, in all the other cases there are important differences.In my opinion, for the distribution assumption, just as the difference between parametric and non-parametric approaches, MLE would be more efficient when the assumption holds, while GLS would be more robust."
Won't highly-correlated variables in random forest distort accuracy and feature-selection?,"
In my understanding, highly correlated variables won't cause multi-collinearity issues in random forest model (Please correct me if I'm wrong). However, on the other way, if I have too many variables containing similar information, will the model weight too much on this set rather than the others? 
For example, there are two sets of information (A,B) with the same predictive power. Variable $X_1$,$X_2$,...$X_{1000}$ all contain information A, and only Y contains information B. When random sampling variables, will most of the trees grow on information A, and as a result information B is not fully captured?
","['random-forest', 'multicollinearity', 'ensemble-learning']","That is correct, but therefore in most of those sub-samplings where variable Y was available it would produce the best possible split.You may try to increase mtry, to make sure this happens more often.You may try either recursive correlation pruning, that is in turns to remove one of two variables whom together have the highest correlation. A sensible threshold to stop this pruning could be that any pair of correlations(pearson) is lower than $R^2<.7$You may try recursive variable importance pruning, that is in turns to remove, e.g. 20% with lowest variable importance. Try e.g. rfcv from randomForest package.You may try some decomposition/aggregation of your redundant variables."
"40,000 neuroscience papers might be wrong","
I saw this article in the Economist about a seemingly devastating paper [1] casting doubt on ""something like 40,000 published [fMRI] studies."" The error, they say, is because of ""erroneous statistical assumptions."" I read the paper and see it's partly a problem with multiple comparison corrections, but I'm not an fMRI expert and am finding it difficult to follow. 
What are the erroneous assumptions the authors are talking about? Why are those assumptions made? What are ways around making these assumptions?
A back of the envelope calculation says 40,000 fMRI papers is over a $billion in funding (grad student salary, operating costs, etc.).

[1] Eklund et al., Cluster failure: Why fMRI inferences for spatial extent
have inflated false-positive rates, PNAS 2016
","['hypothesis-testing', 'multiple-comparisons', 'spatial', 'neuroimaging', 'neuroscience']","The news are really sensationalist, but the paper is really well founded. Discussions raged for days in my laboratory, all in all a really necessary critique that makes researchers introspect their work. I recommend the reading of the following commentary by Thomas Nichols, one of the authors of the ""Cluster Failure: Why fMRI inferences for spatial extent have inflated false-positive rates"" paper (sorry for the long quote).However, there is one number I regret: 40,000. In trying to refer to
  the importance of the fMRI discipline, we used an estimate of the
  entire fMRI literature as number of studies impinged by our findings.
  In our defense, we found problems with cluster size inference in
  general (severe for P=0.01 CDT, biased for P=0.001), the dominant
  inference method, suggesting the majority of the literature was
  affected. The number in the impact statement, however, has been picked
  up by popular press and fed a small twitterstorm. Hence, I feel it’s
  my duty to make at least a rough estimate of “How many articles does
  our work affect?”. I’m not a bibliometrician, and this really a
  rough-and-ready exercise, but it hopefully gives a sense of the order
  of magnitude of the problem.The analysis code (in Matlab) is laid out below, but here is the
  skinny: Based on some reasonable probabilistic computations, but
  perhaps fragile samples of the literature, I estimate about 15,000
  papers use cluster size inference with correction for multiple
  testing; of these, around 3,500 use a CDT of P=0.01. 3,500 is about 9%
  of the entire literature, or perhaps more usefully, 11% of papers
  containing original data. (Of course some of these 15,000 or 3,500
  might use nonparametric inference, but it’s unfortunately rare for
  fMRI—in contrast, it’s the default inference tool for structural
  VBM/DTI analyses in FSL).I frankly thought this number would be higher, but didn’t realise the
  large proportion of studies that never used any sort of multiple
  testing correction. (Can’t have inflated corrected significances if
  you don’t correct!). These calculations suggest 13,000 papers used no
  multiple testing correction. Of course some of these may be using
  regions of interest or sub-volume analyses, but it’s a scant few (i.e.
  clinical trial style outcome) that have absolutely no multiplicity at
  all. Our paper isn’t directly about this group, but for publications
  that used the folk multiple testing correction, P<0.001 & k>10, our
  paper shows this approach has familywise error rates well in excess of
  50%.So, are we saying 3,500 papers are “wrong”? It depends. Our results
  suggest CDT P=0.01 results have inflated P-values, but each study must
  be examined… if the effects are really strong, it likely doesn’t
  matter if the P-values are biased, and the scientific inference will
  remain unchanged. But if the effects are really weak, then the results
  might indeed be consistent with noise. And, what about those 13,000
  papers with no correction, especially common in the earlier
  literature? No, they shouldn’t be discarded out of hand either, but a
  particularly jaded eye is needed for those works, especially when
  comparing them to new references with improved methodological
  standards.He also includes this table at the end:Basically, SPM (Statistical Parametric Mapping, a toolbox for Matlab) is the most widely used tool for fMRI neuroscience studies. If you check the paper you'll see using a CDT of P = 0.001 (the standard) for clusters in SPM gives nearly the expected family-wise error rate.The authors even filled an errata due to the wording of the paper:Given the widespread misinterpretation of our paper, Eklund et al.,
  Cluster Failure: Why fMRI inferences for spatial extent have inflated
  false-positive rates, we filed an errata with the PNAS Editoral
  office:Errata for Eklund et al., Cluster failure: Why fMRI inferences for spatial extent have inflated false-positive rates.
    Eklund, Anders; Nichols, Thomas E; Knutsson, HansTwo sentences were poorly worded and could easily be misunderstood as overstating our results.The last sentence of the Significance statement should read: “These results question the validity of a number of fMRI studies and
    may have a large impact on the interpretation of weakly significant
    neuroimaging results.”The first sentence after the heading “The future of fMRI” should have read: “Due to lamentable archiving and data-sharing practices it
    is unlikely that problematic analyses can be redone.”These replace the two sentences that mistakenly implied that our work
  affected all 40,000 publications (see Bibliometrics of Cluster
  Inference for an guestimate of how much of the literature is
  potentially affected).After initially declining the the errata, on the grounds that it was
  correcting interpretation and not fact, PNAS have agreed to publish it
  as we submitted it above.Some news also mentioned a bug as the cause of the invalidity of the studies. Indeed, one of AFNI tools was undercorrecting inferences, and this was solved after the preprint was posted in arXiv.Functional neuroimaging includes many techniques that aim to measure neuronal activity in the brain (e.g. fMRI, EEG, MEG, NIRS, PET and SPECT). These are based on different contrast mechanisms. fMRI is based on the blood-oxygen level dependent (BOLD) contrast. In task-based fMRI, given a stimulus, the neurons in the brain responsible for the reception of that stimulation start consuming energy and this triggers the haemodynamic response changing the magnetic resonance signal ($\approx 5\%$) in the vicinity of the recruited micro-vascularization.Using a generalized linear model (GLM) you identify which voxel signal time-series are correlated with the design of the paradigm of your experiment (usually a boolean timeseries convoluted with a canonical haemodynamic response function, but variations exist).So this GLM given you how much each voxel time-series resembles the task. Now, say you have two groups of individuals: patients and controls usually. Comparing the GLM scores between the groups could be used to show how the condition of the groups modulates their brain ""activation"" pattern.Voxel-wise comparison between the groups is doable, but due to the point-spread function inherent to the equipment plus a smoothing preprocessing step it isn't reasonable to expect voxels individually carry all the information. The difference in voxels among groups should be, in fact, spread over neighboring voxels.So, cluster-wise comparison is performed, i.e. only differences between groups that form into clusters are considered. This cluster extent thresholding is the most popular multiple comparison correction technique in fMRI studies. The problem lies here.SPM and FSL depend on Gaussian random-field theory (RFT) for
  FWE-corrected voxelwise and clusterwise inference. However, RFT
  clusterwise inference depends on two additional assumptions. The first
  assumption is that the spatial smoothness of the fMRI signal is
  constant over the brain, and the second assumption is that the spatial
  autocorrelation function has a specific shape (a squared exponential)
  (30)In SPM at least you have to set a nominal FWE rate and also a cluster-defining threshold (CDT). Basically, SPM finds voxels highly correlated to the task and, after thresholding with the CDT, neighboring ones are aggregated into clusters. These clusters sizes are compared to the expected cluster extent from Random Field Theory (RFT) given the FWER set [1].Random field theory requires the activity map to be smooth, to be a
  good lattice approximation to random fields. This is related to the
  amount of smoothing that is applied to the volumes. The smoothing also
  affects the assumption that the residuals are normally distributed, as
  smoothing, by the central limit theorem, will make the data more
  Gaussian. The authors have shown in [1] that the expected cluster sizes from RFT are really small when comparing with cluster extent thresholds obtained from random permutation testing (RPT).In their most recent paper, resting-state (another modality of fMRI, where participants are instructed to not think in anything in particular) data was used as if people performed a task during image acquisition, and the group comparison was performed voxel- and cluster-wise. The observed false positive error (i.e. when you observe differences in the signal response to a virtual task between groups) rate should be reasonably lower than the expected FWE rate set at $\alpha = 0.05$. Redoing this analysis millions of times on randomly sampled groups with different paradigms showed most observed FWE rates to be higher than acceptable though.@amoeba raised these two highly pertinent questions in the comments:(1) The Eklund et al. PNAS paper talks about ""nominal 5% level"" of all
  the tests (see e.g. horizontal black line on Fig 1). However, CDT in
  the same figure is varying and can be e.g. 0.01 and 0.001. How does
  CDT threshold relate to the nominal type I error rate? I am confused
  by that. (2) Have you seen Karl Friston's reply
  http://arxiv.org/abs/1606.08199 ? I read it, but I am not quite sure what
  they are saying: do I see correctly that they agree with Eklund et al.
  but say that this is a ""well known"" issue?(1) Good question. I actually reviewed my references, let's see if I can make it clearer now. Cluster-wise inference is based on the extent of clusters that form after a primary threshold (the CDT, which is arbitrary) is applied. In the secondary analysis a threshold on the number of voxels per cluster is applied. This threshold is based on the expected distribution of null cluster extents, which can be estimated from theory (e.g. RFT), and sets a nominal FWER. A good reference is [2].(2) Thanks for this reference, didn't see it before. Flandin & Friston argue Eklund et al. corroborated RFT inference because they basically showed that respecting its assumptions (regarding CDT and smoothing) the results are unbiased. Under this light, the new results show different practices in the literature tend to bias the inference as it breaks down the assumptions of RFT.It's also well known many studies in neuroscience don't correct for multiple comparisons, estimates ranging from 10% to 40% of the literature. But these are not accounted by that claim, everyone knows these papers have fragile validity and possibly huge false positive rates.The authors also reported a procedure that produces FWER in excess of 70%. This ""folk""-procedure consists in applying the CDT to keep only highly significant clusters and then applying another arbitrarily chosen cluster-extent threshold (in number of voxels). This, sometimes called ""set-inference"", has weak statistical bases, and possibly generates the least trustworthy results.The same authors had already reported on problems with the validity of SPM [1] on individual analyses. There are also other cited works in this area.Curiously, several reports on group- and individual-level analysis based on simulated data concluded the RFT threshold were, in fact, conservative. With recent advances in processing power though RPT can be performed much more easily on real data, showing great discrepancies with RFT.A commentary on ""Cluster Failure"" has surfaced last June [3]. There Mueller et al. argue the results presented in Eklund et al might be due to a specific imaging preprocessing technique used in their study. Basically, they resampled the functional images to a higher resolution before smoothing (while probably not done by every researcher, this is a routine procedure in most fMRI analysis software). They also note that Flandin & Friston didn't. I actually got to see Eklund talk at the same month in the Organization for Human Brain Mapping (OHBM) Annual Meeting in Vancouver, but I don't remember any comments on this issue, yet it seems crucial to the question.[1] Eklund, A., Andersson, M., Josephson, C., Johannesson, M., & Knutsson, H. (2012). Does parametric fMRI analysis with SPM yield valid results?—An empirical study of 1484 rest datasets. NeuroImage, 61(3), 565-578.[2] Woo, C. W., Krishnan, A., & Wager, T. D. (2014). Cluster-extent based thresholding in fMRI analyses: pitfalls and recommendations. Neuroimage, 91, 412-419.[3] Mueller, K., Lepsien, J., Möller, H. E., & Lohmann, G. (2017). Commentary: Cluster failure: Why fMRI inferences for spatial extent have inflated false-positive rates. Frontiers in Human Neuroscience, 11. "
How much to pay? A practical problem,"
This is not a home work question but real problem faced by our company. 
Very recently (2 days ago) we ordered for manufacturing of 10000 product labels to a dealer. Dealer is independent person. He gets the labels manufactured from outside and company make payment to the dealer. Each label cost exactly $1 to the company. 
Yesterday, dealer came with labels but labels were bundled in a packet of 100 labels each. In this way there were total 100 packet and each packet contained 100 labels, so total 10000 label. Before making payment to the dealer of $10000, we decided to count few packet to ensure that each packet exactly contain 100 labels. When we counted the labels we found packet short of 100 labels (we found 97 labels).  To ensure that this is not by chance but has done intentionally we counted 5 more packets and found following number of labels in each packet (including first packet) :
Packet Number    Number of labels
1                97 
2                98  
3                96
4                100
5                95 
6                97  

It was not possible to count each and every packet, so we decided to make payment on average basis. So, average number of labels in six packet is 97.166, so total payment decided was $9716.
I just want to know how statistician must have deal with such type of problem.
Further I want to know how much should we pay to get 95% assurance that we have not paid more than actual number of whole labels.  
Additional information: 
P(any packet contained more than 100 label)= 0
P(any packet contained label less than 90) =0 {labels less than 90 would be easily detected while counting packets because packet would be of lesser weight}

EDIT : 
Dealer simply denied of such malpractice. We found these dealer works on a specific commission which they get from the manufacturer on what is being paid by the company.When we communicated directly to manufacturer, we found that it is neither manufacturer nor dealer fault. Manufacturer said, “Labels gets short because sheets are not standardized in size, and whatever number is cut from the single sheet they get them bundle together in a packet”.
Further, we get validated our first assertion given in additional information, because manufacturer admitted that from marginal increase in size of sheet, it is not possible to cut additional labels, also, from a marginal reduction in size of sheet it is not possible to cut 100 labels of exactly same size.  
","['probability', 'bayesian', 'model', 'decision-theory']","I would be interested in feedback on the paragraph beginning ""Upon reflection..."", since particular part of the model has been keeping me up at night.The revised question makes me think that we can develop the model explicitly, without using simulation. Simulation introduced additional variability due to the inherent randomness of sampling. Sophologists answer is great, though.Assumptions: the smallest number of labels per envelope is 90, and the largest is 100. Therefore, the smallest possible number of labels is 9000+7+8+6+10+5+7=9043 (as given by OP's data), 9000 due to our lower bound, and the additional labels coming from the observed data.Denote $Y_i$ the number of labels in an envelope $i$. Denote $X_i$ the number of labels over 90, i.e. $X=Y-90$, so $X\in\{0,1,2,...,10\}$. The binomial distribution models the total number of successes (here a success is the presence of a label in an envelope) in $n$ trials when the trials are independent with constant success probability $p$ so $X$ takes values $0, 1, 2, 3, ..., n.$ We take $n=10$, which gives 11 different possible outcomes. I assume that because the sheet sizes are irregular, some sheets only have room for $X$ additional labels in excess of 90, and that this ""additional space"" for each label in excess of 90 occurs independently with probability $p$. So $X_i\sim\text{Binomial}(10,p).$ (Upon reflection, the independence assumption/binomial model is probably a strange assumption to make, since it effectively fixes the composition of the printer's sheets to be unimodal, and the data can only change the location of the mode, but the model will never admit a multimodal distribution. For example,  under an alternative model, it's possible that the printer only has sheets of sizes 97, 98, 96, 100 and 95: this satisfies all the stated constraints and data doesn't exclude this possibility. It might be more appropriate to regard each sheet size as its own category and then fit a Dirichlet-multinomial model to the data. I do not do this here because the data are so scarce, so posterior probabilities on each of the 11 categories will be very strongly influenced by the prior. On the other hand, by fitting the simpler model we are likewise constricting the kinds of inferences that we can make.)Each envelope $i$ is an iid realization of $X$. The sum of binomial trials with the same success probability $p$ is also binomial, so $\sum_i X_i\sim\text{Binomial}(60,p).$ (This is a theorem -- to verify, use the MGF uniqueness theorem.)I prefer to think about these problems in a Bayesian mode, because you can make direct probability statements about posterior quantities of interest. A typical prior for binomial trials with unknown $p$ is the beta distribution, which is very flexible (varies between 0 and 1, can be symmetric or asymmetric in either direction, uniform or one of two Dirac masses, have an antimode or a mode... It's an amazing tool!). In the absence of data, it seems reasonable to assume uniform probability over $p$. That is, one might expect to see a sheet accommodate 90 labels as often as 91, as often as 92, ..., as often as 100. So our prior is $p\sim\text{Beta}(1,1).$ If you don't think this beta prior is reasonable, the uniform prior can be replaced with another beta prior, and the math won't even increase in difficulty!The posterior distribution on $p$ is $p\sim\text{Beta}(1+43,1+17)$ by the conjugacy properties of this model. This is only an intermediate step, though, because we don't care about $p$ as much as we care about the total number of labels. Forunately, the properties of conjugacy also mean that the posterior predictive distribution of sheets is beta-binomial, with parameters of the beta posterior. There are $940$ reamining ""trials"", i.e. labels for which their presence in the delivery is uncertain, so our posterior model on the remaining labels $Z$ is $Z\sim\text{BB}(44,18,940).$Since we have a distribution on $Z$ and a value model per label (the vendor agreed to one dollar per label), we can also infer a probability distribution over the value of the lot. Denote $D$ the total dollar value of the lot. We know that $D=9043+Z$, because $Z$ only models the labels that we are uncertain about. So the distribution over value is given by $D$. We can find that the quantiles at 0.025 and 0.975 (a 95% interval) are 553 and 769, respectively. So the 95% interval on D is $[9596, 9812]$. Your payment falls in that interval. (The distribution on $D$ is not exactly symmetric, so this is not the central 95% interval -- however, the asymmetry is negligible. Anyway, as I elaborate below, I'm not sure that a central 95% interval is even the correct one to consider!)I'm not aware of a quantile function for beta binomial distribution in R, so I wrote my own using R's root-finding.Another way to think about it is just to think about the expectation. If you repeated this process many times, what's the average cost you would pay? We can compute the expectation of $D$ directly. $\mathbb{E}(D)=\mathbb{E}(9043+Z)=\mathbb{E}(Z)+9043.$ The beta binomial model has expectation $\mathbb{E}(Z)=\frac{n\alpha}{\alpha+\beta}=667.0968$, so $\mathbb{E}(D)=9710.097,$ almost exactly what you paid. Your expected loss on the deal was only 6 dollars! All told, well done! But I'm not sure either of these figures is the most relevant. After all, this vendor is trying to cheat you! If I were doing this deal, I'd stop worrying about breaking even or the fair-value price of the lot and start working out the probability that I'm overpaying! The vendor is clearly trying to defraud me, so I'm perfectly within my rights to minimize my losses and not concern myself with the break-even point. In this setting, the highest price I would offer is 9615 dollars, because this is the 5% quantile of the posterior on $D$, i.e. there's 95% probability that I'm underpaying. The vendor can't prove to me that all the labels are there, so I'm going to hedge my bets.(Of course, the fact that the vendor accepted the deal tells us that he has nonnegative real loss... I haven't figured out a way to use that information to help us determine more precisely how much you were cheated, except to note that because he accepted the offer, you were at best breaking even.)We only have 6 observations to work with. The justification for the bootstrap is asymptotic, so let's consider what the results look like on our small sample. This plot shows the density of the boostrap simulation.
The ""bumpy"" pattern is an artifact of the small sample size. Including or excluding any one point will have a dramatic effect on the mean, creating this ""bunchy"" apperance. The Bayesian approach smooths out these clumps and, in my opinion, is a more believable portrait of what's going on. Vertical lines are the 5% quantiles."
Is it important to scale data before clustering?,"
I found this tutorial, which suggests that you should run the scale function on features before clustering (I believe that it converts data to z-scores).
I'm wondering whether that is necessary. I'm asking mostly because there's a nice elbow point when I don't scale the data, but it disappears when it's scaled. :)
","['clustering', 'k-means']","The issue is what represents a good measure of distance between cases.  If you have two features, one where the differences between cases is large and the other small, are you prepared to have the former as almost the only driver of distance?  So for example if you clustered people on their weights in kilograms and heights in metres, is a 1kg difference as significant as a 1m difference in height?  Does it matter that you would get different clusterings on weights in kilograms and heights in centimetres? If your answers are ""no"" and ""yes"" respectively then you should probably scale.On the other hand, if you were clustering Canadian cities based on distances east/west and distances north/south then, although there will typically be much bigger differences east/west, you may be happy just to use unscaled distances in either kilometres or miles (though you might want to adjust degrees of longitude and latitude for the curvature of the earth).   "
"If we fail to reject the null hypothesis in a large study, isn't it evidence for the null?","

A basic limitation of null hypothesis significance testing is that it does not allow a researcher to gather evidence in favor of the null (Source)

I see this claim repeated in multiple places, but I can't find justification for it. If we perform a large study and we don't find statistically significant evidence against the null hypothesis, isn't that evidence for the null hypothesis?
",['hypothesis-testing'],"Failing to reject a null hypothesis is evidence that the null hypothesis is true, but it might not be particularly good evidence, and it certainly doesn't prove the null hypothesis.Let's take a short detour.  Consider for a moment the old cliché:Absence of evidence is not evidence of absence.Notwithstanding its popularity, this statement is nonsense.  If you look for something and fail to find it, that is absolutely evidence that it isn't there.  How good that evidence is depends on how thorough your search was.  A cursory search provides weak evidence; an exhaustive search provides strong evidence.Now, back to hypothesis testing.  When you run a hypothesis test, you are looking for evidence that the null hypothesis is not true.  If you don't find it, then that is certainly evidence that the null hypothesis is true, but how strong is that evidence?  To know that, you have to know how likely it is that evidence that would have made you reject the null hypothesis could have eluded your search.  That is, what is the probability of a false negative on your test?  This is related to the power, $\beta$, of the test (specifically, it is the complement, 1-$\beta$.)Now, the power of the test, and therefore the false negative rate, usually depends on the size of the effect you are looking for.  Large effects are easier to detect than small ones.  Therefore, there is no single $\beta$ for an experiment, and therefore no definitive answer to the question of how strong the evidence for the null hypothesis is.  Put another way, there is always some effect size small enough that it's not ruled out by the experiment.From here, there are two ways to proceed.  Sometimes you know you don't care about an effect size smaller than some threshold.  In that case, you probably should reframe your experiment such that the null hypothesis is that the effect is above that threshold, and then test the alternative hypothesis that the effect is below the threshold.  Alternatively, you could use your results to set bounds on the believable size of the effect.  Your conclusion would be that the size of the effect lies in some interval, with some probability.  That approach is just a small step away from a Bayesian treatment, which you might want to learn more about, if you frequently find yourself in this sort of situation.There's a nice answer to a related question that touches on evidence of absence testing, which you might find useful."
Is this chart showing the likelihood of a terrorist attack statistically useful?,"
I'm seeing this image passed around a lot.
I have a gut-feeling that the information provided this way is somehow incomplete or even erroneous, but I'm not well versed enough in statistics to respond. It makes me think of this xkcd comic, that even with solid historical data, certain situations can change how things can be predicted.

Is this chart as presented useful for accurately showing what the threat level from refugees is? Is there necessary statistical context that makes this chart more or less useful?

Note: Try to keep it in layman's terms :)
","['probability', 'interpretation', 'prediction']","Imagine your job is to forecast the number of Americans that will die from various causes next year.A reasonable place to start your analysis might be the National Vital Statistics Data final death data for 2014. The assumption is that 2017 might look roughly like 2014. You'll find that approximately 2,626,000 Americans died in 2014:18 from terrorism using a broader definition (University of Maryland Global Terrorism Datbase) See link for definitions.By my quick count, 0 of the perpetrators of these 2014 attacks were born outside the United States.Note that anecdote is not the same as data, but I've assembled links to the underlying news stories here: 1, 2, 3, 4, 5, 6, 7, 8, and 9.Terrorist incidents in the U.S. are quite rare, so estimating off a single year is going to be problematic. Looking at the time-series, what you see is that the vast majority of U.S. terrorism fatalities came during the 9/11 attacks (See this report from the National Consortium for the Study of Terrorism and Responses to Terrorism.) I've copied their Figure 1 below:Immediately you see that you have an outlier, rare events problem. A single outlier is driving the overall number. If you're trying to forecast deaths from terrorism, there are numerous issues:IMHO, the FT graphic picked an overly narrow definition (the 9/11 attacks don't show up in the graphic because the attackers weren't refugees). There are legitimate issues with the chart, but the FT's broader point is correct that terrorism in the U.S. is quite rare. Your chance of being killed by a foreign born terrorist in the United States is close to zero.Life expectancy in the U.S. is about 78.7 years. What has moved life expectancy numbers down in the past has been events like the 1918 Spanish flu pandemic or  WWII. Additional risks to life expectancy now might include obesity and opioid abuse. If you're trying to create a detailed estimate of terrorism risk, there are huge statistical issues, but to understand the big picture requires not so much statistics as understanding orders of magnitude and basic quantitative literacy. Looking back at history, the way huge numbers of people get killed is through disease, genocide, and war. A more reasonable concern might be that some rare, terrorist event triggers something catastrophic (eg. how the assassination of Archduke Ferdinand help set off WWI.) Or one could worry about nuclear weapons in the hands of someone crazy.Thinking about extremely rare but catastrophic events is incredibly difficult. It's a multidisciplinary pursuit and goes far outside of statistics.Perhaps the only statistical point here is that it's hard to estimate the probability and effects of some event which hasn't happened? (Except to say that it can't be that common or it would have happened already.) "
Backpropagation with Softmax / Cross Entropy,"
I'm trying to understand how backpropagation works for a softmax/cross-entropy output layer.
The cross entropy error function is
$$E(t,o)=-\sum_j t_j \log o_j$$
with $t$ and $o$ as the target and output at neuron $j$, respectively. The sum is over each neuron in the output layer. $o_j$ itself is the result of the softmax function:
$$o_j=softmax(z_j)=\frac{e^{z_j}}{\sum_j e^{z_j}}$$
Again, the sum is over each neuron in the output layer and $z_j$ is the input to neuron $j$:
$$z_j=\sum_i w_{ij}o_i+b$$
That is the sum over all neurons in the previous layer with their corresponding output $o_i$ and weight $w_{ij}$ towards neuron $j$ plus a bias $b$.
Now, to update a weight $w_{ij}$ that connects a neuron $j$ in the output layer with a neuron $i$ in the previous layer, I need to calculate the partial derivative of the error function using the chain rule:
$$\frac{\partial E} {\partial w_{ij}}=\frac{\partial E} {\partial o_j} \frac{\partial o_j} {\partial z_{j}} \frac{\partial z_j} {\partial w_{ij}}$$
with $z_j$ as the input to neuron $j$.
The last term is quite simple. Since there's only one weight between $i$ and $j$, the derivative is:
$$\frac{\partial z_j} {\partial w_{ij}}=o_i$$
The first term is the derivation of the error function with respect to the output $o_j$:
$$\frac{\partial E} {\partial o_j} = \frac{-t_j}{o_j}$$
The middle term is the derivation of the softmax function with respect to its input $z_j$ is harder:
$$\frac{\partial o_j} {\partial z_{j}}=\frac{\partial} {\partial z_{j}} \frac{e^{z_j}}{\sum_j e^{z_j}}$$
Let's say we have three output neurons corresponding to the classes $a,b,c$ then $o_b = softmax(b)$ is:
$$o_b=\frac{e^{z_b}}{\sum e^{z}}=\frac{e^{z_b}}{e^{z_a}+e^{z_b}+e^{z_c}} $$
and its derivation using the quotient rule:
$$\frac{\partial o_b} {\partial z_{b}}=\frac{e^{z_b}*\sum e^z - (e^{z_b})^2}{(\sum_j e^{z})^2}=\frac{e^{z_b}}{\sum e^z}-\frac{(e^{z_b})^2}{(\sum e^z)^2}$$
$$=softmax(b)-softmax^2(b)=o_b-o_b^2=o_b(1-o_b)$$
Back to the middle term for backpropagation this means:
$$\frac{\partial o_j} {\partial z_{j}}=o_j(1-o_j)$$
Putting it all together I get
$$\frac{\partial E} {\partial w_{ij}}= \frac{-t_j}{o_j}*o_j(1-o_j)*o_i=-t_j(1-o_j)*o_i$$
which means, if the target for this class is $t_j=0$, then I will not update the weights for this. That does not sound right.
Investigating on this I found people having two variants for the softmax derivation, one where $i=j$ and the other for $i\ne j$, like here or here.
But I can't make any sense out of this. Also I'm not even sure if this is the cause of my error, which is why I'm posting all of my calculations. I hope someone can clarify me where I am missing something or going wrong.
","['backpropagation', 'derivative', 'softmax', 'cross-entropy']","Note: I am not an expert on backprop, but now having read a bit, I think the following caveat is appropriate. When reading papers or books on neural nets, it is not uncommon for derivatives to be written using a mix of the standard summation/index notation, matrix notation, and multi-index notation (include a hybrid of the last two for tensor-tensor derivatives). Typically the intent is that this should be ""understood from context"", so you have to be careful!I noticed a couple of inconsistencies in your derivation. I do not do neural networks really, so the following may be incorrect. However, here is how I would go about the problem.First, you need to take account of the summation in $E$, and you cannot assume each term only depends on one weight. So taking the gradient of $E$ with respect to component $k$ of $z$, we have
$$E=-\sum_jt_j\log o_j\implies\frac{\partial E}{\partial z_k}=-\sum_jt_j\frac{\partial \log o_j}{\partial z_k}$$Then, expressing $o_j$ as
$$o_j=\tfrac{1}{\Omega}e^{z_j} \,,\, \Omega=\sum_ie^{z_i} \implies \log o_j=z_j-\log\Omega$$
we have
$$\frac{\partial \log o_j}{\partial z_k}=\delta_{jk}-\frac{1}{\Omega}\frac{\partial\Omega}{\partial z_k}$$
where $\delta_{jk}$ is the Kronecker delta. Then the gradient of the softmax-denominator is
$$\frac{\partial\Omega}{\partial z_k}=\sum_ie^{z_i}\delta_{ik}=e^{z_k}$$
which gives
$$\frac{\partial \log o_j}{\partial z_k}=\delta_{jk}-o_k$$
or, expanding the log
$$\frac{\partial o_j}{\partial z_k}=o_j(\delta_{jk}-o_k)$$
Note that the derivative is with respect to $z_k$, an arbitrary component of $z$, which gives the $\delta_{jk}$ term ($=1$ only when $k=j$).So the gradient of $E$ with respect to $z$ is then
$$\frac{\partial E}{\partial z_k}=\sum_jt_j(o_k-\delta_{jk})=o_k\left(\sum_jt_j\right)-t_k \implies \frac{\partial E}{\partial z_k}=o_k\tau-t_k$$
where  $\tau=\sum_jt_j$ is constant (for a given $t$ vector).This shows a first difference from your result: the $t_k$ no longer multiplies $o_k$. Note that for the typical case where $t$ is ""one-hot"" we have $\tau=1$ (as noted in your first link).A second inconsistency, if I understand correctly, is that the ""$o$"" that is input to $z$ seems unlikely to be the ""$o$"" that is output from the softmax. I would think that it makes more sense that this is actually ""further back"" in network architecture?Calling this vector $y$, we then have
$$z_k=\sum_iw_{ik}y_i+b_k \implies \frac{\partial z_k}{\partial w_{pq}}=\sum_iy_i\frac{\partial w_{ik}}{\partial w_{pq}}=\sum_iy_i\delta_{ip}\delta_{kq}=\delta_{kq}y_p$$Finally, to get the gradient of $E$ with respect to the weight-matrix $w$, we use the chain rule
$$\frac{\partial E}{\partial w_{pq}}=\sum_k\frac{\partial E}{\partial z_k}\frac{\partial z_k}{\partial w_{pq}}=\sum_k(o_k\tau-t_k)\delta_{kq}y_p=y_p(o_q\tau-t_q)$$
giving the final expression (assuming a one-hot $t$, i.e. $\tau=1$)
$$\frac{\partial E}{\partial w_{ij}}=y_i(o_j-t_j)$$
where $y$ is the input on the lowest level (of your example).So this shows a second difference from your result: the ""$o_i$"" should presumably be from the level below $z$, which I call $y$, rather than the level above $z$ (which is $o$).Hopefully this helps. Does this result seem more consistent?Update: In response to a query from the OP in the comments, here is an expansion of the first step.
  First, note that the vector chain rule requires summations (see here). Second, to be certain of getting all gradient components, you should always introduce a new subscript letter for the component in the denominator of the partial derivative. So to fully write out the gradient with the full chain rule, we have
  $$\frac{\partial E}{\partial w_{pq}}=\sum_i \frac{\partial E}{\partial o_i}\frac{\partial o_i}{\partial w_{pq}}$$ 
  and
  $$\frac{\partial o_i}{\partial w_{pq}}=\sum_k \frac{\partial o_i}{\partial z_k}\frac{\partial z_k}{\partial w_{pq}}$$
  so
  $$\frac{\partial E}{\partial w_{pq}}=\sum_i \left[ \frac{\partial E}{\partial o_i}\left(\sum_k \frac{\partial o_i}{\partial z_k}\frac{\partial z_k}{\partial w_{pq}}\right) \right]$$
  In practice the full summations reduce, because you get a lot of $\delta_{ab}$ terms. Although it involves a lot of perhaps ""extra"" summations and subscripts, using the full chain rule will ensure you always get the correct result."
Explain the xkcd jelly bean comic: What makes it funny?,"
I see that one time out of the twenty total tests they run, $p < 0.05$, so they wrongly assume that during one of the twenty tests, the result is significant ($0.05 = 1/20$). 
xkcd jelly bean comic - ""Significant""

Title: Significant
Hover text: ""'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'""


","['hypothesis-testing', 'statistical-significance', 'confidence-interval', 'p-value', 'humor']","Humor is a very personal thing - some people will find it amusing, but it may not be funny to everyone - and attempts to explain what makes something funny often fail to convey the funny, even if they explain the underlying point. Indeed not all xkcd's are even intended to be actually funny. Many do, however make important points in a way that's thought provoking, and at least sometimes they're amusing while doing that. (I personally find it funny, but I find it hard to clearly explain what, exactly, makes it funny to me. I think partly it's the recognition of the way that a doubtful, or even dubious result turns into a media circus (on which see also this PhD comic), and perhaps partly the recognition of the way some research may actually be done - if usually not consciously.)However, one can appreciate the point whether or not it tickles your funnybone.The point is about doing multiple hypothesis tests at some moderate significance level like 5%, and then publicizing the one that came out significant. Of course, if you do 20 such tests when there's really nothing of any importance going on, the expected number of those tests to give a significant result is 1. Doing a rough in-head approximation for $n$ tests at significance level $\frac{1}{n}$, there's roughly a 37% chance of no significant result, roughly 37% chance of one and roughly 26% chance of more than one (I just checked the exact answers; they're close enough to that).In the comic, Randall depicted 20 tests, so this is no doubt his point (that you expect to get one significant even when there's nothing going on). The fictional newspaper article even emphasizes the problem with the subhead ""Only 5% chance of coincidence!"". (If the one test that ended up in the papers was the only one done, that might be the case.)Of course, there's also the subtler issue that an individual researcher may behave much more reasonably, but the problem of rampant publicizing of false positives still occurs. Let's say that these researchers only do 5 tests, each at the 1% level, so their overall chance of discovering a bogus result like that is only about five percent.So far so good. But now imagine there are 20 such research groups, each testing whichever random subset of colors they think they have reason to try. Or 100 research groups... what chance of a headline like the one in the comic now?So more broadly, the comic may be referencing publication bias more generally. If only significant results are trumpeted, we won't hear about the dozens of groups that found nothing for green jellybeans, only the one that did.Indeed, that's one of the major points being made in this article, which has been in the news in the last few months (e.g. here, even though it's a 2005 article).A response to that article emphasizes the need for replication. Note that if there were to be several replications of the study that was published, the ""Green jellybeans linked to acne"" result would be very unlikely to stand.(And indeed, the hover text for the comic makes a clever reference to the same point.)"
Difference between Random Forest and Extremely Randomized Trees,"
I understood that Random Forest and Extremely Randomized Trees differ in the sense that the splits of the trees in the Random Forest are deterministic whereas they are random in the case of an Extremely Randomized Trees (to be more accurate, the next split is the best split among random uniform splits in the selected variables for the current tree). But I don't fully understand the impact of this different splits in various situations.

How do they compare in terms of bias/variance ?
How do they compare in presence of irrelevant variables ?
How do they compare in presence of correlated variables ?

","['machine-learning', 'correlation', 'references', 'random-forest']","The Extra-(Randomized)-Trees (ET) article contains a bias-variance analysis.
In Fig. 6 (on page 16), you can see a comparison with multiple methods including RF
on six tests (tree classification and three regression).Both methods are about the same, with the ET being a bit worse when there is a high number of noisy features (in high dimensional data-sets).That said, provided the (perhaps manual) feature selection is near optimal, the performance is about the same, however, ET's can be computationally faster.From the article itself:The analysis of the algorithm and the determination of
the optimal value of K on several test problem variants have shown that the value is in
principle dependent on problem specifics, in particular the proportion of irrelevant attributes. [...]
The bias/variance
analysis has shown that Extra-Trees work by decreasing variance while at the same time
increasing bias. [...] When the randomization
is increased above the optimal level, variance decreases slightly while bias
increases often significantly.No silver bullet as always.Pierre Geurts, Damien Ernst, Louis Wehenkel. ""Extremely randomized trees"""
"Is the ""hybrid"" between Fisher and Neyman-Pearson approaches to statistical testing really an ""incoherent mishmash""?","
There exists a certain school of thought according to which the most widespread approach to statistical testing is a ""hybrid"" between two approaches: that of Fisher and that of Neyman-Pearson; these two approaches, the claim goes, are ""incompatible"" and hence the resulting ""hybrid"" is an ""incoherent mishmash"". I will provide a bibliography and some quotes below, but for now suffice it to say that there is a lot written about that in the wikipedia article on Statistical hypothesis testing. Here on CV, this point was repeatedly made by @Michael Lew (see here and here).
My question is: why are F and N-P approaches claimed to be incompatible and why is the hybrid claimed to be incoherent? Note that I read at least six anti-hybrid papers (see below), but still fail to understand the problem or the argument. Note also, that I am not suggesting to debate if F or N-P is a better approach; neither am I offering to discuss frequentist vs. Bayesian frameworks. Instead, the question is: accepting that both F and N-P are valid and meaningful approaches, what is so bad about their hybrid?

Here is how I understand the situation. Fisher's approach is to compute the $p$-value and take it as an evidence against the null hypothesis. The smaller the $p$, the more convincing the evidence. The researcher is supposed to combine this evidence with his background knowledge, decide if it is convincing enough, and proceed accordingly. (Note that Fisher's views changed over the years, but this is what he seems to have eventually converged to.) In contrast, Neyman-Pearson approach is to choose $\alpha$ ahead of time and then to check if $p\le\alpha$; if so, call it significant and reject the null hypothesis (here I omit large part of the N-P story that has no relevance for the current discussion). See also an excellent reply by @gung in When to use Fisher and Neyman-Pearson framework?
The hybrid approach is to compute the $p$-value, report it (implicitly assuming that the smaller the better), and also call the results significant if $p\le\alpha$ (usually $\alpha=0.05$) and nonsignificant otherwise. This is supposed to be incoherent. How can it be invalid to do two valid things simultaneously, beats me.
As particularly incoherent the anti-hybridists view the widespread practice of reporting $p$-values as $p<0.05$, $p<0.01$, or $p<0.001$ (or even $p\ll0.0001$), where always the strongest inequality is chosen. The argument seems to be that (a) the strength of evidence cannot be properly assessed as exact $p$ is not reported, and (b) people tend to interpret the right-hand number in the inequality as $\alpha$ and view it as type I error rate, and that is wrong. I fail to see a big problem here. First, reporting exact $p$ is certainly a better practice, but nobody really cares if $p$ is e.g. $0.02$ or $0.03$, so rounding it on a log scale is not soooo bad (and going below $\sim 0.0001$ does not make sense anyway, see How should tiny p-values be reported?). Second, if the consensus is to call everything below $0.05$ significant, then error rate will be $\alpha=0.05$ and $p \ne \alpha$, as @gung explains in Interpretation of p-value in hypothesis testing. Even though this is potentially a confusing issue, it does not strike me as more confusing than other issues in statistical testing (outside of the hybrid). Also, every reader can have her own favourite $\alpha$ in mind when reading a hybrid paper, and her own error rate as a consequence. So what is the big deal?
One of the reasons I want to ask this question is because it literally hurts to see how much of the wikipedia article on Statistical hypothesis testing is devoted to lambasting hybrid. Following Halpin & Stam, it claims that a a certain Lindquist is to blame (there is even a large scan of his textbook with ""errors"" highlighted in yellow), and of course the wiki article about Lindquist himself starts with the same accusation. But then, maybe I am missing something.

References

Gigerenzer, 1993, The superego, the ego, and the id in statistical reasoning -- introduced the term ""hybrid"" and called it ""incoherent mishmash""

See also more recent expositions by Gigerenzer et al.: e.g. Mindless statistics (2004) and The Null Ritual. What You Always Wanted to Know About
Significance Testing but Were Afraid to Ask (2004).

Cohen, 1994, The Earth Is Round ($p<.05$) -- a very popular paper with almost 3k citations, mostly about different issues but favourably citing Gigerenzer
Goodman, 1999, Toward evidence-based medical statistics. 1: The P value fallacy
Hubbard & Bayarri, 2003, Confusion over measures of evidence ($p$'s) versus errors ($\alpha$'s) in classical statistical testing -- one of the more eloquent papers arguing against ""hybrid""
Halpin & Stam, 2006, Inductive Inference or Inductive Behavior: Fisher and Neyman-Pearson Approaches to Statistical Testing in Psychological Research (1940-1960) [free after registration] -- blames Lindquist's 1940 textbook for introducing the ""hybrid"" approach
@Michael Lew, 2006, Bad statistical practice in pharmacology (and other basic biomedical disciplines): you probably don't know P -- a nice review and overview

Quotes

Gigerenzer: What has become institutionalized as inferential statistics in psychology is not Fisherian statistics. It is an incoherent mishmash of some of Fisher's ideas on one hand, and some of the ideas of Neyman and E. S. Pearson on the other. I refer to this blend as the ""hybrid logic"" of statistical inference. 
Goodman: The [Neyman-Pearson] hypothesis test approach offered scientists a
  Faustian bargain -- a seemingly automatic way to limit the number of mistaken conclusions in the long run, but only by abandoning the ability to measure evidence [a la Fisher] and assess truth from a single experiment. 
Hubbard & Bayarri: Classical statistical testing is an anonymous hybrid of the competing and frequently contradictory approaches [...]. In particular, there is a widespread failure to appreciate the incompatibility of Fisher's evidential $p$ value with the Type I error rate, $\alpha$, of Neyman-Pearson statistical orthodoxy. [...] As a prime example of the bewilderment arising from [this] mixing [...], consider the widely unappreciated fact that the former's $p$ value is incompatible with the Neyman-Pearson hypothesis test in which it has become embedded. [...] For example, Gibbons and Pratt [...] erroneously stated: ""Reporting a P-value, whether exact or within an interval, in effect permits each individual to choose his own level of significance as the maximum tolerable probability of a Type I error."" 
Halpin & Stam: Lindquist's 1940 text was an original source of the hybridization of the Fisher and Neyman-Pearson approaches. [...] rather than adhering to any particular interpretation of statistical testing, psychologists have remained ambivalent about, and indeed largely unaware of, the conceptual difficulties implicated by the Fisher and Neyman-Pearson controversy.
Lew: What we have is a hybrid approach that neither controls error rates nor allows assessment of the strength of evidence. 

","['hypothesis-testing', 'statistical-significance', 'p-value', 'type-i-and-ii-errors', 'history']",
Correlations between continuous and categorical (nominal) variables,"
I would like to find the correlation between a continuous (dependent variable) and a categorical (nominal: gender, independent variable) variable. Continuous data is not normally distributed. Before, I had computed it using the Spearman's $\rho$. However, I have been told that it is not right.
While searching on the internet, I found that the boxplot can provide an idea about how much they are associated; however, I was looking for a quantified value such as Pearson's product moment coefficient or Spearman's $\rho$. Can you please help me on how to do this? Or, inform on which method would be appropriate?
Would Point Biserial Coefficient be the right option?
","['correlation', 'categorical-data', 'descriptive-statistics', 'biostatistics', 'spearman-rho']","The reviewer should have told you why the Spearman $\rho$ is not appropriate.  Here is one version of that:  Let the data be $(Z_i, I_i)$ where $Z$ is the measured variable and $I$ is the gender indicator, say it is 0 (man), 1 (woman). Then Spearman's $\rho$ is calculated based on the ranks of $Z, I$ respectively. Since there are only two possible values for the indicator $I$, there will be a lot of ties, so this formula is not appropriate. If you replace rank with mean rank,  then you will get only two different values, one for men, another for women.  Then $\rho$ will become basically some rescaled version of the mean ranks between the two groups. It would be simpler (more interpretable) to simply compare the means!   Another approach is the following.Let $X_1, \dots, X_n$ be the observations of the continuous variable among men, $Y_1, \dots, Y_m$ same among women. Now, if the distribution of $X$ and of $Y$ are the same, then $P(X>Y)$ will be 0.5 (let's assume the distribution is purely absolutely continuous, so there are no ties). In the general case, define
$$
   \theta = P(X>Y)
$$
where $X$ is a random draw among men, $Y$ among women. Can we estimate $\theta$ from our sample? Form all pairs $(X_i, Y_j)$ (assume no ties) and count for how many we have  ""man is larger"" ($X_i > Y_j$)($M$) and for how many ""woman is larger""  ($ X_i < Y_j$) ($W$).  Then one sample estimate of $\theta$ is
$$
  \frac{M}{M+W}
$$
That is one reasonable measure of correlation!  (If there are only a few ties, just ignore them).  But I am not sure what that is called,  if it has a name. 
This one may be close:       https://en.wikipedia.org/wiki/Goodman_and_Kruskal%27s_gamma"
What does the inverse of covariance matrix say about data? (Intuitively),"
I'm curious about the nature of $\Sigma^{-1}$. Can anybody tell something intuitive about ""What does $\Sigma^{-1}$ say about data?""
Edit:
Thanks for replies
After taking some great courses, I'd like to add some points:

It is measure of information, i.e., $x^T\Sigma^{-1}x$ is amount of info along the direction $x$.
Duality: Since $\Sigma$ is positive definite, so is $\Sigma^{-1}$, so they are dot-product norms, more precisely they are dual norms of each other, so we can derive Fenchel dual for the regularized least squares problem, and do maximization w.r.t dual problem. We can choose either of them, depending on their conditioning.
Hilbert space: Columns (and rows) of $\Sigma^{-1}$ and $\Sigma$ span the same space. So there is not any advantage (other that when one of these matrices is ill-conditioned) between representation with $\Sigma^{-1}$ or $\Sigma$
Bayesian Statistics: norm of $\Sigma^{-1}$ plays an important role in the Bayesian statistics. I.e. it determined how much information we have in prior, e.g., when covariance of the prior density is like $\|\Sigma^{-1}\|\rightarrow 0 $  we have non-informative (or probably Jeffreys prior)
Frequentist Statistics: It is closely related to Fisher information, using the Cramér–Rao bound. In fact, fisher information matrix (outer product of gradient of log-likelihood with itself) is Cramér–Rao bound it, i.e. $\Sigma^{-1}\preceq \mathcal{F}$ (w.r.t positive semi-definite cone, i.e. w.r.t. concentration ellipsoids). So when $\Sigma^{-1}=\mathcal{F}$ the maximum likelihood estimator is efficient, i.e. maximum information exist in the data, so frequentist regime is optimal. In simpler words, for some likelihood functions (note that functional form of the likelihood purely depend on the probablistic model which supposedly generated data, aka generative model), maximum likelihood is efficient and consistent estimator, rules like a boss. (sorry for overkilling it)

","['bayesian', 'maximum-likelihood', 'covariance', 'matrix']","It is a measure of precision just as $\Sigma$ is a measure of dispersion. More elaborately, $\Sigma$ is a measure of how the variables are dispersed around the mean (the diagonal elements) and how they co-vary with other variables (the off-diagonal) elements. The more the dispersion the farther apart they are from the mean and the more they co-vary (in absolute value) with the other variables the stronger is the tendency for them to 'move together' (in the same or opposite direction depending on the sign of the covariance). Similarly,  $\Sigma^{-1}$ is a measure of how tightly clustered the variables are around the mean (the diagonal elements) and the extent to which they do not co-vary with the other variables (the off-diagonal elements). Thus, the higher the diagonal element, the tighter the variable is clustered around the mean. The interpretation of the off-diagonal elements is more subtle and I refer you to the other answers for that interpretation."
Most famous statisticians,"
What are the most important statisticians, and what is it that made them famous?
(Reply just one scientist per answer please.)
","['methodology', 'history']",Reverend Thomas Bayes for discovering Bayes' theorem
How can I determine which of two sequences of coin flips is real and which is fake?,"
This is an interesting problem I came across. I'm attempting to write a Python program to get a solution to it; however, I'm not sure how to proceed. So far, I know that I would expect the counts of heads to follow a binomial, and length of runs (of tails, heads, or both) to follow a geometric.

Below are two sequences of 300 “coin flips” (H for heads, T for
tails). One of these is a true sequence of 300 independent flips of a
fair coin. The other was generated by a person typing out H’s and T’s
and trying to seem random. Which sequence is truly composed of coin
flips?
Sequence 1:
TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHH
TTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHH
TTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHT
THHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHT
HTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTT
HHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT
Sequence 2:
HTHHHTHTTHHTTTTTTTTHHHTTTHHTTTTHHTTHHHTTHTHTTTTTTH
THTTTTHHHHTHTHTTHTTTHTTHTTTTHTHHTHHHHTTTTTHHHHTHHH
TTTTHTHTTHHHHTHHHHHHHHTTHHTHHTHHHHHHHTTHTHTTTHHTTT
THTHHTTHTTHTHTHTTHHHHHTTHTTTHTHTHHTTTTHTTTTTHHTHTH
HHHTTTTHTHHHTHHTHTHTHTHHHTHTTHHHTHHHHHHTHHHTHTTTHH
HTTTHHTHTTHHTHHHTHTTHTTHTTTHHTHTHTTTTHTHTHTTHTHTHT
Both sequences have 148 heads, two
less than the expected number for a 0.5 probability of heads.

",['binomial-distribution'],"This is a variant on a standard intro stats demonstration: for homework after the first class I have assigned my students the exercise of flipping a coin 100 times and recording the results, broadly hinting that they don't really have to flip a coin and assuring them it won't be graded.  Most will eschew the physical process and just write down 100 H's and T's willy-nilly.  After the results are handed in at the beginning of the next class, at a glance I can reliably identify the ones who cheated.  Usually there are no runs of heads or tails longer than about 4 or 5, even though in just 100 flips we ought to see a longer run that that.This case is subtler, but one particular analysis stands out as convincing: tabulate the successive ordered pairs of results.  In a series of independent flips, each of the four possible pairs HH, HT, TH, and TT should occur equally often--which would be $(300-1)/4 = 74.75$ times each, on average.Here are the tabulations for the two series of flips:The first is obviously far from what we might expect.  In that series, an H is more than twice as likely ($102:46$) to be followed by a T than by another H; and a T, in turn, is more than twice as likely ($102:49$) to be followed by an H.  In the second series, those likelihoods are nearly $1:1,$ consistent with independent flips.A chi-squared test works well here, because all the expected counts are far greater than the threshold of 5 often quoted as a minimum.  The chi-squared statistics are 38.3 and 0.085, respectively, corresponding to p-values of less than one in a billion and 77%, respectively.  In other words, a table of pairs as imbalanced as the second one is to be expected (due to the randomness), but a table as imbalanced as the first happens less than one in every billion such experiments.(NB: It has been pointed out in comments that the chi-squared test might not be applicable because these transitions are not independent: e.g., an HT can be followed only by a TT or TH.  This is a legitimate concern.  However, this form of dependence is extremely weak and has little appreciable effect on the null distribution of the chi-squared statistic for sequences as long as $300.$  In fact, the chi-squared distribution is a great approximation to the null sampling distribution even for sequences as short as $21,$ where the counts of the $21-1=20$ transitions that occur are expected to be $20/4=5$ of each type.)If you know nothing about chi-squared tests, or even if you do but don't want to program the chi-square quantile function to compute a p-value, you can achieve a similar result.  First develop a way to quantify the degree of imbalance in a $2\times 2$ table like this.  (There are many ways, but all the reasonable ones are equivalent.)  Then generate, say, a few hundred such tables randomly (by flipping coins--in the computer, of course!).  Compare the imbalances of these two tables to the range of imbalances generated randomly.  You will find the first sequence is far outside the range while the second is squarely within it.This figure summarizes such a simulation using the chi-squared statistic as the measure of imbalance.  Both panels show the same results: one on the original scale and the other on a log scale.  The two dashed vertical lines in each panel show the chi-squared statistics for Series 1 (right) and Series 2 (left).  The red curve is the $\chi^2(1)$ density.  It fits the simulations extremely well at the right (higher values).  The discrepancies for low values occur because this statistic has a discrete distribution which cannot be well approximated by any continuous distribution where it takes on small values -- but for our purposes that makes no difference at all."
Are bayesians slaves of the likelihood function?,"
In his book ""All of Statistics"", Prof. Larry Wasserman presents the following Example (11.10, page 188). Suppose that we have a density $f$ such that $f(x)=c\,g(x)$, where $g$ is a known (nonnegative, integrable) function, and the normalization constant $c>0$ is unknown.
We are interested in those cases where we can't compute $c=1/\int g(x)\,dx$. For example, it may be the case that $f$ is a pdf over a very high-dimensional sample space. 
It is well known that there are simulation techniques that allow us to sample from $f$, even though $c$ is unknown. Hence, the puzzle is: How could we estimate $c$ from such a sample?
Prof. Wasserman describes the following Bayesian solution: let $\pi$ be some prior for $c$. The likelihood is
$$
  L_x(c) = \prod_{i=1}^n f(x_i) = \prod_{i=1}^n \left(c\,g(x_i)\right) = c^n \prod_{i=1}^n g(x_i) \propto c^n \, .
$$
Therefore, the posterior 
$$
  \pi(c\mid x) \propto c^n \pi(c)
$$
does not depend on the sample values $x_1,\dots,x_n$. Hence, a Bayesian can't use the information contained in the sample to make inferences about $c$.
Prof. Wasserman points out that ""Bayesians are slaves of the likelihood function. When the likelihood goes awry, so will Bayesian inference"".
My question for my fellow stackers is: Regarding this particular example, what went wrong (if anything) with Bayesian methodology?
P.S. As Prof. Wasserman kindly explained in his answer, the example is due to Ed George.
","['bayesian', 'mathematical-statistics']","The proposed statistical model may be described as follows: You have a known nonnegative integrable function $g:\mathbb{R}\to\mathbb{R}$, and a nonnegative random variable $C$. The random variables $X_1,\dots,X_n$ are supposed to be conditionally independent and identically distributed, given that $C=c$, with conditional density $f_{X_i\mid C}(x_i\mid c)=c\,g(x_i)$, for $c>0$.Unfortunately, in general, this is not a valid description of a statistical model. The problem is that, by definition, $f_{X_i\mid C}(\,\cdot\mid c)$ must be a probability density for almost every possible value of $c$, which is, in general, clearly false. In fact, it is true just for the single value $c=\left(\int_{-\infty}^\infty g(x)\,dx\right)^{-1}$. Therefore, the model is correctly specified only in the trivial case when the distribution of $C$ is concentrated at this particular value. Of course, we are not interested in this case. What we want is the distribution of $C$ to be dominated by Lebesgue measure, having a nice pdf $\pi$.Hence, defining $x=(x_1,\dots,x_n)$, the expression
$$
  L_x(c) = \prod_{i=1}^n \left(c\,g(x_i)\right) \, ,
$$
taken as a function of $c$, for fixed $x$, does not correspond to a genuine likelihood function.Everything after that inherits from this problem. In particular, the posterior computed with Bayes's Theorem is bogus. It's easy to see that: suppose that you have a proper prior
$$
  \pi(c) = \frac{1}{c^2} \,I_{[1,\infty)}(c) \, .
$$
Note that $\int_0^\infty \pi(c)\,dc=1$. According to the computation presented in the example, the posterior should be
$$
  \pi(c\mid x) \propto \frac{1}{c^{2-n}}\, I_{[1,\infty)}(c) \, .
$$
But if that is right, this posterior would be always improper, because
$$
  \int_0^\infty \frac{1}{c^{2-n}}\,I_{[1,\infty)}(c)\,dc 
$$
diverges for every sample size $n\geq 1$. This is impossible: we know that if we start with a proper prior, our posterior can't be improper for every possible sample (it may be improper inside a set of null prior predictive probability)."
Why does collecting data until finding a significant result increase Type I error rate?,"
I was wondering exactly why collecting data until a significant result (e.g., $p \lt .05$) is obtained (i.e., p-hacking) increases the Type I error rate?
I would also highly appreciate an R demonstration of this phenomenon.
","['r', 'hypothesis-testing', 'p-value', 'simulation', 'type-i-and-ii-errors']","The problem is that you're giving yourself too many chances to pass the test.  It's just a fancy version of this dialog:I'll flip you to see who pays for dinner.OK, I call heads.Rats, you won.  Best two out of three?To understand this better, consider a simplified--but realistic--model of this sequential procedure.  Suppose you will start with a ""trial run"" of a certain number of observations, but are willing to continue experimenting longer in order to get a p-value less than $0.05$.  The null hypothesis is that each observation $X_i$ comes (independently) from a standard Normal distribution. The alternative is that the $X_i$ come independently from a unit-variance normal distribution with a nonzero mean. The test statistic will be the mean of all $n$ observations, $\bar X$, divided by their standard error, $1/\sqrt{n}$.  For a two-sided test, the critical values are the $0.025$ and $0.975$ percentage points of the standard Normal distribution, $ Z_\alpha=\pm 1.96$ approximately.This is a good test--for a single experiment with a fixed sample size $n$.  It has exactly a $5\%$ chance of rejecting the null hypothesis, no matter what $n$ might be.Let's algebraically convert this to an equivalent test based on the sum of all $n$ values, $$S_n=X_1+X_2+\cdots+X_n = n\bar X.$$Thus, the data are ""significant"" when$$\left| Z_\alpha\right| \le \left| \frac{\bar X}{1/\sqrt{n}} \right| = \left| \frac{S_n}{n/\sqrt{n}} \right|  = \left| S_n \right| / \sqrt{n};$$that is,$$\left| Z_\alpha\right| \sqrt{n} \le \left| S_n \right| .\tag{1}$$If we're smart, we'll cut our losses and give up once $n$ grows very large and the data still haven't entered the critical region.This describes a random walk $S_n$.  The formula $(1)$ amounts to erecting a curved parabolic ""fence,"" or barrier, around the plot of the random walk $(n, S_n)$: the result is ""significant"" if any point of the random walk hits the fence.It is a property of random walks that if we wait long enough, it's very likely that at some point the result will look significant. Here are 20 independent simulations out to a limit of $n=5000$ samples.  They all begin testing at $n=30$ samples, at which point we check whether the each point lies outside the barriers that have been drawn according to formula $(1)$.  From the point at which the statistical test is first ""significant,"" the simulated data are colored red.You can see what's going on: the random walk whips up and down more and more as $n$ increases.  The barriers are spreading apart at about the same rate--but not fast enough always to avoid the random walk.In 20% of these simulations, a ""significant"" difference was found--usually quite early on--even though in every one of them the null hypothesis is absolutely correct!  Running more simulations of this type indicates that the true test size is close to $25\%$ rather than the intended value of $\alpha=5\%$: that is, your willingness to keep looking for ""significance"" up to a sample size of $5000$ gives you a $25\%$ chance of rejecting the null even when the null is true.Notice that in all four ""significant"" cases, as testing continued, the data stopped looking significant at some points.  In real life, an experimenter who stops early is losing the chance to observe such ""reversions.""  This selectiveness through optional stopping biases the results.In honest-to-goodness sequential tests, the barriers are lines.  They spread faster than the curved barriers shown here."
Clustering with K-Means and EM: how are they related?,"
I have studied algorithms for clustering data (unsupervised learning): EM, and k-means. 
I keep reading the following : 

k-means is a variant of EM, with the assumptions that clusters are
  spherical.

Can somebody explain the above sentence? I do not understand what spherical means, and how kmeans and EM are related, since one does probabilistic assignment and the other does it in a deterministic way. 
Also, in which situation is it better to use k-means clustering? or use EM clustering? 
","['machine-learning', 'clustering', 'data-mining', 'k-means', 'expectation-maximization']",K means EM
What are alternatives of Gradient Descent?,"
Gradient Descent has a problem of getting stuck in Local Minima. We need to run gradient descent exponential times in order to find global minima.
Can anybody tell me about any alternatives of gradient descent as applied in neural network learning, along with their pros and cons.
","['machine-learning', 'svm', 'neural-networks']","This is more a problem to do with the function being minimized than the method used, if finding the true global minimum is important, then use a method such a simulated annealing.  This will be able to find the global minimum, but may take a very long time to do so.In the case of neural nets, local minima are not necessarily that much of a problem.  Some of the local minima are due to the fact that you can get a functionally identical model by permuting the hidden layer units, or negating the inputs and output weights of the network etc.  Also if the local minima is only slightly non-optimal, then the difference in performance will be minimal and so it won't really matter.  Lastly, and this is an important point, the key problem in fitting a neural network is over-fitting, so aggressively searching for the global minima of the cost function is likely to result in overfitting and a model that performs poorly.Adding a regularisation term, e.g. weight decay, can help to smooth out the cost function, which can reduce the problem of local minima a little, and is something I would recommend anyway as a means of avoiding overfitting.The best method however of avoiding local minima in neural networks is to use a Gaussian Process model (or a Radial Basis Function neural network), which have fewer problems with local minima."
Cross-Entropy or Log Likelihood in Output layer,"
I read this page:
http://neuralnetworksanddeeplearning.com/chap3.html
and it said that sigmoid output layer with cross-entropy is quite similiar with softmax output layer with log-likelihood.
what happen if I use sigmoid with log-likelihood or softmax with cross entropy in the output layer? is it fine? becuase I see there's only little difference in equation between cross entropy (eq.57):
$$C = -\frac{1}{n} \sum\limits_x (y \ln a + (1-y) \ln (1-a))$$
and log likelihood (eq.80):
$$C =-\frac{1}{n} \sum\limits_x(\ln a^L_y)$$
","['neural-networks', 'maximum-likelihood', 'softmax']","The negative log likelihood (eq.80) is also known as the multiclass cross-entropy (ref: Pattern Recognition and Machine Learning Section 4.3.4), as they are in fact two different interpretations of the same formula.eq.57 is the negative log likelihood of the Bernoulli distribution, whereas eq.80 is the negative log likelihood of the multinomial distribution with one observation (a multiclass version of Bernoulli). For binary classification problems, the softmax function outputs two values (between 0 and 1 and sum to 1) to give the prediction of each class. While the sigmoid function outputs one value (between 0 and 1) to give the prediction of one class (so the other class is 1-p).  So eq.80 can't be directly applied to the sigmoid output, though it is essentially the same loss as eq.57.Also see this answer.Following is a simple illustration of the connection between (sigmoid + binary cross-entropy) and (softmax + multiclass cross-entropy) for binary classification problems. Say we take $0.5$ as the split point of the two categories, for sigmoid
output it follows,$$\sigma(wx+b)=0.5$$
$$wx+b=0$$
which is the decision boundary in the feature space.For softmax output it follows
$$\frac{e^{w_1x+b_1}}{e^{w_1x+b_1}+e^{w_2x+b_2}}=0.5$$
$$e^{w_1x+b_1}=e^{w_2x+b_2}$$
$$w_1x+b_1=w_2x+b_2$$
$$(w_1-w_2)x+(b_1-b_2)=0$$
so it remains the same model although there are twice as many parameters.The followings show the decision boundaries obtained using theses two methods, which are almost identical.
"
"What does having ""constant variance"" in a linear regression model mean?","
What does having ""constant variance"" in the error term mean? As I see it, we have a data with one dependent variable and one independent variable. Constant variance is one of the assumptions of linear regression. I am wondering what homoscedasticity means. Since even if I have 500 rows, I would have a single variance value which is obviously constant. With what variable should I compare the variance?  
","['regression', 'heteroscedasticity']","It means that when you plot the individual error against the predicted value, the variance of the error predicted value should be constant. See the red arrows in the picture below, the length of the red lines (a proxy of its variance) are the same."
Does it ever make sense to treat categorical data as continuous?,"
In answering this question on discrete and continuous data I glibly asserted that it rarely makes sense to treat categorical data as continuous.
On the face of it that seems self-evident, but intuition is often a poor guide for statistics, or at least mine is. So now I'm wondering: is it true? Or are there established analyses for which a transform from categorical data to some continuum is actually useful? Would it make a difference if the data were ordinal?
","['categorical-data', 'data-transformation', 'ordinal-data', 'continuous-data']","I will assume that a ""categorical"" variable actually stands for an ordinal variable; otherwise it doesn't make much sense to treat it as a continuous one, unless it's a binary variable (coded 0/1) as pointed by @Rob. Then, I would say that the problem is not that much the way we treat the variable, although many models for categorical data analysis have been developed so far--see e.g., The analysis of ordered categorical data: An overview and a survey of recent developments from Liu and Agresti--, than the underlying measurement scale we assume. My response will focus on this second point, although I will first briefly discuss the assignment of numerical scores to variable categories or levels.By using a simple numerical recoding of an ordinal variable, you are assuming that the variable has interval properties (in the sense of the classification given by Stevens, 1946). From a measurement theory perspective (in psychology), this may often be a too strong assumption, but for basic study (i.e. where a single item is used to express one's opinion about a daily activity with clear wording) any monotone scores should give comparable results. Cochran (1954) already pointed thatany set of scores gives a valid
test, provided that they are
constructed without consulting the
results of the experiment. If the set
of scores is poor, in that it badly
distorts a numerical scale that really
does underlie the ordered
classification, the test will not be
sensitive. The scores should therefore
embody the best insight available
about the way in which the
classification was constructed and
used. (p. 436)(Many thanks to @whuber for reminding me about this throughout one of his comments, which led me to re-read Agresti's book, from which this citation comes.)Actually, several tests treat implicitly such variables as interval scales: for example, the $M^2$ statistic for testing a linear trend (as an alternative to simple independence) is based on a correlational approach ($M^2=(n-1)r^2$, Agresti, 2002, p. 87).Well, you can also decide to recode your variable on an irregular range, or aggregate some of its levels, but in this case strong imbalance between recoded categories may distort statistical tests, e.g. the aforementioned trend test.
A nice alternative for assigning distance between categories was already proposed by @Jeromy, namely optimal scaling.Now, let's discuss the second point I made, that of the underlying measurement model. I'm always hesitating about adding the ""psychometrics"" tag when I see this kind of question, because the construction and analysis of measurement scales come under Psychometric Theory (Nunnally and Bernstein, 1994, for a neat overview). I will not dwell on all the models that are actually headed under the Item Response Theory, and I kindly refer the interested reader to I. Partchev's tutorial, A visual guide to item response theory, for a gentle introduction to IRT, and to references (5-8) listed at the end for possible IRT taxonomies. Very briefly, the idea is that rather than assigning arbitrary distances between variable categories, you assume a latent scale and estimate their location on that continuum, together with individuals' ability or liability. A simple example is worth much mathematical notation, so let's consider the following item (coming from the EORTC QLQ-C30 health-related quality of life questionnaire):Did you worry?which is coded on a four-point scale, ranging from ""Not at all"" to ""Very much"". Raw scores are computed by assigning a score of 1 to 4. Scores on items belonging to the same scale can then be added together to yield a so-called scale score, which denotes one's rank on the underlying construct (here, a mental health component). Such summated scale scores are very practical because of scoring easiness (for the practitioner or nurse), but they are nothing more than a discrete (ordered) scale.We can also consider that the probability of endorsing a given response category obeys some kind of a logistic model, as described in I. Partchev's tutorial, referred above. Basically, the idea is that of a kind of threshold model (which lead to equivalent formulation in terms of the proportional or cumulative odds models) and we model the odds of being in one response category rather the preceding one or the odds of scoring above a certain category, conditional on subjects' location on the latent trait. In addition, we may impose that response categories are equally spaced on the latent scale (this is the Rating Scale model)--which is the way we do by assigning regularly spaced numerical scores-- or not (this is the Partial Credit model).Clearly, we are not adding very much to Classical Test Theory, where ordinal variable are treated as numerical ones. However, we introduce a probabilistic model, where we assume a continuous scale (with interval properties) and where specific errors of measurement can be accounted for, and we can plug these factorial scores in any regression model.References"
Training a decision tree against unbalanced data,"
I'm new to data mining and I'm trying to train a decision tree against a data set which is highly unbalanced. However, I'm having problems with poor predictive accuracy.
The data consists of students studying courses, and the class variable is the course status which has two values - Withdrawn or Current.

Age
Ethnicity
Gender
Course 
...
Course Status

In the data set there are many more instances which are Current than Withdrawn. Withdrawn instances only accounting for 2% of the total instances.
I want to be able to build a model which can predict the probability that a person will withdraw in the future. However when testing the model against the training data, the accuracy of the model is terrible.
I've had similar issues with decision trees where the data is dominated by one or two classes.
What approach can I use to solve this problem and build a more accurate classifier?
","['classification', 'cart', 'unbalanced-classes', 'accuracy']","This is an interesting and very frequent problem in classification - not just in decision trees but in virtually all classification algorithms.As you found empirically, a training set consisting of different numbers of representatives from either class may result in a classifier that is biased towards the majority class. When applied to a test set that is similarly imbalanced, this classifier yields an optimistic accuracy estimate. In an extreme case, the classifier might assign every single test case to the majority class, thereby achieving an accuracy equal to the proportion of test cases belonging to the majority class. This is a well-known phenomenon in binary classification (and it extends naturally to multi-class settings).This is an important issue, because an imbalanced dataset may lead to inflated performance estimates. This in turn may lead to false conclusions about the significance with which the algorithm has performed better than chance.The machine-learning literature on this topic has essentially developed three solution strategies.You can restore balance on the training set by undersampling the large class or by oversampling the small class, to prevent bias from arising in the first place.Alternatively, you can modify the costs of misclassification, as noted in a previous response, again to prevent bias.An additional safeguard is to replace the accuracy by the so-called balanced accuracy. It is defined as the arithmetic mean of the class-specific accuracies, $\phi := \frac{1}{2}\left(\pi^+ + \pi^-\right),$ where $\pi^+$ and $\pi^-$ represent the accuracy obtained on positive and negative examples, respectively. If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions). In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to chance (see sketch below).I would recommend to consider at least two of the above approaches in conjunction. For example, you could oversample your minority class to prevent your classifier from acquiring a bias in favour the majority class. Following this, when evaluating the performance of your classifier, you could replace the accuracy by the balanced accuracy. The two approaches are complementary. When applied together, they should help you both prevent your original problem and avoid false conclusions following from it.I would be happy to post some additional references to the literature if you would like to follow up on this."
What is the variance of the weighted mixture of two gaussians?,"
Say I have two normal distributions A and B with means $\mu_A$ and $\mu_B$ and variances $\sigma_A$ and $\sigma_B$. I want to take a weighted mixture of these two distributions using weights $p$ and $q$ where $0\le p \le 1$ and $q = 1-p$. I know that the mean of this mixture would be $\mu_{AB} = (p\times\mu_A) + (q\times\mu_B)$.
What would the variance be?

A concrete example would be if I knew the parameters for the distribution of male and female height. If I had a room of people that was 60% male, I could produce the expected mean height for the whole room, but what about the variance?
","['normal-distribution', 'mixture-distribution']","The variance is the second moment minus the square of the first moment, so it suffices to compute moments of mixtures.In general, given distributions with PDFs $f_i$ and constant (non-random) weights $p_i$, the PDF of the mixture is$$f(x) = \sum_i{p_i f_i(x)},$$from which it follows immediately for any moment $k$ that$$\mu^{(k)} = \mathbb{E}_{f}[x^k] = \sum_i{p_i \mathbb{E}_{f_i}[x^k]} = \sum_i{p_i \mu_i^{(k)}}.$$I have written $\mu^{(k)}$ for the $k^{th}$ moment of $f$ and  $\mu_i^{(k)}$ for the $k^{th}$ moment of $f_i$.Using these formulae, the variance can be written$$\text{Var}(f) = \mu^{(2)} - \left(\mu^{(1)}\right)^2 = \sum_i{p_i \mu_i^{(2)}} - \left(\sum_i{p_i \mu_i^{(1)}}\right)^2.$$Equivalently, if the variances of the $f_i$ are given as $\sigma^2_i$, then $\mu^{(2)}_i = \sigma^2_i + \left(\mu^{(1)}_i\right)^2$, enabling the variance of the mixture $f$ to be written in terms of the variances and means of its components as$$\eqalign{
\text{Var}(f) &= \sum_i{p_i \left(\sigma^2_i + \left(\mu^{(1)}_i\right)^2\right)} - \left(\sum_i{p_i \mu_i^{(1)}}\right)^2 \\
&= \sum_i{p_i \sigma^2_i} + \sum_i{p_i\left(\mu_i^{(1)}\right)^2} - \left(\sum_{i}{p_i \mu_i^{(1)}}\right)^2.
}$$In words, this is the (weighted) average variance plus the average squared mean minus the square of the average mean.  Because squaring is a convex function, Jensen's Inequality asserts that the average squared mean can be no less than the square of the average mean.  This allows us to understand the formula as stating the variance of the mixture is the mixture of the variances plus a non-negative term accounting for the (weighted) dispersion of the means.In your case the variance is$$p_A  \sigma_A^2 + p_B \sigma_B^2 + \left[p_A\mu_A^2 + p_B\mu_B^2 -  (p_A \mu_A + p_B \mu_B)^2\right].$$We can interpret this is a weighted mixture of the two variances, $p_A\sigma_A^2 + p_B\sigma_B^2$, plus a (necessarily positive) correction term to account for the shifts from the individual means relative to the overall mixture mean.The utility of this variance in interpreting data, such as given in the question, is doubtful, because the mixture distribution will not be Normal (and may depart substantially from it, to the extent of exhibiting bimodality)."
Recommended books on experiment design?,"
What are the panel's recommendations for books on design of experiments?
Ideally, books should be still in print or available electronically, although that may not always be feasible. If you feel moved to add a few words on what's so good about the book that would be great too.
Also, aim for one book per answer so that voting can help sort the suggestions.
(Community Wiki, please edit the question if you can make it better!)
","['references', 'experiment-design']",
Binary classification with strongly unbalanced classes,"
I have a data set in the form of (features, binary output 0 or 1), but 1 happens pretty rarely, so just by always predicting 0, I get accuracy between 70% and 90% (depending on the particular data I look at). The ML methods give me about the same accuracy, and I feel, there should be some standard methods to apply in this situation, that would improve the  accuracy over the obvious prediction rule. 
","['machine-learning', 'classification', 'binary-data', 'unbalanced-classes']","Both hxd1011 and Frank are right (+1). 
Essentially resampling and/or cost-sensitive learning are the two main ways of getting around the problem of imbalanced data; third is to use kernel methods that sometimes might be less effected by the class imbalance.
Let me stress that there is no silver-bullet solution. By definition you have one class that is represented inadequately in your samples. Having said the above I believe that you will find the algorithms SMOTE and ROSE very helpful. SMOTE effectively uses a $k$-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. ROSE tries to create estimates of the underlying distributions of the two classes using a smoothed bootstrap approach and sample them for synthetic examples. Both are readily available in R, SMOTE in the package DMwR and ROSE in the package with the same name. Both SMOTE and ROSE result in a training dataset that is smaller than the original one.I would probably argue that a better (or less bad) metric for the case of imbalanced data is using Cohen's $k$ and/or Receiver operating characteristic's Area under the curve.
Cohen's kappa directly controls for the expected accuracy, AUC as it
is a function of sensitivity and specificity, the curve is insensitive to disparities in the class proportions. Again, notice that these are just metrics that should be used with a large grain of salt. You should ideally adapt them to your specific problem taking account of the gains and costs correct and wrong classifications convey in your case. I have found that looking at lift-curves is actually rather informative for this matter. 
Irrespective of your metric you should try to use a separate test to assess the performance of your algorithm; exactly because of the class imbalanced over-fitting is even more likely so out-of-sample testing is crucial.Probably the most popular recent paper on the matter is Learning from Imbalanced Data by He and Garcia. It gives a very nice overview of the points raised by myself and in other answers. In addition I believe that the walk-through on Subsampling For Class Imbalances, presented by Max Kuhn as part of the caret package is an excellent resource to get a structure example of how under-/over-sampling as well as synthetic data creation can measure against each other."
"List of situations where a Bayesian approach is simpler, more practical, or more convenient","
There have been many debates within statistics between Bayesians and frequentists.  I generally find these rather off-putting (although I think it has died down).  On the other hand, I've met several people who take an entirely pragmatic view of the issue, saying that sometimes it is more convenient to conduct a frequentist analysis and sometimes it's easier to run a Bayesian analysis.  I find this perspective practical and refreshing.  
It occurs to me that it would be helpful to have a list of such cases.  Because there are too many statistical analyses, and because I assume that it is ordinarily more practical to conduct a frequentist analysis (coding a t-test in WinBUGS is considerably more involved than the single function call required to perform the frequentist-based version in R, for example), it would be nice to have a list of the situations where a Bayesian approach is simpler, more practical, and / or more convenient than a frequentist approach. 

(Two answers that I have no interest in are: 'always', and 'never'.  I understand people have strong opinions, but please don't air them here.  If this thread becomes a venue for petty squabbling, I will probably delete it.  My goal here is to develop a resource that will be useful for an analyst with a job to do, not an axe to grind.) 
People are welcome to suggest more than one case, but please use separate answers to do so, so that each situation can be evaluated (voted / discussed) individually.  Answers should list: (1) what the nature of the situation is, and (2) why the Bayesian approach is simpler in this case.  Some code (say, in WinBUGS) demonstrating how the analysis would be done and why the Bayesian version is more practical would be ideal, but I expect will be too cumbersome.  If it can be done easily I would appreciate it, but please include why either way.  
Finally, I recognize that I have not defined what it means for one approach to be 'simpler' than another.  The truth is, I'm not entirely sure what it should mean for one approach to be more practical than the other.  I'm open to different suggestions, just specify your interpretation when you explain why a Bayesian analysis is more convenient in the situation you discuss.  
","['bayesian', 'frequentist']","(1) In contexts where the likelihood function is intractable (at least numerically), the use of the Bayesian approach, by means of Approximate Bayesian Computation (ABC), has gained ground over some frequentist competitors such as composite likelihoods (1, 2) or the empirical likelihood because it tends to be easier to implement (not necessarily correct). Due to this, the use of ABC has become popular in areas where it is common to come across intractable likelihoods such as biology, genetics, and ecology. Here, we could mention an ocean of examples.Some examples of intractable likelihoods are Superposed processes. Cox and Smith (1954) proposed a model in the context of neurophysiology which consists of $N$ superposed point processes. For example consider the times between the electrical pulses observed at some part of the brain that were emited by several neurones during a certain period. This sample contains non iid observations which makes difficult to construct the corresponding likelihood, complicating the estimation of the corresponding parameters. A (partial)frequentist solution was recently proposed in this paper. The implementation of the ABC approach has also been recently studied and it can be found here.Population genetics is another example of models leading to intractable likelihoods. In this case the intractability has a different nature: the likelihood is expressed in terms of a multidimensional integral (sometimes of dimension $1000+$) which would take a couple of decades just to evaluate it at a single point. This area is probably ABC's headquarters. "
How to derive variance-covariance matrix of coefficients in linear regression,"
I am reading a book on linear regression and have some trouble understanding the variance-covariance matrix of $\mathbf{b}$:

The diagonal items are easy enough, but the off-diagonal ones are a bit more difficult, what puzzles me is that 
$$
\sigma(b_0, b_1) = E(b_0 b_1) - E(b_0)E(b_1) = E(b_0 b_1) - \beta_0 \beta_1
$$
but there is no trace of $\beta_0$ and $\beta_1$ here.
",['regression'],
How do I test that two continuous variables are independent?,"
Suppose I have a sample $(X_n,Y_n), n=1..N$ from the joint distribution of $X$ and $Y$. How do I test the hypothesis that $X$ and $Y$ are independent? 
No assumption is made on the joint or marginal distribution laws of $X$ and $Y$ (least of all joint normality, since in that case independence is identical to correlation being $0$).
No assumption is made on the nature of a possible relationship between $X$ and $Y$; it may be non-linear, so the variables are uncorrelated ($r=0$) but highly co-dependent ($I=H$).
I can see two approaches:

Bin both variables and use Fisher's exact test or G-test.

Pro: use well-established statistical tests
Con: depends on binning

Estimate the dependency of $X$ and $Y$: $\frac{I(X;Y)}{H(X,Y)}$ (this is $0$ for independent $X$ and $Y$ and $1$ when they completely determine each other).

Pro: produces a number with a clear theoretical meaning
Con: depends on the approximate entropy computation (i.e., binning again)


Do these approaches make sense?
What other methods people use?
","['hypothesis-testing', 'references', 'independence']","(Answer partially updated in 2023.)This is a very hard problem in general, though your variables are apparently only 1d so that helps. Of course, the first step (when possible) should be to plot the data and see if anything pops out at you; you're in 2d so this should be easy.Here are a few approaches that work in $\mathbb{R}^d$ or even more general settings, to match the general title of the question.One general category is, related to the suggestion here, to estimate the mutual information:Sricharan, Raich, and Hero. Empirical estimation of entropy functionals with confidence. arXiv:1012.4188 [math.ST]Pál, Póczos, and Svepesári. Estimation of Rényi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs, NeurIPS 2010.Poole, Ozair, van den Oord, Alemi, and Tucker. On Variational Bounds of Mutual Information, ICML 2019.There are also other approaches, based on measures other than the mutual information.Schweizer and Wolff, On Nonparametric Measures of Dependence for Random Variables, Annals of Statistics 1981.Gretton, Bousqet, Smola, and Schölkopf, Measuring Statistical Independence with Hilbert-Schmidt Norms, Algorithmic Learning Theory 2005.Székely, Rizzo, and Bakirov, Measuring and testing dependence by correlation of distances, Annals of Statistics 2007."
Best practice when analysing pre-post treatment-control designs,"
Imagine the following common design:

100 participants are randomly allocated to either a treatment or a control group
the dependent variable is numeric and measured pre- and post- treatment

Three obvious options for analysing such data are:

Test the group by time interaction effect in mixed ANOVA
Do an ANCOVA with condition as the IV and the pre- measure as the covariate and post measure as the DV
Do a t-test with condition as the IV and pre-post change scores as the DV

Question: 

What is the best way to analyse such data? 
Are there reasons to prefer one approach over another?

","['ancova', 'clinical-trials', 'pre-post-comparison', 'faq']","There is a huge literature around this topic (change/gain scores), and I think the best references come from the biomedical domain, e.g.Senn, S (2007). Statistical issues in
drug development. Wiley (chap. 7 pp.
96-112)In biomedical research, interesting work has also been done in the study of cross-over trials (esp. in relation to carry-over effects, although I don't know how applicable it is to your study).From Gain Score t to ANCOVA F (and vice versa), from Knapp & Schaffer, provides an interesting review of ANCOVA vs. t approach (the so-called Lord's Paradox). The simple analysis of change scores is not the recommended way for pre/post design according to Senn  in his article Change from baseline and analysis of covariance revisited (Stat. Med. 2006 25(24)). Moreover, using a mixed-effects model (e.g. to account for the correlation between the two time points) is not better because you really need to use the ""pre"" measurement as a covariate to increase precision (through adjustment). Very briefly:I also like Ten Difference Score Myths from Edwards, although it focuses on difference scores in a different context; but here is an annotated bibliography on the analysis of pre-post change (unfortunately, it doesn't cover very recent work). Van Breukelen also compared ANOVA vs. ANCOVA in randomized and non-randomized setting, and his conclusions support the idea that ANCOVA is to be preferred, at least in randomized studies (which prevent from regression to the mean effect)."
Under what conditions should Likert scales be used as ordinal or interval data?,"
Many studies in the social sciences use Likert scales.  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?
","['ordinal-data', 'likert', 'scales', 'measurement']","Maybe too late but I add my answer anyway...It depends on what you intend to do with your data: If you are interested in showing that scores differ when considering different group of participants (gender, country, etc.), you may treat your scores as numeric values, provided they fulfill usual assumptions about variance (or shape) and sample size. If you are rather interested in highlighting how response patterns vary across subgroups, then you should consider item scores as discrete choice among a set of answer options and look for log-linear modeling, ordinal logistic regression, item-response models or any other statistical model that allows to cope with polytomous items.As a rule of thumb, one generally considers that having 11 distinct points on a scale is sufficient to approximate an interval scale (for interpretation purpose, see @xmjx's comment)). Likert items may be regarded as true ordinal scale, but they are often used as numeric and we can compute their mean or SD. This is often done in attitude surveys, although it is wise to report both mean/SD and % of response in, e.g. the two highest categories.When using summated scale scores (i.e., we add up score on each item to compute a ""total score""), usual statistics may be applied, but you have to keep in mind that you are now working with a latent variable so the underlying construct should make sense! In psychometrics, we generally check that (1) unidimensionnality of the scale holds, (2) scale reliability is sufficient. When comparing two such scale scores (for two different instruments), we might even consider using attenuated correlation measures instead of classical Pearson correlation coefficient. Classical textbooks include:
1. Nunnally, J.C. and Bernstein, I.H. (1994). Psychometric Theory (3rd ed.). McGraw-Hill Series in Psychology.
2. Streiner, D.L. and Norman, G.R. (2008). Health Measurement Scales. A practical guide to their development and use (4th ed.). Oxford.
3. Rao, C.R. and Sinharay, S., Eds. (2007). Handbook of Statistics, Vol. 26: Psychometrics. Elsevier Science B.V.
4. Dunn, G. (2000). Statistics in Psychiatry. Hodder Arnold.You may also have a look at Applications of latent trait and latent class models in the social sciences, from Rost & Langeheine, and W. Revelle's website on personality research.When validating a psychometric scale, it is important to look at so-called ceiling/floor effects (large asymmetry resulting from participants scoring at the lowest/highest response category), which may seriously impact on any statistics computed when treating them as numeric variable (e.g., country aggregation, t-test). This raises specific issues in cross-cultural studies since it is known that overall response distribution in attitude or health surveys differ from one country to the other (e.g. chinese people vs. those coming from western countries tend to highlight specific response pattern, the former having generally more extreme scores at the item level, see e.g. Song, X.-Y. (2007) Analysis of multisample structural equation models with applications to Quality of Life data, in Handbook of Latent Variable and Related Models, Lee, S.-Y. (Ed.), pp 279-302, North-Holland).More generally, you should look at the psychometric-related literature which makes extensive use of Likert items if you are interested with measurement issue. Various statistical models have been developed and are currently headed under the Item Response Theory framework."
A more definitive discussion of variable selection,"
Background
I'm doing clinical research in medicine and have taken several statistics courses. I've never published a paper using linear/logistic regression and would like to do variable selection correctly. Interpretability is important, so no fancy machine learning techniques. I've summarized my understanding of variable selection - would someone mind shedding light on any misconceptions? I found two (1) similar (2) CV posts to this one, but they didn't quite fully answer my concerns. Any thoughts would be much appreciated! I have 3 primary questions at the end.
Problem and Discussion
My typical regression/classification problem has 200-300 observations, an adverse event rate of 15% (if classification), and info on 25 out of 40 variables that have been claimed to have a ""statistically significant"" effect in the literature or make plausible sense by domain knowledge. 
I put ""statistically significant"" in quotes, because it seems like everyone and their mother uses stepwise regression, but Harrell (3) and Flom (4) don't appear to like it for a number of good reasons. This is further supported by a Gelman blog post discussion (5). It seems like the only real time that stepwise is acceptable is if this is truly exploratory analysis, or one is interested in prediction and has a cross-validation scheme involved. Especially since many medical comorbidities suffer from collinearity AND studies suffer from small sample size, my understanding is that there will be a lot of false positives in the literature; this also makes me less likely to trust the literature for potential variables to include.
Another popular approach is to use a series of univariate regressions/associations between predictors and independent variable as a starting point. below a particular threshold (say, p < 0.2). This seems incorrect or at least misleading for the reasons outlined in this StackExchange post (6).
Lastly, an automated approach that appears popular in machine learning is to use penalization like L1 (Lasso), L2 (Ridge), or L1+L2 combo (Elastic Net). My understanding is that these do not have the same easy interpretations as OLS or logistic regression.
Gelman + Hill propose the following:  
 
In my Stats course, I also recall using F tests or Analysis of Deviance to compare full and nested models to do model/variable selection variable by variable. This seems reasonable, but fitting sequential nested models systematically to find variables that cause largest drop in deviance per df seems like it could be easily automated (so I'm a bit concerned) and also seems like it suffers from problems of the order in which you test variable inclusion. My understanding is that this should also be supplemented by investigating multicollinearity and residual plots (residual vs. predicted).
Questions:

Is the Gelman summary the way to go? What would you add or change in his proposed strategy?
Aside from purely thinking about potential interactions and transformations (which seems very bias/error/omission prone), is there another way to discover potential ones? Multivariate adaptive regression spline (MARS) was recommended to me, but I was informed that the nonlinearities/transformations don't translate into the same variables in a standard regression model.
Suppose my goal is very simple: say, ""I'd like to estimate the association of X1 on Y, only accounting for X2"". Is it adequate to simply regress Y ~ X1 + X2, report the outcome, without reference to actual predictive ability (as might be measured by cross-validation RMSE or accuracy measures)? Does this change depending on event rate or sample size or if R^2 is super low (I'm aware that R^2 is not good because you can always increase it by overfitting)? I am generally more interested in inference/interpretability than optimizing predictive power.

Example conclusions: 

""Controlling for X2, X1 was not statistically significantly associated with Y relative to X1's reference level."" (logistic regression coefficient)
""X1 was not a statistically significant predictor of Y since in the model drop in deviance was not enough relative to the change in df.""  (Analysis of Deviance) 

Is cross-validation always necessary? In which case, one might also want to do some balancing of classes via SMOTE, sampling, etc.
","['regression', 'feature-selection', 'model-selection']","Andrew Gelman is definitely a respected name in the statistical world. His principles closely align with some of the causal modeling research that has been done by other ""big names"" in the field. But I think given your interest in clinical research, you should be consulting other sources. I am using the word ""causal"" loosely (as do others) because there is a fine line we must draw between performing ""causal inference"" from observational data, and asserting causal relations between variables. We all agree RCTs are the main way of assessing causality. We rarely adjust for anything in such trials per the randomization assumption, with few exceptions (Senn, 2004). Observational studies have their importance and utility (Weiss, 1989) and the counterfactual based approach to making inference from observational data is accepted as a philosophically sound approach to doing so (Höfler, 2005). It often approximates very closely the use-efficacy measured in RCTs (Anglemyer, 2014).Therefore, I'll focus on studies from observational data. My point of contention with Gelman's recommendations is: all predictors in a model and their posited causal relationship between a single exposure of interest and a single outcome of interest should be specified apriori. Throwing in and excluding covariates based on their relationship between a set of main findings is actually inducing a special case of 'Munchausen's statistical grid' (Martin, 1984). Some journals (and the trend is catching on) will summarily reject any article which uses stepwise regression to identify a final model (Babyak, 2004), and I think the problem is seen in similar ways here.The rationale for inclusion and exclusion of covariates in a model is discussed in: Judea Pearl's Causality (Pearl, 2002). It is perhaps one of the best texts around for understanding the principles of statistical inference, regression, and multivariate adjustment. Also practically anything by Sanders and Greenland is illuminating, in particular their discussion on confounding which is regretfully omitted from this list of recommendations (Greenland et al. 1999). Specific covariates can be assigned labels based on a graphical relation with a causal model. Designations such as prognostic, confounder, or precision variables warrant inclusion as covariates in statistical models. Mediators, colliders, or variables beyond the causal pathway should be omitted. The definitions of these terms are made rigorous with plenty of examples in Causality.Given this little background I'll address the points one-by-one.This is generally a sound approach with one MAJOR caveat: these variables must NOT be mediators of the outcome. If, for instance, you are inspecting the relationship between smoking and physical fitness, and you adjust for lung function, that is attenuating the effect of smoking because it's direct impact on fitness is that of reducing lung function. This should NOT be confused with confounding where the third variable is causal of the predictor of interest AND the outcome of interest. Confounders must be included in models. Additionally, overadjustment can cause multiple forms of bias in analyses. Mediators and confounders are deemed as such NOT because of what is found in analyses, but because of what is BELIEVED by YOU as the subject-matter-expert (SME). If you have 20 observations per variable or fewer, or 20 observations per event in time-to-event or logistic analyses, you should consider conditional methods instead.This is an excellent power saving approach that is not so complicated as propensity score adjustment or SEM or factor analysis. I would definitely recommend doing this whenever possible.I disagree wholeheartedly. The point of adjusting for other variables in analyses is to create strata for which comparisons are possible. Misspecifying confounder relations does not generally lead to overbiased analyses, so residual confounding from omitted interaction terms is, in my experience, not a big issue. You might, however, consider interaction terms between the predictor of interest and other variables as a post-hoc analysis. This is a hypothesis generating procedure that is meant to refine any possible findings (or lack thereof) as a. potentially belonging to a subgroup or b. involving a mechanistic interaction between two environmental and/or genetic factors.I also disagree with this wholeheartedly. It does not coincide with the confirmatory analysis based approach to regression. You are the SME. The analyses should be informed by the QUESTION and not the DATA. State with confidence what you believe to be happening, based on a pictoral depiction of the causal model (using a DAG and related principles from Pearl et. al), then choose the predictors for your model of interest, fit, and discuss. Only as a secondary analysis should you consider this approach, even at all.The role of machine learning in all of this is highly debatable. In general, machine learning is focused on prediction and not inference which are distinct approaches to data analysis. You are right that the interpretation of effects from penalized regression are not easily interpreted for a non-statistical community, unlike estimates from an OLS, where 95% CIs and coefficient estimates provide a measure of association. The interpretation of the coefficient from an OLS model Y~X is straightforward: it is a slope, an expected difference in Y comparing groups differing by 1 unit in X. In a multivariate adjusted model Y~X1+X2 we modify this as a conditional slope: it is an expected difference in Y comparing groups differing by 1 unit in X1 who have the same value of X2. Geometrically, adjusting for X2 leads to distinct strata or ""cross sections"" of the three space where we compare X1 to Y, then we average up the findings over each of those strata. In R, the coplot function is very useful for visualizing such relations."
Examples of Bayesian and frequentist approach giving different answers,"
Note: I am aware of philosophical differences between Bayesian and frequentist statistics.
For example ""what is the probability that the coin on the table is heads"" doesn't make sense in frequentist statistics, since it has either already landed heads or tails -- there is nothing probabilistic about it. So the question has no answer in frequentist terms.
But such a difference is specifically not the kind of difference I'm asking about.
Rather, I would like to know how their predictions for well-formed questions actually differ in the real world, excluding any theoretical/philosophical differences such as the example I mentioned above.
So in other words:
What's an example of a question, answerable in both frequentist and Bayesian statistics, whose answer is different between the two?
(e.g. Perhaps one of them answers ""1/2"" to a particular question, and the other answers ""2/3"".)
Are there any such differences?

If so, what are some examples?

If not, then when does it actually ever make a difference whether I use Bayesian or frequentist statistics when solving a particular problem?
Why would I avoid one in favor of the other?


","['bayesian', 'frequentist']","This example is taken from here. (I even think I got this link from SO, but cannot find it anymore.)A coin has been tossed $n=14$ times, coming up heads $k=10$ times. If it is to be tossed twice more, would you bet on two heads? Assume you do not get to see the result of the first toss before the second toss (and also independently conditional on $\theta$), so that you cannot update your opinion on $\theta$ in between the two throws. By independence, $$f(y_{f,1}=\text{heads},y_{f,2}=\text{heads}|\theta)=f(y_{f,1}=\text{heads})f(y_{f,2}=\text{heads}|\theta)=\theta^2.$$ 
Then, the predictive distribution given a $\text{Beta}(\alpha_0,\beta_0)$-prior, becomes
\begin{eqnarray*}
    f(y_{f,1}=\text{heads},y_{f,2}=\text{heads}|y)&=&\int f(y_{f,1}=\text{heads},y_{f,2}=\text{heads}|\theta)\pi(\theta|y)d\theta\notag\\
    &=&\frac{\Gamma\left(\alpha _{0}+\beta_{0}+n\right)}{\Gamma\left(\alpha_{0}+k\right)\Gamma\left(\beta_{0}+n-k\right)}\int \theta^2\theta ^{\alpha _{0}+k-1}\left( 1-\theta \right) ^{\beta _{0}+n-k-1}d\theta\notag\\
    &=&\frac{\Gamma\left(\alpha_{0}+\beta_{0}+n\right)}{\Gamma\left(\alpha_{0}+k\right)\Gamma\left(\beta_{0}+n-k\right)}\frac{\Gamma\left(\alpha_{0}+k+2\right)\Gamma\left(\beta_{0}+n-k\right)}{\Gamma\left(\alpha_{0}+\beta_{0}+n+2\right)}\notag\\
    &=&\frac{(\alpha_{0}+k)\cdot(\alpha_{0}+k+1)}{(\alpha_{0}+\beta_{0}+n)\cdot(\alpha_{0}+\beta_{0}+n+1)}
\end{eqnarray*}
For a uniform prior (a $\text{Beta}(1, 1)$-prior), this gives roughly .485. Hence, you would likely not bet. Based on the MLE 10/14, you would calculate a probability of two heads of $(10/14)^2\approx.51$, such that betting would make sense."
Regression for an outcome (ratio or fraction) between 0 and 1,"
I am thinking of building a model predicting a ratio $a/b$, where $a \le b$ and $a > 0$ and $b > 0$. So, the ratio would be between $0$ and $1$.
I could use linear regression, although it doesn't naturally limit to 0..1.  I have no reason to believe the relationship is linear, but of course it is often used anyway, as a simple first model.
I could use a logistic regression, although it is normally used to predict the probability of a two-state outcome, not to predict a continuous value from the range 0..1.
Knowing nothing more, would you use linear regression, logistic regression, or hidden option c?
","['regression', 'logistic', 'generalized-linear-model', 'beta-distribution', 'beta-regression']","You should choose ""hidden option c"", where c is beta regression.  This is a type of regression model that is appropriate when the response variable is distributed as Beta.  You can think of it as analogous to a generalized linear model.  It's exactly what you are looking for.  There is a package in R called betareg which deals with this.  I don't know if you use R, but even if you don't you could read the 'vignettes' anyway, they will give you general information about the topic in addition to how to implement it in R (which you wouldn't need in that case).  Edit (much later):  Let me make a quick clarification.  I interpret the question as being about the ratio of two, positive, real values.  If so, (and they are distributed as Gammas) that is a Beta distribution.  However, if $a$ is a count of 'successes' out of a known total, $b$, of 'trials', then this would be a count proportion $a/b$, not a continuous proportion, and you should use binomial GLM (e.g., logistic regression). For how to do it in R, see e.g. How to do logistic regression in R when outcome is fractional (a ratio of two counts)?Another possibility is to use linear regression if the ratios can be transformed so as to meet the assumptions of a standard linear model, although I would not be optimistic about that actually working.  "
Is random forest a boosting algorithm?,"
Short  definition of boosting:

Can a set of weak learners create a single strong learner? A weak
  learner is defined to be a classifier which is only slightly
  correlated with the true classification (it can label examples better
  than random guessing).

Short definition of Random Forest:

Random Forests grows many classification trees. To classify a new
  object from an input vector, put the input vector down each of the
  trees in the forest. Each tree gives a classification, and we say the
  tree ""votes"" for that class. The forest chooses the classification
  having the most votes (over all the trees in the forest).

Another short definition of Random Forest:

A random forest is a meta estimator that fits a number of decision
  tree classifiers on various sub-samples of the dataset and use
  averaging to improve the predictive accuracy and control over-fitting.

As I understand Random Forest is an boosting algorithm which uses trees as its weak classifiers. I know that it also uses other techniques and improves upon them. Somebody corrected me that Random Forest is not a boosting algorithm?
Can someone elaborate upon this, why Random Forest is not a boosting algorithm?  
","['machine-learning', 'random-forest', 'boosting', 'bagging']","Random Forest is a bagging algorithm rather than a boosting algorithm.
They are two opposite way to achieve a low error.We know that error can be composited from bias and variance. A too complex model has low bias but large variance, while a too simple model has low variance but large bias, both leading a high error but two different reasons. As a result, two different ways to solve the problem come into people's mind (maybe Breiman and others), variance reduction for a complex model, or bias reduction for a simple model, which refers to random forest and boosting.Random forest reduces variance of a large number of ""complex"" models with low bias. We can see the composition elements are not ""weak"" models but too complex models. If you read about the algorithm, the underlying trees are planted ""somewhat"" as large as ""possible"". The underlying trees are independent parallel models. And additional random variable selection is introduced into them to make them even more independent, which makes it perform better than ordinary bagging and entitle the name ""random"".While boosting reduces bias of a large number of ""small"" models with low variance. They are ""weak"" models as you quoted. The underlying elements are somehow like a ""chain"" or ""nested"" iterative model about the bias of each level. So they are not independent parallel models but each model is built based on all the former small models by weighting. That is so-called ""boosting"" from one by one.Breiman's papers and books discuss about trees, random forest and boosting quite a lot. It helps you to understand the principle behind the algorithm."
Interpretation of log transformed predictor and/or response,"
I'm wondering if it makes a difference in interpretation whether only the dependent, both the dependent and independent, or only the independent variables are log transformed.
Consider the case of
log(DV) = Intercept + B1*IV + Error 

I can interpret the IV as the percent increase but how does this change when I have 
log(DV) = Intercept + B1*log(IV) + Error

or when I have 
DV = Intercept + B1*log(IV) + Error

?
","['regression', 'data-transformation', 'interpretation', 'logarithm', 'faq']",
Two-tailed tests... I'm just not convinced. What's the point?,"
The following excerpt is from the entry, What are the differences between one-tailed and two-tailed tests?, on UCLA's statistics help site.

... consider the consequences of missing an effect in the other direction.  Imagine you have developed a new drug that you believe is an improvement over an existing drug.  You wish to maximize your ability to detect the improvement, so you opt for a one-tailed test. In doing so, you fail to test for the possibility that the new drug is less effective than the existing drug.

After learning the absolute basics of hypothesis testing and getting to the part about one vs two tailed tests... I understand the basic math and increased detection ability of one tailed tests, etc... But I just can't wrap around my head around one thing... What's the point? I'm really failing to understand why you should split your alpha between the two extremes when your is sample result can only be in one or the other, or neither. 
Take the example scenario from the quoted text above. How could you possibly ""fail to test"" for a result in the opposite direction? You have your sample mean. You have your population mean. Simple arithmetic tells you which is higher. What is there to test, or fail to test, in the opposite direction? What's stopping you just starting from scratch with the opposite hypothesis if you clearly see that the sample mean is way off in the other direction? 
Another quote from the same page:  

Choosing a one-tailed test after running a two-tailed test that failed to reject the null hypothesis is not appropriate, no matter how ""close"" to significant the two-tailed test was.

I assume this also applies to switching the polarity of your one-tailed test. But how is this ""doctored"" result any less valid than if you had simply chosen the correct one-tailed test in the first place? 
Clearly I am missing a big part of the picture here. It all just seems too arbitrary. Which it is, I guess, in the sense that what denotes ""statistically significant"" - 95%, 99%, 99.9%... Is arbitrary to begin with. 
","['hypothesis-testing', 'statistical-significance', 'inference', 'two-tailed-test']","Think of the data as the tip of the iceberg – all you can see above the water is the tip of the iceberg but in reality you are interested in learning something about the entire iceberg.Statisticians, data scientists and others working with data are careful to not let what they see above the water line influence and bias their assessment of what's hidden below the water line. For this reason, in a hypothesis testing situation, they tend to formulate their null and alternative hypotheses before they see the tip of the iceberg, based on their expectations (or lack thereof) of what might happen if they could view the iceberg in its entirety.Looking at the data to formulate your hypotheses is a poor practice and should be avoided – it's like putting the cart before the horse.  Recall that the data come from a single sample selected (hopefully using a random selection mechanism) from the target population/universe of interest. The sample has its own idiosyncracies, which may or may not be reflective of the underlying population. Why would you want your hypotheses to reflect a narrow slice of the population instead of the entire population?Another way to think about this is that, every time you select a sample from your target population (using a random selection mechanism), the sample will yield different data. If you use the data (which you shouldn't!!!) to guide your specification of the null and alternative hypotheses, your hypotheses will be all over the map, essentially driven by the idiosyncratic features of each sample. Of course, in practice we only draw one sample, but it would be a very disquieting thought to know that if someone else performed the same study with a different sample of the same size, they would have to change their hypotheses to reflect the realities of their sample.One of my graduate school professors used to have a very wise saying: ""We don't care about the sample, except that it tells us something about the population"". We want to formulate our hypotheses to learn something about the target population, not about the one sample we happened to select from that population."
"Are residuals ""predicted minus actual"" or ""actual minus predicted""","
I've seen ""residuals"" defined variously as being either ""predicted minus actual values"" or ""actual minus predicted values"". For illustration purposes, to show that both formulas are widely used, compare the following Web searches:

residual ""predicted minus actual""
residual ""actual minus predicted""

In practice, it almost never makes a difference, since the sign of the invidividual residuals don't usually matter (e.g. if they are squared or the absolute values are taken). However, my question is: is one of these two versions (prediction first vs. actual first) considered ""standard""? I like to be consistent in my usage, so if there is a well-established conventional standard, I would prefer to follow it. However, if there is no standard, I am happy to accept that as an answer, if it can be convincingly demonstrated that there is no standard convention.
","['residuals', 'terminology', 'error']","The residuals are always actual minus predicted. The models are:
$$y=f(x;\beta)+\varepsilon$$
Hence, the residuals $\hat\varepsilon$, which are estimates of errors $\varepsilon$:
$$\hat\varepsilon=y-\hat y\\\hat y=f(x;\hat\beta)$$I agree with @whuber that the sign doesn't really matter mathematically. It's just good to have a convention though. And the current convention is as in my answer.Since OP challenged my authority on this subject, I'm adding some references:"
Who created the first standard normal table?,"
I'm about to introduce the standard normal table in my introductory statistics class, and that got me wondering: who created the first standard normal table? How did they do it before computers came along? I shudder to think of someone brute-force computing a thousand Riemann sums by hand.
","['normal-distribution', 'algorithms', 'history', 'tables']","Laplace was the first to recognize the need for tabulation, coming up with the approximation:$$\begin{align}G(x)&=\int_x^\infty e^{-t^2}dt\\[2ex]&=\small \frac1 x- \frac{1}{2x^3}+\frac{1\cdot3}{4x^5} -\frac{1\cdot 3\cdot5}{8x^7}+\frac{1\cdot 3\cdot 5\cdot 7}{16x^9}+\cdots\tag{1}
\end{align}$$The first modern table of the normal distribution was later built by the French astronomer Christian Kramp in Analyse des Réfractions Astronomiques et Terrestres (Par le citoyen Kramp, Professeur de Chymie et de Physique expérimentale à l'école centrale du Département de la Roer, 1799). From Tables Related to the Normal Distribution: A Short History Author(s): Herbert A. David Source: The American Statistician, Vol. 59, No. 4 (Nov., 2005), pp. 309-311:Ambitiously, Kramp gave eight-decimal ($8$ D) tables up to $x = 1.24,$ $9$ D to $1.50,$ $10$ D to $1.99,$ and $11$ D to $3.00$ together with the
differences needed for interpolation. Writing down the first six derivatives of $G(x),$ he simply uses a Taylor series expansion of $G(x + h)$ about $G(x),$ with $h = .01,$ up to the term in $h^3.$ This enables him to proceed step by step from $x = 0$ to $x = h, 2h, 3h,\dots,$ upon multiplying $h\,e^{-x^2}$ by $$1-hx+ \frac 1 3 \left(2x^2 - 1\right)h^2 - \frac 1 6 \left(2x^3 - 3x\right)h^3.$$
Thus, at $x = 0$ this product reduces to
$$.01 \left(1 - \frac 1 3 \times .0001 \right) = .00999967,$$
so that at $G(.01) = .88622692 - .00999967 = .87622725.$$$\vdots$$But... how accurate could he be? OK, let's take $2.97$ as an example:Amazing!Let's move on to the modern (normalized) expression of the Gaussian pdf:The pdf of $\mathscr N(0,1)$ is:$$f_X(X=x)=\large \frac{1}{\sqrt{2\pi}}\,e^{-\frac {x^2}{2}}= \frac{1}{\sqrt{2\pi}}\,e^{-\left(\frac {x}{\sqrt{2}}\right)^2}= \frac{1}{\sqrt{2\pi}}\,e^{-\left(z\right)^2}$$where $z = \frac{x}{\sqrt{2}}$. And hence, $x = z \times \sqrt{2}$.So let's go to R, and look up the $P_Z(Z>z=2.97)$... OK, not so fast. First we have to remember that when there is a constant multiplying the exponent in an exponential function $e^{ax}$, the integral will be divided by that exponent: $1/a$. Since we are aiming at replicating the results in the old tables, we are actually multiplying the value of $x$ by $\sqrt{2}$, which will have to appear in the denominator.Further, Christian Kramp did not normalize, so we have to correct the results given by R accordingly, multiplying by $\sqrt{2\pi}$. The final correction will look like this:$$\frac{\sqrt{2\pi}}{\sqrt{2}}\,\mathbb P(X>x)=\sqrt{\pi}\,\,\mathbb P(X>x)$$In the case above, $z=2.97$ and $x=z\times \sqrt{2}=4.200214$. Now let's go to R:Fantastic!Let's go to the top of the table for fun, say $0.06$...What says Kramp? $0.82629882$.So close...The thing is... how close, exactly? After all the up-votes received, I couldn't leave the actual answer hanging. The problem was that all the optical character recognition (OCR) applications I tried were incredibly off - not surprising if you have taken a look at the original. So, I learned to appreciate Christian Kramp for the tenacity of his work as I personally typed each digit in the first column of his Table Première.After some valuable help from @Glen_b, now it may very well be accurate, and it's ready to copy and paste on the R console in this GitHub link.Here is an analysis of the accuracy of his calculations. Brace yourself...$0.000001200764$ - in the course of $301$ calculations, he managed to accumulate an error of approximately $1$ millionth!$0.000000003989249$ - he managed to make an outrageously ridiculous $3$ one-billionth error on average!On the entry in which his calculations were most divergent as compared to [R] the first different decimal place value was in the eighth position (hundred millionth). On average (median) his first ""mistake"" was in the tenth decimal digit (tenth billionth!). And, although he didn't fully agree with with [R] in any instances, the closest entry doesn't diverge until the thirteen digital entry.$0.00000002380406$$0.000000007283493$If you find a picture or portrait of Chistian Kramp, please edit this post and place it here."
Is a sample covariance matrix always symmetric and positive definite?,"
When computing the covariance matrix of a sample, is one then guaranteed to get a symmetric and positive-definite matrix?
Currently my problem has a sample of 4600 observation vectors and 24 dimensions.
","['sampling', 'covariance']","For a sample of vectors $x_i=(x_{i1},\dots,x_{ik})^\top$, with $i=1,\dots,n$, the sample mean vector is 
$$
  \bar{x}=\frac{1}{n} \sum_{i=1}^n x_i \, ,
$$ and the sample covariance matrix is
$$
  Q = \frac{1}{n} \sum_{i=1}^n (x_i-\bar{x})(x_i-\bar{x})^\top \, .
$$
For a nonzero vector $y\in\mathbb{R}^k$, we have
$$
  y^\top Qy = y^\top\left(\frac{1}{n} \sum_{i=1}^n (x_i-\bar{x})(x_i-\bar{x})^\top\right) y
$$
$$
 = \frac{1}{n} \sum_{i=1}^n y^\top (x_i-\bar{x})(x_i-\bar{x})^\top y
$$
$$
  = \frac{1}{n} \sum_{i=1}^n \left( (x_i-\bar{x})^\top y \right)^2 \geq 0 \, . \quad (*)
$$
Therefore, $Q$ is always positive semi-definite.The additional condition for $Q$ to be positive definite was given in whuber's comment bellow. It goes as follows.Define $z_i=(x_i-\bar{x})$, for $i=1,\dots,n$. For any nonzero $y\in\mathbb{R}^k$, $(*)$ is zero if and only if $z_i^\top y=0$, for each $i=1,\dots,n$. Suppose the set $\{z_1,\dots,z_n\}$ spans $\mathbb{R}^k$. Then, there are real numbers $\alpha_1,\dots,\alpha_n$ such that $y=\alpha_1 z_1 +\dots+\alpha_n z_n$. But then we have $y^\top y=\alpha_1 z_1^\top y + \dots +\alpha_n z_n^\top y=0$, yielding that $y=0$, a contradiction. Hence, if the $z_i$'s span $\mathbb{R}^k$, then $Q$ is positive definite. This condition is equivalent to $\mathrm{rank} [z_1 \dots z_n] = k$."
Which permutation test implementation in R to use instead of t-tests (paired and non-paired)?,"
I have data from an experiment that I analyzed using t-tests. The dependent variable is interval scaled and the data are either unpaired (i.e., 2 groups) or paired (i.e., within-subjects). 
E.g. (within subjects):
x1 <- c(99, 99.5, 65, 100, 99, 99.5, 99, 99.5, 99.5, 57, 100, 99.5, 
        99.5, 99, 99, 99.5, 89.5, 99.5, 100, 99.5)
y1 <- c(99, 99.5, 99.5, 0, 50, 100, 99.5, 99.5, 0, 99.5, 99.5, 90, 
        80, 0, 99, 0, 74.5, 0, 100, 49.5)

However, the data are not normal so one reviewer asked us to use something other than the t-test. However, as one can easily see, the data are not only not normally distributed, but the distributions are not equal between conditions:

Therefore, the usual nonparametric tests, the Mann-Whitney-U-Test (unpaired) and the Wilcoxon Test (paired), cannot be used as they require equal distributions between conditions. Hence, I decided that some resampling or permutation test would be best.
Now, I am looking for an R implementation of a permutation-based equivalent of the t-test, or any other advice on what to do with the data. 
I know that there are some R-packages that can do this for me (e.g., coin, perm, exactRankTest, etc.), but I don't know which one to pick. So, if somebody with some experience using these tests could give me a kick-start, that would be ubercool.
UPDATE: It would be ideal if you could provide an example of how to report the results from this test.
","['r', 't-test', 'nonparametric', 'permutation-test']","It shouldn't matter that much since the test statistic will always be the difference in means (or something equivalent). Small differences can come from the implementation of Monte-Carlo methods. Trying the three packages with your data with a one-sided test for two independent variables:To check the exact p-value with a manual calculation of all permutations, I'll restrict the data to the first 9 values.coin and exactRankTests are both from the same author, but coin seems to be more general and extensive - also in terms of documentation. exactRankTests is not actively developed anymore. I'd therefore choose coin (also because of informative functions like support()), unless you don't like to deal with S4 objects.EDIT: for two dependent variables, the syntax is"
Is every covariance matrix positive definite?,"
I guess the answer should be yes, but I still feel something is not right. There should be some general results in the literature, could anyone help me?
","['covariance', 'matrix', 'covariance-matrix', 'linear-algebra']","No. Consider three variables, $X$, $Y$ and $Z = X+Y$. Their covariance matrix, $M$, is not positive definite, since there's a vector $z$ ($= (1, 1, -1)'$) for which $z'Mz$ is not positive.Population covariance matrices are positive semi-definite.(See property 2 here.)The same should generally apply to covariance matrices of complete samples (no missing values), since they can also be seen as a form of discrete population covariance.However due to inexactness of floating point numerical computations, even algebraically positive definite cases might occasionally be computed to not be even positive semi-definite; good choice of algorithms can help with this.More generally, sample covariance matrices - depending on how they deal with missing values in some variables - may or may not be positive semi-definite, even in theory. If pairwise deletion is used, for example, then there's no guarantee of positive semi-definiteness. Further, accumulated numerical error can cause sample covariance matrices that should be notionally positive semi-definite to fail to be.Like so:This happened on the first example I tried (I probably should supply a seed but it's not so rare that you should have to try a lot of examples before you get one).The result came out negative, even though it should be algebraically zero. A different set of numbers might yield a positive number or an ""exact"" zero.--Example of moderate missingness leading to loss of positive semidefiniteness via pairwise deletion:"
How to calculate pseudo-$R^2$ from R's logistic regression?,"
Christopher Manning's writeup on logistic regression in R shows a logistic regression in R as follows:
ced.logr <- glm(ced.del ~ cat + follows + factor(class), 
  family=binomial)

Some output:
> summary(ced.logr)
Call:
glm(formula = ced.del ~ cat + follows + factor(class),
    family = binomial(""logit""))
Deviance Residuals:
Min            1Q    Median       3Q      Max
-3.24384 -1.34325   0.04954  1.01488  6.40094

Coefficients:
              Estimate Std. Error z value Pr(>|z|)
(Intercept)   -1.31827    0.12221 -10.787 < 2e-16
catd          -0.16931    0.10032  -1.688 0.091459
catm           0.17858    0.08952   1.995 0.046053
catn           0.66672    0.09651   6.908 4.91e-12
catv          -0.76754    0.21844  -3.514 0.000442
followsP       0.95255    0.07400  12.872 < 2e-16
followsV       0.53408    0.05660   9.436 < 2e-16
factor(class)2 1.27045    0.10320  12.310 < 2e-16
factor(class)3 1.04805    0.10355  10.122 < 2e-16
factor(class)4 1.37425    0.10155  13.532 < 2e-16
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 958.66 on 51 degrees of freedom
Residual deviance: 198.63 on 42 degrees of freedom
AIC: 446.10
Number of Fisher Scoring iterations: 4

He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.
However, how much variance does the model account for?  A Stata page on logistic regression says:

Technically, $R^2$ cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-$R^2$, in logistic regression, is defined as $1 - \frac{L1}{L0}$, where $L0$ represents the log likelihood for the ""constant-only"" model and $L1$ is the log likelihood for the full model with constant and predictors. 

I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be $-2 \log L$. Perhaps null deviance is constant-only and residual deviance is $-2 \log L$ of the model?  However, I'm not crystal clear on it.
Can someone verify how one actually computes the pseudo-$R^2$ in R using this example?
","['r', 'logistic', 'likelihood', 'pseudo-r-squared']",
How should one interpret the comparison of means from different sample sizes?,"
Take the case of book ratings on a website. Book A is rated by 10,000 people with an average rating of 4.25 and the variance $\sigma = 0.5$. Similarly Book B is rated by 100 people and has a rating of 4.5 with $\sigma = 0.25$. 
Now because of the large sample size of Book A the 'mean stabilized' to 4.25. Now for 100 people, it may be that if more people read Book B the mean rating may fall to 4 or 4.25.

how should one interpret the comparison of means from different samples and what are the best conclusions one can/should draw? 

For example - can we really say Book B is better than Book A.
","['t-test', 'mean', 'sample-size', 'rating']","You can use a t-test to assess if there are differences in the means.  The different sample sizes don't cause a problem for the t-test, and don't require the results to be interpreted with any extra care.  Ultimately, you can even compare a single observation to an infinite population with a known distribution and mean and SD; for example someone with an IQ of 130 is smarter than 97.7% of people.  One thing to note though, is that for a given $N$ (i.e., total sample size), power is maximized if the group $n$'s are equal; with highly unequal group sizes, you don't get as much additional resolution with each additional observation.  To clarify my point about power, here is a very simple simulation written for R:  Notice that in all cases $N=100$, but that in the first case $n_1=50$ & $n_2=50$, in the second case $n_1=75$ & $n_2=25$, and in the last case $n_1=90$ and $n_2=10$.  Note further that the standardized mean difference / data generating process was the same in all cases.  However, whereas the test was 'significant' 70% of the time for the 50-50 sample, power was 56% with 75-25 and only 33% when the group sizes were 90-10.  I think of this by analogy.  If you want to know the area of a rectangle, and the perimeter is fixed, then the area will be maximized if the length and width are equal (i.e., if the rectangle is a square).  On the other hand, as the length and width diverge (as the rectangle becomes elongated), the area shrinks.  "
Comparing SVM and logistic regression,"
Can someone please give me some intuition as to when to choose either SVM or LR? I want to understand the intuition behind what is the difference between the optimization criteria of learning the hyperplane of the two, where the respective aims are as follows:  

SVM: Try to maximize the margin between the closest support vectors
LR: Maximize the posterior class probability 

Let's consider the linear feature space for both SVM and LR.
Some differences I know of already:

SVM is deterministic (but we can use Platts model for probability score) while LR is probabilistic.
For the kernel space, SVM is faster (stores just support vectors)

","['regression', 'logistic', 'svm', 'optimization']",
Why do transformers use layer norm instead of batch norm?,"
Both batch norm and layer norm are common normalization techniques for neural network training.
I am wondering why transformers primarily use layer norm.
","['machine-learning', 'natural-language']","It seems that it has been the standard to use batchnorm in CV tasks, and layernorm in NLP tasks. The original Attention is All you Need paper tested only NLP tasks, and thus used layernorm. It does seem that even with the rise of transformers in CV applications, layernorm is still the most standardly used, so I'm not completely certain as to the pros and cons of each. But I do have some personal intuitions -- which I'll admit aren't grounded in theory, but which I'll nevertheless try to elaborate on in the following.Recall that in batchnorm, the mean and variance statistics used for normalization are calculated across all elements of all instances in a batch, for each feature independently. By ""element"" and ""instance,"" I mean ""word"" and ""sentence"" respectively for an NLP task, and ""pixel"" and ""image"" for a CV task.
On the other hand, for layernorm, the statistics are calculated across the feature dimension, for each element and instance independently (source). In transformers, it is calculated across all features and all elements, for each instance independently. This illustration from this recent article conveys the difference between batchnorm and layernorm:(in the case of transformers, where the normalization stats are calculated across all features and all elements for each instance independently, in the image that would correspond to the left face of the cube being colored blue.)Now onto the reasons why batchnorm is less suitable for NLP tasks. In NLP tasks, the sentence length often varies -- thus, if using batchnorm, it would be uncertain what would be the appropriate normalization constant (the total number of elements to divide by during normalization) to use. Different batches would have different normalization constants which leads to instability during the course of training. According to the paper that provided the image linked above, ""statistics
of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented."" (The paper is concerned with an improvement upon batchnorm for use in transformers that they call PowerNorm, which improves performance on NLP tasks as compared to either batchnorm or layernorm.)Another intuition is that in the past (before Transformers), RNN architectures were the norm. Within recurrent layers, it is again unclear how to compute the normalization statistics. (Should you consider previous words which passed through a recurrent layer?) Thus it's much more straightforward to normalize each word independently of others in the same sentence. Of course this reason does not apply to transformers, since computing on words in transformers has no time-dependency on previous words, and thus you can normalize across the sentence dimension too (in the picture above that would correspond to the entire left face of the cube being colored blue).It may also be worth checking out instance normalization and group normalization, I'm no expert on either but apparently each has its merits."
Can someone explain the concept of 'exchangeability'?,"
I see the concept of 'exchangeability' being used in different contexts (e.g., bayesian models) but I have never understood the term very well. 

What does this concept mean?
Under what circumstances is this concept invoked and why?

","['bayesian', 'intuition', 'exchangeability']","Exchangeability is meant to capture symmetry in a problem, symmetry in a sense that does not require independence. Formally, a sequence is exchangeable if its joint probability distribution is a symmetric function of its $n$ arguments.  Intuitively it means we can swap around, or reorder, variables in the sequence without changing their joint distribution.  For example, every IID (independent, identically distributed) sequence is exchangeable - but not the other way around.  Every exchangeable sequence is identically distributed, though. Imagine a table with a bunch of urns on top, each containing different proportions of red and green balls.  We choose an urn at random (according to some prior distribution), and then take a sample (without replacement) from the selected urn.Note that the reds and greens that we observe are NOT independent.  And it is maybe not a surprise to learn that the sequence of reds and greens we observe is an exchangeable sequence.  What is maybe surprising is that EVERY exchangeable sequence can be imagined this way, for a suitable choice of urns and prior distribution. (see Diaconis/Freedman (1980) ""Finite Exchangeable Sequences"", Ann. Prob.).The concept is invoked in all sorts of places, and it is especially useful in Bayesian contexts because in those settings we have a prior distribution (our knowledge of the distribution of urns on the table) and we have a likelihood running around (a model which loosely represents the sampling procedure from a given, fixed, urn).  We observe the sequence of reds and greens (the data) and use that information to update our beliefs about the particular urn in our hand (i.e., our posterior), or more generally, the urns on the table.Exchangeable random variables are especially wonderful because if we have infinitely many of them then we have tomes of mathematical machinery at our fingertips not the least of which being de Finetti's Theorem; see Wikipedia for an introduction."
Optimal number of folds in $K$-fold cross-validation: is leave-one-out CV always the best choice?,"
Computing power considerations aside, are there any reasons to believe that increasing the number of folds in cross-validation leads to better model selection/validation (i.e. that the higher the number of folds the better)?
Taking the argument to the extreme, does leave-one-out cross-validation  necessarily lead to better models than $K$-fold cross-validation?
Some background on this question: I am working on a problem with very few instances (e.g. 10 positives and 10 negatives), and am afraid that my models may not generalize well/would overfit with so little data.
","['cross-validation', 'bias-variance-tradeoff']","Leave-one-out cross-validation does not generally lead to better performance than K-fold, and may be more likely to be worse, as it can have a relatively high variance (i.e. its value changes more for different samples of data than the value for k-fold cross-validation).  This is bad in a model selection criterion as it means the model selection criterion can be optimised in ways that merely exploit the random variation in the particular sample of data, rather than making genuine improvements in performance, i.e. you are more likely to over-fit the model selection criterion.  The reason leave-one-out cross-validation is used in practice is that for many models it can be evaluated very cheaply as a by-product of fitting the model.If computational expense is not primarily an issue, a better approach is to perform repeated k-fold cross-validation, where the k-fold cross-validation procedure is repeated with different random partitions into k disjoint subsets each time.  This reduces the variance.If you have only 20 patterns, it is very likely that you will experience over-fitting the model selection criterion, which is a much neglected pitfall in statistics and machine learning (shameless plug: see my paper on the topic).  You may be better off choosing a relatively simple model and try not to optimise it very aggressively, or adopt a Bayesian approach and average over all model choices, weighted by their plausibility.  IMHO optimisation is the root of all evil in statistics, so it is better not to optimise if you don't have to, and to optimise with caution whenever you do.Note also if you are going to perform model selection, you need to use something like nested cross-validation if you also need a performance estimate (i.e. you need to consider model selection as an integral part of the model fitting procedure and cross-validate that as well)."
Logistic regression in R resulted in perfect separation (Hauck-Donner phenomenon). Now what? [duplicate],"







This question already has answers here:
                                
                            




How to deal with perfect separation in logistic regression?

                                (10 answers)
                            

Closed 1 year ago.



I'm trying to predict a binary outcome using 50 continuous explanatory variables (the range of most of the variables is $-\infty$ to $\infty$). My data set has almost 24,000 rows. When I run glm in R, I get:
Warning messages:  
1: glm.fit: algorithm did not converge  
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 

I've read the other responses that suggest perfect separation might be occurring, but I'm confident that isn't the case in my data (though quasi-complete separation could exist; how can I test to see if that's the case?). If I remove some variables, the ""did not converge"" error might go away. But that's not always what happens.
I tried using the same variables in a bayesglm function and got the same errors.
What steps would you take to figure out exactly what's going on here? How do you figure out which variables are causing the problems?
","['r', 'regression', 'logistic', 'separation']","With such a large design space ($\mathbb{R}^{50}$!) it is possible to get perfect separation without having separation in any of the variable taken individually. I would even second  David J. Harris's comment in saying that this is likely. You can easily test whether your classes are perfectly separated in your design space. This boils down to solving a linear programming problem. An R implementation of this 'test' (not a test in the statistical sense of the term) is implemented in the safeBinaryRegression package.If it turns out that separation is indeed the issue, and if you are only interested in a plain vanilla use of glm (e.g. glm is not called by a higher level function but by you), then there is an R implementation of an algorithms that slightly modifies the classical one to make it 'robust' against separation. It is implemented in the hlr package "
How to derive the ridge regression solution?,"
I am having some issues with the derivation of the solution for ridge regression.
I know the regression solution without the regularization term:
$$\beta = (X^TX)^{-1}X^Ty.$$
But after adding the L2 term $\lambda\|\beta\|_2^2$ to the cost function, how come the solution becomes 
$$\beta = (X^TX + \lambda I)^{-1}X^Ty.$$
","['regression', 'least-squares', 'regularization', 'ridge-regression']","It suffices to modify the loss function by adding the penalty. In matrix terms, the initial quadratic loss function becomes
$$ (Y - X\beta)^{T}(Y-X\beta) + \lambda \beta^T\beta.$$
Deriving with respect to $\beta$ leads to the normal equation
$$ X^{T}Y = \left(X^{T}X + \lambda I\right)\beta $$
which leads to the Ridge estimator."
A chart of daily cases of COVID-19 in a Russian region looks suspiciously level to me - is this so from the statistics viewpoint?,"
Below is a daily chart of newly-detected COVID infections in Krasnodar Krai, a region of Russia, from April 29 to May 19. The population of the region is 5.5 million people.
I read about it and wondered - does this (relatively smooth dynamics of new cases) look okay from the statistical standpoint? Or does this look suspicious? Can a curve be so level during an epidemic without any tinkering with the data by authorities of the region? In my home region, Sverdlovsk Oblast, for example, the chart is much more chaotic.
I'm an amateur in statistics, so maybe I'm wrong and this chart is nothing out of the ordinary.



According to a news report from 18 May 2020, a total of 136695 tests for COVID-19 had been made in the region since the start of the epidemic period and up to that day.
As of 21 May 2020, a total of 2974 infections have been recorded in the region.
P.S. Here's a link I found to a page with better-looking statistics, and covering a longer period, specifically for Krasnodar Krai. On that page, you can hover your cursor over the chart to get specific numbers for the day. (The title uses term ""daily elicited"" number of cases, and the bar caption ""daily confirmed"" number of cases):



","['time-series', 'epidemiology', 'manipulation-detection']","It is decidedly out of the ordinary.The reason is that counts like these tend to have Poisson distributions.  This implies their inherent variance equals the count.  For counts near $100,$ that variance of $100$ means the standard deviations are nearly $10.$ Unless there is extreme serial correlation of the results (which is not biologically or medically plausible), this means the majority of individual values ought to deviate randomly from the underlying hypothesized ""true"" rate by up to $10$ (above and below) and, in an appreciable number of cases (around a third of them all) should deviate by more than that.This is difficult to test in a truly robust manner, but one way would be to overfit the data, attempting to describe them very accurately, and see how large the residuals tend to be.  Here, for instance, are two such fits, a lowess smooth and an overfit Poisson GLM:The variance of the residuals for this Generalized Linear Model (GLM) fit (on a logit scale) is only $0.07.$  For other models with (visually) close fits the variance tends to be from $0.05$ to $0.10.$ This is too small.How can you know?  Bootstrap it.  I chose a parametric bootstrap in which the data are replaced by independent Poisson values drawn from distributions whose parameters equal the predicted values.  Here is one such bootstrapped dataset:You can see how much more the individual values fluctuate than before, and by how much.Doing this $2000$ times produced $2001$ variances (in two or three seconds of computation).  Here is their histogram:The vertical red line marks the value of the variance for the data.(In a well-fit model, the mean of this histogram should be close to $1.$  The mean is $0.75,$ a little less than $1,$ giving an indication of the degree of overfitting.)The p-value for this test is the fraction of those $2001$ variances that are equal to or less than the observed variance.  Since every bootstrapped variance was larger, the p-value is only $1/2001,$ essentially zero.I repeated this calculation for other models.  In the R code below, the models vary according to the number of knots k and degree d of the spline.  In every case the p-value remained at $1/2001.$This confirms the suspicious look of the data.  Indeed, if you hadn't stated that these are counts of cases, I would have guessed they were percentages of something.  For percentages near $100$ the variation will be very much less than in this Poisson model and the data would not look so suspicious.This is the code that produced the first and third figures.  (A slight variant produced the second, replacing X by X0 at the beginning.)"
Why is the regularization term *added* to the cost function (instead of multiplied etc.)?,"
Whenever regularization is used, it is often added onto the cost function such as in the following cost function.
$$
J(\theta)=\frac 1 2(y-\theta X^T)(y-\theta X^T)^T+\alpha\|\theta\|_2^2
$$
This makes intuitive sense to me since minimize the cost function means minimizing the error (the left term) and minimizing the magnitudes of the coefficients (the right term) at the same time (or at least balancing the two minimizations).
My question is why is this regularization term $\alpha\|\theta\|_2^2$ added onto the original cost function and not multiplied or something else which keeps the spirit of the motivation behind the idea of regularization? Is it because if we simply add the term on it is sufficiently simple and enables us to solve this analytically or is there some deeper reason? 
",['regularization'],"It has quite a nice intuition in the Bayesian framework. Consider that the regularized cost function $J$ has a similar role as the probability of a parameter configuration $\theta$ given the observations $X, y$. Applying the Bayes theorem, we get:$$P(\theta|X,y) = \frac{P(X,y|\theta)P(\theta)}{P(X,y)}.$$Taking the log of the expression gives us:$$\log P(\theta|X,y) = \log P(X,y|\theta) + \log P(\theta) - \log P(X,y).$$Now, let's say $J(\theta)$ is the negative1 log-posterior, $-\log P(\theta|X,y)$. Since the last term does not depend on $\theta$, we can omit it without changing the minimum. You are left with two terms: 1) the likelihood term $\log P(X,y|\theta)$ depending on $X$ and $y$, and 2) the prior term $ \log P(\theta)$ depending on $\theta$ only. These two terms correspond exactly to the data term and the regularization term in your formula.You can go even further and show that the loss function which you posted corresponds exactly to the following model:$$P(X,y|\theta) = \mathcal{N}(y|\theta X, \sigma_1^2),$$
$$P(\theta) = \mathcal{N}(\theta | 0, \sigma_2^2),$$where parameters $\theta$ come from a zero-mean Gaussian distribution and the observations $y$ have zero-mean Gaussian noise. For more details see this answer.1 Negative since you want to maximize the probability but minimize the cost."
Mean absolute deviation vs. standard deviation,"
In the text book ""New Comprehensive Mathematics for O Level"" by Greer (1983), I see averaged deviation calculated like this:

Sum up absolute differences between single values and the mean. Then
  get its average. Througout the chapter the term mean deviation is
  used.

But I've recently seen several references that use the term standard deviation and this is what they do:

Calculate squares of differences between single values and the mean.
  Then get their average and finally the root of the answer.

I tried both methods on a common set of data and their answers differ. I'm not a statistician. I got confused while trying to teach deviation to my kids.
So in short, are the terms standard deviation and mean deviation the same or is my old text book wrong?
","['distributions', 'standard-deviation', 'frequency', 'variability']","Both answer how far your values are spread around the mean of the observations.An observation that is 1 under the mean is equally ""far"" from the mean as a value that is 1 above the mean. Hence you should neglect the sign of the deviation. This can be done in two ways:Calculate the absolute value of the deviations and sum these.Square the deviations and sum these squares. Due to the square, you give more weight to high deviations, and hence the sum of these squares will be different from the sum of the means.After calculating the ""sum of absolute deviations"" or the ""square root of the sum of squared deviations"", you average them to get the ""mean deviation"" and the ""standard deviation"" respectively.The mean deviation is rarely used."
Software needed to scrape data from graph [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question appears to be off-topic because EITHER it is not about statistics, machine learning, data analysis, data mining, or data visualization, OR it focuses on programming, debugging, or performing routine operations within a statistical computing platform. If the latter, you could try the support links we maintain.


Closed 4 years ago.







                        Improve this question
                    



Anybody have any experience with software (preferably free, preferably open source) that will take an image of data plotted on cartesian coordinates (a standard, everyday plot) and extract the coordinates of the points plotted on the graph?
Essentially, this is a data-mining problem and a reverse data-visualization problem.
","['data-visualization', 'data-mining', 'software']",Check out the digitize package for R.  Its designed to solve exactly this sort of problem.
Why do we use Kullback-Leibler divergence rather than cross entropy in the t-SNE objective function?,"
In my mind, KL divergence from sample distribution to true distribution is simply the difference between cross entropy and entropy.
Why do we use cross entropy to be the cost function in many machine learning models, but use Kullback-Leibler divergence in t-sne? Is there any difference in learning speed?
","['kullback-leibler', 'tsne', 'cross-entropy']","KL divergence is a natural way to measure the difference between two probability distributions. The entropy $H(p)$ of a distribution $p$ gives the minimum possible number of bits per message that would be needed (on average) to losslessly encode events drawn from $p$. Achieving this bound would require using an optimal code designed for $p$, which assigns shorter code words to higher probability events. $D_{KL}(p \parallel q)$ can be interpreted as the expected number of extra bits per message needed to encode events drawn from true distribution $p$, if using an optimal code for distribution $q$ rather than $p$. It has some nice properties for comparing distributions. For example, if $p$ and $q$ are equal, then the KL divergence is 0.The cross entropy $H(p, q)$ can be interpreted as the number of bits per message needed (on average) to encode events drawn from true distribution $p$, if using an optimal code for distribution $q$. Note the difference: $D_{KL}(p \parallel q)$ measures the average number of extra bits per message, whereas $H(p, q)$ measures the average number of total bits per message. It's true that, for fixed $p$, $H(p, q)$ will grow as $q$ becomes increasingly different from $p$. But, if $p$ isn't held fixed, it's hard to interpret $H(p, q)$ as an absolute measure of the difference, because it grows with the entropy of $p$.KL divergence and cross entropy are related as:$$D_{KL}(p \parallel q) = H(p, q) - H(p)$$We can see from this expression that, when $p$ and $q$ are equal, the cross entropy is not zero; rather, it's equal to the entropy of $p$.Cross entropy commonly shows up in loss functions in machine learning. In many of these situations, $p$ is treated as the 'true' distribution, and $q$ as the model that we're trying to optimize. For example, in classification problems, the commonly used cross entropy loss (aka log loss), measures the cross entropy between the empirical distribution of the labels (given the inputs) and the distribution predicted by the classifier. The empirical distribution for each data point simply assigns probability 1 to the class of that data point, and 0 to all other classes. Side note: The cross entropy in this case turns out to be proportional to the negative log likelihood, so minimizing it is equivalent maximizing the likelihood.Note that $p$ (the empirical distribution in this example) is fixed. So, it would be equivalent to say that we're minimizing the KL divergence between the empirical distribution and the predicted distribution. As we can see in the expression above, the two are related by the additive term $H(p)$ (the entropy of the empirical distribution). Because $p$ is fixed, $H(p)$ doesn't change with the parameters of the model, and can be disregarded in the loss function. We might still want to talk about the KL divergence for theoretical/philosophical reasons but, in this case, they're equivalent from the perspective of solving the optimization problem. This may not be true for other uses of cross entropy and KL divergence, where $p$ might vary.t-SNE fits a distribution $p$ in the input space. Each data point is mapped into the embedding space, where corresponding distribution $q$ is fit. The algorithm attempts to adjust the embedding to minimize $D_{KL}(p \parallel q)$. As above, $p$ is held fixed. So, from the perspective of the optimization problem, minimizing the KL divergence and minimizing the cross entropy are equivalent. Indeed, van der Maaten and Hinton (2008) say in section 2: ""A natural measure of the faithfulness with which $q_{j \mid i}$ models $p_{j \mid i}$ is the Kullback-Leibler divergence (which is in this case equal to the cross-entropy up to an additive constant).""van der Maaten and Hinton (2008). Visualizing data using t-SNE."
What is the intuition behind conditional Gaussian distributions?,"
Suppose that $\mathbf{X} \sim N_{2}(\mathbf{\mu}, \mathbf{\Sigma})$. Then the conditional distribution of $X_1$ given that $X_2 = x_2$ is multivariate normally distributed with mean: 
$$ E[P(X_1 | X_2 = x_2)] = \mu_1+\frac{\sigma_{12}}{\sigma_{22}}(x_2-\mu_2)$$
and variance: $${\rm Var}[P(X_1 | X_2 = x_2)] = \sigma_{11}-\frac{\sigma_{12}^{2}}{\sigma_{22}}$$
It makes sense that the variance would decrease since we have more information. But what is the intuition behind the mean formula? How does the covariance between $X_1$ and $X_2$ factor into the conditional mean?
","['normal-distribution', 'multivariate-analysis', 'intuition']",
Machine learning cookbook / reference card / cheatsheet?,"
I find resources like the Probability and Statistics Cookbook and The R Reference Card for Data Mining incredibly useful. They obviously serve well as references but also help me to organize my thoughts on a subject and get the lay of the land. 
Q: Does anything like these resources exist for machine learning methods?
I'm imagining a reference card which for each ML method would include:

General properties
When the method works well
When the method does poorly
From which or to which other methods the method generalizes. Has it been mostly superseded?
Seminal papers on the method
Open problems associated with the method
Computational intensity

All these things can be found with some minimal digging through textbooks I'm sure. It would just be really convenient to have them on a few pages. 
","['machine-learning', 'references']","Some of the best and freely available resources are:As to the author's question I haven't met ""All in one page"" solution   "
What is the difference between prediction and inference?,"
I'm reading through ""An Introduction to Statistical Learning"" . In chapter 2, they discuss the reason for estimating a function $f$.

2.1.1 Why Estimate $f$?
There are two main reasons we may wish to estimate f : prediction and inference. We discuss each in turn.

I've read it over a few times, but I'm still partly unclear on the difference between prediction and inference. Could someone provide a (practical) example of the differences?
","['prediction', 'terminology', 'causality']",
How can a distribution have infinite mean and variance?,"
It would be appreciated if the following examples could be given:

A distribution with infinite mean and infinite variance.
A distribution with infinite mean and finite variance.
A distribution with finite mean and infinite variance. 
A distribution with finite mean and finite variance.

It comes from me seeing these unfamiliar terms (infinite mean, infinite variance) used in an article I am reading, googling and reading a thread on the Wilmott forum/website, and not finding it a sufficiently clear explanation. I also haven't found any explanations in any of my own textbooks.
","['distributions', 'variance', 'mean']","The mean and variance are defined in terms of (sufficiently general) integrals. What it means for the mean or variance to be infinite is a statement about the limiting behavior for those integralsFor example, for a continuous density the mean is $\lim_{a,b\to\infty}\int_{-a}^b x f(x)\  dx$ (which might here be considered as a Riemann integral, say).This can happen, for example, if the tail is ""heavy enough""; either the upper or the lower part (or both) may not converge to a finite value. Consider the following examples for four cases of finite/infinite mean and variance:A distribution with infinite mean and non-finite variance.Examples: Pareto distribution with $\alpha= 1$, a zeta(2) distribution.A distribution with infinite mean and finite variance.Not possible.A distribution with finite mean and infinite variance.Examples: $t_2$ distribution. Pareto with $\alpha=\frac{3}{2}$.A distribution with finite mean and finite variance.Examples: Any normal. Any uniform (indeed, any bounded variable has all moments). $t_3$.These notes by Charles Geyer talk about how to compute relevant integrals in simple terms. It looks like it's dealing with Riemann integrals there, which only covers the continuous case but more general definitions of integrals will cover all the cases you will be likely to require [Lebesgue integration is the form of integration used in measure theory (which underlies probability) but the point here works just fine with more basic methods]. It also covers (Sec 2.5, p13-14) why ""2."" isn't possible (the mean exists if the variance exists)."
Is it a good practice to always scale/normalize data for machine learning? [duplicate],"







This question already has answers here:
                                
                            




When conducting multiple regression, when should you center your predictor variables & when should you standardize them?

                                (7 answers)
                            

Closed 4 years ago.



My understanding is that when some features have different ranges in their values (for example, imagine one feature being the age of a person and another one being their salary in USD) will affect negatively algorithms because the feature with bigger values will take more influence, is it a good practice to simply ALWAYS scale/normalize the data?
It looks to me that if the values are already similar among then, then normalizing them will have little effect, but if the values are very different normalization will help, however it feels too simple to be true :)
Am I missing something? Are there situations/algorithms were actually it is desirable to let some features to deliberately outweigh others?
","['machine-learning', 'data-transformation', 'normalization']","First things first, I don't think there are many questions of the form ""Is it a good practice to always X in machine learning"" where the answer is going to be definitive. Always? Always always? Across parametric, non-parametric, Bayesian, Monte Carlo, social science, purely mathematic, and million feature models? That'd be nice, wouldn't it!Concretely though, here are a few ways in which: it just depends.Some times when normalizing is good:1) Several algorithms, in particular SVMs come to mind, can sometimes converge far faster on normalized data (although why, precisely, I can't recall).2) When your model is sensitive to magnitude, and the units of two different features are different, and arbitrary. This is like the case you suggest, in which something gets more influence than it should. But of course -- not all algorithms are sensitive to magnitude in the way you suggest. Linear regression coefficients will be identical if you do, or don't, scale your data, because it's looking at proportional relationships between them.Some times when normalizing is bad:1) When you want to interpret your coefficients, and they don't normalize well. Regression on something like dollars gives you a meaningful outcome. Regression on proportion-of-maximum-dollars-in-sample might not.2) When, in fact, the units on your features are meaningful, and distance does make a difference! Back to SVMs -- if you're trying to find a max-margin classifier, then the units that go into that 'max' matter. Scaling features for clustering algorithms can substantially change the outcome. Imagine four clusters around the origin, each one in a different quadrant, all nicely scaled. Now, imagine the y-axis being stretched to ten times the length of the the x-axis. instead of four little quadrant-clusters, you're going to get the long squashed baguette of data chopped into four pieces along its length! (And, the important part is, you might prefer either of these!)In I'm sure unsatisfying summary, the most general answer is that you need to ask yourself seriously what makes sense with the data, and model, you're using."
What is the objective function of PCA?,"
Principal component analysis can use matrix decomposition, but that is just a tool to get there.
How would you find the principal components without the use of matrix algebra?
What is the objective function (goal), and what are the constraints?
",['pca'],"Without trying to give a full primer on PCA, from an optimization standpoint, the primary objective function is the Rayleigh quotient. The matrix that figures in the quotient is (some multiple of) the sample covariance matrix
$$\newcommand{\m}[1]{\mathbf{#1}}\newcommand{\x}{\m{x}}\newcommand{\S}{\m{S}}\newcommand{\u}{\m{u}}\newcommand{\reals}{\mathbb{R}}\newcommand{\Q}{\m{Q}}\newcommand{\L}{\boldsymbol{\Lambda}}

\S = \frac{1}{n} \sum_{i=1}^n \x_i \x_i^T = \m{X}^T \m{X} / n
$$
where each $\x_i$ is a vector of $p$ features and $\m{X}$ is the matrix such that the $i$th row is $\x_i^T$.PCA seeks to solve a sequence of optimization problems. The first in the sequence is the unconstrained problem
$$
\begin{array}{ll}
\text{maximize} & \frac{\u^T \S \u}{\u^T\u} \;, \u \in \reals^p \> .
\end{array}
$$Since $\u^T \u = \|\u\|_2^2 = \|\u\| \|\u\|$, the above unconstrained problem is equivalent to the constrained problem
$$
\begin{array}{ll}
\text{maximize} & \u^T \S \u \\
\text{subject to} & \u^T \u = 1 \>.
\end{array}
$$Here is where the matrix algebra comes in. Since $\S$ is a symmetric positive semidefinite matrix (by construction!) it has an eigenvalue decomposition of the form
$$
\S = \Q \L \Q^T \>,
$$
where $\Q$ is an orthogonal matrix (so $\Q \Q^T = \m{I}$) and $\L$ is a diagonal matrix with nonnegative entries $\lambda_i$ such that $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$.Hence, $\u^T \S \u = \u^T \Q \L \Q^T \u = \m{w}^T \L \m{w} = \sum_{i=1}^p \lambda_i w_i^2$. Since $\u$ is constrained in the problem to have a norm of one, then so is $\m{w}$ since $\|\m{w}\|_2 = \|\Q^T \u\|_2 = \|\u\|_2 = 1$, by virtue of $\Q$ being orthogonal. But, if we want to maximize the quantity $\sum_{i=1}^p \lambda_i w_i^2$ under the constraints that $\sum_{i=1}^p w_i^2 = 1$, then the best we can do is to set $\m{w} = \m{e}_1$, that is, $w_1 = 1$ and $w_i = 0$ for $i > 1$.Now, backing out the corresponding $\u$, which is what we sought in the first place, we get that
$$
\u^\star = \Q \m{e}_1 = \m{q}_1 
$$
where $\m{q}_1$ denotes the first column of $\Q$, i.e., the eigenvector corresponding to the largest eigenvalue of $\S$. The value of the objective function is then also easily seen to be $\lambda_1$.The remaining principal component vectors are then found by solving the sequence (indexed by $i$) of optimization problems
$$
\begin{array}{ll}
\text{maximize} & \u_i^T \S \u_i \\
\text{subject to} & \u_i^T \u_i = 1 \\
                & \u_i^T \u_j = 0 \quad \forall 1 \leq j < i\>.
\end{array}
$$
So, the problem is the same, except that we add the additional constraint that the solution must be orthogonal to all of the previous solutions in the sequence. It is not difficult to extend the argument above inductively to show that the solution of the $i$th problem is, indeed, $\m{q}_i$, the $i$th eigenvector of $\S$.The PCA solution is also often expressed in terms of the singular value decomposition of $\m{X}$. To see why, let $\m{X} = \m{U} \m{D} \m{V}^T$. Then $n \S = \m{X}^T \m{X} = \m{V} \m{D}^2 \m{V}^T$ and so $\m{V} = \m{Q}$ (strictly speaking, up to sign flips) and $\L = \m{D}^2 / n$.The principal components are found by projecting $\m{X}$ onto the principal component vectors. From the SVD formulation just given, it's easy to see that
$$
\m{X} \m{Q} = \m{X} \m{V} = \m{U} \m{D} \m{V}^T \m{V} = \m{U} \m{D} \> .
$$The simplicity of representation of both the principal component vectors and the principal components themselves in terms of the SVD of the matrix of features is one reason the SVD features so prominently in some treatments of PCA."
Does the optimal number of trees in a random forest depend on the number of predictors?,"
Can someone explain why we need a large number of trees in random forest when the number of predictors is large?  How can we determine the optimal number of trees?
","['machine-learning', 'random-forest']","Random forest uses bagging (picking a sample of observations rather than all of them) and random subspace method (picking a sample of features rather than all of them, in other words - attribute bagging) to grow a tree. If the number of observations is large, but the number of trees is too small, then some observations will be predicted only once or even not at all. If the number of predictors is large but the number of trees is too small, then some features can (theoretically) be missed in all subspaces used. Both cases results in the decrease of random forest predictive power. But the last is a rather extreme case, since the selection of subspace is performed at each node.During classification the subspace dimensionality is $\sqrt{p}$ (rather small, $p$ is the total number of predictors) by default, but a tree contains many nodes. During regression the subspace dimensionality is $p/3$ (large enough) by default, though a tree contains fewer nodes. So the optimal number of trees in a random forest depends on the number of predictors only in extreme cases. The official page of the algorithm states that random forest does not overfit, and you can use as much trees as you want. But Mark R. Segal (April 14 2004. ""Machine Learning Benchmarks and Random Forest Regression."" Center for Bioinformatics & Molecular Biostatistics) has found that it overfits for some noisy datasets. So to obtain optimal number you can try training random forest at a grid of ntree parameter (simple, but more CPU-consuming) or build one random forest with many trees with keep.inbag, calculate out-of-bag (OOB) error rates for first $n$ trees (where $n$ changes from $1$ to ntree) and plot OOB error rate vs. number of trees (more complex, but less CPU-consuming)."
Period detection of a generic time series,"
This post is the continuation of another post related to a generic method for outlier detection in time series.
Basically, at this point I'm interested in a robust way to discover the periodicity/seasonality of a generic time series affected by a lot of noise.
From a developer point of view, I would like a simple interface such as:
unsigned int discover_period(vector<double> v);
Where v is the array containing the samples, and the return value is the period of the signal.
The main point is that, again, I can't make any assumption regarding the analyzed signal.
I already tried an approach based on the signal autocorrelation (detecting the peaks of a correlogram), but it's not robust as I would like.
","['time-series', 'algorithms', 'frequency', 'real-time']",
Why do Convolutional Neural Networks not use a Support Vector Machine to classify?,"
In recent years, Convolutional Neural Networks (CNNs) have become the state-of-the-art for object recognition in computer vision. Typically, a CNN consists of several convolutional layers, followed by two fully-connected layers. An intuition behind this is that the convolutional layers learn a better representation of the input data, and the fully connected layers then learn to classify this representation based into a set of labels.
However, before CNNs started to dominate, Support Vector Machines (SVMs) were the state-of-the-art. So it seems sensible to say that an SVM is still a stronger classifier than a two-layer fully-connected neural network. Therefore, I am wondering why state-of-the-art CNNs tend to use the fully connected layers for classification rather than an SVM? In this way, you would have the best of both worlds: a strong feature representation, and a strong classifier, rather than a strong feature representation but only a weak classifier...
Any ideas? 
","['machine-learning', 'neural-networks', 'svm', 'deep-learning', 'conv-neural-network']","What is an SVM, anyway?I think the answer for most purposes is “the solution to the following optimization problem”:
$$
\begin{split}
\operatorname*{arg\,min}_{f \in \mathcal H} \frac{1}{n} \sum_{i=1}^n \ell_\mathit{hinge}(f(x_i), y_i) \, + \lambda \lVert f \rVert_{\mathcal H}^2
\\ \ell_\mathit{hinge}(t, y) = \max(0, 1 - t y)
,\end{split}
\tag{SVM}
$$
where $\mathcal H$ is a reproducing kernel Hilbert space, $y$ is a label in $\{-1, 1\}$, and $t = f(x) \in \mathbb R$ is a “decision value”; our final prediction will be $\operatorname{sign}(t)$. In the simplest case, $\mathcal H$ could be the space of affine functions $f(x) = w \cdot x + b$, and $\lVert f \rVert_{\mathcal H}^2 = \lVert w \rVert^2 + b^2$. (Handling of the offset $b$ varies depending on exactly what you’re doing, but that’s not important for our purposes.)
In the ‘90s through the early ‘10s, there was a lot of work on solving this particular optimization problem in various smart ways, and indeed that’s what LIBSVM / LIBLINEAR / SVMlight / ThunderSVM / ... do. But I don’t think that any of these particular algorithms are fundamental to “being an SVM,” really.Now, how do we train a deep network? Well, we try to solve something like, say,
$$
\begin{split}
\operatorname*{arg\,min}_{f \in \mathcal F} \frac1n \sum_{i=1}^n \ell_\mathit{CE}(f(x_i), y) + R(f)
\\
\ell_\mathit{CE}(p, y) = - y \log(p) - (1-y) \log(1 - p)
,\end{split}
   \tag{$\star$}
$$
where now $\mathcal F$ is the set of deep nets we consider, which output probabilities $p = f(x) \in [0, 1]$. The explicit regularizer $R(f)$ might be an L2 penalty on the weights in the network, or we might just use $R(f) = 0$. Although we could solve (SVM) up to machine precision if we really wanted, we usually can’t do that for $(\star)$ when $\mathcal F$ is more than one layer; instead we use stochastic gradient descent to attempt at an approximate solution.If we take $\mathcal F$ as a reproducing kernel Hilbert space and $R(f) = \lambda \lVert f \rVert_{\mathcal F}^2$, then $(\star)$ becomes very similar to (SVM), just with cross-entropy loss instead of hinge loss: this is also called kernel logistic regression. My understanding is that the reason SVMs took off in a way kernel logistic regression didn’t is largely due to a slight computational advantage of the former (more amenable to these fancy algorithms), and/or historical accident; there isn’t really a huge difference between the two as a whole, as far as I know. (There is sometimes a big difference between an SVM with a fancy kernel and a plain linear logistic regression, but that’s comparing apples to oranges.)So, what does a deep network using an SVM to classify look like? Well, that could mean some other things, but I think the most natural interpretation is just using $\ell_\mathit{hinge}$ in $(\star)$.One minor issue is that $\ell_\mathit{hinge}$ isn’t differentiable at $\hat y = y$; we could instead use $\ell_\mathit{hinge}^2$, if we want. (Doing this in (SVM) is sometimes called “L2-SVM” or similar names.) Or we can just ignore the non-differentiability; the ReLU activation isn’t differentiable at 0 either, and this usually doesn’t matter. This can be justified via subgradients, although note that the correctness here is actually quite subtle when dealing with deep networks.An ICML workshop paper – Tang, Deep Learning using Linear Support Vector Machines, ICML 2013 workshop Challenges in Representation Learning – found using $\ell_\mathit{hinge}^2$ gave small but consistent improvements over $\ell_\mathit{CE}$ on the problems they considered. I’m sure others have tried (squared) hinge loss since in deep networks, but it certainly hasn’t taken off widely.(You have to modify both $\ell_\mathit{CE}$ as I’ve written it and $\ell_\mathit{hinge}$ to support multi-class classification, but in the one-vs-rest scheme used by Tang, both are easy to do.)Another thing that’s sometimes done is to train CNNs in the typical way, but then take the output of a late layer as ""features"" and train a separate SVM on that. This was common in early days of transfer learning with deep features, but is I think less common now.Something like this is also done sometimes in other contexts, e.g. in meta-learning by Lee et al., Meta-Learning with Differentiable Convex Optimization, CVPR 2019, who actually solved (SVM) on deep network features and backpropped through the whole thing. (They didn't, but you can even do this with a nonlinear kernel in $\mathcal H$; this is also done in some other ""deep kernels"" contexts.) It’s a very cool approach – one that I've also worked on – and in certain domains this type of approach makes a ton of sense, but there are some pitfalls, and I don’t think it’s very applicable to a typical ""plain classification"" problem."
"How are regression, the t-test, and the ANOVA all versions of the general linear model?","
How are they all versions of the same basic statistical method?
","['regression', 'self-study', 'anova', 'generalized-linear-model', 't-test']",
Measuring entropy/ information/ patterns of a 2d binary matrix,"
I want to measure the entropy/ information density/ pattern-likeness of a two-dimensional binary matrix. Let me show some pictures for clarification:
This display should have a rather high entropy:
A)

This should have medium entropy:
B)

These pictures, finally, should all have near-zero-entropy:
C)

D)

E)

Is there some index that captures the entropy, resp. the ""pattern-likeness"" of these displays?
Of course, each algorithm (e.g., compression algorithms; or the rotation algorithm proposed by ttnphns) is sensitive to other features of the display. I am looking for an algorithm that tries to capture following properties:

Rotational and axial symmetry
The amount of clustering
Repetitions

Maybe more complicated, the algorith could be sensitive to properties of the psychological ""Gestalt principle"", in particular:

The law of proximity: 
The law of symmetry: Symmetrical images are perceived collectively, even in spite of distance:

Displays with these properties should get assigned a ""low entropy value""; displays with rather random / unstructured points should get assigned a ""high entropy value"".
I am aware that most probably no single algorithm will capture all of these features; therefore suggestions for algorithms which address only some or even only a single feature are highly welcome as well.
In particular, I am looking for concrete, existing algorithms or for specific, implementable ideas (and I will award the bounty according to these criteria).
","['algorithms', 'binary-data', 'entropy', 'pattern-recognition', 'information-theory']","There is a simple procedure that captures all the intuition, including the psychological and geometrical elements.  It relies on using spatial proximity, which is the basis of our perception and provides an intrinsic way to capture what is only imperfectly measured by symmetries.To do this, we need to measure the ""complexity"" of these arrays at varying local scales.  Although we have much flexibility to choose those scales and choose the sense in which we measure ""proximity,"" it is simple enough and effective enough to use small square neighborhoods and to look at averages (or, equivalently, sums) within them.  To this end, a sequence of arrays can be derived from any $m$ by $n$ array by forming moving neighborhood sums using $k=2$ by $2$ neighborhoods, then $3$ by $3$, etc, up to $\min(n,m)$ by $\min(n,m)$ (although by then there are usually too few values to provide anything reliable).To see how this works, let's do the calculations for the arrays in the question, which I will call $a_1$ through $a_5$, from top to bottom.  Here are plots of the moving sums for $k=1,2,3,4$ ($k=1$ is the original array, of course) applied to $a_1$.Clockwise from the upper left, $k$ equals $1$, $2$, $4$, and $3$.  The arrays are $5$ by $5$, then $4$ by $4$, $2$ by $2$, and $3$ by $3$, respectively.  They all look sort of ""random.""  Let's measure this randomness with their base-2 entropy. When an array $a$ contains various distinct values with proportions $p_1,$ $p_2,$ etc., its entropy (by definition) is$$H(a) = -p_1\log_2(p_1) - p_2\log_2(p_2) - \cdots$$For instance, array $a_1$ has ten black cells and 15 white cells, whence they are in proportions of $10/25$ and $15/25,$ respectively.  Its entropy therefore is $$H(a_1) = -(10/25)\log_2(10/25) - (15/25)\log_2(15/25) \approx 0.970951.$$For $a_1$, the sequence of these entropies for $k=1,2,3,4$ is $(0.97, 0.99, 0.92, 1.5)$.  Let's call this the ""profile"" of $a_1$.Here, in contrast, are the moving sums of $a_4$:For $k=2, 3, 4$ there is little variation, whence low entropy.  The profile is $(1.00, 0, 0.99, 0)$.  Its values are consistently close to or lower than the values for $a_1$, confirming the intuitive sense that there is a strong ""pattern"" present in $a_4$.We need a frame of reference for interpreting these profiles.  A perfectly random array of binary values will have just about half its values equal to $0$ and the other half equal to $1$, for an entropy close to $1$.  The moving sums within $k$ by $k$ neighborhoods will tend to have binomial distributions, giving them predictable entropies (at least for large arrays) that can be approximated by $1 + \log_2(k)$:These results are borne out by simulation with arrays up to $m=n=100$.  However, they break down for small arrays (such as the $5$ by $5$ arrays here) due to correlation among neighboring windows (once the window size is about half the dimensions of the array) and due to the small amount of data.  Here is a reference profile of random $5$ by $5$ arrays generated by simulation along with plots of some actual profiles:In this plot the reference profile is solid blue.  The array profiles correspond to $a_1$: red, $a_2$: gold, $a_3$: green, $a_4$: light blue.  (Including $a_5$ would obscure the picture because it is close to the profile of $a_4$.)  Overall the profiles correspond to the ordering in the question: they get lower at most values of $k$ as the apparent ordering increases.  The exception is $a_1$: until the end, for $k=4$, its moving sums tend to have among the lowest entropies.  This reveals a surprising regularity: every $2$ by $2$ neighborhood in $a_1$ has exactly $1$ or $2$ black squares, never any more or less.  It's much less ""random"" than one might think.  (This is partly due to the loss of information that accompanies summing the values in each neighborhood, a procedure that condenses $2^{k^2}$ possible neighborhood configurations into just $k^2+1$ different possible sums.  If we wanted to account specifically for the clustering and orientation within each neighborhood, then instead of using moving sums we would use moving concatenations.  That is, each $k$ by $k$ neighborhood has $2^{k^2}$ possible different configurations; by distinguishing them all, we can obtain a finer measure of entropy.  I suspect that such a measure would elevate the profile of $a_1$ compared to the other images.)This technique of creating a profile of entropies over a controlled range of scales, by summing (or concatenating or otherwise combining) values within moving neighborhoods, has been used in analysis of images.  It is a two-dimensional generalization of the well-known idea of analyzing text first as a series of letters, then as a series of digraphs (two-letter sequences), then as trigraphs, etc.  It also has some evident relations to fractal analysis (which explores properties of the image at finer and finer scales).  If we take some care to use a block moving sum or block concatenation (so there are no overlaps between windows), one can derive simple mathematical relationships among the successive entropies; however, I suspect that using the moving window approach may be more powerful and is a little less arbitrary (because it does not depend on precisely how the image is divided into blocks).Various extensions are possible.  For instance, for a rotationally invariant profile, use circular neighborhoods rather than square ones.  Everything generalizes beyond binary arrays, of course.  With sufficiently large arrays one can even compute locally varying entropy profiles to detect non-stationarity.If a single number is desired, instead of an entire profile, choose the scale at which the spatial randomness (or lack thereof) is of interest.  In these examples, that scale would correspond best to a $3$ by $3$ or $4$ by $4$ moving neighborhood, because for their patterning they all rely on groupings that span three to five cells (and a $5$ by $5$ neighborhood just averages away all variation in the array and so is useless).  At the latter scale, the entropies for $a_1$ through $a_5$ are $1.50$, $0.81$, $0$, $0$, and $0$; the expected entropy at this scale (for a uniformly random array) is $1.34$.  This justifies the sense that $a_1$ ""should have rather high entropy.""  To distinguish $a_3$, $a_4$, and $a_5$, which are tied with $0$ entropy at this scale, look at the next finer resolution ($3$ by $3$ neighborhoods): their entropies are $1.39$, $0.99$, $0.92$, respectively (whereas a random grid is expected to have a value of $1.77$).  By these measures, the original question puts the arrays in exactly the right order."
Are all values within a 95% confidence interval equally likely?,"
I have found discordant information on the question: ""If one constructs a 95% confidence interval (CI) of a difference in means or a difference in proportions, are all values within the CI equally likely? Or, is the point estimate the most likely, with values near the ""tails"" of the CI less likely than those in the middle of the CI?
For instance, if a randomized clinical trial report states that the relative risk of mortality with a particular treatment is 1.06 (95% CI 0.96 to 1.18), is the likelihood of 0.96 being the correct value the same as 1.06?
I found many references to this concept online, but the following two examples reflect the uncertainty therein:

Lisa Sullivan's module about Confidence Intervals states:

The confidence intervals for the difference in means provide a range of likely values for ($μ_1-μ_2$). It is important to note that all values in the confidence interval are equally likely estimates of the true value of ($μ_1-μ_2$).

This blogpost, titled Within the Margin of Error, states: 

What I have in mind is misunderstanding about “margin of error” that treats all points within the confidence interval as equally likely, as if the central limit theorem implied a bounded uniform distribution instead of a t distribution. [...]
  The thing that talk about “margin of error” misses is that possibilities that are close to the point estimate are much more likely than possibilities that are at the edge of the margin"".


These seem contradictory, so which is correct?
",['confidence-interval'],"One question that needs to be answered is what does ""likely"" mean in this context?If it means probability (as it is sometimes used as a synonym of) and we are using strict frequentist definitions then the true parameter value is a single value that does not change, so the probability (likelihood) of that point is 100% and all other values are 0%.  So almost all are equally likely at 0%, but if the interval contains the true value, then it is different from the others.If we use a Bayesian approach then the CI (Credible Interval) comes from the posterior distribution and you can compare the likelihood at the different points within the interval.  Unless the posterior is perfectly uniform within the interval (theoretically possible I guess, but that would be a strange circumstance) then the values have different likelihoods.If we use likely to be similar to confidence then think about it this way:  Compute a 95% confidence interval, a 90% confidence interval, and an 85% confidence interval.  We would be 5% confident that the true value lies in the region inside of the 95% interval but outside of the 90% interval, we could say that the true value is 5% likely to fall in that region.  The same is true for the region that is inside the 90% interval but outside the 85% interval.  So if every value is equally likely, then the size of the above 2 regions would need to be exactly the same and the same would hold true for the region inside a 10% confidence interval but outside a 5% confidence interval.  None of the standard distributions that intervals are constructed using have this property (except special cases with 1 draw from a uniform).You could further prove this to yourself by simulating a large number of datasets from known populations, computing the confidence interval of interest, then comparing how often the true parameter is closer to the point estimate than to each of the end points."
What is quasi-binomial distribution (in the context of GLM)?,"
I'm hoping someone can provide an intuitive overview of what quasibinomial distribution is and what it does. I'm particularly interested in these points:

How quasibinomial differs to the binomial distribution.
When the response variable is a proportion (example values include 0.23, 0.11, 0.78, 0.98), a quasibinomial model will run in R but a binomial model will not. 
Why quasibinomial models should be used when a TRUE/FALSE response variable is overdispersed.

","['r', 'generalized-linear-model', 'binomial-distribution', 'overdispersion', 'quasi-likelihood']",
Statistics and causal inference?,"
In his 1984 paper ""Statistics and Causal Inference"", Paul Holland raised one of the most fundamental questions in statistics:

What can a statistical model say about
  causation?

This led to his motto:

NO CAUSATION WITHOUT MANIPULATION

which emphasized the importance of restrictions around experiments that consider causation. 
Andrew Gelman makes a similar point:

""To find out what happens when you change something, it is necessary to change it.""...There are things you learn from perturbing a system that you'll never find out from any amount of passive observation.

His ideas are summarized in this article.
What considerations should be made when making a causal inference from a statistical model?
",['causality'],"This is a broad question, but given the Box, Hunter and Hunter quote is true I think what it comes down to isThe quality of the experimental design:The quality of the implementation of the design:The quality of the model to accurately reflect the design:At the risk of stating the obvious I'll try to hit on the key points of each:is a large sub-field of statistics, but in it's most basic form I think it comes down to the fact that when making causal inference we ideally start with identical units that are monitored in identical environments other than being assigned to a treatment.  Any systematic differences between groups after assigment are then logically attributable to the treatment (we can infer cause).  But, the world isn't that nice and units differ prior to treatment and evironments during experiments are not perfectly controlled.  So we ""control what we can and randomize what we can't"", which helps to insure that there won't be systematic bias due to the confounders that we controlled or randomized.  One problem is that experiments tend to be difficult (to impossible) and expensive and a large variety of designs have been developed to efficiently extract as much information as possible in as carefully controlled a setting as possible, given the costs.  Some of these are quite rigorous (e.g. in medicine the double-blind, randomized, placebo-controlled trial) and others less so (e.g. various forms of 'quasi-experiments').  is also a big issue and one that statisticians generally don't think about...though we should.  In applied statistical work I can recall incidences where 'effects' found in the data were spurious results of inconsistency of data collection or handling.  I also wonder how often information on true causal effects of interest is lost due to these issues (I believe students in the applied sciences generally have little-to-no training about ways that data can become corrupted - but I'm getting off topic here...)is another large technical subject, and another necessary step in objective causal inference.  To a certain degree this is taken care of because the design crowd develop designs and models together (since inference from a model is the goal, the attributes of the estimators drive design). But this only gets us so far because in the 'real world' we end up analysing experimental data from non-textbook designs and then we have to think hard about things like the appropriate controls and how they should enter the model and what associated degrees of freedom should be and whether assumptions are met if if not how to adjust of violations and how robust the estimators are to any remaining violations and...Anyway, hopefully some of the above helps in thinking about considerations in making causal inference from a model.  Did I forget anything big?"
Choosing between LM and GLM for a log-transformed response variable,"
I'm trying to understand the philosophy behind using a Generalized Linear Model (GLM) vs a Linear Model (LM). I've created an example data set below where:
$$\log(y) = x + \varepsilon $$ 
The example does not have the error $\varepsilon$ as a function of the magnitude of $y$, so I would assume that a linear model of the log-transformed y would be the best. In the example below, this is indeed the case (I think) - since the AIC of the LM on the log-transformed data is lowest. The AIC of the Gamma distribution GLM with a log-link function has a lower sum of squares (SS), but the additional degrees of freedom result in a slightly higher AIC. I was surprised that the Gaussian distribution AIC is so much higher (even though the SS is the lowest of the models). 
I am hoping to get some advice on when one should approach GLM models - i.e. is there something I should look for in my LM model fit residuals to tell me that another distribution is more appropriate? Also, how should one proceed in selecting an appropriate distribution family. 
Many thanks in advance for your help.
[EDIT]: I have now adjusted the summary statistics so that the SS of the log-transformed linear model is comparable to the GLM models with the log-link function. A graph of the statistics is now shown.
Example
set.seed(1111)
n <- 1000
y <- rnorm(n, mean=0, sd=1)
y <- exp(y)
hist(y, n=20)
hist(log(y), n=20)

x <- log(y) - rnorm(n, mean=0, sd=1)
hist(x, n=20)

df  <- data.frame(y=y, x=x)
df2 <- data.frame(x=seq(from=min(df$x), to=max(df$x),,100))


#models
mod.name <- ""LM""
assign(mod.name, lm(y ~ x, df))
summary(get(mod.name))
plot(y ~ x, df)
lines(predict(get(mod.name), newdata=df2) ~ df2$x, col=2)

mod.name <- ""LOG.LM""
assign(mod.name, lm(log(y) ~ x, df))
summary(get(mod.name))
plot(y ~ x, df)
lines(exp(predict(get(mod.name), newdata=df2)) ~ df2$x, col=2)

mod.name <- ""LOG.GAUSS.GLM""
assign(mod.name, glm(y ~ x, df, family=gaussian(link=""log"")))
summary(get(mod.name))
plot(y ~ x, df)
lines(predict(get(mod.name), newdata=df2, type=""response"") ~ df2$x, col=2)

mod.name <- ""LOG.GAMMA.GLM""
assign(mod.name, glm(y ~ x, df, family=Gamma(link=""log"")))
summary(get(mod.name))
plot(y ~ x, df)
lines(predict(get(mod.name), newdata=df2, type=""response"") ~ df2$x, col=2)

#Results
model.names <- list(""LM"", ""LOG.LM"", ""LOG.GAUSS.GLM"", ""LOG.GAMMA.GLM"")

plot(y ~ x, df, log=""y"", pch=""."", cex=3, col=8)
lines(predict(LM, newdata=df2) ~ df2$x, col=1, lwd=2)
lines(exp(predict(LOG.LM, newdata=df2)) ~ df2$x, col=2, lwd=2)
lines(predict(LOG.GAUSS.GLM, newdata=df2, type=""response"") ~ df2$x, col=3, lwd=2)
lines(predict(LOG.GAMMA.GLM, newdata=df2, type=""response"") ~ df2$x, col=4, lwd=2)
legend(""topleft"", legend=model.names, col=1:4, lwd=2, bty=""n"") 

res.AIC <- as.matrix(
    data.frame(
        LM=AIC(LM),
        LOG.LM=AIC(LOG.LM),
        LOG.GAUSS.GLM=AIC(LOG.GAUSS.GLM),
        LOG.GAMMA.GLM=AIC(LOG.GAMMA.GLM)
    )
)

res.SS <- as.matrix(
    data.frame(
        LM=sum((predict(LM)-y)^2),
        LOG.LM=sum((exp(predict(LOG.LM))-y)^2),
        LOG.GAUSS.GLM=sum((predict(LOG.GAUSS.GLM, type=""response"")-y)^2),
        LOG.GAMMA.GLM=sum((predict(LOG.GAMMA.GLM, type=""response"")-y)^2)
    )
)

res.RMS <- as.matrix(
    data.frame(
        LM=sqrt(mean((predict(LM)-y)^2)),
        LOG.LM=sqrt(mean((exp(predict(LOG.LM))-y)^2)),
        LOG.GAUSS.GLM=sqrt(mean((predict(LOG.GAUSS.GLM, type=""response"")-y)^2)),
        LOG.GAMMA.GLM=sqrt(mean((predict(LOG.GAMMA.GLM, type=""response"")-y)^2))
    )
)

png(""stats.png"", height=7, width=10, units=""in"", res=300)
#x11(height=7, width=10)
par(mar=c(10,5,2,1), mfcol=c(1,3), cex=1, ps=12)
barplot(res.AIC, main=""AIC"", las=2)
barplot(res.SS, main=""SS"", las=2)
barplot(res.RMS, main=""RMS"", las=2)
dev.off()



","['r', 'generalized-linear-model', 'linear-model', 'gamma-distribution', 'link-function']","Good effort for thinking through this issue.  Here's an incomplete answer, but some starters for the next steps.First, the AIC scores - based on likelihoods - are on different scales because of the different distributions and link functions, so aren't comparable.  Your sum of squares and mean sum of squares have been calculated on the original scale and hence are on the same scale, so can be compared, although whether this is a good criterion for model selection is another question (it might be, or might not - search the cross validated archives on model selection for some good discussion of this).For your more general question, a good way of focusing on the problem is to consider the difference between LOG.LM (your linear model with the response as log(y)); and LOG.GAUSS.GLM, the glm with the response as y and a log link function.  In the first case the model you are fitting is:$\log(y)=X\beta+\epsilon$; and in the glm() case it is:$ \log(y+\epsilon)=X\beta$and in both cases $\epsilon$ is distributed $     \mathcal{N}(0,\sigma^2)$."
Bootstrap vs. jackknife,"
Both bootstrap and jackknife methods can be used to estimate bias and standard error of an estimate and mechanisms of both resampling methods are not huge different: sampling with replacement vs. leave out one observation at a time. However, jackknife is not as popular as bootstrap in research and practice.
Is there any obvious advantage of using bootstrap instead of using jackknife?
","['r', 'confidence-interval', 'bootstrap', 'jackknife']","Bootstrapping is a superior technique and can be used pretty much anywhere jackknifing has been used.  Jackknifing is much older (perhaps ~20 years); it's main advantage in the days when computing power was limited, was that it's computationally much simpler.  However, the bootstrap provides information about the whole sampling distribution, and can offer greater precision.  The jackknife is still useful in outlier detection, for example in calculating dfbeta (the change in a parameter estimate when a data point is dropped).  "
What are disadvantages of state-space models and Kalman Filter for time-series modelling?,"
Given all good properties of state-space models and KF, I wonder - what are disadvantages of state-space modelling and using Kalman Filter (or EKF, UKF or particle filter) for estimation? Over let's say conventional methodologies like ARIMA, VAR or ad-hoc/heuristic methods.
Are they hard to calibrate? Are they complicated and hard to see how a change in a model's structure will affect predictions?
Or, put another way - what are advantages of conventional ARIMA, VAR over state-space models?
I can think only of advantages of a state-space model: 

It easily handles structural breaks, shifts, time-varying parameters of some static model - just make those parameters dynamic states of a state-space model and model will automatically adjust to any shifts in parameters;
It handles missing data very naturally, just do transition step of KF and don't do update step;
It allows to change on-a-fly parameters of a state-space model itself (covariances of noises and transition/observation matrices) so if your current observation came from a little different source than others - you can easily incorporate it into estimation without doing anything special;
Using above properties it allows easily handle irregular-spaced data: either change a model each time according to interval between observations or use regular interval and treat intervals without observations as missing data;
It allows to use data from different sources simultaneously in the same model to estimate one underlying quantity;
It allows to construct a model from several interpretable unobservable dynamic components and estimate them;
Any ARIMA model can be represented in a state-space form, but only simple state-space models can be represented exactly in ARIMA form.

","['time-series', 'arima', 'kalman-filter', 'vector-autoregression']",
Brain teaser: How to generate 7 integers with equal probability using a biased coin that has a pr(head) = p?,"
This is a question I found on Glassdoor: How does one generate 7 integers with equal probability using a coin that has a $\mathbb{Pr}(\text{Head}) = p\in(0,1)$?
Basically, you have a coin that may or may not be fair, and this is the only random-number generating process you have, so come up with random number generator that outputs integers from 1 to 7 where the probability of getting each of these integers is 1/7.
Efficiency of the data-generates process matters.
","['probability', 'binomial-distribution', 'random-generation']",
Does 10 heads in a row increase the chance of the next toss being a tail?,"
I assume the following is true: assuming a fair coin, getting 10 heads in a row whilst tossing a coin does not increase the chance of the next coin toss being a tail, no matter what amount of probability and/or statistical jargon is tossed around (excuse the puns). 
Assuming that is the case, my question is this: how the hell do I convince someone that is the case? 
They are smart and educated but seem determined not to consider that I might be in the right on this (argument).
","['probability', 'independence', 'intuition', 'games', 'bernoulli-process']",
Which loss function is correct for logistic regression?,"
I read about two versions of the loss function for logistic regression, which of them is correct and why?

From Machine Learning, Zhou Z.H (in Chinese), with $\beta = (w, b)\text{ and }\beta^Tx=w^Tx +b$:
$$l(\beta) = \sum\limits_{i=1}^{m}\Big(-y_i\beta^Tx_i+\ln(1+e^{\beta^Tx_i})\Big)  \tag 1$$

From my college course, with $z_i = y_if(x_i)=y_i(w^Tx_i + b)$:
$$L(z_i)=\log(1+e^{-z_i}) \tag 2$$



I know that the first one is an accumulation of all samples and the second one is for a single sample, but I am more curious about the difference in the form of two loss functions. Somehow I have a feeling that they are equivalent.
","['logistic', 'loss-functions']","The relationship is as follows: $l(\beta) = \sum_i L(z_i)$. Define a logistic function as $f(z) = \frac{e^{z}}{1 + e^{z}} = \frac{1}{1+e^{-z}}$. They possess the property that $f(-z) = 1-f(z)$. Or in other words:$$
\frac{1}{1+e^{z}} = \frac{e^{-z}}{1+e^{-z}}.
$$If you take the reciprocal of both sides, then take the log you get:$$
\ln(1+e^{z}) = \ln(1+e^{-z}) +  z.
$$Subtract $z$ from both sides and you should see this:$$
-y_i\beta^Tx_i+ln(1+e^{y_i\beta^Tx_i}) = L(z_i).
$$At the moment I am re-reading this answer and am confused about how I got $-y_i\beta^Tx_i+ln(1+e^{\beta^Tx_i})$ to be equal to $-y_i\beta^Tx_i+ln(1+e^{y_i\beta^Tx_i})$. Perhaps there's a typo in the original question. In the case that there wasn't a typo in the original question, @ManelMorales appears to be correct to draw attention to the fact that, when $y \in \{-1,1\}$, the probability mass function can be written as $P(Y_i=y_i) = f(y_i\beta^Tx_i)$, due to the property that $f(-z) = 1 - f(z)$. I am re-writing it differently here, because he introduces a new equivocation on the notation $z_i$. The rest follows by taking the negative log-likelihood for each $y$ coding. See his answer below for more details."
"What does ""Scientists rise up against statistical significance"" mean? (Comment in Nature)","
The title of the Comment in Nature Scientists rise up against statistical significance begins with:

Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.

and later contains statements like:

Again, we are not advocating a ban on P values, confidence intervals or other statistical measures — only that we should not treat them categorically. This includes dichotomization as statistically significant or not, as well as categorization based on other statistical measures such as Bayes factors.

I think I can grasp that the image below does not say that the two studies disagree because one ""rules out"" no effect while the other does not. But the article seems to go into much more depth than I can understand.
Towards the end there seems to be a summary in four points. Is it possible to summarize these in even simpler terms for those of us who read statistics rather than write it?

When talking about compatibility intervals, bear in mind four things.

First, just because the interval gives the values most compatible with the data, given the assumptions, it doesn’t mean values outside it are incompatible; they are just less compatible...

Second, not all values inside are equally compatible with the data, given the assumptions...

Third, like the 0.05 threshold from which it came, the default 95% used to compute intervals is itself an arbitrary convention...

Last, and most important of all, be humble: compatibility assessments hinge on the correctness of the statistical assumptions used to compute the interval...





","['statistical-significance', 'p-value', 'bias']","The first three points, as far as I can tell, are a variation on a single argument.Scientists often treat uncertainty measurements ($12 \pm 1  $, for instance) as probability distributions that look like this:When actually, they are much more likely to look like this: 
As a former chemist, I can confirm that many scientists with non-mathematical backgrounds (primarily non-physical chemists and biologists) don't really understand how uncertainty (or error, as they call it) is supposed to work. They recall a time in undergrad physics where they maybe had to use them, possibly even having to calculate a compound error through several different measurements, but they never really understood them. I too was guilty of this, and assumed all measurements had to come within the $\pm$ interval. Only recently (and outside academia), did I find out that error measurements usually refer to a certain standard deviation, not an absolute limit.So to break down the numbered points in the article: Measurements outside the CI still have a chance of happening, because the real (likely gaussian) probability is non-zero there (or anywhere for that matter, although they become vanishingly small when you get far out). If the values after the $\pm$ do indeed represent one s.d., then there is still a 32% chance of a data point falling outside of them. The distribution is not uniform (flat topped, as in the first graph), it is peaked. You are more likely to get a value in the middle than you are at the edges. It's like rolling a bunch of dice, rather than a single die.95% is an arbitrary cutoff, and coincides almost exactly with two standard deviations.This point is more of a comment on academic honesty in general. A realisation I had during my PhD is that science isn't some abstract force, it is the cumulative efforts of people attempting to do science. These are people who are trying to discover new things about the universe, but at the same time are also trying to keep their kids fed and keep their jobs, which unfortunately in modern times means some form of publish or perish is at play. In reality, scientists depend on discoveries that are both true and interesting, because uninteresting results don't result in publications. Arbitrary thresholds such as $p < 0.05$ can often be self-perpetuating, especially among those who don't fully understand statistics and just need a pass/fail stamp on their results. As such, people do sometimes half-jokingly talk about 'running the test again until you get $p < 0.05$'. It can be very tempting, especially if a Ph.D/grant/employment is riding on the outcome, for these marginal results to be, jiggled around until the desired $p = 0.0498$ shows up in the analysis. Such practices can be detrimental to the science as a whole, especially if it is done widely, all in the pursuit of a number which is in the eyes of nature, meaningless. This part in effect is exhorting scientists to be honest about their data and work, even when that honesty is to their detriment. "
Intuitive explanation of the bias-variance tradeoff?,"
I am looking for an intuitive explanation of the bias-variance tradeoff, both in general and specifically in the context of linear regression.
","['regression', 'variance', 'bias', 'intuition', 'bias-variance-tradeoff']","Imagine some 2D data--let's say height versus weight for students at a high school--plotted on a pair of axes.Now suppose you fit a straight line through it. This line, which of course represents a set of predicted values, has zero statistical variance. But the bias is (probably) high--i.e., it doesn't fit the data very well.Next, suppose you model the data with a high-degree polynomial spline. You're not satisfied with the fit, so you increase the polynomial degree until the fit improves (and it will, to arbitrary precision, in fact). Now you have a situation with bias that tends to zero, but the variance is very high.Note that the bias-variance trade-off doesn't describe a proportional relationship--i.e., if you plot bias versus variance you won't necessarily see a straight line through the origin with slope -1. In the polynomial spline example above, reducing the degree almost certainly increases the variance much less than it decreases the bias.The bias-variance tradeoff is also embedded in the sum-of-squares error function. Below, I have rewritten (but not altered) the usual form of this equation to emphasize this:$$
E\left(\left(y - \dot{f}(x)\right)^2\right) = \sigma^2 + \left[f(x) - \frac{1}{\kappa}\sum_{i=0}^nf(x_n)\right]^2+\frac{\sigma^2}{\kappa}
$$On the right-hand side, there are three terms: the first of these is just the irreducible error (the variance in the data itself); this is beyond our control so ignore it. The second term is the square of the bias; and the third is the variance. It's easy to see that as one goes up the other goes down--they can't both vary together in the same direction. Put another way, you can think of least-squares regression as (implicitly) finding the optimal combination of bias and variance from among candidate models."
How does saddlepoint approximation work?,"
How does saddlepoint approximation work? What sort of problem is it good for?
(Feel free to use a particular example or examples by way of illustration)
Are there any drawbacks, difficulties, things to watch out for, or traps for the unwary?
","['distributions', 'mathematical-statistics', 'moment-generating-function', 'saddlepoint-approximation', 'partial-moments']","The saddlepoint approximation to a probability density function (it works likewise for mass functions, but I will only talk here in terms of densities)
is a surprisingly well working approximation, that can be seen as a refinement on the central limit theorem. So, it will only work in settings where there is a central limit theorem, but it needs stronger assumptions.We start with the assumption that the moment generating function exists and is twice differentiable. This implies in particular that all moments exist. Let $X$ be a random variable with moment generating function (mgf)
$$ \DeclareMathOperator{\E}{\mathbb{E}}
   M(t) = \E  e^{t X}
$$
and cgf (cumulant generating function) $K(t)=\log M(t)$ (where $\log $ denotes the natural logarithm). In the development I will follow closely Ronald W Butler: ""Saddlepoint Approximations with Applications"" (CUP). We will develop the saddlepoint approximation using the Laplace approximation to a certain integral.  Write
$$
e^{K(t)} = \int_{-\infty}^\infty e^{t x} f(x) \; dx =\int_{-\infty}^\infty \exp(tx+\log f(x) ) \; dx \\
    = \int_{-\infty}^\infty \exp(-h(t,x)) \; dx
$$
where
$h(t,x) = -tx - \log f(x) $. Now we will Taylor expand $h(t,x)$ in $x$ considering $t$ as a constant. This gives
$$
  h(t,x)=h(t,x_0) + h'(t,x_0)(x-x_0) +\frac12 h''(t,x_0) (x-x_0)^2 +\dotsm 
$$
where $'$ denotes differentiation with respect to $x$. Note that
$$
h'(t,x)=-t-\frac{\partial}{\partial x}\log f(x) \\
h''(t,x)= -\frac{\partial^2}{\partial x^2} \log f(x) > 0
$$
(the last inequality by assumption as it is needed for the approximation to work). Let $x_t$ be the solution to $h'(t,x_t)=0$. We will assume that this gives a minimum for  $h(t,x)$ as a function of $x$.  Using this expansion in the integral and forgetting about the $\dotsm$ part, gives
$$
e^{K(t)} \approx \int_{-\infty}^\infty \exp(-h(t,x_t)-\frac12 h''(t,x_t) (x-x_t)^2 ) \; dx \\
= e^{-h(t,x_t)} \int_{-\infty}^\infty e^{-\frac12 h''(t,x_t) (x-x_t)^2} \; dx
$$
which is a Gaussian integral, giving
$$
e^{K(t)} \approx e^{-h(t,x_t)} \sqrt{\frac{2\pi}{h''(t,x_t)}}. 
$$
This gives (a first version) of the saddlepoint approximation as
$$ 
f(x_t) \approx \sqrt{\frac{h''(t,x_t)}{2\pi}} \exp(K(t) -t x_t) \\
     \tag{*} \label{*}
$$
Note that the approximation has the form of an exponential family.Now we need to do some work to get this in a more useful form.From $h'(t,x_t)=0$ we get
$$
    t = -\frac{\partial}{\partial x_t} \log f(x_t).
$$
Differentiating this with respect to $x_t$ gives
$$
 \frac{\partial t}{\partial x_t} = -\frac{\partial^2}{\partial x_t^2} \log f(x_t) > 0$$
(by our assumptions), so the relationship between $t$ and $x_t$ is monotone, so $x_t$ is well defined. We need an approximation to $\frac{\partial}{\partial x_t} \log f(x_t)$. To that end, we get by solving from \eqref{*}
$$
\log f(x_t) = K(t) -t x_t -\frac12 \log \frac{2\pi}{-\frac{\partial^2}{\partial x_t^2} \log f(x_t)}.   \tag{**}  \label{**}
$$
Assuming the last term above only depends weakly on $x_t$, so its derivative with respect to $x_t$ is approximately zero (we will come back to comment on this), we get
$$
\frac{\partial \log f(x_t)}{\partial x_t} \approx 
  (K'(t)-x_t) \frac{\partial t}{\partial x_t} - t
$$
Up to this approximation we then have that
$$
0 = t + \frac{\partial \log f(x_t)}{\partial x_t} \approx (K'(t)-x_t) \frac{\partial t}{\partial x_t}
$$
so that $t$ and $x_t$ must be related through the equation
$$
K'(t) - x_t=0, \\
     \tag{§} \label{§}
$$
which is called the saddlepoint equation.What we miss now in determining \eqref{*} is
$$
  h''(t,x_t) = -\frac{\partial^2 \log f(x_t)}{\partial x_t^2} \\
 = -\frac{\partial}{\partial x_t} \left(\frac{\partial \log f(x_t)}{\partial x_t}    \right)
  \\
= -\frac{\partial}{\partial x_t}(-t)= \left(\frac{\partial x_t}{\partial t}\right)^{-1} 
$$
and that we can find by implicit differentiation of the saddlepoint equation $K'(t)=x_t$:
$$
\frac{\partial x_t}{\partial t} = K''(t).
$$
The result is that (up to our approximation)
$$
h''(t,x_t) = \frac1{K''(t)}
$$
Putting everything together, we have the final saddlepoint approximation of the density $f(x)$ as
$$
   f(x_t) \approx e^{K(t)- t x_t} \sqrt{\frac1{2\pi K''(t)}}. 
$$
Now, to use this practically, to approximate the density at a specific point $x_t$, we solve the saddlepoint equation for that $x_t$ to find $t$.The saddlepoint approximation is often stated as an approximation to the density of the mean based on $n$ iid observations $X_1, X_2, \dotsc, X_n$.
The cumulant generating function of the mean is simply $n K(t)$, so the saddlepoint approximation for the mean becomes
$$
f(\bar{x}_t) = e^{nK(t) - n t \bar{x}_t} \sqrt{\frac{n}{2\pi K''(t)}}
$$Let us look at a first example. What does we get if we try to approximate the standard normal density
$$
f(x)=\frac1{\sqrt{2\pi}} e^{-\frac12 x^2}
$$
The mgf is $M(t)=\exp(\frac12 t^2)$ so
$$
   K(t)=\frac12 t^2 \\
   K'(t)=t  \\
   K''(t)=1
$$
so the saddlepoint equation is $t=x_t$ and the saddlepoint approximation gives
$$
  f(x_t) \approx e^{\frac12 t^2 -t x_t} \sqrt{\frac1{2\pi \cdot 1}}
    = \frac1{\sqrt{2\pi}} e^{-\frac12 x_t^2} 
$$
so in this case the approximation is exact.Let us look at a very different application: Bootstrap in the transform domain, we can do bootstrapping analytically using the saddlepoint approximation to the bootstrap distribution of the mean!Assume we have $X_1, X_2, \dotsc, X_n$ iid distributed from some density $f$ (in the simulated example we will use a unit exponential distribution). From the sample we calculate the empirical moment generating function
$$
  \hat{M}(t)= \frac1{n} \sum_{i=1}^n e^{t x_i}
$$
and then the empirical cgf $\hat{K}(t) = \log \hat{M}(t)$. We need the empirical mgf for the mean which is $\log ( \hat{M}(t/n)^n )$ and the empirical cgf for the mean
$$
  \hat{K}_{\bar{X}}(t) = n \log \hat{M}(t/n) 
$$
which we use to construct a saddlepoint approximation. In the following some R code (R version 3.2.3):( I have tried to write this as general code which can be modified easily for other cgfs, but the code is still not very robust ...)Then we use this for a sample of ten independent observations from a unit exponential distribution. We do the usual nonparametric bootstrapping ""by hand"", plot the resulting bootstrap histogram for the mean, and overplot the saddlepoint approximation:Giving the resulting plot:The approximation seems to be rather good!We could get an even better approximation by integrating the saddlepoint approximation and rescaling:Now the cumulative distribution function based on this approximation could be found by numerical integration, but it is also possible to make a direct saddlepoint approximation for that. But that is for another post, this is long enough.Finally, some comments left out of the development above. In \eqref{**} we did an approximation essentially ignoring the third term. Why can we do that?  One observation is that for the normal density function,  the left-out term contributes nothing, so that approximation is exact.  So, since the saddlepoint-approximation is a refinement on the central limit theorem, so we are somewhat close to the normal, so this should work well. One can also look at specific examples. Looking at the saddlepoint approximation to the Poisson distribution, looking at that left-out third term, in this case that becomes a trigamma function, which indeed is rather flat when the argument is not to close to zero.Finally, why the name? The name come from an alternative derivation, using complex-analysis techniques.  Later we can look into that, but in another post!"
"How to generate correlated random numbers (given means, variances and degree of correlation)?","
I'm sorry if this seems a bit too basic, but I guess I'm just looking to confirm understanding here. I get the sense I'd have to do this in two steps, and I've started trying to grok correlation matrices, but it's just starting to seem really involved. I'm looking for a concise explanation (ideally with hints towards a pseudocode solution) of a good, ideally quick way to generate correlated random numbers. 
Given two pseudorandom variables height and weight with known means and variances, and a given correlation, I think I'm basically trying to understand what this second step should look like:
   height = gaussianPdf(height.mean, height.variance)
   weight = gaussianPdf(correlated_mean(height.mean, correlation_coefficient), 
                        correlated_variance(height.variance, 
                        correlation_coefficient))


How do I calculate the correlated mean and variance? But I want to confirm that's really the relevant problem here. 
Do I need to resort to matrix manipulation? Or do I have something else very wrong in my basic approach to this problem?

","['probability', 'correlation', 'conditional-probability', 'random-generation']","To answer your question on ""a good, ideally quick way to generate correlated random numbers"":
Given a desired variance-covariance matrix $C$ that is by definition positive definite, the Cholesky decomposition of it is: $C$=$LL^T$; $L$ being lower triangular matrix. If you now use this matrix $L$ to project an uncorrelated random variable vector $X$, the resulting projection $Y = LX$ will be that of correlated random variables.You can find an concise explanation why this happens here."
Is adjusting p-values in a multiple regression for multiple comparisons a good idea?,"
Lets assume you are a social science researcher/econometrician trying to find relevant predictors of demand for a service. You have 2 outcome/dependent variables describing the demand (using the service yes/no, and the number of occasions). You have 10 predictor/independent variables that could theoretically explain the demand (e.g., age, sex, income, price, race, etc). Running two separate  multiple regressions will yield 20 coefficients estimations and their p-values. With enough independent variables in your regressions you would sooner or later find at least one variable with a statistically significant correlation between the dependent and independent variables. 
My question: is it a good idea to correct the p-values for multiple tests if I want to include all independent variables in the regression? Any references to prior work are much appreciated. 
","['regression', 'multivariate-analysis', 'predictive-models', 'multiple-regression', 'multiple-comparisons']","It seems your question more generally addresses the problem of identifying good predictors. In this case, you should consider using some kind of penalized regression (methods dealing with variable or feature selection are relevant too), with e.g. L1, L2 (or a combination thereof, the so-called elasticnet) penalties (look for related questions on this site, or the R penalized and elasticnet package, among others). Now, about correcting p-values for your regression coefficients (or equivalently your partial correlation coefficients) to protect against over-optimism (e.g. with Bonferroni or, better, step-down methods), it seems this would only be relevant if you are considering one model and seek those predictors that contribute a significant part of explained variance, that is if you don't perform model selection (with stepwise selection, or hierarchical testing). This article may be a good start: Bonferroni Adjustments in Tests for Regression Coefficients. Be aware that such correction won't protect you against multicollinearity issue, which affects the reported p-values. Given your data, I would recommend using some kind of iterative model selection techniques. In R for instance, the stepAIC function allows to perform stepwise model selection by exact AIC. You can also estimate the relative importance of your predictors based on their contribution to $R^2$ using boostrap (see the relaimpo package). I think that reporting effect size measure or % of explained variance are more informative than p-value, especially in a confirmatory model.It should be noted that stepwise approaches have also their drawbacks (e.g., Wald tests are not adapted to conditional hypothesis as induced by the stepwise procedure), or as indicated by Frank Harrell on R mailing, ""stepwise variable selection based on AIC has all the problems of stepwise variable selection based on P-values. AIC is just a restatement of the P-Value"" (but AIC remains useful if the set of predictors is already defined); a related question -- Is a variable significant in a linear regression model? -- raised interesting comments (@Rob, among others) about the use of AIC for variable selection. I append a couple of references at the end (including papers kindly provided by @Stephan); there is also a lot of other references on P.Mean. Frank Harrell authored a book on Regression Modeling Strategy which includes a lot of discussion and advices around this problem (§4.3, pp. 56-60). He also developed efficient R routines to deal with generalized linear models (See the Design or rms packages). So, I think you definitely have to take a look at it (his handouts are available on his homepage).References"
Reference book for linear algebra applied to statistics?,"
I have been working in R for a bit and have been faced with things like PCA, SVD, QR decompositions and many such linear algebra results (when inspecting estimating weighted regressions and such) so I wanted to know if anyone has a recommendation on a good comprehensive linear algebra book which is not too theoretical but is mathematically rigorous and covers all of these such topics.
","['references', 'matrix', 'linear-algebra', 'weighted-regression']","The ""big three"" that I have used/heard of are:Gentle,  Matrix Algebra: Theory, Computations, and Applications in Statistics. (Amazon link).Searle, Matrix Algebra Useful for Statistics. (Amazon link).Harville, Matrix Algebra From a Statistician's Perspective. (Amazon link).I have used Gentle and Harville and found both to be very helpful and quite manageable."
Why sigmoid function instead of anything else?,"
Why is the de-facto standard sigmoid function, $\frac{1}{1+e^{-x}}$, so popular in (non-deep) neural-networks and logistic regression? 
Why don't we use many of the other derivable functions, with faster computation time or slower decay (so vanishing gradient occurs less). Few examples are on Wikipedia about sigmoid functions. One of my favorites with slow decay and fast calculation is $\frac{x}{1+|x|}$.
EDIT
The question is different to Comprehensive list of activation functions in neural networks with pros/cons as I'm only interested in the 'why' and only for the sigmoid.
","['logistic', 'neural-networks', 'least-squares']","Quoting myself from this answer to a different question:In section 4.2 of Pattern Recognition and Machine Learning (Springer 2006), Bishop shows that the logit arises naturally as the form of the posterior probability distribution in a Bayesian treatment of two-class classification. He then goes on to show that the same holds for discretely distributed features, as well as a subset of the family of exponential distributions. For multi-class classification the logit generalizes to the normalized exponential or softmax function.This explains why this sigmoid is used in logistic regression.Regarding neural networks, this blog post explains how different nonlinearities including the logit / softmax and the probit used in neural networks can be given a statistical interpretation and thereby a motivation. The underlying idea is that a multi-layered neural network can be regarded as a hierarchy of generalized linear models; according to this, activation functions are link functions, which in turn correspond to different distributional assumptions."
Recurrent vs Recursive Neural Networks: Which is better for NLP?,"
There are Recurrent Neural Networks and Recursive Neural Networks. Both are usually denoted by the same acronym: RNN. According to Wikipedia, Recurrent NN are in fact Recursive NN, but I don't really understand the explanation.
Moreover, I don't seem to find which is better (with examples or so) for Natural Language Processing. The fact is that, although Socher uses Recursive NN for NLP in his tutorial, I can't find a good implementation of recursive neural networks, and when I search in Google, most of the answers are about Recurrent NN.
Besides that, is there another DNN which applies better for NLP, or it depends on the NLP task? Deep Belief Nets or Stacked Autoencoders? (I don't seem to find any particular util for ConvNets in NLP, and most of the implementations are with machine vision in mind).
Finally, I would really prefer DNN implementations for C++ (better yet if it has GPU support) or Scala (better if it has Spark support) rather than Python or Matlab/Octave.
I've tried Deeplearning4j, but it's under constant development and the documentation is a little outdated and I can't seem to make it work. Too bad because it has the ""black box"" like way of doing things, very much like scikit-learn or Weka, which is what I really want.
","['machine-learning', 'neural-networks', 'deep-learning', 'natural-language']","Recurrent Neural networks are recurring over time. For example if you have a sequencex = ['h', 'e', 'l', 'l']This sequence is fed to a single neuron which has a single connection to itself.At time step 0, the letter 'h' is given as input.At time step 1, 'e' is given as input. The network when unfolded over time will look like this.A recursive network is just a generalization of a recurrent network. In a recurrent network the weights are shared (and dimensionality remains constant) along the length of the sequence because how would you deal with position-dependent weights when you encounter a sequence at test-time of different length to any you saw at train-time. In a recursive network the weights are shared (and dimensionality remains constant) at every node for the same reason.This means that all the W_xh weights will be equal(shared) and so will be the W_hh weight. This is simply because it is a single neuron which has been unfolded in time.This is what a Recursive Neural Network looks like.
It is quite simple to see why it is called a Recursive Neural Network. Each parent node's children are simply a node similar to that node.The Neural network you want to use depends on your usage. In Karpathy's blog, he is generating characters one at a time so a recurrent neural network is good.But if you want to generate a parse tree, then using a Recursive Neural Network is better because it helps to create better hierarchical representations. If you want to do deep learning in c++, then use CUDA. It has a nice user-base, and is fast. I do not know more about that so cannot comment more.In python, Theano is the best option because it provides automatic differentiation, which means that when you are forming big, awkward NNs, you don't have to find gradients by hand. Theano does it automatically for you. This feature is lacked by Torch7. Theano is very fast as it provides C wrappers to python code and can be implemented on GPUs.  It also has an awesome user base, which is very important while learning something new."
"What is a difference between random effects-, fixed effects- and marginal model?","
I am trying to expand my knowledge of statistics. I come from a physical sciences background with a ""recipe based"" approach to statistical testing, where we say is it continuous, is it normally distributed -- OLS regression.
In my reading I have come across the terms: random effects model, fixed effects model, marginal model. My questions are:

In very simple terms, what are they? 
What are the differences between them?  
Are any of them synonyms? 
Where do the traditional tests like OLS regression, ANOVA and ANCOVA fall in this classification? 

Just trying to decide where to go next with the self study. 
","['random-effects-model', 'fixed-effects-model', 'marginal-distribution']","This question has been partially discussed at this site as below, and opinions seem mixed.All terms are generally related to longitudinal / panel / clustered / hierarchical data and repeated measures (in the format of advanced regression and ANOVA), but have multiple meanings in different context. I would like to answer the question in formulas based on my knowledge.Marginal model is generally compared to conditional model (random-effects model), and the former focuses on the population mean (take linear model for an example) $$E(y_{ij})=\boldsymbol x_{ij}^{'}\boldsymbol\beta,$$ while the latter deals with the conditional mean $$E(y_{ij}|\boldsymbol u_i)=\boldsymbol x_{ij}^{'}\boldsymbol\beta + \boldsymbol z_{ij}^{'}\boldsymbol u_i.$$ The interpretation and scale of the regression coefficients between marginal model and random-effects model would be different for nonlinear models (e.g. logistic regression). Let $h(E(y_{ij}|\boldsymbol u_i))=\boldsymbol x_{ij}^{'}\boldsymbol\beta + \boldsymbol z_{ij}^{'}\boldsymbol u_i$, then $$E(y_{ij})=E(E(y_{ij}|\boldsymbol u_i))=E(h^{-1}(\boldsymbol x_{ij}^{'}\boldsymbol\beta + \boldsymbol z_{ij}^{'}\boldsymbol u_i))\neq h^{-1}(\boldsymbol x_{ij}^{'}\boldsymbol\beta),$$ unless trivially the link function $h$ is the identity link (linear model), or $u_i=0$ (no random-effects). Good examples include generalized estimating equations (GEE; Zeger, Liang and Albert, 1988) and marginalized multilevel models (Heagerty and Zeger, 2000)."
"Apply word embeddings to entire document, to get a feature vector","
How do I use a word embedding to map a document to a feature vector, suitable for use with supervised learning?
A word embedding maps each word $w$ to a vector $v \in \mathbb{R}^d$, where $d$ is some not-too-large number (e.g., 500).  Popular word embeddings include word2vec and Glove.
I want to apply supervised learning to classify documents.  I'm currently mapping each document to a feature vector using the bag-of-words representation, then applying an off-the-shelf classifier.  I'd like replace the bag-of-words feature vector with something based on an existing pre-trained word embedding, to take advantage of the semantic knowledge that's contained in the word embedding.  Is there a standard way to do that?
I can imagine some possibilities, but I don't know if there's something that makes the most sense.  Candidate approaches I've considered:

I could compute the vector for each word in the document, and average all of them.  However, this seems like it might lose a lot of information.  For instance, with the bag-of-words representation, if there are a few words that are highly relevant to classification task and most words are irrelevant, the classifier can easily learn that; if I average the vectors for all the words in the document, the classifier has no chance.
Concatenating the vectors for all the words doesn't work, because it doesn't lead to a fixed-size feature vector.  Also it seems like a bad idea because it will be overly sensitive to the specific placement of a word.
I could use the word embedding to cluster the vocabulary of all words into a fixed set of clusters, say, 1000 clusters, where I use cosine similarity on the vectors as a measure of word similarity.  Then, instead of a bag-of-words, I could have a bag-of-clusters: the feature vector I supply to the classifer could be a 1000-vector, where the $i$th component counts the number of words in the document that are part of cluster $i$.
Given a word $w$, these word embeddings let me compute a set of the top 20 most similar words $w_1,\dots,w_{20}$ and their similarity score $s_1,\dots,s_{20}$.  I could adapt the bag-of-words-like feature vector using this.  When I see the word $w$, in addition to incrementing the element corresponding to word $w$ by $1$, I could also increment the element corresponding to word $w_1$ by $s_1$, increment the element corresponding to word $w_2$ by $s_2$, and so on.

Is there any specific approach that is likely to work well for document classification?

I'm not looking for paragraph2vec or doc2vec; those require training on a large data corpus, and I don't have a large data corpus.  Instead, I want to use an existing word embedding.
","['classification', 'natural-language', 'supervised-learning', 'word2vec', 'word-embeddings']",
How does LSTM prevent the vanishing gradient problem?,"
LSTM was invented specifically to avoid the vanishing gradient problem. It is supposed to do that with the Constant Error Carousel (CEC), which on the diagram below (from Greff et al.) correspond to the loop around cell.

(source: deeplearning4j.org)
And I understand that that part can be seen as a sort of identity function, so the derivative is one and the gradient stays constant.
What I don't understand is how it does not vanish due to the other activation functions ? The input, output and forget gates use a sigmoid, which derivative is at most 0.25, and g and h were traditionally tanh. How does backpropagating through those not make the gradient vanish ?
","['neural-networks', 'lstm']","The vanishing gradient is best explained in the one-dimensional case. The multi-dimensional is more complicated but essentially analogous. You can review it in this excellent paper [1].Assume we have a hidden state $h_t$ at time step $t$. If we make things simple and remove biases and inputs, we have
$$h_t = \sigma(w h_{t-1}).$$
Then you can show that \begin{align}
\frac{\partial h_{t'}}{\partial h_t} 
&= \prod_{k=1}^{t' - t} w \sigma'(w h_{t'-k})\\
&= \underbrace{w^{t' - t}}_{!!!}\prod_{k=1}^{t' - t} \sigma'(w h_{t'-k})
\end{align}
The factored marked with !!! is the crucial one. If the weight is not equal to 1, it will either decay to zero exponentially fast in $t'-t$, or grow exponentially fast.In LSTMs, you have the cell state $s_t$. The derivative there is of the form
$$\frac{\partial s_{t'}}{\partial s_t} = \prod_{k=1}^{t' - t} \sigma(v_{t+k}).$$
Here $v_t$ is the input to the forget gate. As you can see, there is no exponentially fast decaying factor involved. Consequently, there is at least one path where the gradient does not vanish. For the complete derivation, see [2].[1] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. ""On the difficulty of training recurrent neural networks."" ICML (3) 28 (2013): 1310-1318.[2] Bayer, Justin Simon. Learning Sequence Representations. Diss. München, Technische Universität München, Diss., 2015, 2015."
How to apply standardization/normalization to train- and testset if prediction is the goal?,"

Do I transform all my data or folds (if CV is applied) at the same time? e.g.
(allData - mean(allData)) / sd(allData)
Do I transform trainset and testset separately? e.g.
(trainData - mean(trainData)) / sd(trainData)
(testData - mean(testData)) / sd(testData)
Or do I transform trainset and use calculations on the testset? e.g.
(trainData - mean(trainData)) / sd(trainData)
(testData - mean(trainData)) / sd(trainData)

I believe 3 is the right way. If 3 is correct do I have to worry about the mean not being 0 or the range not being between [0; 1] or [-1; 1] (normalization) of the testset? 
","['r', 'cross-validation', 'data-transformation', 'normalization', 'standardization']","The third way is correct.  Exactly why is covered in wonderful detail in The Elements of Statistical Learning, see the section ""The Wrong and Right Way to Do Cross-validation"", and also in the final chapter of Learning From Data, in the stock market example.Essentially, procedures 1 and 2 leak information about either the response, or from the future, from your hold out data set into the training, or evaluation, of your model.  This can cause considerable optimism bias in your model evaluation.The idea in model validation is to mimic the situation you would be in when your model is making production decisions, when you do not have access to the true response.  The consequence is that you cannot use the response in the test set for anything except comparing to your predicted values.Another way to approach it is to imagine that you only have access to one data point from your hold out at a time (a common situation for production models).  Anything you cannot do under this assumption you should hold in great suspicion.  Clearly, one thing you cannot do is aggregate over all new data-points past and future to normalize your production stream of data - so doing the same for model validation is invalid.You don't have to worry about the mean of your test set being non-zero, that's a better situation to be in than biasing your hold out performance estimates.  Though, of course, if the test is truly drawn from the same underlying distribution as your train (an essential assumption in statistical learning), said mean should come out as approximately zero."
What is the root cause of the class imbalance problem?,"
I've been thinking a lot about the ""class imbalance problem"" in machine/statistical learning lately, and am drawing ever deeper into a feeling that I just don't understand what is going on.
First let me define (or attempt to) define my terms:

The class imbalance problem in machine/statistical learning is the observation that some binary classification(*) algorithms do not perform well when the proportion of 0 classes to 1 classes is very skewed.

So, in the above, for example, if there were one-hundred $0$ classes for every single $1$ class, I would say the class imbalance is $1$ to $100$, or $1\%$.
Most statements of the problem I have seen lack what I would think of as sufficient qualification (what models struggle, how imbalanced is a problem), and this is one source of my confusion.
A survey of the standard texts in machine/statistical learning turns up little:

Elements of Statistical Leaning and Introduction to Statistical Learning do not contain ""class imbalance"" in the index.

Machine Learning for Predictive Data Analytics also does not contain""class imbalance"" in the index.

Murphy's Machine Learning: A Probabilistic Perspective does contain ""class imbalance* in the index.  The reference is to a section on SVM's, where I found the following tantalizing comment:

It is worth remembering that all these difficulties, and the plethora of heuristics that have been proposed to fix them, fundamentally arise because SVM's do not model uncertainty using probabilities, so their output scores are not comparable across classes.



This comment does jive with my intuition and experience: at my previous job we would routinely fit logistic regressions and gradient boosted tree models (to minimize binomial log-likelihood) to unbalanced data (on the order of a $1\%$ class imbalance), with no obvious issues in performance.
I have read (somewhere) that classification tree based models (trees themselves and random forest) do also suffer from the class imbalance problem.  This muddies the waters a little bit, trees do, in some sense, return probabilities: the voting record for the target class in each terminal node of the tree.
So, to wrap up, what I'm really after is a conceptual understanding of the forces that lead to the class imbalance problem (if it exists).

Is it something we do to ourselves with badly chosen algorithms and lazy default classification thresholds?
Does it vanish if we always fit probability models that optimize proper scoring criteria?  Said differently, is the cause simply a poor choice of loss function, i.e. evaluating the predictive power of a model based on hard classification rules and overall accuracy?
If so, are models that do not optimize proper scoring rules then useless (or at least less useful)?

(*) By classification I mean any statistical model fit to binary response data.  I am not assuming that my goal is a hard assignment to one class or the other, though it may be.
","['classification', 'predictive-models', 'unbalanced-classes', 'scoring-rules', 'faq']","An entry from the Encyclopedia of Machine Learning (https://cling.csd.uwo.ca/papers/cost_sensitive.pdf) helpfully explains that what gets called ""the class imbalance problem"" is better understood as three separate problems:The authors explain:The class imbalanced datasets occurs in many real-world applications where the class distributions of data are highly imbalanced.  Again, without loss of generality, we assume that the minority or rare class is the positive class, and the majority class is the negative class. Often the minority class is very small, such as 1%of the dataset. If we apply most traditional (cost-insensitive) classifiers on the dataset, they will likely to predict everything as negative (the majority class). This was often regarded as a problem in learning from highly imbalanced datasets.However, as pointed out by (Provost, 2000), two fundamental assumptions are often made in the traditional cost-insensitive classifiers. The first is that the goal of the classifiers is to maximize the accuracy (or minimize the error rate); the second is that the class distribution of the training and test datasets is the same.  Under these two assumptions, predicting everything as negative for a highly imbalanced dataset is often the right thing to do. (Drummond and Holte, 2005) show that it is usually very difficult to outperform this simple classifier in this situation.Thus, the imbalanced class problem becomes meaningful only if one or both of the two assumptions above are not true; that is, if the cost of different types of error (false positive and false negative in the binary classification) is not the same, or if the class distribution in the test data is different from that of the training data. The first case can be dealt with effectively using methods in cost-sensitive meta-learning.In the case when the misclassification cost is not equal, it is usually more expensive to misclassify a minority (positive) example into the majority (negative) class, than a majority example into the minority class (otherwise it is more plausible to predict everything as negative).  That is, FN > FP. Thus, given the values of FN and FP, a variety of cost-sensitive meta-learning methods can be, and have been, used to solve the class imbalance problem (Ling and Li, 1998; Japkowicz and Stephen, 2002). If the values of FN and FP are not unknown explicitly, FN and FP can be assigned to be proportional to p(-):p(+) (Japkowicz and Stephen, 2002).In case the class distributions of training and test datasets are different (for example, if the training data is highly imbalanced but the test data is more balanced), an obvious approach is to sample the training data such that its class distribution is the same as the test data (by oversampling the minority class and/or undersampling the majority class)(Provost, 2000).Note that sometimes the number of examples of the minority class is too small for classifiers to learn adequately. This is the problem of insufficient (small) training data, different from that of the imbalanced datasets.Thus, as Murphy implies, there is nothing inherently problematic about using imbalanced classes, provided you avoid these three mistakes.  Models that yield posterior probabilities make it easier to avoid error (1) than do discriminant models like SVM because they enable you to separate inference from decision-making.  (See Bishop's section 1.5.4 Inference and Decision for further discussion of that last point.)Hope that helps."
Industry vs Kaggle challenges. Is collecting more observations and having access to more variables more important than fancy modelling?,"
I'd hope the title is self explanatory. In Kaggle, most winners use stacking with sometimes hundreds of base models, to squeeze a few extra % of MSE, accuracy... In general, in your experience, how important is fancy modelling such as stacking vs simply collecting more data and more features for the data?
","['large-data', 'stacking', 'collecting-data', 'kaggle']",
Neural networks vs support vector machines: are the second definitely superior?,"
Many authors of papers I read affirm SVMs is superior technique to face their regression/classification problem, aware that they couldn't get similar results through NNs. Often the comparison states that
SVMs, instead of NNs,

Have a strong founding theory 
Reach the global optimum due to quadratic programming
Have no issue for choosing a proper number of parameters
Are less prone to overfitting
Needs less memory to store the predictive model
Yield more readable results and a geometrical interpretation

Is it seriously a broadly accepted thought? Don't quote No-Free Lunch Theorem or similar statements, my question is about practical usage of those techniques.
On the other side, which kind of abstract problem you definitely would face with NN?
","['machine-learning', 'svm', 'neural-networks']","It is a matter of trade-offs. SVMs are in right now, NNs used to be in. You'll find a rising number of papers that claim Random Forests, Probabilistic Graphic Models or Nonparametric Bayesian methods are in. Someone should publish a forecasting model in the Annals of Improbable Research on what models will be considered hip.Having said that for many famously difficult supervised problems the best performing single models are some type of NN, some type of SVMs or a problem specific stochastic gradient descent method implemented using signal processing methods.Pros of NN:NN still benefit from feature engineering, e.g. you should have an area feature if you have a length and width. The model will perform better for the same computational effort.Most of supervised machine learning requires you to have your data structured in a observations by features matrix, with the labels as a vector of length observations. This restriction is not necessary with NN. There is fantastic work with structured SVM, but it is unlikely it will ever be as flexible as NNs.Pros of SVM:Fewer hyperparameters. Generally SVMs require less grid-searching to get a reasonably accurate model. SVM with a RBF kernel usually performs quite well.Global optimum guaranteed.Cons of NN and SVM:If you're going to accept a black box then you can usually squeeze out quite a bit more accuracy by bagging/stacking/boosting many many models with different trade-offs.Random forests are attractive because they can produce out-of-bag predictions(leave-one-out predictions) with no extra effort, they are very interpretable, they have an good bias-variance trade-off(great for bagging models) and they are relatively robust to selection bias. Stupidly simple to write a parallel implementation of.Probabilistic graphical models are attractive because they can incorporate domain-specific-knowledge directly into the model and are interpretable in this regard.Nonparametric(or really extremely parametric) Bayesian methods are attractive because they produce confidence intervals directly. They perform very well on small sample sizes and very well on large sample sizes. Stupidly simple to write a linear algebra implementation of."
Box-Cox like transformation for independent variables?,"
Is there a Box-Cox like transformation for independent variables?  That is, a transformation that optimizes the $x$ variable so that the y~f(x) will make a more reasonable fit for a linear model?
If so, is there a function to perform this with R?
","['r', 'regression', 'data-transformation', 'normality-assumption']","John Tukey advocated his ""three point method"" for finding re-expressions of variables to linearize relationships.I will illustrate with an exercise from his book, Exploratory Data Analysis.  These are mercury vapor pressure data from an experiment in which temperature was varied and vapor pressure was measured.The relation is strongly nonlinear: see the left panel in the illustration.Because this is an exploratory exercise, we expect it to be interactive.  The analyst is asked to begin by identifying three ""typical"" points in the plot: one near each end and one in the middle. I have done so here and marked them in red.  (When I first did this exercise long ago, I used a different set of points but arrived at the same results.)In the three point method, one searches--by brute force or otherwise--for a Box-Cox transformation that when applied to one of the coordinates--either y or x--will (a) place the typical points approximately on a line and (b) uses a ""nice"" power, usually chosen from a ""ladder"" of powers that might be interpretable by the analyst.For reasons that will become apparent later, I have extended the Box-Cox family by allowing an ""offset"" so that the transformations are in the form$$x \to \frac{(x + \alpha)^\lambda - 1}{\lambda}.$$Here's a quick and dirty R implementation.  It first finds an optimal $(\lambda,\alpha)$ solution, then rounds $\lambda$ to the nearest value on the ladder and, subject to that restriction, optimizes $\alpha$ (within reasonable limits).  It's incredibly quick because all the calculations are based on just those three typical points out of the original dataset.  (You could do them with pencil and paper, even, which is exactly what Tukey did.)When the three-point method is applied to the pressure (y) values in the mercury vapor dataset, we obtain the middle panel of the plots.In this case, parms turns out to equal $(0,0)$: the method elects to log-transform the pressure.We have reached a point analogous to the context of the question: for whatever reason (usually to stabilize residual variance), we have re-expressed the dependent variable, but we find that the relation with an independent variable is nonlinear. So now we turn to re-expressing the independent variable in an effort to linearize the relation.  This is done in the same way, merely reversing the roles of x and y:The values of parms for the independent variable (temperature) are found to be $(-1, 253.75)$: in other words, we should express the temperature as degrees Celsius above $-254$C and use its reciprocal (the $-1$ power).  (For technical reasons, the Box-Cox transformation further adds $1$ to the result.)  The resulting relation is shown in the right panel.By now, anybody with the least science background has recognized that the data are ""telling"" us to use absolute temperatures--where the offset is $273$ instead of $254$--because those will be physically meaningful.  (When the last plot is re-drawn using an offset of $273$ instead of $254$, there is little visible change.  A physicist would then label the x-axis with $1/(1-x)$: that is, reciprocal absolute temperature.)This is a nice example of how statistical exploration needs to interact with understanding of the subject of investigation. In fact, reciprocal absolute temperatures show up all the time in physical laws.  Consequently, using simple EDA methods alone to explore this century-old, simple, dataset, we have rediscovered the Clausius-Clapeyron relation: the logarithm of the vapor pressure is a linear function of the reciprocal absolute temperature.  Not only that, we have a not very bad estimate of absolute zero ($-254$ degrees C), from the slope of the righthand plot we can calculate the specific enthalpy of vaporization, and--as it turns out--a careful analysis of the residuals identifies an outlier (the value at a temperature of $0$ degrees C), shows us how the enthalphy of vaporization varies (very slightly) with temperature (thereby violating the Ideal Gas Law), and ultimately can give us accurate information about the effective radius of the mercury gas molecules!  All that from 19 data points and some basic skills in EDA."
What is the difference between posterior and posterior predictive distribution?,"
I understand what a Posterior is, but I'm not sure what the latter means?
How are the 2 different?
Kevin P Murphy indicated in his textbook, Machine Learning: a Probabilistic Perspective, that it is ""an internal belief state"". What does that really mean? I was under the impression that a Prior represents your internal belief or bias, where am I going wrong?
","['posterior', 'definition']","The simple difference between the two is that the posterior distribution depends on the unknown parameter $\theta$, i.e., the posterior distribution is:
$$p(\theta|x)=c\times p(x|\theta)p(\theta)$$
where $c$ is the normalizing constant.While on the other hand, the posterior predictive distribution does not depend on the unknown parameter $\theta$ because it has been integrated out, i.e., the posterior predictive distribution is:
$$p(x^*|x)=\int_\Theta c\times p(x^*,\theta|x)d\theta=\int_\Theta c\times p(x^*|\theta)p(\theta|x)d\theta$$where $x^*$ is a new unobserved random variable and is independent of $x$.I won't dwell on the posterior distribution explanation since you say you understand it but the posterior distribution ""is the distribution of an unknown quantity, treated as a random variable, conditional on the evidence obtained"" (Wikipedia). So basically its the distribution that explains your unknown, random, parameter. On the other hand, the posterior predictive distribution has a completely different meaning in that it is the distribution for future predicted data based on the data you have already seen. So the posterior predictive distribution is basically used to predict new data values.If it helps, is an example graph of a posterior distribution and a posterior predictive distribution:"
"Why do statisticians say a non-significant result means ""you can't reject the null"" as opposed to accepting the null hypothesis?","
Traditional statistical tests, like the two sample t-test, focus on trying to eliminate the hypothesis that there is no difference between a function of two independent samples. Then, we choose a confidence level and say that if the difference of means is beyond the 95% level, we can reject the null hypothesis. If not, we ""can't reject the null hypothesis"". This seems to imply that we can't accept it either. Does it mean we're not sure if the null hypothesis is true? 
Now, I want to design a test where my hypothesis is that a function of two samples is the same (which is the opposite of traditional statistics tests where the hypothesis is that the two samples are different). So, my null hypothesis becomes that the two samples are different. How should I design such a test? Will it be as simple as saying that if the p-value is lesser than 5% we can accept the hypothesis that there is no significant difference? 
","['hypothesis-testing', 'statistical-significance', 'confidence-interval', 'equivalence', 'tost']","Traditionally, the null hypothesis is a point value.  (It is typically $0$, but can in fact be any point value.)  The alternative hypothesis is that the true value is any value other than the null value.  Because a continuous variable (such as a mean difference) can take on a value which is indefinitely close to the null value but still not quite equal and thus make the null hypothesis false, a traditional point null hypothesis cannot be proven.  Imagine your null hypothesis is $0$, and the mean difference you observe is $0.01$.  Is it reasonable to assume the null hypothesis is true?  You don't know yet; it would be helpful to know what our confidence interval looks like.  Let's say that your 95% confidence interval is $(-4.99,\ 5.01)$.  Now, should we conclude that the true value is $0$?  I would not feel comfortable saying that, because the CI is very wide, and there are many, large non-zero values that we might reasonably suspect are consistent with our data.  So let's say we gather much, much more data, and now our observed mean difference is $0.01$, but the 95% CI is $(0.005,\ 0.015)$.  The observed mean difference has stayed the same (which would be amazing if it really happened), but the confidence interval now excludes the null value.  Of course, this is just a thought experiment, but it should make the basic ideas clear.  We can never prove that the true value is any particular point value; we can only (possibly) disprove that it is some point value.  In statistical hypothesis testing, the fact that the p-value is > 0.05 (and that the 95% CI includes zero) means that we are not sure if the null hypothesis is true.As for your concrete case, you cannot construct a test where the alternative hypothesis is that the mean difference is $0$ and the null hypothesis is anything other than zero.  This violates the logic of hypothesis testing.  It is perfectly reasonable that it is your substantive, scientific hypothesis, but it cannot be your alternative hypothesis in a hypothesis testing situation.  So what can you do?  In this situation, you use equivalence testing.  (You might want to read through some of our threads on this topic by clicking on the equivalence tag.)  The typical strategy is to use the two one sided tests approach.  Very briefly, you select an interval within which you would consider that the true mean difference might as well be $0$ for all you could care, then you perform a one-sided test to determine if the observed value is less than the upper bound of that interval, and another one-sided test to see if it is greater than the lower bound.  If both of these tests are significant, then you have rejected the hypothesis that the true value is outside the interval you care about.  If one (or both) are non-significant, you fail to reject the hypothesis that the true value is outside the interval.  For example, suppose anything within the interval $(-0.02,\ 0.02)$ is so close to zero that you think it is essentially the same as zero for your purposes, so you use that as your substantive hypothesis.  Now imagine that you get the first result described above.  Although $0.01$ falls within that interval, you would not be able to reject the null hypothesis on either one-sided t-test, so you would fail to reject the null hypothesis.  On the other hand, imagine that you got the second result described above.  Now you find that the observed value falls within the designated interval, and it can be shown to be both less than the upper bound and greater than the lower bound, so you can reject the null.  (It is worth noting that you can reject both the hypothesis that the true value is $0$, and the hypothesis that the true value lies outside of the interval $(-0.02,\ 0.02)$, which may seem perplexing at first, but is fully consistent with the logic of hypothesis testing.)  "
Who are frequentists?,"
We already had a thread asking who are Bayesians and one asking if frequentists are Bayesians, but there was no thread asking directly who are frequentists? This is a question that was asked by @whuber as a comment to this thread and it begs to be answered. Do they exist (are there any self-identified frequentists)? Maybe they were just made-up by Bayesians who needed a scapegoat to blame when criticizing the mainstream statistics?
Meta-comment to the answers that were already given: As contrast, Bayesian statistics are not only defined in terms of using Bayes theorem (non-Bayesians also use it), nor about using subjectivist interpretation of probability (you wouldn't call any layperson saying things like ""I bet the chance is less than 50:50!"" a Bayesian) - so can we define frequentism only in terms of adopted interpretation of probability? Moreover, statistics $\ne$ applied probability, so should definition of frequentism be focused solely on the interpretation of probability?
","['bayesian', 'frequentist']",
What is the difference between N and N-1 in calculating population variance?,"
I did not get the why there are N and N-1 while calculating population variance. When we use N and when we use N-1? 

Click here for a larger version
It says that when population is very big there is no difference between N and N-1 but it does not tell why is there N-1 at the beginning.
Edit: Please don't confuse with n and n-1 which are used in estimating.
Edit2: I'm not talking about population estimation.
","['variance', 'population']",
Practical hyperparameter optimization: Random vs. grid search,"
I'm currently going through Bengio's and Bergstra's Random Search for Hyper-Parameter Optimization [1] where the authors claim random search is more efficient than grid search in achieving approximately equal performance.
My question is: Do people here agree with that claim? In my work I've been using grid search mostly because of the lack of tools available to perform random search easily.
What is the experience of people using grid vs. random search?
","['machine-learning', 'hyperparameter', 'optimization']","Random search has a probability of 95% of finding a combination of parameters within the 5% optima with only 60 iterations. Also compared to other methods it doesn't bog down in local optima.Check this great blog post at Dato by Alice Zheng, specifically the section Hyperparameter tuning algorithms.I love movies where the underdog wins, and I love machine learning
papers where simple solutions are shown to be surprisingly effective.
This is the storyline of “Random search for hyperparameter
optimization” by Bergstra and Bengio. [...] Random search wasn’t taken
very seriously before. This is because it doesn’t search over all the
grid points, so it cannot possibly beat the optimum found by grid
search. But then came along Bergstra and Bengio. They showed that, in
surprisingly many instances, random search performs about as well as
grid search. All in all, trying 60 random points sampled from the grid
seems to be good enough.In hindsight, there is a simple probabilistic explanation for the
result: for any distribution over a sample space with a finite
maximum, the maximum of 60 random observations lies within the top 5%
of the true maximum, with 95% probability. That may sound complicated,
but it’s not. Imagine the 5% interval around the true maximum. Now
imagine that we sample points from his space and see if any of it
lands within that maximum. Each random draw has a 5% chance of landing
in that interval, if we draw n points independently, then the
probability that all of them miss the desired interval is
$\left(1−0.05\right)^{n}$. So the probability that at least one of
them succeeds in hitting the interval is 1 minus that quantity. We
want at least a .95 probability of success. To figure out the number
of draws we need, just solve for n in the equation:$$1−\left(1−0.05\right)^{n}>0.95$$We get $n\geqslant60$. Ta-da!The moral of the story is: if the close-to-optimal region of
hyperparameters occupies at least 5% of the grid surface, then random
search with 60 trials will find that region with high probability.You can improve that chance with a higher number of trials.All in all, if you have too many parameters to tune, grid search may become unfeasible. That's when I try random search."
Confidence interval for Bernoulli sampling,"
I have a random sample of Bernoulli random variables $X_1 ... X_N$, where $X_i$ are i.i.d. r.v. and $P(X_i = 1) = p$, and $p$ is an unknown parameter.
Obviously, one can find an estimate for $p$: $\hat{p}:=(X_1+\dots+X_N)/N$.
My question is how can I build a confidence interval for $p$?
","['confidence-interval', 'binomial-distribution', 'bernoulli-distribution', 'faq']",
Relationship between $R^2$ and correlation coefficient,"
Let's say I have two 1-dimensional arrays, $a_1$ and $a_2$. Each contains 100 data points. $a_1$ is the actual data, and $a_2$ is the model prediction. In this case, the $R^2$ value would be:
$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}} \quad\quad\quad\quad\quad\ \ \quad\quad(1). 
$$
In the meantime, this would be equal to the square value of the correlation coefficient,
$$
R^2 = (\text{Correlation Coefficient})^2 \quad (2).
$$
Now if I swap the two: $a_2$ is the actual data, and $a_1$ is the model prediction. From equation $(2)$, because correlation coefficient does not care which comes first, the $R^2$ value would be the same. However, from equation $(1)$, $SS_{tot}=\sum_i(y_i - \bar y )^2$, the $R^2$ value will change, because the $SS_{tot}$ has changed if we switch $y$ from $a_1$ to $a_2$; in the meantime, $SS_{res}=\sum_i(y_i -f_i)^2$ does not change.
My question is: How can these contradict each other?
Edit:

I was wondering that, will the relationship in Eq. (2) still stand, if it is not a simple linear regression, i.e., the relationship between IV and DV is not linear (could be exponential / log)?

Will this relationship still stand, if the sum of the prediction errors does not equal zero?


","['correlation', 'r-squared']","This is true that $SS_{tot}$ will change ... but you forgot the fact that the regression sum of of squares will change as well. So let's consider the simple regression model and denote the  Correlation Coefficient as $r_{xy}^2=\dfrac{S_{xy}^2}{S_{xx}S_{yy}}$, where I used the sub-index $xy$ to emphasize the fact that $x$ is the independent variable and $y$ is the dependent variable. Obviously, $r_{xy}^2$ is unchanged if you swap $x$ with $y$. We can easily show that $SSR_{xy}=S_{yy}(R_{xy}^2)$, where $SSR_{xy}$ is the regression sum of of squares and  $S_{yy}$ is the total sum of squares where $x$ is independent and $y$ is dependent variable. Therefore: $$R_{xy}^2=\dfrac{SSR_{xy}}{S_{yy}}=\dfrac{S_{yy}-SSE_{xy}}{S_{yy}},$$ where $SSE_{xy}$ is the corresponding residual  sum of of squares  where $x$ is independent and $y$ is dependent variable. Note that in this case, we have $SSE_{xy}=b^2_{xy}S_{xx}$ with $b=\dfrac{S_{xy}}{S_{xx}}$ (See e.g. Eq. (34)-(41) here.) Therefore: $$R_{xy}^2=\dfrac{S_{yy}-\dfrac{S^2_{xy}}{S^2_{xx}}.S_{xx}}{S_{yy}}=\dfrac{S_{yy}S_{xx}-S^2_{xy}}{S_{xx}.S_{yy}}.$$ Clearly above equation is symmetric with respect to $x$ and $y$. In other words: $$R_{xy}^2=R_{yx}^2.$$ To summarize when you change $x$ with $y$ in the simple regression model, both numerator and denominator of $R_{xy}^2=\dfrac{SSR_{xy}}{S_{yy}}$ will change in a way that $R_{xy}^2=R_{yx}^2.$"
Introduction to statistics for mathematicians,"
What is a good introduction to statistics for a mathematician who is already well-versed in probability?  I have two distinct motivations for asking, which may well lead to different suggestions:

I'd like to better understand the statistics motivation behind many problems considered by probabilists.
I'd like to know how to better interpret the results of Monte Carlo simulations which I sometimes do to form mathematical conjectures.

I'm open to the possibility that the best way to go is not to look for something like ""Statistics for Probabilists"" and just go to a more introductory source.
",['references'],"As you said, it's not necessarily the case that a mathematician may want a rigorous book. Maybe the goal is to get some intuition of the concepts quickly, and then fill in the details. I recommend two books from CMU professors, both published by Springer: ""All of Statistics"" by Larry Wasserman is quick and informal. ""Theory of Statistics"" by Mark Schervish is rigorous and relatively complete. It has decision theory, finite sample, some asymptotics and sequential analysis.Added 7/28/10: There is one additional reference that is orthogonal to the other two: very rigorous, focused on learning theory, and short. It's by Smale (Steven Smale!) and Cucker, ""On the Mathematical Foundations of Learning"". Not easy read, but the best crash course on the theory."
Resources for learning Markov chain and hidden Markov models,"
I am looking for resources (tutorials, textbooks, webcast, etc) to learn about Markov Chain and HMMs. My background is as a biologist, and I'm currently involved in a bioinformatics-related project. 
Also, what are the necessary mathematical background I need to have a sufficient understanding of Markov models & HMMs?
I've been looking around using Google but, so far I have yet to find a good introductory tutorial. I'm sure somebody here knows better.
","['references', 'markov-process', 'hidden-markov-model', 'bioinformatics']",
What is the difference between a Normal and a Gaussian Distribution,"
Is there a deep difference between a Normal and a Gaussian distribution, I've seen many papers using them without distinction, and I usually also refer to them as the same thing.
However, my PI recently told me that a normal is the specific case of the Gaussian with mean=0 and std=1, which I also heard some time ago in another outlet, what is the consensus on this?
According to Wikipedia, what they call the normal, is the standard normal distribution, while the Normal is a synonym for the Gaussian, but then again, I'm not sure about Wikipedia either.
Thanks
","['normal-distribution', 'terminology']",Wikipedia is right. The Gaussian is the same as the normal. Wikipedia can usually be trusted on this sort of question. 
ANOVA assumption normality/normal distribution of residuals,"
The Wikipedia page on ANOVA lists three assumptions, namely:

Independence of cases – this is an assumption of the model that simplifies the statistical analysis.
Normality – the distributions of the residuals are normal.
Equality (or ""homogeneity"") of variances, called homoscedasticity...

Point of interest here is the second assumption. Several sources list the assumption differently. Some say normality of the raw data, some claim of residuals.
Several questions pop up:

are normality and normal distribution of residuals the same person (based on Wikipedia entry, I would claim normality is a property, and does not pertain residuals directly (but can be a property of residuals (deeply nested text within brackets, freaky)))?
if not, which assumption should hold? One? Both?
if the assumption of normally distributed residuals is the right one, are we making a grave mistake by checking only the histogram of raw values for normality?

","['anova', 'residuals', 'normality-assumption', 'assumptions', 'faq']","Let's assume this is a fixed effects model.  (The advice doesn't really change for random-effects models, it just gets a little more complicated.)First let us distinguish the ""residuals"" from the ""errors:"" the former are the differences between the responses and their predicted values, while the latter are random variables in the model.  With sufficiently large amounts of data and a good fitting procedure, the distributions of the residuals will approximately look like the residuals were drawn randomly from the error distribution (and will therefore give you good information about the properties of that distribution).The assumptions, therefore, are about the errors, not the residuals.No, normality (of the responses) and normal distribution of errors are not the same.  Suppose you measured yield from a crop with and without a fertilizer application.  In plots without fertilizer the yield ranged from 70 to 130.  In two plots with fertilizer the yield ranged from 470 to 530.  The distribution of results is strongly non-normal: it's clustered at two locations related to the fertilizer application.  Suppose further the average yields are 100 and 500, respectively.  Then all residuals range from -30 to +30, and so the errors will be expected to have a comparable distribution.  The errors might (or might not) be normally distributed, but obviously this is a completely different distribution.The distribution of the residuals matters, because those reflect the errors, which are the random part of the model.  Note also that the p-values are computed from F (or t) statistics and those depend on residuals, not on the original values.If there are significant and important effects in the data (as in this example), then you might be making a ""grave"" mistake.  You could, by luck, make the correct determination: that is, by looking at the raw data you will seeing a mixture of distributions and this can look normal (or not).  The point is that what you're looking it is not relevant.ANOVA residuals don't have to be anywhere close to normal in order to fit the model.  However, unless you have an enormous amount of data, near-normality of the residuals is essential for p-values computed from the F-distribution to be meaningful."
Should I normalize word2vec's word vectors before using them?,"
After training word vectors with word2vec, is it better to normalize them before using them for some downstream applications? I.e what are the pros/cons of normalizing them?
","['natural-language', 'word2vec', 'word-embeddings']",
Can a random forest be used for feature selection in multiple linear regression?,"
Since RF can handle non-linearity but can't provide coefficients, would it be wise to use random forest to gather the most important features and then plug those features into a multiple linear regression model in order to obtain their coefficients? 
","['regression', 'machine-learning', 'feature-selection', 'random-forest', 'regression-strategies']","Since RF can handle non-linearity but can't provide coefficients, would it be wise to use Random Forest to gather the most important Features and then plug those features into a Multiple Linear Regression model in order to explain their signs?I interpret OP's one-sentence question to mean that OP wishes to understand the desirability of the following analysis pipeline:I don't think this pipeline will accomplish what you'd like. Variables that are important in random forest don't necessarily have any sort of linearly additive relationship with the outcome. This remark shouldn't be surprising: it's what makes random forest so effective at discovering nonlinear relationships.Here's an example. I created a classification problem with 10 noise features, two ""signal"" features, and a circular decision boundary. And when we apply the RF model, we are not surprised to find that these features are easily picked out as important by the model. (NB: this model isn't tuned at all.)But when we down-select to just these two, useful features, the resulting linear model is awful.The important part of the summary is the comparison of the residual deviance and the null deviance. We can see that the model does basically nothing to ""move"" the deviance. Moreover, the coefficient estimates are essentially zero.What accounts for the wild difference between the two models? Well, clearly the decision boundary we're trying to learn is not a linear function of the two ""signal"" features. Obviously if you knew the functional form of the decision boundary prior to estimating the regression, you could apply some transformation to encode the data in a way that regression could then discover... (But I've never known the form of the boundary ahead of time in any real-world problem.) Since we're only working with two signal features in this case, a synthetic data set without  noise in the class labels, that boundary between classes is very obvious in our plot. But it's less obvious when working with real data in a realistic number of dimensions.Moreover, in general, random forest can fit different models to different subsets of the data. In a more complicated example, it won't be obvious what's going on from a single plot at all, and building a linear model of similar predictive power will be even harder. Because we're only concerned with two dimensions, we can make a prediction surface. As expected, the random model learns that the neighborhood around the origin is important.As implied by our abysmal model output, the prediction surface for the reduced-variable logistic regression model is basically flat.HongOoi notes that the class membership isn't a linear function of the features, but that it a linear function is under a transformation. Because the decision boundary is  $1=x_1^2+x_2^2,$ if we square these features, we will be able to build a more useful linear model. This is deliberate. While the RF model can find signal in those two features without transformation, the analyst has to be more specific to get similarly helpful results in the GLM. Perhaps that's sufficient for OP: finding a useful set of transformations for 2 features is easier than 12. But my point is that even if a transformation will yield a useful linear model, RF feature importance won't suggest the transformation on its own."
Examples where method of moments can beat maximum likelihood in small samples?,"
Maximum likelihood estimators (MLE) are asymptotically efficient; we see the practical upshot in that they often do better than method of moments (MoM) estimates (when they differ), even at small sample sizes 
Here 'better than' means in the sense of typically having smaller variance when both are unbiased, and typically smaller mean square error (MSE) more generally.
The question, occurs, however: 
Are there cases where the MoM can beat the MLE - on MSE, say - in small samples?
(where this isn't some odd/degenerate situation - i.e. given that conditions for ML to exist/be asymptotically efficient hold) 
A followup question would then be 'how big can small be?' - that is, if there are examples, are there some which still hold at relatively large sample sizes, perhaps even all finite sample sizes? 
[I can find an example of a biased estimator that can beat ML in finite samples, but it isn't MoM.]

Note added retrospectively: my focus here is primarily on the univariate case (which is actually where my underlying curiosity is coming from). I don't want to rule out multivariate cases, but I also don't particularly want to stray into extended discussions of James-Stein estimation. 
","['estimation', 'maximum-likelihood', 'mse', 'method-of-moments', 'efficiency']","This may be considered... cheating, but the OLS estimator is a MoM estimator. Consider a standard linear regression specification (with $K$ stochastic regressors, so magnitudes are conditional on the regressor matrix), and a sample of size $n$. Denote $s^2$ the OLS estimator of the variance $\sigma^2$ of the error term. It is unbiased so$$ MSE(s^2) = \operatorname {Var}(s^2) = \frac {2\sigma^4}{n-K} $$Consider now the MLE of $\sigma^2$. It is$$\hat \sigma^2_{ML} = \frac {n-K}{n}s^2$$
Is it biased. Its MSE is$$MSE (\hat \sigma^2_{ML}) = \operatorname {Var}(\hat \sigma^2_{ML}) + \Big[E(\hat \sigma^2_{ML})-\sigma^2\Big]^2$$
Expressing the MLE in terms of the OLS and using the expression for the OLS estimator variance we obtain$$MSE (\hat \sigma^2_{ML}) = \left(\frac {n-K}{n}\right)^2\frac {2\sigma^4}{n-K} + \left(\frac {K}{n}\right)^2\sigma^4$$
$$\Rightarrow MSE (\hat \sigma^2_{ML}) = \frac {2(n-K)+K^2}{n^2}\sigma^4$$We want the conditions (if they exist) under which$$MSE (\hat \sigma^2_{ML}) > MSE (s^2) \Rightarrow \frac {2(n-K)+K^2}{n^2} > \frac {2}{n-K}$$$$\Rightarrow 2(n-K)^2+K^2(n-K)> 2n^2$$
$$ 2n^2 -4nK + 2K^2 +nK^2 - K^3 > 2n^2 $$
Simplifying we obtain
$$ -4n + 2K +nK - K^2 > 0 \Rightarrow K^2 - (n+2)K + 4n < 0 $$
Is it feasible for this quadratic in $K$ to obtain negative values? We need its discriminant to be positive. We have
$$\Delta_K = (n+2)^2 -16n = n^2 + 4n + 4 - 16n = n^2 -12n + 4$$
which is another quadratic, in $n$ this time. This discriminant is
$$\Delta_n = 12^2 - 4^2 = 8\cdot 16$$
so
$$n_1,n_2 = \frac {12\pm \sqrt{8\cdot 16}}{2} = 6 \pm 4\sqrt2 \Rightarrow n_1,n_2 = \{1, 12\}$$
to take into account the fact that $n$ is an integer. If $n$ is inside this interval we have that  $\Delta_K <0$ and the quadratic in $K$ takes always positive values, so we cannot obtain the required inequality. So: we need a sample size larger than 12. Given this the roots for $K$-quadratic are$$K_1, K_2 = \frac {(n+2)\pm \sqrt{n^2 -12n + 4}}{2} = \frac n2 +1 \pm \sqrt{\left(\frac n2\right)^2 +1 -3n}$$Overall : for sample size $n>12$ and number of regressors $K$ such that $\lceil K_1\rceil <K<\lfloor K_2\rfloor $
we have
$$MSE (\hat \sigma^2_{ML}) > MSE (s^2)$$
For example, if $n=50$ then one finds that the number of regressors must be $5<K<47$ for the inequality to hold. It is interesting that for small numbers of regressors the MLE is better in MSE sense.ADDENDUM
The equation for the roots of the $K$-quadratic can be written$$K_1, K_2 = \left(\frac n2 +1\right) \pm \sqrt{\left(\frac n2 +1\right)^2  -4n}$$
which by a quick look I think implies that the lower root will always be $5$ (taking into account the ""integer-value"" restriction) -so MLE will be MSE-efficient when regressors are up to $5$ for any (finite) sample size."
Explanation of min_child_weight in xgboost algorithm,"
The definition of the min_child_weight parameter in xgboost is given as the:

minimum sum of instance weight (hessian) needed in a child. If the
  tree partition step results in a leaf node with the sum of instance
  weight less than min_child_weight, then the building process will give
  up further partitioning. In linear regression mode, this simply
  corresponds to minimum number of instances needed to be in each node.
  The larger, the more conservative the algorithm will be.

I have read quite a few things on xgboost including the original paper (see formula 8 and the one just after equation 9), this question and most things to do with xgboost that appear on the first few pages of a google search. ;)
Basically I'm still not happy as to why we are imposing a constraint on the sum of the hessian? My only thought at the minute from the original paper is that it relates to the weighted quantile sketch section (and the reformulation as of equation 3 weighted squared loss) which has $h_i$ as the 'weight' of each instance.
A further question relates to why it is simply the number of instances in linear regression mode? I guess this is related to the second derivative of the sum of squares equation?
","['machine-learning', 'boosting', 'hessian']","For a regression, the loss of each point in a node is$\frac{1}{2}(y_i - \hat{y_i})^2$The second derivative of this expression with respect to $\hat{y_i}$ is $1$. So when you sum the second derivative over all points in the node, you get the number of points in the node. Here, min_child_weight means something like ""stop trying to split once your sample size in a node goes below a given threshold"". For a binary logistic regression, the hessian for each point in a node is going to contain terms like$\sigma(\hat{y_i})(1 - \sigma(\hat{y_i}))$where $\sigma$ is the sigmoid function. Say you're at a pure node (e.g., all of the training examples in the node are 1's). Then all of the $\hat{y_i}$'s will probably be large positive numbers, so all of the $\sigma(\hat{y_i})$'s will be near 1, so all of the hessian terms will be near 0. Similar logic holds if all of the training examples in the node are 0. Here, min_child_weight means something like ""stop trying to split once you reach a certain degree of purity in a node and your model can fit it"". The Hessian's a sane thing to use for regularization and limiting tree depth. For regression, it's easy to see how you might overfit if you're always splitting down to nodes with, say, just 1 observation. Similarly, for classification, it's easy to see how you might overfit if you insist on splitting until each node is pure."
Is it unusual for the MEAN to outperform ARIMA?,"
I recently applied a range of forecasting methods (MEAN, RWF, ETS, ARIMA and MLPs) and found that MEAN did surprisingly well. (MEAN: where all future predictions are predicted as been equal to the arithmetic mean of the observed values.) MEAN even outperformed ARIMA on the three series I used.
What I want to know is if this is unusual? Does this mean the times series I'm using are strange? Or does this indicate that I've set something up wrong?
","['forecasting', 'arima']","I'm a practitioner, both producer and user of forecasting and NOT a trained statistician. Below I share some of my thoughts on why your mean forecast turned out better than ARIMA by referring to research article that rely on empirical evidence.  One book that time and time again I go back to refer is the Principles of Forecasting book by Armstrong and its website which I would recommend as an excellent read for any forecaster, provides great insight on usage and guiding principles of extrapolation methods. To answer you first question  - What I want to know is if this is unusual? There is a chapter called Extrapolation for Time-Series and Cross-Sectional Data which also available free in the same website. The following is the quote from the chapter""For example, in the real-time M2-competition, which examined 29
  monthly series, Box-Jenkins proved to be one of the least-accurate
  methods and its overall median error was 17% greater than that for a
  naive forecast""There lies an empirical evidence on why your mean forecasts was better than ARIMA models. There is also been study after study in empirical competitions and the third M3 competition that show Box - Jenkins ARIMA approach fails to produce accurate forecast and lacks evidence that it performs better for univariate trend extrapolation.There is also another paper and an ongoing study by Greene and Armstrong entitled ""Simple Forecasting: Avoid Tears Before Bedtime"" in the same website. The authors of the paper summarize as follows: In total we identified 29 papers incorporating 94 formal comparisons
  of the  accuracy of forecasts from complex methods with those from
  simple—but not in all cases  sophisticatedly simple—methods.
  Eighty-three percent of the comparisons found that forecasts from
  simple methods were more accurate than, or similarly accurate to,
  those  from complex methods. On average, the errors of forecasts from
  complex methods were about 32 percent greater than the errors of
  forecasts from simple methods in the 21 studies that provide
  comparisons of errorsTo answer your third question: does this indicate that I've set something up wrong?
No, I would aconsider ARIMA as complex method and Mean forecast as simple methods. There is ample evidence that simple methods like Mean forecast outperform complex methods like ARIMA.To answer your second question: Does this mean the times series I'm using are strange?Below are what I considered to be experts in real world forecasting:All of the above researchers advocate, simplicity (methods like your mean forecast)   vs. Complex methods like ARIMA. So you should feel comfortable that your forecasts are good and always favor simplicity over complexity based on empirical evidence. These researchers have  all contributed immensely to the field of applied forecasting.In addition to Stephan's good list of simple forecasting method. there is also another method called Theta forecasting method which is a very simple method (basically Simple Exponential smoothing with a drift that equal 1/2 the slope of linear regression)  I would add this to your toolbox. Forecast package in R implements this method."
How does centering the data get rid of the intercept in regression and PCA?,"
I keep reading about instances where we center the data (e.g., with regularization or PCA) in order to remove the intercept (as mentioned in this question). I know it's simple, but I'm having a hard time intuitively understanding this. Could someone provide the intuition or a reference I can read?
","['regression', 'pca', 'centering']","Can these pictures help?The first 2 pictures are about regression. Centering the data does not alter the slope of regression line, but it makes intercept equal 0.The pictures below are about PCA. PCA is a regressional model without intercept$^1$. Thus, principal components inevitably come through the origin. If you forget to center your data, the 1st principal component may pierce the cloud not along the main direction of the cloud, and will be (for statistics purposes) misleading.$^1$ PCA isn't a regression analysis, of course. It however shares formally same linear equation (linear combination) with linear regression. PCA equation is like linear regression equation without intercept - because PCA is a rotation operation."
Alternatives to logistic regression in R,"
I would like as many algorithms that perform the same task as logistic regression.  That is  algorithms/models that can give a prediction to a binary response (Y) with some explanatory variable (X).
I would be glad if after you name the algorithm, if you would also show how to implement it in R.  Here is a code that can be updated with other models:
set.seed(55)
n <- 100
x <- c(rnorm(n), 1+rnorm(n))
y <- c(rep(0,n), rep(1,n))
r <- glm(y~x, family=binomial)
plot(y~x)
abline(lm(y~x), col='red', lty=2)
xx <- seq(min(x), max(x), length=100)
yy <- predict(r, data.frame(x=xx), type='response')
lines(xx, yy, col='blue', lwd=5, lty=2)
title(main='Logistic regression with the ""glm"" function')

","['r', 'regression', 'logistic', 'classification', 'predictive-models']","Popular right now are randomForest and gbm (called MART or Gradient Boosting in machine learning literature), rpart for simple trees.  Also popular is bayesglm, which uses MAP with priors for regularization."
Regression when the OLS residuals are not normally distributed,"
There are several threads on this site discussing how to determine if the OLS residuals are asymptotically normally distributed. Another way to evaluate the normality of the residuals with R code is provided in this excellent answer. This is another discussion on the practical difference between standardized and observed residuals.
But let's say the residuals are definitely not normally distributed, like in this example. Here we have several thousand observations and clearly we must reject the normally-distributed-residuals assumption. One way to address the problem is to employ some form of robust estimator as explained in the answer. However I am not limited to OLS and in facts I would like to understand the benefits of other glm or non-linear methodologies.
What is the most efficient way to model data violating the OLS normality of residuals assumption? Or at least what should be the first step to develop a sound regression analysis methodology?
","['regression', 'least-squares', 'residuals', 'assumptions', 'normality-assumption']","The ordinary least squares estimate is still a reasonable estimator in the face of non-normal errors. In particular, the Gauss-Markov Theorem states that the ordinary least squares estimate is the best linear unbiased estimator (BLUE) of the regression coefficients ('Best' meaning optimal in terms of minimizing mean squared error)as long as the errors(1) have mean zero (2) are uncorrelated (3) have constant varianceNotice there is no condition of normality here (or even any condition that the errors are IID). The normality condition comes into play when you're trying to get confidence intervals and/or $p$-values. As @MichaelChernick mentions (+1, btw) you can use robust inference when the errors are non-normal as long as the departure from normality can be handled by the method - for example, (as we discussed in this thread) the Huber $M$-estimator can provide robust inference when the true error distribution is the mixture between normal and a long tailed distribution (which your example looks like) but may not be helpful for other departures from normality. One interesting possibility that Michael alludes to is bootstrapping to obtain confidence intervals for the OLS estimates and seeing how this compares with the Huber-based inference.Edit: I often hear it said that you can rely on the Central Limit Theorem to take care of non-normal errors - this is not always true (I'm not just talking about counterexamples where the theorem fails). In the real data example the OP refers to, we have a large sample size but can see evidence of a long-tailed error distribution - in situations where you have long tailed errors, you can't necessarily rely on the Central Limit Theorem to give you approximately unbiased inference for realistic finite sample sizes. For example, if the errors follow a $t$-distribution with $2.01$ degrees of freedom (which is not clearly more long-tailed than the errors seen in the OP's data), the coefficient estimates are asymptotically normally distributed, but it takes much longer to ""kick in"" than it does for other shorter-tailed distributions. Below, I demonstrate with a crude simulation in R that when $y_{i} = 1 + 2x_{i} + \varepsilon_i$, where  $\varepsilon_{i} \sim t_{2.01}$, the sampling distribution of $\hat{\beta}_{1}$ is still quite long tailed even when the sample size is $n=4000$:"
How to select a clustering method? How to validate a cluster solution (to warrant the method choice)?,"
One of the biggest issue with cluster analysis is that we may happen to have to derive different conclusion when base on different clustering methods used  (including different linkage methods in hierarchical clustering).
I would like to know your opinion on this - which method will you select, and how. One might say ""the best method of clustering is which gives you the right answer""; but I may question in response that cluster analysis is supposed to be an unsupervised technique - so how do I know which method or linkage is the right answer?
In general: is a clustering alone robust enough to rely on? Or we need a second method and get a shared result to be based on both?
My question is not only about possible ways to validate / evaluate clustering performance, but is broader - on what basis do we select/prefer one clustering method/algorithm over another one. Also, is there common warnings that we should look around when we are selecting a method to cluster our data?
I know that it is very general question and very difficult to answer. I only would like to know if you have any comment or any advise or any suggestion for me to learn more about this.
","['clustering', 'validation', 'model-evaluation', 'hierarchical-clustering']","Often they say that there is no other analytical technique as strongly of the ""as you sow you shall mow"" kind, as cluster analysis is.I can imagine of a number dimensions or aspects of ""rightness"" of this or that clustering method:Cluster metaphor. ""I preferred this method because it constitutes clusters such (or such a way) which meets with my concept of a cluster in my particular project"". Each clustering algorithm or subalgorithm/method implies its corresponding structure/build/shape
of a cluster. In regard to hierarchical methods, I've observed this
in one of points here, and also here. I.e. some methods give clusters that are
prototypically ""types"", other give ""circles [by interest]"", still
other ""[political] platforms"", ""classes"", ""chains"", etc. Select that method which cluster metaphor suits you. For example, if I see my customer segments as types - more or less spherical shapes with compaction(s) in the middle I'll choose Ward's linkage method or K-means, but never single linkage method, clearly. If I need a focal representative point I could use medoid method. If I need to screen points for them being core and peripheral representatives I could use DBSCAN approach.Data/method assumptions. ""I preferred this method because my data nature or format predispose to it"". This important and vast point is also mentioned in my link above. Different algorithms/methods may require different kind of data for them or different proximity measure to be applied to the data, and vice versa, different data may require different methods. There are methods for quantitative and methods for qualitative data. Mixture quantitative + qualitative features dramatically narrows the scope of choice among methods. Ward's or K-means are based - explicitly or implicitly - on (squared) euclidean distance proximity measure only and not on arbitrary measure. Binary data may call for special similarity measures which in turn will strongly question using some methods, for example Ward's or K-means, for them.  Big data may need special algorithms or special implementations.Internal validity. ""I preferred this method because it gave me most clear-cut, tight-and-isolated clusters"". Choose algorithm/method that shows the best results for your data from this point of view. The more tight, dense are clusters inside and the less density is outside of them (or the wider apart are the clusters) - the greater is the internal validity. Select and use appropriate internal clustering criteria (which are plenty - Calinski-Harabasz, Silhouette, etc etc; sometimes also called ""stopping rules"") to assess it. [Beware of overfitting: all clustering methods seek to maximize some version of internal validity$^1$ (it's what clustering is about), so high validity may be partly due to random peculiarity of the given dataset; having a test dataset is always beneficial.]External validity. ""I preferred this method because it gave me clusters which differ by their background or clusters which match with the true ones I know"". If a clustering partition presents clusters which are clearly different on some important background (i.e. not participated in the cluster analysis) characteristics then it is an asset for that method which produced the partition. Use any analysis which applies to check the difference; there also exist a number of useful external clustering criteria (Rand, F-measure, etc etc). Another variant of external validation case is when you somehow know the true clusters in your data (know ""ground truth""), such as when you generated the clusters yourself. Then how accurately your clustering method is able to uncover the real clusters is the measure of external validity.Cross-validity. ""I preferred this method because it is giving me very similar clusters on equivalent samples of the data or extrapolates well onto such samples"". There are various approaches and their hybrids, some more feasible with some clustering methods while others with other methods. Two main approaches are stability check and generalizability check. Checking stability of a clustering method, one randomly splits or resamples the data in partly intersecting or fully disjoint sets and does the clustering on each; then matches and compares the solutions wrt some emergent cluster characteristic (for example, a cluster's central tendency location) whether it is stable across the sets. Checking generalizability implies doing clustering on a train set and then using its emergent cluster characteristic or rule to assign objects of a test set, plus also doing clustering on the test set. The assignment result's and the clustering result's cluster memberships of the test set objects are compared then.Interpretation. ""I preferred this method because it gave me clusters which, explained, are most persuasive that there is meaning in the world"". It's not statistical - it is your psychological validation. How meaningful are the results for you, the domain and, possibly audience/client. Choose method giving most interpretable, spicy results.Gregariousness. Some researches regularly and all researches occasionally would say ""I preferred this method because it gave with my data similar results with a number of other methods among all those I probed"". This is a heuristic but questionable strategy which assumes that there exist quite universal data or quite universal method.Points 1 and 2 are theoretical and precede obtaining the result; exclusive  relying on these points is the haughty, self-assured exploratory strategy. Points 3, 4 and 5 are empirical and follow the result; exclusive relying on these points is the fidgety, try-all-out exploratory strategy. Point 6 is creative which means that it denies any result in order to try to rejustify it. Point 7 is loyal mauvaise foi.Points 3 through 7 can also be judges in your selection of the ""best"" number of clusters.$^1$ A concrete internal clustering criterion is itself not ""orthogonal to"" a clustering method (nor to the data kind). This raises a philosophical question to what extent such a biased or prejudiced criterion can be of utility (see answers just noticing it)."
"When combining p-values, why not just averaging?","
I recently learned about Fisher's method to combine p-values. This is based on the fact that p-value under the null follows a uniform distribution, and that $$-2\sum_{i=1}^n{\log X_i} \sim \chi^2(2n), \text{ given } X \sim \text{Unif}(0,1)$$
which I think is genius. But my question is why going this convoluted way? and why not (what is wrong with) just using mean of p-values and use central limit theorem? or median? I am trying to understand the genius of RA Fisher behind this grand scheme.
","['hypothesis-testing', 'p-value', 'multiple-comparisons', 'central-limit-theorem', 'combining-p-values']","You can perfectly use the mean $p$-value.Fisher’s method set sets a threshold $s_\alpha$ on $-2 \sum_{i=1}^n \log p_i$, such that if the null hypothesis $H_0$ : all $p$-values are $\sim U(0,1)$ holds, then $-2 \sum_i \log p_i$ exceeds $s_\alpha$ with probability $\alpha$. $H_0$ is rejected when this happens.Usually one takes $\alpha = 0.05$ and $s_\alpha$ is given by a quantile of $\chi^2(2n)$. Equivalently, one can work on the product $\prod_i p_i$ which is lower than $e^{-s_\alpha/2}$ with probability $\alpha$. 
Here is, for $n=2$, a graph showing the rejection zone (in red) (here we use $s_\alpha = 9.49$. The rejection zone has area = 0.05.Now you can chose to work on ${1\over n} \sum_{i=1}^n p_i$ instead, or equivalently on $\sum_i p_i$. You just need to find a threshold $t_\alpha$ such that $\sum p_i$ is below $t_\alpha$ with probability $\alpha$; exact computation $t_\alpha$ is tedious – for $n$ big enough you can rely on central limit theorem; for $n = 2$, $t_\alpha = (2\alpha)^{1\over 2}$. The following graph shows the rejection zone (area = 0.05 again).As you can imagine, many other shapes for the rejection zone are possibles, and have been proposed. It is not a priori clear which is better – i.e. which has greater power.Let‘s assume that $p_1$, $p_2$ come from a bilateral $z$-test with non-centrality parameter 1 :Let's have a look on the scatterplot with in red the points for which the null hypothesis is rejected.The power of Fisher’s product method is approximatelyThe power of the method based on the sum of $p$-values is approximatelySo Fisher’s method wins – at least in this case."
Where does the misconception that Y must be normally distributed come from?,"
Seemingly reputable sources claim that the dependent variable must be normally distributed:

Model assumptions: $Y$ is normally distributed, errors are normally
  distributed, $e_i \sim N(0,\sigma^2)$, and independent, and $X$ is fixed, and
  constant variance $\sigma^2$.
Penn State, STAT 504 Analysis of Discrete Data


Secondly, the linear regression analysis requires all variables to be
  multivariate normal.
StatisticsSolutions, Assumptions of Linear Regression


This is appropriate when the response variable has a normal
  distribution
Wikipedia, Generalized linear model

Is there a good explanation for how or why this misconception has spread? Is its origin known?
Related

Linear regression and assumptions about response variable

","['regression', 'least-squares', 'linear-model', 'dependent-variable']","'Y must be normally distributed'must?In the cases that you mention it is sloppy language (abbreviating 'the error in Y must be normally distributed'), but they don't really (strongly) say that the response must be normally distributed, or at least it does not seem to me that their words were intended like that.speaks about ""a continuous variable $Y$"", but also about ""$Y_i$"" as in $$E(Y_i) = \beta_0 + \beta_1 x_i$$ where we could regard $Y_i$, which is as amoeba called in the comments 'conditional', normally distributed,$$Y_i \sim N(\beta_0 + \beta_1x_i,\sigma^2)$$ The article uses $Y$ and $Y_i$ interchangeably. Throughout the entire article one speaks about the 'distribution of Y', for instance:  when explaining some variant of GLM (binary logistic regression), Random component: The distribution of $Y$ is assumed to be $Binomial(n,\pi)$,...in some definitionRandom Component – refers to the probability distribution of the response variable ($Y$); e.g. normal distribution for $Y$ in the linear regression, or binomial distribution for $Y$ in the binary logistic regression.however at some other point they also refer to $Y_i$ instead of $Y$:The dependent variable $Y_i$ does NOT need to be normally distributed, but it typically assumes a distribution from an exponential family (e.g. binomial, Poisson, multinomial, normal,...) is an extremely brief, simplified, stylized description. I am not sure you should take this serious. For instance, it speaks about..requires all variables to be multivariate normal...so that is not just the response variable,and also the the 'multivariate' descriptor is vague. I am not sure 
how to get that interpreted.has an additional context explained in brackets:Ordinary linear regression predicts the expected value of a given
  unknown quantity (the response variable, a random variable) as a
  linear combination of a set of observed values (predictors). This
  implies that a constant change in a predictor leads to a constant
  change in the response variable (i.e. a linear-response model).
  This is appropriate when the response variable has a normal
  distribution (intuitively, when a response variable can vary
  essentially indefinitely in either direction with no fixed ""zero
  value"", or more generally for any quantity that only varies by a
  relatively small amount, e.g. human heights).This 'no fixed zero value' seems to point to the case that a linear combination $y+\epsilon$ when $\epsilon \sim N(0,\sigma)$ has an infinite domain (from minus infinity to plus infinity) whereas often many variables have some finite cut-off value (such as counts not allowing negative values). The particular line has been added on March 8 2012, but note that the first line of the Wikipedia article still reads ""a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution"" and is not so much (not everywhere) wrong.So, based on these three examples (which indeed could generate misconceptions, or at least could be misunderstood) I would not say that ""this misconception has spread"". Or at least it does not seem to me that the intention of those three examples is to argue that Y must be normally distributed (although I do remember this issue has arised before here on stackexchange, the swap between normally distributed errors and normally distributed response variable is easy to make). So, the assumption that 'Y must be normally distributed' seems to me not like a widespread believe/misconception (as in something that spreads like a red herring), but more like a common error (which is not spread but made independently each time).An example of the mistake on this website is in the following questionWhat if residuals are normally distributed, but y is not?I would consider this as a beginners question. It is not present in the materials like the Penn State course material, the Wikipedia website, and recently noted in the comments the book  'Extending the Linear Regression with R'. The writers of those works do correctly understand the material. Indeed, they use phrases such as 'Y must be normally distributed', but based on the context and the used formulas you can see that they all mean 'Y, conditional on X, must be normally distributed' and not 'the marginal Y must be normally distributed'. They are not misconceiving the idea themselves, and at least the idea is not widespread among statisticians and people that write books and other course materials. But misreading their ambiguous words may indeed cause the misconception."
Is there any gold standard for modeling irregularly spaced time series?,"
In field of economics (I think) we have  ARIMA and GARCH for regularly spaced time series and Poisson, Hawkes for modeling point processes, so how about attempts for modeling irregularly (unevenly) spaced time series - are there (at least) any common practices?
(If you have some knowledge in this topic you can also expand the corresponding wiki article.)
I see irregular time series simply as series of pairs (value, time_of_event), so we have to model not only value to value dependencies but also value and time_of_event and timestamps themselves.
Edition (about missing values and irregular spaced time series) :
Answer to @Lucas Reis comment. If gaps between measurements or realizations variable are spaced due to (for example) Poisson process there is no much room for this kind of regularization, but it exists simple procedure : t(i) is the i-th time index of variable x (i-th time of realization x), then define gaps between times of measurements as g(i)=t(i)-t(i-1), then we discretize g(i) using constant c, dg(i)=floor(g(i)/c and create new time series with number of blank values between old observations from original time series i and i+1 equal to dg(i), but the problem is that this procedure can easily produce time series with number of missing data much larger then number of observations, so the reasonable estimation of missing observations' values could be impossible and too large c delete ""time structure/time dependence etc."" of analysed problem (extreme case is given by taking c>=max(floor(g(i)/c)) which simply collapse irregularly spaced time series into regularly spaced
Edition2 (just for fun):
Image accounting for missing values in irregularly spaced time series or even case of point process.
","['time-series', 'garch', 'poisson-process', 'point-process', 'unevenly-spaced-time-series']",
"Interview question: If correlation doesn't imply causation, how do you detect causation?","
I got this question: 

If correlation doesn't imply causation, how do you detect causation? 

in an interview. 
My answer was: You do some form of A/B testing. The interviewer kept prodding me for another approach but I couldn't think of any, and he wouldn't tell me whether my initial response was correct or not. 
Are there any other approaches? And was my response correct? 
","['self-study', 'correlation', 'causality']",
Logistic Regression: Scikit Learn vs Statsmodels,"
I am trying to understand why the output from logistic regression of these
two libraries gives different results.
I am using the dataset from UCLA idre tutorial, predicting admit based
on gre, gpa and rank. rank is treated as categorical variable, so it
is first converted to dummy variable with rank_1 dropped. An intercept
column is also added.
py
from patsy import dmatrices
from sklearn.linear_model import LogisticRegression
import pandas as pd
import statsmodels.api as sm

df = pd.read_csv(""https://stats.idre.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
>  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

# Output from scikit-learn
model = LogisticRegression(fit_intercept = False)
mdl = model.fit(X, y)
model.coef_
> array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]

# Output from statsmodels
logit = sm.Logit(y, X)
logit.fit().params
> Optimization terminated successfully.
     Current function value: 0.573147
     Iterations 6
Intercept      -3.989979
C(rank)[T.2]   -0.675443
C(rank)[T.3]   -1.340204
C(rank)[T.4]   -1.551464
gre             0.002264
gpa             0.804038
dtype: float64

The output from statsmodels is the same as shown on the idre website, but I
am not sure why scikit-learn produces a different set of coefficients. Does
it minimize some different loss function? Is there any documentation that
states the implementation?
","['regression', 'logistic', 'python', 'scikit-learn', 'statsmodels']","Your clue to figuring this out should be that the parameter estimates from the scikit-learn estimation are uniformly smaller in magnitude than the statsmodels counterpart. This might lead you to believe that scikit-learn applies some kind of parameter regularization. You can confirm this by reading the scikit-learn documentation.There is no way to switch off regularization in scikit-learn, but you can make it ineffective by setting the tuning parameter C to a large number. Here is how that works in your case:UPDATE: As correctly pointed out in the comments below, now you can switch off the relularization in scikit-learn by setting penalty='none' (see the docs)."
Multinomial logistic regression vs one-vs-rest binary logistic regression,"
Lets say we have a dependent variable $Y$ with few categories and set of independent variables.  
What are the advantages of multinomial logistic regression over set of binary logistic regressions (i.e. one-vs-rest scheme)? By set of binary logistic regression I mean that for each category $y_{i} \in Y$ we build separate binary logistic regression model with target=1 when $Y=y_{i}$ and 0 otherwise.
","['logistic', 'categorical-data', 'multinomial-distribution']","If $Y$ has more than two categories your question about ""advantage"" of one regression over the other is probably meaningless if you aim to compare the models' parameters, because the models will be fundamentally different:$\bf log \frac{P(i)}{P(not~i)}=logit_i=linear~combination$ for each $i$ binary logistic regression, and$\bf log \frac{P(i)}{P(r)}=logit_i=linear~combination$ for each $i$ category in multiple logistic regression, $r$ being the chosen reference category ($i \ne r$).However, if your aim is only to predict probability of each category $i$ either approach is justified, albeit they may give different probability estimates. The formula to estimate a probability is generic:$\bf P'(i)= \frac{exp(logit_i)}{exp(logit_i)+exp(logit_j)+\dots+exp(logit_r)}$, where $i,j,\dots,r$ are all the categories, and if $r$ was chosen to be the reference one its $\bf exp(logit)=1$. So, for binary logistic that same formula becomes $\bf P'(i)= \frac{exp(logit_i)}{exp(logit_i)+1}$. Multinomial logistic relies on the (not always realistic) assumption of independence of irrelevant alternatives whereas a series of binary logistic predictions does not.A separate theme is what are technical differences between multinomial and binary logistic regressions in case when $Y$ is dichotomous. Will there be any difference in results? Most of the time in the absence of covariates the results will be the same, still, there are differences in the algorithms and in output options. Let me just quote SPSS Help about that issue in SPSS:Binary logistic regression models can be fitted using either the
Logistic Regression procedure or the Multinomial Logistic Regression
procedure. Each procedure has options not available in the other. An
important theoretical distinction is that the Logistic Regression
procedure produces all predictions, residuals, influence statistics,
and goodness-of-fit tests using data at the individual case level,
regardless of how the data are entered and whether or not the number
of covariate patterns is smaller than the total number of cases, while
the Multinomial Logistic Regression procedure internally aggregates
cases to form subpopulations with identical covariate patterns for the
predictors, producing predictions, residuals, and goodness-of-fit
tests based on these subpopulations. If all predictors are categorical
or any continuous predictors take on only a limited number of
values—so that there are several cases at each distinct covariate
pattern—the subpopulation approach can produce valid goodness-of-fit
tests and informative residuals, while the individual case level
approach cannot.Logistic Regression provides the following unique features:Multinomial Logistic Regression provides the following unique
features:"
How can I change the title of a legend in ggplot2? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it's on-topic for Cross Validated.


Closed 10 years ago.







                        Improve this question
                    



I have a plot I'm making in ggplot2 to summarize data that are from a 2 x 4 x 3 celled dataset.  I have been able to make panels for the 2-leveled variable using facet_grid(. ~ Age) and to set the x and y axes using aes(x=4leveledVariable, y=DV).  I used aes(group=3leveledvariable, lty=3leveledvariable) to produce the plot so far.  This gives me a visualization that is paneled by the 2-leveled variable, with the X axis representing the 4 leveled variable and different lines plotted within the panels for the 3-leveled variable.  But the key for the 3-leveled variable is titled with the 3-leveled variable's name and I want it to be a title that has a character space in it.  How can I rename the title of the legend?  
Things I've tried that don't seem to work (where abp is my ggplot2 object):
 abp <- abp + opts(legend.title=""Town Name"")
 abp <- abp + scale_fill_continuous(""Town Name"")
 abp <- abp + opts(group=""Town Name"")
 abp <- abp + opts(legend.title=""Town Name"")

Example data:  
ex.data <- data.frame(DV=rnorm(2*4*3), V2=rep(1:2,each=4*3), V4=rep(1:4,each=3), V3=1:3)

","['r', 'data-visualization', 'ggplot2']","Another option is to use For example, Chase's example would look like:and yield the figure:
"
Why does shrinkage work?,"
In order to solve problems of model selection, a number of methods (LASSO, ridge regression, etc.) will shrink the coefficients of predictor variables towards zero. I am looking for an intuitive explanation of why this improves predictive ability. If the true effect of the variable was actually very large, why doesn't shrinking the parameter result in a worse prediction?
","['lasso', 'ridge-regression', 'intuition', 'regularization']",
How to identify a bimodal distribution?,"
I understand that once we plot the values as a chart, we can identify a bimodal distribution by observing the twin-peaks, but how does one find it programmatically? (I am looking for an algorithm.)
",['distributions'],"Identifying a mode for a continuous distribution requires smoothing or binning the data.Binning is typically too procrustean: the results often depend on where you place the bin cutpoints.Kernel smoothing (specifically, in the form of kernel density estimation) is a good choice.  Although many kernel shapes are possible, typically the result does not depend much on the shape.  It depends on the kernel bandwidth.  Thus, people either use an adaptive kernel smooth or conduct a sequence of kernel smooths for varying fixed bandwidths in order to check the stability of the modes that are identified.  Although using an adaptive or ""optimal"" smoother is attractive, be aware that most (all?) of these are designed to achieve a balance between precision and average accuracy: they are not designed to optimize estimation of the location of modes.As far as implementation goes, kernel smoothers locally shift and scale a predetermined function to fit the data.  Provided that this basic function is differentiable--Gaussians are a good choice because you can differentiate them as many times as you like--then all you have to do is replace it by its derivative to obtain the derivative of the smooth.  Then it's simply a matter of applying a standard zero-finding procedure to detect and test the critical points.  (Brent's method works well.)  Of course you can do the same trick with the second derivative to get a quick test of whether any critical point is a local maximum--that is, a mode.For more details and working code (in R) please see https://stats.stackexchange.com/a/428083/919."
What is maxout in neural network?,"
Can anyone explain what maxout units in a neural network do? How do they perform and how do they differ from conventional units?
I tried to read the 2013 ""Maxout Network"" paper by Goodfellow et al. (from Professor Yoshua Bengio's group), but I don't quite get it.
","['machine-learning', 'neural-networks']",
What is the difference between a particle filter (sequential Monte Carlo) and a Kalman filter?,"
A particle filter and Kalman filter are both recursive Bayesian estimators.  I often encounter Kalman filters in my field, but very rarely see the usage of a particle filter.  
When would one be used over the other?
","['bayesian', 'particle-filter', 'kalman-filter']","From Dan Simon's ""Optimal State Estimation"":In a linear system with Gaussian noise, the Kalman filter is optimal. In a system that is nonlinear, the Kalman filter can be used for state estimation, but the particle filter may give better results at the price of additional computational effort. In a system that has non-Gaussian noise, the Kalman filter is the optimal linear filter, but again the particle filter may perform better. The unscented Kalman filter (UKF) provides a balance between the low computational effort of the Kalman filter and the high performance of the particle filter.The particle filter has some similarities with the UKF in that it transforms a set of points via known nonlinear equations and combines the results to estimate the mean and covariance of the state. However, in the particle filter the points are chosen randomly, whereas in the UKF the points are chosen on the basis of a specific algorithm*.  Because of this, the number of points used in a particle filter generally needs to be much greater than the number of points in a UKF. Another difference between the two filters is that the estimation error in a UKF does not converge to zero in any sense, but the estimation error in a particle filter does converge to zero as the number of particles (and hence the computational effort) approaches infinity.*The unscented transformation is a method for calculating the statistics of a random variable which undergoes a nonlinear transformation and uses the intuition (which also applies to the particle filter) that it is easier to approximate a probability distribution than it is to approximate an arbitrary nonlinear function or transformation. See also this as an example of how the points are chosen in UKF."""
A/B tests: z-test vs t-test vs chi square vs fisher exact test,"
I'm trying to understand the reasoning by choosing a specific test approach when dealing with a simple A/B test - (i.e. two variations/groups with a binary respone (converted or not). As an example I will be using the data below
Version  Visits  Conversions
A        2069     188
B        1826     220

The top answer here is great and talks about some of the underlying assumptions for z, t and chi square tests. But what I find confusing is that different online resources will cite different approaches, and you would think the assumptions for a basic A/B test should be pretty much the same?

For instance, this article uses z-score:
This article uses the following formula (which I'm not sure if it's different from the zscore calculation?): 



This paper references the t test(p 152):


So what arguemnts can be made in favor of these different approaches? Why would one have a preference? 
To throw in one more candidate, the table above can be rewritten as a 2x2 contingency table, where Fisher's exact test (p5) can be used 
              Non converters  Converters  Row Total
Version A     1881            188         2069  
Versions B    1606            220         1826
Column Total  3487            408         3895

But according to this thread fisher's exact test should only be used with smaller sample sizes (what's the cut off?)
And then there's paired t and z tests,f test (and logistic regression, but I want to leave that out for now)....I feel like I'm drowning in different test approaches, and I just want to be able to make some kind of argument for the different methods in this simple A/B test case.
Using the example data I'm getting the following p-values

https://vwo.com/ab-split-test-significance-calculator/ gives a
p-value of 0.001 (z-score)
http://www.evanmiller.org/ab-testing/chi-squared.html (using chi
square test) gives a p-value of 0.00259
And in R fisher.test(rbind(c(1881,188),c(1606,220)))$p.value gives
a p-value of  0.002785305

Which I guess are all pretty close...
Anyway - just hoping for some healthy discussion on what approaches to use in online testing where sample sizes are usually in the thousands, and response ratios are often 10% or less. My gut is telling me to use chi-square, but I want to be able to answer exactly why I'm choosing it over the other multitude of ways to do it.
","['statistical-significance', 'chi-squared-test', 'p-value', 'fishers-exact-test', 'z-statistic']","We use these tests for different reasons and under different circumstances.$z$-test. A $z$-test assumes that our observations are independently drawn from a Normal distribution with unknown mean and known variance. A $z$-test is used primarily when we have quantitative data. (i.e. weights of rodents, ages of individuals, systolic blood pressure, etc.) However, $z$-tests can also be used when interested in proportions. (i.e. the proportion of people who get at least eight hours of sleep, etc.)$t$-test. A $t$-test assumes that our observations are independently drawn from a Normal distribution with unknown mean and unknown variance. Note that with a $t$-test, we do not know the population variance. This is far more common than knowing the population variance, so a $t$-test is generally more appropriate than a $z$-test, but practically there will be little difference between the two if sample sizes are large.With $z$- and $t$-tests, your alternative hypothesis will be that your population mean (or population proportion) of one group is either not equal, less than, or greater than the population mean (or proportion) of the other group. This will depend on the type of analysis you seek to do, but your null and alternative hypotheses directly compare the means/proportions of the two groups.Chi-squared test. Whereas $z$- and $t$-tests concern quantitative data (or proportions in the case of $z$), chi-squared tests are appropriate for qualitative data. Again, the assumption is that observations are independent of one another. In this case, you aren't seeking a particular relationship. Your null hypothesis is that no relationship exists between variable one and variable two. Your alternative hypothesis is that a relationship does exist. This doesn't give you specifics as to how this relationship exists (i.e. in which direction the relationship goes) but it will provide evidence that a relationship does (or does not) exist between your independent variable and your groups.Fisher's exact test. One drawback to the chi-squared test is that it is asymptotic. This means that the $p$-value is accurate for very large sample sizes. However, if your sample sizes are small, then the $p$-value may not be quite as accurate. As such, Fisher's exact test allows you to exactly calculate the $p$-value of your data and not rely on approximations that will be poor if your sample sizes are small.I keep discussing sample sizes - different references will give you different metrics as to when your samples are large enough. I would just find a reputable source, look at their rule, and apply their rule to find the test you want. I would not ""shop around"", so to speak, until you find a rule that you ""like"".Ultimately, the test you choose should be based on a) your sample size and b) what form you want your hypotheses to take. If you are looking for a specific effect from your A/B test (for example, my B group has higher test scores), then I would opt for a $z$-test or $t$-test, pending sample size and the knowledge of the population variance. If you want to show that a relationship merely exists (for example, my A group and B group are different based on the independent variable but I don't care which group has higher scores), then the chi-squared or Fisher's exact test is appropriate, depending on sample size.Does this make sense? Hope this helps!"
Graph for relationship between two ordinal variables,"
What is an appropriate graph to illustrate the relationship between two ordinal variables?
A few options I can think of:

Scatter plot with added random jitter to stop points hiding each other. Apparently a standard graphic - Minitab calls this an ""individual values plot"". In my opinion it may be misleading as it visually encourages a kind of linear interpolation between ordinal levels, as if the data were from an interval scale.
Scatter plot adapted so that size (area) of point represents frequency of that combination of levels, rather than drawing one point for each sampling unit. I have occasionally seen such plots in practice. They can be hard to read, but the points lie on a regularly-spaced lattice which somewhat overcomes the criticism of the jittered scatter plot that it visually ""intervalises"" the data.
Particularly if one of the variables is treated as dependent, a box plot grouped by the levels of the independent variable. Likely to look terrible if the number of levels of the dependent variable is not sufficiently high (very ""flat"" with missing whiskers or even worse collapsed quartiles which makes visual identification of median impossible), but at least draws attention to median and quartiles which are relevant descriptive statistics for an ordinal variable.
Table of values or blank grid of cells with heat map to indicate frequency. Visually different but conceptually similar to the scatter plot with point area showing frequency.

Are there other ideas, or thoughts on which plots are preferable? Are there any fields of research in which certain ordinal-vs-ordinal plots are regarded as standard? (I seem to recall frequency heatmap being widespread in genomics but suspect that is more often for nominal-vs-nominal.) Suggestions for a good standard reference would also be very welcome, I am guessing something from Agresti.
If anyone wants to illustrate with a plot, R code for bogus sample data follows.
""How important is exercise to you?"" 1 = not at all important, 2 = somewhat unimportant, 3 = neither important nor unimportant, 4 = somewhat important, 5 = very important.
""How regularly do you take a run of 10 minutes or longer?"" 1 = never, 2 = less than once per fortnight, 3 = once every one or two weeks, 4 = two or three times per week, 5 = four or more times per week.
If it  would be natural to treat ""often"" as a dependent variable and ""importance"" as an independent variable, if a plot distinguishes between the two.
importance <- rep(1:5, times = c(30, 42, 75, 93, 60))
often <- c(rep(1:5, times = c(15, 07, 04, 03, 01)), #n=30, importance 1
           rep(1:5, times = c(10, 14, 12, 03, 03)), #n=42, importance 2
           rep(1:5, times = c(12, 23, 20, 13, 07)), #n=75, importance 3
           rep(1:5, times = c(16, 14, 20, 30, 13)), #n=93, importance 4
           rep(1:5, times = c(12, 06, 11, 17, 14))) #n=60, importance 5
running.df <- data.frame(importance, often)
cor.test(often, importance, method = ""kendall"") #positive concordance
plot(running.df) #currently useless

A related question for continuous variables I found helpful, maybe a useful starting point: What are alternatives to scatterplots when studying the relationship between two numeric variables?
","['data-visualization', 'categorical-data', 'ordinal-data', 'scatterplot']","A spineplot (mosaic plot) works well for the example data here, but can be difficult to read or interpret if some combinations of categories are rare or don't exist. Naturally it's reasonable, and expected, that a low frequency is represented by a small tile, and zero by no tile at all, but the psychological difficulty can remain. It's also natural that people fond of spineplots choose examples which work well for their papers or presentations, but I've often produced examples that were too messy to use in public. Conversely, a spineplot does use the available space well. Some implementations presuppose interactive graphics, so that the user can interrogate each tile to learn more about it. An alternative which can also work quite well is a two-way bar chart (many other names exist). See for example tabplot within http://www.surveydesign.com.au/tipsusergraphs.htmlFor these data, one possible plot (produced using tabplot in Stata, but should be easy in any decent software) is The format means it is easy to relate individual bars to row and column identifiers and that you can annotate with frequencies, proportions or percents (don't do that if you think the result is too busy, naturally). Some possibilities: If one variable can be thought of a response to another as predictor, then it is worth thinking of plotting it on the vertical axis as usual. Here I think of ""importance"" as measuring an attitude, the question then being whether it affects behaviour (""often""). The causal issue is often more complicated even for these imaginary data, but the point remains. Suggestion #1 is always to be trumped if the reverse works better, meaning, is easier to think about and interpret. Percent or probability breakdowns often make sense. A plot of raw frequencies can be useful too. (Naturally, this plot lacks the virtue of mosaic plots of showing both kinds of information at once.) You can of course try the (much more common) alternatives of grouped bar charts or stacked bar charts (or the still fairly uncommon grouped dot charts in the sense of W.S. Cleveland). In this case, I don't think they work as well, but sometimes they work better. Some might want to colour different response categories differently. I've no objection, and if you want that you wouldn't take objections seriously any way. The strategy of hybridising graph and table can be useful more generally, or indeed not what you want at all. An often repeated argument is that the separation of Figures and Tables was just a side-effect of the invention of printing and the division of labour it produced; it's once more unnecessary, just as it was to manuscript writers putting illustrations exactly how and where they liked. "
"Which has the heavier tail, lognormal or gamma?","
(This is based on a question that just came to me via email; I've added some context from a previous brief conversation with the same person.)
Last year I was told that the gamma distribution is heavier tailed than the lognormal, and I've since been told that's not the case.

Which is heavier tailed?
What are some resources I can use to explore the relationship?

","['distributions', 'gamma-distribution', 'lognormal-distribution', 'heavy-tailed']","The (right) tail of a distribution describes its behavior at large values.  The correct object to study is not its density--which in many practical cases does not exist--but rather its distribution function $F$.  More specifically, because $F$ must rise asymptotically to $1$ for large arguments $x$ (by the Law of Total Probability), we are interested in how rapidly it approaches that asymptote: we need to investigate the behavior of its survival function $1- F(x)$ as $x \to \infty$.Specifically, one distribution $F$ for a random variable $X$ is ""heavier"" than another one $G$ provided that eventually $F$ has more probability at large values than $G$.  This can be formalized: there must exist a finite number $x_0$ such that for all $x \gt x_0$, $${\Pr}_F(X\gt x) = 1 - F(x) \gt 1 - G(x) = {\Pr}_G(X\gt x).$$The red curve in this figure is the survival function for a Poisson$(3)$ distribution.  The blue curve is for a Gamma$(3)$ distribution, which has the same variance.  Eventually the blue curve always exceeds the red curve, showing that this Gamma distribution has a heavier tail than this Poisson distribution.  These distributions cannot readily be compared using densities, because the Poisson distribution has no density.It is true that when the densities $f$ and $g$ exist and $f(x) \gt g(x)$ for $x \gt x_0$ then $F$ is heavier-tailed than $G$.  However, the converse is false--and this is a compelling reason to base the definition of tail heaviness on survival functions rather than densities, even if often the analysis of tails may be more easily carried out using the densities.Counter-examples can be constructed by taking a discrete distribution $H$ of positive unbounded support that nevertheless is no heavier-tailed than $G$ (discretizing $G$ will do the trick).  Turn this into a continuous distribution by replacing the probability mass of $H$ at each of its support points $k$, written $h(k)$, by (say) a scaled Beta$(2,2)$ distribution with support on a suitable interval $[k-\varepsilon(k), k+\varepsilon(k)]$ and weighted by $h(k)$.  Given a small positive number $\delta,$ choose $\varepsilon(k)$ sufficiently small to ensure that the peak density of this scaled Beta distribution exceeds $f(k)/\delta$.  By construction, the mixture $\delta H + (1-\delta )G$ is a continuous distribution $G^\prime$ whose tail looks like that of $G$ (it is uniformly a tiny bit lower by an amount $\delta$) but has spikes in its density at the support of $H$ and all those spikes have points where they exceed the density of $f$.  Thus $G^\prime$ is lighter-tailed than $F$ but no matter how far out in the tail we go there will be points where its density exceeds that of $F$.The red curve is the PDF of a Gamma distribution $G$, the gold curve is the PDF of a lognormal distribution $F$, and the blue curve (with spikes) is the PDF of a mixture $G^\prime$ constructed as in the counterexample. (Notice the logarithmic density axis.) The survival function of $G^\prime$ is close to that of a Gamma distribution (with rapidly decaying wiggles): it will eventually grow less than that of $F$, even though its PDF will always spike above that of $F$ no matter how far out into the tails we look.Incidentally, we can perform this analysis directly on the survival functions of lognormal and Gamma distributions, expanding them around $x=\infty$ to find their asymptotic behavior, and conclude that all lognormals have heavier tails than all Gammas.  But, because these distributions have ""nice"" densities, the analysis is more easily carried out by showing that for sufficiently large $x$, a lognormal density exceeds a Gamma density.  Let us not, however, confuse this analytical convenience with the meaning of a heavy tail.Similarly, although higher moments and their variants (such as skewness and kurtosis) say a little about the tails, they do not provide sufficient information.  As a simple example, we may truncate any lognormal distribution at such a large value that any given number of its moments will scarcely change--but in so doing we will have removed its tail entirely, making it lighter-tailed than any distribution with unbounded support (such as a Gamma).A fair objection to these mathematical contortions would be to point out that behavior so far out in the tail has no practical application, because nobody would ever believe that any distributional model will be valid at such extreme (perhaps physically unattainable) values.  That shows, however, that in applications we ought to take some care to identify which portion of the tail is of concern and analyze it accordingly.  (Flood recurrence times, for instance, can be understood in this sense: 10-year floods, 100-year floods, and 1000-year floods characterize particular sections of the tail of the flood distribution.)  The same principles apply, though: the fundamental object of analysis here is the distribution function and not its density."
Is there a test to determine whether GLM overdispersion is significant?,"
I'm creating Poisson GLMs in R. To check for overdispersion I'm looking at the ratio of residual deviance to degrees of freedom provided by summary(model.name). 
Is there a cutoff value or test for this ratio to be considered ""significant?"" I know that if it's >1 then the data are overdispersed, but if I have ratios relatively close to 1 [for example, one ratio of 1.7 (residual deviance = 25.48, df=15) and another of 1.3 (rd = 324, df = 253)], should I still switch to quasipoisson/negative binomial? I found here this test for significance: 1-pchisq(residual deviance,df), but I've only seen that once, which makes me nervous. I also read (I can't find the source) that a ratio < 1.5 is generally safe. Opinions?
","['statistical-significance', 'overdispersion']","In the R package AER you will find the function dispersiontest, which implements a Test for Overdispersion by Cameron & Trivedi (1990).It follows a simple idea: In a Poisson model, the mean is $E(Y)=\mu$ and the variance is $Var(Y)=\mu$ as well. They are equal. The test simply tests this assumption as a null hypothesis against an alternative where $Var(Y)=\mu + c * f(\mu)$ where the constant $c < 0$ means underdispersion and $c > 0$ means overdispersion. The function $f(.)$ is some monoton function (often linear or quadratic; the former is the default).The resulting test is equivalent to testing $H_0: c=0$ vs. $H_1: c \neq 0$ and the test statistic used is a $t$ statistic which is asymptotically standard normal under the null.Example:Here we clearly see that there is evidence of overdispersion (c is estimated to be 5.57) which speaks quite strongly against the assumption of equidispersion (i.e. c=0).Note that if you not use trafo=1, it will actually do a test of $H_0: c^*=1$ vs. $H_1: c^* \neq 1$ with $c^*=c+1$ which has of course the same result as the other test apart from the test statistic being shifted by one. The reason for this, though, is that the latter corresponds to the common parametrization in a quasi-Poisson model."
What is perplexity?,"
I came across term perplexity which refers to the log-averaged inverse probability on unseen data. Wikipedia article on perplexity does not give an intuitive meaning for the same. 
This perplexity measure was used in pLSA paper.
Can anyone explain the need and intuitive meaning of perplexity measure? 
","['intuition', 'information-theory', 'measurement', 'perplexity']","You have looked at the Wikipedia article on perplexity. It gives the perplexity of a discrete distribution as $$2^{-\sum_x p(x)\log_2 p(x)}$$ which could also be written as $$\exp\left({\sum_x p(x)\log_e \frac{1}{p(x)}}\right)$$  i.e. as a weighted geometric average of the inverses of the probabilities. For a continuous distribution, the sum would turn into a integral.The article also gives a way of estimating perplexity for a model using $N$ pieces of test data $$2^{-\sum_{i=1}^N \frac{1}{N} \log_2 q(x_i)}$$ which could also be written $$\exp\left(\frac{{\sum_{i=1}^N \log_e \left(\dfrac{1}{q(x_i)}\right)}}{N}\right) \text{ or } \sqrt[N]{\prod_{i=1}^N  \frac{1}{q(x_i)}}$$or in a variety of other ways, and this should make it even clearer where ""log-average inverse probability"" comes from. "
Adam optimizer with exponential decay,"
In most Tensorflow code I have seen Adam Optimizer is used with a constant Learning Rate of 1e-4 (i.e. 0.0001). The code usually looks the following:
...build the model...
# Add the optimizer
train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
# Add the ops to initialize variables.  These will include 
# the optimizer slots added by AdamOptimizer().
init_op = tf.initialize_all_variables()

# launch the graph in a session
sess = tf.Session()
# Actually intialize the variables
sess.run(init_op)
# now train your model
for ...:
  sess.run(train_op)

I am wondering, whether it is useful to use exponential decay when using adam optimizer, i.e. use the following Code:
...build the model...
# Add the optimizer
step = tf.Variable(0, trainable=False)
rate = tf.train.exponential_decay(0.15, step, 1, 0.9999)
optimizer = tf.train.AdamOptimizer(rate).minimize(cross_entropy, global_step=step)
# Add the ops to initialize variables.  These will include 
# the optimizer slots added by AdamOptimizer().
init_op = tf.initialize_all_variables()

# launch the graph in a session
sess = tf.Session()
# Actually intialize the variables
sess.run(init_op)
# now train your model
for ...:
  sess.run(train_op)

Usually, people use some kind of learning rate decay, for Adam it seems uncommon. Is there any theoretical reason for this? Can it be useful to combine Adam optimizer with decay?
","['neural-networks', 'deep-learning', 'gradient-descent', 'tensorflow', 'adam']","Empirically speaking: definitely try it out, you may find some very useful training heuristics, in which case, please do share!Usually people use some kind of decay, for Adam it seems uncommon. Is there any theoretical reason for this? Can it be useful to combine Adam optimizer with decay?I haven't seen enough people's code using ADAM optimizer to say if this is true or not. If it is true, perhaps it's because ADAM is relatively new and learning rate decay ""best practices"" haven't been established yet. I do want to note however that learning rate decay is actually part of the theoretical guarantee for ADAM. Specifically in Theorem 4.1 of their ICLR article, one of their hypotheses is that the learning rate has a square root decay, $\alpha_t = \alpha/\sqrt{t}$. Furthermore, for their logistic regression experiments they use the square root decay as well. Simply put: I don't think anything in the theory discourages using learning rate decay rules with ADAM. I have seen people report some good results using ADAM and finding some good training heuristics would be incredibly valuable. "
Manually Calculating P value from t-value in t-test,"
I have a sample dataset with 31 values. I ran a two-tailed t-test using R to test if the true mean is equal to 10:
t.test(x=data, mu=10, conf.level=0.95)

Output:
t = 11.244, df = 30, p-value = 2.786e-12
alternative hypothesis: true mean is not equal to 10 
95 percent confidence interval:
 19.18980 23.26907 
sample estimates:
mean of x 
 21.22944 

Now I'm trying to do the same thing manually:
t.value = (mean(data) - 10) / (sd(data) / sqrt(length(data))) 
p.value = dt(t.value, df=length(lengths-1))

The t-value calculated using this method is the same as output by the t-test R function. The p-value, however, comes out to be 3.025803e-12.
Any ideas what I'm doing wrong?
Thanks!
EDIT
Here is the full R code, including my dataset:
# Raw dataset -- 32 observations
data = c(21.75, 18.0875, 18.75, 23.5, 14.125, 16.75, 11.125, 11.125, 14.875, 15.5, 20.875,
            17.125, 19.075, 25.125, 27.75, 29.825, 17.825, 28.375, 22.625, 28.75, 27, 12.825, 
            26, 32.825, 25.375, 24.825, 25.825, 15.625, 26.825, 24.625, 26.625, 19.625)

# Student t-Test
t.test(x=data, mu=10, conf.level=0.95)

# Manually calculate p-value
t.value = (mean(data) - 10) / (sd(data) / sqrt(length(data)))
p.value = dt(t.value, df=length(data) - 1)

","['r', 'statistical-significance', 't-test', 'p-value']",Use pt and make it two-tailed.
Kullback–Leibler vs Kolmogorov-Smirnov distance,"
I can see that there are a lot of formal differences between Kullback–Leibler vs Kolmogorov-Smirnov distance measures.
However, both are used to measure the distance between distributions.

Is there a typical situation where one should be used instead of the other? 
What is the rationale to do so?

","['distributions', 'distance-functions', 'kolmogorov-smirnov-test', 'kullback-leibler']","The KL-divergence is typically used in information-theoretic settings, or even Bayesian settings, to measure the information change between distributions before and after applying some inference, for example. It's not a distance in the typical (metric) sense, because of lack of symmetry and triangle inequality, and so it's used in places where the directionality is meaningful. The KS-distance is typically used in the context of a non-parametric test. In fact, I've rarely seen it used as a generic ""distance between distributions"", where the $\ell_1$ distance, the Jensen-Shannon distance, and other distances are more common. "
Visually interesting statistics concepts that are easy to explain,"
I noticed on Math Stack Exchange a terrific thread which highlighted a number of very visually interesting math concepts. I would be curious to see graphics/gifs which anyone has that very clearly illustrate a statistics concept (particularly those that might serve as motivation for students just starting to learn statistics). 
I am thinking of things along the lines of how videos of a Galton board make the CLT instantly relatable.
","['self-study', 'data-visualization']",
Is the COVID-19 pandemic curve a Gaussian curve?,"
We've all heard a lot about ""flattening the curve"". I was wondering if these curve – that look like bells – can be qualified as Gaussian despite the fact that there is a temporal dimension.

","['normal-distribution', 'spatio-temporal', 'epidemic-curve']","It seems like there are three questions here:Is the actual distribution of cases Gaussian? No. Are the curves given in the graphic Gaussian? Not quite. I think the red one is a little bit skewed, and the blue one is definitely skewed.Can plots of a value versus time be considered Gaussian? Yes. In mathematics, a Gaussian function, often simply referred to as a Gaussian, is a function of the form $$f(x) = ae^{-{\frac {(x-b)^{2}}{2c^{2}}}}$$
  for arbitrary real constants a, b and non zero c. https://en.wikipedia.org/wiki/Gaussian_functionThere is no requirement that it be a probability distribution."
What are the breakthroughs in Statistics of the past 15 years?,"
I still remember the Annals of Statistics paper on Boosting by Friedman-Hastie-Tibshirani, and the comments on that same issues by other authors (including Freund and Schapire). At that time, clearly Boosting was viewed as a breakthrough in many respects: computationally feasible, an ensemble method, with excellent yet mysterious performance. Around the same time, SVM came of age, offering a framework underpinned by solid theory and with plenty of variants and applications. 
That was in the marvelous 90s. In the past 15 years, it seems to me that a lot of Statistics has been a cleaning and detailing operation, but with few truly new views.
So I'll ask two questions:

Have I missed some revolutionary/seminal paper?
If not, are there new approaches
that you think have the potential to
change the viewpoint of statistical
inference?

Rules: 

One answer per post; 
References or links welcome.

P.S.: I have a couple of candidates for promising breakthroughs. I will post them later.
","['mathematical-statistics', 'history']",
Why continue to teach and use hypothesis testing (when confidence intervals are available)?,"
Why continue to teach and use hypothesis testing (with all its difficult concepts and which are among the most statistical sins) for problems where there is an interval estimator (confidence, bootstrap, credibility or whatever)? What is the best explanation (if any) to be given to students? Only tradition? The views will be very welcome.
","['hypothesis-testing', 'confidence-interval', 'teaching']",
What are some examples of anachronistic practices in statistics?,"
I am referring to practices that still maintain their presence, even though the problems (usually computational) they were designed to cope with have been mostly solved. 
For example, Yates' continuity correction was invented to approximate Fisher's exact test with $\chi^2$ test, but it is no longer practical since software can now handle Fisher's test even with large samples (I know this may not be a good example of ""maintaining its presence"", since textbooks, like Agresti's Categorical Data Analysis, often acknowledge that Yates' correction ""is no longer needed""). 
What are some other examples of such practices?
","['references', 'philosophical']",
Interpreting QQplot - Is there any rule of thumb to decide for non-normality?,"
I have read enough threads on QQplots here to understand that a QQplot can be more informative than other normality tests. However, I am inexperienced with interpreting QQplots. I googled a lot; I found a lot of graphs of non-normal QQplots, but no clear rules on how to interpret them, other than what it seems to be comparison with know distributions plus ""gut feeling"". 
I would like to know if you have (or you know of) any rule of thumb to help you decide for non-normality.
This question came up when I saw these two graphs:


I understand that the decision of non-normality depends on the data and what I want to do with them; however, my question is: generally, when do the observed departures from the straight line constitute enough evidence to make unreasonable the approximation of normality?
For what it's worth, the Shapiro-Wilk test failed to reject the hypothesis of non-normality in both cases.
","['interpretation', 'normality-assumption', 'qq-plot']","Note that the Shapiro-Wilk is a powerful test of normality.The best approach is really to have a good idea of how sensitive any procedure you want to use is to various kinds of non-normality (how badly non-normal does it have to be in that way for it to affect your inference more than you can accept).An informal approach for looking at the plots would be to generate a number of data sets that are actually normal of the same sample size as the one you have - (for example, say 24 of them). Plot your real data among a grid of such plots (5x5 in the case of 24 random sets). If it's not especially unusual looking (the worst looking one, say), it's reasonably consistent with normality.To my eye, data set ""Z"" in the center looks roughly on a par with ""o"" and ""v"" and maybe even ""h"", while ""d"" and ""f"" look slightly worse. ""Z"" is the real data. While I don't believe for a moment that it's actually normal, it's not particularly unusual-looking when you compare it with normal data.[Edit: I just conducted a random poll --- well, I asked my daughter, but at a fairly random time -- and her choice for the least like a straight line was ""d"". So 100% of those surveyed thought ""d"" was the most-odd one.]More formal approach would be to do a Shapiro-Francia test (which is effectively based on the correlation in the QQ-plot), but (a) it's not even as powerful as the Shapiro Wilk test, and (b) formal testing answers a question (sometimes) that you should already know the answer to anyway (the distribution your data were drawn from isn't exactly normal), instead of the question you need answered (how badly does that matter?).As requested, code for the above display. Nothing fancy involved:Note that this was just for the purposes of illustration; I wanted a small data set that looked mildly non-normal which is why I used the residuals from a linear regression on the cars data (the model isn't quite appropriate). However, if I was actually generating such a display for a set of residuals for a regression, I'd regress all 25 data sets on the same $x$'s as in the model, and display QQ plots of their residuals, since residuals have some structure not present in normal random numbers.(I've been making sets of plots like this since the mid-80s at least. How can you interpret plots if you are unfamiliar with how they behave when the assumptions hold --- and when they don't?)See more:Buja, A., Cook, D. Hofmann, H., Lawrence, M. Lee, E.-K., Swayne,  D.F
and Wickham, H. (2009) Statistical Inference for exploratory  data
analysis and model diagnostics Phil. Trans. R. Soc. A 2009  367,
4361-4383 doi: 10.1098/rsta.2009.0120Edit: I mentioned this issue in my second paragraph but I want to emphasize the point again, in case it gets forgotten along the way. What usually matters is not whether you can tell something is not-actually-normal (whether by formal test or by looking at a plot) but rather how much it matters for what you would be using that model to do: How sensitive are the properties you care about to the amount and manner of lack of fit you might have between your model and the actual population?The answer to the question ""is the population I'm sampling actually normally distributed"" is, essentially always, ""no"" (you don't need a test or a plot for that), but the question is rather ""how much does it matter?"". If the answer is ""not much at all"", the fact that the assumption is false is of little practical consequence. A plot can help some since it at least shows you something of the 'amount and manner' of deviation between the sample and the distributional model, so it's a starting point for considering whether it would matter. However, whether it does depends on the properties of what you are doing (consider a t-test vs a test of variance for example; the t-test can in general tolerate much more substantial deviations from the assumptions that are made in its derivation than an F-ratio test of equality variances can)."
Are we exaggerating importance of model assumption and evaluation in an era when analyses are often carried out by laymen,"
Bottom line, the more I learn about statistics, the less I trust published papers in my field; I simply believe that researchers are not doing their statistics well enough.

I'm a layman, so to speak. I'm trained in biology but I have no formal education in statistics or mathematics. I enjoy R and often make an effort to read (and understand...) some of the theoretical foundations of the methods that I apply when doing research. It wouldn't surprise me if the majority of people doing analyses today are actually not formally trained. I've published around 20 original papers, some of which have been accepted by recognized journals and statisticians have frequently been involved in the review-process. My analyses commonly include survival analysis, linear regression, logistic regression, mixed models. Never ever has a reviewer asked about model assumptions, fit or evaluation.
Thus, I never really bothered too much about model assumptions, fit and evaluation. I start with a hypothesis, execute the regression and then present the results. In some instances I made an effort to evaluate these things, but I always ended up with ""well it didn't fulfill all assumptions, but I trust the results (""subject matter knowledge"") and they are plausible, so it's fine"" and when consulting a statistician they always seemed to agree.
Now, I've spoken to other statisticians and non-statisticians (chemists, physicians and biologists) who perform analyses themselves; it seems that people don't really bother too much about all these assumptions and formal evaluations. But here on CV, there is an abundance of people asking about residuals, model fit, ways to evaluate it, eigenvalues, vectors and the list goes on. Let me put it this way, when lme4 warns about large eigenvalues, I really doubt that many of its users care to address that...
Is it worth the extra effort? Is it not likely that the majority of all published results do not respect these assumptions and perhaps have not even assessed them? This is probably a growing issue since databases grow larger every day and there is a notion that the bigger the data, the less important is the assumptions and evaluations.
I could be absolutely wrong, but this is how I have perceived this.
Update:
Citation borrowed from StasK (below): http://www.nature.com/news/science-joins-push-to-screen-statistics-in-papers-1.15509
","['mathematical-statistics', 'multiple-regression', 'modeling']","I am trained as a statistician not as a biologist or medical doctor.  But I do quite a bit of medical research (working with biologists and medical doctors), as part of my research I have learned quite a bit about treatment of several different diseases.  Does this mean that if a friend asks me about a disease that I have researched that I can just write them a prescription for a medication that I know is commonly used for that particular disease?  If I were to do this (I don't), then in many cases it would probably work out OK (since a medical doctor would just have prescribed the same medication), but there is always a possibility that they have an allergy/drug interaction/other that a doctor would know to ask about, that I do not and end up causing much more harm than good.  If you are doing statistics without understanding what you are assuming and what could go wrong (or consulting with a statistician along the way that will look for these things) then you are practicing statistical malpractice.  Most of the time it will probably be OK, but what about the occasion where an important assumption does not hold, but you just ignore it?I work with some doctors who are reasonably statistically competent and can do much of their own analysis, but they will still run it past me.  Often I confirm that they did the correct thing and that they can do the analysis themselves (and they are generally grateful for the confirmation) but occasionally they will be doing something more complex and when I mention a better approach they will usually turn the analysis over to me or my team, or at least bring me in for a more active role.So my answer to your title question is ""No"" we are not exaggerating, rather we should be stressing some things more so that laymen will be more likely to at least double check their procedures/results with a statistician.EditThis is an addition based on Adam's comment below (will be a bit long for another comment).Adam, Thanks for your comment.  The short answer is ""I don't know"".  I think that progress is being made in improving the statistical quality of articles, but things have moved so quickly in many different ways that it will take a while to catch up and guarentee the quality.  Part of the solution is focusing on the assumptions and the consequences of the violations in intro stats courses.  This is more likely to happen when the classes are taught by statisticians, but needs to happen in all classes. Some journals are doing better, but I would like to see a specific statistician reviewer become the standard.  There was an article a few years back (sorry don't have the reference handy, but it was in either JAMA or the New England Journal of Medicine) that showed a higher probability of being published (though not as big a difference as it should be) in JAMA or NEJM if a biostatistican or epidemiologist was one of the co-authors.An interesting article that came out recently is: http://www.nature.com/news/statistics-p-values-are-just-the-tip-of-the-iceberg-1.17412 which discusses some of the same issues."
Software for drawing bayesian networks (graphical models),"
I am searching for [free] software that can produce nice looking graphical models, e.g. 

Any suggestions would be appreciated.
","['graphical-model', 'software']","I currently have a similar problem (drawing multiple path diagrams for my dissertation), and so I was examining many of the options listed here already to draw similar diagrams. Many of the listed resources for drawing such vector graphics (such as in microsoft office or google drawings) can produce really nice path diagrams, with fairly minimal effort. But, part of the reason I was unsatisfied with such programs is that I needed to produce many diagrams, with only fairly minor changes between each diagram (e.g. add another node, change a label). The point and click vector graphics tools aren't well suited for this, and take more effort than need be to make such minor changes. Also it becomes difficult to maintain a template between many drawings. So, I decided to examine options to produce such graphics programattically.Graphviz, as was already mentioned by thias, came really close to having all the bells and whistles I wanted for my graphics (as well as quite simple code to produce them), but it fell short for my needs in two ways; 1) mathematical fonts are lacking (e.g. I'm not sure if you can label a node with the $\beta$ symbol in Graphviz, 2) curved lines are hard to draw (see this post on drawing path diagrams using Graphviz on @Stask's website). Because of these limitations I have currently settled (very happily) on using the Tikz/pgf drawing library in Latex. An example is below of my attempt at reproducing your graphic (the biggest pain was the labels in the lower right corners of the boxes!);Now, I am already writing up my dissertation in Latex, so if you just want the image without having to compile a whole Latex document it is slightly inconveniant, but there are some fairly minor workarounds to produce an image more directly (see this question over on stackoverflow). There are a host of other benifits to using Tikz for such a project thoughAt this time I have not considered some of the libraries for drawing the diagrams in the statistical package R directly from the specified models, but in the future I may consider them to a greater extent. There are some nice examples from the qgraph library for a proof of concept of what can be accomplished in R."
What is the difference between GARCH and ARMA?,"
I am confused. I don't understand the difference a ARMA and a GARCH process.. to me there are the same no ?
Here is the (G)ARCH(p, q) process
$$\sigma_t^2 = 
				\underbrace{
				\underbrace{
					\alpha_0 
					+ \sum_{i=1}^q \alpha_ir_{t-i}^2}
				_{ARCH}  
					+ \sum_{i=1}^p\beta_i\sigma_{t-i}^2}
				_{GARCH}$$
And here is the ARMA($p, q$):
$$ X_t = c + \varepsilon_t +  \sum_{i=1}^p \varphi_i X_{t-i} + \sum_{i=1}^q \theta_i \varepsilon_{t-i}.\,$$
Is the ARMA simply an extension of the GARCH, GARCH being used only for returns and with the assumption $r = \sigma\varepsilon$ where $\varepsilon$ follows a strong white process?
","['time-series', 'arima', 'garch', 'finance']","You are conflating the features of a process with its representation. Consider the (return) process $(Y_t)_{t=0}^\infty$. $$
\begin{align}
\mathbb{E}(Y_t \mid \mathcal{I}_t) &= \alpha_0 + \sum_{j=1}^p \alpha_j Y_{t-j}+ \sum_{k=1}^q \beta_k\epsilon_{t-k}\\
\end{align}
$$
Here, $\mathcal{I}_t$ is the information set at time $t$, which is the $\sigma$-algebra generated by the lagged values of the outcome process $(Y_t)$. Note in particular the first equivalence $ \mathbb{V}(Y_t \mid \mathcal{I}_t)= \mathbb{V}(\epsilon_t \mid \mathcal{I}_t)$.Aside: Based on this representation, you can write
$$
\epsilon_t \equiv \sigma_t Z_t
$$
where $Z_t$ is a strong white noise process, but this follows from the way the process is defined."
"Statistical inference when the sample ""is"" the population","
Imagine you have to do reporting on the numbers of candidates who yearly take a given test. It seems rather difficult to infer the observed % of success, for instance, on a wider population due to the specifity of the target population. So you may consider that these data represent the whole population. 
Are results of tests indicating that the proportions of males and females are different really correct? Does a test comparing observed and theoretical proportions appear to be a correct one, since you consider a whole population (and not a sample)?
","['hypothesis-testing', 'population', 'sampling']","There may be varying opinions on this, but I would treat the population data as a sample and assume a hypothetical population, then make inferences in the usual way.  One way to think about this is that there is an underlying data generating process responsible for the collected data, the ""population"" distribution.  In your particular case, this might make even more sense since you will have cohorts in the future.  Then your population is really cohorts who take the test even in the future.  In this way, you could account for time based variations if you have data for more than a year, or try to account for latent factors through your error model.  In short, you can develop richer models with greater explanatory power."
Random forest computing time in R,"
I am using the party package in R with 10,000 rows and 34 features, and some factor features have more than 300 levels.  The computing time is too long. (It has taken 3 hours so far and it hasn't finished yet.)  
I want to know what elements have a big effect on the computing time of a random forest.  Is it having factors with too many levels? Are there any optimized methods to improve the RF computing time? 
","['r', 'random-forest']",
How to decide on the correct number of clusters?,"
We find the cluster centers and assign points to k different cluster bins in k-means clustering  which is a very well known algorithm and is found almost in every machine learning package on the net. But the missing and most important part in my opinion is the choice of a correct k. What is the best value for it? And, what is meant by best?
I use MATLAB for scientific computing where looking at silhouette plots is given as a way to decide on k discussed here. However, I would be more interested in Bayesian approaches. Any suggestions are appreciated.
","['clustering', 'k-means']","This has been asked a couple of times on stackoverflow: here, here and here. You can take a look at what the crowd over there thinks about this question (or a small variant thereof).Let me also copy my own answer to this question, on stackoverflow.com:Unfortunately there is no way to automatically set the ""right"" K nor is there a definition of what ""right"" is. There isn't a principled statistical method, simple or complex that can set the ""right K"". There are heuristics, rules of thumb that sometimes work, sometimes don't.The situation is more general as many clustering methods have these type of parameters, and I think this is a big open problem in the clustering/unsupervised learning research community."
How to simulate artificial data for logistic regression?,"
I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.
As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:
#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr > 0.5               # take as '1' if probability > 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")

and I get the following error message: 

Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred 

I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?
","['r', 'regression', 'logistic', 'generalized-linear-model', 'simulation']",No. The response variable $y_i$ is a Bernoulli random variable taking value $1$ with probability $pr(i)$.
What is the distribution of the sum of non i.i.d. gaussian variates?,"
I know that if
$$X\sim N(\mu_X, \sigma^2_X), \hspace{1em} Y \sim N(\mu_Y, \sigma^2_Y), \hspace{1em} Z = X + Y, \hspace{1em} X,Y \text{ independent}$$
then
$$Z \sim N(\mu_X + \mu_Y, \sigma^2_X + \sigma^2_Y).$$
But what would happen if $X,Y$ were not independent? i.e. if
$$
\begin{pmatrix}X\\Y\end{pmatrix} \sim N\left( \begin{pmatrix}\mu_X\\\mu_Y\end{pmatrix}, \begin{pmatrix} \sigma^2_X & \sigma_{X,Y} \\ \sigma_{X,Y} & \sigma^2_Y \end{pmatrix} \right).
$$
Would this affect how the sum $Z$ is distributed?
","['normal-distribution', 'mathematical-statistics']","See my comment on probabilityislogic's answer to this question.  Here,
$$
\begin{align*}
X + Y &\sim N(\mu_X + \mu_Y,\; \sigma_X^2 + \sigma_Y^2 + 2\sigma_{X,Y})\\ 
aX + bY &\sim N(a\mu_X + b\mu_Y,\; a^2\sigma_X^2 + b^2\sigma_Y^2 + 2ab\sigma_{X,Y})
\end{align*}
$$ 
where $\sigma_{X,Y}$ is the covariance of $X$ and $Y$.  Nobody writes the off-diagonal entries in the covariance matrix as $\sigma_{xy}^2$ as you have 
done. The off-diagonal entries are covariances which 
can be negative."
Why do we only see $L_1$ and $L_2$ regularization but not other norms?,"
I am just curious why there are usually only $L_1$ and $L_2$ norms regularization. Are there proofs of why these are better?
","['lasso', 'regularization', 'ridge-regression']",
Mathematical Statistics Videos,"
A question previously sought recommendations for textbooks on mathematical statistics
Does anyone know of any good online video lectures on mathematical statistics?
The closest that I've found are:

Machine Learning 
Econometrics 

UPDATE: A number of the suggestions mentioned below are good statistics-101 type videos.
However, I'm specifically wondering whether there are any videos that provide a rigorous mathematical presentation of statistics.
i.e., videos that might accompany a course that use a textbook mentioned in this discussion on mathoverflow
","['mathematical-statistics', 'references']",
"What are the values p, d, q, in ARIMA?","
In the arima function in R, what does order(1, 0, 12) mean? What are the values that can be assigned to p, d, q, and what is the process to find those values?
","['r', 'time-series', 'arima']",
Standard deviation of standard deviation,"
What is an estimator of standard deviation of standard deviation if normality of data can be assumed?
","['estimation', 'standard-deviation', 'normality-assumption']","Let $X_1, ..., X_n \sim N(\mu, \sigma^2)$. As shown in this thread, the standard deviation of the sample standard deviation, $$ 
s = \sqrt{ \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X}) }, $$ is$$ {\rm SD}(s) = \sqrt{ E \left( [E(s)- s]^2 \right) } = \sigma \sqrt{  1 - \frac{2}{n-1} \cdot \left( \frac{ \Gamma(n/2) }{ \Gamma( \frac{n-1}{2} ) } \right)^2 } $$ where $\Gamma(\cdot)$ is the gamma function, $n$ is the sample size and $\overline{X} = \frac{1}{n} \sum_{i=1}^{n} X_i$ is the sample mean. Since $s$ is a consistent estimator of $\sigma$, this suggests replacing $\sigma$ with $s$ in the equation above to get a consistent estimator of ${\rm SD}(s)$. If it is an unbiased estimator you seek, we see in this thread that $ E(s)
= \sigma \cdot \sqrt{ \frac{2}{n-1} }  \cdot \frac{ \Gamma(n/2) }{ \Gamma( \frac{n-1}{2} ) } $, which, by linearity of expectation, suggests $$ s \cdot  \sqrt{ \frac{n-1}{2} } \cdot \frac{\Gamma( \frac{n-1}{2} )}{ \Gamma(n/2) } $$as an unbiased estimator of $\sigma$. All of this together with linearity of expectation gives an unbiased estimator of ${\rm SD}(s)$:  $$ s \cdot \frac{\Gamma( \frac{n-1}{2} )}{ \Gamma(n/2) } \cdot \sqrt{\frac{n-1}{2} - \left( \frac{ \Gamma(n/2) }{ \Gamma( \frac{n-1}{2} ) } \right)^2 } $$ "
Why would R return NA as a lm() coefficient?,"
I am fitting an lm() model to a data set that includes indicators for the financial quarter (Q1, Q2, Q3, making Q4 a default). Using lm(Y~., data = data) I get a NA as the coefficient for Q3, and a warning that one variable was exclude because of singularities. 
Do I need to add a Q4 column?
","['r', 'regression']","NA as a coefficient in a regression indicates that the variable in question is linearly related to the other variables. In your case, this means that $Q3 = a \times Q1 + b \times Q2 + c$ for some $a, b, c$. If this is the case, then there's no unique solution to the regression without dropping one of the variables. Adding $Q4$ is only going to make matters worse."
How does centering make a difference in PCA (for SVD and eigen decomposition)?,"
What difference does centering (or de-meaning) your data make for PCA? I've heard that it makes the maths easier or that it prevents the first PC from being dominated by the variables' means, but I feel like I haven't been able to firmly grasp the concept yet. 
For example, the top answer here How does centering the data get rid of the intercept in regression and PCA? describes how not centering would pull the first PCA through the origin, rather than the main axis of the point cloud. Based on my understanding of how the PC's are obtained from the covariance matrix's eigenvectors, I can't understand why this would happen.
Moreover, my own calculations with and without centering seem to make little sense.
Consider the setosa flowers in the iris dataset in R. I calculated the eigenvectors and eigenvalues of the sample covariance matrix as follows.
data(iris)
df <- iris[iris$Species=='setosa',1:4]
e <- eigen(cov(df))
> e
$values
[1] 0.236455690 0.036918732 0.026796399 0.009033261

$vectors
            [,1]       [,2]       [,3]        [,4]
[1,] -0.66907840  0.5978840  0.4399628 -0.03607712
[2,] -0.73414783 -0.6206734 -0.2746075 -0.01955027
[3,] -0.09654390  0.4900556 -0.8324495 -0.23990129
[4,] -0.06356359  0.1309379 -0.1950675  0.96992969

If I center the dataset first, I get exactly the same results. This seems quite obvious, since centering does not change the covariance matrix at all. 
df.centered <- scale(df,scale=F,center=T)
e.centered<- eigen(cov(df.centered))
e.centered

The prcomp function results in exactly this eigenvalue-eigenvector combination as well, for both the centered and uncentered dataset. 
p<-prcomp(df)
p.centered <- prcomp(df.centered)
Standard deviations:
[1] 0.48626710 0.19214248 0.16369606 0.09504347

Rotation:
                     PC1        PC2        PC3         PC4
Sepal.Length -0.66907840  0.5978840  0.4399628 -0.03607712
Sepal.Width  -0.73414783 -0.6206734 -0.2746075 -0.01955027
Petal.Length -0.09654390  0.4900556 -0.8324495 -0.23990129
Petal.Width  -0.06356359  0.1309379 -0.1950675  0.96992969

However, the prcomp function has the default option center = TRUE. Disabling this option results in the following PC's for the uncentered data (p.centered remains the same when center is set to false):
p.uncentered <- prcomp(df,center=F)
> p.uncentered
Standard deviations:
[1] 6.32674700 0.22455945 0.16369617 0.09766703

Rotation:
                    PC1         PC2        PC3         PC4
Sepal.Length -0.8010073  0.40303704  0.4410167  0.03811461
Sepal.Width  -0.5498408 -0.78739486 -0.2753323 -0.04331888
Petal.Length -0.2334487  0.46456598 -0.8317440 -0.19463332
Petal.Width  -0.0395488  0.04182015 -0.1946750  0.97917752

Why is this different from my own eigenvector calculations on the covariance matrix of the uncentered data? Does it have to do with the calculation? I've seen mentioned that prcomp uses something called the SVD method rather than the eigenvalue decomposition to calculate the PC's. The function princomp uses the latter, but its results are identical to prcomp. Does my issue relate to the answer I described at the top of this post?
EDIT: Issue was cleared up by the helpful @ttnphns. See his comment below, on this question: What does it mean to compute eigenvectors of a covariance matrix if the data were not centered first? and in this answer: https://stats.stackexchange.com/a/22520/3277. In short: a covariance matrix implicitly involves centering of the data already. PCA uses either SVD or eigendecomposition of the centered data $\bf X$, and the covariance matrix is then equal to ${\bf X'X}/(n-1)$.
","['r', 'pca', 'svd', 'eigenvalues', 'centering']","As you remarked yourself and as explained by @ttnphns in the comments, computing covariance matrix implicitly performs centering: variance, by definition, is the average squared deviation from the mean. Centered and non-centered data will have identical covariance matrices. So if by PCA we understand the following procedure: $$\mathrm{Data}\to\text{Covariance matrix}\to\text{Eigen-decomposition},$$ then centering does not make any difference.[Wikipedia:] To find the axes of the ellipse, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data...And so you are right to observe that this is not a very accurate formulation.When people talk about ""PCA on non-centered data"", they mean that instead of covariance matrix the eigen-decomposition is performed on the $\mathbf X^\top \mathbf X/(n-1)$ matrix. If $\mathbf X$ is centered then this will
be exactly the covariance matrix. If not then not. So if by PCA we understand the following procedure:$$\text{Data } \mathbf X\to\text{Matrix } \mathbf X^\top \mathbf X/(n-1)\to\text{Eigen-decomposition},$$then centering matters a lot and has the effect described and illustrated by @ttnphns in How does centering the data get rid of the intercept in regression and PCA?It might seem weird to even mention this ""strange"" procedure, however consider that PCA can be very conveniently performed via singular value decomposition (SVD) of the data matrix $\mathbf X$ itself. I describe this in detail here: Relationship between SVD and PCA. How to use SVD to perform PCA? In this case the procedure is as follows:$$\text{Data } \mathbf X \to \text{Singular value decomposition}.$$If $\mathbf X$ is centered then this is equivalent to standard PCA done via covariance matrix. But if not, then it's equivalent to the ""non-centered"" PCA as described above. Since SVD is a very common and very convenient way to perform PCA, in practice it can be quite important to remember to center the data before calling svd function. I certainly had my share of bugs because of forgetting to do it."
What is a good algorithm for estimating the median of a huge read-once data set? [duplicate],"







This question already has answers here:
                                
                            




Algorithm to dynamically monitor quantiles

                                (6 answers)
                            

Closed 2 years ago.



I'm looking for a good algorithm (meaning minimal computation, minimal storage requirements) to estimate the median of a data set that is too large to store, such that each value can only be read once (unless you explicitly store that value). There are no bounds on the data that can be assumed.
Approximations are fine, as long as the accuracy is known.
Any pointers?
","['algorithms', 'median', 'large-data', 'online-algorithms']","Could you group the data set into much smaller data sets (say 100 or 1000 or 10,000 data points) If you then calculated the median of each of the groups. If you did this with enough data sets you could plot something like the average of the results of each of the smaller sets and this woul, by running enough smaller data sets converge to an 'average' solution. "
How to determine whether or not the y-axis of a graph should start at zero?,"
One common way to ""lie with data"" is to use a y-axis scale that makes it seem as if changes are more significant than they really are.
When I review scientific publications, or students' lab reports, I am often frustrated by this ""data visualization sin"" (which I believe the authors commit unintentionally, but still results in a misleading presentation.)
However, ""always start the y-axis at zero"" is not a hard-and-fast rule. For example, Edward Tufte points out that in a time series, the baseline is not necessarily zero:

In general, in a time-series, use a baseline that shows the data not the zero point. If the zero point reasonably occurs in plotting the data, fine. But don't spend a lot of empty vertical space trying to reach down to the zero point at the cost of hiding what is going on in the data line itself. (The book, How to Lie With Statistics, is wrong on this point.)
For examples, all over the place, of absent zero points in time-series, take a look at any major scientific research publication. The scientists want to show their data, not zero.
The urge to contextualize the data is a good one, but context does not come from empty vertical space reaching down to zero, a number which does not even occur in a good many data sets. Instead, for context, show more data horizontally!

I want to point out misleading presentation in papers I review, but I don't want to be a zero-y-axis purist.
Are there any guidelines that address when to start the y-axis at zero, and when this is unnecessary and/or inappropriate? (Especially in the context of academic work.)
",['data-visualization'],"Don't use space in a graph in any way that doesn't help understanding. Space is needed to show the data!Use your scientific (engineering, medical, social, business, ...) judgement as well as your statistical judgement. (If you are not the client or customer, talk to someone in the field to get an idea of what is interesting or important, preferably those commissioning the analysis.)Show zero on the $y$ axis if comparisons with zero are central to the problem, or even of some interest.Those are three simple rules. (Nothing rules out some tension between them on occasion.)Here is a simple example, but all three points arise: You measure body temperature of a patient in Celsius, or in Fahrenheit, or even in kelvin: take your pick. In what sense whatsoever is it either helpful or even logical to insist on showing zero temperatures? Important, even medically or physiologically crucial, information will be obscured otherwise.Here is a true story from a presentation. A researcher was showing data on sex ratios for various states and union territories in India. The graphic was a bar chart with all bars starting at zero. All bars were close to the same length despite some considerable variation. That was correct, but the interesting story was that areas were different despite similarities, not that they were similar despite differences. I suggested that parity between males and females (1 or 100 females/100 males) was a much more natural reference level. (I would also be open to using some overall level, such as the national mean, as a reference.) Even some statistical people who have heard this little story have sometimes replied, ""No; bars should always start at zero."" To me that is no better than irrelevant dogma in such a case. (I would also argue that dot charts make as much or more sense for such data.)EDIT 27 December 2022. See Smith, Alan. 2022. How Charts Work: Understand and Explain Data with Confidence. Harlow: Pearson, pp.155-161 for an extended example with similar flavour, using the principle that bars showing Gender Parity Index may and should start at the reference value of 1 (genders equally represented).Mentioning bar charts points up that the kind of graph used is important too.
Suppose for body temperatures a $y$ axis range from 35 to 40$^\circ$C is chosen for convenience as including all the data, so that the $y$ axis ""starts"" at 35. Clearly bars all starting at 35 would be a poor encoding of the data. But here the problem would be inappropriate choice of graph element, not poorly chosen axis range.A common kind of plot, especially it seems in some biological and medical sciences, shows means or other summaries by thick bars starting at zero and standard error or standard deviation-based intervals indicating uncertainty by thin bars. Such detonator or dynamite plots, as they have been called by those who disapprove, may be popular partly because of a dictum that zero should always be shown. The net effect is to emphasise comparisons with zero that are often lacking in interest or utility.Some people would want to show zero, but also to add a scale break to show that the scale is interrupted. Fashions change and technology changes. Decades ago, when researchers drew their own graphs or delegated the task to technicians, it was easier to ask that this be done by hand. Now graphics programs often don't support scale breaks, which I think is no loss. Even if they do, that is fussy addition that can waste a moderate fraction of the graphic's area.Note that no-one insists on the same rule for the $x$ axis. Why not? If you show climatic or economic fluctuations for the last century or so, it would be bizarre to be told that the scale should start at the BC/CE boundary or any other origin.There is naturally a zeroth rule that applies in addition to the three mentioned.Thus on this point I agree strongly with Edward Tufte, and I disagree with Darrell Huff.EDIT 9 May 2016:rather than trying to invariably include a 0-baseline in all your
charts, use logical and meaningful baselines insteadCairo, A. 2016.
The Truthful Art: Data, Charts, and Maps for Communication.
San Francisco, CA: New Riders, p.136."
Prediction interval for lmer() mixed effects model in R,"
I want to get a prediction interval around a prediction from a lmer() model. I have found some discussion about this:
http://rstudio-pubs-static.s3.amazonaws.com/24365_2803ab8299934e888a60e7b16113f619.html
http://glmm.wikidot.com/faq
but they seem to not take the uncertainty of the random effects into account.
Here's a specific example. I am racing gold fish. I have data on the past 100 races. I want to predict the 101st, taking into account uncertainty of my RE estimates, and FE estimates. I am including a random intercept for fish (there are 10 different fish), and fixed effect for weight (less heavy fish are quicker).
library(""lme4"")

fish <- as.factor(rep(letters[1:10], each=100))
race <- as.factor(rep(900:999, 10))
oz <- round(1 + rnorm(1000)/10, 3)
sec <- 9 + rep(1:10, rep(100,10))/10 + oz + rnorm(1000)/10

fishDat <- data.frame(fishID = fish, 
      raceID = race, fishWt = oz, time = sec)
head(fishDat)
plot(fishDat$fishID, fishDat$time)

lme1 <- lmer(time ~ fishWt + (1 | fishID), data=fishDat)
summary(lme1)

Now, to predict the 101st race. The fish have been weighed and are ready to go:
newDat <- data.frame(fishID = letters[1:10], 
    raceID = rep(1000, 10),
    fishWt = 1 + round(rnorm(10)/10, 3))
newDat$pred <- predict(lme1, newDat)
newDat

   fishID raceID fishWt     pred
1       a   1000  1.073 10.15348
2       b   1000  1.001 10.20107
3       c   1000  0.945 10.25978
4       d   1000  1.110 10.51753
5       e   1000  0.910 10.41511
6       f   1000  0.848 10.44547
7       g   1000  0.991 10.68678
8       h   1000  0.737 10.56929
9       i   1000  0.993 10.89564
10      j   1000  0.649 10.65480

Fish D has really let himself go (1.11 oz) and is actually predicted to lose to Fish E and Fish F, both of whom he has been better than in the past. However, now I want to be able to say, ""Fish E (weighing 0.91oz) will beat Fish D (weighing 1.11oz) with probability p."" Is there a way to make such a statement using lme4? I want my probability p to take into account my uncertainty in both the fixed effect, and the random effect.
Thanks!
PS looking at the predict.merMod documentation, it suggests ""There is no option for computing standard errors of predictions because it is difficult to define an efficient method that incorporates uncertainty in the variance parameters; we recommend bootMer for this task,"" but by golly, I cannot see how to use bootMer to do this. It seems bootMer would be used to get bootstrapped confidence intervals for parameter estimates, but I could be wrong. 
UPDATED Q:
OK, I think I was asking the wrong question. I want to be able to say, ""Fish A, weighing w oz, will have a race time that is (lcl, ucl) 90% of the time."" 
In the example I have laid out, Fish A, weighing 1.0 oz, will have a race time of 9 + 0.1 + 1 = 10.1 sec on average, with a standard deviation of 0.1. Thus, his observed race time will be between
x <- rnorm(mean = 10.1, sd = 0.1, n=10000)
quantile(x, c(0.05,0.50,0.95))
       5%       50%       95% 
 9.938541 10.100032 10.261243 

90% of the time. I want a prediction function that attempts to give me that answer. Setting all fishWt = 1.0 in newDat, re-running the sim, and using (as suggested by Ben Bolker below)
predFun <- function(fit) {
  predict(fit,newDat)
}
bb <- bootMer(lme1,nsim=1000,FUN=predFun, use.u = FALSE)
predMat <- bb$t

gives
> quantile(predMat[,1], c(0.05,0.50,0.95))
      5%      50%      95% 
10.01362 10.55646 11.05462 

This seems to actually be centered around the population average? As if it's not taking the FishID effect into account? I thought maybe it was a sample size issue, but when I bumped the number of observed races from 100 to 10000, I still get similar results.
I'll note bootMer uses use.u=FALSE by default. On the flip side, using
bb <- bootMer(lme1,nsim=1000,FUN=predFun, use.u = TRUE)

gives
> quantile(predMat[,1], c(0.05,0.50,0.95))
      5%      50%      95% 
10.09970 10.10128 10.10270 

That interval is too narrow, and would seem to be a confidence interval for Fish A's mean time. I want a confidence interval for Fish A's observed race time, not his average race time. How can I get that?
UPDATE 2, ALMOST:
I thought I found what I was looking for in Gelman and Hill (2007) , page 273. Need to utilize the arm package.
library(""arm"")

For Fish A: 
x.tilde <- 1    #observed fishWt for new race
sigma.y.hat <- sigma.hat(lme1)$sigma$data        #get uncertainty estimate of our model
coef.hat <- as.matrix(coef(lme1)$fishID)[1,]    #get intercept (random) and fishWt (fixed) parameter estimates
y.tilde <- rnorm(1000, coef.hat %*% c(1, x.tilde), sigma.y.hat) #simulate
quantile (y.tilde, c(.05, .5, .95))

  5%       50%       95% 
 9.930695 10.100209 10.263551 

For all the fishes:
x.tilde <- rep(1,10)  #assume all fish weight 1 oz
#x.tilde <- 1 + rnorm(10)/10  #alternatively, draw random weights as in original example
sigma.y.hat <- sigma.hat(lme1)$sigma$data
coef.hat <- as.matrix(coef(lme1)$fishID)
y.tilde <- matrix(rnorm(1000, coef.hat %*% matrix(c(rep(1,10), x.tilde), nrow = 2 , byrow = TRUE), sigma.y.hat), ncol = 10, byrow = TRUE)
quantile (y.tilde[,1], c(.05, .5, .95))
       5%       50%       95% 
 9.937138 10.102627 10.234616 

Actually, this probably isn't exactly what I want. I'm only taking into account the overall model uncertainty. In a situation where I have, say, 5 observed races for Fish K and 1000 observed races for Fish L, I think the uncertainty associated with my prediction for Fish K should be much larger than the uncertainty associated with my prediction for Fish L. 
Will look further into Gelman and Hill 2007. I feel I may end up having to switch to BUGS (or Stan).
UPDATE THE 3rd:
Perhaps I am conceptualizing things poorly. Using the predictInterval() function given by Jared Knowles in an answer below gives intervals that aren't quite what I would expect...
library(""lattice"")
library(""lme4"")
library(""ggplot2"")

fish <- c(rep(letters[1:10], each = 100), rep(""k"", 995), rep(""l"", 5))
oz <- round(1 + rnorm(2000)/10, 3)
sec <- 9 + c(rep(1:10, each = 100)/10,rep(1.1, 995), rep(1.2, 5)) + oz + rnorm(2000)

fishDat <- data.frame(fishID = fish, fishWt = oz, time = sec)
dim(fishDat)
head(fishDat)
plot(fishDat$fishID, fishDat$time)

lme1 <- lmer(time ~ fishWt + (1 | fishID), data=fishDat)
summary(lme1)
dotplot(ranef(lme1, condVar = TRUE))

I have added two new fish. Fish K, for whom we have observed 995 races, and Fish L, for whom we have observed 5 races. We have observed 100 races for Fish A-J. I fit the same lmer() as before. Looking at the dotplot() from the lattice package:

By default, dotplot() reorders the random effects by their point estimate. The estimate for Fish L is on the top line, and has a very wide confidence interval. Fish K is on the third line, and has a very narrow confidence interval. This makes sense to me. We have lots of data on Fish K, but not a lot of data on Fish L, so we are more confident in our guesstimate about Fish K's true swimming speed. Now, I would think this would lead to a narrow prediction interval for Fish K, and a wide prediction interval for Fish L when using predictInterval(). Howeva:
newDat <- data.frame(fishID = letters[1:12],
                     fishWt = 1)

preds <- predictInterval(lme1, newdata = newDat, n.sims = 999)
preds
ggplot(aes(x=letters[1:12], y=fit, ymin=lwr, ymax=upr), data=preds) +
  geom_point() + 
  geom_linerange() +
  labs(x=""Index"", y=""Prediction w/ 95% PI"") + theme_bw()


All of those prediction intervals appear to be identical in width. Why isn't our prediction for Fish K narrower the others? Why isn't our prediction for Fish L wider than others?
","['r', 'mixed-model', 'prediction', 'prediction-interval', 'lme4-nlme']","This question and excellent exchange was the impetus for creating the predictInterval function in the merTools package. bootMer is the way to go, but for some problems it is not feasible computationally to generate bootstrapped refits of the whole model (in cases where the model is large). In those cases, predictInterval is designed to use the arm::sim functions to generate distributions of parameters in the model and then to use those distributions to generate simulated values of the response given the newdata provided by the user. It's simple to use -- all you would need to do is:You can specify a whole host of other values to predictInterval including setting the interval for the prediction intervals, choosing whether to report the mean or median of the distribution, and choosing whether or not to include the residual variance from the model. It's not a full prediction interval because the variability of the theta parameters in the lmer object are not included, but all of the other variation is captured through this method, giving a pretty decent approximation. "
Is it possible to do time-series clustering based on curve shape?,"
I have sales data for a series of outlets, and want to categorise them based on the shape of their curves over time.  The data looks roughly like this (but obviously isn't random, and has some missing data):
n.quarters <- 100
n.stores <- 20
if (exists(""test.data"")){
  rm(test.data)
}
for (i in 1:n.stores){
  interval <- runif(1, 1, 200)
  new.df <- data.frame(              
    var0 = interval + c(0, cumsum(runif(49, -5, 5))),
    date = seq.Date(as.Date(""1990-03-30""), by=""3 month"", length.out=n.quarters),
    store = rep(paste(""Store"", i, sep=""""), n.quarters))
  if (exists(""test.data"")){
    test.data <- rbind(test.data, new.df)    
  } else {
    test.data <- new.df
  }
}
test.data$store <- factor(test.data$store)

I would like to know how I can cluster based on the shape of the curves in R.  I had considered the following approach:

Create a new column by linearly transforming each store's var0 to a value between 0.0 and 1.0 for the entire time series.
Cluster these transformed curves using the kml package in R.

I have two questions:

Is this a reasonable exploratory approach?
How can I transform my data into the longitudinal data format that kml will understand?  Any R snippets would be much appreciated!

","['r', 'time-series', 'clustering']","Several directions for analyzing longitudinal data were discussed in the link provided by @Jeromy, so I would suggest you to read them carefully, especially those on functional data analysis. Try googling for ""Functional Clustering of Longitudinal Data"", or the PACE Matlab toolbox which is specifically concerned with model-based clustering of irregularly sampled trajectories (Peng and Müller, Distance-based clustering of sparsely observed stochastic processes, with applications to online auctions, Annals of Applied Statistics 2008 2: 1056). I can imagine that there may be a good statistical framework for financial time series, but I don't know about that.The kml package basically relies on k-means, working (by default) on euclidean distances between the $t$ measurements observed on $n$ individuals. What is called a trajectory is just the series of observed values for individual $i$, $y_i=(y_{i1},y_{i2},\dots,y_{it})$, and $d(y_i,y_j)=\sqrt{t^{-1}\sum_{k=1}^t(y_{ik}-y_{jk})^2}$. Missing data are handled through a slight modification of the preceding distance measure (Gower adjustment) associated to a nearest neighbor-like imputation scheme (for computing Calinski criterion). As I don't represent myself what you real data would look like, I cannot say if it will work. At least, it work with longitudinal growth curves, ""polynomial"" shape, but I doubt it will allow you to detect very specific patterns (like local minima/maxima at specific time-points with time-points differing between clusters, by a translation for example). If you are interested in clustering possibly misaligned curves, then you definitively have to look at other solutions; Functional clustering and alignment, from Sangalli et al., and references therein may provide a good starting point.Below, I show you some code that may help to experiment with it (my seed is generally set at 101, if you want to reproduce the results). Basically, for using kml you just have to construct a clusterizLongData object (an id number for the first column, and the $t$ measurements in the next columns). The next two figures are the raw simulated data and the five-cluster solution (according to Calinski criterion, also used in the fpc package). I don't show the scaled version."
"Is it wrong to rephrase ""1 in 80 deaths is caused by a car accident"" as ""1 in 80 people die as a result of a car accident?""","

Statement One (S1): ""One in 80 deaths is caused by a car accident.""
Statement Two (S2): ""One in 80 people dies as a result of a car accident.""

Now, I personally don't see very much difference at all between these two statements. When writing, I would consider them interchangeable to a lay audience. However, I've been challenged on this by two people now, and am looking for some additional perspective.
My default interpretation of S2 is, ""Of 80 people drawn uniformly at random from the population of humans, we would expect one of them to die as a result of a car accident""- and I do consider this qualified statement equivalent to S1. 
My questions are as follows:

Q1) Is my default interpretation indeed equivalent to Statement One?
Q2) Is unusual or reckless for this to be my default interpretation?
Q3) If you do think S1 and S2 different, such that to state the second when one means the first is misleading/incorrect, could you please provide a fully-qualified revision of S2 that is equivalent?

Let's put aside the obvious quibble that S1 does not specifically refer to human deaths and assume that that is understood in context. Let us also put aside any discussion of the veracity of the claim itself: it is meant to be illustrative.
As best I can tell, the disagreements I've heard so far seem to center around defaulting to different interpretations of the first and second statement.
For the first, my challengers seem to interpret it as as 1/80 * num_deaths = number of deaths caused by car accidents, but for some reason, default to a different interpretation of the second along the lines of, ""if you have any set of 80 people, one of them will die in a car accident"" (which is obviously not an equivalent claim). I would think that given their interpretation of S1, their default for S2 would be to read it as (1/80 * num_dead_people = number of people who died in a car accident == number of deaths caused by car accident). I'm not sure why the discrepancy in interpretation (their default for S2 is a much stronger assumption), or if they have some innate statistical sense that I'm in fact lacking. 
","['interpretation', 'risk']","First of all, my first intuitive thought was: ""S2 can only be the same as S1 if the  traffic death rate stays constant, possibly over decades"" - which certainly wouldn't have been a good assumption in the last so many decades. This already hints that one difficulty lies with implicit/unspoken temporal assumptions.I'd say your statements have the form1 in $x$ $population$ experience $event$.In S1, the population are deaths, and the implied temporal specification is at present or ""in a suitably large [to have sufficent case numbers] but not too wide time frame [to have approximately constant car accident characteristics] around the present""In S2, the population are people. And others seem to read this not as ""dying people"" but as ""living people"" (which after all, is what people more frequently/longer do).
If you read the population as living people, clearly, not one of every 80 people living now dies ""now"" of a car accident. So that is read as ""when they are dying [possibly decades from now], the cause of death is car accident"".Take home message: always be careful to spell out who your population are and the denominator of fractions in general. (Gerd Gigerenzer has papers about not spelling out the denominator being a major cause of confusion, particularly in statistics and risk communication)."
Is there a 1 in 20 or 1 in 400 chance of guessing the outcome of a d20 roll before it happens?,"
My friends are in a bit of an argument over Dungeons & Dragons.
My player managed to guess the outcome of a D20 roll before it happened, and my friend said that his chance of guessing the number was 1 in 20. Another friend argues that his chance of guessing the roll is 1 in 400 because the probability of him randomly guessing a number and then rolling it were both 1 in 20 so the compound probability is 1 in 400. Which of these probabilities is a better characterization of our situation, and what were really his chances?
","['probability', 'dice']","There are 400 possibilities and 20 of them, each occuring with probability $\frac{1}{400}$, have the guess equal to the outcome. So the total probability of having the guess equal to the outcome is $20\cdot \frac{1}{400} = \frac{20}{400} = \frac{1}{20}$$$\small{ \begin{array}{rc}
& \text{OUTCOME}\\
\begin{array}{}
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{S}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{S}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{E}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{U}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{G}} \\
\end{array}
&\begin{array}{c|ccccccccccccccccccccccccccc}
&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18&19&20 \\
\hline
1 & \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
2 & \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
3 & \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
4 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
5 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
6 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
7 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
8 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
9 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
10 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
11 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
12 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
13 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
14 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
15 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
16 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
17 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}\\
18 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}& \frac{1}{400}\\
19 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}& \frac{1}{400}\\
20 & \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \frac{1}{400}& \color{red}{ \frac{1}{400}}\\
\end{array}\end{array}}$$If the guesses do not have equal ${1}/{20}$ probability for each number, but instead values $p_i$ then the 400 possibilities would not be all with probability $(1/20)\cdot(1/20)={1}/{400}$, but instead ${p_i}/{20}$.The concept is not different however. The answer boils down again to the sum of the diagonal and is $\sum_{i=1}^{20} \frac{p_i}{20} = \frac{1}{20} \sum_{i=1}^{20} p_i = \frac{1}{20}$.$$\small{ \begin{array}{rc}
& \text{OUTCOME}\\
\begin{array}{}
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{S}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{S}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{E}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{U}} \\
\require{HTML} \style{display: inline-block; transform: rotate(270deg)}{\text{G}} \\
\end{array}
&\begin{array}{c|ccccccccccccccccccccccccccc}
&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18&19&20 \\
\hline
1& \color{red}{ \frac{p_{1}}{20}}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}& \frac{p_{1}}{20}\\2& \frac{p_{2}}{20}& \color{red}{ \frac{p_{2}}{20}}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}& \frac{p_{2}}{20}\\3& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \color{red}{ \frac{p_{3}}{20}}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}& \frac{p_{3}}{20}\\4& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \color{red}{ \frac{p_{4}}{20}}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}& \frac{p_{4}}{20}\\5& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \color{red}{ \frac{p_{5}}{20}}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}& \frac{p_{5}}{20}\\6& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \color{red}{ \frac{p_{6}}{20}}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}& \frac{p_{6}}{20}\\7& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \color{red}{ \frac{p_{7}}{20}}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}& \frac{p_{7}}{20}\\8& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \color{red}{ \frac{p_{8}}{20}}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}& \frac{p_{8}}{20}\\9& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \color{red}{ \frac{p_{9}}{20}}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}& \frac{p_{9}}{20}\\10& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \color{red}{ \frac{p_{10}}{20}}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}& \frac{p_{10}}{20}\\11& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \color{red}{ \frac{p_{11}}{20}}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}& \frac{p_{11}}{20}\\12& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \color{red}{ \frac{p_{12}}{20}}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}& \frac{p_{12}}{20}\\13& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \color{red}{ \frac{p_{13}}{20}}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}& \frac{p_{13}}{20}\\14& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \color{red}{ \frac{p_{14}}{20}}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}& \frac{p_{14}}{20}\\15& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \color{red}{ \frac{p_{15}}{20}}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}& \frac{p_{15}}{20}\\16& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \color{red}{ \frac{p_{16}}{20}}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}& \frac{p_{16}}{20}\\17& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \color{red}{ \frac{p_{17}}{20}}& \frac{p_{17}}{20}& \frac{p_{17}}{20}& \frac{p_{17}}{20}\\18& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \frac{p_{18}}{20}& \color{red}{ \frac{p_{18}}{20}}& \frac{p_{18}}{20}& \frac{p_{18}}{20}\\19& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \frac{p_{19}}{20}& \color{red}{ \frac{p_{19}}{20}}& \frac{p_{19}}{20}\\20& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \frac{p_{20}}{20}& \color{red}{ \frac{p_{20}}{20}}\\
\end{array}\end{array}}$$More interesting is the case when both probabilities, for the guess and for the outcome, are not uniform (not equal probabilities). For instance, we could imagine rolling an unfair d20 two times. Then the probability will be equal to the expectation $E[p_i] = \sum_{i=1}^{20} p_i^2$. This will be larger than $\frac{1}{20}$ if the $p_i$ are unequal."
What is the difference between the forward-backward and Viterbi algorithms?,"
I want to know what the differences between the forward-backward algorithm and the Viterbi algorithm for inference in hidden Markov models (HMM) are.
","['algorithms', 'hidden-markov-model', 'viterbi-algorithm', 'forward-backward']",
Linear kernel and non-linear kernel for support vector machine?,"
When using support vector machine, are there any guidelines on choosing linear kernel vs. nonlinear kernel, like RBF? I once heard that non-linear kernel tends not to perform well once the number of features is large. Are there any references on this issue?
","['machine-learning', 'classification', 'svm', 'references', 'kernel-trick']","Usually, the decision is whether to use linear or an RBF (aka Gaussian) kernel. There are two main factors to consider:It's been shown that the linear kernel is a degenerate version of RBF, hence the linear kernel is never more accurate than a properly tuned RBF kernel. Quoting the abstract from the paper I linked:The analysis also indicates that if complete model selection using the  Gaussian kernel has been conducted, there is no need to consider linear SVM.A basic rule of thumb is briefly covered in NTU's practical guide to support vector classification (Appendix C).If the number of features is large, one may not need to map data to a higher dimensional space. That is, the nonlinear mapping does not improve the performance.
  Using the linear kernel is good enough, and one only searches for the parameter C.Your conclusion is more or less right but you have the argument backwards. In practice, the linear kernel tends to perform very well when the number of features is large (e.g. there is no need to map to an even higher dimensional feature space). A typical example of this is document classification, with thousands of dimensions in input space.In those cases, nonlinear kernels are not necessarily significantly more accurate than the linear one. This basically means nonlinear kernels lose their appeal: they require way more resources to train with little to no gain in predictive performance, so why bother.Always try linear first since it is way faster to train (AND test). If the accuracy suffices, pat yourself on the back for a job well done and move on to the next problem. If not, try a nonlinear kernel."
"What do ""endogeneity"" and ""exogeneity"" mean substantively?","
I understand that the basic definition of endogeneity is that 
$$
X'\epsilon=0
$$ 
is not satisfied, but what does this mean in a real world sense? I read the Wikipedia article, with the supply and demand example, trying to make sense of it, but it didn't really help. I've heard the other description of endogenous and exogenous as being within the system and being outside the system and that still doesn't make sense to me.
","['regression', 'causality', 'instrumental-variables']",
Logistic Regression in R (Odds Ratio),"
I'm trying to undertake a logistic regression analysis in R. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in R. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing epicalc and/or epitools and/or others, none of which I can get to work, are outdated or lack documentation. I've used glm to do the logistic regression. Any suggestions would be welcome.  
I'd better make this a real question. How do I run a logistic regression and produce odds rations in R?  
Here's what I've done for a univariate analysis:  
x = glm(Outcome ~ Age, family=binomial(link=""logit"")) 
And for multivariate:  
y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit"")) 
I've then looked at x, y, summary(x) and summary(y).  
Is x$coefficients of any value?
","['r', 'logistic', 'odds-ratio']","if you want to interpret the estimated effects as relative odds ratios, just do exp(coef(x)) (gives you $e^\beta$, the multiplicative change in the odds ratio for $y=1$ if the covariate associated with $\beta$ increases by 1). For profile likelihood intervals for this quantity, you can doEDIT: @caracal was quicker..."
How to assess the similarity of two histograms?,"
Given two histograms, how do we assess whether they are similar or not?
Is it sufficient to simply look at the two histograms?
The simple one to one mapping has the problem that if a histogram is slightly different and slightly shifted then we'll not get the desired result.
Any suggestions?
","['histogram', 'image-processing']","A recent paper that may be worth reading is:Cao, Y. Petzold, L. Accuracy limitations and the measurement of errors in the stochastic simulation of chemically reacting systems, 2006.Although this paper's focus is on comparing stochastic simulation algorithms, essentially the main idea is how to compare two histogram. You can access the pdf from the author's webpage."
"Consider the sum of $n$ uniform distributions on $[0,1]$, or $Z_n$. Why does the cusp in the PDF of $Z_n$ disappear for $n \geq 3$?","
I've been wondering about this one for a while; I find it a little weird how abruptly it happens. Basically, why do we need just three uniforms for $Z_n$ to smooth out like it does? And why does the smoothing-out happen so relatively quickly?
$Z_2$:
 
$Z_3$:

(images shamelessly stolen from John D. Cook's blog: http://www.johndcook.com/blog/2009/02/12/sums-of-uniform-random-values/)
Why doesn't it take, say, four uniforms? Or five? Or...?
","['normal-distribution', 'mathematical-statistics', 'uniform-distribution', 'central-limit-theorem']","We can take various approaches to this, any of which may seem intuitive to some people and less than intuitive to others.  To accommodate such variation, this answer surveys several such approaches, covering the major divisions of mathematical thought--analysis (the infinite and the infinitesimal), geometry/topology (spatial relationships), and algebra (formal patterns of symbolic manipulation)--as well as probability itself.  It culminates in an observation that unifies all four approaches, demonstrates there is a genuine question to be answered here, and shows exactly what the issue is.  Each approach provides, in its own way, deeper insight into the nature of the shapes of the probability distribution functions of sums of independent uniform variables.The Uniform $[0,1]$ distribution has several basic descriptions.  When $X$ has such a distribution,The chance that $X$ lies in a measurable set $A$ is just the measure (length) of $A \cap [0,1]$, written $|A \cap [0,1]|$.From this it is immediate that the cumulative distribution function (CDF) is $$F_X(x) = \Pr(X \le x) = |(-\infty, x] \cap [0,1]| = |[0,\min(x,1)]| = \begin{array}{ll} \left\{  
\begin{array}{ll}
 0 & x\lt 0 \\
 x & 0\leq x\leq 1 \\
 1 & x\gt 1.
\end{array}\right.
\end{array} $$The probability density function (PDF), which is the derivative of the CDF, is $f_X(x) = 1$ for $0 \le x \le 1$ and $f_X(x)=0$ otherwise.  (It is undefined at $0$ and $1$.)The characteristic function (CF) of any random variable $X$ is the expectation of $\exp(i t X)$ (where $i$ is the imaginary unit, $i^2=-1$).  Using the PDF of a uniform distribution we can compute$$\phi_X(t) = \int_{-\infty}^\infty \exp(i t x) f_X(x) dx = \int_0^1 \exp(i t x) dx = \left. \frac{\exp(itx)}{it} \right|_{x=0}^{x=1} = \frac{\exp(it)-1}{it}.$$The CF is a (version of the) Fourier transform of the PDF, $\phi(t) = \hat{f}(t)$.  The most basic theorems about Fourier transforms are:The CF of a sum of independent variables $X+Y$ is the product of their CFs.When the original PDF $f$ is continuous and $X$ is bounded, $f$ can be recovered from the CF $\phi$ by a closely related version of the Fourier transform,$$f(x) = \check{\phi}(x) = \frac{1}{2\pi} \int_{-\infty}^\infty \exp(-i x t) \phi(t) dt.$$When $f$ is differentiable, its derivative can be computed under the integral sign:$$f'(x) = \frac{d}{dx} \frac{1}{2\pi} \int_{-\infty}^\infty \exp(-i x t) \phi(t) dt = \frac{-i}{2\pi} \int_{-\infty}^\infty t \exp(-i x t) \phi(t) dt.$$For this to be well-defined, the last integral must converge absolutely; that is,$$\int_{-\infty}^\infty |t \exp(-i x t) \phi(t)| dt = \int_{-\infty}^\infty |t| |\phi(t)| dt$$must converge to a finite value.  Conversely, when it does converge, the derivative exists everywhere by virtue of these inversion formulas.It is now clear exactly how differentiable the PDF for a sum of $n$ uniform variables is: from the first bullet, the CF of the sum of iid variables is the CF of one of them raised to the $n^\text{th}$ power, here equal to $(\exp(i t) - 1)^n / (i t)^n$.  The numerator is bounded (it consists of sine waves) while the denominator is $O(t^{n})$.  We can multiply such an integrand by  $t^{s}$ and it will still converge absolutely when $s \lt n-1$ and converge conditionally when $s = n-1$.  Thus, repeated application of the third bullet shows that the PDF for the sum of $n$ uniform variates will be continuously $n-2$ times differentiable and, in most places, it will be $n-1$ times differentiable.The blue shaded curve is a log-log plot of the absolute value of the real part of the CF of the sum of $n=10$ iid uniform variates.  The dashed red line is an asymptote; its slope is $-10$, showing that the PDF is $10 - 2 = 8$ times differentiable.  For reference, the gray curve plots the real part of the CF for a similarly shaped Gaussian function (a normal PDF).Let $Y$ and $X$ be independent random variables where $X$ has a Uniform $[0,1]$ distribution.  Consider a narrow interval $(t, t+dt]$.  We decompose the chance that $X+Y \in (t, t+dt]$ into the chance that $Y$ is sufficiently close to this interval times the chance that $X$ is just the right size to place $X+Y$ in this interval, given that $Y$ is close enough:$$\eqalign{
f_{X+Y}(t) dt = &\Pr(X+Y\in (t,t+dt])\\
& = \Pr(X+Y\in (t,t+dt] | Y \in (t-1, t+dt]) \Pr(Y \in (t-1, t+dt]) \\
& = \Pr(X \in (t-Y, t-Y+dt] | Y \in (t-1, t+dt]) \left(F_Y(t+dt) - F_Y(t-1)\right) \\
& = 1 dt \left(F_Y(t+dt) - F_Y(t-1)\right).
}$$The final equality comes from the expression for the PDF of $X$.  Dividing both sides by $dt$ and taking the limit as $dt\to 0$ gives$$f_{X+Y}(t) = F_Y(t) - F_Y(t-1).$$In other words, adding a Uniform $[0,1]$ variable $X$ to any variable $Y$ changes the pdf $f_Y$ into a differenced CDF $F_Y(t) - F_Y(t-1)$.  Because the PDF is the derivative of the CDF, this implies that each time we add an independent uniform variable to $Y$, the resulting PDF is one time more differentiable than before.Let's apply this insight, starting with a uniform variable $Y$.  The original PDF is not differentiable at $0$ or $1$: it is discontinuous there.  The PDF of $Y+X$ is not differentiable at $0$, $1$, or $2$, but it must be continuous at those points, because it is the difference of integrals of the PDF of $Y$.  Add another independent uniform variable $X_2$: the PDF of $Y+X+X_2$ is differentiable at $0$,$1$,$2$, and $3$--but it does not necessarily have second derivatives at those points.  And so on.The CDF at $t$ of a sum of $n$ iid uniform variates equals the volume of the unit hypercube $[0,1]^n$ lying within the half-space $x_1+x_2+\cdots+x_n \le t$.  The situation for $n=3$ variates is shown here, with $t$ set at $1/2$, $3/2$, and then $5/2$.As $t$ progresses from $0$ through $n$, the hyperplane $H_n(t): x_1+x_2+\cdots+x_n=t$ crosses vertices at $t=0$, $t=1, \ldots, t=n$.  At each time the shape of the cross section changes: in the figure it first is a triangle (a $2$-simplex), then a hexagon, then a triangle again.  Why doesn't the PDF have sharp bends at these values of $t$?To understand this, first consider small values of $t$.  Here, the hyperplane $H_n(t)$ cuts off an $n-1$-simplex.  All $n-1$ dimensions of the simplex are directly proportional to $t$, whence its ""area"" is proportional to $t^{n-1}$.  Some notation for this will come in handy later.  Let $\theta$ be the ""unit step function,"" $$\theta(x) = \begin{array}{ll} \left\{  
\begin{array}{ll}
 0 & x \lt 0 \\
 1 & x\ge 0.
\end{array}\right.
\end{array} $$If it were not for the presence of the other corners of the hypercube, this scaling would continue indefinitely.  A plot of the area of the $n-1$-simplex would look like the solid blue curve below: it is zero at negative values and equals $t^{n-1}/(n-1)!$ at the positive one, conveniently written $\theta(t) t^{n-1}/(n-1)!$. It has a ""kink"" of order $n-2$ at the origin, in the sense that all derivatives through order $n-3$ exist and are continuous, but that left and right derivatives of order $n-2$ exist but do not agree at the origin.(The other curves shown in this figure are $-3\theta(t-1) (t-1)^{2}/2!$ (red),  $3\theta(t-2) (t-2)^{2}/2!$ (gold), and  $-\theta(t-3) (t-3)^{2}/2!$ (black).  Their roles in the case $n=3$ are discussed further below.) To understand what happens when $t$ crosses $1$, let's examine in detail the case $n=2$, where all the geometry happens in a plane.  We may view the unit ""cube"" (now just a square) as a linear combination of quadrants, as shown here:The first quadrant appears in the lower left panel, in gray.  The value of $t$ is $1.5$, determining the diagonal line shown in all five panels.  The CDF equals the yellow area shown at right.  This yellow area is comprised of:The triangular gray area in the lower left panel,minus the triangular green area in the upper left panel,minus the triangular red area in the low middle panel,plus any blue area in the upper middle panel (but there isn't any such area, nor will there be until $t$ exceeds $2$).Every one of these $2^n=4$ areas is the area of a triangle.  The first one scales like $t^n=t^2$, the next two are zero for $t\lt 1$ and otherwise scale like $(t-1)^n = (t-1)^2$, and the last is zero for $t\lt 2$ and otherwise scales like $(t-2)^n$.  This geometric analysis has established that the CDF is proportional to $\theta(t)t^2 - \theta(t-1)(t-1)^2 - \theta(t-1)(t-1)^2 + \theta(t-2)(t-2)^2$ =  $\theta(t)t^2 - 2 \theta(t-1)(t-1)^2 + \theta(t-2)(t-2)^2$; equivalently, the PDF is proportional to the sum of the three functions $\theta(t)t$, $-2\theta(t-1)(t-1)$, and $\theta(t-2)(t-2)$ (each of them scaling linearly when $n=2$).  The left panel of this figure shows their graphs: evidently, they are all versions of the original graph $\theta(t)t$, but (a) shifted by $0$, $1$, and $2$ units to the right and (b) rescaled by $1$, $-2$, and $1$, respectively.The right panel shows the sum of these graphs (the solid black curve, normalized to have unit area: this is precisely the angular-looking PDF shown in the original question.Now we can understand the nature of the ""kinks"" in the PDF of any sum of iid uniform variables.  They are all exactly like the ""kink"" that occurs at $0$ in the function $\theta(t)t^{n-1}$, possibly rescaled, and shifted to the integers $1,2,\ldots, n$ corresponding to where the hyperplane $H_n(t)$ crosses the vertices of the hypercube.  For $n=2$, this is a visible change in direction: the right derivative of $\theta(t)t$ at $0$ is $0$ while its left derivative is $1$.  For $n=3$, this is a continuous change in direction, but a sudden (discontinuous) change in second derivative.  For general $n$, there will be continuous derivatives through order $n-2$ but a discontinuity in the $n-1^\text{st}$ derivative.The integration to compute the CF, the form of the conditional probability in the probabilistic analysis, and the synthesis of a hypercube as a linear combination of quadrants all suggest returning to the original uniform distribution and re-expressing it as a linear combination of simpler things. Indeed, its PDF can be written$$f_X(x) = \theta(x) - \theta(x-1).$$Let us introduce the shift operator $\Delta$: it acts on any function $f$ by shifting its graph one unit to the right:$$(\Delta f)(x) = f(x-1).$$Formally, then, for the PDF of a uniform variable $X$ we may write$$f_X = (1 - \Delta)\theta.$$The PDF of a sum of $n$ iid uniforms is the convolution of $f_X$ with itself $n$ times.  This follows from the definition of a sum of random variables: the convolution of two functions $f$ and $g$ is the function$$(f \star g)(x) = \int_{-\infty}^{\infty} f(x-y)g(y) dy.$$It is easy to verify that convolution commutes with $\Delta$.  Just change the variable of integration from $y$ to $y+1$:$$\eqalign{
(f \star (\Delta g)) &= \int_{-\infty}^{\infty} f(x-y)(\Delta g)(y) dy \\
&= \int_{-\infty}^{\infty} f(x-y)g(y-1) dy \\
&= \int_{-\infty}^{\infty} f((x-1)-y)g(y) dy \\
&= (\Delta (f \star g))(x).
}$$For the PDF of the sum of $n$ iid uniforms, we may now proceed algebraically to write$$f = f_X^{\star n} = ((1 - \Delta)\theta)^{\star n} = (1-\Delta)^n \theta^{\star n}$$(where the $\star n$ ""power"" denotes repeated convolution, not pointwise multiplication!).  Now $\theta^{\star n}$ is a direct, elementary integration, giving$$\theta^{\star n}(x) = \theta(x) \frac{x^{n-1}}{{n-1}!}.$$The rest is algebra, because the Binomial Theorem applies (as it does in any commutative algebra over the reals):$$f = (1-\Delta)^n \theta^{\star n} = \sum_{i=0}^{n} (-1)^i \binom{n}{i} \Delta^i \theta^{\star n}.$$Because $\Delta^i$ merely shifts its argument by $i$, this exhibits the PDF $f$ as a linear combination of shifted versions of $\theta(x) x^{n-1}$, exactly as we deduced geometrically:$$f(x) = \frac{1}{(n-1)!}\sum_{i=0}^{n} (-1)^i \binom{n}{i} (x-i)^{n-1}\theta(x-i).$$(John Cook quotes this formula later in his blog post, using the notation $(x-i)^{n-1}_+$ for $(x-i)^{n-1}\theta(x-i)$.) Accordingly, because $x^{n-1}$ is a smooth function everywhere, any singular behavior of the PDF will occur only at places where $\theta(x)$ is singular (obviously just $0$) and at those places shifted to the right by $1, 2, \ldots, n$.  The nature of that singular behavior--the degree of smoothness--will therefore be the same at all $n+1$ locations.Illustrating this is the picture for $n=8$, showing (in the left panel) the individual terms in the sum and (in the right panel) the partial sums, culminating in the sum itself (solid black curve): It is useful to note that this last approach has finally yielded a compact, practical expression for computing the PDF of a sum of $n$ iid uniform variables.  (A formula for the CDF is similarly obtained.)The Central Limit Theorem has little to say here. After all, a sum of iid Binomial variables converges to a Normal distribution, but that sum is always discrete: it never even has a PDF at all!  We should not hope for any intuition about ""kinks"" or other measures of differentiability of a PDF to come from the CLT."
How do R and Python complement each other in data science?,"
In many tutorials or manuals the narrative seems to imply that R and python coexist as complementary components of the analysis process. To my untrained eye, however, it seems that both languages sort of do the same thing.
So my question is if there are really specialized niches for the two languages or if it's just a personal preference whether to use one or the other? 
","['r', 'python', 'software']",
Generic sum of Gamma random variables,"
I have read that the sum of Gamma random variables with the same scale parameter is another Gamma random variable. I've also seen the paper by Moschopoulos describing a method for the summation of a general set of Gamma random variables. I have tried implementing Moschopoulos' method but have yet to have success.
What does the summation of a general set of Gamma random variables look like? To make this question concrete, what does it look like for:
$\text{Gamma}(3,1) + \text{Gamma}(4,2) + \text{Gamma}(5,1)$
If the parameters above are not particularly revealing, please suggest others.
","['probability', 'distributions', 'gamma-distribution', 'sum', 'saddlepoint-approximation']","First, combine any sums having the same scale factor: a $\Gamma(n, \beta)$ plus a $\Gamma(m,\beta)$ variate form a $\Gamma(n+m,\beta)$ variate.Next, observe that the characteristic function (cf) of $\Gamma(n, \beta)$ is $(1-i \beta  t)^{-n}$, whence the cf of a sum of these distributions is the product$$\prod_{j} \frac{1}{(1-i \beta_j  t)^{n_j}}.$$When the $n_j$ are all integral, this product expands as a partial fraction into a linear combination of $(1-i \beta_j  t)^{-\nu}$ where the $\nu$ are integers between $1$ and $n_j$.  In the example with $\beta_1 = 1, n_1=8$ (from the sum of $\Gamma(3,1)$ and $\Gamma(5,1)$) and $\beta_2 = 2, n_2=4$ we find$$\begin{aligned}&\frac{1}{(1-i t)^{8}}\frac{1}{(1- 2i t)^{4}} = \\
&\frac{1}{(t+i)^8}-\frac{8 i}{(t+i)^7}-\frac{40}{(t+i)^6}+\frac{160 i}{(t+i)^5}+\frac{560}{(t+i)^4}-\frac{1792 i}{(t+i)^3}\\
&-\frac{5376}{(t+i)^2}+\frac{15360 i}{t+i}+\frac{256}{(2t+i)^4}+\frac{2048 i}{(2 t+i)^3}-\frac{9216}{(2t+i)^2}-\frac{30720 i}{2t+i}.
\end{aligned}$$The inverse of taking the cf is the inverse Fourier Transform, which is linear: that means we may apply it term by term.  Each term is recognizable as a multiple of the cf of a Gamma distribution and so is readily inverted to yield the PDF.  In the example we obtain$$\begin{aligned}
&\frac{e^{-t} t^7}{5040}+\frac{1}{90} e^{-t} t^6+\frac{1}{3} e^{-t} t^5+\frac{20}{3} e^{-t} t^4+\frac{8}{3} e^{-\frac{t}{2}} t^3+\frac{280}{3} e^{-t} t^3\\
&-128 e^{-\frac{t}{2}} t^2+896 e^{-t} t^2+2304 e^{-\frac{t}{2}} t+5376 e^{-t} t-15360 e^{-\frac{t}{2}}+15360 e^{-t}
\end{aligned}$$for the PDF of the sum.This is a finite mixture of Gamma distributions having scale factors equal to those within the sum and shape factors less than or equal to those within the sum.  Except in special cases (where some cancellation might occur), the number of terms is given by the total shape parameter $n_1 + n_2 + \cdots$ (assuming all the $n_j$ are different).As a test, here is a histogram of $10^4$ results obtained by adding independent draws from the $\Gamma(8,1)$ and $\Gamma(4,2)$ distributions.  On it is superimposed the graph of $10^4$ times the preceding function.  The fit is very good.Moschopoulos carries this idea one step further by expanding the cf of the sum into an infinite series of Gamma characteristic functions whenever one or more of the $n_i$ is non-integral, and then terminates the infinite series at a point where it is reasonably well approximated."
How to determine best cutoff point and its confidence interval using ROC curve in R?,"
I have the data of a test that could be used to distinguish normal and tumor cells. According to ROC curve it looks good for this purpose (area under curve is 0.9):

My questions are: 

How to determine cutoff point for this test and its confidence interval where readings should be judged as ambiguous?
What is the best way to visualize this (using ggplot2)?

Graph is rendered using ROCR and ggplot2 packages:
#install.packages(""ggplot2"",""ROCR"",""verification"") #if not installed yet
library(""ggplot2"")
library(""ROCR"")
library(""verification"")
d <-read.csv2(""data.csv"", sep="";"")
pred <- with(d,prediction(x,test))
perf <- performance(pred,""tpr"", ""fpr"")
auc <-performance(pred, measure = ""auc"")@y.values[[1]]
rd <- data.frame(x=perf@x.values[[1]],y=perf@y.values[[1]])
p <- ggplot(rd,aes(x=x,y=y)) + geom_path(size=1)
p <- p + geom_segment(aes(x=0,y=0,xend=1,yend=1),colour=""black"",linetype= 2)
p <- p + geom_text(aes(x=1, y= 0, hjust=1, vjust=0, label=paste(sep = """", ""AUC = "",round(auc,3) )),colour=""black"",size=4)
p <- p + scale_x_continuous(name= ""False positive rate"")
p <- p + scale_y_continuous(name= ""True positive rate"")
p <- p + opts(
            axis.text.x = theme_text(size = 10),
            axis.text.y = theme_text(size = 10),
            axis.title.x = theme_text(size = 12,face = ""italic""),
            axis.title.y = theme_text(size = 12,face = ""italic"",angle=90),
            legend.position = ""none"",
            legend.title = theme_blank(),
            panel.background = theme_blank(),
            panel.grid.minor = theme_blank(), 
            panel.grid.major = theme_line(colour='grey'),
            plot.background = theme_blank()
            )
p

data.csv contains the following data:
x;group;order;test
56;Tumor;1;1
55;Tumor;1;1
52;Tumor;1;1
60;Tumor;1;1
54;Tumor;1;1
43;Tumor;1;1
52;Tumor;1;1
57;Tumor;1;1
50;Tumor;1;1
34;Tumor;1;1
24;Normal;2;0
34;Normal;2;0
22;Normal;2;0
32;Normal;2;0
25;Normal;2;0
23;Normal;2;0
23;Normal;2;0
19;Normal;2;0
56;Normal;2;0
44;Normal;2;0

","['r', 'data-visualization', 'confidence-interval', 'roc', 'ggplot2']",Thanks to all who aswered this question. I agree that there could be no one correct answer and criteria greatly depend on the aims that stand behind of the certain diagnostic test.Finally I had found an R package OptimalCutpoints dedicated exactly to finding cutoff point in such type of analysis. Actually there are several methods of determining cutoff point. So now the task is narrowed to selecting the method that is the best match for each situation.There are many other configuration options described in package documentation including several methods of determining confidence intervals and detailed description of each of the methods.
Interpretation of R's output for binomial regression,"
I'm quite new on this with binomial data tests, but needed to do one and now I´m not sure how to interpret the outcome. The y-variable, the response variable, is binomial and the explanatory factors are continuous. This is what I got when summarizing the outcome:
glm(formula = leaves.presence ~ Area, family = binomial, data = n)

Deviance Residuals: 
Min      1Q  Median      3Q     Max  
-1.213  -1.044  -1.023   1.312   1.344  

Coefficients:
                        Estimate Std. Error z value Pr(>|z|) 
(Intercept)           -0.3877697  0.0282178 -13.742  < 2e-16 ***
leaves.presence        0.0008166  0.0002472   3.303 0.000956 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 16662  on 12237  degrees of freedom
Residual deviance: 16651  on 12236  degrees of freedom
(314 observations deleted due to missingness)
AIC: 16655
Number of Fisher Scoring iterations: 4

There's a number of things I don't get here, what does this really say:
                        Estimate Std. Error z value Pr(>|z|) 
(Intercept)           -0.3877697  0.0282178 -13.742  < 2e-16 ***
leaves.presence        0.0008166  0.0002472   3.303 0.000956 ***

And what does AIC and Number of Fisher Scoring iterations mean?
> fit
Call:  glm(formula = Lövförekomst ~ Areal, family = binomial, data = n)

Coefficients:
(Intercept)        Areal  
-0.3877697    0.0008166  

Degrees of Freedom: 12237 Total (i.e. Null);  12236 Residual
(314 observations deleted due to missingness)
Null Deviance:      16660 
Residual Deviance: 16650        AIC: 16650

And here what does this mean:
Coefficients:
(Intercept)        Areal  
-0.3877697    0.0008166 

","['r', 'regression', 'logistic', 'binomial-distribution', 'interpretation']",
What are the main theorems in Machine (Deep) Learning?,"
Al Rahimi has recently given a very provocative talk in NIPS 2017 comparing current Machine Learning to Alchemy. One of his claims is that we need to get back to theoretical developments, to have simple theorems proving foundational results. 
When he said that, I started looking for the main theorems for ML, but could not find a good reference making sense of the main results. So here is my question: what are the current main mathematical theorems (theory) in ML/DL and what do they prove? I would guess Vapnik's work would go somewhere here. As an extra, what are the main theoretical open problems?
","['machine-learning', 'deep-learning', 'mathematical-statistics']","As I wrote in the comments, this question seems too broad to me, but I'll make an attempt to an answer. In order to set some boundaries, I will start with a little math which underlies most of ML, and then concentrate on recent results for DL.The bias-variance tradeoff is referred to in countless books, courses, MOOCs, blogs, tweets, etc. on ML, so we can't start without mentioning it:$$\mathbb{E}[(Y-\hat{f}(X))^2|X=x_0]=\sigma_{\epsilon}^2+\left(\mathbb{E}\hat{f}(x_0)-f(x_0)\right)^2+\mathbb{E}\left[\left(\hat{f}(x_0)-\mathbb{E}\hat{f}(x_0)\right)^2\right]=\text{Irreducible error + Bias}^2 \text{ + Variance}$$Proof here: https://web.stanford.edu/~hastie/ElemStatLearn/The Gauss-Markov Theorem (yes, linear regression will remain an important part of Machine Learning, no matter what: deal with it) clarifies that, when the linear model is true and some assumptions on the error term are valid, OLS has the minimum mean squared error (which in the above expression is just $\text{Bias}^2 \text{ + Variance}$) only among the unbiased linear estimators of the linear model. Thus there could well be linear estimators with bias (or nonlinear estimators) which have a better mean square error, and thus a better expected prediction error, than OLS. And this paves the way to all the regularization arsenal (ridge regression, LASSO, weight decay, etc.) which is a workhorse of ML. A proof is given here (and in countless other books):
https://www.amazon.com/Linear-Statistical-Models-James-Stapleton/dp/0470231467Probably more relevant to the explosion of regularization approaches, as noted by Carlos Cinelli in the comments, and definitely more fun to learn about, is the James-Stein theorem. Consider $n$ independent, same variance but not same mean Gaussian random variables:$$X_i|\mu_i\sim \mathcal{N}(\theta_i,\sigma^2), \quad i=1,\dots,n$$in other words, we have an $n-$components Gaussian random vector $\mathbf{X}\sim \mathcal{N}(\boldsymbol{\theta},\sigma^2I)$. We have one sample $\mathbf{x}$ from $\mathbf{X}$ and we want to estimate $\boldsymbol{\theta}$. The MLE (and also UMVUE) estimator is obviously $\hat{\boldsymbol{\theta}}_{MLE}=\mathbf{x}$. Consider the James-Stein estimator$$\hat{\boldsymbol{\theta}}_{JS}= \left(1-\frac{(n-2)\sigma^2}{||\mathbf{x}||^2}\right)\mathbf{x} $$Clearly, if $(n-2)\sigma^2\leq||\mathbf{x}||^2$, $\hat{\boldsymbol{\theta}}_{JS}$ shrinks the MLE estimate towards zero. The James-Stein theorem states that for $n\geq4$, $\hat{\boldsymbol{\theta}}_{JS}$ strictly dominates $\hat{\boldsymbol{\theta}}_{MLE}$, i.e., it has lower MSE $\forall \ \boldsymbol{\theta}$. Pheraps surprisingly, even if we shrink towards any other constant $\boldsymbol{c}\neq \mathbf{0}$, $\hat{\boldsymbol{\theta}}_{JS}$ still dominates $\hat{\boldsymbol{\theta}}_{MLE}$. Since the $X_i$ are independent, it may seem weird that, when trying to estimate  the height of three unrelated persons, including a sample from the number of apples produced in Spain, may improve our estimate on average. The key point here is ""on average"": the mean square error for the simultaneous estimation of all the components of the parameter vector is smaller, but the square error for one or more components  may well be larger, and indeed it often is, when you have ""extreme"" observations.Finding out that MLE, which was indeed the ""optimal"" estimator for the univariate estimation case, was dethroned for multivariate estimation, was quite a shock at the time, and led to a great interest in shrinkage, better known as regularization in ML parlance. One could note some similarities with mixed models and the concept of ""borrowing strength"": there is indeed some connection, as discussed hereUnified view on shrinkage: what is the relation (if any) between Stein's paradox, ridge regression, and random effects in mixed models?Reference: James, W., Stein, C., Estimation with Quadratic Loss. Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, 361--379, University of California Press, Berkeley, Calif., 1961Principal Component Analysis is key to the important topic of dimension reduction, and it's based on the Singular Value Decomposition: for each $N\times p$ real matrix $X$ (although the theorem easily generalizes to complex matrices) we can write$$X=UDV^T$$where $U$ of size $N \times p$ is orthogonal, $D$ is a $p \times p$ diagonal matrix with nonnegative diagonal elements and $U$ of size $p \times p$ is again orthogonal. For proofs and algorithms on how to compute it see: Golub, G., and Van Loan, C. (1983), Matrix computations, John Hopkins University press, Baltimore.Mercer's theorem is the founding stone for a lot of different ML methods: thin plate splines, support vector machines, the Kriging estimate of a Gaussian random process, etc. Basically, is one of the two theorems behind the so-called kernel trick. Let $K(x,y):[a,b]\times[a,b]\to\mathbb{R}$ be a symmmetric continuous function or kernel. if $K$ is positive semidefinite, then it admits an orthornormal basis of eigenfunctions corresponding to nonnegative eigenvalues:$$K(x,y)=\sum_{i=1}^\infty\gamma_i \phi_i(x)\phi_i(y)$$The importance of this theorem for ML theory is testified by the number of references it gets in famous texts, such as for example Rasmussen & Williams text on Gaussian processes.Reference: J. Mercer, Functions of positive and negative type, and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 209:415-446, 1909There is also a simpler presentation in Konrad Jörgens, Linear integral operators, Pitman, Boston, 1982.The other theorem which, together with Mercer's theorem, lays out the theoretical foundation of the kernel trick, is the representer theorem. Suppose you have a sample space $\mathcal{X}$ and a symmetric positive semidefinite kernel $K: \mathcal{X} \times \mathcal{X}\to \mathbb{R}$. Also let $\mathcal{H}_K$ be the RKHS associated with $K$. Finally, let $S=\{\mathbb{x}_i,y_i\}_{i=1}^n$ be a training sample. The theorem says that among all functions $f\in \mathcal{H}_K$, which all admit an infinite representation in terms of eigenfunctions of $K$ because of Mercer's theorem, the one that minimizes the regularized risk always has a finite representation in the basis formed by the kernel evaluated at the $n$ training points, i.e.$$\min_{f \in \mathcal{H}_K} \sum_{i=1}^n L(y_i,f(x_i))+\lambda||f||^2_{\mathcal{H}_K}=\min_{\{c_j\}_1^\infty} \sum_{i=1}^n L(y_i,\sum_j^\infty c_j\phi_j(x_i))+\lambda\sum_j^\infty \frac{c_j^2}{\gamma_j}=\sum_{i=1}^n\alpha_i K(x,x_i)$$(the theorem is the last equality). References: Wahba, G. 1990, Spline Models for Observational Data, SIAM, Philadelphia.The universal approximation theorem has been already cited by user Tobias Windisch and is much less relevant to Machine Learning than it is to functional analysis, even if it may not seem so at a first glance. The problem is that the theorem only says that such a network exists, but:A smaller pain point with the Hornik's version of this theorem is that it doesn't hold for ReLU activation functions. However, Bartlett has since proved  an extended version which covers this gap.Until now, I guess all the theorems I considered were well-known to anybody. So now it's time for the fun stuff :-) Let's see a few Deep Learning theorems:Assumptions:Then:This is very interesting: CNNs made only of convolutional layers, ReLU, max-pooling, fully connected ReLU and linear layers are positively homogenous functions, while if we include sigmoid activation functions, this isn't true anymore, which may partly explain the superior performance in some applications of ReLU + max pooling with respect to sigmoids. What's more, the theorems only hold if also $\Theta$ is positively homogeneous in $W$ of the same degree as $\Phi$. Now, the fun fact is that  $l_1$ or $l_2$ regularization, although positively homogeneous, don't have the same degree of $\Phi$ (the degree of $\Phi$, in the simple CNN case mentioned before, increases with the number of layers). Instead, more modern regularization methods such as batch normalization and path-SGD do correspond to a positively homogeneous regularization function of the same degree as $\Phi$, and dropout, while not fitting this framework exactly, holds strong similarities to it. This may explain why, in order to get high accuracy with CNNs, $l_1$ and $l_2$ regularization are not enough, but we need to employ all kinds of devilish tricks, such as dropout and batch normalization! To the best of my knowledge, this is the closest thing to an explanation of the efficacy of batch normalization, which is otherwise very obscure, as correctly noted by Al Rahimi in his talk.Another observation that some people make, based on Theorem 1, is that it could explain why ReLU work well, even with the problem of dead neurons. According to this intuition, the fact that, during training, some ReLU neurons ""die"" (go to zero activation and then never recover from that, since for $x<0$ the gradient of ReLU is zero) is ""a feature, not a bug"", because if we have reached a minimum and a full subnetwork has died, then we're provably reached a global minimum (under the hypotheses of Theorem 1). I may be missing something, but I think this interpretation is far-fetched. First of all, during training ReLUs can ""die"" well before we have reached a local minimun. Secondly, it has to be proved that when ReLU units ""die"", they always do it over a full subnetwork: the only case where this is trivially true is when you have just one hidden layer, in which case of course each single neuron is a subnetwork. But in general I would be very cautious in seeing ""dead neurons"" as a good thing.References:B. Haeffele and R. Vidal, Global optimality in neural network training,
In IEEE Conference on Computer Vision and Pattern Recognition,
2017.B. Haeffele and R. Vidal. Global optimality in tensor factorization,
deep learning, and beyond, arXiv, abs/1506.07540, 2015.Image classification requires learning representations which are invariant (or at least robust, i.e., very weakly sensitive) to various transformations such as  location, pose, viewpoint, lighting, expression, etc. which are commonly present in natural images, but do not contain info for the classification task. Same thing for speech recognition: changes in pitch, volume, pace, accent. etc. should not lead to a change in the classification of the word. Operations such as convolution, max pooling, average pooling, etc., used in CNNs, have exactly this goal, so intuitively we expect that they would work for these applications. But do we have theorems to support this intuition? There is a vertical translation invariance theorem, which, notwithstanding the name, has nothing to do with translation in the vertical direction, but it's basically a result which says that features learnt in following layers get more and more invariant, as the number of layers grows. This is opposed to an older horizontal translation invariance theorem which however holds for scattering networks, but not for CNNs.
The theorem is very technical, however:Indicate with $\Phi^n(f)$ the output of layer $n$ of the CNN, when the input is $f$. Then finally:$$\lim_{n\to\infty}|||\Phi^n(T_f f)-\Phi^n(f)|||=0$$(the triple bars are not an error) which basically means that each layer learns features which become more and more invariant, and in the limit of an infinitely deep network we have a perfectly invariant architecture. Since CNNs have a finite number of layers, they're not perfectly translation-invariant, which is something well-known to practitioners.Reference: T. Wiatowski and H. Bolcskei, A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction, arXiv:1512.06293v3.To conclude, numerous bounds for the generalization error of a Deep Neural Network based on its Vapnik-Chervonkensis dimension or on the Rademacher complexity grow with the number of parameters (some even exponentially), which means they can't explain why DNNs work so well in practice even when the number of parameters is considerably larger than the number of training samples. As a matter of fact, VC theory is not very useful in Deep Learning.Conversely, some results from last year bound the generalization error of a DNN classifier with a quantity which is independent of the neural network's depth and size, but depends only on the structure of the training set and the input space. Under some pretty technical assumptions on the learning procedure, and on the training set and input space, but with very little assumptions on the DNN (in particular, CNNs are fully covered), then with probability at least $1-\delta$, we have$$\text{GE} \leq \sqrt{2\log{2}N_y\frac{\mathcal{N_{\gamma}}}{m}}+\sqrt{\frac{2\log{(1/\delta)}}{m}}$$where:J. Sokolic, R. Giryes, G. Sapiro, and M. Rodrigues. Generalization
error of invariant classifiers. In AISTATS, 2017"
Logistic regression: anova chi-square test vs. significance of coefficients (anova() vs summary() in R),"
I have a logistic GLM model with 8 variables. I ran a chi-square test in R anova(glm.model,test='Chisq') and 2 of the variables turn out to be predictive when ordered at the top of the test and not so much when ordered at the bottom. The summary(glm.model) suggests that their coefficients are insignificant (high p-value). In this case it seems that the variables are not significant.
I wanted to ask which is a better test of variables significance - the coefficient significance in the model summary or the chi-square test from anova(). Also - when is either one better over the other?
I guess it's a broad question but any pointers on what to consider will be appreciate.
","['r', 'regression', 'logistic', 'statistical-significance', 'generalized-linear-model']","In addition to @gung's answer, I'll try to provide an example of what the anova function actually tests. I hope this enables you to decide what tests are appropriate for the hypotheses you are interested in testing.Let's assume that you have an outcome $y$ and 3 predictor variables: $x_{1}$, $x_{2}$, and $x_{3}$. Now, if your logistic regression model would be my.mod <- glm(y~x1+x2+x3, family=""binomial""). When you run anova(my.mod, test=""Chisq""), the function compares the following models in sequential order. This type is also called Type I ANOVA or Type I sum of squares (see this post for a comparison of the different types):So it sequentially compares the smaller model with the next more complex model by adding one variable in each step. Each of those comparisons is done via a likelihood ratio test (LR test; see example below). To my knowledge, these hypotheses are rarely of interest, but this has to be decided by you.Here is an example in R:  The $p$-values in the output of summary(my.mod) are Wald tests which test the following hypotheses (note that they're interchangeable and the order of the tests does not matter):So each coefficient against the full model containing all coefficients. Wald tests are an approximation of the likelihood ratio test. We could also do the likelihood ratio tests (LR test). Here is how:The $p$-values from the likelihood ratio tests are very similar to those obtained by the Wald tests by summary(my.mod) above.Note: The third model comparison for rank of anova(my.mod, test=""Chisq"") is the same as the comparison for rank in the example below (anova(mod1.2, my.mod, test=""Chisq"")). Each time, the $p$-value is the same, $7.088\cdot 10^{-5}$. It is each time the comparison between the model without rank vs. the model containing it. "
Do we have to tune the number of trees in a random forest?,"
Software implementations of random forest classifiers have a number of parameters to allow users to fine-tune the algorithm's behavior, including the number of trees $T$ in the forest. Is this a parameter that needs to be tuned, in the same way as $m$, the number of features to try at each split (what Leo Breiman calls mtry)?
","['classification', 'optimization', 'random-forest', 'hyperparameter']","It's common to find code snippets that treat $T$ as a hyper-parameter, and attempt to optimize over it in the same way as any other hyper-parameter. This is just wasting computational power: when all other hyper-parameters are fixed, the model’s loss stochastically decreases as the number of trees increases.Each tree in a random forest is identically distributed. The trees are identically distributed because each tree is grown using a randomization strategy that is repeated for each tree: boot-strap the training data, and then grow each tree by picking the best split for a feature from among the $m$ features selected for that node. The random forest procedure stands in contrast to boosting because the trees are grown on their own bootstrap subsample without regard to any of the other trees. (It is in this sense that the random forest algorithm is ""embarrassingly parallel"": you can parallelize tree construction because each tree is fit independently.)In the binary case, each random forest tree votes 1 for the positive class or 0 for the negative class for each sample. The average of all of  these votes is taken as the classification score of the entire forest. (In the general $k$-nary case, we simply have a categorical distribution instead, but all of these arguments still apply.)The Weak Law of Large Numbers is applicable in these circumstances becauseApplying WLLN in this case implies that, for each sample, the ensemble will tend toward a particular mean prediction value for that sample as the number of trees tends towards infinity. Additionally, for a given set of samples, a statistic of interest among those samples (such as the expected log-loss) will converge to a mean value as well, as the number of trees tends toward infinity.Hastie et al. address this question very briefly in ESL (page 596).Another claim is that random forests “cannot overfit” the data. It is certainly true that increasing $\mathcal{B}$ [the number of trees in the ensemble] does not cause the random forest sequence to overfit... However, this limit can overfit the data; the average of fully grown trees can result in too rich a model, and incur unnecessary variance. Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter.Stated another way, for a fixed hyperparameter configuration, increasing the number of trees cannot overfit the data; however, the other hyperparameters might be a source of overfit.This section summarizes Philipp Probst & Anne-Laure Boulesteix ""To tune or not to tune the number of trees in random forest?"". The key results area. The expected error rate (equiv. $\text{error rate} = 1 - \text{accuracy}$) as a function of $T$ the number of trees is given by
$$
E(e_i(T)) = P\left(\sum_{t=1}^T e_{it} > 0.5\cdot T\right)
$$
where $e_{it}$ is a binomial r.v. with expectation $E(e_{it}) = \epsilon_i$, the decision of a particular tree indexed by $t$. This function is increasing in $T$ for $\epsilon_{i} > 0.5$ and decreasing in $T$ for $\epsilon_{i} < 0.5$. The authors observeWe see that the convergence rate of the error rate curve is only dependent on the distribution of the $\epsilon_i$ of the observations. Hence, the convergence rate of the error rate curve is not directly dependent on the number of observations n or the number of features, but these characteristics could influence the empirical distribution of the $\epsilon_i$’s and hence possibly the convergence rate as outlined in Section 4.3.1b. The authors note that ROC AUC (aka $c$-statistic) can be manipulated to have monotonous or non-monotonous curves as a function of $T$ depending on how the samples' expected scores align to their true classes.a. The Breier Score has expectation
$$
E(b_i(T)) = E(e_{it})^2 + \frac{\text{Var}(e_{it})}{T}
$$
which is clearly a monotonously decreasing function of $T$.b. The log-loss (aka cross entropy loss) has expectation which can be approximated by a Taylor expansion
$$
E(l_i(T)) \approx -\log(1 - \epsilon_i + a) + \frac{\epsilon_i (1 - \epsilon_i) }{ 2 T (1 - \epsilon_i + a)^2}
$$
which is likewise a decreasing function of $T$. (The constant $a$ is a small positive number that keeps the values inside the logarithm and denominator away from zero.)This is a practical demonstration using the diamonds data that ships with ggplot2. I turned it into a classification task by binarizing the price into ""high"" and ""low"" categories, with the dividing line determined by the median price.From the perspective of cross-entropy, model improvements are very smooth. (However, the plot is not monotonic -- the divergence from the theoretical results presented above is because the theoretical results pertain to the expectation, rather than to the particular realizations of any one experiment.)On the other hand, error rate is deceptive in the sense that it can swing up or down, and sometimes stay there for a number of additional trees, before reverting. This is because it does not measure the degree of incorrectness of the classification decision. This can cause the error rate to have ""blips"" of improved performance w.r.t. the number of trees, by which I mean that some sample which is on the decision boundary will bounce back and forth between predicted classes. A very large number of trees can be required for this behavior to be more-or-less suppressed.Also, look at the behavior of error rate for a very small number of trees -- the results are wildly divergent! This implies that a method premised on choosing the number of trees this way is subject to a large amount of randomness. Moreover, repeating the same experiment with a different random seed could lead one to select a different number of trees purely on the basis of this randomness. In this sense, the behavior of the error rate for a small number of trees is entirely an artifact, both because we know that the LLN means that as the number of trees increases, this will tend towards its expectation, and because of the theoretical results in section 2. (Cross-validated hosts a number of questions comparing the merits of error rate/accuracy to other statistics.)By contrast, the cross-entropy measurement is essentially stable after 200 trees, and virtually flat after 500.Finally, I repeated the exact same experiment for error rate with a different random seed. The results are strikingly different for small $T$.Code for this demonstration is available in this gist.Tuning the number of trees is unnecessary; instead, simply set the number of trees to a large, computationally feasible number, and let the asymptotic behavior of LLN do the rest.In the case that you have some kind of constraint (a cap on the total number of terminal nodes, a cap on the model estimation time, a limit to the size of the model on disk), this amounts to choosing the largest $T$ that satisfies your constraint.This is purely speculation, but I think that the belief that tuning the number of trees in a random forest persists is related to two facts:Boosting algorithms like AdaBoost and XGBoost do require users to tune the number of trees in the ensemble and some software users are not sophisticated enough to distinguish between boosting and bagging. (For a discussion of the distinction between boosting and bagging, see Is random forest a boosting algorithm?)Standard random forest implementations, like R's randomForest (which is, basically, the R interface to Breiman's FORTRAN code), only report error rate (or, equivalently, accuracy) as a function of trees. This is deceptive, because the accuracy is not a monotonic function of the number of trees, whereas continuous proper scoring rules such as Brier score and logloss are monotonic functions."
R libraries for deep learning,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I was wondering if there's any good R libraries out there for deep learning neural networks?  I know there's the nnet, neuralnet, and RSNNS, but none of these seem to implement deep learning methods.
I'm especially interested in unsupervised followed by supervised learning, and using dropout to prevent co-adaptation.
/edit: After a few years, I've found the h20 deep learning package very well-designed and easy to install.  I also love the mxnet package, which is (a little) harder to install but supports things like covnets, runs on GPUs, and is really fast.
","['r', 'neural-networks', 'deep-learning', 'restricted-boltzmann-machine', 'deep-belief-networks']","OpenSource h2o.deepLearning() is package for deeplearning in R from h2o.ai
here's a write up http://www.r-bloggers.com/things-to-try-after-user-part-1-deep-learning-with-h2o/And code: https://gist.github.com/woobe/3e728e02f6cc03ab86d8#file-link_data-r"
Intuition behind why Stein's paradox only applies in dimensions $\ge 3$,"
Stein's Example shows that the maximum likelihood estimate of $n$ normally distributed variables with means $\mu_1,\ldots,\mu_n$ and variances $1$ is inadmissible (under a square loss function) iff $n\ge 3$. For a neat proof, see the first chapter of Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction by Bradley Effron.
This was highly surprising to me at first, but there is some intuition behind why one might expect the standard estimate to be inadmissible (most notably, if $x \sim \mathcal N(\mu,1)$, then $\mathbb{E}\|x\|^2\approx \|\mu\|^2+n$, as outlined in Stein's original paper, linked to below).
My question is rather: What property of $n$-dimensional space (for $n\ge 3$) does $\mathbb{R}^2$ lack which facilitates Stein's example? Possible answers could be about the curvature of the $n$-sphere, or something completely different.
In other words, why is the MLE admissible in $\mathbb{R}^2$?

Edit 1: In response to @mpiktas concern about 1.31 following from 1.30:
$$E_\mu\left(\|z-\hat{\mu}\|^2\right)=E_\mu\left(S\left(\frac{N-2}{S}\right)^2\right)=E_\mu\left(\frac{(N-2)^2}{S}\right).$$
$$\hat{\mu_i} = \left(1-\frac{N-2}{S}\right)z_i$$ so $$E_\mu\left(\frac{\partial\hat{\mu_i}}{\partial z_i} \right)=E_\mu\left( 1-\frac{N-2}{S}+2\frac{z_i^2}{S^2}\right).$$ Therefore we have:
$$2\sum_{i=1}^N E_\mu\left(\frac{\partial\hat{\mu_i}}{\partial z_i} \right)=2N-2E_\mu\left(\frac{N(N-2)}{S}\right)+4E_\mu\left(\frac{(N-2)}{S}\right)\\=2N-E_\mu\frac{2(N-2)^2}{S}.$$
Edit 2: In this paper, Stein proves that the MLE is admissible for $N=2$.
","['maximum-likelihood', 'unbiased-estimator', 'intuition', 'steins-phenomenon']","The dichotomy between the cases $d < 3$ and $d \geq 3$ for the admissibility of the MLE of the mean of a $d$-dimensional multivariate normal random variable is certainly shocking.There is another very famous example in probability and statistics in which there is a dichotomy between the $d < 3$ and $d \geq 3$ cases. This is the recurrence of a simple random walk on the lattice $\mathbb{Z}^d$. That is, the $d$-dimensional simple random walk is recurrent in 1 or 2 dimensions, but is transient in $d \geq 3$ dimensions. The continuous-time analogue (in the form of Brownian motion) also holds.It turns out that the two are closely related.Larry Brown proved that the two questions are essentially equivalent. That is, the best invariant estimator $\hat{\mu} \equiv \hat{\mu}(X) = X$ of a $d$-dimensional multivariate normal mean vector is admissible if and only if the $d$-dimensional Brownian motion is recurrent.In fact, his results go much further. For any sensible (i.e., generalized Bayes) estimator $\tilde{\mu} \equiv \tilde{\mu}(X)$ with bounded (generalized) $L_2$ risk, there is an explicit(!) corresponding $d$-dimensional diffusion such that the estimator $\tilde{\mu}$ is admissible if and only if its corresponding diffusion is recurrent.The local mean of this diffusion is essentially the discrepancy between the two estimators, i.e., $\tilde{\mu} - \hat{\mu}$ and the covariance of the diffusion is $2 I$. From this, it is easy to see that for the case of the MLE $\tilde{\mu} = \hat{\mu} = X$, we recover (rescaled) Brownian motion.So, in some sense, we can view the question of admissibility through the lens of stochastic processes and use well-studied properties of diffusions to arrive at the desired conclusions.References"
Prediction in Cox regression,"
I am doing a multivariate Cox regression, I have my significant independent variables and beta values. The model fits to my data very well. 
Now, I would like to use my model and predict the survival of a new observation.
I am unclear how to do this with a Cox model. In a linear or logistic regression, it would be easy, just put the values of new observation in the regression and multiply them with betas and so I have the prediction of my outcome.
How can I determine my baseline hazard? I need it in addition to computing the prediction.
How is this done in a Cox model?
","['regression', 'survival', 'prediction', 'cox-model']","Following Cox model, the estimated hazard for individual $i$ with covariate vector $x_i$ has the form
$$\hat{h}_i(t) = \hat{h}_0(t) \exp(x_i' \hat{\beta}),$$
where $\hat{\beta}$ is found by maximising the partial likelihood, while $\hat{h}_0$ follows from the Nelson-Aalen estimator,
$$
\hat{h}_0(t_i) = \frac{d_i}{\sum_{j:t_j \geq t_i} \exp(x_j' \hat{\beta})}
$$ 
with $t_1$, $t_2, \dotsc$ the distinct event times and $d_i$ the number of deaths at $t_i$ 
(see, e.g., Section 3.6).Similarly, 
$$\hat{S}_i(t) = \hat{S}_0(t)^{\exp(x_i' \hat{\beta})}$$
with $\hat{S}_0(t) = \exp(- \hat{\Lambda}_0(t))$ and
$$\hat{\Lambda}_0(t) = \sum_{j:t_j \leq t} \hat{h}_0(t_j).$$EDIT:
This might also be of interest :-)"
Why do we use ReLU in neural networks and how do we use it?,"
Why do we use rectified linear units (ReLU) with neural networks? How does that improve neural network?
Why do we say that ReLU is an activation function? Isn't softmax activation function for neural networks? I am guessing that we use both, ReLU and softmax, like this:  
neuron 1 with softmax output ----> ReLU on the output of neuron 1, which is
input of neuron 2 ---> neuron 2 with softmax output --> ...
so that the input of neuron 2 is basically ReLU(softmax(x1)). Is this correct?
",['neural-networks'],"The ReLU function is $f(x)=\max(0, x).$ Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like.One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of $x$). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations.Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is  has its own problem, called ""dead neurons,"" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU, or PReLU, etc.) can ameliorate this.$\frac{d}{dx}\text{ReLU}(x)=1\forall x > 0$ . By contrast, the gradient of a sigmoid unit is at most $0.25$; on the other hand, $\tanh$ fares better for inputs in a region near 0 since $0.25 < \frac{d}{dx}\tanh(x) \le 1 \forall x \in [-1.31, 1.31]$ (approximately)."
Regularization methods for logistic regression,"
Regularization using methods such as Ridge, Lasso, ElasticNet is quite common for linear regression. I wanted to know the following:
Are these methods applicable for logistic regression? If so, are there any differences in the way they need to be used for logistic regression? If these methods are not applicable, how does one regularize a logistic regression?   
","['regression', 'logistic', 'regularization']",
When will L1 regularization work better than L2 and vice versa?,"
Note: I know that L1 has feature selection property. I am trying to understand  which one to choose when feature selection is completely irrelevant.

How to decide which regularization (L1 or L2) to use?
What are the pros & cons of each of L1 / L2 regularization?
Is it recommended to 1st do feature selection using L1 & then apply L2 on these selected variables?

","['regression', 'lasso', 'regularization', 'ridge-regression']","How to decide which regularization (L1 or L2) to use?What is your goal? Both can improve model generalization by penalizing coefficients, since features with opposite relationship to the outcome can ""offset"" each other (a large positive value is counterbalanced by a large negative value). This can arise when there are collinear features. Small changes in the data can result in dramatically different parameter estimates (high variance estimates). Penalization can restrain both coefficients to be smaller. (Hastie et al, Elements of Statistical Learning, 2nd edition, p. 63)What are the pros & cons of each of L1 / L2 regularization?L1 regularization can address the multicollinearity problem by constraining the coefficient norm and pinning some coefficient values to 0. Computationally, Lasso regression (regression with an L1 penalty) is a quadratic program which requires some special tools to solve. When you have more features than observations $N$, lasso will keep at most $N$ non-zero coefficients. Depending on context, that might not be what you want.L1 regularization is sometimes used as a feature selection method. Suppose you have some kind of hard cap on the number of features you can use (because data collection for all features is expensive, or you have tight engineering constraints on how many values you can store, etc.). You can try to tune the L1 penalty to hit your desired number of non-zero features.L2 regularization can address the multicollinearity problem by constraining the coefficient norm and keeping all the variables. It's unlikely to estimate a coefficient to be exactly 0. This isn't necessarily a drawback, unless a sparse coefficient vector is important for some reason.In the regression setting, it's the ""classic"" solution to the problem of estimating a regression with more features than observations. L2 regularization can estimate a coefficient for each feature even if there are more features than observations (indeed, this was the original motivation for ""ridge regression"").As an alternative, elastic net allows L1 and L2 regularization as special cases. A typical use-case in for a data scientist in industry is that you just want to pick the best model, but don't necessarily care if it's penalized using L1, L2 or both. Elastic net is nice in situations like these.Is it recommended to 1st do feature selection using L1 & then apply L2 on these selected variables?I'm not familiar with a publication proposing an L1-then-L2 pipeline, but this is probably just ignorance on my part. There doesn't seem to be anything wrong with it. I'd conduct a literature review.A few examples of similar ""phased"" pipelines exist. One is the ""relaxed lasso"", which applies lasso regression twice, once to down-select from a large group to a small group of features, and second to estimate coefficients for use in a model. This uses cross-validation at each step to choose the magnitude of the penalty. The reasoning is that in the first step, you cross-validate and will likely choose a large penalty to screen out irrelevant predictors; in the second step, you cross-validate and will likely pick a smaller penalty (and hence larger coefficients). This is mentioned briefly in Elements of Statistical Learning with a citation to Nicolai Meinshausen (""Relaxed Lasso."" Computational Statistics & Data Analysis Volume 52, Issue 1, 15 September 2007, pp 374-393).User @amoeba also suggests an L1-then-OLS pipeline; this might be nice because it only has 1 hyperparameter for the magnitude of the L1 penalty, so less fiddling would be required.One problem that can arise with any ""phased"" analysis pipeline (that is, a pipeline which does some steps, and then some other steps separately) is that there's no ""visibility"" between those different phases (algorithms applied at each step). This means that one process inherits any data snooping that happened at the previous steps. This effect is not negligible; poorly-conceived modeling can result in garbage models.One way to hedge against data-snooping side-effects is to cross-validate all of your choices. However, the increased computational costs can be prohibitive, depending on the scale of the data and the complexity of each step."
Clustering a long list of strings (words) into similarity groups,"
I have the following problem at hand: I have a very long list of words, possibly names, surnames, etc. I need to cluster this word list, such that similar words, for example words with similar edit (Levenshtein) distance appears in the same cluster. For example ""algorithm"" and ""alogrithm"" should have high chances to appear in the same cluster.
I am well aware of the classical unsupervised clustering methods like k-means clustering, EM clustering in the Pattern Recognition literature. The problem here is that these methods work on points which reside in a vector space. I have words of strings at my hand here. It seems that, the question of how to represent strings in a numerical vector space and to calculate ""means"" of string clusters is not sufficiently answered, according to my survey efforts until now. A naive approach to attack this problem would be to combine k-Means clustering with Levenshtein distance, but the question still remains ""How to represent ""means"" of strings?"". There is a weight called as TF-IDF weight, but it seems that it is mostly related to the area of ""text document"" clustering, not for the clustering of single words. It seems that there are some special string clustering algorithms existing, like the one at  http://pike.psu.edu/cleandb06/papers/CameraReady_120.pdf 
My search in this area is going on still, but I wanted to get ideas from here as well. What would you do recommend in this case, is anyone aware of any methods for this kind of problem?
","['clustering', 'k-means', 'pattern-recognition']","Seconding @micans recommendation for Affinity Propagation.From the paper: L Frey, Brendan J., and Delbert Dueck. ""Clustering by passing messages between data points."" science 315.5814 (2007): 972-976..It's super easy to use via many packages.
It works on anything you can define the pairwise similarity on. Which you can get by multiplying the Levenshtein distance by -1.I threw together a quick example using the first paragraph of your question as input. In Python 3:Output was (exemplars in italics to the left of the cluster they are exemplar of):Running it on a list of 50 random first names:Looks pretty great to me (that was fun)."
How to calculate relative error when the true value is zero?,"
How do I calculate relative error when the true value is zero?
Say I have $x_{true} = 0$ and $x_{test}$. If I define relative error as:
$$\text{relative error} = \frac{x_{true}-x_{test}}{x_{true}}$$
Then the relative error is always undefined. If instead I use the definition:
$$\text{relative error} = \frac{x_{true}-x_{test}}{x_{test}}$$
Then the relative error is always 100%. Both methods seem useless. Is there another alternative?
","['error', 'measurement-error']","There are many alternatives, depending on the purpose.A common one is the ""Relative Percent Difference,"" or RPD, used in laboratory quality control procedures.  Although you can find many seemingly different formulas, they all come down to comparing the difference of two values to their average magnitude:$$d_1(x,y) = \frac{x - y}{(|x| + |y|)/2} = 2\frac{x - y}{|x| + |y|}.$$This is a signed expression, positive when $x$ exceeds $y$ and negative when $y$ exceeds $x$.  Its value always lies between $-2$ and $2$.  By using absolute values in the denominator it handles negative numbers in a reasonable way.  Most of the references I can find, such as the New Jersey DEP Site Remediation Program Data Quality Assessment and Data Usability Evaluation Technical Guidance, use the absolute value of $d_1$ because they are interested only in the magnitude of the relative error.A Wikipedia article on Relative Change and Difference observes that $$d_\infty(x,y) = \frac{|x - y|}{\max(|x|, |y|)}$$is frequently used as a relative tolerance test in floating point numerical algorithms.  The same article also points out that formulas like $d_1$ and $d_\infty$ may be generalized to$$d_f(x,y) = \frac{x - y}{f(x,y)}$$where the function $f$ depends directly on the magnitudes of $x$ and $y$ (usually assuming $x$ and $y$ are positive).  As examples it offers their max, min, and arithmetic mean (with and without taking the absolute values of $x$ and $y$ themselves), but one could contemplate other sorts of averages such as the geometric mean $\sqrt{|x y|}$, the harmonic mean $2/(1/|x| + 1/|y|)$ and $L^p$ means $((|x|^p + |y|^p)/2)^{1/p}$.  ($d_1$ corresponds to $p=1$ and $d_\infty$ corresponds to the limit as $p\to \infty$.)  One might choose an $f$ based on the expected statistical behavior of $x$ and $y$.  For instance, with approximately lognormal distributions the geometric mean would be an attractive choice for $f$ because it is a meaningful average in that circumstance.Most of these formulas run into difficulties when the denominator equals zero.  In many applications that either is not possible or it is harmless to set the difference to zero when $x=y=0$.Note that all these definitions share a fundamental invariance property: whatever the relative difference function $d$ may be, it does not change when the arguments are uniformly rescaled by $\lambda \gt 0$:$$d(x,y) = d(\lambda x, \lambda y).$$It is this property that allows us to consider $d$ to be a relative difference.  Thus, in particular, a non-invariant function like$$d(x,y) =?\ \frac{|x-y|}{1 + |y|}$$simply does not qualify.  Whatever virtues it might have, it does not express a relative difference.The story does not end here.  We might even find it fruitful to push the implications of invariance a little further. The set of all ordered pairs of real numbers $(x,y)\ne (0,0)$ where $(x,y)$ is considered to be the same as $(\lambda x, \lambda y)$ is the Real Projective Line $\mathbb{RP}^1$. In both a topological sense and an algebraic sense, $\mathbb{RP}^1$ is a circle.  Any $(x,y)\ne (0,0)$ determines a unique line through the origin $(0,0)$.  When $x\ne 0$ its slope is $y/x$; otherwise we may consider its slope to be ""infinite"" (and either negative or positive).  A neighborhood of this vertical line consists of lines with extremely large positive or extremely large negative slopes.  We may parameterize all such lines in terms of their angle $\theta = \arctan(y/x)$, with $-\pi/2 \lt \theta \le \pi/2$.  Associated with every such $\theta$ is a point on the circle,$$(\xi, \eta) = (\cos(2\theta), \sin(2\theta)) = \left(\frac{x^2-y^2}{x^2+y^2}, \frac{2xy}{x^2+y^2}\right).$$Any distance defined on the circle can therefore be used to define a relative difference.As an example of where this can lead, consider the usual (Euclidean) distance on the circle, whereby the distance between two points is the size of the angle between them.  The relative difference is least when $x=y$, corresponding to $2\theta = \pi/2$ (or $2\theta = -3\pi/2$ when $x$ and $y$ have opposite signs).  From this point of view a natural relative difference for positive numbers $x$ and $y$ would be the distance to this angle:$$d_S(x,y) = \left|2\arctan\left(\frac{y}{x}\right) - \pi/2\right|.$$To first order, this is the relative distance $|x-y|/|y|$--but it works even when $y=0$.  Moreover, it doesn't blow up, but instead (as a signed distance) is limited between $-\pi/2$ and $\pi/2$, as this graph indicates:This hints at how flexible the choices are when selecting a way to measure relative differences."
Probability distribution for different probabilities,"
If I wanted to get the probability of 9 successes in 16 trials with each trial having a probability of 0.6 I could use a binomial distribution. What could I use if each of the 16 trials has a different probability of success?
","['distributions', 'probability', 'binomial-distribution']","This is the sum of 16 (presumably independent) Binomial trials.  The assumption of independence allows us to multiply probabilities.  Whence, after two trials with probabilities $p_1$ and $p_2$ of success the chance of success on both trials is $p_1 p_2$, the chance of no successes is $(1-p_1)(1-p_2)$, and the chance of one success is $p_1(1-p_2) + (1-p_1)p_2$.  That last expression owes its validity to the fact that the two ways of getting exactly one success are mutually exclusive: at most one of them can actually happen.  That means their probabilities add.By means of these two rules--independent probabilities multiply and mutually exclusive ones add--you can work out the answers for, say, 16 trials with probabilities $p_1, \ldots, p_{16}$.  To do so, you need to account for all the ways of obtaining each given number of successes (such as 9).  There are $\binom{16}{9} = 11440$ ways to achieve 9 successes.  One of them, for example, occurs when trials 1, 2, 4, 5, 6, 11, 12, 14, and 15 are successes and the others are failures.  The successes had probabilities $p_1, p_2, p_4, p_5, p_6, p_{11}, p_{12}, p_{14},$ and $p_{15}$ and the failures had probabilities $1-p_3, 1-p_7, \ldots, 1-p_{13}, 1-p_{16}$.  Multiplying these 16 numbers gives the chance of this particular sequence of outcomes.  Summing this number along with the 11,439 remaining such numbers gives the answer.Of course you would use a computer.With many more than 16 trials, there is a need to approximate the distribution.  Provided none of the probabilities $p_i$ and $1-p_i$ get too small, a Normal approximation tends to work well.  With this method you note that the expectation of the sum of $n$ trials is $\mu = p_1 + p_2 + \cdots + p_n$ and (because the trials are independent) the variance is $\sigma^2 = p_1(1-p_1) + p_2(1-p_2) + \cdots + p_n(1-p_n)$.  You then pretend the distribution of sums is Normal with mean $\mu$ and standard deviation $\sigma$.  The answers tend to be good for computing probabilities corresponding to a proportion of successes that differs from $\mu$ by no more than a few multiples of $\sigma$.  As $n$ grows large this approximation gets ever more accurate and works for even larger multiples of $\sigma$ away from $\mu$."
Normalization vs. scaling,"
What is the difference between data 'Normalization' and data 'Scaling'? Till now I thought both terms refers to same process but now I realize there is something more that I don't know/understand. Also if there is a difference between Normalization and Scaling, when should we use Normalization but not Scaling and vice versa? 
Please elaborate with some example.
","['data-transformation', 'scales', 'normality-assumption', 'normalization']","I am not aware of an ""official"" definition and even if there it is, you shouldn't trust it as you will see it being used inconsistently in practice.This being said, scaling in statistics usually means a linear transformation of the form
$f(x) = ax+b$. Normalizing can either mean applying a transformation so that you transformed data is roughly normally distributed, but it can also simply mean putting different variables on a common scale. Standardizing, which means subtracting the mean and dividing by the standard deviation, is an example of the later usage. As you may see it's also an example of scaling. An example for the first would be taking the log for lognormal distributed data. But what you should take away is that when you read it you should look for a more precise description of what the author did. Sometimes you can get it from the context."
How does linear regression use the normal distribution?,"
In linear regression, each predicted value is assumed to have been picked from a normal distribution of possible values. See below.
But why is each predicted value assumed to have come from a normal distribution? How does linear regression use this assumption? What if possible values are not normally distributed?

","['regression', 'probability', 'distributions', 'normal-distribution', 'modeling']",
"Why are regression problems called ""regression"" problems?","
I was just wondering why regression problems are called ""regression"" problems. What is the story behind the name? 

One definition for regression:
  ""Relapse to a less perfect or
  developed state.""

","['regression', 'terminology', 'history', 'etymology']","The term ""regression"" was used by Francis Galton in his 1886 paper ""Regression towards mediocrity in hereditary stature"". To my knowledge he only used the term in the context of regression toward the mean. The term was then adopted by others to get more or less the meaning it has today as a general statistical method. "
How to visualize a fitted multiple regression model?,"
I am currently writing a paper with several multiple regression analyses. While visualizing univariate linear regression is easy via scatter plots, I was wondering whether there is any good way to visualize multiple linear regressions? 
I am currently just plotting scatter plots like dependent variable vs. 1st independent variable, then vs. 2nd independent variable, etc. I would really appreciate any suggestions.
","['regression', 'multiple-regression', 'data-visualization', 'reporting']","There is nothing wrong with your current strategy.  If you have a multiple regression model with only two explanatory variables then you could try to make a 3D-ish plot that displays the predicted regression plane, but most software don't make this easy to do.  Another possibility is to use a coplot (see also: coplot in R or this pdf), which can represent three or even four variables, but many people don't know how to read them.  Essentially however, if you don't have any interactions, then the predicted marginal relationship between $x_j$ and $y$ will be the same as predicted conditional relationship (plus or minus some vertical shift) at any specific level of your other $x$ variables.  Thus, you can simply set all other $x$ variables at their means and find the predicted line $\hat y = \hat\beta_0 + \cdots + \hat\beta_j x_j + \cdots + \hat\beta_p \bar x_p$ and plot that line on a scatterplot of $(x_j, y)$ pairs.  Moreover, you will end up with $p$ such plots, although you might not include some of them if you think they are not important. (For example, it is common to have a multiple regression model with a single variable of interest and some control variables, and only present the first such plot).On the other hand, if you do have interactions, then you should figure out which of the interacting variables you are most interested in and plot the predicted relationship between that variable and the response variable, but with several lines on the same plot.  The other interacting variable is set to different levels for each of those lines.  Typical values would be the mean and $\pm$ 1 SD of the interacting variable.  To make this clearer, imagine you have only two variables, $x_1$ and $x_2$, and you have an interaction between them, and that $x_1$ is the focus of your study, then you might make a single plot with these three lines:
\begin{align}
\hat y &= \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 (\bar x_2 - s_{x_2})  + \hat\beta_3 x_1(\bar x_2 - s_{x_2}) \\
\hat y &= \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 \bar x_2 \quad\quad\quad\  + \hat\beta_3 x_1\bar x_2 \\
\hat y &= \hat\beta_0 + \hat\beta_1 x_1 + \hat\beta_2 (\bar x_2 + s_{x_2}) + \hat\beta_3 x_1(\bar x_2 + s_{x_2}) 
\end{align}An example plot that's similar (albeit with a binary moderator) can be seen in my answer to Plot regression with interaction in R."
Multivariate linear regression vs neural network?,"
It seems that it is possible to get similar results to a neural network with a multivariate linear regression in some cases, and multivariate linear regression is super fast and easy.
Under what circumstances can neural networks give better results than multivariate linear regression?
","['regression', 'multiple-regression', 'neural-networks']","Neural networks can in principle model nonlinearities automatically (see the universal approximation theorem), which you would need to explicitly model using transformations (splines etc.) in linear regression.The caveat: the temptation to overfit can be (even) stronger in neural networks than in regression, since adding hidden layers or neurons looks harmless. So be extra careful to look at out-of-sample prediction performance."
Modern successor to Exploratory Data Analysis by Tukey?,"
I've been reading Tukey's book ""Exploratory Data Analysis"".  Being written in 1977, the book emphasizes paper/pencil methods.  Is there a more 'modern' successor which takes into account that we can now instantaneosly plot large data sets?  
","['data-visualization', 'references', 'descriptive-statistics', 'exploratory-data-analysis']",
How do I find peaks in a dataset?,"
If I have a data set that produces a graph such as the following, how would I algorithmically determine the x-values of the peaks shown (in this case three of them):

","['data-visualization', 'mode']",
Pandas / Statsmodel / Scikit-learn,"

Are Pandas, Statsmodels and Scikit-learn different implementations of machine learning/statistical operations, or are these complementary to one another?
Which of these has the most comprehensive functionality?
Which one is actively developed and/or supported?
I have to implement logistic regression. Any suggestions as to which of these I should use?

","['machine-learning', 'python', 'scikit-learn', 'statsmodels', 'pandas']","Scikit-learn (sklearn) is the best choice for machine learning, out of the three listed. While Pandas and Statsmodels do contain some predictive learning algorithms, they are hidden/not production-ready yet. Often, as authors will work on different projects, the libraries are complimentary. For example, recently Pandas' Dataframes were integrated into Statsmodels. A relationship between sklearn and Pandas is not present (yet).Define functionality. They all run. If you mean what is the most useful, then it depends on your application. I would definitely give Pandas a +1 here, as it has added a great new data structure to Python (dataframes). Pandas also probably has the best API.They are all actively supported, though I would say Pandas has the best code base. Sklearn and Pandas are more active than Statsmodels.The clear choice is Sklearn. It is easy and clear how to perform it."
Book for reading before Elements of Statistical Learning?,"
Based on this post, I want to digest Elements of Statistical Learning. Fortunately it is available for free and I started reading it.
I don't have enough knowledge to understand it. Can you recommend a book that is a better introduction to the topics in the book? Hopefully something that will give me the knowledge needed to understand it?
Related:
Is a strong background in maths a total requisite for ML?
","['machine-learning', 'references']","I bought, but have not yet read, S. Marsland, Machine Learning: An Algorithmic Perspective, Chapman & Hall, 2009. However, the reviews are favorable and state that it is more suitable for beginners than other ML books that have more depth. Flipping through the pages, it looks to me to be good for me because I have little math background."
Why does correlation matrix need to be positive semi-definite and what does it mean to be or not to be positive semi-definite?,"
I have been researching the meaning of positive semi-definite property of correlation or covariance matrices. 
I am looking for any information on

Definition of positive semi-definiteness;
Its important properties, practical implications;
The consequence of having negative determinant, impact on multivariate analysis or simulation results etc.

","['covariance-matrix', 'eigenvalues', 'determinant', 'correlation-matrix']",
"Replicating Stata's ""robust"" option in R","
I have been trying to replicate the results of the Stata option robust in R. I have used the rlm command form the MASS package and also the command lmrob from the package ""robustbase"". In both cases the results are quite different from the ""robust"" option in Stata. Can anybody please suggest something in this context?
Here are the results I obtained when I ran the robust option in Stata:
. reg yb7 buildsqb7 no_bed no_bath rain_harv swim_pl pr_terrace, robust

Linear regression                                      Number of obs =    4451
                                                       F(  6,  4444) =  101.12
                                                       Prob > F      =  0.0000
                                                       R-squared     =  0.3682
                                                       Root MSE      =   .5721

------------------------------------------------------------------------------
             |               Robust
         yb7 |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   buildsqb7 |   .0046285   .0026486     1.75   0.081    -.0005639     .009821
      no_bed |   .3633841   .0684804     5.31   0.000     .2291284    .4976398
     no_bath |   .0832654   .0706737     1.18   0.239    -.0552904    .2218211
   rain_harv |   .3337906   .0395113     8.45   0.000     .2563289    .4112524
     swim_pl |   .1627587   .0601765     2.70   0.007     .0447829    .2807346
  pr_terrace |   .0032754   .0178881     0.18   0.855    -.0317941    .0383449
       _cons |   13.68136   .0827174   165.40   0.000     13.51919    13.84353

And this is what I obtained in R with the lmrob option:
> modelb7<-lmrob(yb7~Buildsqb7+No_Bed+Rain_Harv+Swim_Pl+Gym+Pr_Terrace, data<-bang7)
> summary(modelb7)

Call:
lmrob(formula = yb7 ~ Buildsqb7 + No_Bed + Rain_Harv + Swim_Pl + Gym + Pr_Terrace, 
    data = data <- bang7)
 \--> method = ""MM""
Residuals:
      Min        1Q    Median        3Q       Max 
-51.03802  -0.12240   0.02088   0.18199   8.96699 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 12.648261   0.055078 229.641   <2e-16 ***
Buildsqb7    0.060857   0.002050  29.693   <2e-16 ***
No_Bed       0.005629   0.019797   0.284   0.7762    
Rain_Harv    0.230816   0.018290  12.620   <2e-16 ***
Swim_Pl      0.065199   0.028121   2.319   0.0205 *  
Gym          0.023024   0.014655   1.571   0.1162    
Pr_Terrace   0.015045   0.013951   1.078   0.2809    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Robust residual standard error: 0.1678 
Multiple R-squared:  0.8062,    Adjusted R-squared:  0.8059 

","['r', 'stata', 'robust', 'robust-standard-error']",
Is sampling relevant in the time of 'big data'?,"
Or more so ""will it be""? Big Data makes statistics and relevant knowledge all the more important but seems to underplay Sampling Theory. 
I've seen this hype around 'Big Data' and can't help wonder that ""why"" would I want to analyze everything? Wasn't there a reason for ""Sampling Theory"" to be designed/implemented/invented/discovered? I don't get the point of analyzing the entire 'population' of the dataset. Just because you can do it doesn't mean you should (Stupidity is a privilege but you shouldn't abuse it :)
So my question is this: Is it statistically relevant to analyze the entire data set? The best you could do would be to minimize error if you did sampling. But is the cost of minimizing that error really worth it? Is the ""value of information"" really worth the effort, time cost etc. that goes in analyzing big data over massively parallel computers?
Even if one analyzes the entire population, the outcome would still be at best a guess with a higher probability of being right. Probably a bit higher than sampling (or would it be a lot more?) Would the insight gained from analyzing the population vs analyzing the sample differ widely? 
Or should we accept it as ""times have changed""? Sampling as an activity could become less important given enough computational power :)
Note: I'm not trying to start a debate but looking for an answer to understand the why big data does what it does (i.e. analyze everything) and disregard the theory of sampling (or it doesn't?)
","['sampling', 'data-mining', 'large-data']","In a word, yes. I believe there are still clear situations where sampling is appropriate, within and without the ""big data"" world, but the nature of big data will certainly change our approach to sampling, and we will use more datasets that are nearly complete representations of the underlying population.On sampling: Depending on the circumstances it will almost always be clear if sampling is an appropriate thing to do. Sampling is not an inherently beneficial activity; it is just what we do because we need to make tradeoffs on the cost of implementing data collection. We are trying to characterize populations and need to select the appropriate method for gathering and analyzing data about the population. Sampling makes sense when the marginal cost of a method of data collection or data processing is high. Trying to reach 100% of the population is not a good use of resources in that case, because you are often better off addressing things like non-response bias than making tiny improvements in the random sampling error.How is big data different? ""Big data"" addresses many of the same questions we've had for ages, but what's ""new"" is that the data collection happens off an existing, computer-mediated process, so the marginal cost of collecting data is essentially zero. This dramatically reduces our need for sampling.When will we still use sampling? If your ""big data"" population is the right population for the problem, then you will only employ sampling in a few cases: the need to run separate experimental groups, or if the sheer volume of data is too large to capture and process (many of us can handle millions of rows of data with ease nowadays, so the boundary here is getting further and further out). If it seems like I'm dismissing your question, it's probably because I've rarely encountered situations where the volume of the data was a concern in either the collection or processing stages, although I know many haveThe situation that seems hard to me is when your ""big data"" population doesn't perfectly represent your target population, so the tradeoffs are more apples to oranges. Say you are a regional transportation planner, and Google has offered to give you access to its Android GPS navigation logs to help you. While the dataset would no doubt be interesting to use, the population would probably be systematically biased against the low-income, the public-transportation users, and the elderly. In such a situation, traditional travel diaries sent to a random household sample, although costlier and smaller in number, could still be the superior method of data collection. But, this is not simply a question of ""sampling vs. big data"", it's a question of which population combined with the relevant data collection and analysis methods you can apply to that population will best meet your needs."
Choosing the right linkage method for hierarchical clustering,"
I am performing hierarchical clustering on data I've gathered and processed from the reddit data dump on Google BigQuery. 
My process is the following:

Get the latest 1000 posts in /r/politics
Gather all the comments
Process the data and compute an n x m data matrix (n:users/samples, m:posts/features)
Calculate the distance matrix for hierarchical clustering
Choose a linkage method and perform the hierarchical clustering
Plot the data as a dendrogram

My question is, how do I determine what the best linkage method is? I'm currently using Ward but how do I know if I should be using single, complete, average, etc?
I'm very new to this stuff but I can't find a clear answer online as I'm not sure there is one. So what might be a good idea for my application? Note the data is relatively sparse in the sense that the n x m matrix has a lot of zeroes (most people don't comment on more than a few posts).
","['clustering', 'distance', 'unsupervised-learning', 'hierarchical-clustering']",
"Why do my p-values differ between logistic regression output, chi-squared test, and the confidence interval for the OR?","
I have built a logistic regression where the outcome variable is being cured after receiving treatment (Cure vs. No Cure). All patients in this study received treatment. I am interested in seeing if having diabetes is associated with this outcome. 
In R my logistic regression output looks as follows: 
Call:
glm(formula = Cure ~ Diabetes, family = binomial(link = ""logit""), data = All_patients)
...
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   1.2735     0.1306   9.749   <2e-16 ***
Diabetes     -0.5597     0.2813  -1.990   0.0466 *  
...
    Null deviance: 456.55  on 415  degrees of freedom
Residual deviance: 452.75  on 414  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 456.75

However, the confidence interval for the odds ratio includes 1:
                   OR     2.5 %   97.5 %
(Intercept) 3.5733333 2.7822031 4.646366
Diabetes    0.5713619 0.3316513 1.003167

When I do a chi-squared test on these data I get the following:
data:  check
X-squared = 3.4397, df = 1, p-value = 0.06365

If you'd like to calculate it on your own the distribution of diabetes in the cured and uncured groups are as follows:
Diabetic cure rate:      49 /  73 (67%)
Non-diabetic cure rate: 268 / 343 (78%)

My question is: Why don't the p-values and the confidence interval including 1 agree? 
","['r', 'hypothesis-testing', 'logistic', 'generalized-linear-model', 'odds-ratio']","With generalized linear models, there are three different types of statistical tests that can be run.  These are: Wald tests, likelihood ratio tests, and score tests.  The excellent UCLA statistics help site has a discussion of them here.  The following figure (copied from their site) helps to illustrate them:  The tests that come with summary.glm() are Wald tests.  You don't say how you got your confidence intervals, but I assume you used confint(), which in turn calls profile().  More specifically, those confidence intervals are calculated by profiling the likelihood (which is a better approach than multiplying the SE by $1.96$).  That is, they are analogous to the likelihood ratio test, not the Wald test.  The $\chi^2$-test, in turn, is a score test.  As your $N$ becomes indefinitely large, the three different $p$'s should converge on the same value, but they can differ slightly when you don't have infinite data.  It is worth noting that the (Wald) $p$-value in your initial output is just barely significant and there is little real difference between just over and just under $\alpha=.05$ (quote).  That line isn't 'magic'.  Given that the two more reliable tests are just over $.05$, I would say that your data are not quite 'significant' by conventional criteria.  Below I profile the coefficients on the scale of the linear predictor and run the likelihood ratio test explicitly (via anova.glm()).  I get the same results as you:  As @JWilliman pointed out in a comment (now deleted), in R, you can also get a score-based p-value using anova.glm(model, test=""Rao"").  In the example below, note that the p-value isn't quite the same as in the chi-squared test above, because by default, R's chisq.test() applies a continuity correction.  If we change that setting, the p-values match:  "
CNN architectures for regression?,"
I've been working on a regression problem where the input is an image, and the label is a continuous value between 80 and 350. The images are of some chemicals after a reaction takes place. The color that turns out indicates the concentration of another chemical that's left over, and that's what the model is to output - the concentration of that chemical. The images can be rotated, flipped, mirrored, and the expected output should still be the same. This sort of analysis is done in real labs (very specialized machines output the concentration of the chemicals using color analysis just like I'm training this model to do).
So far I've only experimented with models roughly based off VGG (multiple sequences of conv-conv-conv-pool blocks). Before experimenting with more recent architectures (Inception, ResNets, etc.), I thought I'd research if there are other architectures more commonly used for regression using images. 
The dataset looks like this:

The dataset contains about 5,000 250x250 samples, which I've resized to 64x64 so training is easier. Once I find a promising architecture, I'll experiment with larger resolution images. 
So far, my best models have a mean squared error on both training and validation sets of about 0.3, which is far from acceptable in my use case.
My best model so far looks like this:
// pseudo code
x = conv2d(x, filters=32, kernel=[3,3])->batch_norm()->relu()
x = conv2d(x, filters=32, kernel=[3,3])->batch_norm()->relu()
x = conv2d(x, filters=32, kernel=[3,3])->batch_norm()->relu()
x = maxpool(x, size=[2,2], stride=[2,2])

x = conv2d(x, filters=64, kernel=[3,3])->batch_norm()->relu()
x = conv2d(x, filters=64, kernel=[3,3])->batch_norm()->relu()
x = conv2d(x, filters=64, kernel=[3,3])->batch_norm()->relu()
x = maxpool(x, size=[2,2], stride=[2,2])

x = conv2d(x, filters=128, kernel=[3,3])->batch_norm()->relu()
x = conv2d(x, filters=128, kernel=[3,3])->batch_norm()->relu()
x = conv2d(x, filters=128, kernel=[3,3])->batch_norm()->relu()
x = maxpool(x, size=[2,2], stride=[2,2])

x = dropout()->conv2d(x, filters=128, kernel=[1, 1])->batch_norm()->relu()
x = dropout()->conv2d(x, filters=32, kernel=[1, 1])->batch_norm()->relu()

y = dense(x, units=1)

// loss = mean_squared_error(y, labels)

Question
What is an appropriate architecture for regression output from an image input?
Edit
I've rephrased my explanation and removed mentions of accuracy.
Edit 2
I've restructured my question so hopefully it's clear what I'm after
","['regression', 'machine-learning', 'neural-networks', 'conv-neural-network', 'tensorflow']","First of all a general suggestion: do a literature search before you start making experiments on a topic you're not familiar with. You'll save yourself a lot of time. In this case, looking at existing papers you may have noticed that Regression with CNNs is not a trivial problem. Looking again at the first paper, you'll see that they have a problem where they can basically generate infinite data. Their objective is to predict the rotation angle needed to rectify 2D pictures. This means that I can basically take my training set and augment it by rotating each image by arbitrary angles, and I'll obtain a valid, bigger training set. Thus the problem seems relatively simple, as far as Deep Learning problems go. By the way, note the other data augmentation tricks they use: We use translations (up to 5% of the image width), brightness
  adjustment in the range [−0.2, 0.2], gamma adjustment with γ ∈ [−0.5,
  0.1] and Gaussian pixel noise with a standard deviation in the range [0, 
  0.02].I don't know your problem well enough to say if it makes sense to consider 
variations in position, brightness and gamma noise for your pictures, carefully 
shot in a lab. But you can always try, and remove it if it doesn't improve your test set loss. Actually, you should really use a validation set or $k-$fold cross-validation for these kinds of experiments, and don't look at the test set until you have defined your setup, if you want the test set loss to be representative of the generalization error.Anyway, even in their ideal conditions, the naive approach didn't work that well (section 4.2). They stripped out the output layer (the softmax layer) and substituted it with a layer with two units which would predict the sine $y$ and cosine $x$ of the rotation angle. The actual angle would then be computed as $\alpha=\text{atan2}(y,x)$. The neural network was also pretrained on ImageNet (this is called transfer learning). Of course the training on ImageNet had been for a different task (classification), but still training the neural network from scratch must have given such horrible results that they decided not to publish them. So you had all ingredients to make a good omelette: potentially infinite training data, a pretrained network and an apparently simple regression problem (predict two numbers between -1 and 1). Yet, the best they could get with this approach was a 21° error. It's not clear if this is an RMSE error, a MAD error or what, but still it's not great: since the maximum error you can make is 180°, the average error is $>11\%$ of the maximum possible error. They did slightly better by using two networks in series: the first one would perform  classification (predict whether the angle would be in the $[-180°,-90°],[-90°,0°],[0°,90°]$ or $[90°,180°]$ class), then the image, rotated by the amount predicted by the first network, would be fed to another neural network (for regression, this time), which would predict the final additional rotation in the $[-45°,45°]$ range.On a much simpler (rotated MNIST) problem, you can get something better, but still you don't go below an RMSE error which is $2.6\%$ of the maximum possible error. So, what can we learn from this? First of all, that 5000 images is a small data set for your task. The first paper used a network which was pretrained on images similar to that for which they wanted to learn the regression task: not only you need to learn a different task from that for which the architecture was designed (classification), but your training set doesn't look anything at all like the training sets on which these networks are usually trained (CIFAR-10/100 or ImageNet). So you probably won't get any benefits from transfer learning. The MATLAB example had 5000 images, but they were black and white and semantically all very similar (well, this could be your case too). Then, how realistic is doing better than 0.3? We must first of all understand what do you mean by 0.3 average loss. Do you mean that the RMSE error is 0.3,$$\frac{1}{N}\sum_{i=1}^N (h(\mathbf{x}_i)-y_i)^2$$where $N$ is the size of your training set (thus, $N< 5000$), $h(\mathbf{x}_i)$ is the output of your CNN for image $\mathbf{x}_i$ and $y_i$ is the corresponding concentration of the chemical? Since $y_i\in[80,350]$, then assuming that you clip the predictions of your CNN between 80 and 350 (or you just use a logit to make them fit in that interval), you're getting less than $0.12\%$ error. Seriously, what do you expect? it doesn't seem to me a big error at all. Also, just try to compute the number of parameters in your network: I'm in a hurry and I may be making silly mistakes, so by all means double check my computations with some summary function from whatever framework you may be using. However, roughly I would say you have$$9\times(3\times 32 + 2\times 32\times 32 + 32\times64+2\times64\times64+ 64\times128+2\times128\times128) +128\times128+128\times32+32 \times32\times32=533344$$(note I skipped the parameters of the batch norm layers, but they're just 4 parameters for layer so they don't make a difference). You have half a million parameters and 5000 examples...what would you expect? Sure, the number of parameters is not a good indicator for the capacity of a neural network (it's a non-identifiable model), but still...I don't think you can do much better than this, but you can try a few things:"
Best method for short time-series,"
I have a question related to modeling short time-series. It is not a question if to model them, but how. What method would you recommend for modeling (very) short time-series (say of length $T \leq 20$)? By ""best"" I mean here the most robust one, that is the least prone to errors due the fact of limited numbers of observations. With short series single observations could influence the forecast, so the method should provide a cautious estimate of errors and possible variability connected to the forecast. I am generally interested in univariate time-series but it would be also interesting to know about other methods.
","['time-series', 'forecasting', 'small-sample']","It is very common for extremely simple forecasting methods like ""forecast the historical average"" to outperform more complex methods. This is even more likely for short time series. Yes, in principle you can fit an ARIMA or even more complex model to 20 or fewer observations, but you will be rather likely to overfit and get very bad forecasts.So: start with a simple benchmark, e.g.,Assess these on out-of-sample data. Compare any more complex model to these benchmarks. You may be surprised at seeing how hard it is to outperform these simple methods. In addition, compare the robustness of different methods to these simple ones, e.g., by not only assessing average accuracy out-of-sample, but also the error variance, using your favorite error measure.Yes, as Rob Hyndman writes in his post that Aleksandr links to, out-of-sample testing is a problem in itself for short series - but there really is no good alternative. (Don't use in-sample fit, which is no guide to forecasting accuracy.) The AIC won't help you with the median and the random walk. However, you could use time-series cross-validation, which AIC approximates, anyway."
Why does frequentist hypothesis testing become biased towards rejecting the null hypothesis with sufficiently large samples?,"
I was just reading this article on the Bayes factor for a completely unrelated problem when I stumbled upon this passage

Hypothesis testing with Bayes factors is more robust than frequentist hypothesis testing, since the Bayesian form avoids model selection bias, evaluates evidence in favor the null hypothesis, includes model uncertainty, and allows non-nested models to be compared (though of course the model must have the same dependent variable). Also, frequentist significance tests become biased in favor of rejecting the null hypothesis with sufficiently large sample size. [emphasis added]

I've seen this claim before in Karl Friston's 2012 paper in NeuroImage, where he calls it the fallacy of classical inference.
I've had a bit of trouble finding a truly pedagogical account of why this should be true.  Specifically, I'm wondering:

why this occurs
how to guard against it
failing that, how to detect it

","['hypothesis-testing', 'frequentist']","Answer to question 1: This occurs because the $p$-value becomes arbitrarily small as the sample size increases in frequentist tests for difference (i.e. tests with a null hypothesis of no difference/some form of equality) when a true difference exactly equal to zero, as opposed to arbitraily close to zero, is not realistic (see Nick Stauner's comment to the OP). The $p$-value becomes arbitrarily small because the error of frequentist test statistics generally decreases with sample size, with the upshot that all differences are significant to an arbitrary level with a large enough sample size. Cosma Shalizi has written eruditely about this.Answer to question 2: Within a frequentist hypothesis testing framework, one can guard against this by not making inference solely about detecting difference. For example, one can combine inferences about difference and equivalence so that one is not favoring (or conflating!) the burden of proof on evidence of effect versus evidence of absence of effect. Evidence of absence of an effect comes from, for example:What these approaches all share is an a priori decision about what effect size constitutes a relevant difference and a null hypothesis framed in terms of a difference at least as large as what is considered relevant.Combined inference from tests for difference and tests for equivalence thus protects against the bias you describe when sample sizes are large in this way (two-by-two table showing the four possibilities resulting from combined tests for difference—positivist null hypothesis, $\text{H}_{0}^{+}$—and equivalence—negativist null hypothesis, $\text{H}_{0}^{-}$):Notice the upper left quadrant: an overpowered test is one where yes you reject the null hypothesis of no difference, but you also reject the null hypothesis of relevant difference, so yes there's a difference, but you have a priori decided you do not care about it because it is too small.Answer to question 3: See answer to 2."
"If the t-test and the ANOVA for two groups are equivalent, why aren't their assumptions equivalent?","
I'm sure I've got this completely wrapped round my head, but I just can't figure it out.
The t-test compares two normal distributions using the Z distribution. That's why there's an assumption of normality in the DATA.
ANOVA is equivalent to linear regression with dummy variables, and uses sums of squares, just like OLS. That's why there's an assumption of normality of RESIDUALS.
It's taken me several years, but I think I've finally grasped those basic facts. So why is it that the t-test is equivalent to ANOVA with two groups? How can they be equivalent if they don't even assume the same things about the data?
","['distributions', 'regression', 'normality-assumption', 't-test', 'anova']","The t-test with two groups assumes that each group is normally distributed with the same variance (although the means may differ under the alternative hypothesis). That is equivalent to a regression with a dummy variable as the regression allows the mean of each group to differ but not the variance. Hence the residuals (equal to the data with the group means subtracted) have the same distribution --- that is, they are normally distributed with zero mean.A t-test with unequal variances is not equivalent to a one-way ANOVA."
When is a biased estimator preferable to unbiased one?,"
It's obvious many times why one prefers an unbiased estimator. But, are there any circumstances under which we might actually prefer a biased estimator over an unbiased one?
","['mathematical-statistics', 'bias', 'estimators', 'unbiased-estimator', 'bias-variance-tradeoff']","Yes. Often it is the case that we are interested in minimizing the mean squared error, which can be decomposed into variance + bias squared. This is an extremely fundamental idea in machine learning, and statistics in general. Frequently we see that a small increase in bias can come with a large enough reduction in variance that the overall MSE decreases.A standard example is ridge regression. We have $\hat \beta_R = (X^T X + \lambda I)^{-1}X^T Y$ which is biased; but if $X$ is ill conditioned then $Var(\hat \beta) \propto (X^T X)^{-1}$ may be monstrous whereas $Var(\hat \beta_R)$ can be much more modest.Another example is the kNN classifier. Think about $k = 1$: we assign a new point to its nearest neighbor. If we have a ton of data and only a few variables we can probably recover the true decision boundary and our classifier is unbiased; but for any realistic case, it is likely that $k = 1$ will be far too flexible (i.e. have too much variance) and so the small bias is not worth it (i.e. the MSE is larger than more biased but less variable classifiers).Finally, here's a picture. Suppose that these are the sampling distributions of two estimators and we are trying to estimate 0. The flatter one is unbiased, but also much more variable. Overall I think I'd prefer to use the biased one, because even though on average we won't be correct, for any single instance of that estimator we'll be closer.$$ \ $$
UpdateI mention the numerical issues that happen when $X$ is ill conditioned and how ridge regression helps. Here's an example.I'm making a matrix $X$ which is $4 \times 3$ and the third column is nearly all 0, meaning that it is almost not full rank, which means that $X^T X$ is really close to being singular.Update 2As promised, here's a more thorough example.First, remember the point of all of this: we want a good estimator. There are many ways to define 'good'. Suppose that we've got $X_1, ..., X_n \sim \ iid \ \mathcal N(\mu, \sigma^2)$ and we want to estimate $\mu$.Let's say that we decide that a 'good' estimator is one that is unbiased. This isn't optimal because, while it is true that the estimator $T_1(X_1, ..., X_n) = X_1$ is unbiased for $\mu$, we have $n$ data points so it seems silly to ignore almost all of them. To make that idea more formal, we think that we ought to be able to get an estimator that varies less from $\mu$ for a given sample than $T_1$. This means that we want an estimator with a smaller variance.So maybe now we say that we still want only unbiased estimators, but among all unbiased estimators we'll choose the one with the smallest variance. This leads us to the concept of the uniformly minimum variance unbiased estimator (UMVUE), an object of much study in classical statistics. IF we only want unbiased estimators, then choosing the one with the smallest variance is a good idea. In our example, consider $T_1$ vs. $T_2(X_1, ..., X_n) = \frac{X_1 + X_2}{2}$ and $T_n(X_1, ..., X_n) = \frac{X_1 + ... + X_n}{n}$. Again, all three are unbiased but they have different variances: $Var(T_1) = \sigma^2$, $Var(T_2) = \frac{\sigma^2}{2}$, and $Var(T_n) = \frac{\sigma^2}{n}$. For $n > 2$ $T_n$ has the smallest variance of these, and it's unbiased, so this is our chosen estimator.But often unbiasedness is a strange thing to be so fixated on (see @Cagdas Ozgenc's comment, for example). I think this is partly because we generally don't care so much about having a good estimate in the average case, but rather we want a good estimate in our particular case. We can quantify this concept with the mean squared error (MSE) which is like the average squared distance between our estimator and the thing we're estimating. If $T$ is an estimator of $\theta$, then $MSE(T) = E((T - \theta)^2)$. As I've mentioned earlier, it turns out that $MSE(T) = Var(T) + Bias(T)^2$, where bias is defined to be $Bias(T) = E(T) - \theta$. Thus we may decide that rather than UMVUEs we want an estimator that minimizes MSE.Suppose that $T$ is unbiased. Then $MSE(T) = Var(T) = Bias(T)^2 = Var(T)$, so if we are only considering unbiased estimators then minimizing MSE is the same as choosing the UMVUE. But, as I showed above, there are cases where we can get an even smaller MSE by considering non-zero biases.In summary, we want to minimize $Var(T) + Bias(T)^2$. We could require $Bias(T) = 0$ and then pick the best $T$ among those that do that, or we could allow both to vary. Allowing both to vary will likely give us a better MSE, since it includes the unbiased cases. This idea is the variance-bias trade-off that I mentioned earlier in the answer.Now here are some pictures of this trade-off. We're trying to estimate $\theta$ and we've got five models, $T_1$ through $T_5$. $T_1$ is unbiased and the bias gets more and more severe until $T_5$. $T_1$ has the largest variance and the variance gets smaller and smaller until $T_5$. We can visualize the MSE as the square of the distance of the distribution's center from $\theta$ plus the square of the distance to the first inflection point (that's a way to see the SD for normal densities, which these are). We can see that for $T_1$ (the black curve) the variance is so large that being unbiased doesn't help: there's still a massive MSE. Conversely, for $T_5$ the variance is way smaller but now the bias is big enough that the estimator is suffering. But somewhere in the middle there is a happy medium, and that's $T_3$. It has reduced the variability by a lot (compared with $T_1$) but has only incurred a small amount of bias, and thus it has the smallest MSE.You asked for examples of estimators that have this shape: one example is ridge regression, where you can think of each estimator as $T_\lambda(X, Y) = (X^T X + \lambda I)^{-1} X^T Y$. You could (perhaps using cross-validation) make a plot of MSE as a function of $\lambda$ and then choose the best $T_\lambda$."
Is there any difference between lm and glm for the gaussian family of glm?,"
Specifically, I want to know if there is a difference between lm(y ~ x1 + x2) and glm(y ~ x1 + x2, family=gaussian). I think that this particular case of glm is equal to lm. Am I wrong? 
","['r', 'normal-distribution', 'generalized-linear-model', 'lm']","While for the specific form of model mentioned in the body of the question (i.e. lm(y ~ x1 + x2) vs glm(y ~ x1 + x2, family=gaussian)), regression and GLMs are the same model, the title question asks something slightly more general:Is there any difference between lm and glm for the gaussian family of glm?To which the answer is ""Yes!"".The reason that they can be different is because you can also specify a link function in the GLM. This allows you to fit particular forms of nonlinear relationship between $y$ (or rather its conditional mean) and the $x$-variables; while you can do this in nls as well, there's no need for starting values, sometimes the convergence is better (also the syntax is a bit easier).Compare, for example, these models (you have R so I assume you can run these yourself):Note that the first pair are the same model ($y_i \sim N(\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i},\sigma^2)\,$), and the second pair are the same model ($y_i \sim N(\exp(\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}),\sigma^2)\,$ and the fits are essentially the same within each pair.So - in relation to the title question - you can fit a substantially wider variety of Gaussian models with a GLM than with regression."
Online vs offline learning?,"
What is the difference between offline and online learning?  Is it just a matter of learning over the entire dataset (offline) vs. learning incrementally (one instance at a time)?  What are examples of algorithms used in both?
","['machine-learning', 'online-algorithms']","Online learning means that you are doing it as the data comes in. Offline means that you have a static dataset.So, for online learning, you (typically) have more data, but you have time constraints. Another wrinkle that can affect online learning is that your concepts might change through time.Let's say you want to build a classifier to recognize spam. You can acquire a large corpus of e-mail, label it, and train a classifier on it. This would be offline learning. Or, you can take all the e-mail coming into your system, and continuously update your classifier (labels may be a bit tricky). This would be online learning."
How large should the batch size be for stochastic gradient descent?,"
I understand that stochastic gradient descent may be used to optimize a neural network using backpropagation by updating each iteration with a different sample of the training dataset. How large should the batch size be?
","['machine-learning', 'neural-networks', 'gradient-descent', 'backpropagation']","The ""sample size"" you're talking about is referred to as batch size, $B$. The batch size parameter is just one of the hyper-parameters you'll be tuning when you train a neural network with mini-batch Stochastic Gradient Descent (SGD) and is data dependent. The most basic method of hyper-parameter search is to do a grid search over the learning rate and batch size to find a pair which makes the network converge.To understand what the batch size should be, it's important to see the relationship between batch gradient descent, online SGD, and mini-batch SGD. Here's the general formula for the weight update step in mini-batch SGD, which is a generalization of all three types. [2]$$
\theta_{t+1} \leftarrow \theta_{t} - \epsilon(t) \frac{1}{B} \sum\limits_{b=0}^{B - 1} \dfrac{\partial \mathcal{L}(\theta, \textbf{m}_b)}{\partial \theta}
$$Note that with 1, the loss function is no longer a random variable and is not a stochastic approximation.SGD converges faster than normal ""batch"" gradient descent because it updates the weights after looking at a randomly selected subset of the training set. Let $x$ be our training set and let $m \subset x$. The batch size $B$ is just the cardinality of $m$: $B = |m|$.Batch gradient descent updates the weights $\theta$ using the gradients of the entire dataset $x$; whereas SGD updates the weights using an average of the gradients for a mini-batch $m$. (Using the average as opposed to a sum prevents the algorithm from taking steps that are too large if the dataset is very large. Otherwise, you would need to adjust your learning rate based on the size of the dataset.) The expected value of this stochastic approximation of the gradient used in SGD is equal to the deterministic gradient used in batch gradient descent. $\mathbb{E}[\nabla \mathcal{L}_{SGD}(\theta, \textbf{m})] = \nabla \mathcal{L}(\theta, \textbf{x})$.Each time we take a sample and update our weights it is called a mini-batch. Each time we run through the entire dataset, it's called an epoch.Let's say that we have some data vector $\textbf{x} : \mathbb{R}^D$, an initial weight vector that parameterizes our neural network, $\theta_0 : \mathbb{R}^{S}$, and a loss function $\mathcal{L}(\theta, \textbf{x}) : \mathbb{R}^{S} \rightarrow \mathbb{R}^{D} \rightarrow \mathbb{R}^S$ that we are trying to minimize. If we have $T$ training examples and a batch size of $B$, then we can split those training examples into C mini-batches:$$
C = \lceil T / B \rceil
$$For simplicity we can assume that T is evenly divisible by B. Although, when this is not the case, as it often is not, proper weight should be assigned to each mini-batch as a function of its size.An iterative algorithm for SGD with $M$ epochs is given below:\begin{align*}
t &\leftarrow 0 \\
\textrm{while } t &< M \\
\theta_{t+1} &\leftarrow \theta_{t} - \epsilon(t) \frac{1}{B} \sum\limits_{b=0}^{B - 1} \dfrac{\partial \mathcal{L}(\theta, \textbf{m}_b)}{\partial \theta}  \\
t &\leftarrow t + 1
\end{align*}Note: in real life we're reading these training example data from memory and, due to cache pre-fetching and other memory tricks done by your computer, your algorithm will run faster if the memory accesses are coalesced, i.e. when you read the memory in order and don't jump around randomly. So, most SGD implementations shuffle the dataset and then load the examples into memory in the order that they'll be read.The major parameters for the vanilla (no momentum) SGD described above are:I like to think of epsilon as a function from the epoch count to a learning rate. This function is called the learning rate schedule.$$
    \epsilon(t) : \mathbb{N} \rightarrow \mathbb{R}
$$If you want to have the learning rate fixed, just define epsilon as a constant function.Batch size determines how many examples you look at before making a weight update. The lower it is, the noisier the training signal is going to be, the higher it is, the longer it will take to compute the gradient for each step.Citations & Further Reading:"
How do we decide when a small sample is statistically significant or not?,"
Sorry if the title isn't clear, I'm not a statistician, and am not sure how to phrase this.
I was looking at the global coronavirus statistics on worldometers, and sorted the table by cases per million population to get an idea of how different countries had fared.
Note My use of Vatican City below is purely because that was the first tiny country I saw in the list. As @smci pointed out, Vatican City has a few issues that may make it different from others. Therefore, please keep ""tiny country"" in mind when reading on, as my question applies to any tiny country.
The table shows the Vatican City as being the 7th worst country, with 33,666 cases per million. Now given that the total population of Vatican City is only 802, I'm not sure how much we can make of this figure. When the country's population is small, even a minor fluctuation in the number of cases would make a significant difference to the cases per million. As an artificial example, consider a fictional country with only 1 inhabitant. If that person got the virus, then the cases per million would be 1,000,000, which is way higher than anything in that table.
Obviously the Vatican City is an extreme example, but there are other countries with smallish populations that appear quite high on the list, and I guess the same question would apply to them.
So is there a way of deciding what is ""too small"" a population to be significant?
If this question isn't clear enough, please explain why rather than downvoting, as I would like to understand it, and am happy to clarify if I didn't explain it well enough.
","['statistical-significance', 'population']","I will describe how a statistician interprets count data.  With a tiny bit of practice you can do it, too.When cases arise randomly and independently, the times of their occurrences are reasonably accurately modeled with a Poisson process.  This implies that the number of cases appearing in any predetermined interval has a Poisson distribution.  The only thing we need to remember about that is that its variance equals its expectation.  In less technical jargon, this means that the amount by which the value is likely to differ from the average (its standard error) is proportional to the square root of the average.  (See Why is the square root transformation recommended for count data? for an explanation and discussion of the square root and some related transformations of count data.)In practice, we estimate the average by using the observed value.  Thus,The standard error of a count of independent events with equal expected rates of occurrence is the square root of the count.(Various modifications of this rule exist for really small counts, especially counts of zero, but that shouldn't be an issue in the present application.)In the case of Vatican City, a rate of 33,666 cases per million corresponds to$$\frac{33666}{10^6} \times 802 = 27$$cases.  The square root of $27$ is $5$ (we usually don't need to worry about additional significant figures for this kind of analysis, which is usually done mentally and approximately).Equivalently, this standard error is $\sqrt{27}$ cases out of $802$ people, equivalent to $6500$ per million.  We are therefore justified in statingThe Vatican City case rate is $33666\pm 6500$ per million.This shows how silly it is to quote five significant figures for the rate.  It is better to acknowledge the large standard error by limiting the sig figs, as inThe observed Vatican City case rate is $34000 \pm 6500$ per million.(Do not make the mistake of just taking the square root of the rate!  In this example, the square root of 33,666 is only 183, which is far too small.  For estimating standard errors square roots apply to counts, not rates.)A good rule of thumb is to use one additional significant digit when reporting the standard error, as I did here (the case rate was rounded to the nearest thousand and its SE was rounded to the nearest 100).Cases are not independent: people catch them from other people and because human beings do not dart about the world like atoms in a vial of hot gas, cases occur in clusters.  This violates the independence assumption.  What really happens, then, is that the effective count should be somewhere between the number of cases and the number of distinct clusters.  We cannot know the latter: but surely it is smaller (perhaps far smaller) than the number of cases.  Thus,The square root rule gives a lower bound on the standard error when the events are (positively) correlated.You can sometimes estimate how to adjust the standard error.  For instance, if you guess that cases occur in clusters of ten or so, then you should multiply the standard error by the square root of ten.  Generally,The standard error of a count of positively correlated events is, very roughly, the square root of the count times the square root of a typical cluster size.This approximation arises by assuming all cases in a cluster are perfectly correlated and otherwise the cases in any two different clusters are independent.If we suspect the Vatican City cases are clustered, then in the most extreme case it is a single cluster: the count is $1,$ its square root is $1,$ and the standard error therefore is one whole cluster: namely, about $27$ people.  If you want to be cautious about not exaggerating the reliability of the numbers, then, you might think of this Vatican City rate as being somewhere between just above zero and likely less than 70,000 per million ($1\pm 1$ clusters of $27$ of out a population of $802$)."
Intuitive explanations of differences between Gradient Boosting Trees (GBM) & Adaboost,"
I'm trying to understand the differences between GBM & Adaboost. 
These are what I've understood so far:

There are both boosting algorithms, which learns from previous model's errors and finally make a weighted sum of the models. 
GBM and Adaboost are pretty similar except for their loss functions.

But still it is difficult for me to grab an idea of differences between them. 
Can someone give me intuitive explanations?
","['boosting', 'adaboost']","I found this introduction which provides some intuitive explanations:By means of an exponential loss function, AdaBoost gives more weights to those samples fitted worse in previous steps. Today, AdaBoost is regarded as a special case of Gradient Boosting in terms of loss function. Historically it preceded Gradient Boosting to which it was later generalized, as shown in the history provided in the introduction:"
Using deep learning for time series prediction,"
I'm new in area of deep learning and for me first step was to read interesting articles from deeplearning.net site. In papers about deep learning, Hinton and others mostly talk about applying it to image problems. Can someone try to answer me can it be applied to problem of predicting time series values (financial, internet traffic,...) and what are important things that I should focus if it is possible?
","['time-series', 'machine-learning', 'prediction', 'deep-learning', 'deep-belief-networks']",
Understanding LSTM units vs. cells,"
I have been studying LSTMs for a while. I understand at a high level how everything works. However, going to implement them using Tensorflow I've noticed that BasicLSTMCell requires a number of units (i.e. num_units) parameter. 
From this very thorough explanation of LSTMs, I've gathered that a single LSTM unit is one of the following

which is actually a GRU unit.
I assume that parameter num_units of the BasicLSTMCell is referring to how 
many of these we want to hook up to each other in a layer.
That leaves the question - what is a ""cell"" in this context? Is a ""cell"" equivalent to a layer in a normal feed-forward neural network? 
","['neural-networks', 'terminology', 'lstm', 'recurrent-neural-network', 'tensorflow']","The terminology is unfortunately inconsistent. num_units in TensorFlow is the number of hidden states, i.e. the dimension of $h_t$ in the equations you gave.Also, from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.nn.rnn_cell.RNNCell.md :The definition of cell in this package differs from the definition used in the literature. In the literature, cell refers to an object with a single scalar output. The definition in this package refers to a horizontal array of such units.""LSTM layer"" is probably more explicit, example:"
Best PCA algorithm for huge number of features (>10K)?,"
I previously asked this on StackOverflow, but it seems like it might be more appropriate here, given that it didn't get any answers on SO.  It's kind of at the intersection between statistics and programming.
I need to write some code to do PCA (Principal Component Analysis). I've browsed through the well-known algorithms and implemented this one, which as far as I can tell is equivalent to the NIPALS algorithm.  It works well for finding the first 2-3 principal components, but then seems to become very slow to converge (on the order of hundreds to thousands of iterations). Here are the details of what I need:

The algorithm must be efficient when dealing with huge numbers of features (order 10,000 to 20,000) and sample sizes on the order of a few hundred.
It must be reasonably implementable without a decent linear algebra/matrix library, as the target language is D, which doesn't have one yet, and even if it did, I would prefer not to add it as a dependency to the project in question.

As a side note, on the same dataset R seems to find all principal components very fast, but it uses singular value decomposition, which is not something I want to code myself.
","['pca', 'algorithms', 'model-evaluation', 'high-dimensional']",
Should you ever standardise binary variables?,"
I have a data set with a set of features. Some of them are binary $(1=$ active or fired, $0=$ inactive or dormant), and the rest are real valued, e.g. $4564.342$.
I want to feed this data to a machine learning algorithm, so I $z$-score all the real-valued features. I get them between ranges $3$ and $-2$ approximately. Now the binary values are also $z$-scored, therefore the zeros become $-0.222$ and the ones become $0.5555$.
Does standardising binary variables like this make sense?
","['machine-learning', 'normalization', 'binary-data']",
Empirical justification for the one standard error rule when using cross-validation,"
Are there any empirical studies justifying the use of the one standard error rule in favour of parsimony? Obviously it depends on the data-generation process of the data, but anything which analyses a large corpus of datasets would be a very interesting read.

The ""one standard error rule"" is applied when selecting models through cross-validation (or more generally through any randomization-based procedure).
Assume we consider models $M_\tau$ indexed by a complexity parameter $\tau\in\mathbb{R}$, such that $M_\tau$ is ""more complex"" than $M_{\tau'}$ exactly when $\tau>\tau'$. Assume further that we assess the quality of a model $M$ by some randomization process, e.g., cross-validation. Let $q(M)$ denote the ""average"" quality of $M$, e.g., the mean out-of-bag prediction error across many cross-validation runs. We wish to minimize this quantity.
However, since our quality measure comes from some randomization procedure, it comes with variability. Let $s(M)$ denote the standard error of the quality of $M$ across the randomization runs, e.g., the standard deviation of the out-of-bag prediction error of $M$ over cross-validation runs.
Then we choose the model $M_\tau$, where $\tau$ is the smallest $\tau$ such that
$$q(M_\tau)\leq q(M_{\tau'})+s(M_{\tau'}),$$
where $\tau'$ indexes the (on average) best model, $q(M_{\tau'})=\min_\tau q(M_\tau)$.
That is, we choose the simplest model (the smallest $\tau$) which is no more than one standard error worse than the best model $M_{\tau'}$ in the randomization procedure.
I have found this ""one standard error rule"" referred to in the following places, but never with any explicit justification:

Page 80 in Classification and Regression Trees by Breiman, Friedman, Stone & Olshen (1984)
Page 415 in Estimating the Number of Clusters in a Data Set via the Gap Statistic by Tibshirani, Walther & Hastie (JRSS B, 2001) (referencing Breiman et al.)
Pages 61 and 244 in Elements of Statistical Learning by Hastie, Tibshirani & Friedman (2009)
Page 13 in Statistical Learning with Sparsity by Hastie, Tibshirani & Wainwright (2015)

","['cross-validation', 'model-selection', 'regularization']",
"How can I calculate $\int^{\infty}_{-\infty}\Phi\left(\frac{w-a}{b}\right)\phi(w)\,\mathrm dw$","
Suppose $\phi(\cdot)$ and $\Phi(\cdot)$ are density function and distribution function of the standard normal distribution.
How can one calculate the integral:
$$\int^{\infty}_{-\infty}\Phi\left(\frac{w-a}{b}\right)\phi(w)\,\mathrm dw$$
","['mathematical-statistics', 'normal-distribution', 'integral']","A more conventional notation is$$y(\mu, \sigma) = \int\Phi\left(\frac{x-\mu}{\sigma}\right)\phi(x) dx = \Phi\left(\frac{-\mu}{\sqrt{1+\sigma^2}}\right).$$This can be found by differentiating the integral with respect to $\mu$ and $\sigma$, producing elementary integrals which can be expressed in closed form:$$\frac{\partial y}{\partial \mu}(\mu, \sigma) = -\frac{1}{\sqrt{2 \pi } \sqrt{\sigma ^2+1}}e^{-\frac{1}{2}\frac{\mu ^2}{\sigma ^2+1}},$$$$\frac{\partial y}{\partial \sigma}(\mu, \sigma) = \frac{\mu\sigma  }{\sqrt{2 \pi } \left(\sigma ^2+1\right)^{3/2}}e^{-\frac{1}{2}\frac{\mu ^2}{\sigma ^2+1}}.$$This system can be integrated, beginning with the initial condition $y(0,1)$ = $\int\Phi(x)\phi(x)dx$ = $1/2$, to obtain the given solution (which is easily checked by differentiation)."
Debunking wrong CLT statement,"
The central limit theorem (CLT) gives some nice properties about converging to a normal distribution. Prior to studying statistics formally, I was under the extremely wrong impression that the CLT said that data approached normality.
I now find myself arguing with collaborators about this. I say that $68\%$ of the data need not be within one standard deviation of the mean when we have non-normal distributions. They agree but then say that, by the CLT, since we have many observations (probably 50,000), our data are very close to normal, so we can use the empirical rule and say that $68\%$ of the data are within one standard deviation of the mean. This is, of course, false. The population does not care how many observations are drawn from it; the population is the population, whether we sample from it or not!
What would be a good way to explain why the central limit theorem is not about the empirical distribution converging?
","['normal-distribution', 'convergence', 'intuition', 'central-limit-theorem', 'communication']","This is quite a ubiquitous misunderstanding of the central limit theorem, which I have also encountered in my statistical teaching.  Over the years I have encountered this problem so often that I have developed a Socratic method to deal with it.  I identify a student that has accepted this idea and then engage the student to tease out what this would logically imply.  It is fairly simple to get to the reductio ad absurdum of the false version of the theorem, which is that every sequence of IID random variables has a normal distribution.  A typical conversation would go something like this.Teacher: I noticed in this assignment question that you said that because $n$ is large, the data are approximately normally distributed.  Can you take me through your reasoning for that bit?Student: Is that wrong?Teacher: I don't know.  Let's have a look at it.Student: Well, I used that theorem you talked about in class; that main one you mentioned a bunch of times.  I forget the name.Teacher: The central limit theorem?Student: Yeah, the central limit theorem.Teacher: Great, and when does that theorem apply?Student: I think if the variables are IID.Teacher: And have finite variance.Student: Yeah, and finite variance.Teacher: Okay, so the random variables have some fixed distribution with finite variance, is that right?Student: Yeah.Teacher: And the distribution isn't changing or anything?Student: No, they're IID with a fixed distribution.Teacher: Okay great, so let me see if I can state the theorem.  The central limit theorem says that if you have an IID sequence of random variables with finite variance, and you take a sample of $n$ of them, then as that sample size $n$ gets large the distribution of the random variables converges to a normal distribution.  Is that right?Student: Yeah, I think so.Teacher: Okay great, so let's think about what that would mean.  Suppose I have a sequence like that.  If I take say, a thousand sample values, what is the distribution of those random variables?Student: It's approximately a normal distribution.Teacher: How close?Student: Pretty close I think.Teacher: Okay, what if I take a billion sample values.  How close now?Student: Really close I'd say.Teacher: And if we have a sequence of these things, then in theory we can take $n$ as high as we want can't we?  So we can make the distribution as close to a normal distribution as we want.Student: Yeah.Teacher: So let's say we take $n$ big enough that we're happy to say that the random variables basically have a normal distribution.  And that's a fixed distribution right?Student: Yeah.Teacher: And they're IID right?  These random variables are IID?Student: Yeah, they're IID.Teacher: Okay, so they all have the same distribution.Student: Yeah.Teacher: Okay, so that means the first value in the sequence, it also has a normal distribution.  Is that right?Student: Yeah.  I mean, it's an approximation, but yeah, if $n$ is really large then it effectively has a normal distribution.Teacher: Okay great.  And so does the second value in the sequence, and so on, right?Student: Yeah.Teacher: Okay, so really, as soon as we started sampling, we were already getting values that are essentially normal distributed.  We didn't really need to wait until $n$ gets large before that started happening.Student: Hmmm.  I'm not sure.  That sounds wrong.  The theorem says you need a large $n$, so I guess I think you can't apply it if you only sampled a small number of values.Teacher: Okay, so let's say we are sampling a billion values.  Then we have large $n$.  And we've established that this means that the first few random variables in the sequence are normally distributed, to a very close approximation.  If that's true, can't we just stop sampling early?  Say we were going to sample a billion values, but then we stop sampling after the first value.  Was that random variable still normally distributed?Student: I think maybe it isn't.Teacher: Okay, so at some point its distribution changes?Student: I'm not sure.  I'm a bit confused about it now.Teacher: Hmmm, well it seems we have something strange going on here.  Why don't you have another read of the material on the central limit theorem and see if you can figure out how to resolve that contradiction.  Let's talk more about it then.That is one possible approach, which seeks to reduce the false theorem down to the reductio which says that every IID sequence (with finite variance) must be composed of normal random variables.  Either the student will get to this conclusion, and realise something is wrong, or they will defend against this conclusion by saying that the distribution changes as $n$ gets large (or they may handwave a bit, and you might have to lawyer them to a conclusion).  Either way, this usually provokes some further thinking that can lead them to re-read the theorem.  Here is another approach:Teacher: Let's look at this another way.  Suppose we have an IID sequence of random variables from some other distribution; one that is not a normal distribution.  Is that possible?  For example, could we have a sequence of random variables representing outcome of coin flip, from the Bernoulli distribution?Student: Yeah, we can have that.Teacher: Okay, great.  And these are all IID values, so again, they all have the same distribution.  So every random variable in that sequence is going to have a distribution that is not a normal distribution, right?Student: Yeah.Teacher: In fact, in this case, every value in the sequence will be the outcome of a coin flip, which we set as zero or one.  Is that right?Student: Yeah, as long as we label them that way.Teacher: Okay, great.  So if all the values in the sequence are zeroes or ones,
no matter how many of them we sample, we are always going to get a histogram showing values at zero and one, right?Student: Yeah.Teacher: Okay.  And do you think if we sample more and more values, we will get closer and closer to the true distribution?  Like, if it is a fair coin, does the histogram eventually converge to where the relative frequency bars are the same height?Student: I guess so.  I think it does.Teacher: I think you're right.  In fact, we call that result the ""law of large numbers"".  Anyway, it seems like we have a bit of a problem here doesn't it.  If we sample a large number of the values then the central limit theorem says we converge to a normal distribution, but it sounds like the ""law of large numbers"" says we actually converge to the true distribution, which isn't a normal distribution.  In fact, it's a distribution that is just probabilities on the zero value and the one value, which looks nothing like the normal distribution.  So which is it?Student: I think when $n$ is large it looks like a normal distribution.Teacher: So describe it to me.  Let's say we have flipped the coin a billion times.  Describe the distribution of the outcomes and explain why that looks like a normal distribution.Student: I'm not really sure how to do that.Teacher: Okay.  Well, do you agree that if we have a billion coin flips, all those outcomes are zeroes and ones?Student: Yeah.Teacher: Okay, so describe what its histogram looks like.Student: It's just two bars on those values.Teacher: Okay, so not ""bell curve"" shaped?Student: Yeah, I guess not.Teacher: Hmmm, so perhaps the central limit theorem doesn't say what we thought.  Why don't you read the material on the central limit theorem again and see if you can figure out what it says.  Let's talk more about it then."
Why is polynomial regression considered a special case of multiple linear regression?,"
If polynomial regression models nonlinear relationships, how can it be considered a special case of multiple linear regression?
Wikipedia notes that ""Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function $\mathbb{E}(y | x)$ is linear in the unknown parameters that are estimated from the data.""
How is polynomial regression linear in the unknown parameters if the parameters are coefficients for terms with order $\ge$ 2?
","['regression', 'multiple-regression', 'linear-model', 'nonlinear-regression', 'polynomial']","When you fit a regression model such as $\hat y_i = \hat\beta_0 + \hat\beta_1x_i + \hat\beta_2x^2_i$, the model and the OLS estimator doesn't 'know' that $x^2_i$ is simply the square of $x_i$, it just 'thinks' it's another variable.  Of course there is some collinearity, and that gets incorporated into the fit (e.g., the standard errors are larger than they might otherwise be), but lots of pairs of variables can be somewhat collinear without one of them being a function of the other.  We don't recognize that there are really two separate variables in the model, because we know that $x^2_i$ is ultimately the same variable as $x_i$ that we transformed and included in order to capture a curvilinear relationship between $x_i$ and $y_i$.  That knowledge of the true nature of $x^2_i$, coupled with our belief that there is a curvilinear relationship between $x_i$ and $y_i$ is what makes it difficult for us to understand the way that it is still linear from the model's perspective.  In addition, we visualize $x_i$ and $x^2_i$ together by looking at the marginal projection of the 3D function onto the 2D $x, y$ plane.  If you only have $x_i$ and $x^2_i$, you can try to visualize them in the full 3D space (although it is still rather hard to really see what is going on).  If you did look at the fitted function in the full 3D space, you would see that the fitted function is a 2D plane, and moreover that it is a flat plane.  As I say, it is hard to see well because the $x_i, x^2_i$ data exist only along a curved line going through that 3D space (that fact is the visual manifestation of their collinearity).  We can try to do that here.  Imagine this is the fitted model:   It may be easier to see in these images, which are screenshots of a rotated 3D figure made with the same data using the rgl package.  When we say that a model that is ""linear in the parameters"" really is linear, this isn't just some mathematical sophistry.  With $p$ variables, you are fitting a $p$-dimensional hyperplane in a $p\!+\!1$-dimensional hyperspace (in our example a 2D plane in a 3D space).  That hyperplane really is 'flat' / 'linear'; it isn't just a metaphor.  "
R - QQPlot: how to see whether data are normally distributed,"
I have plotted this after I did a Shapiro-Wilk normality test. The test showed that it is likely that the population is normally distributed. However, how to see this ""behaviour"" on this plot? 
UPDATE
A simple histogram of the data:

UPDATE
The Shapiro-Wilk test says:

","['r', 'data-visualization', 'normal-distribution', 'histogram', 'qq-plot']","""The test showed that it is likely that the population is normally distributed.""No; it didn't show that.Hypothesis tests don't tell you how likely the null is. In fact you can bet this null is false.The Q-Q plot doesn't give a strong indication of non-normality (the plot is fairly straight); there's perhaps a slightly shorter left tail than you'd expect but that really won't matter much.The histogram as-is probably doesn't say a lot either; it does also hint at a slightly shorter left tail. But see hereThe population distribution your data are from isn't going to be exactly normal. However, the Q-Q plot shows that normality is probably a reasonably good approximation.If the sample size was not too small, a lack of rejection of the Shapiro-Wilk would probably be saying much the same.Update: your edit to include the actual Shapiro-Wilk p-value is important because in fact that would indicate you would reject the null at typical significant levels. That test indicates the population your data were sampled from (assuming a simple random sample of that population) is not normally distributed and the mild skewness indicated by the plots is probably what is being picked up by the test. For typical procedures that might assume normality of the variable itself (the one-sample t-test is one that comes to mind), at what appears to be a fairly large sample size, this mild non-normality will be of almost no consequence at all -- one of the problems with goodness of fit tests is they're more likely to reject just when it doesn't matter (when the sample size is large enough to detect some modest non-normality); similarly they're more likely to fail to reject when it matters most (when the sample size is small)."
How to tell the probability of failure if there were no failures?,"
I was wondering if there is a way to tell the probability of something failing (a product) if we have 100,000 products in the field for 1 year and with no failures? What is the probability that one of the next 10,000 products sold fail?
","['probability', 'survival', 'binomial-distribution']",
Fast linear regression robust to outliers,"
I am dealing with linear data with outliers, some of which are at more the 5 standard deviations away from the estimated regression line. I'm looking for a linear regression technique that reduces the influence of these points. 
So far what I did is to estimate the regression line with all the data, then discard the data point with very large squared residuals (say the top 10%) and repeated the regression without those points. 
In the literature there are lots of possible approaches: least trimmed squares, quantile regression , m-estimators, etc. I really don't know which approach I should try, so I'm looking for suggestions. The important for me is that the chosen method should be fast because the robust regression will be computed at
each step of an optimization routine. Thanks a lot!
","['regression', 'linear-model', 'outliers', 'robust', 'fused-lasso']","If your data contains a single outlier, then it can be found reliably using the approach you suggest (without the iterations though). A formal approach to this
isCook, R. Dennis (1979). Influential Observations in Linear Regression. Journal of the American Statistical Association (American Statistical Association) 74 (365): 169–174.For finding more than one outlier, for many years, the leading method was the so-called $M$-estimation family of approach. This is a rather broad family of estimators that includes Huber's $M$ estimator of regression, Koenker's L1 regression as well as the approach proposed by Procastinator in his comment to your question.
The $M$ estimators with convex $\rho$ functions have the advantage that they have about the same numerical complexity as a regular regression estimation. The big disadvantage is that they can only reliably find the outliers if:You can find good implementation of $M$ ($l_1$) estimates of regression in the robustbase (quantreg) R package.If your data contains more than $\lfloor\frac{n}{p+1}\rfloor$ outlier potentially also outlying on the design space, then, finding them amounts to solving a combinatorial problem (equivalently the solution to an $M$ estimator with re-decending/non-convex $\rho$ function).In the last 20 years (and specially last 10) a large body of fast and reliable outlier detection algorithms have been designed to approximately solve this combinatorial problem. These are now widely implemented in the most popular statistical packages (R, Matlab, SAS, STATA,...).Nonetheless, the numerical complexity of finding outliers with these approaches is typically of order $O(2^p)$. Most algorithm can be used in practice for values of $p$ in the mid teens. Typically these algorithms are linear in $n$ (the number of observations) so the number of observation isn't an issue. A big advantage is that most of these algorithms are embarrassingly parallel. More recently, many approaches specifically designed for higher dimensional data have been proposed.Given that you did not specify $p$ in your question, I will list some references
for the case $p<20$. Here are some papers that explain this in greater details in these series of review articles:Rousseeuw, P. J. and van Zomeren B.C. (1990). Unmasking Multivariate Outliers and Leverage Points. Journal of the American Statistical Association, Vol. 85, No. 411, pp. 633-639.Rousseeuw, P.J. and Van Driessen, K. (2006). Computing LTS Regression for Large Data Sets. Data Mining and Knowledge Discovery archive Volume 12 Issue 1, Pages 29 - 45.Hubert, M., Rousseeuw, P.J. and Van Aelst, S. (2008). High-Breakdown Robust Multivariate Methods. Statistical Science, Vol. 23, No. 1, 92–119Ellis S. P.  and  Morgenthaler S. (1992). Leverage and Breakdown in L1 Regression. Journal of the American Statistical Association, Vol. 87,
No. 417, pp. 143-148A recent reference book on the problem of outlier identification is:Maronna R. A., Martin R. D. and Yohai V. J. (2006). Robust Statistics: Theory and
Methods. Wiley, New York.These (and many other variations of these) methods are implemented (among other) in the robustbase R package."
How to read Cook's distance plots?,"
Does anyone know how to work out whether points 7, 16 and 29 are influential points or not?
I read somewhere that because Cook's distance is lower than 1, they are not. Am, I right?  

","['r', 'regression', 'residuals', 'diagnostic', 'cooks-distance']","Some texts tell you that points for which Cook's distance is higher than 1 are to be considered as influential. Other texts give you a threshold of $4/N$ or $4/(N - k - 1)$, where $N$ is the number of observations and $k$ the number of explanatory variables. In your case the latter formula should yield a threshold around 0.1 . John Fox (1), in his booklet on regression diagnostics is rather cautious when it comes to giving numerical thresholds. He advises the use of graphics and to examine in closer details the points with ""values of D that are substantially larger than the rest"". According to Fox, thresholds should just be used to enhance graphical displays. In your case the observations 7 and 16 could be considered as influential. Well, I would at least have a closer look at them. The observation 29 is not substantially different from a couple of other observations. (1) Fox, John. (1991). Regression Diagnostics: An Introduction. Sage Publications."
Suppression effect in regression: definition and visual explanation/depiction,"
What is a suppressor variable in multiple regression and what might be the ways to display suppression effect visually (its mechanics or its evidence in results)? I'd like to invite everybody who has a thought, to share.
","['multiple-regression', 'data-visualization', 'geometry', 'suppressor']","There exist a number of frequenly mentioned regressional effects which conceptually are different but share much in common when seen purely statistically (see e.g. this paper ""Equivalence of the Mediation, Confounding and Suppression
Effect"" by David MacKinnon et al., or Wikipedia articles):I'm not going to discuss to what extent some or all of them are technically similar (for that, read the paper linked above). My aim is to try to show graphically what suppressor is. The above definition that ""suppressor is a variable which inclusion strengthens the effect of another IV on the DV"" seems to me potentially broad because it does not tell anything about mechanisms of such enhancement. Below I'm discussing one mechanism - the only one I consider to be suppression. If there are other mechanisms as well (as for right now, I haven't tried to meditate of any such other) then either the above ""broad"" definition should be considered imprecise or my definition of suppression should be considered too narrow.Suppressor is the independent variable which, when added to the model, raises observed R-square mostly due to its accounting for the residuals left by the model without it, and not due to its own association with the DV (which is comparatively weak). We know that the increase in R-square in response to adding a IV is the squared part correlation of that IV in that new model. This way, if the part correlation of the IV with the DV is greater (by absolute value) than the zero-order $r$ between them, that IV is a suppressor.So, a suppressor mostly ""suppresses"" the error of the reduced model, being weak as a predictor itself. The error term is the complement to the prediction. The prediction is ""projected on"" or ""shared between"" the IVs (regression coefficients), and so is the error term (""complements"" to the coefficients). The suppressor suppresses such error components unevenly: greater for some IVs, lesser for other IVs. For those IVs ""whose"" such components it suppresses greatly it lends considerable facilitating aid by actually raising their regression coefficients.Not strong suppressing effects occurs often and wildly (an example on this site). Strong suppression is typically introduced consciously. A researcher seeks for a characteristic which must correlate with the DV as weak as possible and at the same time would correlate with something in the IV of interest which is considered irrelevant, prediction-void, in respect to the DV. He enters it to the model and gets considerable increase in that IV's predictive power. The suppressor's coefficient is typically not interpreted.I could summarize my definition as follows [up on @Jake's answer and @gung's comments]:""Suppessor"" is a role of a IV in a specific model only, not the characteristic of the separate variable. When other IVs are added or removed, the suppressor can suddenly stop suppressing or resume suppressing or change the focus of its suppressing activity.The first picture below shows a typical regression with two predictors (we'll speak of linear regression). The picture is copied from here where it is explained in more details. In short, moderately correlated (= having acute angle between them) predictors $X_1$ and $X_2$ span 2-dimesional space ""plane X"". The dependent variable $Y$ is projected onto it orthogonally, leaving the predicted variable $Y'$ and the residuals with st. deviation equal to the length of $e$. R-square of the regression is the angle between $Y$ and $Y'$, and the two regression coefficients are directly related to the skew coordinates $b_1$ and $b_2$, respectively. This situation I've called normal or typical because both $X_1$ and $X_2$ correlate with $Y$ (oblique angle exists between each of the independents and the dependent) and the predictors compete for the prediction because they are correlated.It is shown on the next picture. This one is like the previous; however $Y$ vector now directs somewhat away from the viewer and $X_2$ changed its direction considerably. $X_2$ acts as a suppressor. Note first of all that it hardly correlates with $Y$. Hence it cannot be a valuable predictor itself. Second. Imagine $X_2$ is absent and you predict only by $X_1$; the prediction of this one-variable regression is depicted as $Y^*$ red vector, the error as $e^*$ vector, and the coefficient is given by $b^*$ coordinate (which is the endpoint of $Y^*$).Now bring yourself back to the full model and notice that $X_2$ is fairly correlated with $e^*$. Thus, $X_2$ when introduced in the model, can explain a considerable portion of that error of the reduced model, cutting down $e^*$ to $e$. This constellation: (1) $X_2$ is not a rival to $X_1$ as a predictor; and (2) $X_2$ is a dustman to pick up unpredictedness left by $X_1$, - makes $X_2$ a suppressor. As a result of its effect, predictive strength of $X_1$ has grown to some extent: $b_1$ is larger than $b^*$.Well, why is $X_2$ called a suppressor to $X_1$ and how can it reinforce it when ""suppressing"" it? Look at the next picture.It is exactly the same as the previous. Think again of the model with the single predictor $X_1$. This predictor could of course be decomposed in two parts or components (shown in grey): the part which is ""responsible"" for prediction of $Y$ (and thus coinciding with that vector) and the part which is ""responsible"" for the unpredictedness (and thus parallel to $e^*$). It is this second part of $X_1$ - the part irrelevant to $Y$ - is suppressed by $X_2$ when that suppressor is added to the model. The irrelevant part is suppressed and thus, given that the suppressor doesn't itself predict $Y$ any much, the relevant part looks stronger. A suppressor is not a predictor but rather a facilitator for another/other predictor/s. Because it competes with what impedes them to predict.It is the sign of the correlation between the suppressor and the error variable $e^*$ left by the reduced (without-the-suppressor) model. In the depiction above, it is positive. In other settings (for example, revert the direction of $X_2$) it could be negative.Example data:Linear regression results:Observe that $X_2$ served as suppressor. Its zero-order correlation with $Y$ is practically zero but its part correlation is much larger by magnitude, $-.224$. It strengthened to some extent the predictive force of $X_1$ (from r $.419$, a would-be beta in simple regression with it, to beta $.538$ in the multiple regression).According to the formal definition, $X_1$ appeared a suppressor too, because its part correlation is greater than its zero-order correlation. But that is because we have only two IV in the simple example. Conceptually, $X_1$ isn't a suppressor because its $r$ with $Y$ is not about $0$.By way, sum of squared part correlations exceeded R-square: .4750^2+(-.2241)^2 = .2758 > .2256, which would not occur in normal regressional situation (see the Venn diagram below).Adding a variable that will serve a supressor may as well as may not change the sign of some other variables' coefficients. ""Suppression"" and ""change sign"" effects are not the same thing. Moreover, I believe that a suppressor can never change sign of those predictors whom they serve suppressor. (It would be a shocking discovery to add the suppressor on purpose to facilitate a variable and then to find it having become indeed stronger but in the opposite direction! I'd be thankful if somebody could show me it is possible.)To cite an earlier passage: ""For those IVs ""whose"" such components [error components] it suppresses greatly the suppressor lends considerable facilitating aid by actually raising their regression coefficients"". Indeed, in our Example above, $X_2$, the suppressor, raised the coefficient for $X_1$. Such enhancement of the unique predictive power of another regressor is often the aim of a suppressor to a model but it is not the definition of suppressor or of suppression effect. For, the aforementioned enhancement of another predictor's capacity via adding more regressors can easily occure in a normal regressional situation without those regressors being suppressors. Here is an example.Regressions results without and with $X_3$:Inclusion of $X_3$ in the model raised the beta of $X_1$ from $.381$ to $.399$ (and its corresponding partial correlation with $Y$ from $.420$ to $.451$). Still, we find no suppressor in the model. $X_3$'s part correlation ($.229$) is not greater than its zero-order correlation ($.427$). Same is for the other regressors. ""Facilitation"" effect was there, but not due to ""suppression"" effect. Definition of a suppessor is different from just strenghtening/facilitation; and it is about picking up mostly errors, due to which the part correlation exceeds the zero-order one.Normal regressional situation is often explained with the help of Venn diagram.A+B+C+D = 1, all $Y$ variability. B+C+D area is the variability accounted by the two IV ($X_1$ and $X_2$), the R-square; the remaining area A is the error variability. B+C = $r_{YX_1}^2$; D+C = $r_{YX_2}^2$, Pearson zero-order correlations. B and D are the squared part (semipartial) correlations: B = $r_{Y(X_1.X_2)}^2$; D = $r_{Y(X_2.X_1)}^2$. B/(A+B) = $r_{YX_1.X_2}^2$ and D/(A+D) = $r_{YX_2.X_1}^2$ are the squared partial correlations which have the same basic meaning as the standardized regression coefficients betas.According to the above definition (which I stick to) that a suppressor is the IV with part correlation greater than zero-order correlation, $X_2$ is the suppressor if D area > D+C area. That cannot be displayed on Venn diagram. (It would imply that C from the view of $X_2$ is not ""here"" and is not the same entity than C from the view of $X_1$. One must invent perhaps something like multilayered Venn diagram to wriggle oneself to show it.)P.S. Upon finishing my answer I found this answer (by @gung) with a nice simple (schematic) diagram, which seems to be in agreement with what I showed above by vectors."
Help me understand the quantile (inverse CDF) function,"
I am reading about the quantile function, but it is not clear to me. Could you provide a more intuitive explanation than the one provided below?

Since the cdf $F$ is a monotonically increasing function, it has an
  inverse; let us denote this by $F^{−1}$. If $F$ is the cdf of $X$,
  then $F^{−1}(\alpha)$ is the value of $x_\alpha$ such that $P(X \le
 x_\alpha) = \alpha$; this is called the $\alpha$ quantile of $F$. The
  value $F^{−1}(0.5)$ is the median of the distribution, with half of
  the probability mass on the left, and half on the right. The values
  $F^{−1}(0.25)$ and $F^{−1}(0.75)$ are the lower and upper quartiles.

","['distributions', 'cumulative-distribution-function', 'quantiles']","All this may sound complicated at first, but it is essentially about something very simple.By cumulative distribution function we denote the function that returns probabilities of $X$ being smaller than or equal to some value $x$,$$ \Pr(X \le x) = F(x).$$This function takes as input $x$ and returns values from the $[0, 1]$ interval (probabilities)—let's denote them as $p$. The inverse of the cumulative distribution function (or quantile function) tells you what $x$ would make $F(x)$ return some value $p$,$$ F^{-1}(p) = x.$$ This is illustrated in the diagram below which uses the normal cumulative distribution function (and its inverse) as an example.As an simple example, you can take a standard Gumbel distribution. Its cumulative distribution function is$$ F(x) = e^{-e^{-x}} $$and it can be easily inverted: recall natural logarithm function is an inverse of exponential function, so it is instantly obvious that quantile function for Gumbel distribution is$$ F^{-1}(p) = -\ln(-\ln(p)) $$As you can see, the quantile function, according to its alternative name, ""inverts"" the behaviour of cumulative distribution function.Not every function has an inverse. That is why the quotation you refer to says  ""monotonically increasing function"". Recall that from the definition of the function, it has to assign for each input value exactly one output. Cumulative distribution functions for continuous random variables satisfy this property since they are monotonically increasing. For discrete random variables cumulative distribution functions are not continuous and increasing, so we use generalized inverse distribution functions which need to be non-decreasing. More formally, the generalized inverse distribution function is defined as$$ F^{-1}(p) = \inf  \big\{x \in \mathbb{R}: F(x) \ge p \big\}. $$The definition, translated to plain English, says that for given probability value $p$, we are looking for some $x$, that results in $F(x)$ returning value greater or equal then $p$, but since there could be multiple values of $x$ that meet this condition (e.g. $F(x) \ge 0$ is true for any $x$), so we take the smallest $x$ of those.In general, there are no inverses for functions that can return same value for different inputs, for example density functions (e.g., the standard normal density function is symmetric, so it returns the same values for $-2$ and $2$ etc.).  The normal distribution is an interesting example for one more reason—it is one of the examples of cumulative distribution functions that do not have a closed-form inverse. Not every cumulative distribution function has to have a closed-form inverse! Hopefully in such cases the inverses can be found using numerical methods.The quantile function can be used for random generation as described in How does the inverse transform method work?"
Why downsample?,"
Suppose I want to learn a classifier that predicts if an email is spam.  And suppose only 1% of emails are spam.
The easiest thing to do would be to learn the trivial classifier that says none of the emails are spam.  This classifier would give us 99% accuracy, but it wouldn't learn anything interesting, and would have a 100% rate of false negatives.
To solve this problem, people have told me to ""downsample"", or learn on a subset of the data where 50% of the examples are spam and 50% are not spam.
But I'm worried about this approach, since once we build this classifier and start using it on a real corpus of emails (as opposed to a 50/50 test set), it may predict that a lot of emails are spam when they're really not.  Just because it's used to seeing much more spam than there actually is in the dataset.
So how do we fix this problem?
(""Upsampling,"" or repeating the positive training examples multiple times so 50% of the data is positive training examples, seems to suffer from similar problems.)
","['machine-learning', 'classification']","Most classification models in fact don't yield a binary decision, but rather a continuous decision value (for instance, logistic regression models output a probability, SVMs output a signed distance to the hyperplane, ...). Using the decision values we can rank test samples, from 'almost certainly positive' to 'almost certainly negative'.Based on the decision value, you can always assign some cutoff that configures the classifier in such a way that a certain fraction of data is labeled as positive. Determining an appropriate threshold can be done via the model's ROC or PR curves. You can play with the decision threshold regardless of the balance used in the training set. In other words, techniques like up -or downsampling are orthogonal to this.Assuming the model is better than random, you can intuitively see that increasing the threshold for positive classification (which leads to less positive predictions) increases the model's precision at the cost of lower recall and vice versa.Consider SVM as an intuitive example: the main challenge is to learn the orientation of the separating hyperplane. Up -or downsampling can help with this (I recommend preferring upsampling over downsampling). When the orientation of the hyperplane is good, we can play with the decision threshold (e.g. signed distance to the hyperplane) to get a desired fraction of positive predictions."
Why is Entropy maximised when the probability distribution is uniform?,"
I know that entropy is the measure of randomness of a process/variable and it can be defined as follows. for a random variable $X \in$ set $A$ :- $H(X)= \sum_{x_i \in A} -p(x_i) \log (p(x_i)) $. In the book on Entropy and Information Theory by MacKay, he provides this statement in Ch2
Entropy is maximized if p is uniform.
Intuitively, I am able to understand it, like if all datapoints in set $A$ are picked with equal probability $1/m$ ($m$ being cardinality of set $A$), then the randomness or the entropy increases. But if we know that some points in set $A$ are going to occur with more probability than others (say in the case of normal distribution, where the maximum concentration of data points is around the mean and small standard deviation area around it, then the randomness or entropy should decrease.
But is there any mathematical proof for this ? Like the equation for $H(X)$ I differentiate it with respect to $p(x)$ and set it to 0 or something like that.
On a side note, is there any connnection between the entropy that occurs information theory and the entropy calculations in chemistry (thermodynamics) ?
","['uniform-distribution', 'entropy', 'maximum-entropy']",
Machine Learning using Python,"
I am considering using Python libraries for doing my Machine Learning experiments. Thus far, I had been relying on WEKA, but have been pretty dissatisfied on the whole. This is primarily because I have found WEKA to be not so well supported (very few examples, documentation is sparse and community support is less than desirable in my experience), and have found myself in sticky situations with no help forthcoming. Another reason I am contemplating this move is because I am really liking Python (I am new to Python), and don't want to go back to coding in Java. 
So my question is, what are the more 

comprehensive 
scalable (100k features, 10k examples) and
well supported libraries for doing ML in Python out there? 

I am particularly interested in doing text classification, and so would like to use a library that has a good collection of classifiers, feature selection methods (Information Gain, Chi-Sqaured etc.), and text pre-processing capabilities (stemming, stopword removal, tf-idf etc.).
Based on the past e-mail threads here and elsewhere, I have been looking at PyML, scikits-learn and Orange so far. How have people's experiences been with respect to the above 3 metrics that I mention?
Any other suggestions?
","['machine-learning', 'python']","About the scikit-learn option: 100k (sparse) features and 10k samples is reasonably small enough to fit in memory hence perfectly doable with scikit-learn (same size as the 20 newsgroups dataset).Here is a tutorial I gave at PyCon 2011 with a chapter on text classification with exercises and solutions:https://github.com/scikit-learn/scikit-learn (online HTML version)https://github.com/downloads/scikit-learn/scikit-learn-tutorial/scikit_learn_tutorial.pdf (PDF version)https://github.com/scikit-learn/scikit-learn-tutorial (source code + exercises)I also gave a talk on the topic which is an updated version of the version I gave at PyCon FR. Here are the slides (and the embedded video in the comments):As for feature selection, have a look at this answer on quora where all the examples are based on the scikit-learn documentation:We don't have collocation feature extraction in scikit-learn yet. Use nltk and nltk-trainer to do this in the mean time:"
Does the sign of scores or of loadings in PCA or FA have a meaning? May I reverse the sign?,"
I performed principal component analysis (PCA) with R using two different functions (prcomp and princomp) and observed that the PCA scores differed in sign. How can it be?
Consider this:
set.seed(999)
prcomp(data.frame(1:10,rnorm(10)))$x

            PC1        PC2
 [1,] -4.508620 -0.2567655
 [2,] -3.373772 -1.1369417
 [3,] -2.679669  1.0903445
 [4,] -1.615837  0.7108631
 [5,] -0.548879  0.3093389
 [6,]  0.481756  0.1639112
 [7,]  1.656178 -0.9952875
 [8,]  2.560345 -0.2490548
 [9,]  3.508442  0.1874520
[10,]  4.520055  0.1761397

set.seed(999)
princomp(data.frame(1:10,rnorm(10)))$scores
         Comp.1     Comp.2
 [1,]  4.508620  0.2567655
 [2,]  3.373772  1.1369417
 [3,]  2.679669 -1.0903445
 [4,]  1.615837 -0.7108631
 [5,]  0.548879 -0.3093389
 [6,] -0.481756 -0.1639112
 [7,] -1.656178  0.9952875
 [8,] -2.560345  0.2490548
 [9,] -3.508442 -0.1874520
[10,] -4.520055 -0.1761397

Why do the signs (+/-) differ for the two analyses? If I was then using principal components PC1 and PC2 as predictors in a regression, i.e. lm(y ~ PC1 + PC2), this would completely change my understanding of the effect of the two variables on y depending on which method I used! How could I then say that PC1 has e.g. a positive effect on y and PC2 has e.g. a negative effect on y?

In addition: If the sign of PCA components is meaningless, is this true for factor analysis (FA) as well? Is it acceptable to flip (reverse) the sign of individual PCA/FA component scores (or of loadings, as a column of loading matrix)?
","['r', 'pca', 'factor-analysis', 'faq']","PCA is a simple mathematical transformation. If you change the signs of the component(s), you do not change the variance that is contained in the first component.  Moreover, when you change the signs, the weights (prcomp( ... )$rotation) also change the sign, so the interpretation stays exactly the same:showsand pca2$loadings showSo, why does the interpretation stays the same?You do the PCA regression of y on component 1. In the first version (prcomp), say the coefficient is positive: the larger the component 1, the larger the y. What does it mean when it comes to the original variables? Since the weight of the variable 1 (1:10 in a) is positive, that shows that the larger the variable 1, the larger the y.Now use the second version (princomp). Since the component has the sign changed, the larger the y, the smaller the component 1 -- the coefficient of y< over PC1 is now negative. But so is the loading of the variable 1; that means, the larger variable 1, the smaller the component 1, the larger y -- the interpretation is the same.Possibly, the easiest way to see that is to use a biplot.showsThe same biplot for the second variant showsAs you see, the images are rotated by 180°. However, the relation between the weights / loadings (the red arrows) and the data points (the black dots) is exactly the same; thus, the interpretation of the components is unchanged."
What is pre training a neural network?,"
Well the question says it all.
What is meant by ""pre training a neural network""? Can someone explain in pure simple English?
I can't seem to find any resources related to it. It would be great if someone can point me to them.
","['neural-networks', 'pre-training']","The usual way of training a network:You want to train a neural network to perform a task (e.g. classification) on a data set (e.g. a set of images). You start training by initializing the weights randomly. As soon as you start training, the weights are changed in order to perform the task with less mistakes (i.e. optimization).
Once you're satisfied with the training results you save the weights of your network somewhere.You are now interested in training a network to perform a new task (e.g. object detection) on a different data set (e.g. images too but not the same as the ones you used before). Instead of repeating what you did for the first network and start from training with randomly initialized weights, you can use the weights you saved from the previous network as the initial weight values for your new experiment. Initializing the weights this way is referred to as using a pre-trained network. The first network is your pre-trained network. The second one is the network you are fine-tuning.The idea behind pre-training is that random initialization is...well...random, the values of the weights have nothing to do with the task you're trying to solve. Why should a set of values be any better than another set? But how else would you initialize the weights? If you knew how to initialize them properly for the task, you might as well set them to the optimal values (slightly exaggerated). No need to train anything. You have the optimal solution to your problem.
Pre-training gives the network a head start. As if it has seen the data before.What to watch out for when pre-training:The first task used in pre-training the network can be the same as the fine-tuning stage. The datasets used for pre-training vs. fine-tuning can also be the same, but can also be different. It's really interesting to see how pre-training on a different task and different dataset can still be transferred to a new dataset and new task that are slightly different.
Using a pre-trained network generally makes sense if both tasks or both datasets have something in common. The bigger the gap, the less effective pre-training will be. It makes little sense to pre-train a network for image classification by training it on financial data first. In this case there's too much disconnect between the pre-training and fine-tuning stages."
Danger of setting all initial weights to zero in Backpropagation [duplicate],"







This question already has answers here:
                                
                            




Why doesn't backpropagation work when you initialize the weights the same value?

                                (2 answers)
                            

Closed 7 months ago.



Why is it dangerous to initialize weights with zeros? Is there any simple example that demonstrates it?
","['neural-networks', 'backpropagation']","edit see alfa's comment below. I'm not an expert on neural nets, so I'll defer to him. My understanding is different from the other answers that have been posted here.I'm pretty sure that backpropagation involves adding to the existing weights, not multiplying.  The amount that you add is specified by the delta rule.  Note that wij doesn't appear on the right-hand-side of the equation.My understanding is that there are at least two good reasons not to set the initial weights to zero:First, neural networks tend to get stuck in local minima, so it's a good idea to give them many different starting values.  You can't do that if they all start at zero.Second, if the neurons start with the same weights, then all the neurons will follow the same gradient, and will always end up doing the same thing as one another."
Class imbalance in Supervised Machine Learning,"
This is a question in general, not specific to any method or data set. How do we deal with a class imbalance problem in Supervised Machine learning where the number of 0 is around 90% and number of 1 is around 10% in your dataset.How do we optimally train the classifier.
One of the ways which I follow is sampling to make the dataset balanced and then train the classifier and repeat this for multiple samples. 
I feel this is random, Is there any framework to approach these kind of problems.
","['machine-learning', 'unbalanced-classes', 'supervised-learning']","There are many frameworks and approaches. This is a recurrent issue.Examples:Some lit reviews, in increasing order of technical complexity\level of details:Oh, and by the way, 90%/10% is not unbalanced. Card transaction fraud datasets often are split 99.97%/0.03%. This is unbalanced."
Why do we care so much about normally distributed error terms (and homoskedasticity) in linear regression when we don't have to?,"
I suppose I get frustrated every time I hear someone say that non-normality of residuals and /or heteroskedasticity violates OLS assumptions.  To estimate parameters in an OLS model neither of these assumptions are necessary by the Gauss-Markov theorem.  I see how this matters in Hypothesis Testing for the OLS model, because assuming these things give us neat formulas for t-tests, F-tests, and more general Wald statistics.  
But it is not too hard to do hypothesis testing without them.  If we drop just homoskedasticity we can calculate robust standard errors and clustered standard errors easily.  If we drop normality altogether we can use bootstrapping and, given another parametric specification for the error terms, likelihood ratio, and Lagrange multiplier tests.  
It's just a shame that we teach it this way, because I see a lot of people struggling with assumptions they do not have to meet in the first place. 
Why is it that we stress these assumptions so heavily when we have the ability to easily apply more robust techniques?  Am I missing something important?
","['regression', 'assumptions', 'normality-assumption', 'robust', 'teaching']","In Econometrics, we would say that non-normality violates the conditions of the Classical Normal Linear Regression Model, while heteroskedasticity violates both the assumptions of the CNLR and of the Classical Linear Regression Model.  But those that say ""...violates OLS"" are also justified: the name Ordinary Least-Squares comes from Gauss directly and essentially refers to normal errors. In other words ""OLS"" is not an acronym for least-squares estimation (which is a much more general principle and approach), but of the CNLR. Ok, this was history, terminology and semantics. I understand the core of the OP's question as follows: ""Why should we emphasize the ideal, if we have found solutions for the case when it is not present?"" (Because the CNLR assumptions are ideal, in the sense that they provide excellent least-square estimator properties ""off-the-shelf"", and without the need to resort to asymptotic results. Remember also that OLS is maximum likelihood when the errors are normal).As an ideal, it is a good place to start teaching. This is what we always do in teaching any kind of subject: ""simple"" situations are ""ideal"" situations, free of the complexities one will actually encounter in real life and real research, and for which no definite solutions exist.And this is what I find problematic about the OP's post: he writes about robust standard errors and bootstrap as though they are ""superior alternatives"", or foolproof solutions to the lack of the said assumptions under discussion for which moreover the OP writes""..assumptions that people do not have to meet""Why? Because there are some methods of dealing with the situation, methods that have some validity of course, but they are far from ideal? Bootstrap and heteroskedasticity-robust standard errors are not the solutions -if they indeed were, they would have become the dominant paradigm, sending the CLR and the CNLR to the history books. But they are not. So we start from the set of assumptions that guarantees those estimator properties that we have deemed important (it is another discussion whether the properties designated as desirable are indeed the ones that should be), so that we keep visible that any violation of them, has consequences which cannot be fully offset through the methods we have found in order to deal with the absence of these assumptions. It would be really dangerous, scientifically speaking, to convey the feeling that ""we can bootstrap our way to the truth of the matter"" -because, simply, we cannot.So, they remain imperfect solutions to a problem, not an alternative and/or definitely superior way to do things. Therefore, we have first to teach the problem-free situation, then point to the possible problems, and then discuss possible solutions. Otherwise, we would elevate these solutions to a status they don't really have."
What is the difference between NaN and NA?,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I would like to know why some languages like R has both NA and NaN. What are the differences or are they equally the same? Is it really needed to have NA?
",['r'],
Gradient Boosting for Linear Regression - why does it not work?,"
While learning about Gradient Boosting, I haven't heard about any constraints regarding the properties of a ""weak classifier"" that the method uses to build and ensemble model. However, I could not imagine an application of a GB that uses linear regression, and in fact when I've performed some tests - it doesn't work. I was testing the most standard approach with a gradient of sum of squared residuals and adding the subsequent models together.
The obvious problem is that the residuals from the first model are populated in such manner that there is really no regression line to fit anymore. My another observation is that a sum of subsequent linear regression models can be represented as a single regression model as well (adding all intercepts and corresponding coefficients) so I cannot imagine how that could ever improve the model. The last observation is that a linear regression (the most typical approach) is using sum of squared residuals as a loss function - the same one that GB is using.
I also thought about lowering the learning rate or using only a subset of predictors for each iteration, but that could still be summed up to a single model representation eventually, so I guess it would bring no improvement.
What am I missing here? Is linear regression somehow inappropriate to use with Gradient Boosting? Is it because the linear regression uses the sum of squared residuals as a loss function? Are there any particular constraints on the weak predictors so they can be applied to Gradient Boosting?
","['regression', 'machine-learning', 'boosting', 'ensemble-learning', 'gradient']","What am I missing here?I don't think you're really missing anything!Another observation is that a sum of subsequent linear regression models can be represented as a single regression model as well (adding all intercepts and corresponding coefficients) so I cannot imagine how that could ever improve the model. The last observation is that a linear regression (the most typical approach) is using sum of squared residuals as a loss function - the same one that GB is using.Seems to me that you nailed it right there, and gave a short sketch of a proof that linear regression just beats boosting linear regressions in this setting.To be pedantic, both methods are attempting to solve the following optimization problem$$ \hat \beta = \text{argmin}_\beta (y - X \beta)^t (y - X \beta) $$Linear regression just observes that you can solve it directly, by finding the solution to the linear equation$$ X^t X \beta = X^t y $$This automatically gives you the best possible value of $\beta$ out of all possibilities.Boosting, whether your weak classifier is a one variable or multi variable regression, gives you a sequence of coefficient vectors $\beta_1, \beta_2, \ldots$.  The final model prediction is, as you observe, a sum, and has the same functional form as the full linear regressor$$ X \beta_1 + X \beta_2 + \cdots + X \beta_n = X (\beta_1 + \beta_2 + \cdots + \beta_n) $$Each of these steps is chosen to further decrease the sum of squared errors.  But we could have found the minimum possible sum of square errors within this functional form by just performing a full linear regression to begin with.A possible defense of boosting in this situation could be the implicit regularization it provides.  Possibly (I haven't played with this) you could use the early stopping feature of a gradient booster, along with a cross validation, to stop short of the full linear regression. This would provide a regularization to your regression, and possibly help with overfitting.  This is not particularly practical, as one has very efficient and well understood options like ridge regression and the elastic net in this setting.Boosting shines when there is no terse functional form around.  Boosting decision trees lets the functional form of the regressor/classifier evolve slowly to fit the data, often resulting in complex shapes one could not have dreamed up by hand and eye.  When a simple functional form is desired, boosting is not going to help you find it (or at least is probably a rather inefficient way to find it)."
Understanding Naive Bayes,"
From StatSoft, Inc. (2013), Electronic Statistics Textbook, ""Naive Bayes Classifier"":


To demonstrate the concept of Naïve Bayes Classification, consider the
  example displayed in the illustration above. As indicated, the objects
  can be classified as either GREEN or RED. My task is to classify new
  cases as they arrive, i.e., decide to which class label they belong,
  based on the currently exiting objects.
Since there are twice as many GREEN objects as RED, it is reasonable
  to believe that a new case (which hasn't been observed yet) is twice
  as likely to have membership GREEN rather than RED. In the Bayesian
  analysis, this belief is known as the prior probability. Prior
  probabilities are based on previous experience, in this case the
  percentage of GREEN and RED objects, and often used to predict
  outcomes before they actually happen.
Thus, we can write:

Since there is a total of 60 objects, 40 of which are GREEN and 20
  RED, our prior probabilities for class membership are:
 
Having formulated our prior probability, we are now ready to classify
  a new object (WHITE circle). Since the objects are well clustered, it
  is reasonable to assume that the more GREEN (or RED) objects in the
  vicinity of X, the more likely that the new cases belong to that
  particular color. To measure this likelihood, we draw a circle around
  X which encompasses a number (to be chosen a priori) of points
  irrespective of their class labels. Then we calculate the number of
  points in the circle belonging to each class label. From this we
  calculate the likelihood:

From the illustration above, it is clear that Likelihood of X given
  GREEN is smaller than Likelihood of X given RED, since the circle
  encompasses 1 GREEN object and 3 RED ones. Thus:


Although the prior probabilities indicate that X may belong to GREEN
  (given that there are twice as many GREEN compared to RED) the
  likelihood indicates otherwise; that the class membership of X is RED
  (given that there are more RED objects in the vicinity of X than
  GREEN). In the Bayesian analysis, the final classification is produced
  by combining both sources of information, i.e., the prior and the
  likelihood, to form a posterior probability using the so-called Bayes'
  rule (named after Rev. Thomas Bayes 1702-1761).

Finally, we classify X as RED since its class membership achieves the
  largest posterior probability.

This is where the difficulty of my maths understanding comes in. 

p(Cj | x1,x2,x...,xd) is the posterior probability of class membership, i.e., the probability that X belongs to Cj but why write it like this? 
Calculating the likelihood?

Posterior Probability?

I never took math, but my understanding of naive bayes is fine I think just when it comes to these decomposed methods confuses me. Could some one help with visualizing these methods and how to write the math out in an understandable way?
","['machine-learning', 'naive-bayes']","I'm going to run through the whole Naive Bayes process from scratch, since it's not totally clear to me where you're getting hung up.We want to find the probability that a new example belongs to each class: $P(class|feature_1, feature_2,..., feature_n$). We then compute that probability for each class, and pick the most likely class. The problem is that we usually don't have those probabilities. However, Bayes' Theorem lets us rewrite that equation in a more tractable form.Bayes' Thereom is simply$$P(A|B)=\frac{P(B|A) \cdot P(A)}{P(B)}$$ or in terms of our problem:
$$P(class|features)=\frac{P(features|class) \cdot P(class)}{P(features)}$$We can simplify this by removing $P(features)$. We can do this because we're going to rank $P(class|features)$ for each value of $class$; $P(features)$ will be the same every time--it doesn't depend on $class$. This leaves us with
$$ P(class|features) \propto P(features|class) \cdot P(class)$$The prior probabilities, $P(class)$, can be calculated as you described in your question. That leaves $P(features|class)$. We want to eliminate the massive, and probably very sparse, joint probability $P(feature_1, feature_2, ..., feature_n|class)$. If each feature is independent , then $$P(feature_1, feature_2, ..., feature_n|class) = \prod_i{P(feature_i|class})$$ Even if they're not actually independent, we can assume they are (that's the ""naive"" part of naive Bayes). I personally think it's easier to think this through for discrete (i.e., categorical) variables, so let's use a slightly different version of your example. Here, I've divided each feature dimension into two categorical variables..To train the classifer, we count up various subsets of points and use them to compute the prior and conditional probabilities.The priors are trivial: There are sixty total points, forty are green while twenty are red. Thus $$P(class=green)=\frac{40}{60} = 2/3 \text{    and   } P(class=red)=\frac{20}{60}=1/3$$Next, we have to compute the conditional probabilities of each feature-value given a class. Here, there are two features: $feature_1$ and $feature_2$, each of which takes one of two values (A or B for one, X or Y for the other). We therefore need to know the following:These are easy to compute by counting and dividing too. For example, for $P(feature_1=A|class=red)$, we look only at the red points and count how many of them are in the 'A' region for $feature_1$. There are twenty red points, all of which are in the 'A' region, so $P(feature_1=A|class=red)=20/20=1$. None of the red points are in the B region, so $P(feature_1|class=red)=0/20=0$. Next, we do the same, but consider only the green points. This gives us $P(feature_1=A|class=green)=5/40=1/8$ and $P(feature_1=B|class=green)=35/40=7/8$. We repeat that process for $feature_2$, to round out the probability table. Assuming I've counted correctly, we get Those ten probabilities (the two priors plus the eight conditionals) are our modelLet's classify the white point from your example. It's in the ""A"" region for $feature_1$ and the ""Y"" region for $feature_2$. We want to find the probability that it's in each class. Let's start with red. Using the formula above, we know that:
$$P(class=red|example) \propto P(class=red)  \cdot P(feature_1=A|class=red) \cdot P(feature_2=Y|class=red)$$
Subbing in the probabilities from the table, we get $$P(class=red|example) \propto \frac{1}{3} \cdot 1 \cdot \frac{7}{10} = \frac{7}{30}$$
We then do the same for green:
$$P(class=green|example) \propto P(class=green) \cdot P(feature_1=A|class=green) \cdot P(feature_2=Y|class=green) $$Subbing in those values gets us 0 ($2/3 \cdot 0 \cdot 2/10$). Finally, we look to see which class gave us the highest probability. In this case, it's clearly the red class, so that's where we assign the point.In your original example, the features are continuous. In that case, you need to find some way of assigning P(feature=value|class) for each class. You might consider fitting then to a known probability distribution (e.g., a Gaussian). During training, you would find the mean and variance for each class along each feature dimension. To classify a point, you'd find $P(feature=value|class)$ by plugging in the appropriate mean and variance for each class. Other distributions might be more appropriate, depending on the particulars of your data, but a Gaussian would be a decent starting point. I'm not too familiar with the DARPA data set, but you'd do essentially the same thing. You'll probably end up computing something like P(attack=TRUE|service=finger), P(attack=false|service=finger), P(attack=TRUE|service=ftp), etc. and then combine them in the same way as the example. As a side note, part of the trick here is to come up with good features. Source IP , for example, is probably going to be hopelessly sparse--you'll probably only have one or two examples for a given IP. You might do much better if you geolocated the IP and use ""Source_in_same_building_as_dest (true/false)"" or something as a feature instead. I hope that helps more. If anything needs clarification, I'd be happy to try again!"
"Do we have a problem of ""pity upvotes""?","
I know, this may sound like it is off-topic, but hear me out. 
At Stack Overflow and here we get votes on posts, this is all stored in a tabular form.
E.g.:

post id     voter id    vote type     datetime
-------     --------    ---------     --------
10          1           2             2000-1-1 10:00:01 
11          3           3             2000-1-1 10:00:01 
10          5           2             2000-1-1 10:00:01 

... and so on. Vote type 2 is an upvote, vote type 3 is a downvote. You can query an anonymized version of this data at http://data.stackexchange.com
There is a perception that if a post reaches the score of -1 or lower it is more likely to be upvoted. This may be simply confirmation bias or it may be rooted in fact. 
How would we analyze this data to confirm or deny this hypothesis? How would we measure the effect of this bias? 
","['time-series', 'hypothesis-testing', 'data-mining', 'markov-process', 'censoring']",
How to interpret Mean Decrease in Accuracy and Mean Decrease GINI in Random Forest models,"
I'm having some difficulty understanding how to interpret variable importance output from the Random Forest package. Mean decrease in accuracy is usually described as ""the decrease in model accuracy from permuting the values in each feature"".
Is this a statement about the feature as a whole or about specific values within the feature? In either case, is the Mean Decrease in Accuracy the number or proportion of observations that are incorrectly classified by removing the feature (or values from the feature) in question from the model?
Say we have the following model:
require(randomForest)
data(iris)
set.seed(1)
dat <- iris
dat$Species <- factor(ifelse(dat$Species=='virginica','virginica','other'))
model.rf <- randomForest(Species~., dat, ntree=25,
importance=TRUE, nodesize=5)
model.rf
varImpPlot(model.rf)


Call:
 randomForest(formula = Species ~ ., data = dat, ntree = 25,
 proximity = TRUE, importance = TRUE, nodesize = 5)

Type of random forest: classification
Number of trees: 25
No. of variables tried at each split: 2

        OOB estimate of  error rate: 3.33%
Confusion matrix:
          other virginica class.error
other        97         3        0.03
virginica     2        48        0.04


In this model, the OOB rate is rather low (around 5%). Yet, the Mean Decrease in Accuracy for the predictor (Petal.Width) with the highest value in this measure is only around 8.
Does this mean that removing Petal.Width from the model would only result in an additional misclassification of approximately 8 observations on average?
How could the Mean Decrease in Accuracy for Petal.Length be so low, given that it's the highest in this measure, and thus the other variables have even lower values on this measure?
","['r', 'machine-learning', 'classification', 'random-forest']","""Is this a statement about the feature as a whole or about specific values within the feature?""""In either case, is the Mean Decrease in Accuracy the number or proportion of observations that are incorrectly classified by removing the feature (or values from the feature) in question from the model?""-""Does this mean that removing Petal.Length from the model would only result in an additional misclassification of 8 or so observations on average?""""How could the Mean Decrease in Accuracy for Petal.Length be so low, given that it's the highest in this measure, and thus the other variables have even lower values on this measure?""Mainly use variable importance mainly to rank the usefulness of your variables. A clear interpretation of the absolute values of variable importance is hard to do well.GINI:
GINI importance measures the average gain of purity by splits of a given variable. If the variable is useful, it tends to split mixed labeled nodes into pure single class nodes. Splitting by a permuted variables tend neither to increase nor decrease node purities. Permuting a useful variable, tend to give relatively large decrease in mean gini-gain. GINI importance is closely related to the local decision function, that random forest uses to select the best available split. Therefore, it does not take much extra time to compute. On the other hand, mean gini-gain in local splits, is not necessarily what is most useful to measure, in contrary to change of overall model performance. Gini importance is overall inferior to (permutation based) variable importance as it is relatively more biased, more unstable and tend to answer a more indirect question."
Would PCA work for boolean (binary) data types?,"
I want to reduce the dimensionality of higher order systems and capture most of the covariance on a preferably 2 dimensional or 1 dimensional field. I understand this can be done via principal component analysis, and I have used PCA in many scenarios. However, I have never used it with boolean data types, and I was wondering if it is meaningful to do PCA with this set. So for example, pretend I have qualitative or descriptive metrics, and I assign a ""1"" if that metric is valid for that dimension, and a ""0"" if it is not (binary data). So for example, pretend you are trying to compare the Seven Dwarfs in Snow White. We have:
Doc, Dopey, Bashful, Grumpy, Sneezy, Sleepy and Happy, and you want to arrange them based on qualities, and did so as is:
$$\begin{pmatrix}
  & Lactose\ Intolerant & A \ Honor\ Roll & Athletic       & Wealthy \\
Doc & 1 & 0 & 1 & 1 \\
Dopey & 0 & 0 & 0 & 0 \\
Bashful & 1 & 0 & 1 & 1 \\
Grumpy & 1 & 1 & 1 & 1 \\
Sneezy & 0 & 1 & 1 & 0 \\
Sleepy & 1 & 0 & 0 & 0 \\
Happy  & 1 & 1 & 0 & 0
\end{pmatrix}$$
So for example Bashful is lactose intolerant and not on the A honor roll. This is a purely hypothetical matrix, and my real matrix will have many more descriptive columns. My question is, would it still be appropriate to do PCA on this matrix as a means of finding the similarity between individuals?
","['pca', 'data-visualization', 'binary-data', 'dimensionality-reduction', 'correspondence-analysis']","I would like to suggest you a relatively recent technique for automatic structure extraction from categorical variable data (this includes binary). The method is called CorEx from Greg van Steeg from University of Southern California. The idea is to use the notion of Total Correlation based on the entropy measures. It is appealing due to its simplicity and no tuning of large number of hyperparameters.The paper about hierarchical representations (the most recent, builds on the top of the previous measures).
http://arxiv.org/pdf/1410.7404.pdf"
What does the term saturating nonlinearities mean?,"
I was reading the paper ImageNet Classification with Deep Convolutional Neural Networks and in section 3 were they explain the architecture of their Convolutional Neural Network they explain how they preferred using:

non-saturating nonlinearity $f(x) = max(0, x). $

because it was faster to train. In that paper they seem to refer to saturating nonlinearities as the more traditional functions used in CNNs, the sigmoid and the hyperbolic tangent functions (i.e. $f(x) = tanh(x)$ and $f(x) = \frac{1}{1 + e^{-x}} = (1 + e^{-x})^{-1}$ as saturating). 
Why do they refer to these functions as ""saturating"" or ""non-saturating""? In what sense are these function ""saturating"" or ""non-saturating""? What do those terms mean in the context of convolutional neural networks? Are they used in other areas of machine learning (and statistics)? 
","['machine-learning', 'neural-networks', 'terminology', 'conv-neural-network']","A saturating activation function squeezes the input.These definitions are not specific to convolutional neural networks.The Rectified Linear Unit (ReLU) activation function, which is defined as $f(x)=max(0,x)$ is non-saturating because $\lim_{z\to+\infty} f(z) = +\infty$:The sigmoid activation function, which is defined as $f(x) = \frac{1}{1 + e^{-x}}$ is saturating, because it squashes real numbers to range between $[0,1]$: The tanh (hyperbolic tangent) activation function is saturating as it squashes real numbers to range between $[-1,1]$:(figures are from CS231n,  MIT License)"
"Correct spelling (capitalization, italicization, hyphenation) of ""p-value""?","
I realize this is pedantic and trite, but as a researcher in a field outside of statistics, with limited formal education in statistics, I always wonder if I'm writing ""p-value"" correctly. Specifically:

Is the ""p"" supposed to be capitalized?
Is the ""p"" supposed to be italicized? (Or in mathematical font, in TeX?)
Is there supposed to be a hyphen between ""p"" and ""value""?
Alternatively, is there no ""proper"" way of writing ""p-value"" at all, and any dolt will understand what I mean if I just place ""p"" next to ""value"" in some permutation of these options?

","['hypothesis-testing', 'p-value', 'terminology']","There do not appear to be ""standards"".  For example:My brief, unscientific survey suggests that the most common combination is lower-case, italicized p without a hyphen."
What are posterior predictive checks and what makes them useful?,"
I understand what the posterior predictive distribution is, and I have been reading about posterior predictive checks, although it isn't clear to me what it does yet.

What exactly is the posterior predictive check?
Why do some authors say that running posterior predictive checks is ""using the data twice"" and should not be abused ? (or even that it is not Bayesian)? (e.g. see this or this)
What is this check exactly useful for? Can it really be used for model selection? (e.g. does it factor in both, fitness and model complexity?)

","['bayesian', 'model-selection', 'posterior']","Posterior predictive checks are, in simple words, ""simulating replicated data under the
fitted model and then comparing these to the observed data"" (Gelman and Hill, 2007, p. 158). So, you use posterior predictive to ""look for systematic discrepancies between real and simulated data"" (Gelman et al. 2004, p. 169).The argument about ""using the data twice"" is that you use your data for estimating the model and then, for checking if the model fits the data, while generally it is a bad idea and it would be better to validate your model on external data, that was not used for estimation.Posterior predictive checks are helpful in assessing if your model gives you ""valid"" predictions about the reality - do they fit the observed data or not. It is a helpful phase of model building and checking. It does not give you a definite answer on if your model is ""ok"" or if it is ""better"" then other model, however, it can help you to check if your model makes sens.This is nicely described in LaplacesDemon vignette Bayesian Inference:Comparing the predictive distribution $y^\text{rep}$ to the observed
  data $y$ is generally termed a ""posterior predictive check"". This type
  of check includes the uncertainty associated with the estimated
  parameters of the model, unlike frequentist statistics.Posterior predictive checks (via the predictive distribution) involve
  a double-use of the data, which violates the likelihood principle.
  However, arguments have been made in favor of posterior predictive
  checks, provided that usage is limited to measures of discrepancy to
  study model adequacy, not for model comparison and inference (Meng
  1994).Gelman recommends at the most basic level to compare $y^\text{rep}$ to $y$,
  looking for any systematic differences, which could indicate potential
  failings of the model (Gelman et al. 2004, p. 159). It is often first
  recommended to compare graphical plots, such as the distribution of
  $y$ and $y^\text{rep}$."
"How are propensity scores different from adding covariates in a regression, and when are they preferred to the latter?","
I admit I'm relatively new to propensity scores and causal analysis.
One thing that's not obvious to me as a newcomer is how the ""balancing"" using propensity scores is mathematically different from what happens when we add covariates in a regression? What's different about the operation, and why is it (or is it) better than adding subpopulation covariates in a regression?
I've seen some studies that do an empirical comparison of the methods, but I haven't seen a good discussion relating the mathematical properties of the two methods and why PSM lends itself to causal interpretations while including regression covariates does not. There also seems to be a lot of confusion and controversy in this field, which makes things even more difficult to pick up.
Any thoughts on this or any pointers to good resources/papers to better understand the distinction? (I'm slowly making my way through Judea Pearl's causality book, so no need to point me to that)
","['regression', 'multivariate-analysis', 'causality', 'propensity-scores']",
Using lmer for repeated-measures linear mixed-effect model,"
EDIT 2: I originally thought I needed to run a two-factor ANOVA with repeated measures on one factor, but I now think a linear mixed-effect model will work better for my data. I think I nearly know what needs to happen, but am still confused by few points.
The experiments I need to analyze look like this: 

Subjects were assigned to one of several treatment groups
Measurements of each subject were taken on multiple days
So:

Subject is nested within treatment
Treatment is crossed with day


(each subject is assigned to only one treatment, and measurements are taken on each subject on each day)
My dataset contains the following information:

Subject = blocking factor (random factor)
Day = within subject or repeated measures factor (fixed factor)
Treatment = between subject factor (fixed factor)
Obs = measured (dependent) variable

UPDATE
OK, so I went and talked to a statistician, but he's an SAS user.  He thinks that the model should be:
Treatment + Day + Subject(Treatment) + Day*Subject(Treatment)
Obviously his notation is different from the R syntax, but this model is supposed to account for:

Treatment   (fixed)
Day   (fixed)
the Treatment*Day interaction
Subject nested within Treatment  (random)
Day crossed with ""Subject within Treatment""   (random)

So, is this the correct syntax to use? 
m4 <- lmer(Obs~Treatment*Day + (1+Treatment/Subject) + (1+Day*Treatment/Subject), mydata)

I'm particularly concerned about whether the Day crossed with ""Subject within Treatment"" part is right.  Is anyone familiar with SAS, or confident that they understand what's going on in his model, able to comment on whether my sad attempt at R syntax matches?
Here are my previous attempts at building a model and writing syntax (discussed in answers & comments):
m1 <- lmer(Obs ~ Treatment * Day + (1 | Subject), mydata)

How do I deal with the fact that subject is nested within treatment?  How does m1 differ from: 
m2 <- lmer(Obs ~ Treatment * Day + (Treatment|Subject), mydata)
m3 <- lmer(Obs ~ Treatment * Day + (Treatment:Subject), mydata)

and are m2 and m3 equivalent (and if not, why)?
Also, do I need to be using nlme instead of lme4 if I want to specify the correlation structure (like correlation = corAR1)?  According to Repeated Measures, for a repeated-measures analysis with repeated measures on one factor, the covariance structure (the nature of the correlations between measurements of the same subject) is important. 
When I was trying to do a repeated-measures ANOVA, I'd decided to use a Type II SS; is this still relevant, and if so, how do I go about specifying that?
Here's an example of what the data look like:
mydata <- data.frame(
  Subject  = c(13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 
               34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 
               19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 
               40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 
               29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65), 
  Day       = c(rep(c(""Day1"", ""Day3"", ""Day6""), each=28)), 
  Treatment = c(rep(c(""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", 
                      ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A""), each = 4)), 
  Obs       = c(6.472687, 7.017110, 6.200715, 6.613928, 6.829968, 7.387583, 7.367293, 
                8.018853, 7.527408, 6.746739, 7.296910, 6.983360, 6.816621, 6.571689, 
                5.911261, 6.954988, 7.624122, 7.669865, 7.676225, 7.263593, 7.704737, 
                7.328716, 7.295610, 5.964180, 6.880814, 6.926342, 6.926342, 7.562293, 
                6.677607, 7.023526, 6.441864, 7.020875, 7.478931, 7.495336, 7.427709, 
                7.633020, 7.382091, 7.359731, 7.285889, 7.496863, 6.632403, 6.171196, 
                6.306012, 7.253833, 7.594852, 6.915225, 7.220147, 7.298227, 7.573612, 
                7.366550, 7.560513, 7.289078, 7.287802, 7.155336, 7.394452, 7.465383, 
                6.976048, 7.222966, 6.584153, 7.013223, 7.569905, 7.459185, 7.504068, 
                7.801867, 7.598728, 7.475841, 7.511873, 7.518384, 6.618589, 5.854754, 
                6.125749, 6.962720, 7.540600, 7.379861, 7.344189, 7.362815, 7.805802, 
                7.764172, 7.789844, 7.616437, NA, NA, NA, NA))

","['r', 'anova', 'mixed-model', 'repeated-measures', 'lme4-nlme']","I think that your approach is correct. Model m1 specifies a separate intercept for each subject. Model m2 adds a separate slope for each subject. Your slope is across days as subjects only participate in one treatment group. If you write model m2 as follows it's more obvious that you model a separate intercept and slope for each subjectThis is equivalent to:I.e. the main effects of treatment, day and the interaction between the two.I think that you don't need to worry about nesting as long as you don't repeat subject ID's within treatment groups. Which model is correct, really depends on your research question. Is there reason to believe that subjects' slopes vary in addition to the treatment effect? You could run both models and compare them with anova(m1,m2) to see if the data supports either one.I'm not sure what you want to express with model m3? The nesting syntax uses a /, e.g. (1|group/subgroup). I don't think that you need to worry about autocorrelation with such a small number of time points. "
Why does the correlation coefficient between X and X-Y random variables tend to be 0.7,"
Taken from Practical Statistics for Medical Research where Douglas Altman writes in page 285: 

...for any two quantities X and Y, X will be correlated with X-Y.
  Indeed, even if X and Y are samples of random numbers we would expect
  the correlation of X and X-Y to be 0.7

I tried this in R and it seems to be the case:
x <- rnorm(1000000, 10, 2)
y <- rnorm(1000000, 10, 2)
cor(x, x-y)

xu <- sample(1:100, size = 1000000, replace = T)
yu <- sample(1:100, size = 1000000, replace = T)
cor(xu, xu-yu)

Why is that? What is the theory behind this?
","['correlation', 'random-variable', 'intuition']","If $X$ and $Y$ are uncorrelated random variables with equal variance $\sigma^2$, then we have that
$$\begin{align}
\operatorname{var}(X-Y) &= \operatorname{var}(X) + \operatorname{var}(-Y)\\
&= \operatorname{var}(X) + \operatorname{var}(Y)\\
&=2\sigma^2,\\
\operatorname{cov}(X, X-Y) &= \operatorname{cov}(X,X) - \operatorname{cov}(X,Y)
& \text{bilinearity of covariance operator}\\
&= \operatorname{var}(X) - 0 & 0 ~\text{because}~X ~\text{and}~ Y ~\text{are 
uncorrelated}\\
&= \sigma^2.
\end{align}$$
Consequently, $$\rho_{X,X-Y} = \frac{\operatorname{cov}(X, X-Y)}{\sqrt{\operatorname{var}(X)\operatorname{var}(X-Y)}}= \frac{\sigma^2}{\sqrt{\sigma^2\cdot2\sigma^2}} = \frac{1}{\sqrt{2}}.$$
So, when you find 
$$\frac{\sum_{i=1}^n\left(x_i - \bar{x}\right)
\left((x_i-y_i) - (\bar{x}-\bar{y})\right)}{
\sqrt{\sum_{i=1}^n\left(x_i - \bar{x}\right)^2
\sum_{i=1}^n\left((x_i-y_i) - (\bar{x}-\bar{y})\right)^2}} $$
the sample correlation of $x$ and $x-y$ for a large data set $\{(x_i,y_i)\colon 1 \leq i \leq n\}$ drawn from a population with these properties, 
which includes ""random numbers"" as a special case, the result tends to 
be close to the population correlation value $\frac{1}{\sqrt{2}} \approx 0.7071\ldots$"
Most confusing statistical terms,"
We statisticians use many words in ways that are slightly different from the way everyone else uses them. This cause lots of problems when we teach or explain what we are doing.  I'll start a list (and now I'll add some definitions, per comments):

Power is the probability of correctly reject a false null hypothesis in a hypothetical situation where the data comes from a specific alternate hypothesis or range of alternates. Usually, this means ""our statistical method should succeed"" if ""something is happening"".
Bias - a statistic is biased if it is systematically different from the population parameter associated with it.
Significance - results are statistically significant at some percent (often 5%) in the following situation: If the population which the sample comes from has a true effect of 0, a statistic at least as extreme as the one gotten from the sample would only occur 5% of the time.
Interaction - Two independent variables interact if the relationship between the dependent variable and one independent variable is different at different levels of the other independent variable

But there have to be many others!
","['terminology', 'communication']",
Dynamic Time Warping Clustering,"
What would be the approach to use Dynamic Time Warping (DTW) to perform clustering of time series?
I have read about DTW as a way to find similarity between two time series, while they could be shifted in time. Can I use this method as a similarity measure for clustering algorithm like k-means?
","['time-series', 'clustering', 'dynamic-time-warping']","Do not use k-means for timeseries. DTW is not minimized by the mean; k-means may not converge and even if it converges it will not yield a very good result. The mean is an least-squares estimator on the coordinates. It minimizes variance, not arbitrary distances, and k-means is designed for minimizing variance, not arbitrary distances. Assume you have two time series. Two sine waves, of the same frequency, and a rather long sampling period; but they are offset by $\pi$. Since DTW does time warping, it can align them so they perfectly match, except for the beginning and end. DTW will assign a rather small distance to these two series.
However, if you compute the mean of the two series, it will be a flat 0 - they cancel out. The mean does not do dynamic time warping, and loses all the value that DTW got. On such data, k-means may fail to converge, and the results will be meaningless. K-means really should only be used with variance (= squared Euclidean), or some cases that are equivalent (like cosine, on L2 normalized data, where cosine similarity is the same as $2 -$ squared Euclidean distance)Instead,  compute a distance matrix using DTW,  then run hierarchical clustering such as single-link. In contrast to k-means, the series may even have different length. "
Is minimizing squared error equivalent to minimizing absolute error? Why squared error is more popular than the latter?,"
When we conduct linear regression $y=ax+b$ to fit a bunch of data points $(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$, the classic approach minimizes the squared error. I have long been puzzled by a question that will minimizing the squared error yield the same result as minimizing the absolute error? If not, why minimizing squared error is better? Is there any reason other than ""the objective function is differentiable""?
Squared error is also widely used to evaluate model performance, but absolute error is less popular. Why squared error is more commonly used than the absolute error? If taking derivatives is not involved, calculating absolute error is as easy as calculating squared error, then why squared error is so prevalent? Is there any unique advantage that can explain its prevalence?
Thank you.
","['least-squares', 'error']","Minimizing square errors (MSE) is definitely not the same as minimizing absolute deviations (MAD) of errors. MSE provides the mean response of $y$ conditioned on $x$, while MAD provides the median response of $y$ conditioned on $x$.Historically, Laplace originally considered the maximum observed error as a measure of the correctness of a model. He soon moved to considering MAD instead. Due to his inability to exact solving both situations, he soon considered the differential MSE. Himself and Gauss (seemingly concurrently) derived the normal equations, a closed-form solution for this problem. Nowadays, solving the MAD is relatively easy by means of linear programming. As it is well known, however, linear programming does not have a closed-form solution.From an optimization perspective, both correspond to convex functions. However, MSE is differentiable, thus, allowing for gradient-based methods, much efficient than their non-differentiable counterpart. MAD is not differentiable at $x=0$.A further theoretical reason is that, in a bayesian setting, when assuming uniform priors of the model parameters, MSE yields normal distributed errors, which has been taken as a proof of correctness of the method. Theorists like the normal distribution because they believed it is an empirical fact, while experimentals like it because they believe it a theoretical result.A final reason of why MSE may have had the wide acceptance it has is that it is based on the euclidean distance (in fact it is a solution of the projection problem on an euclidean banach space) which is extremely intuitive given our geometrical reality."
Why zero correlation does not necessarily imply independence,"
If two variables have 0 correlation, why are they not necessarily independent? Are zero correlated variables independent under special circumstances ?  If possible, I am looking for an intuitive explanation, not a highly technical one.
","['correlation', 'independence']","Correlation measures linear association between two given variables and it has no obligation to detect any other form of association else.So those two variables might be associated in several other non-linear ways and correlation could not distinguish from independent case.As a very didactic, artificial and non realistic example, one can consider $X$  such that $P(X=x)=1/3$ for $x=-1, 0, 1$ and $Y=X^2$. Notice that they are not only associated, but one is a function of the other. Nonetheless, their correlation is 0, for their association is orthogonal to the association that correlation can detect. "
Excel as a statistics workbench,"
It seems that lots of people (including me) like to do exploratory data analysis in Excel. Some limitations, such as the number of rows allowed in a spreadsheet, are a pain but in most cases don't make it impossible to use Excel to play around with data.
A paper by McCullough and Heiser, however, practically screams that you will get your results all wrong -- and probably burn in hell as well -- if you try to use Excel.
Is this paper correct or is it biased? The authors do sound like they hate Microsoft.
","['software', 'computational-statistics', 'excel']","Use the right tool for the right job and exploit the strengths of the tools you are familiar with.In Excel's case there are some salient issues:Please don't use a spreadsheet to manage data, even if your data will fit into one. You're just asking for trouble, terrible trouble.  There is virtually no protection against typographical errors, wholesale mixing up of data, truncating data values, etc., etc.Many of the statistical functions indeed are broken.  The t distribution is one of them.The default graphics are awful.It is missing some fundamental statistical graphics, especially boxplots and histograms.The random number generator is a joke (but despite that is still effective for educational purposes).Avoid the high-level functions and most of the add-ins; they're c**p.  But this is just a general principle of safe computing: if you're not sure what a function is doing, don't use it.  Stick to the low-level ones (which include arithmetic functions, ranking, exp, ln, trig functions, and--within limits--the normal distribution functions).  Never use an add-in that produces a graphic: it's going to be terrible.  (NB: it's dead easy to create your own probability plots from scratch.  They'll be correct and highly customizable.)In its favor, though, are the following:Its basic numerical calculations are as accurate as double precision floats can be.  They include some useful ones, such as log gamma.It's quite easy to wrap a control around input boxes in a spreadsheet, making it possible to create dynamic simulations easily.If you need to share a calculation with non-statistical people, most will have some comfort with a spreadsheet and none at all with statistical software, no matter how cheap it may be.It's easy to write effective numerical macros, including porting old Fortran code, which is quite close to VBA.  Moreover, the execution of VBA is reasonably fast.  (For example, I have code that accurately computes non-central t distributions from scratch and three different implementations of Fast Fourier Transforms.)It supports some effective simulation and Monte-Carlo add-ons like Crystal Ball and @Risk.  (They use their own RNGs, by the way--I checked.)The immediacy of interacting directly with (a small set of) data is unparalleled: it's better than any stats package, Mathematica, etc.  When used as a giant calculator with loads of storage, a spreadsheet really comes into its own.Good EDA, using robust and resistant methods, is not easy, but after you have done it once, you can set it up again quickly.  With Excel you can effectively reproduce all the calculations (although only some of the plots) in Tukey's EDA book, including median polish of n-way tables (although it's a bit cumbersome).In direct answer to the original question, there is a bias in that paper: it focuses on the material that Excel is weakest at and that a competent statistician is least likely to use.  That's not a criticism of the paper, though, because warnings like this need to be broadcast."
Bayesian vs frequentist Interpretations of Probability,"
Can someone give a good rundown of the differences between the Bayesian and the frequentist approach to probability?
From what I understand:
The frequentists view is that the data is a repeatable random sample (random variable) with a specific frequency/probability (which is defined as the relative frequency of an event as the number of trials approaches infinity). The underlying parameters and probabilities remain constant during this repeatable process and that the variation is due to variability in $X_n$ and not the probability distribution (which is fixed for a certain event/process).
The bayesian view is that the data is fixed while the frequency/probability for a certain event can change meaning that the parameters of the distribution changes. In effect, the data that you get changes the prior distribution of a parameter which gets updated for each set of data.
To me it seems that the frequentist approach is more practical/logical since it seems reasonable that events have a specific probability and that the variation is in our sampling.
Furthermore, most data analysis from studies is usually done using the frequentist approach (i.e. confidence intervals, hypothesis testing with p-values etc) since it is easily understandable.
I was just wondering whether anyone could give me a quick summary of their interpretation of bayesian vs frequentist approach including bayesian statistical equivalents of the frequentist p-value and confidence interval. In addition, specific examples of where 1 method would be preferable to the other is appreciated.
","['probability', 'bayesian', 'frequentist']","In the frequentist approach, it is asserted that the only sense in which probabilities have meaning is as the limiting value of the number of successes in a sequence of trials, i.e. as$$p = \lim_{n\to\infty} \frac{k}{n}$$where $k$ is the number of successes and $n$ is the number of trials. In particular, it doesn't make any sense to associate a probability distribution with a parameter.For example, consider samples $X_1, \dots, X_n$ from the Bernoulli distribution with parameter $p$ (i.e. they have value 1 with probability $p$ and 0 with probability $1-p$). We can define the sample success rate to be$$\hat{p} = \frac{X_1+\cdots +X_n}{n}$$and talk about the distribution of $\hat{p}$ conditional on the value of $p$, but it doesn't make sense to invert the question and start talking about the probability distribution of $p$ conditional on the observed value of $\hat{p}$. In particular, this means that when we compute a confidence interval, we interpret the ends of the confidence interval as random variables, and we talk about ""the probability that the interval includes the true parameter"", rather than ""the probability that the parameter is inside the confidence interval"".In the Bayesian approach, we interpret probability distributions as quantifying our uncertainty about the world. In particular, this means that we can now meaningfully talk about probability distributions of parameters, since even though the parameter is fixed, our knowledge of its true value may be limited. In the example above, we can invert the probability distribution $f(\hat{p}\mid p)$ using Bayes' law, to give$$\overbrace{f(p\mid \hat{p})}^\text{posterior} = \underbrace{\frac{f(\hat{p}\mid p)}{f(\hat{p})}}_\text{likelihood ratio} \overbrace{f(p)}^\text{prior}$$The snag is that we have to introduce the prior distribution into our analysis - this reflects our belief about the value of $p$ before seeing the actual values of the $X_i$. The role of the prior is often criticised in the frequentist approach, as it is argued that it introduces subjectivity into the otherwise austere and object world of probability.In the Bayesian approach one no longer talks of confidence intervals, but instead of credible intervals, which have a more natural interpretation - given a 95% credible interval, we can assign a 95% probability that the parameter is inside the interval."
Alternatives to one-way ANOVA for heteroskedastic data,"
I have data from 3 groups of algae biomass ($A$, $B$, $C$) which contain unequal sample sizes ($n_A=15$, $n_B=13$, $n_C=12$) and I would like compare if these groups are from the same population.
One-way ANOVA would definitely be the way to go, however upon conducting normality tests on my data, heteroskedascity seems to the main issue. My raw data, without any transformation, produced a  ratio of variances ($F_{\max} = 19.1$) which is very much higher than the critical value ($F_{\rm crit} = 4.16$) and therefore I cannot perform one-way ANOVA. 
I also tried transformation to normalise my data. Even after trials of various transformations (log, square root, square), the lowest $F_{\max}$ produced after transformation with a $\log_{10}$ transformation was $7.16$, which was still higher compared to $F_{\rm crit}$.
Can anyone here advise me on where to go from here? I can't think of other methods of transformation to normalise by data. Are there any alternatives to a one-way ANOVA?
P.S.: my raw data are below: 
A: 0.178 0.195 0.225 0.294 0.315 0.341 0.36  0.363 0.371 0.398 0.407 0.409 0.432 
   0.494 0.719
B: 0.11  0.111 0.204 0.416 0.417 0.441 0.492 0.965 1.113 1.19  1.233 1.505 1.897
C: 0.106 0.114 0.143 0.435 0.448 0.51  0.576 0.588 0.608 0.64  0.658 0.788 0.958

","['r', 'anova', 'data-transformation', 'heteroscedasticity']",
What is difference-in-differences?,"
Difference in differences has long been popular as a non-experimental tool, especially in economics. Can somebody please provide a clear and non-technical answer to the following questions about difference-in-differences.
What is a difference-in-difference estimator?
Why is a difference-in-difference estimator any use?
Can we actually trust difference-in-difference estimates?
","['regression', 'econometrics', 'difference-in-difference']",
Bayesian equivalent of two sample t-test?,"
I'm not looking for a plug and play method like BEST in R but rather a mathematical explanation of what are some Bayesian methods I can use to test the difference between the mean of two samples. 
","['hypothesis-testing', 'bayesian', 't-test']",
Obtaining predicted values (Y=1 or 0) from a logistic regression model fit,"
Let's say that I have an object of class glm (corresponding to a logistic regression model) and I'd like to turn the predicted probabilities given by predict.glm using the argument type=""response"" into binary responses, i.e. $Y=1$ or $Y=0$. What's the quickest & most canonical way to do this in R? 
While, again, I'm aware of predict.glm, I don't know where exactly the cutoff value $P(Y_i=1|\hat X_{i})$ lives -- and I guess this is my main stumbling block here.
","['r', 'generalized-linear-model', 'logistic']","Once you have the predicted probabilities, it is up to you what threshold you would like to use. You may choose the threshold to optimize sensitivity, specificity or whatever measure it most important in the context of the application (some additional info would be helpful here for a more specific answer). You may want to look at ROC curves and other measures related to optimal classification. Edit: To clarify this answer somewhat I'm going to give an example. The real answer is that the optimal cutoff depends on what properties of the classifier are important in the context of the application. Let $Y_{i}$ be the true value for observation $i$, and $\hat{Y}_{i}$ be the predicted class. Some common measures of performance are (1) Sensitivity: $P(\hat{Y}_i=1 | Y_i=1)$ - the proportion of '1's that are correctly identified as so. (2) Specificity: $P(\hat{Y}_i=0 | Y_i=0)$ - the proportion of '0's that are correctly identified as so (3) (Correct) Classification Rate: $P(Y_i = \hat{Y}_i)$ - the proportion of predictions that were correct. (1) is also called True Positive Rate, (2) is also called True Negative Rate.For example, if your classifier were aiming to evaluate a diagnostic test for a serious disease that has a relatively safe cure, the sensitivity is far more important that the specificity. In another case, if the disease were relatively minor and the treatment were risky, specificity would be more important to control. For general classification problems, it is considered ""good"" to jointly optimize the sensitivity and specification - for example, you may use the classifier that minimizes their Euclidean distance from the point $(1,1)$: $$ \delta = \sqrt{ [P(Y_i=1 | \hat{Y}_i=1)-1]^2 + [P(Y_i=0 | \hat{Y}_i=0)-1]^2 }$$$\delta$ could be weighted or modified in another way to reflect a more reasonable measure of distance from $(1,1)$ in the context of the application - euclidean distance from (1,1) was chosen here arbitrarily for illustrative purposes. In any case, all of these four measures could be most appropriate, depending on the application. Below is a simulated example using prediction from a logistic regression model to classify. The cutoff is varied to see what cutoff gives the ""best"" classifier under each of these three measures. In this example the data comes from a logistic regression model with three predictors (see R code below plot). As you can see from this example, the ""optimal"" cutoff depends on which of these measures is most important - this is entirely application dependent. Edit 2: $P(Y_i = 1 | \hat{Y}_i = 1)$ and $P(Y_i = 0 | \hat{Y}_i = 0)$, the Positive Predictive Value and Negative Predictive Value (note these are NOT the same as sensitivity and specificity) may also be useful measures of performance. "
"Understanding ""almost all local minimum have very similar function value to the global optimum""","
In a recent blog post by Rong Ge, it was said that:  

It is believed that for many problems including learning deep nets, almost all local minimum have very similar function value to the global optimum, and hence finding a local minimum is good enough.

Where does this belief come from?
","['machine-learning', 'neural-networks', 'optimization', 'deep-learning']","A recent paper The Loss Surfaces of Multilayer Networks offers some possible explanations for this. From their abstract (bold is mine):""We conjecture that both simulated annealing and SGD converge
to the band of low critical points, and that all critical points found there are local minima of  high  quality  measured  by  the  test  error.
This emphasizes a major difference between large- and small-size networks where for the latter  poor  quality  local  minima  have  non-zero probability of being recovered.  Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.""A lot of the influential people in deep learning (Yann LeCunn and Yoshua Bengio to name a few) and some researchers coming more from the mathematical angle (Rong Ge and other Sanjeev Arora collaborators) have been discussing and exploring these ideas.In the above referenced paper, see Figure 3, which shows a banding/concentration phenomenon of the local minima values as the nets have more hidden units. The banding/concentration represents some empirical evidence that for deeper or larger models, a local minima is ""good enough"", since their loss values are roughly similar. And most importantly, they have a loss which is closer to the global minimum as the model gets more complex (in this case wider, but in practice, deeper).Furthermore, they use a spin-glass model, which they even state is just a model and not necessarily indicative of the true picture, to show that reaching the global minimizer from a local minima may take exponentially long:""In order to find a further low lying minimum we must pass through a saddle point. Therefore we must go up at  least to the level where there is an equal  amount of saddle points to have a decent chance of finding a
path that might possibly take us to another local minimum. This process takes an exponentially long time so in practice finding the global minimum is not feasible.""The Rong Ge research is centered around breaking through saddle points. Yoshua Bengio and his collaborators have posed a pretty bold Saddle Point Hypothesis:Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum.source here: Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.To some extent, the above two approaches aren't exactly the same (the Saddle Point Hypothesis might question what is really a local minima and what is merely a poorly conditioned saddle point with a very long plateau region?). The idea behind the Saddle Point Hypothesis is that it is possible to design optimization methods to break through saddle points, for example Saddle-Free Newton from the Bengio article, to potentially speed up convergence and maybe even reach the global optimum. The first Multilayer Loss Surface article is not really concerned with reaching the global optimum and actually believes it to have some poor overfitting properties. Curiously, both articles use ideas from statistical physics and spin-glass models.But they are sort of related in that both articles believe that in order to reach the global minimizer, one must overcome the optimization challenge of saddle points. The first article just believes that local minima are good enough.It is fair to wonder if momentum methods and other new optimization algorithms, which can estimate some 2nd order curvature properties can escape saddle points. A famous animation by Alec Radford here.To answer your question: ""where does this belief come from"" I personally think it comes from the fact that it's possible to use different random seeds to learn different weights, but the corresponding nets have similar quantitative performance. For example, if you set two different random seeds for Glorot weight initialization, you will probably learn different weights, but if you train using similar optimization methods, the nets will have similar performance. One common folklore belief is that the optimization landscape is similar to that of an egg carton, another good blog post on this here: No more local minima? with the egg-carton analogy.Edit: I just wanted to be clear that the egg carton analogy is not true, otherwise there would be no need for momentum or other more advanced optimization techniques. But it is known that SGD does not perform as well as SGD+Momentum or more modern optimization algorithms, perhaps due to the existence of saddle points."
What is a latent space?,"
In the context of machine learning, I often hear the term latent space, sometimes qualified with the word ""high dimensional"" or ""low dimensional"" latent space.
I am a bit puzzled by this term (as it is almost never defined rigorously). 
Can someone please provide a definition or motivation to the concept of a latent space?
","['machine-learning', 'neural-networks', 'definition']","Latent space refers to an abstract multi-dimensional space containing feature values that we cannot interpret directly, but which encodes a meaningful internal representation of externally observed events.Just as we, humans, have an understanding of a broad range of topics and the events belonging to those topics, latent space aims to provide a similar understanding to a computer through a quantitative spatial representation/modeling. The motivation to learn a latent space (set of hidden topics/ internal representations) over the observed data (set of events) is that large differences in observed space/events could be due to small variations in latent space (for the same topic). Hence, learning a latent space would help the model make better sense of observed data than from observed data itself, which is a very large space to learn from.Some examples of latent space are:1) Word Embedding Space - consisting of word vectors where words similar in meaning have vectors that lie close to each other in space (as measured by cosine-similarity or euclidean-distance) and words that are unrelated lie far apart (Tensorflow's Embedding Projector provides a good visualization of word embedding spaces).2) Image Feature Space - CNNs in the final layers encode higher-level features in the input image that allows it to effectively detect, for example, the presence of a cat in the input image under varying lighting conditions, which is a difficult task in the raw pixel space.3) Topic Modeling methods such as LDA, PLSA use statistical approaches to obtain a latent set of topics from an observed set of documents and word distribution. (PyLDAvis provides a good visualization of topic models)4) VAEs & GANs aim to obtain a latent space/distribution that closely approximates the real latent space/distribution of the observed data.In all the above examples, we quantitatively represent the complex observation space with a (relatively simple) multi-dimensional latent space that approximates the real latent space of the observed data.The terms ""high dimensional"" and ""low dimensional"" help us define how specific or how general the kinds of features we want our latent space to learn and represent. High dimensional latent space is sensitive to more specific features of the input data and can sometimes lead to overfitting when there isn't sufficient training data. Low dimensional latent space aims to capture the most important features/aspects required to learn and represent the input data (a good example is a low-dimensional bottleneck layer in VAEs). If this answer helped, please don't forget to up-vote it :)"
How to choose between ROC AUC and F1 score?,"
I recently completed a Kaggle competition in which roc auc score was used as per competition requirement. Before this project, I normally used f1 score as the metric to measure model performance. Going forward, I wonder how should I choose between these two metrics? When to use which, and what are their respective pros and cons?
Btw, I read the article here What are the differences between AUC and F1-score?, but it doesn't tell me when to use which.
Thanks in advance for any help!
","['machine-learning', 'modeling', 'roc', 'scoring-rules']",
How to determine the optimal threshold for a classifier and generate ROC curve?,"
Let say we have a SVM classifier, how do we generate ROC curve? (Like theoretically) (because we are generate TPR and FPR with each of the threshold). And how do we determine the optimal threshold for this SVM classifier?
","['machine-learning', 'svm', 'roc']","Use the SVM classifier to classify a set of annotated examples, and ""one point"" on the ROC space based on one prediction of the examples can be identified. Suppose the number of examples is 200, first count the number of examples of the four cases.
\begin{array} {|r|r|r|}
\hline
  & \text{labeled true} & \text{labeled false} \\ 
 \hline
\text{predicted true}  &71& 28\\
 \hline
\text{predicted false}  &57&44 \\
\hline
\end{array}
Then compute TPR (True Positive Rate) and FPR (False Positive Rate). $TPR = 71/ (71+57)=0.5547$, and $FPR=28/(28+44) = 0.3889$ On the ROC space, the x-axis is FPR, and the y-axis is TPR. So point $(0.3889, 0.5547)$ is obtained.

To draw an ROC curve, just Some details can be checked in http://en.wikipedia.org/wiki/Receiver_operating_characteristic.Besides, these two links are useful about how to determine an optimal threshold. A simple method is to take the one with maximal sum of true positive and false negative rates. Other finer criteria may include other variables involving different thresholds like financial costs, etc.
http://www.medicalbiostatistics.com/roccurve.pdf 
http://www.kovcomp.co.uk/support/XL-Tut/life-ROC-curves-receiver-operating-characteristic.html"
Are all models useless? Is any exact model possible -- or useful?,"
This question has been festering in my mind for over a month. The February 2015 issue of Amstat News contains an article by Berkeley Professor Mark van der Laan that scolds people for using inexact models. He states that by using models, statistics is then an art rather than a science. According to him, one can always use ""the exact model"" and that our failure to do so contributes to a ""lack of rigor ... I fear that our representation in data science is becoming marginalized."" 
I agree that we are in danger of becoming marginalized, but the threat usually comes from those who claim (sounding a lot like Professor van der Laan, it seems) that they are not using some approximate method, but whose methods are in fact far less rigorous than are carefully applied statistical models -- even wrong ones. 
I think it is fair to say that Prof van der Laan is rather scornful of those who repeat Box's oft-used quote, ""all models are wrong, but some are useful."" Basically, as I read it, he says that all models are wrong, and all are useless. Now, who am I to disagree with a Berkeley professor? On the other hand, who is he to so cavalierly dismiss the views of one of the real giants in our field?
In elaborating, Dr van der Laan states that ""it is complete nonsense to state that all models are wrong, ... For example, a statistical model that makes no assumptions is always true."" He continues: ""But often, we can do much better than that: We might know that the data are the result of $n$ independent identical experiments."" I do not see how one can know that except in very narrow random-sampling or controlled experimental settings. The author points to his work in targeted maximum likelihood learning and targeted minimum loss-based learning, which ""integrates the state of the art in machine learning/data-adaptive estimation, all the incredible advances in causal inference, censored data, efficiency and empirical process theory while still providing formal statistical inference."" Sounds great!
There are also some statements I agree with. He says that we need to take our work, our role as a statistician, and our scientific collaborators seriously. Hear hear! It is certainly bad news when people routinely use a logistic regression model, or whatever, without carefully considering whether it is adequate to answering the scientific question or if it fits the data. And I do see plenty of such abuses in questions posted in this forum. But I also see effective and valuable uses of inexact models, even parametric ones. And contrary to what he says, I have seldom been ""bored to death by another logistic regression model."" Such is my naivety, I guess.
So here are my questions:

What useful statistical inferences can be made using a model that makes no assumptions at all?
Does there exist a case study, with important, real data in the use of targeted maximum likelihood? Are these methods widely used and accepted?
Are all inexact models indeed useless?
Is it possible to know that you have the exact model other than in trivial cases?
If this is too opinion-based and hence off-topic, where can it be discussed? Because Dr van der Laan's article definitely does need some discussion.

","['machine-learning', 'maximum-likelihood', 'modeling', 'nonparametric', 'targeted-maximum-likelihood']",
When conducting a t-test why would one prefer to assume (or test for) equal variances rather than always use a Welch approximation of the df?,"
It seems like when the assumption of homogeneity of variance is met that the results from a Welch adjusted t-test and a standard t-test are approximately the same.  Why not simply always use the Welch adjusted t?
","['variance', 't-test', 'heteroscedasticity']","I would like to oppose the other two answers based on a paper (in German) by Kubinger, Rasch and Moder (2009).They argue, based on ""extensive"" simulations from distributions either meeting or not meeting the assumptions imposed by a t-test, (normality and homogenity of variance) that the welch-tests performs equally well when the assumptions are met (i.e., basically same probability of committing alpha and beta errors) but outperforms the t-test if the assumptions are not met, especially in terms of power. Therefore, they recommend to always use the welch-test if the sample size exceeds 30.As a meta-comment: For people interested in statistics (like me and probably most other here) an argument based on data (as mine) should at least count equally as arguments solely based on theoretical grounds (as the others here).Update:
After thinking about this topic again, I found two further recommendations of which the newer one assists my point. Look at the original papers (which are both, at least for me, freely available) for the argumentations that lead to these recommendations.The first recommendation comes from Graeme D. Ruxton in 2006: ""If you want to compare the central tendency of 2 populations based on samples of unrelated data, then the unequal variance t-test should always be used in preference to the Student's t-test or Mann–Whitney U test.""
In:
Ruxton, G.D., 2006. The unequal variance t-test is an underused
alternative to Student’s t-test and the Mann–Whitney U test.
Behav. Ecol. 17, 688–690.The second (older) recommendation is from Coombs et al. (1996, p. 148): ""In summary, the independent samples t test is generally acceptable in terms of controlling Type I error rates provided there are sufficiently large equal-sized samples, even when the equal population variance assumption is violated. For unequal-sized samples, however, an alternative that does not assume equal population variances is preferable. Use the James second-order test when distributions are either short-tailed symmetric or normal. Promising alternatives include the Wilcox H and Yuen trimmed means tests, which provide broader control of Type I error rates than either the Welch test or the James test and have greater power when data are long-tailed."" (emphasis added)
In:
Coombs WT, Algina J, Oltman D. 1996. Univariate and multivariate omnibus hypothesis tests selected to control type I error rates when population variances are not necessarily equal. Rev Educ Res 66:137–79."
Why is a Bayesian not allowed to look at the residuals?,"
In the article ""Discussion: Should Ecologists Become Bayesians?"" Brian Dennis gives a surprisingly balanced and positive view of Bayesian statistics when his aim seems to be to warn people about it. However, in one paragraph, without any citations or justifications, he says:

Bayesians, you see, are not allowed to look at their residuals. It
  violates the likelihood principle to judge an outcome by how extreme
  it is under a model. To a Bayesian, there are no bad models, just bad
  beliefs.

Why would a Bayesian not be allowed to look at the residuals? What would be the appropriate citation for this (i.e. who is he quoting)?
Dennis, B.
Discussion: Should Ecologists Become Bayesians?
Ecological Applications, Ecological Society of America, 1996, 6, 1095-1103
","['bayesian', 'residuals', 'frequentist', 'likelihood-principle']","Of course Bayesians can look at the residuals! And of course there are bad models in Bayesian analysis. Maybe a few Bayesians in the 70's supported views like that (and I doubt that), but you will hardly find any Bayesian supporting this view these days.I didn't read the text, but Bayesians use things like Bayes factors to compare models. Actually, a Bayesian can even compute the probability of a model being true and pick the model which is more likely to be true. Or a Bayesian can average across models, to achieve a better model. Or can use posterior predictive checks. There are a lot of options to check a model and each one may favor one approach or another, but to say that there are no bad models in Bayesian analysis is non-sense.So, at most, it would be more appropriate to say that in some extreme versions of Bayesianism (extreme versions that almost no one uses in applied settings, by the way) you're not allowed to check your model. But than you could say that in some extreme versions of frequentism you're not allowed to use observational data as well. But why waste time discussing these silly things, when we can discuss if and when, in an applied setting, we should use Bayesian or frequentist methods or whatever? That's what's important, in my humble opinion. Update: The OP asked for a reference of someone advocating the extreme version of Bayes. Since I never read any extreme version of Bayes, I can't provide this reference. But I'd guess that Savage may be such a reference. I never read anything written by him, so I may be wrong. ps.: Think about the problem of the ""well-calibrated Bayesian"" (Dawid (1982), JASA, 77, 379). A coherent subjectivist Bayesian forecaster can't be uncalibrated, and so wouldn't review his model/forecasts despite any overwhelming evidence that he's uncalibrated. But I don't think anyone in practice can claim to be that coherent. Thus, model review is important.ps2.: I like this paper by Efron as well. The full reference is: Efron, Bradley (2005). ""Bayesians, frequentists, and scientists."" Journal of the American Statistical Association 100(469)."
Why is softmax output not a good uncertainty measure for Deep Learning models?,"
I've been working with Convolutional Neural Networks (CNNs) for some time now, mostly on image data for semantic segmentation/instance segmentation. I've often visualized the softmax of the network output as a ""heat map"" to see how high per pixel activations for a certain class are.
I've interpreted low activations as ""uncertain"" / ""unconfident"" and high activations as ""certain"" / ""confident"" predictions. Basically this means interpreting the softmax output (values within $(0,1)$) as a probability or (un)certainty measure of the model. 
(E.g. I've interpreted an object/area with a low softmax activation averaged over its pixels to be difficult for the CNN to detect, hence the CNN being ""uncertain"" about predicting this kind of object.)
In my perception this often worked, and adding additional samples of ""uncertain"" areas to the training results improved results on these. However I've heard quite often now from different sides that using/interpreting softmax output as an (un)certainty measure is not a good idea and is generally discouraged. Why?

EDIT: 
To clarify what I'm asking here I'll elaborate on my insights so far in answering this question. However none of the following arguments made clear to me ** why it is generally a bad idea**, as I was repeatedly told by colleagues, supervisors and is also stated e.g. here in section ""1.5""

In classification models, the probability vector obtained at the end of the pipeline (the softmax output) is often erroneously interpreted as model confidence

or here in section ""Background"" :

Although it may be tempting to interpret the values given by the final softmax layer of a convolutional neural network as confidence scores, we need to be careful not to read too much into this.


The sources above reason that using the softmax output as uncertainty measure is bad because:

imperceptible perturbations to a real image can change a deep network’s softmax output to arbitrary values

This means that softmax output isn't robust to ""imperceptible perturbations"" and hence it's output isn't usuable as probability. 
Another paper picks up on the ""softmax output = confidence"" idea and argues that with this intuition networks can be easily fooled, producing ""high confidence outputs for unrecognizable images"". 

(...) the region (in the input domain) corresponding to a particular class may be much larger than the space in that region occupied by training examples from that class. The result of this is that an image may lie within the region assigned to a class and so be classified with a large peak in the softmax output, while still being far from images that occur naturally in that class in the training set.

This means that data that is far away from training data should never get a high confidence, since the model ""can't"" be sure about it (as it has never seen it).
However: Isn't this generally simply questioning the generalization properties of NNs as a whole? I.e. that the NN's with softmax loss don't generalize well to (1) ""imperceptible perturbations"" or (2) input data samples that are far away from the training data, e.g. unrecognizable images.
Following this reasoning I still don't understand, why in practice with data that is not abstractly and artifically altered vs. the training data (i.e. most ""real"" applications), interpreting the softmax output as a ""pseudo-probability"" is a bad idea. After all, they seem to represent well what my model is sure about, even if it isn't correct (in which case I need to fix my model). And isn't model uncertainty always ""only"" an approximation?
","['probability', 'deep-learning', 'conv-neural-network', 'uncertainty', 'softmax']",
How to do logistic regression subset selection?,"
I am fitting a binomial family glm in R, and I have a whole troupe of explanatory variables, and I need to find the best (R-squared as a measure is fine). Short of writing a script to loop through random different combinations of the explanatory variables and then recording which performs the best, I really don't know what to do. And the leaps function from package leaps does not seem to do logistic regression.
Any help or suggestions would be greatly appreciated.
","['r', 'logistic']",
"Why are MA(q) time series models called ""moving averages""?","
When I read ""moving average"" in relation to a time series, I think something like $\frac{(x_{t-1} + x_{t-2} + x_{t-3})}3$, or perhaps a weighted average like $0.5x_{t-1} + 0.3x_{t-2} + 0.2x_{t-3}$.
(I realize these are actually AR(3) models, but these are what my brain jumps to.)
Why are MA(q) models formulas of error terms, or ""innovations""?  What does $\{\epsilon\}$ have to do with a moving average?
I feel like I'm missing some obvious intuition.
","['time-series', 'arima', 'terminology', 'moving-average']","A footnote in Pankratz (1983), on page 48, says:The label ""moving average"" is technically incorrect since the MA
  coefficients may be negative and may not sum to unity. This label is
  used by convention.Box and Jenkins (1976) also says something similar. On page 10:The name ""moving average"" is somewhat misleading because the weights
  $1, -\theta_{1}, -\theta_{2}, \ldots, -\theta_{q}$, which multiply the
  $a$'s, need not total unity nor need that be positive. However, this
  nomenclature is in common use, and therefore we employ it.I hope this helps."
How to interpret p-value of Kolmogorov-Smirnov test (python)?,"
I have Two samples that I want to test (using python) if they are drawn from the same distribution. To do that I use the statistical function ks_2samp from scipy.stats. It returns 2 values and I find difficulties how to interpret them. 
Help please!
",['python'],
Area under Precision-Recall Curve (AUC of PR-curve) and Average Precision (AP),"
Is Average Precision (AP) the Area under Precision-Recall Curve (AUC of PR-curve)  ?
EDIT:
here is some comment about difference in PR AUC and AP.

The AUC is obtained by trapezoidal interpolation of the precision. An
  alternative and usually almost equivalent metric is the Average
  Precision (AP), returned as info.ap. This is the average of the
  precision obtained every time a new positive sample is recalled. It is
  the same as the AUC if precision is interpolated by constant segments
  and is the definition used by TREC most often.

http://www.vlfeat.org/overview/plots-rank.html
Moreover, the auc and the average_precision_score results are not the same in scikit-learn. This is strange, because in the documentation we have: 

Compute average precision (AP) from prediction scores This score
  corresponds to the area under the precision-recall curve.

here is the code:
# Compute Precision-Recall and plot curve
precision, recall, thresholds = precision_recall_curve(y_test, clf.predict_proba(X_test)[:,1])
area = auc(recall, precision)
print ""Area Under PR Curve(AP): %0.2f"" % area  #should be same as AP?

print 'AP', average_precision_score(y_test, y_pred, average='weighted')
print 'AP', average_precision_score(y_test, y_pred, average='macro')
print 'AP', average_precision_score(y_test, y_pred, average='micro')
print 'AP', average_precision_score(y_test, y_pred, average='samples')

for my classifer I have something like:
Area Under PR Curve(AP): 0.65
AP 0.676101781304
AP 0.676101781304
AP 0.676101781304
AP 0.676101781304

","['scikit-learn', 'precision-recall', 'auc', 'average-precision']",Short answer is: YES. Average Precision is a single number used to summarise a Precision-Recall curve:You can approximate the integral (area under the curve) with:Please take a look at this link for a good explanation.
What book is recommendable to start learning statistics using R at the same time?,"
Books to Learn Statistics using R
What exactly is the book I'm looking for.
What I am looking for is a book that teaches you statistics while using R to give you hands-on experience and thus end up helping you learn R together. I've seen on amazon many books that attempts to do that, but not with R. Examples are Minitab and SAS.
Are the R Book and Statistical Computing an option? - Still not answered.
The R Book and Statistical Computing: An Introduction to Data Analysis using S-Plus seems viable, but a reader opinion here would be helpful and welcome.
How the book relate to statistics courses?
To be even more precise on what I was looking for, consider these two courses learning outcomes on statistics from a math department at the university Im currently a student:
Intermediate Statistics and Probability & Statistics, that is, I'm looking in a book a normal statistics course going to intermediate level but rather than just board and paper having you learning and using R instead. That also means I am looking for a book that assume I want to learn statistics from the beginning. 
This book is for researchers too.
I am also a software engineer researcher, but I guess the current situation where you are found with mountains of data and want to learn statistics to go on writing code to automate that is pretty much applicable to many other fields. 
That means I'm am not interested on learning every single detail of every single property for every single curve, but am more concerned on making sense of data for my research domain, although I would not mind if the book wanted to go deep on that. 
As a final motivation, I find myself reading scientific papers in different sort of communities that claim results based on statistical inference while there is no readable proof if the statistics assumptions/constraints are being violated or not. 
A R book that is not much about statistics won't ensure I am not following up on this practice, which is also why I decided looking for a book that is akin to a statistics course using R rather than playing around with a overview book. 
Related questions in Cross Validated.

What books provide an overview of computational statistics as it applies to computer science? - Differs that the question looks for an overview while this is to learn statistics using R.
Open source statistical books gives a list of open source (open books) available online. 

Answers and feedback for this question.
@Julie
Suggested books were few I already come across but are an example that unfortunately doesn't suits me:
Introductory Statistics with R, Using R for Introductory Statistics, Statistics: An Introduction using R are few of the books that I already looked on amazon but are about an statistics overview or make assumptions that requires previous statistics knowledge. The problem with overview books is mostly about not calling attention to the assumptions, constraints and provide enough explanation to result in make sense of the information. 
If you believe there is no book that could fit on this needing as well or think the R book or the Statistical Computing: An Introduction to Data Analysis using S-Plus would fit this, I would also appreciate this type of answer.
@Christopher Aden
Introduction to Probability and Statistics Using R seems to be the closest one but still broad general to what I was looking for.
What I was expecting for is a book such as David S. Moore, The Basics of Statistics because:

It covers all statistics subjects. 
It uses two tools, miniTab and other to give hands-on learning on the just explained method.
It very much highlight assumptions and constraints. This is very important for a researcher who has not taken a in depth statistics course and want to use statistics. Hardly overview books will cover them, which is dangerous for researchers.


You can see the book table of contents here. Notice how the focus is statistics and the tool usage is to improve understanding and get the student to know how to use tools to do the statistics after learning in an easier way. Its not about the tool, its about statistics! 


I want exactly the same thing, but using R.
@Gregory Demin
It uses R as pedagogy examples, assumes you want to learn statistics and best of all, it is open source. Unfortunately, does not cover ANOVA nor ANCOVA, or more advanced subjects.
@Peter Ellis
Good suggestion for a textbook that covers what is wanted in this question.
Books in the asker opinion that answer the question.
@Peter Ellis and @Gregory Demin.
Collection of R Books on Amazon
Amazon discussion about R books for different students background may be found here. 
Video Lectures teaching Statistics using R
Google Tech Talks from 2007 that also motivated this question and covers more about Data Mining rather than statistics but using R together here.
","['r', 'references']","I think one reason it is so hard to answer this is that R is so powerful and flexible that a real introduction to R programming goes well beyond what is normally needed in an introduction to statistics.  The books that teach statistics using MiniTab, JMP or SPSS are doing relatively straightforward things with the software that barely scratch the surface of what R is capable of when it comes to data manipulation, simulations, custom-built functions, etc.Having said that, I think that Wilcox's Modern Statistics for the Social and Behavioral Sciences: A Practical Introduction (2012) is a brilliant new book.  It assumes no statistical knowledge and takes you from scratch right through to a big range of modern robust techniques; and assumes not much more R knowledge than the ability to open it up and load a dataset.    It covers many of the classical techniques too including ANOVA (mentioned in the OP).I would see this book as the equivalent of the books that introduce stats and a stats package like SPSS at the same time.  However, it won't teach you to program in R - only how to do modern statistical analysis with it, with an emphasis on robust techniques that address the known problems with classical analysis that are sidelined by most other approaches to teaching statistics.The three problems with classical methods that this book particularly addresses right from the beginning are sampling from heavy-tailed distributions; skewness; and heteroscedasticity.Wilcox uses R because ""In terms of taking advantage of modern statistical techniques, R clearly dominates.  When analyzing data, it is undoubtedly the most important software development during the last quarter of a century.  And it is free. Although classic methods have fundamental flaws, it is not suggested that they be completely abandoned... Consequently, illustrations are provided on how to apply standard methods with R.  Of particular importance here is that, in addition, illustrations are provided regarding how to apply modern methods using over 900 R functions written for this book.""This book is so excellent that after we bought a copy for work I purchased my own copy at home.  The chapter headings are: Further edit - having checked out the David Moore example of what you are looking for, I really think Wilcox's book meets the need."
Different ways to write interaction terms in lm?,"
I have a question about which is the best way to specify an interaction in a regression model. Consider the following data:
d <- structure(list(r = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
     1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""r1"",""r2""),
     class = ""factor""), s = structure(c(1L, 1L, 1L, 1L, 1L, 
     2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L), 
    .Label = c(""s1"",""s2""), class = ""factor""), rs = structure(c(1L, 1L,
     1L,1L, 1L,2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L),
    .Label = c(""r1s1"",""r1s2"", ""r2s1"", ""r2s2""), class = ""factor""), 
     y = c(19.3788027518437, 23.832287726332, 26.2533235300492,
     15.962906892112, 24.2873740664331, 28.5181676764727, 25.2757801195961,
     25.3601044326474, 25.3066440027202, 24.3298865128677, 32.5684219007394,
     31.0048406654209, 31.671238316086, 34.1933764518288, 36.8784821769123,
     41.6691435168277, 40.4669714825801, 39.2664137501106, 39.4884849591932,
     49.247505535468)), .Names = c(""r"",""s"", ""rs"", ""y""), 
     row.names = c(NA, -20L), class = ""data.frame"")

Two equivalent ways to specify the model with interactions are:
lm0 <- lm(y ~ r*s, data=d)
lm1 <- lm(y ~ r + s + r:s, data=d)

My question is if I could specify the interaction considering a new variable (rs) with the same levels of interaction:
lm2 <- lm(y ~ r + s + rs, data=d)

What advantages/disadvantages have this approach? And why the results from these two approaches are different?
summary(lm1)

lm(formula = y ~ r + s + r:s, data = d, x = TRUE)
            coef.est coef.se
(Intercept) 21.94     1.46  
rr2         11.32     2.07  
ss2          3.82     2.07  
rr2:ss2      4.95     2.92  
---
n = 20, k = 4
residual sd = 3.27, R-Squared = 0.87


summary(lm2)

lm(formula = y ~ r + s + rs, data = d, x = TRUE)
            coef.est coef.se
(Intercept) 21.94     1.46  
rr2         11.32     2.07  
ss2          8.76     2.07   # ss2 coef is different from lm1
rsr1s2      -4.95     2.92  
---
n = 20, k = 4
residual sd = 3.27, R-Squared = 0.87

","['r', 'regression', 'interaction']","The results are different because the way lm sets up the model with the interaction is different from how it is set up when you set it up yourself.  If you look at the residual sd, it's the same, which indicates (not definitively) that the underlying models are the same, just expressed (to the lm internals) differently.  If you define your interaction as paste(d$s, d$r) instead of paste(d$r, d$s) your parameter estimates will change again, in interesting ways.Note how in your model summary for lm1 the coefficient estimate for ss2 is 4.94 lower than in the summary for lm2, with the coefficient for rr2:ss2 being 4.95 (if you print to 3 decimal places, the difference goes away).  This is another indication that an internal rearrangement of terms has occurred.I can't think of any advantage to doing it yourself, but there may be one with more complex models where you don't want a full interaction term but instead only some of the terms in the ""cross"" between two or more factors."
Data APIs/feeds available as packages in R,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






EDIT: The Web Technologies and Services CRAN task view contains a much more comprehensive list of data sources and APIs available in R.  You can submit a pull request on github if you wish to add a package to the task view.

I'm making a list of the various data feeds that are already hooked into R or that are easy to setup. Here's my initial list of packages, and I was wondering what else I'm missing.
I'm trying to limit this list to ""real time"" or ""close to real time"" data feeds/APIs, where the underlying data might change between downloads.  There's plenty of lists out there for static datasets, which only require one download.
This list is currently biased towards financial/time series data, and I could use some help expanding it into other domains.
Free Data:
Data Source -  Package
Google Finance historical data - quantmod
Google Finance balance sheets - quantmod
Yahoo Finance historical data - quantmod
Yahoo Finance historical data - tseries
Yahoo Finance current options chain - quantmod
Yahoo Finance historical analyst estimates - fImport
Yahoo Finance current key stats - fImport - seems to be broken
OANDA historic exchange rates/metal prices - quantmod
FRED historic macroeconomic indicators - quantmod
World Bank historic  macroeconomic indicators - WDI
Google Trends historic search volume data - RGoogleTrends
Google Docs - RGoogleDocs
Google Storage - RGoogleStorage
Twitter - twitteR
Zillow - Zillow
New York Times - RNYTimes
US Census 2000 - UScensus2000
infochimps - infochimps
datamarket - rdatamarket - requires free account
Factual.com - factualR
Geocode addresses - RDSTK
Map coordinates to political boundaries - RDSTK
Weather Underground - Roll your own
Google News - Roll your own
Earth Sciences netCDF Data - Roll your own
Climate Data - Roll your own
Public health data - Roll your own
OAI Harvester - Open Archives Initiative harvester
RAmazonS3 -  S3 Amazon storage server
Rflikr - Flikr api    
Requires a subscription:
Bloomberg - RBloomberg
LIM - LIM
Trades and Quotes from NYSE - RTAQ
Interactive Brokers - IBrokers 
","['r', 'references', 'dataset']","Instructions for using R to download netCDF data can be found here, a common format used for storing Earth science data, e.g. as in marine geospatial data from OpenEarth or climate model driver and forecasts from UCARrnpn (under development) enables you to get data from the National Phenology Network - a citizen science project to track the timing of plant green-up, flowering, and senescence. See the developer's blog post.-obsolete- RClimate provides tools to download and manipulate flat-file climate data (with tutorials, including here-Download historical finance data with tseries::get.hist.quoteMichael Samuel's documents downloading public health dataraster::getData provides access to climate variables via worldclim "
A smaller dataset is better: Is this statement false in statistics? How to refute it properly?,"
Dr. Raoult, who promotes Hydroxychloroquine, has some really intriguing statement about statistics in the biomedical field:

It's counterintuitive, but the smaller the sample size of a clinical test, the more significant its results are. The differences in a sample of 20 people may be more significant than in a sample of 10,000 people. If we need such a sample, there is a risk of being wrong. With 10,000 people, when the differences are small, sometimes they don't exist. 

Is this a false statement in statistics? If so, is it therefore also false in the Biomedical field? On which basis can we refute it properly, by a confidence interval?
Dr. Raoult promotes Hydroxychloroquine as a cure for Covid-19, thanks to an article about data from 24 patients. His claims have been repeated a lot, but mainly in the mainstream media, not in the scientific press.
In machine learning, the SciKit workflow states that before choosing any model, you NEED a dataset with at least 50 samples, whether it be for a simple regression, or the most advance clustering technique, etc., which is why I find this statement really intriguing.

EDIT: some of the answers below make the assumption of no result bias. They deal with the concept of power and effect size.
However it seems there is a bias in Dr. Raoult data. The most striking being removing data for the dead, for the reason they could not provide data for the entire duration of the study.  
My question remains however focused on the impact of using a small sample size.

Source of the statement about Statistics in a French magazine
Reference to the scientific paper in question.

","['hypothesis-testing', 'statistical-significance', 'sample-size']","I agree with many of the other answers here but think the statement is even worse than they make it out to be. The statement is an explicit version of an implicit claim in many shoddy analyses of small datasets. These hint that because they have found a significant result in a small sample, their claimed result must be real and important because it is 'harder' to find a significant effect in a small sample. This belief is simply wrong, because random error in small samples means that any result is less trustworthy, whether the effect size is large or small. Large and significant effects are therefore more likely to be of the incorrect magnitude and more importantly, they can be in the wrong direction. Andrew Gelman refers to these usefully as 'Type S' errors (estimates whose sign is wrong) as opposed to 'Type M' errors (estimates whose magnitude is wrong). Combine this with the file-drawer effect (small, non-significant results go unpublished, while large, significant ones are published) and you are most of the way to the replication crisis and a lot of wasted time, effort and money. Thanks to @Adrian below for digging up a figure from Gelman that illustrates this point well: This may seem to be an extreme example but the point is entirely relevant to the argument made by Raoult. "
Famous easy to understand examples of a confounding variable invalidating a study,"
Are there any well-known statistical studies that were originally published and thought to be valid, but later had to be thrown out due to a confounding variable that wasn't taken into account?  I'm looking for something easy to understand that could be explained to and appreciated by a quantitative literacy class that has zero pre-requisites.  
","['experiment-design', 'confounding', 'observational-study', 'paradox']",
Bootstrap vs. permutation hypothesis testing,"
There are several popular resampling techniques, which are often used in practice, such as bootstrapping, permutation test, jackknife, etc. There are numerous articles & books discuss these techniques, for example Philip I Good (2010) Permutation, Parametric, and Bootstrap Tests of Hypotheses
My question is which resampling technique has gained the more popularity and easier to implement? Bootstrapping or permutation tests? 
","['hypothesis-testing', 'nonparametric', 'bootstrap', 'permutation-test']","Both are popular and useful, but primarily for different uses.  The permutation test is best for testing hypotheses and bootstrapping is best for estimating confidence intervals.Permutation tests test a specific null hypothesis of exchangeability, i.e. that only the random sampling/randomization explains the difference seen.  This is the common case for things like t-tests and ANOVA.  It can also be expanded to things like time series (null hypothesis that there is no serial correlation) or regression (null hypothesis of no relationship).  Permutation tests can be used to create confidence intervals, but it requires many more assumptions, that may or may not be reasonable (so other methods are preferred).  The Mann-Whitney/Wilcoxon test is actually a special case of a permutation test, so they are much more popular than some realize.The bootstrap estimates the variability of the sampling process and works well for estimating confidence intervals.  You can do a test of hypothesis this way but it tends to be less powerful than the permutation test for cases that the permutation test assumptions hold."
What kind of information is Fisher information?,"
Suppose we have a random variable $X \sim f(x|\theta)$. If $\theta_0$ were the true parameter, the the likelihood function should be maximized and the derivative equal to zero. This is the basic principle behind the maximum likelihood estimator. 
As I understand it, Fisher information is defined as 
$$I(\theta) = \Bbb E \Bigg[\left(\frac{\partial}{\partial \theta}f(X|\theta)\right)^2\Bigg ]$$
Thus, if $\theta_0$ is the true parameter, $I(\theta) = 0$. But if it $\theta_0$ is not the true parameter, then we will have a larger amount of Fisher information. 
my questions 

Does Fisher information measure the ""error"" of a given MLE? In other words, doesn't the existence of positive Fisher information imply my MLE can't be ideal? 
How does this definition of ""information"" differ from that used by Shannon? Why do we call it information? 

","['bayesian', 'maximum-likelihood', 'likelihood', 'intuition', 'fisher-information']","Trying to complement the other answers... What kind of information is Fisher information?  Start with the loglikelihood function
$$
   \ell (\theta) = \log f(x;\theta)
$$
as a function of $\theta$ for $\theta \in \Theta$, the parameter space.
Assuming some regularity conditions we do not discuss here, we have
$\DeclareMathOperator{\E}{\mathbb{E}}  \E \frac{\partial}{\partial \theta} \ell (\theta) = \E_\theta \dot{\ell}(\theta) = 0$ (we will write derivatives with respect to the parameter as dots as here).  The variance is the Fisher information
$$
    I(\theta) = \E_\theta ( \dot{\ell}(\theta) )^2= -\E_\theta \ddot{\ell}(\theta)
$$
the last formula showing that it is the (negative) curvature of the loglikelihood function.  One often finds the maximum likelihood estimator (mle) of $\theta$ by solving the likelihood equation $\dot{\ell}(\theta)=0$ when the Fisher information as the variance of the score $\dot{\ell}(\theta)$ is large, then the solution to that equation will be very sensitive to the data, giving a hope for high precision of the mle.  That is confirmed at least asymptotically, the asymptotic variance of the mle being the inverse of Fisher information.How can we interpret this?   $\ell(\theta)$ is the likelihood information about the parameter $\theta$ from the sample. This can really only be interpreted in a relative sense, like when we use it to compare the plausibilities of two distinct possible parameter values via the likelihood ratio test $\ell(\theta_0) - \ell(\theta_1)$.  The rate of change of the loglikelihood is the score function $\dot{\ell}(\theta)$ tells us how fast the likelihood changes, and its variance $I(\theta)$ how much this varies from sample to sample, at a given parameter value, say $\theta_0$.  The equation (which is really surprising!)
$$
    I(\theta) = - \E_\theta \ddot{\ell}(\theta)
$$
tells us there is a relationship (equality) between the variability in the information (likelihood) for a given parameter value, $\theta_0$, and the curvature of the likelihood function for that parameter value.  This is a surprising relationship between the variability (variance) of ths statistic $\dot{\ell}(\theta) \mid_{\theta=\theta_0}$ and the expected change in likelihood when we vary the parameter $\theta$ in some interval around $\theta_0$ (for the same data).   This is really both strange, surprising and powerful!So what is the likelihood function?  We usually think of the statistical model $\{ f(x;\theta), \theta \in \Theta \} $ as a family of probability distributions for data $x$, indexed by the parameter $\theta$ some element in the parameter space $\Theta$. We think of this model as being true if there exists some value $\theta_0 \in \Theta$ such that the data $x$ actually have the probability distribution $f(x;\theta_0)$. So we get a statistical model by imbedding the true data-generating probability distribution $f(x;\theta_0)$ in a family of probability distributions. But, it is clear that such an imbedding can be done in many different ways, and each such imbedding will be a ""true"" model, and they will give different likelihood functions. And, without such an imbedding, there is no likelihood function. It seems that we really do need some help, some principles for how to choose an imbedding wisely!So, what does this mean? It means that the choice of likelihood function tells us how we would expect the data to change, if the truth changed a little bit. But, this cannot really be verified by the data, as the data only gives information about the true model function $f(x;\theta_0)$ which actually generated the data, and not nothing about all the other elements in the choosen model. This way we see that choice of the likelihood function is similar to choice of a prior in Bayesian analysis, it injects non-data information into the analysis. Let us look at this in a simple (somewhat artificial) example, and look at the effect of imbedding $f(x;\theta_0)$ in a model in different ways.Let us assume that $X_1, \dotsc, X_n$ are iid as $N(\mu=10, \sigma^2=1)$. So, that is the true, data-generating distribution. Now, let us embed this in a model in two different ways, model A and model B.
$$
A \colon X_1, \dotsc, X_n ~\text{iid}~N(\mu, \sigma^2=1),\mu \in \mathbb{R} \\
B \colon X_1, \dotsc, X_n ~\text{iid}~N(\mu, \mu/10), \mu>0
$$
you can check that this coincides for $\mu=10$.The loglikelihood functions become
$$
\ell_A(\mu) = -\frac{n}{2} \log (2\pi) -\frac12\sum_i (x_i-\mu)^2 \\
\ell_B(\mu) = -\frac{n}{2} \log (2\pi) - \frac{n}{2}\log(\mu/10) - \frac{10}{2}\sum_i \frac{(x_i-\mu)^2}{\mu}
$$The score functions: (loglikelihood derivatives):
$$
\dot{\ell}_A(\mu) = n (\bar{x}-\mu)       \\
\dot{\ell}_B(\mu) = -\frac{n}{2\mu}- \frac{10}{2}\sum_i \left(\frac{x_i}{\mu}\right)^2 - 
15 n
$$
and the curvatures
$$
   \ddot{\ell}_A(\mu) = -n   \\
   \ddot{\ell}_B(\mu) =  \frac{n}{2\mu^2} + \frac{10}{2}\sum_i \frac{2 x_i^2}{\mu^3}
$$
so, the Fisher information do really depend on the imbedding. Now, we calculate the Fisher information at the true value $\mu=10$,
$$
  I_A(\mu=10) = n, \\
  I_B(\mu=10) = n \cdot \left(\frac1{200}+\frac{2020}{2000}\right) > n
$$
so the Fisher information about the parameter is somewhat larger in model B.This illustrates that, in some sense, the Fisher information tells us how fast the information from the data about the parameter would have changed if the governing parameter changed in the way postulated by the imbedding in a model family.  The explanation of higher information in model B is that our model family B postulates that if the expectation would have increased, then the variance too would have increased.  So that, under model B, the sample variance will also carry information about $\mu$, which it will not do under model A.Also, this example illustrates that we really do need some theory for helping us in how to construct model families."
"Why is there a difference between manually calculating a logistic regression 95% confidence interval, and using the confint() function in R?","
Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function confint() give different results.
I've been going through Hosmer & Lemeshow's Applied logistic regression (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:
Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(>|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4

However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:
> exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080

Hosmer & Lemeshow suggest the following formula:
$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
and they calculate the confidence interval for as.factor(dataset$dich.age)1 to be (2.9, 22.9).
This seems straightforward to do in R:
# upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])

gives the same answer as the book.
However, any thoughts on why confint() seems to give different results?  I've seen lots of examples of people using confint().
","['r', 'regression', 'logistic', 'confidence-interval', 'profile-likelihood']","After having fetched the data from the accompanying website, here is how I would do it:The 95% CIs based on profile likelihood are obtained withThis often is the default if the MASS package is automatically loaded. In this case, I get Now, if I wanted to compare with 95% Wald CIs (based on asymptotic normality) like the one you computed by hand, I would use confint.default() instead; this yieldsWald CIs are good in most situations, although profile likelihood-based may be useful with complex sampling strategies. If you want to grasp the idea of how they work, here is a brief overview of the main principles: Confidence intervals by the profile likelihood method, with applications in veterinary epidemiology. You can also take a look at Venables and Ripley's MASS book, §8.4, pp. 220-221."
How to perform a test using R to see if data follows normal distribution,"
I have a data set with following structure:
a word | number of occurrence of a word in a document | a document id 

How can I perform a test for normal distribution in R? Probably it is an easy question but I am a R newbie.
","['r', 'distributions', 'normality-assumption']","If I understand your question correctly, then to test if word occurrences in a set of documents follows a Normal distribution you can just use a shapiro-Wilk test and some qqplots. For example,The qqplot commands give:
You can see that the second data set is clearly not Normal by the heavy tails (More Info). In the Shapiro-Walk normality test, the p-value is large for the first data set (>.9) but very small for the second data set (<.01). This will lead you to reject the null hypothesis for the second."
What is Deviance? (specifically in CART/rpart),"
What is ""Deviance,"" how is it calculated, and what are its uses in different fields in statistics?
In particular, I'm personally interested in its uses in CART (and its implementation in rpart in R). 
I'm asking this since the wiki-article seems somewhat lacking and your insights will be most welcomed.
","['r', 'cart', 'rpart', 'deviance']","Deviance and GLMFormally, one can view deviance as a sort of distance between two probabilistic models; in GLM context, it amounts to two times the log ratio of likelihoods between two nested models $\ell_1/\ell_0$ where $\ell_0$ is the ""smaller"" model; that is, a linear restriction on model parameters (cf. the Neyman–Pearson lemma), as @suncoolsu said. As such, it can be used to perform model comparison. It can also be seen as a generalization of the RSS used in OLS estimation (ANOVA, regression), for it provides a measure of goodness-of-fit of the model being evaluated when compared to the null model (intercept only). It works with LM too:The residuals SS (RSS) is computed as $\hat\varepsilon^t\hat\varepsilon$, which is readily obtained as:or from the (unadjusted) $R^2$ since $R^2=1-\text{RSS}/\text{TSS}$ where $\text{TSS}$ is the total variance. Note that it is directly available in an ANOVA table, likeNow, look at the deviance:In fact, for linear models the deviance equals the RSS (you may recall that OLS and ML estimates coincide in such a case).Deviance and CARTWe can see CART as a way to allocate already $n$ labeled individuals into arbitrary classes (in a classification context). Trees can be viewed as providing a probability model for individuals class membership. So, at each node $i$, we have a probability distribution $p_{ik}$ over the classes. What is important here is that the leaves of the tree give us a random sample $n_{ik}$ from a multinomial distribution specified by $p_{ik}$. We can thus define the deviance of a tree, $D$, as the sum over all leaves of$$D_i=-2\sum_kn_{ik}\log(p_{ik}),$$following Venables and Ripley's notations (MASS, Springer 2002, 4th ed.). If you have access to this essential reference for R users (IMHO), you can check by yourself how such an approach is used for splitting nodes and fitting a tree to observed data (p. 255 ff.); basically, the idea is to minimize, by pruning the tree, $D+\alpha \#(T)$ where $\#(T)$ is the number of nodes in the tree $T$. Here we recognize the cost-complexity trade-off. Here, $D$ is equivalent to the concept of node impurity (i.e., the heterogeneity of the distribution at a given node) which are based on a measure of entropy or information gain, or the well-known Gini index, defined as $1-\sum_kp_{ik}^2$ (the unknown proportions are estimated from node proportions).With a regression tree, the idea is quite similar, and we can conceptualize the deviance as sum of squares defined for individuals $j$ by$$D_i=\sum_j(y_j-\mu_i)^2,$$summed over all leaves. Here, the probability model that is considered within each leaf is a gaussian $\mathcal{N}(\mu_i,\sigma^2)$. Quoting Venables and Ripley (p. 256), ""$D$ is the usual scaled deviance for a gaussian GLM. However, the distribution at internal nodes of the tree is then a mixture of normal distributions, and so $D_i$ is only appropriate at the leaves. The tree-construction process has to be seen as a hierarchical refinement of probability models, very similar to forward variable selection in regression."" Section 9.2 provides further detailed information about rpart implementation, but you can already look at the residuals() function for rpart object, where ""deviance residuals"" are computed as the square root of minus twice the logarithm of the fitted model.An introduction to recursive partitioning using the rpart routines, by Atkinson and Therneau, is also a good start. For more general review (including bagging), I would recommend"
Rank in R - descending order [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it's on-topic for Cross Validated.


Closed 10 years ago.







                        Improve this question
                    



I am looking to rank data that, in some cases, the larger value has the rank of 1.  I am relatively new to R, but I don't see how I can adjust this setting in the rank function.
x <- c(23,45,12,67,34,89)
rank(x)

generates:
[1] 2 4 1 5 3 6

when I want it to be:
[1] 5 3 6 2 4 1

I assume this is very basic, but any  help you can provide will be very much appreciated.
",['r'],
Eliciting priors from experts,"
How should I elicit prior distributions from experts when fitting a Bayesian model?
","['bayesian', 'prior', 'elicitation']","John Cook gives some interesting recommendations. Basically, get percentiles/quantiles (not means or obscure scale parameters!) from the experts, and fit them with the appropriate distribution.http://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/"
Approximate order statistics for normal random variables,"
Are there well known formulas for the order statistics of certain random distributions? Particularly the first and last order statistics of a normal 
random variable, but a more general answer would also be appreciated.
Edit: To clarify, I am looking for approximating formulas that can be more-or-less explicitly evaluated, not the exact integral expression.
For example, I have seen the following two approximations for the first order statistic (ie the minimum) of a normal rv:
$e_{1:n} \geq \mu - \frac{n-1}{\sqrt{2n-1}}\sigma$
and
$e_{1:n} \approx \mu + \Phi^{-1} \left( \frac{1}{n+1} \right)\sigma$
The first of these, for $n=200$, gives approximately $e_{1:200} \geq \mu - 10\sigma$ which seems like a wildly loose bound.
The second gives $e_{1:200} \approx \mu - 2.58\sigma$ whereas a quick Monte Carlo  gives $e_{1:200} \approx \mu - 2.75\sigma$, so it's not a bad approximation but not great either, and more importantly I don't have any intuition about where it comes from.
Any help?
","['distributions', 'normal-distribution', 'approximation', 'order-statistics']","The classic reference is Royston (1982)[1] which has algorithms going beyond explicit formulas. It also quotes a well-known formula by Blom (1958):
$E(r:n) \approx \mu + \Phi^{-1}(\frac{r-\alpha}{n-2\alpha+1})\sigma$ with $\alpha=0.375$. This formula gives a multiplier of -2.73 for $n=200, r=1$.[1]: Algorithm AS 177: Expected Normal Order Statistics (Exact and Approximate) J. P. Royston. Journal of the Royal Statistical Society. Series C (Applied Statistics) Vol. 31, No. 2 (1982), pp. 161-165"
What is a good resource on table design?,"
I've seen various theoretical treatments of graphics, such as the Grammar of Graphics. But I have seen nothing equivalent with regards to tables. Over the while I have developed an informal model of good practice in table design. 
However, I'd like to be able to provide a good reference to students.
The APA Style Manual has a few tips on table design, but it is only a starting point.
Question:
What is a good resource that provides theoretical and practical advice on the presentation of numeric results in tables?
UPDATE: It would be particularly useful to have a good free online resource.
Note: I'm not sure if this should be community wiki. I feel as if there might be a correct answer.
",['tables'],"Ed Tufte has a few pages on this in his classic ""The Visual Display of Quantitative Information"".For a much more detailed treatment, there is Jane Miller's Chicago Guide to Writing about Numbers. I've never seen anything else like it. It has a whole chapter on ""Creating Effective Tables""."
Deriving Bellman's Equation in Reinforcement Learning,"
I see the following equation in ""In Reinforcement Learning. An Introduction"", but don't quite follow the step I have highlighted in blue below. How exactly is this step derived?

","['expected-value', 'reinforcement-learning']","There are already a great many answers to this question, but most involve few words describing what is going on in the manipulations.  I'm going to answer it using way more words, I think.  To start,$$G_{t} \doteq \sum_{k=t+1}^{T} \gamma^{k-t-1} R_{k}$$is defined in equation 3.11 of Sutton and Barto, with a constant discount factor $0 \leq \gamma \leq 1$ and we can have $T = \infty$ or $\gamma = 1$, but not both.  Since the rewards, $R_{k}$, are random variables, so is $G_{t}$ as it is merely a linear combination of random variables.$$\begin{align}
  v_\pi(s) & \doteq \mathbb{E}_\pi\left[G_t \mid S_t = s\right] \\
  & = \mathbb{E}_\pi\left[R_{t+1} + \gamma G_{t+1} \mid S_t = s\right] \\
  & = \mathbb{E}_{\pi}\left[ R_{t+1} | S_t = s \right] + \gamma \mathbb{E}_{\pi}\left[ G_{t+1} | S_t = s \right]
\end{align}$$That last line follows from the linearity of expectation values.  $R_{t+1}$ is the reward the agent gains after taking action at time step $t$.  For simplicity, I assume that it can take on a finite number of values $r \in \mathcal{R}$.Work on the first term.  In words, I need to compute the expectation values of $R_{t+1}$ given that we know that the current state is $s$.  The formula for this is$$\begin{align}
\mathbb{E}_{\pi}\left[ R_{t+1} | S_t = s \right] = \sum_{r \in \mathcal{R}} r p(r|s).
\end{align}$$In other words the probability of the appearance of reward $r$ is conditioned on the state $s$; different states may have different rewards.  This $p(r|s)$ distribution is a marginal distribution of a distribution that also contained the variables $a$ and $s'$, the action taken at time $t$ and the state at time $t+1$ after the action, respectively:$$\begin{align}
p(r|s) = \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} p(s',a,r|s) =  \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} \pi(a|s) p(s',r | a,s).
\end{align}$$Where I have used $\pi(a|s) \doteq p(a|s)$, following the book's convention.  If that last equality is confusing: forget the sums, suppress the $s$ (the probability now looks like a joint probability), use the law of multiplication, and finally reintroduce the condition on $s$ in all the new terms.A short proof for the same is below.
$$\begin{align}
p(s',r,a|s)=p(s',r|a,s)p(a|s)=p(s',r|a,s)\pi(a|s)
\end{align}$$It is now easy to see that the first term is$$\begin{align}
\mathbb{E}_{\pi}\left[ R_{t+1} | S_t = s \right] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} r \pi(a|s) p(s',r | a,s),
\end{align}$$as required.  On to the second term, where I assume that $G_{t+1}$ is a random variable that takes on a finite number of values $g \in \Gamma$.  Just like the first term:$$\begin{align}
\mathbb{E}_{\pi}\left[ G_{t+1} | S_t = s \right] = \sum_{g \in \Gamma} g p(g|s).  \qquad\qquad\qquad\qquad  (*)
\end{align}$$Once again, I ""un-marginalize"" the probability distribution by writing (law of multiplication again)$$\begin{align}
p(g|s) & = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} p(s',r,a,g|s) = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} p(g | s', r, a, s) p(s', r, a | s) \\
& =  \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} p(g | s', r, a, s) p(s', r | a, s) \pi(a | s) \\
& =  \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} p(g | s', r, a, s) p(s', r | a, s) \pi(a | s) \\
& = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} p(g | s') p(s', r | a, s) \pi(a | s) \qquad\qquad\qquad\qquad (**)
\end{align}$$The last line in there follows from the Markovian property.  Remember that $G_{t+1}$ is the sum of all the future (discounted) rewards that the agent receives after state $s'$.  The Markovian property is that the process is memory-less with regards to previous states, actions and rewards.  Future actions (and the rewards they reap) depend only on the state in which the action is taken, so $p(g | s', r, a, s) = p(g | s')$, by assumption.  Ok, so the second term in the proof is now$$\begin{align}
\gamma \mathbb{E}_{\pi}\left[ G_{t+1} | S_t = s \right] & = \gamma \sum_{g \in \Gamma} \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} g p(g | s') p(s', r | a, s) \pi(a | s) \\
& =  \gamma \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} \mathbb{E}_{\pi}\left[ G_{t+1} | S_{t+1} = s' \right] p(s', r | a, s) \pi(a | s) \\
& = \gamma \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} \sum_{a \in \mathcal{A}} v_{\pi}(s') p(s', r | a, s) \pi(a | s)
\end{align}$$as required, once again.    Combining the two terms completes the proof$$\begin{align}
  v_\pi(s) & \doteq \mathbb{E}_\pi\left[G_t \mid S_t = s\right] \\
  & = \sum_{a \in \mathcal{A}} \pi(a | s)  \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} p(s', r | a, s) \left[ r + \gamma v_{\pi}(s') \right].
\end{align}$$UPDATEI want to address what might look like a sleight of hand in the derivation of the second term.  In the equation marked with $(*)$, I use a term $p(g|s)$ and then later in the equation marked $(**)$ I claim that $g$ doesn't depend on $s$, by arguing the Markovian property.  So, you might say that if this is the case, then $p(g|s) = p(g)$.  But this is not true.  I can take $p(g | s', r, a, s) \rightarrow p(g | s')$ because the probability on the left side of that statement says that this is the probability of $g$ conditioned on $s'$, $a$, $r$, and $s$.  Because we either know or assume the state $s'$, none of the other conditionals matter, because of the Markovian property.  If you do not know or assume the state $s'$, then the future rewards (the meaning of $g$) will depend on which state you begin at, because that will determine (based on the policy) which state $s'$ you start at when computing $g$.If that argument doesn't convince you, try to compute what $p(g)$ is:$$\begin{align}
p(g) & = \sum_{s' \in \mathcal{S}} p(g, s') =  \sum_{s' \in \mathcal{S}} p(g | s') p(s') \\
& = \sum_{s' \in \mathcal{S}} p(g | s') \sum_{s,a,r} p(s', a, r, s) \\
& = \sum_{s' \in \mathcal{S}} p(g | s') \sum_{s,a,r} p(s', r | a, s) p(a, s) \\
& = \sum_{s \in \mathcal{S}} p(s) \sum_{s' \in \mathcal{S}} p(g | s') \sum_{a,r} p(s', r | a, s) \pi(a | s) \\
& \doteq \sum_{s \in \mathcal{S}} p(s) p(g|s) = \sum_{s \in \mathcal{S}} p(g,s) = p(g).
\end{align}$$As can be seen in the last line, it is not true that $p(g|s) = p(g)$.  The expected value of $g$ depends on which state you start in (i.e. the identity of $s$), if you do not know or assume the state $s'$."
How does rectilinear activation function solve the vanishing gradient problem in neural networks?,"
I found rectified linear unit (ReLU) praised at several places as a solution to the vanishing gradient problem for neural networks. That is, one uses max(0,x) as activation function. When the activation is positive, it is obvious that this is better than, say, the sigmoid activation function, since its derivation is always 1 instead of an arbitrarily small value for large x. On the other hand, the derivation is exactly 0 when x is smaller than 0. In the worst case, when a unit is never activated, the weights for this unit would also never change anymore, and the unit would be forever useless - which seems much worse than even vanishingly small gradients. How do learning algorithms deal with that problem when they use ReLU?
","['machine-learning', 'neural-networks', 'deep-learning', 'gradient-descent']",
Hierarchical clustering with mixed type data - what distance/similarity to use?,"
In my dataset we have both continuous and naturally discrete variables. I want to know whether we can do hierarchical clustering using both type of variables. And if yes, what distance measure is appropriate?
","['clustering', 'similarities', 'distance-functions', 'mixed-type-data', 'gower-similarity']","One way is to use Gower similarity coefficient which is a composite measure$^1$; it takes quantitative (such as rating scale), binary (such as present/absent) and nominal (such as worker/teacher/clerk) variables. Later Podani$^2$ added an option to take ordinal variables as well.The coefficient is easily understood even without a formula; you compute the similarity value between the individuals by each variable, taking the type of the variable into account, and then average across all the variables. Usually, a program calculating Gower will allow you to weight variables, that is, their contribution, to the composite formula. However, proper weighting of variables of different type is a problem, no clear-cut guidelines exist, which makes Gower or other ""composite"" indices of proximity pull ones face.The facets of Gower similarity ($GS$):(It is easy to extend the list of types. For example, one could add a summand for count variables, using normalized chi-squared distance converted to similarity.)The coefficient ranges between 0 and 1.""Gower distance"". Without ordinal variables present (i.e. w/o using the Podani's option) $\sqrt{1-GS}$ behaves as Euclidean distance, it fully supports euclidean space. But $1-GS$ is only metric (supports triangular inequality), not Euclidean. With ordinal variables present (using the Podani's option) $\sqrt{1-GS}$ is only metric, not Euclidean; and $1-GS$ isn't metric at all. See also.With euclidean distances (distances supporting Euclidean space), virtually any classic clustering technique will do. Including K-means (if your K-means program can process distance matrices, of course) and including Ward's, centroid, median methods of Hierarchical clustering. Using K-means or other those methods based on Euclidean distance with non-euclidean still metric distance is heuristically admissible, perhaps. With non-metric distances, no such methods may be used.The previous paragraph talks about if K-means or Ward's or such clustering is legal or not with Gower distance mathematically (geometrically). From the measurement-scale (""psychometric"") point of view one should not compute mean or euclidean-distance deviation from it in any categorical (nominal, binary, as well as ordinal) data; therefore from this stance you just may not process Gower coefficient by K-means, Ward etc. This viewpoint warns that even if a Euclidean space is present it may be granulated, not smooth (see related).If you want all the formulae and additional info on Gower similarity / distance, please read the description of my SPSS macro !KO_gower; it's in the Word document found in collection ""Various proximities"" on my web-page.$^1$ Gower J. C. A general coefficient of similarity and some of its properties // Biometrics, 1971, 27, 857-872$^2$ Podani, J. Extending Gower’s general coefficient of similarity to ordinal characters // Taxon, 1999, 48, 331-340"
What is difference between “in-sample” and “out-of-sample” forecasts?,"
I don't understand what exactly is the difference between ""in-sample"" and ""out of sample"" prediction?
An in-sample forecast utilizes a subset of the available data to forecast values outside of the estimation period. An out of sample forecast instead uses all available data.
Are these correct?
Very specifically is the following definition correct?
A within sample forecast utilizes a subset of the available data to forecast values outside of the estimation period and compare them to the corresponding known or actual outcomes. This is done to assess the ability of the model to forecast known values. For example, a within sample forecast from 1980 to 2015 might use data from 1980 to 2012 to estimate the model. Using this model, the forecaster would then predict values for 2013-2015 and compare the forecasted values to the actual known values.
An out of sample forecast instead uses all available data in the sample to estimate a models. For the previous example, estimation would be performed over 1980-2015, and the forecast(s) would commence in 2016.
",['forecasting'],"By the ""sample"" it is meant the data sample that you are using to fit the model.First - you have a sample
Second - you fit a model on the sample
Third - you can use the model for forecasting  If you are forecasting for an observation that was part of the data sample - it is in-sample forecast.If you are forecasting for an observation that was not part of the data sample - it is out-of-sample forecast.So the question you have to ask yourself is: Was the particular observation used for the model fitting or not ? If it was used for the model fitting, then the forecast of the observation is in-sample. Otherwise it is out-of-sample.if you use data 1990-2013 to fit the model and then you forecast for
  2011-2013, it's in-sample forecast. but if you only use 1990-2010 for
  fitting the model and then you forecast 2011-2013, then its
  out-of-sample forecast."
Why would someone use a Bayesian approach with a 'noninformative' improper prior instead of the classical approach?,"
If the interest is merely estimating the parameters of a model (pointwise and/or interval estimation) and the prior information is not reliable, weak, (I know this is a bit vague but I am trying to establish an scenario where the choice of a prior is difficult) ... Why would someone choose to use the Bayesian approach with 'noninformative' improper priors instead of the classical approach?
","['bayesian', 'inference', 'prior', 'likelihood', 'information-theory']",Two reasons one may go with a Bayesian approach even if you're using highly non-informative priors:
Why does inversion of a covariance matrix yield partial correlations between random variables?,"
I heard that partial correlations between random variables can be found by inverting the covariance matrix and taking appropriate cells from such resulting precision matrix (this fact is mentioned in http://en.wikipedia.org/wiki/Partial_correlation, but without a proof).
Why is this the case?
","['covariance', 'covariance-matrix', 'linear-algebra', 'partial-correlation', 'matrix-inverse']","When a multivariate random variable $(X_1,X_2,\ldots,X_n)$ has a nondegenerate covariance matrix $\mathbb{C} = (\gamma_{ij}) = (\text{Cov}(X_i,X_j))$, the set of all real linear combinations of the $X_i$ forms an $n$-dimensional real vector space with basis $E=(X_1,X_2,\ldots, X_n)$ and a non-degenerate inner product given by$$\langle X_i,X_j \rangle = \gamma_{ij}\ .$$Its dual basis with respect to this inner product, $E^{*} = (X_1^{*},X_2^{*}, \ldots, X_n^{*})$, is uniquely defined by the relationships$$\langle X_i^{*}, X_j \rangle = \delta_{ij}\ ,$$the Kronecker delta (equal to $1$ when $i=j$ and $0$ otherwise).The dual basis is of interest here because the partial correlation of $X_i$ and $X_j$ is obtained as the correlation between the part of $X_i$ that is left after projecting it into the space spanned by all the other vectors (let's simply call it its ""residual"", $X_{i\circ}$) and the comparable part of $X_j$, its residual $X_{j\circ}$. Yet $X_i^{*}$ is a vector that is orthogonal to all vectors besides $X_i$ and has positive inner product with $X_i$ whence $X_{i\circ}$ must be some non-negative multiple of $X_i^{*}$, and likewise for $X_j$.  Let us therefore write$$X_{i\circ} = \lambda_i X_i^{*},\ X_{j\circ} = \lambda_j X_j^{*}$$for positive real numbers $\lambda_i$ and $\lambda_j$.The partial correlation is the normalized dot product of the residuals, which is unchanged by rescaling:$$\rho_{ij\circ} = \frac{\langle X_{i\circ}, X_{j\circ} \rangle}{\sqrt{\langle X_{i\circ}, X_{i\circ} \rangle\langle X_{j\circ}, X_{j\circ} \rangle}} =  \frac{\lambda_i\lambda_j\langle X_{i}^{*}, X_{j}^{*} \rangle}{\sqrt{\lambda_i^2\langle X_{i}^{*}, X_{i}^{*} \rangle\lambda_j^2\langle X_{j}^{*}, X_{j}^{*} \rangle}} = \frac{\langle X_{i}^{*}, X_{j}^{*} \rangle}{\sqrt{\langle X_{i}^{*}, X_{i}^{*} \rangle\langle X_{j}^{*}, X_{j}^{*} \rangle}}\ .$$(In either case the partial correlation will be zero whenever the residuals are orthogonal, whether or not they are nonzero.)We need to find the inner products of dual basis elements.  To this end, expand the dual basis elements in terms of the original basis $E$:$$X_i^{*} = \sum_{j=1}^n \beta_{ij} X_j\ .$$Then by definition$$\delta_{ik} = \langle X_i^{*}, X_k \rangle = \sum_{j=1}^n \beta_{ij}\langle X_j, X_k \rangle = \sum_{j=1}^n \beta_{ij}\gamma_{jk}\ .$$In matrix notation with $\mathbb{I} = (\delta_{ij})$ the identity matrix and $\mathbb{B} = (\beta_{ij})$ the change-of-basis matrix, this states$$\mathbb{I} = \mathbb{BC}\ .$$That is, $\mathbb{B} = \mathbb{C}^{-1}$, which is exactly what the Wikipedia article is asserting.  The previous formula for the partial correlation gives$$\rho_{ij\cdot} =  \frac{\beta_{ij}}{\sqrt{\beta_{ii} \beta_{jj}}} = \frac{\mathbb{C}^{-1}_{ij}}{\sqrt{\mathbb{C}^{-1}_{ii} \mathbb{C}^{-1}_{jj}}}\ .$$"
How can I test if given samples are taken from a Poisson distribution?,"
I know of normality tests, but how do I test for ""Poisson-ness""?
I have sample of  ~1000 non-negative integers, which I suspect are taken from a Poisson distribution, and I would like to test that.
","['hypothesis-testing', 'distributions', 'poisson-distribution', 'goodness-of-fit']","First of all my advice is you must refrain from trying out a Poisson distribution just as it is to the data. I suggest you must first make a theory as to why should Poisson distribution fit a particular dataset or a phenomenon.Once you have established this, the next question is whether the distribution is homogeneous or not. This means whether all parts of the data are handled by the same poisson distribution or is there a variation in this based on some aspect like time or space. Once you have convinced of these aspects, try the following three tests:search for these and you will find them easily on the net. "
Regression: Transforming Variables,"
When transforming variables, do you have to use all of the same transformation? For example, can I pick and choose differently transformed variables, as in: 
Let, $x_1,x_2,x_3$ be age, length of employment, length of residence, and income.
Y = B1*sqrt(x1) + B2*-1/(x2) + B3*log(x3)

Or, must you be consistent with your transforms and use all of the same? As in: 
Y = B1*log(x1) + B2*log(x2) + B3*log(x3) 

My understanding is that the goal of transformation is to address the problem of normality. Looking at histograms of each variable we can see that they present very different distributions, which would lead me to believe that the transformations required are different on a variable by variable basis. 
## R Code
df <- read.spss(file=""http://www.bertelsen.ca/R/logistic-regression.sav"", 
                use.value.labels=T, to.data.frame=T)
hist(df[1:7]) 


Lastly, how valid is it to transform variables using $\log(x_n + 1)$ where $x_n$ has $0$ values? Does this transform need to be consistent across all variables or is it used adhoc even for those variables which do not include $0$'s?
## R Code 
plot(df[1:7])


","['r', 'regression', 'logistic', 'data-transformation']","One transforms the dependent variable to achieve approximate symmetry and homoscedasticity of the residuals.  Transformations of the independent variables have a different purpose: after all, in this regression all the independent values are taken as fixed, not random, so ""normality"" is inapplicable.  The main objective in these transformations is to achieve linear relationships with the dependent variable (or, really, with its logit).  (This objective over-rides auxiliary ones such as reducing excess leverage or achieving a simple interpretation of the coefficients.)  These relationships are a property of the data and the phenomena that produced them, so you need the flexibility to choose appropriate re-expressions of each of the variables separately from the others.  Specifically, not only is it not a problem to use a log, a root, and a reciprocal, it's rather common.  The principle is that there is (usually) nothing special about how the data are originally expressed, so you should let the data suggest re-expressions that lead to effective, accurate, useful, and (if possible) theoretically justified models.The histograms--which reflect the univariate distributions--often hint at an initial transformation, but are not dispositive.  Accompany them with scatterplot matrices so you can examine the relationships among all the variables.Transformations like $\log(x + c)$ where $c$ is a positive constant ""start value"" can work--and can be indicated even when no value of $x$ is zero--but sometimes they destroy linear relationships.  When this occurs, a good solution is to create two variables.  One of them equals $\log(x)$ when $x$ is nonzero and otherwise is anything; it's convenient to let it default to zero.  The other, let's call it $z_x$, is an indicator of whether $x$ is zero: it equals 1 when $x = 0$ and is 0 otherwise.  These terms contribute a sum$$\beta \log(x) + \beta_0 z_x$$to the estimate.  When $x \gt 0$, $z_x = 0$ so the second term drops out leaving just $\beta \log(x)$.  When $x = 0$, ""$\log(x)$"" has been set to zero while $z_x = 1$, leaving just the value $\beta_0$.  Thus, $\beta_0$ estimates the effect when $x = 0$ and otherwise $\beta$ is the coefficient of $\log(x)$."
Cumming (2008) claims that distribution of p-values obtained in replications depends only on the original p-value. How can it be true?,"
I have been reading Geoff Cumming's 2008 paper Replication and $p$ Intervals: $p$ values predict the future only vaguely, but confidence intervals do much better [~200 citations in Google Scholar] -- and  am confused by one of its central claims. This is one in the series of papers where Cumming argues against $p$-values and in favour of confidence intervals; my question, however, is not about this debate and only concerns one specific claim about $p$-values. 
Let me quote from the abstract:

This article shows that, if an initial
  experiment results in two-tailed $p= .05$, there is an $80\%$
  chance the one-tailed $p$-value from a replication will fall in
  the interval $(.00008, .44)$, a $10\%$ chance that $p < .00008$,
  and fully a $10\%$ chance that $p > .44$. Remarkably, the
  interval—termed a $p$ interval—is this wide however large
  the sample size.

Cumming claims that this ""$p$ interval"", and in fact the whole distribution of $p$-values that one would obtain when replicating the original experiment (with the same fixed sample size), depend only on the original $p$-value $p_\mathrm{obt}$ and do not depend on the true effect size, power, sample size, or anything else:

[...] the probability distribution of $p$ can be derived without
  knowing or assuming a value for $\delta$ (or power). [...] We do not assume any prior knowledge
  about $\delta$, and we use only the information $M_\mathrm{diff}$ [observed between-group difference] gives about $\delta$ as
  the basis for the calculation for a given $p_\mathrm{obt}$ of the distribution of
  $p$ and of $p$ intervals.
$\quad\quad\quad$ 

I am confused by this because to me it seems that the distribution of $p$-values strongly depends on power, whereas the original $p_\mathrm{obt}$ on its own does not give any information about it. It might be that the true effect size is $\delta=0$ and then the distribution is uniform; or maybe the true effect size is huge and then we should expect mostly very small $p$-values. Of course one can start with assuming some prior over possible effect sizes and integrate over it, but Cumming seems to claim that this is not what he is doing.
Question: What exactly is going on here?

Note that this topic is related to this question: What fraction of repeat experiments will have an effect size within the 95% confidence interval of the first experiment? with an excellent answer by @whuber. Cumming has a paper on this topic to: Cumming & Maillardet, 2006, Confidence Intervals and Replication: Where Will the Next Mean Fall? -- but that one is clear and unproblematic.
I also note that Cumming's claim is repeated several times in the 2015 Nature Methods paper The fickle $P$ value generates irreproducible results that some of you might have come across (it already has ~100 citations in Google Scholar):

[...] there will be substantial variation in the $P$ value of repeated experiments. In reality, experiments are rarely repeated; we do not know how different the next $P$ might be. But it is likely that it could be very different. For example, regardless of the statistical power of an experiment, if a single replicate returns a $P$ value of $0.05$, there is an $80\%$ chance that a repeat experiment would return a $P$ value between $0$ and $0.44$ (and a $20\%$ change [sic] that $P$ would be even larger).

(Note, by the way, how, irrespective of whether Cumming's statement is correct or not, Nature Methods paper quotes it inaccurately: according to Cumming, it's only $10\%$ probability above $0.44$. And yes, the paper does say ""20% change"". Pfff.)
","['hypothesis-testing', 'p-value', 'statistical-power', 'replicability']","Summary: The trick appears to be a Bayesian approach which assumes a uniform (Jeffreys) prior for the hidden parameter ($z_\mu$ in appendix B of the paper, $\theta$ here).I believe there may be a Bayesian-style approach to get the equations given in the paper's appendix B.As I understand it, the experiment boils down to a statistic $z\sim\mathrm{N}_{\theta,1}$. The mean $\theta$ of the sampling distribution is unknown, but vanishes under the null hypothesis, $\theta\mid{}H_0=0$.Call the experimentally observed statistic $\hat{z}\mid\theta\sim\mathrm{N}_{\theta,1}$. Then if we assume a ""uniform"" (improper) prior on $\theta\sim1$, the Bayesian posterior is $\theta\mid\hat{z}\sim\mathrm{N}_{\hat{z},1}$. If we then update the original sampling distribution by marginalizing over $\theta\mid\hat{z}$, the posterior becomes $z\mid\hat{z}\sim\mathrm{N}_{\hat{z},2}$. (The doubled variance is due to convolution of Gaussians.)Mathematically at least, this seems to work. And it explains how the $\frac{1}{\sqrt{2}}$ factor ""magically"" appears going from equation B2 to equation B3.DiscussionHow can this result be reconciled with the standard null hypothesis testing framework? One possible interpretation is as follows.In the standard framework, the null hypothesis is in some sense the ""default"" (e.g. we speak of ""rejecting the null""). In the above Bayesian context this would be a non-uniform prior that prefers $\theta=0$. If we take this to be $\theta\sim\mathrm{N}_{0,\lambda^2}$, then the variance $\lambda^2$ represents our prior uncertainty.Carrying this prior through the analysis above, we find
$$\theta\sim\mathrm{N}_{0,\lambda^2} \implies \theta\mid\hat{z}\sim\mathrm{N}_{\delta^2\hat{z},\delta^2} \,,\, z\mid\hat{z}\sim\mathrm{N}_{\delta^2\hat{z},1+\delta^2} \,,\, \delta^2\equiv\tfrac{1}{1+\lambda^{-2}}\in[0,1]$$
From this we can see that in the limit $\lambda\to\infty$ we recover the analysis above. But in the limit $\lambda\to{0}$ our ""posteriors"" become the null, $\theta\mid\hat{z}\sim\mathrm{N}_{0,0}$ and $z\mid\hat{z}\sim\mathrm{N}_{0,1}$, so we recover the standard result, ${p}\mid{\hat{z}}\sim\mathrm{U}_{0,1}$.(For repeated studies, the above suggests an interesting question here about the implications for Bayesian updating vs. ""traditional"" methods for meta-analysis. I am completely ignorant on the subject of meta-analysis though!)AppendixAs requested in the comments, here is a plot for comparison. This is a relatively straightforward application of the formulas in the paper. However I will write these out to ensure no ambiguity.Let $p$ denote the one-sided p value for the statistic $z$, and denote its (posterior) CDF by $F[u]\equiv\Pr\big[\,p\leq{u}\mid{\hat{z}}\,\big]$. Then equation B3 from the appendix is equivalent to
$$F[p]=1-\Phi\left[\tfrac{1}{\sqrt{2}}\left(z[p]-\hat{z}\right)\right]
\,,\, z[p]=\Phi^{-1}[1-p]$$
where $\Phi[\,\,]$ is the standard normal CDF. The corresponding density is then
$$f\big[p\big]\equiv{F^\prime}\big[p\big]=\frac{\phi\Big[(z-\hat{z})/\sqrt{2}\,\Big]}{\sqrt{2}\,\phi\big[z\big]}$$
where $\phi[\,\,]$ is the standard normal PDF, and $z=z[p]$ as in the CDF formula.
Finally, if we denote by $\hat{p}$ the observed two-sided p value corresponding to $\hat{z}$, then we have
$$\hat{z}=\Phi^{-1}\Big[1-\tfrac{\hat{p}}{2}\Big]$$Using these equations gives the figure below, which should be comparable to the paper's figure 5 quoted in the question.
(This was produced by the following Matlab code; run here.)"
how to weight KLD loss vs reconstruction loss in variational auto-encoder,"
in nearly all code examples I've seen of a VAE, the loss functions are defined as follows (this is tensorflow code, but I've seen similar for theano, torch etc. It's also for a convnet, but that's also not too relevant, just affects the axes the sums are taken over):
# latent space loss. KL divergence between latent space distribution and unit gaussian, for each batch.
# first half of eq 10. in https://arxiv.org/abs/1312.6114
kl_loss = -0.5 * tf.reduce_sum(1 + log_sigma_sq - tf.square(mu) - tf.exp(log_sigma_sq), axis=1)

# reconstruction error, using pixel-wise L2 loss, for each batch
rec_loss = tf.reduce_sum(tf.squared_difference(y, x), axis=[1,2,3])

# or binary cross entropy (assuming 0...1 values)
y = tf.clip_by_value(y, 1e-8, 1-1e-8) # prevent nan on log(0)
rec_loss = -tf.reduce_sum(x * tf.log(y) + (1-x) * tf.log(1-y), axis=[1,2,3])

# sum the two and average over batches
loss = tf.reduce_mean(kl_loss + rec_loss)

However the numeric range of kl_loss and rec_loss are very dependent on latent space dims and input feature size (e.g. pixel resolution) respectively. Would it be sensible to replace the reduce_sum's with reduce_mean to get per z-dim KLD and per pixel (or feature) LSE or BCE? More importantly, how do we weight latent loss with reconstruction loss when summing together for the final loss? Is it just trial and error? or is there some theory (or at least rule of thumb) for it? I couldn't find any info on this anywhere (including the original paper). 

The issue I'm having, is that if the balance between my input feature (x) dimensions and latent space (z) dimensions is not 'optimum', either my reconstructions are very good but the learnt latent space is unstructured (if x dimensions is very high and reconstruction error dominates over KLD), or vice versa (reconstructions are not good but learnt latent space is well structured if KLD dominates).
I'm finding myself having to normalise reconstruction loss (dividing by input feature size), and KLD (dividing by z dimensions) and then manually weighting the KLD term with an arbitrary weight factor (The normalisation is so that I can use the same or similar weight independent of dimensions of x or z). Empirically I've found around 0.1 to provide a good balance between reconstruction and structured latent space which feels like a 'sweet spot' to me. I'm looking for prior work in this area.

Upon request, maths notation of above (focusing on L2 loss for reconstruction error)
$$\mathcal{L}_{latent}^{(i)} = -\frac{1}{2} \sum_{j=1}^{J}(1+\log (\sigma_j^{(i)})^2 - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2)$$
$$\mathcal{L}_{recon}^{(i)} = -\sum_{k=1}^{K}(y_k^{(i)}-x_k^{(i)})^2$$
$$\mathcal{L}^{(m)} = \frac{1}{M}\sum_{i=1}^{M}(\mathcal{L}_{latent}^{(i)} + \mathcal{L}_{recon}^{(i)})$$
where $J$ is the dimensionality of latent vector $z$ (and corresponding mean $\mu$ and variance $\sigma^2$), $K$ is the dimensionality of the input features, $M$ is the mini-batch size, the superscript $(i)$ denotes the $i$th data point and $\mathcal{L}^{(m)}$ is the loss for the $m$th mini-batch.
","['machine-learning', 'deep-learning', 'tensorflow', 'autoencoders', 'variational-bayes']","For anyone stumbling on this post also looking for an answer, this twitter thread has added a lot of very useful insight.Namely:beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Frameworkdiscusses my exact question with a few experiments. Interestingly, it seems their $\beta_{norm}$ (which is similar to my normalised KLD weight) is also centred around 0.1, with higher values giving more structured latent space at the cost of poorer reconstruction, and lower values giving better reconstruction with less structured latent space (though their focus is specifically on learning disentangled representations).and related reading (where similar issues are discussed)"
Determining sample size necessary for bootstrap method / Proposed Method,"
I know this is a rather hot topic where no one really can give a simple answer for.  Nevertheless I am wondering if the following approach couldn’t be useful.
The bootstrap method is only useful if your sample follows more or less (read exactly) the same distribution as the original population.  In order to be certain this is the case you need to make your sample size large enough.  But what is large enough?
If my premise is correct you have the same problem when  using the central limit theorem to determine the population mean.  Only when your sample size is large enough you can be certain that the population of your sample means is normally distributed (around the population mean).  In other words, your samples need to represent your population (distribution) well enough.  But again, what is large enough?
In my case (administrative processes: time needed to finish a demand vs amount of demands) I have a population with a multi-modal distribution (all the demands that are finished in 2011)  of which I am 99% certain that it is even less normally distributed than the population (all the demands that are finished between present day and a day in the past, ideally this timespan is as small as possible) I want to research.
My 2011 population exists out of enough units to make $x$ samples of a sample size $n$.
I choose a value of  $x$, suppose $10$ ($x=10$).   Now I use trial and error to determine a good sample size.  I take an $n=50$, and see if my sample mean population is normally distributed by using Kolmogorov-Smirnov.  If so I repeat the same steps but with a sample size of $40$, if not repeat with a sample size of $60$ (etc.).
After a while I conclude that $n=45$ is the absolute minimum sample size to get a more or less good representation of my 2011 population.  Since I know my population of interest (all the demands that are finished between present day and a day in the past) has less variance I can safely use a sample size of $n=45$ to bootstrap.   (Indirectly, the $n=45$ determines the size of my timespan: time needed to finish $45$ demands.)
This is, in short, my idea.  But since I am not a statistician but an engineer whose statistics lessons took place in the days of yonder I cannot exclude the possibility I just generated a lot of rubbish :-).  What do you guys think?  If my premise makes sense, do I need to chose an $x$ larger than $10$, or smaller?  Depending on your answers (do I need to feel embarrassed or not? :-) I'll be posting some more discussion ideas.
response on first answer
Thanks for replying,  Your answer was very useful to me especially the book links.
But I am afraid that in my attempt to give information I completely clouded my question.  I know that the bootstrap samples take over the distribution of the population sample.  I follow you completely but...
Your original population sample needs to be large enough to be moderately certain that the distribution of your population sample corresponds (equals) with the 'real' distribution of the population.
This is merely an idea on how to determine how large your original sample size needs to be in order to be reasonably certain that the sample distribution corresponds with the population distribution.
Suppose you have a bimodal population distribution and one top is a lot larger than the other one.  If your sample size is 5 the chance is large that all 5 units have a value very close to the large top (chance to ad randomly draw a unit there is the largest).  In this case your sample distribution will look unimodal.
With a sample size of a hundred the chance that your sample distribution is also bimodal is a lot larger!!  The trouble with bootstrapping is that you only have one sample (and you build further on that sample).  If the sample distribution really does not correspond with the population distribution you are in trouble.  This is just an idea to make the chance of having 'a bad sample distribution' as low as possible without having to make your sample size infinitely large.
","['bootstrap', 'sample-size', 'methodology']",
Derive Variance of regression coefficient in simple linear regression,"
In simple linear regression, we have $y = \beta_0 + \beta_1 x + u$, where $u \sim iid\;\mathcal N(0,\sigma^2)$. I derived the estimator: 
$$
\hat{\beta_1} = \frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2}\ ,
$$
where $\bar{x}$ and $\bar{y}$ are the sample means of $x$ and $y$. 
Now I want to find the variance of $\hat\beta_1$. I derived something like the following: 
$$
\text{Var}(\hat{\beta_1}) = \frac{\sigma^2(1 - \frac{1}{n})}{\sum_i (x_i - \bar{x})^2}\ .
$$
The derivation is as follow:
\begin{align}
&\text{Var}(\hat{\beta_1})\\
& =
\text{Var} \left(\frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} \right) \\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} \text{Var}\left( \sum_i (x_i - \bar{x})\left(\beta_0 + \beta_1x_i + u_i - \frac{1}{n}\sum_j(\beta_0 + \beta_1x_j + u_j) \right)\right)\\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} 
\text{Var}\left( \beta_1 \sum_i (x_i - \bar{x})^2 + 
\sum_i(x_i - \bar{x})
\left(u_i - \sum_j \frac{u_j}{n}\right) \right)\\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\text{Var}\left( \sum_i(x_i - \bar{x})\left(u_i - \sum_j \frac{u_j}{n}\right)\right)\\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\;\times \\
&\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;E\left[\left( \sum_i(x_i - \bar{x})(u_i - \sum_j \frac{u_j}{n}) - \underbrace{E\left[\sum_i(x_i - \bar{x})(u_i - \sum_j \frac{u_j}{n})\right] }_{=0}\right)^2\right]\\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} 
E\left[\left( \sum_i(x_i - \bar{x})(u_i - \sum_j \frac{u_j}{n})\right)^2 \right] \\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} E\left[\sum_i(x_i - \bar{x})^2(u_i - \sum_j \frac{u_j}{n})^2 \right]\;\;\;\;\text{ , since } u_i \text{ 's are iid} \\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\sum_i(x_i - \bar{x})^2E\left(u_i - \sum_j \frac{u_j}{n}\right)^2\\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\sum_i(x_i - \bar{x})^2 \left(E(u_i^2) - 2 \times E \left(u_i \times (\sum_j \frac{u_j}{n})\right) + E\left(\sum_j \frac{u_j}{n}\right)^2\right)\\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2}\sum_i(x_i - \bar{x})^2 
\left(\sigma^2 - \frac{2}{n}\sigma^2 + \frac{\sigma^2}{n}\right)\\
& =
\frac{\sigma^2}{\sum_i (x_i - \bar{x})^2}\left(1 - \frac{1}{n}\right)
\end{align}
Did I do something wrong here?
I know if I do everything in matrix notation, I would get ${\rm Var}(\hat{\beta_1}) = \frac{\sigma^2}{\sum_i (x_i - \bar{x})^2}$. But I am trying to derive the answer without using the matrix notation just to make sure I understand the concepts. 
","['regression', 'mathematical-statistics', 'variance', 'linear-model', 'regression-coefficients']","At the start of your derivation you multiply out the brackets $\sum_i (x_i - \bar{x})(y_i - \bar{y})$, in the process expanding both $y_i$ and $\bar{y}$. The former depends on the sum variable $i$, whereas the latter doesn't. If you leave $\bar{y}$ as is, the derivation is a lot simpler, because
\begin{align}
\sum_i (x_i - \bar{x})\bar{y}
&= \bar{y}\sum_i (x_i - \bar{x})\\
&= \bar{y}\left(\left(\sum_i x_i\right) - n\bar{x}\right)\\
&= \bar{y}\left(n\bar{x} - n\bar{x}\right)\\
&= 0
\end{align}Hence\begin{align}
\sum_i (x_i - \bar{x})(y_i - \bar{y})
&= \sum_i (x_i - \bar{x})y_i - \sum_i (x_i - \bar{x})\bar{y}\\
&= \sum_i (x_i - \bar{x})y_i\\
&= \sum_i (x_i - \bar{x})(\beta_0 + \beta_1x_i + u_i )\\
\end{align}and\begin{align}
\text{Var}(\hat{\beta_1})
& = \text{Var} \left(\frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} \right) \\
&= \text{Var} \left(\frac{\sum_i (x_i - \bar{x})(\beta_0 + \beta_1x_i + u_i )}{\sum_i (x_i - \bar{x})^2} \right), \;\;\;\text{substituting in the above} \\
&= \text{Var} \left(\frac{\sum_i (x_i - \bar{x})u_i}{\sum_i (x_i - \bar{x})^2} \right), \;\;\;\text{noting only $u_i$ is a random variable} \\
&=  \frac{\sum_i (x_i - \bar{x})^2\text{Var}(u_i)}{\left(\sum_i (x_i - \bar{x})^2\right)^2} , \;\;\;\text{independence of } u_i \text{ and, Var}(kX)=k^2\text{Var}(X) \\
&= \frac{\sigma^2}{\sum_i (x_i - \bar{x})^2} \\
\end{align}which is the result you want.As a side note, I spent a long time trying to find an error in your derivation. In the end I decided that discretion was the better part of valour and it was best to try the simpler approach. However for the record I wasn't sure that this step was justified
$$\begin{align}
& =.
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} 
E\left[\left( \sum_i(x_i - \bar{x})(u_i - \sum_j \frac{u_j}{n})\right)^2 \right] \\
& =
\frac{1}{(\sum_i (x_i - \bar{x})^2)^2} E\left[\sum_i(x_i - \bar{x})^2(u_i - \sum_j \frac{u_j}{n})^2 \right]\;\;\;\;\text{ , since } u_i \text{ 's are iid} \\
\end{align}$$
because it misses out the cross terms due to $\sum_j \frac{u_j}{n}$."
Random forest assumptions,"
I am kind of new to random forest so I am still struggling with some basic concepts.
In linear regression, we assume independent observations, constant variance…  

What are the basic assumptions/hypothesis we make, when we use random forest?   
What are the key differences between random forest and naive bayes in terms of model assumptions?  

","['regression', 'classification', 'random-forest']","Thanks for a very good question! I will try to give my intuition behind it.In order to understand this, remember the ""ingredients"" of random forest classifier (there are some modifications, but this is the general pipeline):Assume first point. It is not always possible to find the best split. For example in the following dataset each split will give exactly one misclassified object.
And I think that exactly this point can be confusing: indeed, the behaviour of the individual split is somehow similar to the behaviour of Naive Bayes classifier: if the variables are dependent - there is no better split for Decision Trees and Naive Bayes classifier also fails (just to remind: independent variables is the main assumption that we make in Naive Bayes classifier; all other assumptions come from the probabilistic model that we choose).But here comes the great advantage of decision trees: we take any split and continue splitting further. And for the following splits we will find a perfect separation (in red).
And as we have no probabilistic model, but just binary split, we don't need to make any assumption at all.That was about Decision Tree, but it also applies for Random Forest. The difference is that for Random Forest we use Bootstrap Aggregation. It has no model underneath, and the only assumption that it relies is that sampling is representative. But this is usually a common assumption. For example, if one class consist of two components and in our dataset one component is represented by 100 samples, and another component is represented by 1 sample - probably most individual decision trees will see only the first component and Random Forest will misclassify the second one.
Hope it will give some further understanding."
What is the distribution of the Euclidean distance between two normally distributed random variables?,"
Assume you are given two objects whose exact locations are unknown, but are distributed according to normal distributions with known parameters (e.g. $a \sim N(m, s)$ and $b \sim N(v, t))$. We can assume these are both bivariate normals, such that the positions are described by a distribution over $(x,y)$ coordinates (i.e. $m$ and $v$ are vectors containing the expected $(x,y)$ coordinates for $a$ and $b$ respectively). We will also assume the objects are independent.
Does anyone know if the distribution of the squared Euclidean distance between these two objects is a known parametric distribution? Or how to derive the PDF / CDF for this function analytically?
","['normal-distribution', 'distance-functions']","The answer to this question can be found in the book Quadratic forms in random variables by Mathai and Provost (1992, Marcel Dekker, Inc.). As the comments clarify, you need to find the distribution of $Q = z_1^2 + z_2^2$ where 
$z = a - b$ follows a bivariate normal distribution with mean $\mu$ and covariance matrix $\Sigma$. This is a quadratic form in the bivariate random variable $z$. Briefly, one nice general result for the $p$-dimensional case where $z \sim N_p(\mu, \Sigma)$ and
$$Q = \sum_{j=1}^p z_j^2$$ 
is that the moment generating function is 
$$E(e^{tQ}) = e^{t \sum_{j=1}^p \frac{b_j^2 \lambda_j}{1-2t\lambda_j}}\prod_{j=1}^p (1-2t\lambda_j)^{-1/2}$$
where $\lambda_1, \ldots, \lambda_p$ are the eigenvalues of $\Sigma$ and $b$ is a linear function of $\mu$. See Theorem 3.2a.2 (page 42) in the book cited above (we assume here that $\Sigma$ is non-singular). Another useful representation is 3.1a.1 (page 29) 
$$Q = \sum_{j=1}^p \lambda_j(u_j + b_j)^2$$
where $u_1, \ldots, u_p$ are i.i.d. $N(0, 1)$. The entire Chapter 4 in the book is devoted to the representation and computation of densities and distribution functions, which is not at all trivial. I am only superficially familiar with the book, but my impression is that all the general representations are in terms of infinite series expansions.  So in a certain way the answer to the question is, yes, the distribution of the squared euclidean distance between two bivariate normal vectors belongs to a known (and well studied) class of distributions parametrized by the four parameters $\lambda_1, \lambda_2 > 0$ and $b_1, b_2 \in \mathbb{R}$. However, I am pretty sure you won't find this distribution in your standard textbooks. Note, moreover, that $a$ and $b$ do not need to be independent. Joint normality is enough (which is automatic if they are independent and each normal), then the difference $a-b$ follows a normal distribution. "
What algorithm is used in linear regression?,"
I usually hear about ""ordinary least squares"".  Is that the most widely used algorithm used for linear regression?  Are there reasons to use a different one?
","['regression', 'least-squares', 'algorithms', 'computational-statistics', 'numerics']","Regarding the question in the title, about what is the algorithm that is used:In a linear algebra perspective, the linear regression algorithm is the way to solve a linear system $\mathbf{A}x=b$ with more equations than unknowns. In most of the cases there is no solution to this problem. And this is because the vector $b$ doesn't belong to the column space of $\mathbf{A}$, $C(\mathbf{A})$. The best straight line is the one that makes the overall error $e=\mathbf{A}x-b$ as small as it takes. And is convenient to think as small to be the squared length, $\lVert e \rVert^2$, because it's non negative, and it equals 0 only when $b\in C(\mathbf{A})$. Projecting (orthogonally) the vector $b$ to the nearest point in the column space of $\mathbf{A}$ gives the vector $b^*$ that solves the system (it's components lie on the best straight line) with the minimum error. $\mathbf{A}^T\mathbf{A}\hat{x}=\mathbf{A}^Tb \Rightarrow \hat{x}=(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^Tb$and the projected vector $b^*$ is given by:$b^*=\mathbf{A}\hat{x}=\mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^Tb$Perhaps the least squares method is not exclusively used because that squaring  overcompensates for outliers. Let me give a simple example in R, that solves the regression problem using this algorithm:"
What references should be cited to support using 30 as a large enough sample size?,"
I have read/heard many times that the sample size of at least 30 units is considered as ""large sample"" (normality assumptions of means usually approximately holds due to the CLT, ...).  Therefore, in my experiments, I usually generate samples of 30 units. Can you please give me some reference which should be cited when using sample size 30?
","['references', 'sample-size', 'normality-assumption', 'central-limit-theorem', 'rule-of-thumb']","The choice of n = 30 for a boundary between small and large samples is a rule of thumb, only. There is a large number of books that quote (around) this value, for example, Hogg and Tanis' Probability and Statistical Inference (7e) says ""greater than 25 or 30"".That said, the story told to me was that the only reason 30 was regarded as a good boundary was because it made for pretty Student's t tables in the back of textbooks to fit nicely on one page.  That, and the critical values (between Student's t and Normal) are only off by approximately up to 0.25, anyway, from df = 30 to df = infinity.  For hand computation the difference didn't really matter.Nowadays it is easy to compute critical values for all sorts of things to 15 decimal places.  On top of that we have resampling and permutation methods for which we aren't even restricted to parametric population distributions.In practice I never rely on n = 30.  Plot the data.  Superimpose a normal distribution, if you like.  Visually assess whether a normal approximation is appropriate (and ask whether an approximation is even really needed).  If generating samples for research and an approximation is obligatory, generate enough of a sample size to make the approximation as close as desired (or as close as computationally feasible)."
"Why is ""statistically significant"" not enough?","
I have completed my data analysis and got ""statistically significant results"" which is consistent with my hypothesis. However, a student in statistics told me this is a premature conclusion. Why? Is there anything else needed to be included in my report?
","['hypothesis-testing', 'statistical-significance', 'spss', 'p-value']","Typically, hypotheses are framed in a binary way. I'll put directional hypotheses to one side, as they don't change the issue much. It is common, at least in psychology, to talk about hypotheses such as: the difference between group means is or is not zero; the correlation is or is not zero; the regression coefficient is or is not zero; the r-square is or is not zero. In all these cases, there is a null hypothesis of no effect, and an alternative hypothesis of an effect.This binary thinking is generally not what we are most interested in. Once you think about your research question, you will almost always find that you are actually interested in estimating parameters. You are interested in the actual difference between group means, or the size of the correlation, or the size of the regression coefficient, or the amount of variance explained.Of course, when we get a sample of data, the sample estimate of a parameter is not the same as the population parameter. So we need a way of quantifying our uncertainty about what the value of the parameter might be. From a frequentist perspective, confidence intervals provide a means of doing, although Bayesian purists might argue that they don't strictly permit the inference you might want to make. From a Bayesian perspective, credible intervals on posterior densities provide a more direct means of quantifying your uncertainty about the value of a population parameter.Moving away from the binary hypothesis testing approach forces you to think in a continuous way. For example, what size difference in group means would be theoretically interesting? How would you map difference between group means onto subjective language or practical implications? Standardised measures of effect along with contextual norms are one way of building a language for quantifying what different parameter values mean. Such measures are often labelled ""effect sizes"" (e.g., Cohen's d, r, $R^2$, etc.). However, it is perfectly reasonable, and often preferable, to talk about the importance of an effect using unstandardised measures (e.g., the difference in group means on meaningful unstandardised variables such as  income levels, life expectancy, etc.).There's a huge literature in psychology (and other fields) critiquing a focus on p-values, null hypothesis significance testing, and so on (see this Google Scholar search). This literature often recommends reporting effect sizes with confidence intervals as a resolution (e.g., APA Task force by Wilkinson, 1999). If you are thinking about adopting this thinking, I think there are progressively more sophisticated approaches you can take:Among many possible references, you'll see Andrew Gelman talk a lot about these issues on his blog and in his research."
Statistical tests when sample size is 1,"
I'm a high school math teacher who is a bit stumped. A Biology student came to me with his experiment wanting to know what kind of statistical analysis he can do with his data (yes, he should have decided that BEFORE the experiment, but I wasn't consulted until after). 
He is trying to determine what effect insulin has on the concentration of glucose in a cell culture. There are six culture grouped into three pairs (one with insulin and one without) each under slightly different conditions.  
The problem is that he only took one sample from each so the there is no standard deviation (or the standard deviation is 0 since the value varies from itself by 0). 
Is there any statistical analysis he can perform with this data? What advice should I give him other than to redo the experiment?
","['hypothesis-testing', 'estimation', 'experiment-design']","Unfortunately, your student has a problem.The idea of any (inferential) statistical analysis is to understand whether a pattern of observations can be simply due to natural variation or chance, or whether there is something systematic there. If the natural variation is large, then the observed difference may be simply due to chance. If the natural variation is small, then it may be indicative of a true underlying effect.With only a single pair of observations, we have no idea of the natural variation in the data we observe. So we are missing half of the information we need.You note that your student has three pairs of observations. Unfortunately, they were collected under different conditions. So the variability we observe between these three pairs may simply be due to the varying conditions, and won't help us for the underlying question about a possible effect of insulin.One straw to grasp at would be to get an idea of the natural variation through other channels. Maybe similar observations under similar conditions have been made before and reported in the literature. If so, we could compare our observations to these published data. (This would still be problematic, because the protocols will almost certainly have been slightly different, but it might be better than nothing.)EDIT: note that my explanation here applies to the case where the condition has a potential impact on the effect of insulin, an interaction. If we can disregard this possibility and expect only main effects (i.e., the condition will have an additive effect on glucose that is independent of the additional effect of insulin), then we can at least formally run an ANOVA as per BruceET's answer. This may be the best the student can do. (And they at least get to practice writing up the limitations of their study, which is also an important skill!)Failing that, I am afraid the only possibility would be to go back to the lab bench and collect more data.In any case, this is a (probably painful, but still) great learning opportunity! I am sure this student will in the future always think about the statistical analysis before planning their study, which is how it should be. Better to learn this in high school rather than only in college.Let me close with a relevant quote attributed to Ronald Fisher:To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of."
Logistic regression vs. LDA as two-class classifiers,"
I am trying to wrap my head around the statistical difference between Linear discriminant analysis and Logistic regression. Is my understanding right that, for a two class classification problem, LDA predicts two normal density functions (one for each class) that creates a linear boundary where they intersect, whereas logistic regression only predicts the log-odd function between the two classes, which creates a boundary but does not assume density functions for each class? 
","['regression', 'logistic', 'classification', 'discriminant-analysis']","It sounds to me that you are correct. Logistic regression indeed does not assume any specific shapes of densities in the space of predictor variables, but LDA does. Here are some differences between the two analyses, briefly.Binary Logistic regression (BLR) vs Linear Discriminant analysis (with 2 groups: also known as Fisher's LDA):BLR: Based on Maximum likelihood estimation.
LDA: Based on Least squares estimation; equivalent to linear regression with binary predictand (coefficients are proportional and R-square = 1-Wilk's lambda).BLR: Estimates probability (of group membership) immediately (the predictand is itself taken as probability, observed one) and conditionally.
LDA: estimates probability mediately (the predictand is viewed as binned continuous variable, the discriminant) via classificatory device (such as naive Bayes) which uses both conditional and marginal information.BLR: Not so exigent to the level of the scale and the form of the distribution in predictors.
LDA: Predictirs desirably interval level with multivariate normal distribution.BLR: No requirements about the within-group covariance matrices of the predictors.
LDA: The within-group covariance matrices should be identical in population.BLR: The groups may have quite different $n$.
LDA: The groups should have similar $n$.BLR: Not so sensitive to outliers.
LDA: Quite sensitive to outliers.BLR: Younger method.
LDA: Older method.BLR: Usually preferred, because less exigent / more robust.
LDA: With all its requirements met, often classifies better than BLR (asymptotic relative efficiency 3/2 time higher then)."
Can a deep neural network approximate multiplication function?,"
Let's say we want to do regression for simple f = x * y using standard fully-connected deep neural network. The network takes x and y as input, and should learn to output x * y.
I remember that there is research proving that NN with one hidden layer can approximate any function, but I have tried and was unable to approximate even this simple multiplication.
Only log-normalization of the data helped m = x*y => ln(m) = ln(x) + ln(y) because it turns multiplication into addition, but that looks like a cheat. Can NN do this without log-normalization?
The answer seems to be Yes, so the question is more what should the type/configuration/layout of such NN be?
","['regression', 'machine-learning', 'neural-networks']","A big multiplication function gradient forces the net probably almost immediately into some horrifying state where all its hidden nodes have a zero gradient (because of neural network implementation details and limitations). We can use two approaches:Divide by a constant. We are just dividing everything before the learning and multiply after.Use log-normalization. It makes multiplication into addition:\begin{align}
m &= x \cdot y\\
&\Rightarrow  \\
\ln(m) &= \ln(x) + \ln(y)
\end{align}"
Maximum Likelihood Estimators - Multivariate Gaussian,"
Context
The Multivariate Gaussian appears frequently in Machine Learning and the following results are used in many ML books and courses without the derivations. 

Given data in form of a matrix $\mathbf{X} $ of dimensions 
  $ m \times p$, if we assume that the data follows a $p$-variate Gaussian
  distribution with parameters mean $\mu$ ( $p \times 1 $) and
  covariance matrix $\Sigma$ ($p \times p$) the Maximum Likelihood
  Estimators are given by:

$\hat \mu =  \frac{1}{m} \sum_{i=1}^m \mathbf{  x^{(i)} } = \mathbf{\bar{x}}$
$\hat \Sigma  = \frac{1}{m} \sum_{i=1}^m \mathbf{(x^{(i)} - \hat \mu) (x^{(i)} -\hat  \mu)}^T $


I understand that knowledge of the multivariate Gaussian is a pre-requisite for many ML courses, but it would be helpful to have the full derivation in a self contained answer once and for all as I feel many self-learners are bouncing around the stats.stackexchange and math.stackexchange websites looking for answers. 

Question
What is the full derivation of the Maximum Likelihood Estimators for the multivariate Gaussian

Examples:
These lecture notes (page 11) on Linear Discriminant Analysis, or these ones make use of the results and assume previous knowledge. 
There are also a few posts which are partly answered or closed: 

Maximum likelihood estimator for multivariate normal distribution
Need help to understand Maximum Likelihood Estimation for multivariate normal distribution?

","['normal-distribution', 'maximum-likelihood', 'estimators', 'multivariate-normal-distribution']","Assume that we have $m$ random vectors, each of size $p$: $\mathbf{X^{(1)}, X^{(2)}, \dotsc, X^{(m)}}$ where each random vectors can be interpreted as an observation (data point) across $p$ variables. If each $\mathbf{X}^{(i)}$ are i.i.d. as multivariate Gaussian vectors:$$ \mathbf{X^{(i)}} \sim \mathcal{N}_p(\mu, \Sigma) $$Where the parameters $\mu, \Sigma$ are unknown. To obtain their estimate we can use the method of maximum likelihood and maximize the log likelihood function.Note that by the independence of the random vectors, the joint density of the data $\mathbf{ \{X^{(i)}}, i = 1,2, \dotsc ,m\}$ is the product of the individual densities, that is $\prod_{i=1}^m f_{\mathbf{X^{(i)}}}(\mathbf{x^{(i)} ; \mu , \Sigma })$. Taking the logarithm gives the log-likelihood function\begin{aligned}
	l(\mathbf{ \mu, \Sigma | x^{(i)} }) & = \log \prod_{i=1}^m f_{\mathbf{X^{(i)}}}(\mathbf{x^{(i)} | \mu , \Sigma })
	\\
	& =  \log  \ \prod_{i=1}^m \frac{1}{(2 \pi)^{p/2} |\Sigma|^{1/2}} \exp \left( - \frac{1}{2} \mathbf{(x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu) } \right) 
	\\
	& = \sum_{i=1}^m \left( - \frac{p}{2} \log (2 \pi) - \frac{1}{2} \log |\Sigma|  - \frac{1}{2}   \mathbf{(x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu) }  \right)
\end{aligned}\begin{aligned}
	l(\mu, \Sigma ; ) & = - \frac{mp}{2} \log (2 \pi) - \frac{m}{2} \log |\Sigma|  - \frac{1}{2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu) }  
\end{aligned}To take the derivative with respect to $\mu$ and equate to zero we will make use of the following matrix calculus identity:$\mathbf{ \frac{\partial w^T A w}{\partial w} = 2Aw}$ if $\mathbf{w}$
does not depend on $\mathbf{A}$ and $\mathbf{A}$ is symmetric.\begin{aligned}
	\frac{\partial }{\partial \mu} l(\mathbf{ \mu, \Sigma | x^{(i)} }) & = \sum_{i=1}^m  \mathbf{ \Sigma^{-1} ( x^{(i)} - \mu ) }  = 0
	\\ 
	& \text{Since $\Sigma$ is positive definite}
	\\
	0 & = m \mu - \sum_{i=1}^m  \mathbf{  x^{(i)} } 
	\\
	\hat \mu &=  \frac{1}{m} \sum_{i=1}^m \mathbf{  x^{(i)} } = \mathbf{\bar{x}}
\end{aligned}Which is often called the sample mean vector.Deriving the MLE for the covariance matrix requires more work and the use of the following linear algebra and calculus properties:Combining these properties allows us to calculate$$ \frac{\partial}{\partial A}  x^TAx =\frac{\partial}{\partial A}  \mathrm{tr}\left[xx^TA\right] = [xx^T]^T = \left(x^{T}\right)^Tx^T = xx^T $$Which is the outer product of the vector $x$ with itself.We can now re-write the log-likelihood function and compute the derivative w.r.t. $\Sigma^{-1}$ (note $C$ is constant)\begin{aligned}
	l(\mathbf{ \mu, \Sigma | x^{(i)} })  & = \text{C} - \frac{m}{2} \log |\Sigma|  - \frac{1}{2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu) }  
	\\
	& = \text{C} + \frac{m}{2} \log |\Sigma^{-1}|  - \frac{1}{2}  \sum_{i=1}^m  \mathrm{tr}\left[ \mathbf{(x^{(i)} - \mu) (x^{(i)} - \mu)^T \Sigma^{-1} }  \right]
	\\
	\frac{\partial }{\partial \Sigma^{-1}} l(\mathbf{ \mu, \Sigma | x^{(i)} }) & = \frac{m}{2} \Sigma - \frac{1}{2}  \sum_{i=1}^m \mathbf{(x^{(i)} - \mu) (x^{(i)} - \mu)}^T \ \ \text{Since $\Sigma^T = \Sigma$}
\end{aligned}Equating to zero and solving for $\Sigma$\begin{aligned}
	0 &= m \Sigma -   \sum_{i=1}^m \mathbf{(x^{(i)} - \mu) (x^{(i)} - \mu)}^T 
	\\
	\hat \Sigma & = \frac{1}{m} \sum_{i=1}^m \mathbf{(x^{(i)} - \hat \mu) (x^{(i)} -\hat  \mu)}^T 
\end{aligned}"
How do I test a nonlinear association?,"
For plot 1, I can test the association between x and y by doing a simple correlation.

For plot 2, where the relationship is nonlinear yet there is a clear relation between x and y, how can I test the association and label its nature? 

","['nonlinear-regression', 'non-independent', 'association-measure']","...the relationship is nonlinear yet there is a clear relation between x and y, how can I test the association and label its nature?One way of doing this would be to fit $y$ as a semi-parametrically estimated function of $x$ using, for example, a generalized additive model and testing whether or not that functional estimate is constant, which would indicate no relationship between $y$ and $x$. This approach frees you from having to do polynomial regression and making sometimes arbitrary decisions about the order of the polynomial, etc. Specifically, if you have observations, $(Y_i, X_i)$, you could fit the model: $$ E(Y_i | X_i) = \alpha + f(X_i) + \varepsilon_i $$ and test the hypothesis $H_{0} : f(x) = 0, \ \forall x$. In R, you can do this using the gam() function. If y is your outcome and x is your predictor, you could type: Typing summary(g) will give you the result of the hypothesis test above. As far as characterizing the nature of the relationship, this would be best done with a plot. One way to do this in R (assuming the code above has already been entered)If your response variable is discrete (e.g. binary), you can accommodate that within this framework by fitting a logistic GAM (in R, you'd add family=binomial to your call to gam). Also, if you have multiple predictors, you can include multiple additive terms (or ordinary linear terms), or fit multivariable functions, e.g. $f(x,z)$ if you had predictors x, z. The complexity of the relationship is automatically selected by cross validation if you use the default methods, although there is a lot of flexibility here - see the gam help file if interested. "
PP-plots vs. QQ-plots,"
What is the difference between probability plots, PP-plots and QQ-plots when trying to analyse a fitted distribution to data?
","['probability', 'data-visualization', 'goodness-of-fit', 'qq-plot']",
How to interpret error measures?,"
I am running the classify in Weka for a certain dataset and I've noticed that if I'm trying to predict a nominal value the output specifically shows the correctly and incorrectly predicted values. However, now I'm running it for a numerical attribute and the output is:
Correlation coefficient                 0.3305
Mean absolute error                     11.6268
Root mean squared error                 46.8547
Relative absolute error                 89.2645 %
Root relative squared error             94.3886 %
Total Number of Instances               36441 

How do I interpret this? I've tried googling each notion but I don't understand much since statistics is not at all in my field of expertise. I would greatly appreciate an ELI5 type of answer in terms of statistics.
","['machine-learning', 'error', 'weka', 'mse', 'rms']","Let's denote the true value of interest as $\theta$ and the value estimated using some algorithm as $\hat{\theta}$. Correlation tells you how much $\theta$ and $\hat{\theta}$ are related. It gives values between $-1$ and $1$, where $0$ is no relation, $1$ is very strong, linear relation and $-1$ is an inverse linear relation (i.e. bigger values of $\theta$ indicate smaller values of $\hat{\theta}$, or vice versa). Below you'll find an illustrated example of correlation.(source: http://www.mathsisfun.com/data/correlation.html)Mean absolute error is:$$\mathrm{MAE} = \frac{1}{N} \sum^N_{i=1} | \hat{\theta}_i - \theta_i | $$Root mean square error is:$$ \mathrm{RMSE} = \sqrt{ \frac{1}{N} \sum^N_{i=1} \left( \hat{\theta}_i - \theta_i \right)^2 } $$Relative absolute error:$$ \mathrm{ RAE} = \frac{ \sum^N_{i=1} | \hat{\theta}_i - \theta_i | } {  \sum^N_{i=1} | \overline{\theta} - \theta_i | } $$where $\overline{\theta}$ is a mean value of $\theta$.Root relative squared error:$$ \mathrm{ RRSE }= \sqrt{ \frac{ \sum^N_{i=1} \left( \hat{\theta}_i - \theta_i \right)^2 } {  \sum^N_{i=1} \left( \overline{\theta} - \theta_i \right)^2 }} $$As you see, all the statistics compare true values to their estimates, but do it in a slightly different way. They all tell you ""how far away"" are your estimated values from the true value of $\theta$. Sometimes square roots are used and sometimes absolute values - this is because when using square roots the extreme values have more influence on the result (see Why square the difference instead of taking the absolute value in standard deviation? or on Mathoverflow).In $ \mathrm{ MAE}$ and $ \mathrm{ RMSE}$ you simply look at the ""average difference"" between those two values - so you interpret them comparing to the scale of your valiable, (i.e. $ \mathrm{ MSE}$ of 1 point is a difference of 1 point of $\theta$ between $\hat{\theta}$ and $\theta$).In $ \mathrm{ RAE}$ and $ \mathrm{ RRSE}$ you divide those differences by the variation of $\theta$ so they have a scale from 0 to 1 and if you multiply this value by 100 you get similarity in 0-100 scale (i.e. percentage). The values of $\sum(\overline{\theta} - \theta_i)^2$ or $\sum|\overline{\theta} - \theta_i|$ tell you how much $\theta$ differs from it's mean value - so you could tell that it is about how much $\theta$ differs from itself (compare to variance). Because of that the measures are named ""relative"" - they give you result related to the scale of $\theta$.Check also those slides."
How are we defining 'reproducible research'?,"
This has come up in a few questions now, and I've been wondering about something. Has the field as a whole moved toward ""reproducibility"" focusing on the availability of the original data, and the code in question?
I was always taught that the core of reproducibility was not necessarily, as I've referred to it, the ability to click Run and get the same results. The data-and-code approach seems to assume that the data are correct - that there isn't a flaw in the collection of the data itself (often demonstrably false in the case of scientific fraud). It also focuses on a single  sample of the target population, rather than the replicability of the finding over multiple independent samples.
Why is the emphasis then on being able to re-run analysis, rather than duplicate the study from the ground up?
The article mentioned in the comments below is available here.
","['reproducible-research', 'philosophical']","Reproducible research is a term used in some research domains to  refer specifically to conducting analyses such thatWhen such data and code are shared, this allows other researchers to:This usage can be seen in discussions of technologies like Sweave. E.g., Friedrich Leisch writes in the context of Sweave that ""the report can be automatically updated if data or analysis change, which allows for truly reproducible research.""
It can also be seen in the CRAN Task View on Reproducible Research which states that ""the goal of reproducible research is to tie specific instructions to data analysis and experimental data so that scholarship can be recreated, better understood and verified.""Reproducibility is a fundamental aim of science. It's not new.
Research reports include method and results sections that should outline how the data was generated, processed, and analysed. A general rule is that the details provided should be sufficient to enable an appropriately competent researcher to take the information provided and replicate the study.Reproducibility is also closely related to the concepts of replicability and generalisation.Thus, the term ""reproducible research"", taken literally, as applied to technologies like Sweave, is a misnomer, given that it suggests a relevance broader than it covers. 
Also, when presenting technologies like Sweave to researchers who have not used such technologies, such researchers are often surprised when I call the process ""reproducible research"".Given that ""reproducible research"" as used within Sweave-like contexts only pertains to one aspect of reproducible research, perhaps an alternative term should be adopted.
Possible alternatives include:All of the above terms are a more accurate reflection of what Sweave-like analyses entail. Reproducible analysis is short and sweet. Adding ""data"" or ""statistical"" further clarifies things, but also makes the term both longer and narrower. Furthermore, ""statistical"" has a narrow and a broad meaning, and certainly within the narrow meaning, much of data processing is not statistical. Thus, the breadth implied by the term ""reproducible analysis"" has its advantages.The other additional issue with the term ""reproducible research"" is the aim of  Sweave-like technologies is not just ""reproducibility"". There are several interrelated aims:There is an argument that reproducible analysis should promote correct analyses, because there is a written record of analyses that can be checked. Furthermore if data and code is shared, it creates accountability which motivates researchers to check their analyses, and enables other researchers to note corrections.Reproducible analysis also fits in closely with concepts around open research. Of course, a researcher can use Sweave-like technologies just for themselves. Open research principles encourage sharing the data and analysis code to enable greater reuse and accountability.This is not really a critique of the use of the word ""reproducible"". Rather, it just highlights that using Sweave-like technologies is necessary but not sufficient to achieving open scientific research aims."
How to make a time series stationary?,"
Besides taking differences, what are other techniques for making a non-stationary time series, stationary?
Ordinarily one refers to a series as ""integrated of order p"" if it can be made stationary through a lag operator $(1-L)^P X_t$.
","['time-series', 'stationarity']","De-trending is fundamental.  This includes regressing against covariates other than time.Seasonal adjustment is a version of taking differences but could be construed as a separate technique.Transformation of the data implicitly converts a difference operator into something else; e.g., differences of the logarithms are actually ratios.Some EDA smoothing techniques (such as removing a moving median) could be construed as non-parametric ways of detrending.  They were used as such by Tukey in his book on EDA.  Tukey continued by detrending the residuals and iterating this process for as long as necessary (until he achieved residuals that appeared stationary and symmetrically distributed around zero)."
Is it possible to give variable sized images as input to a convolutional neural network?,"
Can we give images with variable size as input to a convolutional neural network for object detection? If possible, how can we do that?

But if we try to crop the image, we will be loosing some portion of the image and if we try to resize, then, the clarity of the image will be lost. Does it mean that using inherent network property is the best if image clarity is the main point of consideration?
","['neural-networks', 'tensorflow', 'keras', 'computer-vision', 'object-detection']",
Does anyone know any good open source software for visualizing data from database?,"
Recently I came across Tableau and tried to visualize the data from database and csv file. The user iterface enables the user to visualize time and spatial data and create plots in an instant. Such tool is really useful as it enables to observe the data graphically without writing the code.
As there are many data sources from which I have to retrieve and visualize the data it would be very useful to have a tool which enabled to generate charts by simply dragging columns on axes and additionally modify the visualization with dragging the column names as well.
Does anyone know any free or open source software of that kind?
","['data-visualization', 'software', 'interactive-visualization']","I've never tried it, but there's an open source desktop / browser-based visualisation suite called WEAVE (short for Web-based Analysis and Visualization Environment). Like Tableau, it's intended to let you explore data through an interactive click-based interface. Unlike Tableau, it's open source: you can download the source code and install your own version on your own machine which can be as private or as public as you want it to be. Don't expect anything nearly as slick and user-friendly as Tableau, but it looks like an interesting, powerful project for someone prepared to put the time in to learning to use it.

Or, you can look into rolling your own. There are some really good open source javacript tools for supporting programming data visualisation in a browser. If you don't mind coding some Javascript and some kind of server-side layer to serve up the data, give these a try:Raphael if you need SVG output to work in Internet Explorer 6, 7, and 8. If you're interested in the web programming option, here's a slightly more detailed write-up I wrote on Raphael and D3 for stackoverflow.There are also some free (not open source) online datavis suites worth mentioning (probably not suitable for direct DB connection but worth a look):And something completely different: if you have a quality server lying around and you happen to want to make awesome google-maps style tile-based 'slippy' maps using open source tech (probably not what you're looking for - but it's possible!), check out MapBox TileMill. Have a look through the gallery of examples on their home page - some of them are truly stunning. See also related project Modest Maps, an open source Javascript library for interacting with maps developed by Stamen Design (a really highly rated agency specialising in interactive maps). It's considered to be an improvement on the more established OpenLayers. All open source.WEAVE is the best GUI-based open-source tool I know of for personal visual analysis. The other tools listed are top of the range tools for online publishing of visualisations (for example, D3 is used by and developed by the award-winning NY Times graphics team), and are more often used for visualisation in the context of public-facing communications than exploratory analysis, but they can be used for analysis too."
Multiple regression or partial correlation coefficient? And relations between the two,"
I don't even know if this question makes sense, but what is the difference between multiple regression and partial correlation (apart from the obvious differences between correlation and regression, which is not what I am aiming at)?
I want to figure out the following:
I have two independent variables ($x_1$, $x_2$) and one dependent variable ($y$). Now individually the independent variables are not correlated with the dependent variable. But for a given $x_1$ $y$ decreases when $x_2$ decreases. So do I analyze that by means of multiple regression or partial correlation?
edit to hopefully improve my question:
I am trying to understand the difference between multiple regression and partial correlation. So, when $y$ decreases for a given $x_1$ when $x_2$ decreases, is that due to the combined effect of $x_1$ and $x_2$ on $y$ (multiple regression) or is it due to removing the effect of $x_1$ (partial correlation)?
","['multiple-regression', 'regression-coefficients', 'partial-correlation']","Multiple linear regression coefficient and partial correlation are directly linked and have the same significance (p-value). Partial r is just another way of standardizing the coefficient, along with beta coefficient (standardized regression coefficient)$^1$. So, if the dependent variable is $y$ and the independents are $x_1$ and $x_2$ then$$\text{Beta:} \quad \beta_{x_1} = \frac{r_{yx_1} - r_{yx_2}r_{x_1x_2} }{1-r_{x_1x_2}^2}$$$$\text{Partial r:} \quad  r_{yx_1.x_2} = \frac{r_{yx_1} - r_{yx_2}r_{x_1x_2} }{\sqrt{ (1-r_{yx_2}^2)(1-r_{x_1x_2}^2) }}$$You see that the numerators are the same which tell that both formulas measure the same unique effect of $x_1$. I will try to explain how the two formulas are structurally identical and how they are not.Suppose that you have z-standardized (mean 0, variance 1) all three variables. The numerator then is equal to the covariance between two kinds of residuals: the (a) residuals left in predicting $y$ by $x_2$ [both variables standard] and the (b) residuals left in predicting $x_1$ by $x_2$ [both variables standard]. Moreover, the variance of the residuals (a) is $1-r_{yx_2}^2$; the variance of the residuals (b) is $1-r_{x_1x_2}^2$.The formula for the partial correlation then appears clearly the formula of plain Pearson $r$, as computed in this instance between residuals (a) and residuals (b): Pearson $r$, we know, is covariance divided by the denominator that is the geometric mean of two different variances.Standardized coefficient beta is structurally like Pearson $r$, only that the denominator is the geometric mean of a variance with own self. The variance of residuals (a) was not counted; it was replaced by second counting of the variance of residuals (b). Beta is thus the covariance of the two residuals relative the variance of one of them (specifically, the one pertaining to the predictor of interest, $x_1$). While partial correlation, as already noticed, is that same covariance relative their hybrid variance. Both types of coefficient are ways to standardize the effect of $x_1$ in the milieu of other predictors.Some numerical consequences of the difference. If R-square of multiple regression of $y$ by $x_1$ and $x_2$ happens to be 1 then both partial correlations of the predictors with the dependent will be also 1 absolute value (but the betas will generally not be 1). Indeed, as said before, $r_{yx_1.x_2}$ is the correlation between the residuals of y <- x2 and the residuals of x1 <- x2. If what is not $x_2$ within $y$ is exactly what is not $x_2$ within $x_1$ then there is nothing within $y$ that is neither $x_1$ nor $x_2$: complete fit. Whatever is the amount of the unexplained (by $x_2$) portion left in $y$ (the $1-r_{yx_2}^2$), if it is captured relatively highly by the independent portion of $x_1$ (by the $1-r_{x_1x_2}^2$), the $r_{yx_1.x_2}$ will be high. $\beta_{x_1}$, on the other hand, will be high only provided that the being captured unexplained portion of $y$ is itself a substantial portion of $y$.From the above formulas one obtains (and extending from 2-predictor regression to a regression with arbitrary number of predictors $x_1,x_2,x_3,...$) the conversion formula between beta and corresponding partial r:$$r_{yx_1.X} = \beta_{x_1} \sqrt{ \frac {\text{var} (e_{x_1 \leftarrow X})} {\text{var} (e_{y \leftarrow X})}},$$where $X$ stands for the collection of all predictors except the current ($x_1$); $e_{y \leftarrow X}$ are the residuals from regressing $y$ by $X$, and $e_{x_1 \leftarrow X}$ are the residuals from regressing $x_1$ by $X$, the variables in both these regressions enter them standardized.Note: if we need to to compute partial correlations of $y$ with every predictor $x$ we usually won't use this formula requiring to do two additional regressions. Rather, the sweep operations (often used in stepwise and all subsets regression algorithms) will be done or anti-image correlation matrix will be computed.$^1$ $\beta_{x_1} = b_{x_1} \frac {\sigma_{x_1}}{\sigma_y}$ is the relation between the raw $b$ and the standardized $\beta$ coefficients in regression with intercept.Addendum. Geometry of regression $beta$ and partial $r$.On the picture below, a linear regression with two correlated predictors, $X_1$ and $X_2$, is shown. The three variables, including the dependent $Y$, are drawn as vectors (arrows). This way of display is different from usual scatterplot (aka variable space display) and is called subject space display. (You may encounter similar drawings locally here, here, here, here, here, here, here and in some other threads.)The pictures are drawn after all the three variables were centered, and so (1) every vector's length = st. deviation of the respective variable, and (2) angle (its cosine) between every two vectors = correlation between the respective variables.$Y'$ is the regression prediction (orthogonal projection of $Y$ onto ""plane X"" spanned by the regressors); $e$ is the error term; $\cos \angle{Y Y'}={|Y'|}/|Y|$ is the multiple correlation coefficient.The skew coordinates of $Y'$ on the predictors $X1$ and $X2$ relate their multiple regression coefficients. These lengths from the origin are the scaled $b$'s or $beta$'s. For example, the magnitude of the skew coordinate onto $X_1$ equals $\beta_1\sigma_Y= b_1\sigma_{X_1}$; so, if $Y$ is standardized ($|Y|=1$), the coordinate = $\beta_1$. See also.But how to obtain an impression of the corresponding partial correlation $r_{yx_1.x_2}$? To partial out $X_2$ from the other two variables one has to project them on the plane which is orthogonal to $X_2$. Below, on the left, this plane perpendicular to $X_2$ has been drawn. It is shown at the bottom - and not on the level of the origin - simply in order not to jam the pic. Let's inspect what's going on in that space. Put your eye to the bottom (of the left pic) and glance up, $X_2$ vector starting right from your eye.All the vectors are now the projections. $X_2$ is a point since the plane was produced as the one perpendicular to it. We look so that ""Plane X"" is horizontal line to us. Therefore of the four vectors only (the projection of) $Y$ departs the line.From this perspective, $r_{yx_1.x_2}$ is $\cos \alpha$. It is the angle between the projection vectors of $Y$ and of $X_1$. On the plane orthogonal to $X_2$. So it is very simple to understand.Note that $r_{yx_1.x_2}=r_{yy'.x_2}$, as both $Y'$ and $X_1$ belong to ""plane X"".We can trace back the projections on the right picture back on the left one. Find that $Y$ on the right pic is $Y\perp$ of the left, which is the residuals of regressing $Y$ by $X_2$. Likewise, $X_1$ on the right pic is $X_1\perp$ of the left, which is the residuals of regressing $X_1$ by $X_2$. Correlation between these two residual vectors is $r_{yx_1.x_2}$, as we know."
What is it meant with the $\sigma$-algebra generated by a random variable?,"
Often, in the course of my (self-)study of statistics, I've met the terminology ""$\sigma$-algebra generated by a random variable"". I don't understand the definition on Wikipedia, but most importantly I don't get the intuition behind it. Why/when do we need $\sigma-$algebras generated by random variables? What is their meaning? I know the following:

a $\sigma$-algebra on a set $\Omega$ is a nonempty collection of subsets of $\Omega$ which contains $\Omega$, is closed under complement and under countable union.
we introduce $\sigma$-algebras to build probability spaces on infinite sample spaces. In particular, if $\Omega$ is uncountably infinite, we know there can exist unmeasurable subsets (sets for which we cannot define a probability). Thus, we can't just use the power set of $\Omega$ $\mathcal{P}(\Omega)$ as our set of events $\mathcal{F}$. We need a smaller set, which is still large enough so that we can define the probability of interesting events, and we can talk about convergence of a sequence of random variables.

In short, I think I have a fair intuitive understanding of $\sigma-$algebras. I would like to have a similar understanding for the $\sigma-$algebras generated by random variables: definition, why we need them, intuition, an example...
","['probability', 'random-variable', 'sigma-algebra']","Consider a random variable $X$. We know that $X$ is nothing but a measurable function from $\left(\Omega, \mathcal{A} \right)$ into $\left(\mathbb{R}, \mathcal{B}(\mathbb{R}) \right)$, where $\mathcal{B}(\mathbb{R})$ are the Borel sets of the real line. By definition of measurability we know that we have $$X^{-1} \left(B \right) \in \mathcal{A}, \quad \forall B \in \mathcal{B}\left(\mathbb{R}\right)$$But in practice the preimages of the Borel sets may not be all of $\mathcal{A}$ but instead they may constitute a much coarser subset of it. To see this, let us define $$\mathcal{\Sigma} = \left\{ S \in \mathcal{A}: S = X^{-1}(B), \  B \in \mathcal{B}(\mathbb{R}) \right\}$$Using the properties of preimages, it is not too difficult to show that $\mathcal{\Sigma}$ is a sigma-algebra. It also follows immediately that $\mathcal{\Sigma} \subset \mathcal{A}$, hence $\mathcal{\Sigma}$ is a sub-sigma-algebra. Further, by the definitions it is easy to see that the mapping $X: \left( \Omega, \mathcal{\Sigma} \right) \to \left( \mathbb{R}, \mathcal{B} \left(\mathbb{R} \right) \right)$ is measurable.   $\mathcal{\Sigma}$ is in fact the smallest sigma-algebra that makes $X$ a random variable as all other sigma-algebras of that kind would at the very least include $\mathcal{\Sigma}$. For the reason that we are dealing with preimages of the random variable $X$, we call $\mathcal{\Sigma}$ the sigma-algebra induced by the random variable $X$.Here is an extreme example: consider a constant random variable $X$, that is, $X(\omega) \equiv \alpha$. Then $X^{-1} \left(B \right), \ B \in \mathcal{B} \left(\mathbb{R} \right)$ equals either $\Omega$ or $\varnothing$ depending on whether $\alpha \in B$. The sigma-algebra thus generated is trivial and as such, it is definitely included in $\mathcal{A}$.Hope this helps."
Comparing two models using anova() function in R,"
From the documentation for anova():

When given a sequence of objects, ‘anova’ tests the models against one another in the order specified...

What does it mean to test the models against one another? And why does the order matter?
Here is an example from the GenABEL tutorial:
    >  modelGen = lm(qt~snp1)
    >  modelAdd = lm(qt~as.numeric(snp1))
    >  modelDom = lm(qt~I(as.numeric(snp1)>=2))
    >  modelRec = lm(qt~I(as.numeric(snp1)>=3))
     anova(modelAdd, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ as.numeric(snp1)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(>Chi)
    1   2372 2320                      
    2   2371 2320  1    0.0489     0.82
     anova(modelDom, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ I(as.numeric(snp1) >= 2)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(>Chi)
    1   2372 2322                      
    2   2371 2320  1      1.77     0.18
     anova(modelRec, modelGen, test=""Chisq"")
    Analysis of Variance Table

    Model 1: qt ~ I(as.numeric(snp1) >= 3)
    Model 2: qt ~ snp1
      Res.Df  RSS Df Sum of Sq Pr(>Chi)  
    1   2372 2324                        
    2   2371 2320  1      3.53    0.057 .
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

How do I interpret this output?
","['r', 'anova']","When you use anova(lm.1,lm.2,test=""Chisq""), it performs the Chi-square test to compare lm.1 and lm.2 (i.e. it tests whether reduction in the residual sum of squares are statistically significant or not). Note that this makes sense only if lm.1 and lm.2 are nested models. For example, in the 1st anova that you used, the p-value of the test is 0.82. It means that the fitted model ""modelAdd"" is not significantly different from modelGen at the level of $\alpha=0.05$. However, using the p-value in the 3rd anova, the model ""modelRec"" is significantly different form model ""modelGen"" at $\alpha=0.1$. Check out ANOVA for Linear Model Fits as well."
How do you do bootstrapping with time series data?,"
I recently learned about using bootstrapping techniques to calculate standard errors and confidence intervals for estimators. What I learned was that if the data is IID, you can treat the sample data as the population, and do sampling with replacement and this will allow you to get multiple simulations of a test statistic.
In the case of time series, you clearly can't do this because autocorrelation is likely to exist. I have a time series and would like to calculate the mean of the data before and after a fixed date. Is there a correct way to do so using a modified version of bootstrapping? 
","['time-series', 'bootstrap']",
Motivation for Kolmogorov distance between distributions,"
There are many ways to measure how similar two probability distributions are.  Among methods which are popular (in different circles) are:

the Kolmogorov distance: the sup-distance between the distribution functions;
the Kantorovich-Rubinstein distance: the maximum difference between the expectations w.r.t. the two distributions of functions with Lipschitz constant $1$, which also turns out to be the $L^1$ distance between the distribution functions;
the bounded-Lipschitz distance: like the K-R distance but the functions are also required to have absolute value at most $1$.

These have different advantages and disadvantages.  Only convergence in the sense of 3. actually corresponds precisely to convergence in distribution; convergence in the sense of 1. or 2. is slightly stronger in general.  (In particular, if $X_n=\frac{1}{n}$ with probability $1$, then $X_n$ converges to $0$ in distribution, but not in the Kolmogorov distance. However, if the limit distribution is continuous then this pathology doesn't occur.) 
From the perspective of elementary probability or measure theory, 1. is very natural because it compares the probabilities of being in some set. A more sophisticated probabilistic perspective, on the other hand, tends to focus more on expectations than probabilities. Also, from the perspective of functional analysis, distances like 2. or 3. based on duality with some function space are very appealing, because there is a large set of mathematical tools for working with such things.
However, my impression (correct me if I'm wrong!) is that in statistics, the Kolmogorov distance is the usually preferred way of measuring similarity of distributions.  I can guess one reason: if one of the distributions is discrete with finite support -- in particular, if it is the distribution of some real-world data -- then the Kolmogorov distance to a model distribution is easy to compute.  (The K-R distance would be slightly harder to compute, and the B-L distance would probably be impossible in practical terms.)
So my question (finally) is, are there other reasons, either practical or theoretical, to favor the Kolmogorov distance (or some other distance) for statistical purposes?
","['distributions', 'probability', 'hypothesis-testing', 'mathematical-statistics']",
Survival Analysis tools in Python [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question appears to be off-topic because EITHER it is not about statistics, machine learning, data analysis, data mining, or data visualization, OR it focuses on programming, debugging, or performing routine operations within a statistical computing platform. If the latter, you could try the support links we maintain.


Closed 4 years ago.







                        Improve this question
                    



I am wondering if there are any packages for python that is capable of performing survival analysis. I have been using the survival package in R but would like to port my work to python. 
","['python', 'survival', 'mortality']","AFAIK, there aren't any survival analysis packages in python. As mbq comments above, the only route available would be to Rpy. Even if there were a pure python package available, I would be very careful in using it, in particular I would look at:One of the benefits of R, is that these standard packages get a massive amount of testing and user feed back. When dealing with real data, unexpected edge cases can creep in."
Why is median age a better statistic than mean age?,"

If you look at Wolfram Alpha



Or this Wikipedia page List of countries by median age


Clearly median seems to be the statistic of choice when it comes to ages.
I am not able to explain to myself why arithmetic mean would be a worse statistic. Why is it so?
Originally posted here because I did not know this site existed.
","['mean', 'median', 'types-of-averages']","Statistics does not provide a good answer to this question, in my opinion.  A mean can be relevant in mortality studies for example, but ages are not as easy to measure as you might think.  Older people, illiterate people, and people in some third-world countries tend to round their ages to a multiple of 5 or 10, for instance.The median is more resistant to such errors than the mean.  Moreover, median ages are typically 20 – 40, but people can live to 100 and more (an increasing and noticeable proportion of the population of modern countries now lives beyond 100).  People of such age have 1.5 to 4 times the influence on the mean than they do on the median compared to very young people.  Thus, the median is a bit more up-to-date statistic concerning a country's age distribution and is a little more independent of mortality rates and life expectancy than the mean is.Finally, the median gives us a slightly better picture of what the age distribution itself looks like: when you see a median of 35, for example, you know that half the population is older than 35 and you can infer some things about birth rates, ages of parents, and so on; but if the mean is 35, you can't say as much, because that 35 could be influenced by a large population bulge at age 70, for example, or perhaps a population gap in some age range due to an old war or epidemic.Thus, for demographic, not statistical, reasons, a median appears more worthy of the role of an omnibus value for summarizing the ages of relatively large populations of people."
How to interpret coefficients from a polynomial model fit?,"
I'm trying to create a second order polynomial fit to some data I have. Let's say I plot this fit with ggplot():
ggplot(data, aes(foo, bar)) + geom_point() + 
       geom_smooth(method=""lm"", formula=y~poly(x, 2))

I get:

So, a second order fit works quite well. I calculate it with R:
summary(lm(data$bar ~ poly(data$foo, 2)))

And I get:
lm(formula = data$bar ~ poly(data$foo, 2))
# ...
# Coefficients:
#                     Estimate Std. Error t value Pr(>|t|)    
# (Intercept)         3.268162   0.008282 394.623   <2e-16 ***
# poly(data$foo, 2)1 -0.122391   0.096225  -1.272    0.206
# poly(data$foo, 2)2  1.575391   0.096225  16.372   <2e-16 ***
# ....

Now, I would assume the formula for my fit is:
$$
\text{bar} = 3.268 - 0.122 \cdot \text{foo} + 1.575 \cdot \text{foo}^2
$$
But that just gives me the wrong values. For example, with $\text{foo}$ being 3 I would expect $\text{bar}$ to become something around 3.15. However, inserting into above formula I get:  
$$
\text{bar} = 3.268 - 0.122 \cdot 3 + 1.575 \cdot 3^2 = 17.077
$$
What gives? Am I incorrectly interpreting the coefficients of the model?
","['r', 'regression', 'interpretation', 'regression-coefficients']","My detailed answer is below, but the general (i.e. real) answer to this kind of question is: 1) experiment, mess around, look at the data, you can't break the computer no matter what you do, so ... experiment; or 2) read the documentation.Here is some R code which replicates the problem identified in this question, more or less:The first lm returns the expected answer:The second lm returns something odd:Since lm is the same in the two calls, it has to be the arguments of lm which are different.  So, let's look at the arguments.  Obviously, y is the same.  It's the other parts.  Let's look at the first few observations on the right-hand-side variables in the first call of lm.  The return of head(cbind(x,x^2)) looks like:This is as expected.  First column is x and second column is x^2.  How about the second call of lm, the one with poly?  The return of head(poly(x,2)) looks like:OK, that's really different.  First column is not x, and second column is not x^2.  So, whatever poly(x,2) does, it does not return x and x^2.  If we want to know what poly does, we might start by reading its help file.  So we say help(poly).  The description says:Returns or evaluates orthogonal polynomials of degree 1 to degree over the specified set of points x. These are all orthogonal to the constant polynomial of degree 0. Alternatively, evaluate raw polynomials.Now, either you know what ""orthogonal polynomials"" are or you don't.  If you don't, then use Wikipedia or Bing (not Google, of course, because Google is evil---not as bad as Apple, naturally, but still bad).  Or, you might decide you don't care what orthogonal polynomials are.  You might notice the phrase ""raw polynomials"" and you might notice a little further down in the help file that poly has an option raw which is, by default, equal to FALSE.  Those two considerations might inspire you to try out head(poly(x, 2, raw=TRUE)) which returns:Excited by this discovery (it looks right, now, yes?), you might go on to try summary(lm(y ~ poly(x, 2, raw=TRUE)))  This returns:There are at least two levels to the above answer.  First, I answered your question.  Second, and much more importantly, I illustrated how you are supposed to go about answering questions like this yourself.  Every single person who ""knows how to program"" has gone through a sequence like the one above sixty million times.  Even people as depressingly bad at programming as I am go through this sequence all the time.  It's normal for code not to work.  It's normal to misunderstand what functions do.  The way to deal with it is to screw around, experiment, look at the data, and RTFM.  Get yourself out of ""mindlessly following a recipe"" mode and into ""detective"" mode."
What is your favorite data visualization blog?,"










Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






What is the best blog on data visualization?
I'm making this question a community wiki since it is highly subjective.  Please limit each answer to one link.

Please note the following criteria for proposed answers: 

[A]cceptable answers to questions like this ...need to supply adequate descriptions and reasoned justification. A mere hyperlink doesn't do it. ...[A]ny future replies [must] meet ...[these] standards; otherwise, they will be deleted without further comment. 

","['data-visualization', 'references']",
Features for time series classification,"
I consider the problem of (multiclass) classification based on time series of variable length $T$, that is, to find a function
$$f(X_T) = y \in [1..K]\\
\text{for } X_T = (x_1, \dots, x_T)\\
\text{with } x_t \in \mathbb{R}^d ~,$$
via a global representation of the time serie by a set of selected features $v_i$ of fixed size $D$ independent of $T$,
$$\phi(X_T) = v_1, \dots, v_D \in \mathbb{R}~,$$
and then use standard classification methods on this feature set.
I'm not interested in forecasting, i.e. predicting $x_{T+1}$.
For example, we may analyse the way a person walks to predict the gender of the person.
What are the standard features that I may take into account ?
In example, we can obviously use the mean and variance of the serie (or higher order moments) and also look into the frequency domain, like the energy contained in some interval of the Discrete Fourier Transform of the serie (or Discrete Wavelet Transform).
","['time-series', 'classification', 'feature-selection', 'signal-processing']",
Difference between GradientDescentOptimizer and AdamOptimizer (TensorFlow)?,"
I've written a simple MLP in TensorFlow which is modelling a XOR-Gate.
So for:
input_data = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]

it should produce the following:
output_data = [[0.], [1.], [1.], [0.]]

The network has an input layer, a hidden layer and an output layer with 2, 5 and 1 neurons each.
Currently I have the following cross entropy:
cross_entropy = -(n_output * tf.log(output) + (1 - n_output) * tf.log(1 - output))

I've also tried this simpler alternative:
cross_entropy = tf.square(n_output - output)

alongside with some other tries.

However, no matter what my setup was, the error with a GradientDescentOptimizer was decreasing much slower than an AdamOptimizer.
In fact tf.train.AdamOptimizer(0.01) produced really good results after 400-800 learning steps (in dependency of the learning rate, where 0.01 had the best results) while tf.train.GradientDescentOptimizer always needed over 2000 learning steps no matter what cross entropy calculation or learning rate was used.
Why is this so? It seems the AdamOptimizer is always a better choice?!
","['machine-learning', 'neural-networks', 'error', 'gradient-descent', 'supervised-learning']","The tf.train.AdamOptimizer uses Kingma and Ba's Adam algorithm to control the learning rate. Adam offers several advantages over the simple tf.train.GradientDescentOptimizer. Foremost is that it uses moving averages of the parameters (momentum); Bengio discusses the reasons for why this is beneficial in Section 3.1.1 of this paper. Simply put, this enables Adam to use a larger effective step size, and the algorithm will converge to this step size without fine tuning.The main down side of the algorithm is that Adam requires more computation to be performed for each parameter in each training step (to maintain the moving averages and variance, and calculate the scaled gradient); and more state to be retained for each parameter (approximately tripling the size of the model to store the average and variance for each parameter). A simple tf.train.GradientDescentOptimizer could equally be used in your MLP, but would require more hyperparameter tuning before it would converge as quickly."
What is the difference between a loss function and an error function?,"
Is the term ""loss"" synonymous with ""error""? Is there a difference in definition?
Also, what is the origin of the term ""loss""?
NB: The error function mentioned here is not to be confused with normal error.
",['loss-functions'],"In the context of a predictive or inferential model, the term ""error"" generally refers to the deviation from an actual value by a prediction or expectation of that value.  It is determined entirely by the prediction mechanism and the actual behaviour of the quantities under observation.  The ""loss"" is a quantified measure of how bad it is to get an error of a particular size/direction, which is affected by the negative consequences that accrue for inaccurate prediction.An error function measures the deviation of an observable value from a prediction, whereas a loss function operates on the error to quantify the negative consequence of an error.  For example, in some contexts it might be reasonable to posit that there is squared error loss, where the negative consequence of an error is quantified as being proportional to the square of the error.  In other contexts we might be more negatively affected by an error in a particular direction (e.g., false positive vs. false negative) and therefore we might adopt a non-symmetric loss function.The error function is a purely statistical object, whereas the loss function is a decision-theoretic object that we are bringing in to quantify the negative consequences of error.  The latter is used in decision theory and economics (usually through its opposite - a cardinal utility function).An example: You are a criminal racketeer running an illegal betting parlour for the Mob.  Each week you have to pay 50% of the profits to the Mob boss, but since you run the place, the boss relies on you to give a true accounting of the profits.  If you have a good week you might be able to stiff him out of some dough by underrepresenting your profit, but if you underpay the boss, relative to what he suspects is the real profit, you’re a dead man.  So you want to predict how much he expects to get, and pay accordingly.  Ideally you will give him exactly what he is expecting, and keep the rest, but you could potentially make a prediction error, and pay him too much, or (yikes!) too little.You have a good week and earn $\pi = \$40,000$ in profit, so the boss is owed $\tfrac{1}{2} \pi = \$20,000$.  He doesn’t know what a good week you had, so his true expectation of his share is only $\theta = \$15,000$ (unknown to you).  You decide to pay him $\hat{\theta}$.  Then your error function is:$$\text{Error}(\hat{\theta}, \theta) = \hat{\theta} - \theta ,$$and (if we assume that loss is linear in money) your loss function is:$$\text{Loss}(\hat{\theta}, \theta) = \begin{cases}
\infty & & \text{if } \hat{\theta} < \theta \quad \text{(sleep wit' da fishes)} \\[6pt]
\hat{\theta} - \pi & & \text{if } \hat{\theta} \geqslant \theta \quad \text{(live to spend another week)} \\
\end{cases} $$This is an example of an asymmetric loss function (solution discussed in the comments below) which differs substantially from the error function.  The asymmetric nature of the loss function in this case stresses the catastrophic outcome in the case where there is underestimation of the unknown parameter."
Difference between forecast and prediction?,"
I was wondering what difference and relation are between forecast and prediction? Especially in time series and regression?
For example, am I correct that:

In time series, forecasting seems to mean to estimate a future values given past values of a time series.
In regression, prediction seems to mean to estimate a value whether it is future, current or past with respect to the given data.

","['regression', 'time-series', 'forecasting', 'terminology']",
"Is Amazon's ""average rating"" misleading?","
If I understand correctly, book ratings on a 1-5 scale are Likert scores. That is, a 3 for me may not necessarily be a 3 for someone else. It's an ordinal scale IMO. One shouldn't really average ordinal scales but can definitely take the mode, median and percentiles.
So is it 'okay' to bend the rules since the large part of the population understands means than the above statistics? Although the research community strongly rebukes taking averages of Likert scale based data, is it fine to do this with the masses (practically speaking)? Is taking the average in this case even misleading to start with?
Seems unlikely that a company like Amazon would fumble on basic statistics, but if not then what am I missing here? Can we claim that the ordinal scale is a convenient approximation to the ordinal to justify taking the mean? On what grounds?
","['mean', 'ordinal-data', 'likert']","As @gung mentioned I think there are often very good reasons for taking the mean of a five-point item as an index of central tendency. I have already outlined these reasons here.To paraphrase:Think about the goals of Amazon in reporting the mean. They might be aiming toAmazon provides some sort of rounded mean, frequency counts for each rating option, and the sample size (i.e., number of ratings). This information presumably is enough for most people to appreciate both the general sentiment regarding the item and the confidence in such a rating (i.e., a 4.5 with 20 ratings is more likely to be accurate than a 4.5 with 2 ratings; an item with 10 5-star ratings, and one 1-star rating with no comments might still be a good item).You could even see the mean as a democratic option. Many elections are decided based on which candidate gets the highest mean on a two-point scale. Similarly, if you take the argument that each person who submits a review gets a vote, then you can see the mean as a form that weights each person's vote equally.There are a wide range of rating biases known in the psychological literature (for a review, see Saal et al 1980), such as central tendency bias, leniency bias, strictness bias. Also, some raters will be more arbitrary and some will be more reliable. Some may even systematically lie giving fake positive or fake negative reviews. This will create various forms of error when trying to calculate the true mean rating for an item.However, if you were to take a random sample of the population, such biases would cancel out, and with a sufficient sample size of raters, you would still get the true mean.Of course, you don't get a random sample on Amazon, and there is the risk that the particular set of raters you get for an item is systematically biased to be more lenient or strict and so on. That said, I think users of Amazon would appreciate that user submitted ratings come from an imperfect sample. I also think that it's quite likely that with a reasonable sample size that in many cases, the majority of response bias differences would start to disappear. In terms of improving the accuracy of the rating, I wouldn't challenge the general concept of the mean, but rather I think there are other ways of estimating the true population mean rating for an item (i.e., the mean rating that would be obtained were a large representative sample asked to rate the item).Thus, if accuracy in rating was the primary goal of Amazon, I think it should endeavour to increase the number of ratings per item and adopt some of the above strategies. Such approaches might be particularly relevant when creating ""best-of"" rankings. However, for the humble rating on the page, it may well be that  the sample mean better meets the goals of simplicity and transparency. "
What are the factors that cause the posterior distributions to be intractable?,"
In Bayesian statistics, it is often mentioned that the posterior distribution is intractable and thus approximate inference must be applied. What are the factors that cause this intractability? 
","['bayesian', 'approximation', 'inference']",
Negative values for AIC in General Mixed Model [duplicate],"







This question already has answers here:
                                
                            




Negative values for AICc (corrected Akaike Information Criterion)

                                (5 answers)
                            

Closed 4 years ago.



I'm trying to select the best model by the AIC in the General Mixed Model test. The best model is the model with the lowest AIC, but all my AIC's are negative!

So is the biggest negative AIC the lowest value?
Or is the smallest negative AIC the lowest value, because it's closer to 0?

For example is AIC -201,928 or AIC -237,847 the lowest value and thus the best model?
","['aic', 'mixed-model']",
Percentage of overlapping regions of two normal distributions,"
I was wondering, given two normal distributions with $\sigma_1,\ \mu_1$ and $\sigma_2, \ \mu_2$

how can I calculate the percentage of overlapping regions of two distributions?
I suppose this problem has a specific name, are you aware of any particular name describing this problem? 
Are you aware of any implementation of this (e.g., Java code)?

","['normal-distribution', 'similarities', 'metric', 'bhattacharyya']","This is also often called the ""overlapping coefficient"" (OVL). Googling for this will give you lots of hits. You can find a nomogram for the bi-normal case here.  A useful paper may be:EditNow you got me interested in this more, so I went ahead and created R code to compute this (it's a simple integration). I threw in a plot of the two distributions, including the shading of the overlapping region:  For this example, the result is: 0.6099324 with absolute error < 1e-04. Figure below."
Intuitive difference between hidden Markov models and conditional random fields,"
I understand that HMMs (Hidden Markov Models) are generative models, and CRF are discriminative models. I also understand how CRFs (Conditional Random Fields) are designed and used. What I do not understand is how they are different from HMMs?  I read that in the case of HMM, we can only model our next state on the previous node, current node, and transition probability, but in the case of CRFs we can do this and can connect an arbitrary number of nodes together to form dependencies or contexts? Am I correct here?
","['machine-learning', 'hidden-markov-model', 'natural-language', 'conditional-random-field']",
Choosing variables to include in a multiple linear regression model,"
I am currently working to build a model using a multiple linear regression. After fiddling around with my model, I am unsure how to best determine which variables to keep and which to remove.
My model started with 10 predictors for the DV. When using all 10 predictors, four were considered significant. If I remove only some of the obviously-incorrect predictors, some of my predictors that were not initially significant become significant. Which leads me to my question: How does one go about determining which predictors to include in their model? It seemed to me you should run the model once with all predictors, remove those that are not significant, and then rerun. But if removing only some of those predictors makes others significant, I am left wondering if I am taking the wrong approach to all this.
I believe that this thread is similar to my question, but I am unsure I am interpreting the discussion correctly. Perhaps this is more of an experimental design topic, but maybe someone has some experience they can share.
","['regression', 'multiple-regression', 'feature-selection', 'modeling', 'model-selection']","Based on your reaction to my comment:You are looking for prediction. Thus, you should not really rely on (in)significance of the coefficients. You would be better toWrt each model of interest: herein lies quite a catch. With 10 potential predictors, that is a truckload of potential models. If you've got the time or the processors for this (or if your data is small enough so that models get fit and evaluated fast enough): have a ball. If not, you can go about this by educated guesses, forward or backward modelling (but using the criterion instead of significance), or better yet: use some algorithm that picks a reasonable set of models. One algorithm that does this, is penalized regression, in particular Lasso regression. If you're using R, just plug in the package glmnet and you're about ready to go."
PCA and the train/test split,"
I have a dataset for which I have multiple sets of binary labels. For each set of labels, I train a classifier, evaluating it by cross-validation. I want to reduce dimensionality using principal component analysis (PCA). My question is:
Is it possible to do the PCA once for the whole dataset and then use the new dataset of lower dimensionality for cross-validation as described above?
Or do I need to do a separate PCA for every training set (which would mean doing a separate PCA for every classifier and for every cross-validation fold)?
On one hand, the PCA does not make any use of the labels. On the other hand, it does use the test data to do the transformation, so I am afraid it could bias the results.
I should mention that in addition to saving me some work, doing the PCA once on the whole dataset would allow me to visualize the dataset for all label sets at once. If I have a different PCA for each label set, I would need to visualize each label set separately.
","['machine-learning', 'classification', 'pca', 'cross-validation']","For measuring the generalization error, you need to do the latter: a separate PCA for every training set (which would mean doing a separate PCA for every classifier and for every CV fold).You then apply the same transformation to the test set: i.e. you do not do a separate PCA on the test set! You subtract the mean (and if needed divide by the standard deviation) of the training set, as explained here: Zero-centering the testing set after PCA on the training set. Then you project the data onto the PCs of the training set.You'll need to define an automatic criterium for the number of PCs to use.
As it is just a first data reduction step before the ""actual"" classification, using a few too many PCs will likely not hurt the performance. If you have an expectation how many PCs would be good from experience, you can maybe just use that.You can also test afterwards whether redoing the PCA for every surrogate model was necessary (repeating the analysis with only one PCA model). I think the result of this test is worth reporting.I once measured the bias of not repeating the PCA, and found that with my  spectroscopic classification data, I detected only half of the generalization error rate when not redoing the PCA for every surrogate model.Also relevant: https://stats.stackexchange.com/a/240063/4598That being said, you can build an additional PCA model of the whole data set for descriptive (e.g. visualization) purposes. Just make sure you keep the two approaches separate from each other.I am still finding it difficult to get a feeling of how an initial PCA on the whole dataset would bias the results without seeing the class labels.But it does see the data. And if the between-class variance is large compared to the within-class variance, between-class variance  will influence the PCA projection. Usually the PCA step is done because you need to stabilize the classification. That is, in a situation where additional cases do influence the model.If between-class variance is small, this bias won't be much, but in that case neither would PCA help for the classification: the PCA projection then cannot help emphasizing the separation between the classes."
How does the Adam method of stochastic gradient descent work?,"
I'm familiar with basic gradient descent algorithms for training neural networks. I've read the paper proposing Adam: ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION.
While I've definitely got some insights (at least), the paper seems to be too high level for me overall. For example, a cost function $J(\theta)$ is often a sum of many different functions, therefore a vast amount of calculations have to be made to optimize its value; stochastic gradient descents - as far as I'm understanding the topic - calculate the optimization only for a subset of the these functions. To me it is unclear, how Adam does this and why this results in a decreased training error for the whole of $J(\theta)$.
I think Adam updates its gradient by taking into account the previous gradient. They call it something like utilizing the momentum? What exactly is this momentum? According to the algorithm on page two in the paper, it is some kind of moving average, like some estimates of the first and second moments of the ""regular"" gradient?
Practically, I would suspect that Adam enables one to use larger effective step sizes for decreasing the gradient and therefore the training error in combination with the stochastic approximation. Thus, the resulting update vector should ""jump"" around more in spatial dimensions, rather describing some curve like normal gradient descent algorithms would do.
Can someone de-mystify how Adam works? Especially how it converges, specifically why Adam's method work and what exactly the benefit is?
","['neural-networks', 'optimization', 'gradient-descent', 'adam']","The Adam paper says, ""...many objective functions are composed of a sum of subfunctions evaluated at different subsamples of data; in this case optimization can be made more efficient by taking gradient steps w.r.t.  individual subfunctions..."" Here, they just mean that the objective function is a sum of errors over training examples, and training can be done on individual examples or minibatches. This is the same as in stochastic gradient descent (SGD), which is more efficient for large scale problems than batch training because parameter updates are more frequent.As for why Adam works, it uses a few tricks.One of these tricks is momentum, which can give faster convergence. Imagine an objective function that's shaped like a long, narrow canyon that gradually slopes toward a minimum. Say we want to minimize this function using gradient descent. If we start from some point on the canyon wall, the negative gradient will point in the direction of steepest descent, i.e. mostly toward the canyon floor. This is because the canyon walls are much steeper than the gradual slope of the canyon toward the minimum. If the learning rate (i.e. step size) is small, we could descend to the canyon floor, then follow it toward the minimum. But, progress would be slow. We could increase the learning rate, but this wouldn't change the direction of the steps. In this case, we'd overshoot the canyon floor and end up on the opposite wall. We would then repeat this pattern, oscillating from wall to wall while making slow progress toward the minimum. Momentum can help in this situation.Momentum simply means that some fraction of the previous update is added to the current update, so that repeated updates in a particular direction compound; we build up momentum, moving faster and faster in that direction. In the case of the canyon, we'd build up momentum in the direction of the minimum, since all updates have a component in that direction. In contrast, moving back and forth across the canyon walls involves constantly reversing direction, so momentum would help to damp the oscillations in those directions.Another trick that Adam uses is to adaptively select a separate learning rate for each parameter. Parameters that would ordinarily receive smaller or less frequent updates receive larger updates with Adam (the reverse is also true). This speeds learning in cases where the appropriate learning rates vary across parameters. For example, in deep networks, gradients can become small at early layers, and it make sense to increase learning rates for the corresponding parameters. Another benefit to this approach is that, because learning rates are adjusted automatically, manual tuning becomes less important. Standard SGD requires careful tuning (and possibly online adjustment) of learning rates, but this less true with Adam and related methods. It's still necessary to select hyperparameters, but performance is less sensitive to them than to SGD learning rates.Related methods:Momentum is often used with standard SGD. An improved version is called Nesterov momentum or Nesterov accelerated gradient. Other methods that use automatically tuned learning rates for each parameter include: Adagrad, RMSprop, and Adadelta. RMSprop and Adadelta solve a problem with Adagrad that could cause learning to stop. Adam is similar to RMSprop with momentum. Nadam modifies Adam to use Nesterov momentum instead of classical momentum.References:Kingma and Ba (2014). Adam: A Method for Stochastic Optimization.Goodfellow et  al. (2016). Deep learning, chapter 8.Slides from Geoff Hinton's courseDozat (2016). Incorporating Nesterov Momentum into Adam."
Variational inference versus MCMC: when to choose one over the other?,"
I think I get the general idea of both VI and MCMC including the various flavors of MCMC like Gibbs sampling, Metropolis Hastings etc. This paper provides a wonderful exposition of both methods.
I have the following questions:

If I wish to do Bayesian inference, why would I choose one method over the other?
What are the pros and cons of each of the methods?

I understand that this is a pretty broad question, but any insights would be highly appreciated.
","['machine-learning', 'bayesian', 'markov-chain-montecarlo', 'variational-bayes', 'approximate-inference']","For a long answer, see Blei, Kucukelbir and McAuliffe here. This short answer draws heavily therefrom.Meaning, when we have computational time to kill and value precision of our estimates, MCMC wins. If we can tolerate sacrificing that for expediency—or we're working with data so large we have to make the tradeoff—VI is a natural choice.Or, as more eloquently and thoroughly described by the authors mentioned above:Thus, variational inference is suited to large data sets and scenarios where we want to
  quickly explore many models; MCMC is suited to smaller data sets and scenarios where
  we happily pay a heavier computational cost for more precise samples. For example, we
  might use MCMC in a setting where we spent 20 years collecting a small but expensive data
  set, where we are confident that our model is appropriate, and where we require precise
  inferences. We might use variational inference when fitting a probabilistic model of text to
  one billion text documents and where the inferences will be used to serve search results
  to a large population of users. In this scenario, we can use distributed computation and
  stochastic optimization to scale and speed up inference, and we can easily explore many
  different models of the data."
"AIC,BIC,CIC,DIC,EIC,FIC,GIC,HIC,IIC --- Can I use them interchangeably?","
On p. 34 of his PRNN Brian Ripley comments that ""The AIC was named by Akaike (1974) as 'An Information Criterion' although it seems commonly believed that the A stands for Akaike"". Indeed, when introducing the AIC statistic, Akaike (1974, p.719) explains that 
""IC stands for information criterion and A is added so that similar statistics, BIC, DIC
etc may follow"".

Considering this quotation as a prediction made in 1974, it is interesting to note 
that in just four years two types of the BIC statistic (Bayesian IC) were proposed by Akaike (1977, 1978) and Schwarz (1978). It took Spiegelhalter et al. (2002) much
longer to come up with DIC (Deviance IC). While the appearance of the CIC criterion
was not predicted by Akaike (1974), it would be naive to believe that it was never
contemplated. It was proposed by Carlos C. Rodriguez in 2005. (Note that R. Tibshirani
and K. Knight's CIC (Covariance Inflation Criterion) is a different thing.)
I knew that EIC (Empirical IC) was proposed by people of Monash University in around 2003. 
I've just discovered the Focused Information Criterion (FIC). Some books refer to Hannan and Quinn IC as HIC, see e.g. this one).  I know there should be GIC (Generalised IC) and I've just discovered the Information Investing Criterion (IIC). There is NIC, TIC and more. 
I think I could possibly cover the rest of the alphabet, so I am not asking where the sequence AIC,BIC,CIC,DIC,EIC,FIC,GIC,HIC,IIC,... stops, or what letters of the alphabet have not been used or been used at least twice (e.g. the E in EIC can stand for either Extended or Empirical). My question is simpler and I hope more practically useful. Can I use those statistics interchangeably, ignoring the specific assumptions they were derived under, the specific situations they were meant to be applicable in, and so on? 
This question is partly motivated by Burnham & Anderson (2001) writing that: 
...the comparison of AIC and BIC model selection ought to be based on their performance 
properties such as mean square error for parameter estimation (includes prediction) and 
confidence interval coverage: tapering effects or not, goodness-of-fit issues, 
derivation of theory is irrelevant as it can be frequentist or Bayes. 

Chapter 7 of Hyndman et al.'s monograph on exponential smoothing appears to follow 
the B-A advice when looking into how well the five alternative ICs (AIC, BIC, AICc, HQIC, LEIC) perform in selecting the model that forecasts best (as measured by a newly proposed error measure called MASE) to conclude that the AIC was a better alternative more often. (The HQIC was reported as the best model selector just once.)  
I am not sure what is the useful purpose of the research exercises that implicitly treat all 
ICc as though they were derived to answer one and the same question under equivalent sets of assumptions. In particular, I am not sure how it is useful to investigate the predictive performance of the consistent criterion for determining the order of an autoregression (that Hannan and Quinn derived for ergodic stationary sequences) by using it in the context of the non-stationary exponentially smoothing models described and analysed in the monograph by Hyndman et al. Am I missing something here?
References:
Akaike, H. (1974), A new look at the statistical model identification, IEEE Transactions
on Automatic Control 19(6), 716-723.
Akaike, H. (1977), On entropy maximization principle, in P. R. Krishnaiah, ed., Applications
of statistics, Vol. 27, Amsterdam: North Holland, pp. 27-41. 
Akaike, H. (1978), A Bayesian analysis of the minimum AIC procedure, Annals of the
Institute of Statistical Mathematics 30(1), 9-14.
Burnham, K. P. & Anderson, D. R. (2001) Kullback–Leibler information as a basis for strong 
inference in ecological studies, Wildlife Research 28, 111-119
Hyndman, R. J., Koehler, A. B., Ord, J. K. & Snyder, R. D. Forecasting with exponential smoothing: 
the state space approach. New York: Springer, 2008
Ripley, B.D. Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press, 1996
Schwarz, G. (1978), Estimating the dimension of a model, Annals of Statistics 6(2), 461-464.
Spiegelhalter, D. J., Best, N. G., Carlin, B. P. and van der Linde, A. (2002), Bayesian
measures of model complexity and t (with discussion), Journal of the Royal Statistical
Society. Series B (Statistical Methodology) 64(4), 583-639.
","['forecasting', 'model-selection', 'aic', 'bic']",
Are splines overfitting the data?,"
My problem: I recently met a statistician that informed me that splines are only useful for exploring data and are subjected to overfitting, thus not useful in prediction. He preferred exploring with simple polynomials ... As I’m a big fan of splines, and this goes against my intuition I’m interested in finding out how valid these arguments are, and if there is a large group of anti-spline-activists out there? 
Background: I try to follow Frank Harrell, Regression Modelling Strategies (1), when I create my models. He argues that restricted cubic splines are a valid tool for exploring continuous variables. He also argues that the polynomials are poor at modelling certain relationships such as thresholds, logarithmic (2). For testing the linearity of the model he suggests an ANOVA test for the spline:
$H_0: \beta_2 = \beta_3 = \dotsm = \beta_{k-1} = 0 $
I’ve googled for overfitting with splines but not found that much useful (apart from general warnings about not using too many knots). In this forum there seems to be a preference for spline modelling, Kolassa, Harrell, gung.
I found one blog post about polynomials, the devil of overfitting that talks about predicting polynomials. The post ends with these comments:

To some extent the examples presented here are cheating — polynomial
  regression is known to be highly non-robust.  Much better in practice
  is to use splines rather than polynomials.

Now this prompted me to check how splines would perform in with the example:
library(rms)
p4 <- poly(1:100, degree=4)
true4 <- p4 %*% c(1,2,-6,9)
days <- 1:70

set.seed(7987)
noise4 <- true4 + rnorm(100, sd=.5)
reg.n4.4 <- lm(noise4[1:70] ~ poly(days, 4))
reg.n4.4ns <- lm(noise4[1:70] ~ ns(days,4))
dd <- datadist(noise4[1:70], days)
options(""datadist"" = ""dd"")
reg.n4.4rcs_ols <- ols(noise4[1:70] ~ rcs(days,5))

plot(1:100, noise4)
nd <- data.frame(days=1:100)
lines(1:100, predict(reg.n4.4, newdata=nd), col=""orange"", lwd=3)
lines(1:100, predict(reg.n4.4ns, newdata=nd), col=""red"", lwd=3)
lines(1:100, predict(reg.n4.4rcs_ols, newdata=nd), col=""darkblue"", lwd=3)

legend(""top"", fill=c(""orange"", ""red"",""darkblue""), 
       legend=c(""Poly"", ""Natural splines"", ""RCS - ols""))

Gives the following image: 

In conclusion I have not found much that would convince me of reconsidering splines, what am I missing? 

F. E. Harrell, Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis, Softcover reprint of hardcover 1st ed. 2001. Springer, 2010.
F. E. Harrell, K. L. Lee, and B. G. Pollock, “Regression Models in Clinical Studies: Determining Relationships Between Predictors and Response,” JNCI J Natl Cancer Inst, vol. 80, no. 15, pp. 1198–1202, Oct. 1988.

Update
The comments made me wonder what happens within the data span but with uncomfortable curves. In most of the situations I'm not going outside the data boundary, as the example above indicates. I'm not sure this qualifies as prediction ... 
Anyway here's an example where I create a more complex line that cannot be translated into a polynomial. Since most observations are in the center of the data I tried to simulate that as well:
library(rms)
cmplx_line <-  1:200/10
cmplx_line <- cmplx_line + 0.05*(cmplx_line - quantile(cmplx_line, .7))^2
cmplx_line <- cmplx_line - 0.06*(cmplx_line - quantile(cmplx_line, .3))^2
center <- (length(cmplx_line)/4*2):(length(cmplx_line)/4*3)
cmplx_line[center] <- cmplx_line[center] + 
    dnorm(6*(1:length(center)-length(center)/2)/length(center))*10

ds <- data.frame(cmplx_line, x=1:200)

days <- 1:140/2

set.seed(1234)
sample <- round(rnorm(600, mean=100, 60))
sample <- sample[sample <= max(ds$x) & 
                     sample >= min(ds$x)]
sample_ds <- ds[sample, ]

sample_ds$noise4 <- sample_ds$cmplx_line + rnorm(nrow(sample_ds), sd=2)
reg.n4.4 <- lm(noise4 ~ poly(x, 6), data=sample_ds)
dd <- datadist(sample_ds)
options(""datadist"" = ""dd"")
reg.n4.4rcs_ols <- ols(noise4 ~ rcs(x, 7), data=sample_ds)
AIC(reg.n4.4)

plot(sample_ds$x, sample_ds$noise4, col=""#AAAAAA"")
lines(x=ds$x, y=ds$cmplx_line, lwd=3, col=""black"", lty=4)

nd <- data.frame(x=ds$x)
lines(ds$x, predict(reg.n4.4, newdata=ds), col=""orange"", lwd=3)
lines(ds$x, predict(reg.n4.4rcs_ols, newdata=ds), col=""lightblue"", lwd=3)

legend(""bottomright"", fill=c(""black"", ""orange"",""lightblue""), 
       legend=c(""True line"", ""Poly"", ""RCS - ols""), inset=.05)

This gives the following plot:

Update 2
Since this post I've published an article that looks into non-linearity for age on a large dataset. The supplement compares different methods and I've written a blog post about it. 
","['regression', 'splines']","Overfitting comes from allowing too large a class of models. This gets a bit tricky with models with continuous parameters (like splines and polynomials), but if you discretize the parameters into some number of distinct values, you'll see that increasing the number of knots/coefficients will increase the number of available models exponentially. For every dataset there is a spline and a polynomial that fits precisely, so long as you allow enough coefficients/knots. It may be that a spline with three knots overfits more than a polynomial with three coefficients, but that's hardly a fair comparison.If you have a low number of parameters, and a large dataset, you can be reasonably sure you're not overfitting. If you want to try higher numbers of parameters you can try cross validating within your test set to find the best number, or you can use a criterion like Minimum Description Length.EDIT: As requested in the comments, an example of how one would apply MDL. First you have to deal with the fact that your data is continuous, so it can't be represented in a finite code. For the sake of simplicity we'll segment the data space into boxes of side $\epsilon$ and instead of describing the data points, we'll describe the boxes that the data falls into. This means we lose some accuracy, but we can make $\epsilon$ arbitrarily small, so it doesn't matter much.Now, the task is to describe the dataset as sucinctly as possible with the help of some polynomial. First we describe the polynomial. If it's an n-th order polynomial, we just need to store (n+1) coefficients. Again, we need to discretize these values. After that we need to store first the value $n$ in prefix-free coding (so we know when to stop reading) and then the $n+1$ parameter values. With this information a receiver of our code could restore the polynomial. Then we add the rest of the information required to store the dataset. For each datapoint we give the x-value, and then how many boxes up or down the data point lies off the polynomial. Both values we store in prefix-free coding so that short values require few bits, and we won't need delimiters between points. (You can shorten the code for the x-values by only storing the increments between values)The fundamental point here is the tradeoff. If I choose a 0-order polynomial (like f(x) = 3.4), then the model is very simple to store, but for the y-values, I'm essentially storing the distance to the mean. More coefficients give me a better fitting polynomial (and thus shorter codes for the y values), but I have to spend more bits describing the model. The model that gives you the shortest code for your data is the best fit by the MDL criterion. (Note that this is known as 'crude MDL', and there are some refinements you can make to solve various technical issues)."
Is it possible to interpret the bootstrap from a Bayesian perspective?,"
Ok, this is a question that keeps me up at night.
Can the bootstrap procedure be interpreted as approximating some Bayesian procedure (except for the Bayesian bootstrap)?
I really like the Bayesian ""interpretation"" of statistics which I find nicely coherent and easy to understand. However, I also have a weakness for the bootstrap procedure which is so simple, yet delivers reasonable inferences in many situations. I would be more happy with bootstrapping, however, if I knew that the bootstrap was approximating a posterior distribution in some sense.
I know of the ""Bayesian bootstrap"" (Rubin, 1981), but from my perspective that version of the bootstrap is as problematic as the standard bootstrap. The problem is the really peculiar model assumption that you make, both when doing the classical and the Bayesian bootstrap, that is, the possible values of the distribution are only the values I've already seen. How can these strange model assumptions still yield the very reasonable inferences that bootstrap procedures yield? I have been looking for articles that have investigated the properties of the bootstrap (e.g. Weng, 1989) but I haven't found any clear explanation that I'm happy with.
References
Donald B. Rubin (1981). The Bayesian Bootstrap.
Ann. Statist. Volume 9, Number 1 , 130-134.
Chung-Sing Weng (1989). On a Second-Order Asymptotic Property of the Bayesian Bootstrap Mean.
The Annals of Statistics , Vol. 17, No. 2 , pp. 705-710.
","['bayesian', 'bootstrap']","Section 8.4 of The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman is ""Relationship Between the Bootstrap and Bayesian Inference."" That might be just what you are looking for. I believe that this book is freely available through a Stanford website, although I don't have the link on hand.Edit:Here is a link to the book, which the authors have made freely available online:http://www-stat.stanford.edu/~tibs/ElemStatLearn/On page 272, the authors write:In this sense, the bootstrap distribution represents an (approximate)
  nonparametric, noninformative posterior distribution for our
  parameter. But this bootstrap distribution is obtained
  painlessly — without having to formally specify a prior and without
  having to sample from the posterior distribution. Hence we might think
  of the bootstrap distribution as a “poor man’s” Bayes posterior. By
  perturbing the data, the bootstrap approximates the Bayesian effect
  of perturbing the parameters, and is typically much simpler to carry
  out.One more piece of the puzzle is found in this cross validated question which mentions the Dvoretzky–Kiefer–Wolfowitz inequality that ""shows [...] that the empirical distribution function converges uniformly to the true distribution function exponentially fast in probability.""So all in all the non-parametric bootstrap could be seen as an asymptotic method that produces ""an (approximate) nonparametric, noninformative posterior distribution for our parameter"" and where this approximation gets better ""exponentially fast"" as the number of samples increases."
How are kernels applied to feature maps to produce other feature maps?,"
I am trying to understand the convolution part of convolutional neural networks. Looking at the following figure:

I have no problems understanding the first convolution layer where we have 4 different kernels (of size $k \times k$), which we convolve with the input image to obtain 4 feature maps.
What I do not understand is the next convolution layer, where we go from 4 feature maps to 6 feature maps. I assume we have 6 kernels in this layer (consequently giving 6 output feature maps), but how do these kernels work on the 4 feature maps shown in C1? Are the kernels 3-dimensional, or are they 2-dimensional and replicated across the 4 input feature maps?
","['machine-learning', 'neural-networks', 'deep-learning', 'conv-neural-network']","The kernels are 3-dimensional, where width and height can be chosen, while the depth is equal to the number of maps in the input layer - in general. They are certainly not 2-dimensional and replicated across the input feature maps at the same 2D location! That would mean a kernel wouldn't be able to distinguish between its input features at a given location, since it would use one and the same weight across the input feature maps!"
Normality of dependent variable = normality of residuals?,"
This issue seems to rear its ugly head all the time, and I'm trying to decapitate it for my own understanding of statistics (and sanity!). 
The assumptions of general linear models (t-test, ANOVA, regression etc.) include the ""assumption of normality"", but I have found this is rarely described clearly. 
I often come across statistics textbooks / manuals / etc. simply stating that the ""assumption of normality"" applies to each group (i.e., categorical X variables), and we should we examining departures from normality for each group. 
Questions:  

does the assumption refer to the values of Y or the residuals of Y? 
for a particular group, is it possible to have a strongly non-normal distribution of Y values (e.g., skewed) BUT an approximately (or at least more normal) distribution of residuals of Y? 
Other sources describe that the assumption pertains to the residuals of the model (in cases where there are groups, e.g. t-tests / ANOVA), and we should be examining departures of normality of these residuals (i.e., only one Q-Q plot/test to run). 
does normality of residuals for the model imply normality of residuals for the groups? In other words, should we just examine the model residuals (contrary to instructions in many texts)?
To put this in a context, consider this hypothetical example:

I want to compare tree height (Y) between two populations (X).  
In one population the distribution of Y is strongly right-skewed (i.e.,
most trees short, very few tall), while the other is virtually normal 
Height is higher overall in the normally distributed population (suggesting there may be a 'real' difference).
Transformation of the data does not substantially improve the distribution of the first    population.

Firstly, is it valid to compare the groups given the radically different height distributions?
How do I approach the ""assumption of normality"" here? Recall height in one population is not normally distributed. Do I examine residuals for both populations separately OR residuals for the model (t-test)?


Please refer to questions by number in replies, experience has shown me people get lost or sidetracked easily (especially me!). Keep in mind I am not a statistician; though I have a reasonably conceptual (i.e., not technical!) understanding of statistics.
P.S., I have searched the archives and read the following threads which have not cemented my understanding:

ANOVA assumption normality/normal distribution of residuals
Normality of residuals vs sample data; what about t-tests?
Is normality testing 'essentially useless'?
Testing normality
Assessing normality of distribution
What tests do I use to confirm that residuals are normally distributed?
What to do when Kolmogorov-Smirnov test is significant for residuals of parametric test but skewness and kurtosis look normal?

","['normal-distribution', 'residuals', 'normality-assumption', 'faq']",
Can machine learning decode the SHA256 hashes?,"
I have a 64 character SHA256 hash.
I'm hoping to train a model that can predict if the plaintext used to generate the hash begins with a 1 or not.
Regardless if this is ""Possible"", what algorithm would be the best approach?
My initial thoughts:

Generate a large sample of hashes that begin with a 1 and a large sample of hashes that do not begin with a 1
Set each of the 64 characters of a hash as a parameter for some sort of unsupervised logistic regression model.
Train the model by telling it when it is right/wrong.
Hopefully be able to create a model that can predict if the plaintext begins with a 1 or not with a high enough accuracy (and with a decent kappa)

","['machine-learning', 'logistic']",
Expected number of ratio of girls vs boys birth,"
I have came across a question in job interview aptitude test for critical thinking. It is goes something like this:

The Zorganian Republic has some very strange customs. Couples only
  wish to have female children as only females can inherit the family's
  wealth, so if they have a male child they keep having more children
  until they have a girl. If they have a girl, they stop having
  children. What is the ratio of girls to boys in Zorgania?

I don't agree with the model answer given by the question writer, which is about 1:1. The justification was any birth will always have a 50% chance of being male or female.
Can you convince me with a more mathematical vigorous answer of $\text{E}[G]:\text{E}[B]$ if $G$ is the number of girls and B is the number of boys in the country?
","['probability', 'ratio']",
Would a Bayesian admit that there is one fixed parameter value?,"
In Bayesian data analysis, parameters are treated as random variables. This stems from the Bayesian subjective conceptualization of probability. But do Bayesians theoretically acknowledge that there is one true fixed parameter value out in the 'real world?'
It seems like the obvious answer is 'yes', because then trying to estimate the parameter would almost be nonsensical. An academic citation for this answer would be greatly appreciated.
","['probability', 'bayesian', 'parameterization']","IMHO ""yes""! Here is one of my favorite quotes by Greenland (2006: 767):It is often said (incorrectly) that ‘parameters are treated as fixed
  by the frequentist but as random by the Bayesian’. For frequentists
  and Bayesians alike, the value of a parameter may have been fixed from
  the start or may have been generated from a physically random
  mechanism. In either case, both suppose it has taken on some fixed
  value that we would like to know. The Bayesian uses formal probability
  models to express personal uncertainty about that value. The
  ‘randomness’ in these models represents personal uncertainty about the
  parameter’s value; it is not a property of the parameter (although we
  should hope it accurately reflects properties of the mechanisms that
  produced the parameter).Greenland, S. (2006). Bayesian perspectives for epidemiological research: I. Foundations and basic methods. International Journal of Epidemiology, 35(3), 765–774."
What are the cons of Bayesian analysis?,"
What are some practical objections to the use of Bayesian statistical methods in any context? No, I don't mean the usual carping about choice of prior. I'll be delighted if this gets no answers.
",['bayesian'],"I'm going to give you an answer. Four drawbacks actually. Note that none of these are actually objections that should drive one all the way to frequentist analysis, but there are cons to going with a Bayesian framework:None of these things should stop you. Indeed, none of these things have stopped me, and hopefully doing Bayesian analysis will help address at least number 4."
Are CDFs more fundamental than PDFs?,"
My stat prof basically said, if given one of the following three, you can find the other two:  

Cumulative distribution function 
Moment Generating Function 
Probability Density Function

But my econometrics professor said CDFs are more fundamental than PDFs because there are examples where you can have a CDF but the PDF isn't defined. 
Are CDFs more fundamental than PDFs? How do I know whether a PDF or a MGF can be derived from a CDF? 
","['probability', 'density-function', 'cumulative-distribution-function', 'moment-generating-function']","Every probability distribution on (a subset of) $\mathbb R^n$ has a cumulative distribution function, and it uniquely defines the distribution.  So, in this sense, the CDF is indeed as fundamental as the distribution itself.A probability density function, however, exists only for (absolutely) continuous probability distributions.  The simplest example of a distribution lacking a PDF is any discrete probability distribution, such as the distribution of a random variable that only takes integer values.Of course, such discrete probability distributions can be characterized by a probability mass function instead, but there are also distributions that have neither and PDF or a PMF, such as any mixture of a continuous and a discrete distribution:
(Diagram shamelessly stolen from Glen_b's answer to a related question.)There are even singular probability distributions, such as the Cantor distribution, which cannot be described even by a combination of a PDF and a PMF.  Such distributions still have a well defined CDF, though.  For example, here is the CDF of the Cantor distribution, also sometimes called the ""Devil's staircase"":
(Image from Wikimedia Commons by users Theon and Amirki, used under the CC-By-SA 3.0 license.)The CDF, known as the Cantor function, is continuous but not absolutely continuous.  In fact, it is constant everywhere except on a Cantor set of zero Lebesgue measure, but which still contains infinitely many points.  Thus, the entire probability mass of the Cantor distribution is concentrated on this vanishingly small subset of the real number line, but every point in the set still individually has zero probability.There are also probability distributions that do not have a moment-generating function.  Probably the best known example is the Cauchy distribution, a fat-tailed distribution which has no well-defined moments of order 1 or higher (thus, in particular, having no well-defined mean or variance!).All probability distributions on $\mathbb R^n$ do, however, have a (possibly complex-valued) characteristic function), whose definition differs from that of the MGF only by a multiplication with the imaginary unit.  Thus, the characteristic function may be regarded as being as fundamental as the CDF."
"Does correlation = 0.2 mean that there is an association ""in only 1 in 5 people""?","
In The Idiot Brain: A Neuroscientist Explains What Your Head is Really Up To, Dean Burnett wrote 

The correlation between height and intelligence is usually cited as
  being about $0.2$, meaning height and intelligence seem to be associated in only $1$ in $5$ people.

To me, this sound wrong: I understand the correlation more like the (lack of) error we get when we try to predict one measure (here intelligence) if the only thing we know about that person is the other measure (here height). If the correlation is $1$ or $-1$, then we don't make any error in our prediction, if the correlation is $0.8$, then there is more error. Thus the correlation would apply to anyone one, not just $1$ in $5$ people.
I have looked at this question but I am not good enough in maths to understand the answer. This answer which talks about the strength of the linear relationship seems in line which my understanding but I am not sure.
","['correlation', 'neuroscience']","The quoted passage is indeed incorrect. A correlation coefficient quantifies the degree of association throughout an entire population (or sample, in the case of the sample correlation coefficient). It does not divide the population into parts with one part showing an association and the other part not. It could be the case that the population actually consists of two subpopulations with different degrees of association, but a correlation coefficient alone doesn't imply this."
What is residual standard error?,"
When running a multiple regression model in R, one of the outputs is a residual standard error of 0.0589 on 95,161 degrees of freedom. I know that the 95,161 degrees of freedom is given by the difference between the number of observations in my sample and the number of variables in my model. What is the residual standard error?
","['regression', 'standard-error', 'residuals']","A fitted regression model uses the parameters to generate point estimate predictions which are the means of observed responses if you were to replicate the study with the same $X$ values an infinite number of times (and when the linear model is true). The difference between these predicted values and the ones used to fit the model are called ""residuals"" which, when replicating the data collection process, have properties of random variables with 0 means. The observed residuals are then used to subsequently estimate the variability in these values and to estimate the sampling distribution of the parameters. When the residual standard error is exactly 0 then the model fits the data perfectly (likely due to overfitting). If the residual standard error can not be shown to be significantly different from the variability in the unconditional response, then there is little evidence to suggest the linear model has any predictive ability."
What can we say about population mean from a sample size of 1?,"
I am wondering what we can say, if anything, about the population mean, $\mu$ when all I have is one measurement, $y_1$ (sample size of 1).  Obviously, we'd love to have more measurements, but we can't get them.
It seems to me that since the sample mean, $\bar{y}$, is trivially equal to $y_1$, then $E[\bar{y}]=E[y_1]=\mu$.  However, with a sample size of 1, the sample variance is undefined, and thus our confidence in using $\bar{y}$ as an estimator of $\mu$ is also undefined, correct?  Would there be any way to constrain our estimate of $\mu$ at all? 
","['mean', 'sample-size', 'small-sample', 'unbiased-estimator']","Here is a brand-new article on this question for the Poisson case, taking a nice pedagogical approach:Andersson. Per Gösta (2015). A Classroom Approach to the Construction of an Approximate Confidence Interval of a Poisson Mean Using One Observation. The American Statistician, 69(3), 160-164, DOI: 10.1080/00031305.2015.1056830."
What is model identifiability?,"
I know that with a model that is not identifiable the data can be said to be generated by multiple different assignments to the model parameters. I know that sometimes it's possible to constrain parameters so that all are identifiable, as in the example in Cassella & Berger 2nd ed, section 11.2.
Given a particular model, how can I evaluate whether or not it's identifiable?
",['identifiability'],"For identifiability we are talking about a parameter $\theta$ (which could be a vector), which ranges over a parameter space $\Theta$, and a family of distributions (for simplicity, think PDFs) indexed by $\theta$ which we typically write something like $\{ f_{\theta}|\, \theta \in \Theta\}$.  For instance, $\theta$ could be $\theta = \beta$ and $f$ could be$$
f_{\theta}(x) = \frac{1}{\beta}\mathrm{e}^{-x/\beta}, \ x>0,\ \beta >0,
$$
which would mean that $\Theta = (0,\infty)$.  In order for the model to be identifiable, the transformation which maps $\theta$ to $f_{\theta}$ should be one-to-one.  Given a model in your lap, the most straightforward way to check this is to start with the equation $f_{\theta_{1}} = f_{\theta_{2}}$, (this equality should hold for (almost) all $x$ in the support) and to try to use algebra (or some other argument) to show that just such an equation implies that, in fact, $\theta_{1} = \theta_{2}$.If you succeed with this plan, then your model is identifiable; go on with your business.  If you don't, then either your model isn't identifiable, or you need to find another argument.  The intuition is the same, regardless: in an identifiable model it is impossible for two distinct parameters (which could be vectors) to give rise to the same likelihood function.This makes sense, because if, for fixed data, two unique parameters gave rise to the same likelihood, then it would be impossible to distinguish between the two candidate parameters based on the data alone.  It would be impossible to identify the true parameter, in that case.For the example above, the equation $f_{\theta_{1}} = f_{\theta_{2}}$ is
$$
\frac{1}{\beta_{1}}\mathrm{e}^{-x/\beta_{1}} = \frac{1}{\beta_{2}}\mathrm{e}^{-x/\beta_{2}},
$$
for (almost) all $x > 0$. If we take logs of both sides we get
$$
-\ln\,\beta_{1} - \frac{x}{\beta_{1}} = -\ln\,\beta_{2} - \frac{x}{\beta_{2}}
$$
for $x > 0$, which implies the linear function
$$
-\left(\frac{1}{\beta_{1}} - \frac{1}{\beta_{2}}\right)x - (\ln\,\beta_{1} - \ln\,\beta_{2})
$$
is (almost) identically zero.  The only line which does such a thing is the one which has slope 0 and y-intercept zero.  Hopefully you can see the rest.By the way, if you can tell by looking at your model that it isn't identifiable (sometimes you can), then it is common to introduce additional constraints on it to make it identifiable (as you mentioned).  This is akin to recognizing that the function $f(y) = y^{2}$ isn't one-to-one for $y$ in $[-1,1]$, but it is one-to-one if we restrict $y$ to lie inside $[0,1]$.  In more complicated models the equations are tougher but the idea is the same."
What is the difference between errors and residuals?,"
While these two ubiquitous terms are often used synonymously, there sometimes seems to be a distinction. Is there indeed a difference, or are they exactly synonymous?
","['residuals', 'error', 'terminology']","Errors pertain to the true data generating process (DGP), whereas residuals are what is left over after having estimated your model.  In truth, assumptions like normality, homoscedasticity, and independence apply to the errors of the DGP, not your model's residuals.  (For example, having fit $p+1$ parameters in your model, only $N-(p+1)$ residuals can be independent.)  However, we only have access to the residuals, so that's what we work with.  "
Clarification on interpreting confidence intervals?,"
My current understanding of the notion ""confidence interval with confidence level $1 - \alpha$"" is that if we tried to calculate the confidence interval many times (each time with a fresh sample), it would contain the correct 
parameter $1 - \alpha$ of the time.
Though I realize that this is not the same as ""probability that the true parameter lies in this interval"", there's something I want to clarify.
[Major Update]
Before we calculate a 95% confidence interval, there is a 95% probability that the interval we calculate will cover the true parameter.  After we've calculated the confidence interval and obtained a particular interval $[a,b]$, we can no longer say this.  We can't even make some sort of non-frequentist argument that we're 95% sure the true parameter will lie in $[a,b]$; for if we could, it would contradict counterexamples such as this one: What, precisely, is a confidence interval?
I don't want to make this a debate about the philosophy of probability; instead, I'm looking for a precise, mathematical explanation of the how and why seeing the particular interval $[a,b]$ changes (or doesn't change) the 95% probability we had before seeing that interval.  If you argue that ""after seeing the interval, the notion of probability no longer makes sense"", then fine, let's work in an interpretation of probability in which it does make sense.
More precisely:
Suppose we program a computer to calculate a 95% confidence interval.  The computer does some number crunching, calculates an interval, and refuses to show me the interval until I enter a password.  Before I've entered the password and seen the interval (but after the computer has already calculated it), what's the probability that the interval will contain the true parameter?  It's 95%, and this part is not up for debate: this is the interpretation of probability that I'm interested in for this particular question (I realize there are major philosophical issues that I'm suppressing, and this is intentional).
But as soon as I type in the password and make the computer show me the interval it calculated, the probability (that the interval contains the true parameter) could change.  Any claim that this probability never changes would contradict the counterexample above.  In this counterexample, the probability could change from 50% to 100%, but...

Are there any examples where the probability changes to something other than 100% or 0% (EDIT: and if so, what are they)?
Are there any examples where the probability doesn't change after seeing the particular interval $[a,b]$ (i.e. the probability that the true parameter lies in $[a,b]$ is still 95%)?
How (and why) does the probability change in general after seeing the computer spit out $[a,b]$?

[Edit]
Thanks for all the great answers and helpful discussions!
",['confidence-interval'],"I think the fundamental problem is that frequentist statistics can only assign a probability to something that can have a long run frequency.  Whether the true value of a parameter lies in a particular interval or not doesn't have a long run frequency, becuase we can only perform the experiment once, so you can't assign a frequentist probability to it.  The problem arises from the definition of a probability.  If you change the definition of a probability to a Bayesian one, then the problem instantly dissapears as you are no longer tied to discussion of long run frequencies.  See my (rather tounge in cheek) answer to a related question here:""A Frequentist is someone that believes probabilies represent long run frequencies with which events ocurr; if needs be, he will invent a fictitious population from which your particular situation could be considered a random sample so that he can meaningfully talk about long run frequencies. If you ask him a question about a particular situation, he will not give a direct answer, but instead make a statement about this (possibly imaginary) population.""In the case of a confidence interval, the question we normally would like to ask (unless we have a problem in quality control for example) is ""given this sample of data, return the smallest interval that contains the true value of the parameter with probability X"".  However a frequentist can't do this as the experiment is only performed once and so there are no long run frequencies that can be used to assign a probability.  So instead the frequentist has to invent a population of experiments (that you didn't perform) from which the experiment you did perform can be considered a random sample.  The frequentist then gives you an indirect answer about that fictitious population of experiments, rather than a direct answer to the question you really wanted to ask about a particular experiment.Essentially it is a problem of language, the frequentist definition of a popuation simply doesn't allow discussion of the probability of the true value of a parameter lying in a particular interval.  That doesn't mean frequentist statistics are bad, or not useful, but it is important to know the limitations.Regarding the major updateI am not sure we can say that ""Before we calculate a 95% confidence interval, there is a 95% probability that the interval we calculate will cover the true parameter."" within a frequentist framework.  There is an implicit inference here that the long run frequency with which the true value of the parameter lies in confidence intervals constructed by some particular method is also the probability that that the true value of the parameter will lie in the confidence interval for the particular sample of data we are going to use.  This is a perfectly reasonable inference, but it is a Bayesian inference, not a frequentist one, as the probability that the true value of the parameter lies in the confidence interval that we construct for a particular sample of data has no long run freqency, as we only have one sample of data.  This is exactly the danger of frequentist statistics, common sense reasoning about probability is generally Bayesian, in that it is about the degree of plausibility of a proposition.We can however ""make some sort of non-frequentist argument that we're 95% sure the true parameter will lie in [a,b]"", that is exactly what a Bayesian credible interval is, and for many problems the Bayesian credible interval exactly coincides with the frequentist confidence interval.""I don't want to make this a debate about the philosophy of probability"", sadly this is unavoidable, the reason you can't assign a frequentist probability to whether the true value of the statistic lies in the confidence interval is a direct consequence of the frequentist philosophy of probability.  Frequentists can only assign probabilities to things that can have long run frequencies, as that is how frequentists define probability in their philosophy.  That doesn't make frequentist philosophy wrong, but it is important to understand the bounds imposed by the definition of a probability.""Before I've entered the password and seen the interval (but after the computer has already calculated it), what's the probability that the interval will contain the true parameter? It's 95%, and this part is not up for debate:""  This is incorrect, or at least in making such a statement, you have departed from the framework of frequentist statistics and have made a Bayesian inference involving a degree of plausibility in the truth of a statement, rather than a long run frequency.  However, as I have said earlier, it is a perfectly reasonable and natural inference.Nothing has changed before or after entering the password, because niether event can be assigned a frequentist probability.  Frequentist statistics can be rather counter-intuitive as we often want to ask questions about degrees of plausibility of statements regarding particular events, but this lies outside the remit of frequentist statistics, and this is the origin of most misinterpretations of frequentist procedures.  "
Importance of local response normalization in CNN,"
I've found that Imagenet and other large CNN makes use of local response normalization layers. However, I cannot find that much information about them. How important are they and when should they be used?
From http://caffe.berkeleyvision.org/tutorial/layers.html#data-layers:

""The local response normalization layer performs a kind of “lateral
  inhibition” by normalizing over local input regions. In
  ACROSS_CHANNELS mode, the local regions extend across nearby channels,
  but have no spatial extent (i.e., they have shape local_size x 1 x 1).
  In WITHIN_CHANNEL mode, the local regions extend spatially, but are in
  separate channels (i.e., they have shape 1 x local_size x local_size).
  Each input value is divided by (1+(α/n)∑ix2i)β, where n is the size of
  each local region, and the sum is taken over the region centered at
  that value (zero padding is added where necessary).""

Edit:
It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods. See my answer below for more details.
","['deep-learning', 'convolution', 'conv-neural-network']","It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods. This is what is written in the lecture notes for the Stanford Course CS321n on ConvNets: Normalization LayerMany types of normalization layers have been proposed for use in
  ConvNet architectures, sometimes with the intentions of implementing
  inhibition schemes observed in the biological brain. However, these
  layers have recently fallen out of favor because in practice their
  contribution has been shown to be minimal, if any. For various types
  of normalizations, see the discussion in Alex Krizhevsky's
  cuda-convnet library API."
Confidence interval around binomial estimate of 0 or 1,"
What is the best technique to calculate a confidence interval of a binomial experiment, if your estimate is that $p=0$ (or similarly $p=1$) and sample size is relatively small, for example $n=25$?
","['confidence-interval', 'binomial-distribution']","Much has been written about this problem. A general advice is to never use the normal approximation (i.e., the asymptotic/Wald confidence interval), as it has terrible coverage properties. R code for illustrating this:For small success probabilities, you might ask for a 95% confidence interval, but actually get, say, a 10% confidence interval!So what should we use? I believe the current recommendations are the ones listed in the paper Interval Estimation for a Binomial Proportion by Brown, Cai and DasGupta in Statistical Science 2001, vol. 16, no. 2, pages 101–133. The authors examined several methods for calculating confidence intervals, and came to the following conclusion.[W]e recommend the Wilson interval or the equal-tailed Jeffreys prior interval for small n and the interval suggested in Agresti and Coull for larger n.The Wilson interval is also sometimes called the score interval, since it’s based on inverting a score test.To calculate these confidence intervals, you can use this online calculator or the binom.confint() function in the binom package in R. For example, for 0 successes in 25 trials, the R code would be:Here bayes is the Jeffreys interval. (The argument type=""central"" is needed to get the equal-tailed interval.)Note that you should decide on which of the three methods you want to use before calculating the interval. Looking at all three and selecting the shortest will naturally give you too small coverage probability.As a final note, if you observe exactly zero successes in your n trials and just want a very quick approximate confidence interval, you can use the rule of three. Simply divide the number 3 by n. In the above example n is 25, so the upper bound is 3/25 = 0.12 (the lower bound is of course 0)."
What is the rationale of the Matérn covariance function?,"
The Matérn covariance function is commonly used as kernel function in Gaussian Process. It is defined like this
$$
{\displaystyle C_{\nu }(d)=\sigma ^{2}{\frac {2^{1-\nu }}{\Gamma (\nu )}}{\Bigg (}{\sqrt {2\nu }}{\frac {d}{\rho }}{\Bigg )}^{\nu }K_{\nu }{\Bigg (}{\sqrt {2\nu }}{\frac {d}{\rho }}{\Bigg )}}
$$
where $d$ is a distance function (such as Euclidean distance), $\Gamma$ is the gamma function, $K_\nu$ is the modified Bessel function of the second kind, $\rho$ and $\nu$ are positive parameters. $\nu$ is a lot of time chosen to be $\frac{3}{2}$ or $\frac{5}{2}$ in practice.
A lot of time this kernel works better than the standard Gaussian kernel as it is 'less smooth', but except that, are there any other reason why one would prefer this kernel? Some geometric intuition about how it behaves, or some explanation of the seemingly cryptic formula would be highly appreciated.
","['spatial', 'gaussian-process', 'kernel-trick']","In addition to @Dahn's nice answer, I thought I would try to say a little bit more about where the Bessel and Gamma functions come from. One starting point for arriving at the covariance function is Bochner's theorem.Theorem (Bochner) A continuous stationary function $k(x, y) = \widetilde{k}(|x − y|)$ is positive definite if and only if
$\widetilde{k}$ is the Fourier transform of a finite positive measure:
$$\widetilde{k}(t) = \int_{\mathbb{R}} e^{−iωt}\mathrm{d}µ(ω) .$$From this you can deduce that the Matérn covariance matrix is derived as the Fourier transform of $\frac{1}{(1+\omega^2)^p}$ (Source: Durrande). That's all good but it doesn't really tell us how you arrive at this finite positive measure given by $\frac{1}{(1+\omega^2)^p}$.  Well,
it's the (power) spectral density of a stochastic process $f(x)$.Which stochastic process? It's known that a random process on $\mathbb{R}^d$ with a Matérn covariance function is a solution to the stochastic partial differential equation (SPDE)
$$
(κ^2 − ∆)^{α/2} X(s) = φW(s),
$$
where $W(s)$ is Gaussian white noise with unit variance, $$\Delta = \sum_{i=1}^d \frac{\partial^2}{\partial x^2_i}$$ is the Laplace operator, and $α =ν + d/2$ (I think this is in Cressie and Wikle).Why pick this particular SPDE/stochastic process? The origin is in spatial statistics where it's argued that this is the simplest and natural covariance that works well in $\mathbb{R}^2$:The exponential correlation function is a natural correlation in one
dimension, since it corresponds to a Markov process. In two dimensions
this is no longer so, although the exponential is a common correlation
function in geostatistical work. Whittle (1954) determined the
correlation corresponding to a stochastic differential equation of
Laplace type:$$ \left[ \left(\frac{\partial}{\partial t_1}\right)^2 + \left(\frac{\partial}{\partial t_2}\right)^2 - \kappa^2 \right] X(t_1, t_2) = \epsilon(t_1 , t_2) $$
where $\epsilon$ is white noise. The corresponding discrete lattice process is a second order
autoregression. (Source: Guttorp&Gneiting)The family of processes included in the SDE associated with the Matérn equation includes the $AR(1)$ Ornstein–Uhlenbeck model of the velocity of
a particle undergoing Brownian motion. More generally, you can define a power spectrum for a family of $AR(p)$ processes for every integer $p$ which also have a Matérn family covariance. This is in the appendix of Rasmussen and Williams.This covariance function is not related to Matérn cluster process.ReferencesCressie, Noel, and Christopher K. Wikle. Statistics for spatio-temporal data. John Wiley & Sons, 2015.Guttorp, Peter, and Tilmann Gneiting. ""Studies in the history of probability and statistics XLIX On the Matern correlation family."" Biometrika 93.4 (2006): 989-995.Rasmussen, C. E.  and  Williams, C. K. I. Gaussian Processes for Machine Learning. the MIT Press, 2006."
Statistical test to tell whether two samples are pulled from the same population?,"
Let's say I have two samples. If I want to tell whether they are pulled from different populations, I can run a t-test. But let's say I want to test whether the samples are from the same population. How does one do this? That is, how do I calculate the statistical probability that these two samples were pulled from the same population?
",['statistical-significance'],"The tests that compare distributions are rule-out tests.  They start with the null hypothesis that the 2 populations are identical, then try to reject that hypothesis.  We can never prove the null to be true, just reject it, so these tests cannot really be used to show that 2 samples come from the same population (or identical populations).This is because there could be minor differences in the distributions (meaning they are not identical), but so small that tests cannot really find the difference.Consider 2 distributions, the first is uniform from 0 to 1, the second is a mixture of 2 uniforms, so it is 1 between 0 and 0.999, and also 1 between 9.999 and 10 (0 elsewhere).  So clearly these distributions are different (whether the difference is meaningful is another question), but if you take a sample size of 50 from each (total 100) there is over a 90% chance that you will only see values between 0 and 0.999 and be unable to see any real difference.There are ways to do what is called equivalence testing where you ask if the 2 distributions/populations are equivalent, but you need to define what you consider to be equivalent.  It is usually that some measure of difference is within a given range, i.e. the difference in the 2 means is less than 5% of the average of the 2 means, or the KS statistic is below a given cut-off, etc.  If you can then calculate a confidence interval for the difference statistic (difference of means could just be the t confidence interval, bootstrapping, simulation, or other methods may be needed for other statistics).  If the entire confidence interval falls in the ""equivalence region"" then we consider the 2 populations/distributions to be ""equivalent"". The hard part is figuring out what the equivalence region should be."
What is the derivative of the ReLU activation function?,"
What is the derivative of the ReLU activation function defined as:
$$ \mathrm{ReLU}(x) = \mathrm{max}(0, x)$$
What about the special case where there is a discontinuity in the function at $x=0$?
","['self-study', 'neural-networks']","The derivative is:$$ f(x)=
\begin{cases} 
0 & \text{if  }  x < 0 \\
1 & \text{if  }  x > 0 \\
\end{cases}
$$And undefined in $x=0$.The reason for it being undefined at $x=0$ is that its left- and right derivative are not equal."
Why do we need multivariate regression (as opposed to a bunch of univariate regressions)?,"
I just browsed through this wonderful book: Applied multivariate statistical analysis by Johnson and Wichern. The irony is, I am still not able to understand the motivation for using multivariate (regression) models instead of separate univariate (regression) models. I went through stats.statexchange posts 1 and 2 that explain (a) difference between multiple and multivariate regression and (b) interpretation of multivariate regression results, but I am not able to tweak out the use of multivariate statistical models from all the information I get online about them. 
My questions are:

Why do we need multivariate regression? What is the advantage of considering outcomes simultaneously rather than individually, in order to draw inferences.
When to use multivariate models and when to use multiple univariate models (for multiple outcomes). 
Take an example given in the UCLA site with three outcomes: locus of control, self-concept, and motivation. With respect to 1. and 2., can we compare the analysis when we do three univariate multiple regression versus one multivariate multiple regression? How to justify one over another?
I haven't come across many scholarly papers that utilize multivariate statistical models. Is this because of the multivariate normality assumption, the complexity of model fitting/interpretation or any other specific reason?

","['regression', 'multiple-regression', 'inference', 'multivariate-regression']","Be sure to read the full example on the UCLA site that you linked.Regarding 1:
Using a multivariate model helps you (formally, inferentially) compare coefficients across outcomes.
In that linked example, they use the multivariate model to test whether the write coefficient is significantly different for the locus_of_control outcome vs for the self_concept outcome. I'm no psychologist, but presumably it's interesting to ask whether your writing ability affects/predicts two different psych variables in the same way. (Or, if we don't believe the null, it's still interesting to ask whether you have collected enough data to demonstrate convincingly that the effects really do differ.)
If you ran separate univariate analyses, it would be harder to compare the write coefficient across the two models. Both estimates would come from the same dataset, so they would be correlated. The multivariate model accounts for this correlation.Also, regarding 4:
There are some very commonly-used multivariate models, such as Repeated Measures ANOVA . With an appropriate study design, imagine that you give each of several drugs to every patient, and measure each patient's health after every drug. Or imagine you measure the same outcome over time, as with longitudinal data, say children's heights over time. Then you have multiple outcomes for each unit (even when they're just repeats of ""the same"" type of measurement). You'll probably want to do at least some simple contrasts: comparing the effects of drug A vs drug B, or the average effects of drugs A and B vs placebo. For this, Repeated Measures ANOVA is an appropriate multivariate statistical model/analysis."
Where does $\sqrt{n}$ come from in central limit theorem (CLT)?,"
A very simple version of central limited theorem as below
$$
 \sqrt{n}\bigg(\bigg(\frac{1}{n}\sum_{i=1}^n X_i\bigg) - \mu\bigg)\ \xrightarrow{d}\ \mathcal{N}(0,\;\sigma^2)
$$
which is Lindeberg–Lévy CLT. I do not understand why there is a $\sqrt{n}$ on the left handside. And Lyapunov CLT says 
$$
\frac{1}{s_n} \sum_{i=1}^{n} (X_i - \mu_i) \ \xrightarrow{d}\ \mathcal{N}(0,\;1)
$$
but why not $\sqrt{s_n}$? Would anyone tell me what are these factors, such $\sqrt{n}$ and $\frac{1}{s_n}$? how do we get them in the theorem?
","['central-limit-theorem', 'intuition']","Nice question (+1)!!You will remember that for independent random variables $X$ and $Y$, $Var(X+Y) = Var(X) + Var(Y)$ and $Var(a\cdot X) = a^2 \cdot Var(X)$. So the variance of $\sum_{i=1}^n X_i$ is $\sum_{i=1}^n \sigma^2 = n\sigma^2$, and the variance of $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$ is $n\sigma^2 / n^2 = \sigma^2/n$.This is for the variance. To standardize a random variable, you divide it by its standard deviation. As you know, the expected value of $\bar{X}$ is $\mu$, so the variable$$ \frac{\bar{X} - E\left( \bar{X} \right)}{\sqrt{ Var(\bar{X}) }} = \sqrt{n} \frac{\bar{X} - \mu}{\sigma}$$ has expected value 0 and variance 1. So if it tends to a Gaussian, it has to be the standard Gaussian $\mathcal{N}(0,\;1)$. Your formulation in the first equation is equivalent. By multiplying the left hand side by $\sigma$ you set the variance to $\sigma^2$.Regarding your second point, I believe that the equation shown above illustrates that you have to divide by $\sigma$ and not $\sqrt{\sigma}$ to standardize the equation, explaining why you use $s_n$ (the estimator of $\sigma)$ and not $\sqrt{s_n}$.Addition: @whuber suggests to discuss the why of the scaling by $\sqrt{n}$. He does it there, but because the answer is very long I will try to capture the essense of his argument (which is a reconstruction of de Moivre's thoughts).If you add a large number $n$ of +1's and -1's, you can approximate the probability that the sum will be $j$ by elementary counting. The log of this probability is proportional to $-j^2/n$. So if we want the probability above to converge to a constant as $n$ goes large, we have to use a normalizing factor in $O(\sqrt{n})$.Using modern (post de Moivre) mathematical tools, you can see the approximation mentioned above by noticing that the sought probability is$$P(j) = \frac{{n \choose n/2+j}}{2^n} = \frac{n!}{2^n(n/2+j)!(n/2-j)!}$$which we approximate by Stirling's formula$$ P(j) \approx \frac{n^n e^{n/2+j} e^{n/2-j}}{2^n e^n (n/2+j)^{n/2+j} (n/2-j)^{n/2-j} } = \left(\frac{1}{1+2j/n}\right)^{n+j} \left(\frac{1}{1-2j/n}\right)^{n-j}. $$$$ \log(P(j)) = -(n+j) \log(1+2j/n) - (n-j) \log(1-2j/n) \\
\sim -2j(n+j)/n + 2j(n-j)/n \propto -j^2/n.$$"
Latent Class Analysis vs. Cluster Analysis - differences in inferences?,"
What are the differences in inferences that can be made from a latent class analysis (LCA) versus a cluster analysis?  Is it correct that a LCA assumes an underlying latent variable that gives rise to the classes, whereas the cluster analysis is an empirical description of correlated attributes from a clustering algorithm?  It seems that in the social sciences, the LCA has gained popularity and is considered methodologically superior given that it has a formal chi-square significance test, which the cluster analysis does not.  
It would be great if examples could be offered in the form of, ""LCA would be appropriate for this (but not cluster analysis), and cluster analysis would be appropriate for this (but not latent class analysis).
Thanks!
Brian
","['clustering', 'latent-variable', 'latent-class']","Latent Class Analysis is in fact an Finite Mixture Model (see here). The main difference between FMM and other clustering algorithms is that FMM's offer you a ""model-based clustering"" approach that derives clusters using a probabilistic model that describes distribution of your data. So instead of finding clusters with some arbitrary chosen distance measure, you use a model that describes distribution of your data and based on this model you assess probabilities that certain cases are members of certain latent classes. So you could say that it is a top-down approach (you start with describing distribution of your data) while other clustering algorithms are rather bottom-up approaches (you find similarities between cases).Because you use a statistical model for your data model selection and assessing goodness of fit are possible - contrary to clustering. Also, if you assume that there is some process or ""latent structure"" that underlies structure of your data then FMM's seem to be a appropriate choice since they enable you to model the latent structure behind your data (rather then just looking for similarities).Other difference is that FMM's are more flexible than clustering. Clustering algorithms just do clustering, while there are FMM- and LCA-based models thatFor more examples see:Hagenaars J.A. & McCutcheon, A.L. (2009). Applied Latent Class
  Analysis. Cambridge University Press.and the documentation of flexmix and poLCA packages in R, including the following papers:Linzer, D. A., & Lewis, J. B. (2011). poLCA: An R package for
  polytomous variable latent class analysis. Journal of Statistical
  Software, 42(10), 1-29.Leisch, F. (2004). Flexmix: A general framework for finite mixture
  models and latent glass regression in R. Journal of Statistical
  Software, 11(8), 1-18.Grün, B., & Leisch, F. (2008). FlexMix version 2: finite mixtures with
  concomitant variables and varying and constant parameters. Journal of
  Statistical Software, 28(4), 1-35."
How to interpret and report  eta squared / partial eta squared in statistically significant and non-significant analyses?,"
I have data that has eta squared values and partial eta squared values calculated as a measure of effect size for group mean differences.

What is the difference between eta squared and partial eta squared? Can they both be interpreted using the same Cohen's guidelines (1988 I think: 0.01 = small, 0.06 = medium, 0.13 = large)?
Also, is there use in reporting effect size if the comparison test (ie t-test or one-way ANOVA) is non-significant? In my head, this is like saying ""the mean difference did not reach statistical significance but is still of particular note because the effect size indicated from the eta squared is medium"". Or, is effect size a replacement value for significance testing, rather than complementary? 

","['anova', 'statistical-significance', 'effect-size']",
Can simple linear regression be done without using plots and linear algebra?,"
I'm completely blind and come from a programming background.
What I'm trying to do is to learn machine learning, and to do this, I first need to learn about linear regression. All the explanations on the Internet I am finding about this subject plot the data first. I'm looking for a practical explanation of linear regression that is not dependent on graphs and plots.
Here is my understanding of the aim of simple linear regression:
Simple linear regression is trying to find the formula that once you give X to it, would provide you with the closest estimation of Y.
So, as I understand it, what needs to be done is to compare the predictor (for example the area of a house in square feet) with the independent variable (the price). In my example, you probably can create a non-visual way of getting the best formula to calculate the price of a house from its area. For example, maybe you would get the area and price of 1000 houses in a neighborhood, and divide the price to the area? The result (at least in Iran which is where I live) would have a very negligible variance. So you'd probably get something like this:

Price = 2333 Rials * Area of the house

Of course, you would then need to go through all the 1000 houses in your data set, put the area in the formula above, compare the estimate with the real price, square the results (I guess to prevent variances from canceling each other out) and then get a number, then keep playing around with the 2333 to decrease the errors.
Of course, this is the brute force option where it will probably take ages to compute the errors and arrive at the best option, but you see what I'm saying? I didn't say anything about a graph, or a line, or points on a plot, or the best way of fitting a line to your existing data.
So, why would you need a scatter plot and linear algebra for this? Isn't there a non-visual way?
First, am I right in my assumptions? If not, I'd love to be corrected. Whether or not I am, though, is there a way to come up with the formula without playing around with linear algebra?
I would really appreciate it if I could get an example with the explanation, so that I can do it along with the text to test my understanding.
","['regression', 'intuition']","Yes your onto it. You have to keep playing around with the 2333 until you find the right one which minimizes the error. But there's a mathematical way to find the ""right"" one. Let's call that number $\beta$. $E$, the sum of the squared errors (SSE) is a function of $\beta$ since for each choice of $\beta$ can calculate the amount each estimate is off, square it, and sum them together. What $\beta$ minimizes the total sum of the squared errors? This is just a calculus problem. Take the derivative of $E$ by $\beta$ and set it equal to zero. This gives an equation for $\beta$. Check the second derivative is positive to know that it's a minimium. Thus you get an equation for $\beta$ which minimizes the error.If you derive it this way, you will get $\beta$ as a summation. If you write out the linear algebra form of the estimate you will see that this is the same thing.Edit: Here's a link to some notes with this type of derivation. The math gets a little messy, but at it's core it's just a calculus problem."
What is the relationship between the mean squared error and the residual sum of squares function?,"
Looking at the Wikipedia definitions of:

Mean Squared Error (MSE)
Residual Sum of Squares (RSS)

It looks to me that 
$$\text{MSE} = \frac{1}{N} \text{RSS} = \frac{1}{N} \sum (f_i -y_i)^2$$
where $N$ is he number of samples and $f_i$ is our estimation of $y_i$.
However, none of the Wikipedia articles mention this relationship. Why? Am I missing something?
","['residuals', 'mse']","Actually it's mentioned in the Regression section of Mean squared error in Wikipedia:In regression analysis, the term mean squared error is sometimes used
  to refer to the unbiased estimate of error variance: the residual sum
  of squares divided by the number of degrees of freedom.You can also find some informations here: Errors and residuals in statistics
It says the expression mean squared error may have different meanings in different cases, which is tricky sometimes."
"Is regression with L1 regularization the same as Lasso, and with L2 regularization the same as ridge regression? And how to write ""Lasso""?","
I'm a software engineer learning machine learning, particularly through Andrew Ng's machine learning courses. While studying linear regression with regularization, I've found terms that are confusing:

Regression with L1 regularization or L2 regularization 
LASSO 
Ridge regression

So my questions:

Is regression with L1 regularization exactly the same as LASSO?
Is regression with L2 regularization exactly the same as Ridge Regression?
How is ""LASSO"" used in writing? Should it be ""LASSO regression""? I've seen usage like ""the lasso is more appropriate"".

If the answer is ""yes"" for 1 and 2 above, then why are there different names for these two terms? Does ""L1"" and ""L2"" come from computer science / math, and ""LASSO"" and ""Ridge"" from stats?
The use of these terms is confusing when I see posts like:
""What is the difference between L1 and L2 regularization?"" (quora.com)
""When should I use lasso vs ridge?"" (stats.stackexchange.com)
","['regression', 'terminology', 'lasso', 'regularization', 'ridge-regression']","Yes.Yes.LASSO is actually an acronym (least absolute shrinkage and selection operator), so it ought to be capitalized, but modern writing is the lexical equivalent of Mad Max. On the other hand, Amoeba writes that even the statisticians who coined the term LASSO now use the lower-case rendering (Hastie, Tibshirani and Wainwright, Statistical Learning with Sparsity). One can only speculate as to the motivation for the switch. If you're writing for an academic press, they typically have a style guide for this sort of thing. If you're writing on this forum, either is fine, and I doubt anyone really cares.The $L$ notation is a reference to Minkowski norms and $L^p$ spaces. These just generalize the notion of taxicab and Euclidean distances to $p>0$ in the following expression:
$$
\|x\|_p=(|x_1|^p+|x_2|^p+...+|x_n|^p)^{\frac{1}{p}}
$$
 Importantly, only $p\ge 1$ defines a metric distance; $0<p<1$ does not satisfy the triangle inequality, so it is not a distance by most definitions.I'm not sure when the connection between ridge and LASSO was realized.As for why there are multiple names, it's just a matter that these methods developed in different places at different times. A common theme in statistics is that concepts often have multiple names, one for each sub-field in which it was independently discovered (kernel functions vs covariance functions, Gaussian process regression vs Kriging, AUC vs $c$-statistic). Ridge regression should probably be called Tikhonov regularization, since I believe he has the earliest claim to the method. Meanwhile, LASSO was only introduced in 1996, much later than Tikhonov's ""ridge"" method!"
Logistic regression model does not converge,"
I've got some data about airline flights (in a data frame called flights) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...
flights$BigDelay <- flights$ArrDelay >= 10
delay.model <- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)

...but got the following output.
> flights$BigDelay <- flights$ArrDelay >= 10
> delay.model <- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
> summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25

What does it mean that the algorithm did not converge? I thought it be because the BigDelay values were TRUE and FALSE instead of 0 and 1, but I got the same error after I converted everything. Any ideas?
","['r', 'logistic', 'separation']","glm() uses an iterative re-weighted least squares algorithm. The algorithm hit the maximum number of allowed iterations before signalling convergence. The default, documented in ?glm.control is 25. You pass control parameters as a list in the glm call:As @Conjugate Prior says, you seem to be predicting the response with the data used to generate it. You have complete separation as any ArrDelay < 10 will predict FALSE and any ArrDelay >= 10 will predict TRUE. The other warning message tells you that the fitted probabilities for some observations were effectively 0 or 1 and that is a good indicator you have something wrong with the model.The two warnings can go hand in hand. The likelihood function can be quite flat when some $\hat{\beta}_i$ get large, as in your example. If you allow more iterations, the model coefficients will diverge further if you have a separation issue."
scale a number between a range [duplicate],"







This question already has answers here:
                                
                            




How to normalize data to 0-1 range?

                                (7 answers)
                            


How to normalize data between -1 and 1?

                                (2 answers)
                            


Proper way to scale feature data

                                (1 answer)
                            


Normalize sample data for clustering

                                (2 answers)
                            


What's the difference between Normalization and Standardization?

                                (5 answers)
                            

Closed 6 years ago.



I have been trying to achieve a system which can scale a number down and in between two ranges. I have been stuck with the mathematical part of it.
What im thinking is lets say number 200 to be normalized so it falls between a range lets say  0 to 0.66 or 0.66 to 1 or 1 to 1.66. The range being variable as well.
Any help would be appreciated.
Thanks
","['normalization', 'scales']","Your scaling will need to take into account the possible range of the original number. There is a difference if your 200 could have been in the range [200,201] or in [0,200] or in [0,10000].So let Then$$ m\mapsto \frac{m-r_{\text{min}}}{r_{\text{max}}-r_{\text{min}}}\times (t_{\text{max}}-t_{\text{min}}) + t_{\text{min}}$$will scale $m$ linearly into $[t_{\text{min}},t_{\text{max}}]$ as desired.To go step by step,Next,
$$ m\mapsto \frac{m-r_{\text{min}}}{r_{\text{max}}-r_{\text{min}}} $$maps $m$ to the interval $[0,1]$, with $m=r_{\text{min}}$ mapped to $0$ and $m=r_{\text{max}}$ mapped to $1$.Multiplying this by $(t_{\text{max}}-t_{\text{min}})$ maps $m$ to $[0,t_{\text{max}}-t_{\text{min}}]$.Finally, adding $t_{\text{min}}$ shifts everything and maps $m$ to $[t_{\text{min}},t_{\text{max}}]$ as desired."
What exactly is the alpha in the Dirichlet distribution?,"
I'm fairly new to Bayesian statistics and I came across a corrected correlation measure, SparCC, that uses the Dirichlet process in the backend of it's algorithm.  I have been trying to go through the algorithm step-by-step to really understand what is happening but I am not sure exactly what the alpha vector parameter does in a Dirichlet distribution and how it normalizes the alpha vector parameter? 
The implementation is in Python using NumPy:
https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.dirichlet.html
The docs say:

alpha : array
       Parameter of the distribution (k dimension for sample of
       dimension k).

My questions:

How do the alphas affect the distribution?;
How are the alphas being normalized?;
and 
What happens when the alphas are not integers?

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Reproducibility
np.random.seed(0)

# Integer values for alphas
alphas = np.arange(10)
# array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

# Dirichlet Distribution
dd = np.random.dirichlet(alphas) 
# array([ 0.        ,  0.0175113 ,  0.00224837,  0.1041491 ,  0.1264133 ,
#         0.06936311,  0.13086698,  0.15698674,  0.13608845,  0.25637266])

# Plot
ax = pd.Series(dd).plot()
ax.set_xlabel(""alpha"")
ax.set_ylabel(""Dirichlet Draw"")


","['distributions', 'bayesian', 'dirichlet-distribution']","The Dirichlet distribution is a multivariate probability distribution that describes $k\ge2$ variables $X_1,\dots,X_k$, such that each $x_i \in (0,1)$ and $\sum_{i=1}^N x_i = 1$, that is parametrized by a vector of positive-valued parameters $\boldsymbol{\alpha} = (\alpha_1,\dots,\alpha_k)$. The parameters do not have to be integers, they only need to be positive real numbers. They are not ""normalized"" in any way, they are parameters of this distribution.The Dirichlet distribution is a generalization of the beta distribution into multiple dimensions, so you can start by learning about the beta distribution. Beta is a univariate distribution of a random variable $X \in (0,1)$ parameterized by parameters $\alpha$ and $\beta$. The nice intuition about it comes if you recall that it is a conjugate prior for the binomial distribution and if we assume a beta prior parameterized by $\alpha$ and $\beta$ for the binomial distribution's probability parameter $p$, then the posterior distribution of $p$ is also a beta distribution parameterized by $\alpha' = \alpha + \text{number of successes}$ and $\beta' = \beta + \text{number of failures}$. So you can think of $\alpha$ and $\beta$ as of pseudocounts (they do not need to be integers) of successes and failures (check also this thread).In the case of the Dirichlet distribution, it is a conjugate prior for the multinomial distribution. If in the case of the binomial distribution we can think of it in terms of drawing white and black balls with replacement from the urn, then in case of the multinomial distribution we are drawing with replacement $N$ balls appearing in $k$ colors, where each of colors of the balls can be drawn with probabilities $p_1,\dots,p_k$. The Dirichlet distribution is a conjugate prior for $p_1,\dots,p_k$ probabilities and $\alpha_1,\dots,\alpha_k$ parameters can be thought of as pseudocounts of balls of each color assumed a priori (but you should read also about the pitfalls of such reasoning). In Dirichlet-multinomial model $\alpha_1,\dots,\alpha_k$ get updated by summing them with observed counts in each category: $\alpha_1+n_1,\dots,\alpha_k+n_k$ in similar fashion as in case of beta-binomial model.The higher value of $\alpha_i$, the greater ""weight"" of $X_i$ and the greater amount of the total ""mass"" is assigned to it (recall that in total it must be $x_1+\dots+x_k=1$). If all $\alpha_i$ are equal, the distribution is symmetric. If $\alpha_i < 1$, it can be thought of as anti-weight that pushes away $x_i$ toward extremes, while when it is high, it attracts $x_i$ toward some central value (central in the sense that all points are concentrated around it, not in the sense that it is symmetrically central). If $\alpha_1 = \dots = \alpha_k = 1$, then the points are uniformly distributed.This can be seen on the plots below, where you can see trivariate Dirichlet distributions (unfortunately we can produce reasonable plots only up to three dimensions) parameterized by (a) $\alpha_1 = \alpha_2 = \alpha_3 = 1$, (b) $\alpha_1 = \alpha_2 = \alpha_3 = 10$, (c) $\alpha_1 = 1, \alpha_2 = 10, \alpha_3 = 5$, (d) $\alpha_1 = \alpha_2 = \alpha_3 = 0.2$.The Dirichlet distribution is sometimes called a ""distribution over distributions"" since it can be thought of as a distribution of probabilities themselves. Notice that since each $x_i \in (0,1)$ and $\sum_{i=1}^k x_i = 1$, then $x_i$'s are consistent with the first and second axioms of probability. So you can use the Dirichlet distribution as a distribution of probabilities for discrete events described by distributions such as categorical or multinomial. It is not true that it is a distribution over any distributions, for example it is not related to probabilities of continuous random variables, or even some discrete ones (e.g. a Poisson distributed random variable describes probabilities of observing values that are any natural numbers, so to use a Dirichlet distribution over their probabilities, you'd need an infinite number of random variables $k$)."
Why don't linear regression assumptions matter in machine learning?,"
When I learned linear regression in my statistics class, we are asked to check for a few assumptions which need to be true for linear regression to make sense. I won't delve deep into those assumptions, however, these assumptions don't appear when learning linear regression from machine learning perspective.
Is it because the data is so large that those assumptions are automatically taken care of? Or is it because of the loss function (i.e. gradient descent)?
","['regression', 'machine-learning', 'mathematical-statistics', 'least-squares']","It’s because statistics puts an emphasis on model inference, while machine learning puts an emphasis on accurate predictions.We like normal residuals in linear regression because then the usual $\hat{\beta}=(X^TX)^{-1}X^Ty$ is a maximum likelihood estimator.We like uncorrelated predictors because then we get tighter confidence intervals on the parameters than we would if the predictors were correlated.In machine learning, we often don’t care about how we get the answer, just that the result has a tight fit both in- and out-of-sample.Leo Breiman has a famous article on the ""two cultures"" of modeling: Breiman, Leo. ""Statistical modeling: The two cultures (with comments and a rejoinder by the author)."" Statistical science 16.3 (2001): 199-231."
Why do people use p-values instead of computing probability of the model given data?,"
Roughly speaking a p-value gives a probability of the observed outcome of an experiment given the hypothesis (model). Having this probability (p-value) we want to judge our hypothesis (how likely it is). But wouldn't it be more natural to calculate the probability of the hypothesis given the observed outcome?
In more details. We have a coin. We flip it 20 times and we get 14 heads (14 out of 20 is what I call ""outcome of experiment""). Now, our hypothesis is that the coin is fair (probabilities of head and tail are equal to each other). Now we calculate the p-value, that is equal to the probability to get 14 or more heads in 20 flips of coin. OK, now we have this probability (0.058) and we want to use this probability to judge our model (how is it likely that we have a fair coin). 
But if we want to estimate the probability of the model, why don't we calculate the probability of the model given the experiment? Why do we calculate the probability of the experiment given the model (p-value)?
","['likelihood', 'p-value']","Computing the probability that the hypothesis is correct doesn't fit well within the frequentist definition of a probability (a long run frequency), which was adopted to avoid the supposed subjectivity of the Bayesian definition of a probability.  The truth of a particular hypothesis is not a random variable, it is either true or it isn't and has no long run frequency.  It is indeed more natural to be interested in the probability of the truth of the hypothesis, which is IMHO why p-values are often misinterpreted as the probability that the null hypothesis is true.  Part of the difficulty is that from Bayes rule, we know that to compute the posterior probability that a hypothesis is true, you need to start with a prior probability that the hypothesis is true.A Bayesian would compute the probability that the hypothesis is true, given the data (and his/her prior belief).  Essentially in deciding between frequentist and Bayesian approaches is a choice whether the supposed subjectivity of the Bayesian approach is more abhorrent than the fact that the frequentist approach generally does not give a direct answer to the question you actually want to ask - but there is room for both.In the case of asking whether a coin is fair, i.e. the probability of a head is equal to the probability of a tail, we also have an example of a hypothesis that we know in the real world is almost certainly false right from the outset.  The two sides of the coin are non-symmetric, so we should expect a slight asymmetry in the probabilities of heads and tails, so if the coin ""passes"" the test, it just means we don't have enough observations to be able to conclude what we already know to be true - that the coin is very slightly biased!"
How do I avoid overlapping labels in an R plot? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question appears to be off-topic because EITHER it is not about statistics, machine learning, data analysis, data mining, or data visualization, OR it focuses on programming, debugging, or performing routine operations within a statistical computing platform. If the latter, you could try the support links we maintain.


Closed 7 years ago.







                        Improve this question
                    



I'm trying to label a pretty simple scatterplot in R. This is what I use:
plot(SI, TI)
text(SI, TI, Name, pos=4, cex=0.7)

The result is mediocre, as you can see (click to enlarge):

I tried to compensate for this using the textxy function, but it's not better. Making the image itself larger doesn't work for the dense clusters.
Is there any function or easy way to compensate for this and let R plot labels that don't overlap?

Here is a small subset of the data I have:
Name;SI;TI
01_BAD_talking_head;6.944714;4.421208
01_GOOD_talking_head;5.680141;4.864035
01_GOOD_talking_head_subtitles;7.170114;4.664205

","['r', 'data-visualization', 'scatterplot']","Check out the new package ggrepel.
ggrepel provides geoms for ggplot2 to repel overlapping text labels. It works both for geom_text and geom_label.Figure is taken from this blog post."
AIC guidelines in model selection,"
I typically use BIC as my understanding is that it values parsimony more strongly than does AIC. However, I have decided to use a more comprehensive approach now and would like to use AIC as well. I know that Raftery (1995) presented nice guidelines for BIC differences: 0-2 is weak, 2-4 is positive evidence for one model being better, etc. 
I looked in textbooks and they seem strange on AIC (it looks like a larger difference is weak and a smaller difference in AIC means one model is better). This goes against what I know I have been taught. My understanding is that you want lower AIC.
Does anyone know if Raftery's guidelines extend to AIC as well, or where I might cite some guidelines for ""strength of evidence"" for one model vs. another? 
And yes, cutoffs are not great (I kind of find them irritating) but they are helpful when comparing different kinds of evidence. 
","['r', 'model-selection', 'references', 'aic', 'bic']","AIC and BIC hold the same interpretation in terms of model comparison. That is, the larger difference in either AIC or BIC indicates stronger evidence for one model over the other (the lower the better). It's just the the AIC doesn't penalize the number of parameters as strongly as BIC. There is also a correction to the AIC (the AICc) that is used for smaller sample sizes. More information on the comparison of AIC/BIC can be found here. "
Simple examples of uncorrelated but not independent $X$ and $Y$,"
Any hard-working student is a counterexample to ""all students are lazy"".
What are some simple counterexamples to ""if random variables $X$ and $Y$ are uncorrelated then they are independent""?
","['correlation', 'random-variable', 'independence']",
Dice-coefficient loss function vs cross-entropy,"
When training a pixel segmentation neural network, such as a fully convolutional network, how do you make the decision to use the cross-entropy loss function versus Dice-coefficient loss function? 
I realize this is a short question, but not quite sure what other information to provide. I looked at a bunch of documentation about the two loss functions but am not able to get a intuitive sense of when to use one over the other.
","['neural-networks', 'loss-functions', 'cross-entropy']","One compelling reason for using cross-entropy over dice-coefficient or the similar IoU metric is that the gradients are nicer.The gradients of cross-entropy wrt the logits is something like $p - t$, where $p$ is the softmax outputs and $t$ is the target. Meanwhile, if we try to write the dice coefficient in a differentiable form: $\frac{2pt}{p^2+t^2}$ or $\frac{2pt}{p+t}$, then the resulting gradients wrt $p$ are much uglier: $\frac{2t(t^2-p^2)}{(p^2+t^2)^2}$ and $\frac{2t^2}{(p+t)^2}$. It's easy to imagine a case where both $p$ and $t$ are small, and the gradient blows up to some huge value. In general, it seems likely that training will become more unstable.The main reason that people try to use dice coefficient or IoU directly is that the actual goal is maximization of those metrics, and cross-entropy is just a proxy which is easier to maximize using backpropagation. In addition, Dice coefficient performs better at class imbalanced problems by design: However, class imbalance is typically taken care of simply by assigning loss multipliers to each class, such that the network is highly disincentivized to simply ignore a class which appears infrequently, so it's unclear that Dice coefficient is really necessary in these cases. I would start with cross-entropy loss, which seems to be the standard loss for training segmentation networks, unless there was a really compelling reason to use Dice coefficient."
How to calculate a confidence level for a Poisson distribution?,"
Would like to know how confident I can be in my $\lambda$. Anyone know of a way to set upper and lower confidence levels for a Poisson distribution? 

Observations ($n$) = 88 
Sample mean ($\lambda$) = 47.18182

what would the 95% confidence look like for this?
","['poisson-distribution', 'confidence-interval']","For Poisson, the mean and the variance are both $\lambda$. If you want the confidence interval around lambda, you can calculate the standard error as $\sqrt{\lambda / n}$. The 95-percent confidence interval is $\hat{\lambda} \pm 1.96\sqrt{\hat{\lambda} / n}$."
Intuitive explanation for density of transformed variable?,"
Suppose $X$ is a random variable with pdf $f_X(x)$. Then the random variable $Y=X^2$ has the pdf 
$$f_Y(y)=\begin{cases}\frac{1}{2\sqrt{y}}\left(f_X(\sqrt{y})+f_X(-\sqrt{y})\right) & y \ge 0 \\ 0 & y \lt 0\end{cases}$$ 
I understand the calculus behind this. But I'm trying to think of a way to explain it to someone who doesn't know calculus. In particular, I'm trying to explain why the factor $\frac{1}{\sqrt{y}}$ appears out front. I'll take a stab at it:
Suppose $X$ has a Gaussian distribution. Almost all the weight of its pdf is between the values, say, $-3$ and $3.$ But that maps to 0 to 9 for $Y$. So, the heavy weight in the pdf for $X$ has been extended across a wider range of values in the transformation to $Y$. Thus, for $f_Y(y)$ to be a true pdf the extra heavy weight must be downweighted by the multiplicative factor $\frac{1}{\sqrt{y}}$
How does that sound?
If anyone can provide a better explanation of their own or link to one in a document or textbook I'd greatly appreciate it. I find this variable transformation example in several intro mathematical probability/stats books. But I never find an intuitive explanation with it :(
","['random-variable', 'density-function', 'intuition']","PDFs are heights but they are used to represent probability by means of area.  It therefore helps to express a PDF in a way that reminds us that area equals height times base.Initially the height at any value $x$ is given by the PDF $f_X(x)$.  The base is the infinitesimal segment $dx$, whence the distribution (that is, the probability measure as opposed to the distribution function) is really the differential form, or ""probability element,""$$\operatorname{PE}_X(x) = f_X(x) \, dx.$$This, rather than the PDF, is the object you want to work with both conceptually and practically, because it explicitly includes all the elements needed to express a probability.When we re-express $x$ in terms of $y = x^2$, the base segments $dx$ get stretched (or squeezed): by squaring both ends of the interval from $x$ to $x + dx$ we see that the base of the $y$ area must be an interval of length$$dy = (x + dx)^2 - x^2 = 2 x \, dx + (dx)^2.$$Because the product of two infinitesimals is negligible compared to the infinitesimals themselves, we conclude$$dy = 2 x \, dx, \text{ whence }dx = \frac{dy}{2x} = \frac{dy}{2\sqrt{y}}.$$Having established this, the calculation is trivial because we just plug in the new height and the new width:$$\operatorname{PE}_X(x) = f_X(x) \, dx = f_X(\sqrt{y}) \frac{dy}{2\sqrt{y}} = \operatorname{PE}_Y(y).$$Because the base, in terms of $y$, is $dy$, whatever multiplies it must be the height, which we can read directly off the middle term as$$\frac{1}{2\sqrt{y}}f_X(\sqrt{y}) = f_Y(y).$$This equation $\operatorname{PE}_X(x) = \operatorname{PE}_Y(y)$  is effectively a conservation of area (=probability) law.This graphic accurately shows narrow (almost infinitesimal) pieces of two PDFs related by $y=x^2$.  Probabilities are represented by the shaded areas.  Due to the squeezing of the interval $[0.32, 0.45]$ via squaring, the height of the red region ($y$, at the left) has to be proportionally expanded to match the area of the blue region ($x$, at the right)."
"SVM, Overfitting, curse of dimensionality","
My dataset is small (120 samples), however the number of features are large varies from (1000-200,000). Although I'm doing feature selection to pick a subset of features, it might still overfit. 
My first question is, how does SVM handle overfitting, if at all. 
Secondly, as I study more about overfitting in case of classification, I came to the conclusion that even datasets with small number of features can overfit. If we do not have features correlated to the class label, overfitting takes place anyways. So I'm now wondering what's the point of automatic classification if we cannot find the right features for a class label. In case of document classification, this would mean manually crafting a thesaurus of words that relate to the labels, which is very time consuming. I guess what I'm trying to say is, without hand-picking the right features it is very difficult to build a generalized model ? 
Also, if the experimental results don't show that the results have low/no overfitting it becomes meaningless. Is there a way to measure it ? 
","['classification', 'svm']",
When should I balance classes in a training data set?,"
I had an online course, where I learned, that unbalanced classes in the training data might lead to problems, because classification algorithms go for the majority rule, as it gives good results if the unbalance is too much. In an assignment one had to balance the data via undersampling the majority class.
In this blog however, someone claims that balanced data is even worse:
https://matloff.wordpress.com/2015/09/29/unbalanced-data-is-a-problem-no-balanced-data-is-worse/
So which one is it? Should I balance the data or not? Does it depend on the algorithm used, as some might be able to adept to the unbalanced proportions of classes? If so, which ones are reliable on unbalanced data?
","['machine-learning', 'classification', 'unbalanced-classes']","The intuitive reasoning has been explained in the blogpost:If our goal is Prediction, this will cause a definite bias. And worse,
  it will be a permanent bias, in the sense that we will not have
  consistent estimates as the sample size grows.So, arguably the problem of (artificially) balanced data is worse than
  the unbalanced case.Balanced data are good for classification, but you obviously loose information about appearance frequencies, which is going to affect accuracy metrics themselves, as well as production performance.Let's say you're recognizing hand-written letters from English alphabet (26 letters). Overbalancing every letter appearance will give every letter a probability of being classified (correctly or not) roughly 1/26, so classifier will forget about actual distribution of letters in the original sample. And it's ok when classifier is able to generalize and recognize every letter with high accuracy. But if accuracy and most importantly generalization isn't ""so high"" (I can't give you a definition - you can think of it just as a ""worst case"") - the misclassified points will most-likely equally distribute among all letters, something like:As opposed to without balancing (assuming that ""A"" and ""C"" have much higher probabilities of appearance in text)So frequent cases will get fewer misclassifications. Whether it's good or not depends on your task. For natural text recognition, one could argue that letters with higher frequencies are more viable, as they would preserve semantics of the original text, bringing the recognition task closer to prediction (where semantics represent tendencies). But if you're trying to recognize something like screenshot of ECDSA-key (more entropy -> less prediction) - keeping data unbalanced wouldn't help. So, again, it depends.The most important distinction is that the accuracy estimate is, itself, getting biased (as you can see in the balanced alphabet example), so you don't know how the model's behavior is getting affected by most rare or most frequent points.P.S. You can always track performance of unbalanced classification with Precision/Recall metrics first and decide whether you need to add balancing or not.EDIT: There is additional confusion that lies in estimation theory precisely in the difference between sample mean and population mean. For instance, you might know (arguably) actual distribution of English letters in the alphabet $p(x_i | \theta)$, but your sample (training set) is not large enough to estimate it correctly (with $p(x_i | \hat \theta)$). So in order to compensate for a  $\hat \theta_i - \theta_i$, it is sometimes recommended to rebalance classes according to either population itself or parameters known from a larger sample (thus better estimator). However, in practice there is no guarantee that ""larger sample"" is identically distributed due to risk of getting biased data on every step (let's say English letters collected from technical literature vs fiction vs the whole library) so balancing could still be harmful.This answer should also clarify applicability criteria for balancing:The class imbalance problem is caused by there not being enough
  patterns belonging to the minority class, not by the ratio of positive
  and negative patterns itself per se. Generally if you have enough data, the ""class imbalance problem"" doesn't ariseAs a conclusion, artificial balancing is rarely useful if training set is large enough. Absence of statistical data from a larger identically distributed sample also suggests no need for artificial balancing (especially for prediction), otherwise the quality of estimator is as good as ""probability to meet a dinosaur"":What is the probability to meet a dinosaur out in the street?1/2 you either meet a dinosaur or you do not meet a dinosaur"
Is a time series the same as a stochastic process?,"
A stochastic process is a process that evolves over time, so is it really a fancier way of saying ""time series""?
","['time-series', 'stochastic-processes', 'definition']","Because many troubling discrepancies are showing up in comments and answers, let's refer to some authorities.James Hamilton does not even define a time series, but he is clear about what one is:... this set of $T$ numbers is only one possible outcome of the underlying stochastic process that generated the data.  Indeed, even if we were to imagine having observed the process for an infinite period of time, arriving at the sequence $$\{y_t\}_{t=\infty}^\infty = \{\ldots, y_{-1}, y_0, y_1, y_2, \ldots, y_T, y_{T+1}, y_{T+2}, \ldots, \},$$ the infinite sequence  $\{y_t\}_{t=\infty}^\infty$ would still be viewed as a single realization from a time series process. ...Imagine a battery of $I$ ... computers generating sequences $\{y_t^{(1)}\}_{t=-\infty}^{\infty},$ $\{y_t^{(2)}\}_{t=-\infty}^{\infty}, \ldots,$ $ \{y_t^{(I)}\}_{t=-\infty}^{\infty}$, and consider selecting the observation associated with date $t$ from each sequence: $$\{y_t^{(1)}, y_t^{(2)}, \ldots, y_t^{(I)}\}.$$  This would be described as a sample of $I$ realizations of the random variable $Y_t$. ...(Time Series Analysis, Chapter 3.)Thus, a ""time series process"" is a set of random variables $\{Y_t\}$ indexed by integers $t$.In Stochastic Differential Equations, Bernt Øksendal provides a standard mathematical definition of a general stochastic process:Definition 2.1.4. A stochastic process is a parametrized collection of random variables $$\{X_t\}_{t\in T}$$ defined on a probability space $(\Omega, \mathcal{F}, \mathcal{P})$ and assuming values in $\mathbb{R}^n$.The parameter space $T$ is usually (as in this book) the halfline $[0,\infty)$, but it may also be an interval $[a,b]$, the non-negative integers, and even subsets of $\mathbb{R}^n$ for $n\ge 1$.Putting the two together, we see that a time series process is a stochastic process indexed by integers.Some people use ""time series"" to refer to a realization of a time series process (as in the Wikipedia article).  We can see in Hamilton's language a reasonable effort to distinguish the process from the realization by his use of ""time series process,"" so that he can use ""time series"" to refer to realizations (or even data)."
Combining probabilities/information from different sources,"
Lets say I have three independent sources and each of them make predictions for the weather tomorrow. The first one says that the probability of rain tomorrow is 0, then the second one says that the probability is 1, and finally the last one says that the probability is 50%. I would like to know the total probability given that information.
If apply the multiplication theorem for independent events I get 0, which doesn't seem correct. Why is not possible to multiply all three if all sources are independent? Is there some Bayesian way to update the prior as I get new information?
Note: This is not homework, is something that I was thinking about.
","['probability', 'bayesian', 'pooling', 'model-averaging', 'forecast-combination']","You ask about three things: (a) how to combine several forecasts to get single forecast, (b) if Bayesian approach can be used in here, and (c) how to deal with zero-probabilities.Combining forecasts, is a common practice. If you have several forecasts than if you take average of those forecasts the resulting combined forecast should be better in terms of accuracy than any of the individual forecasts. To average them you could use weighted average where weights are based on inverse errors (i.e. precision), or information content. If you had knowledge on reliability of each source you could assign weights that are proportional to reliability of each source, so more reliable sources have greater impact on the final combined forecast. In your case you do not have any knowledge about their reliability so each of the forecasts have the same weight and so you can use simple arithmetic mean of the three forecasts$$ 0\%\times.33+50\%\times.33+100\%\times.33 = (0\%+50\%+100\%)/3=50\% $$As was suggested in comments by @AndyW and @ArthurB., other methods besides simple weighted mean are available. Many such methods are described in literature about averaging expert forecasts, that I was not familiar with before, so thanks guys. In averaging expert forecasts sometimes we want to correct for the fact that experts tend to regress to the mean (Baron et al, 2013), or make their forecasts more extreme (Ariely et al, 2000; Erev et al, 1994). To achieve this one could use transformations of individual forecasts $p_i$, e.g. logit function$$ \mathrm{logit}(p_i) = \log\left( \frac{p_i}{1-p_i} \right) \tag{1} $$odds to the $a$-th power$$ g(p_i) = \left( \frac{p_i}{1-p_i} \right)^a \tag{2} $$where $0 < a < 1$, or more general transformation of form$$ t(p_i) = \frac{p_i^a}{p_i^a + (1-p_i)^a} \tag{3} $$where if $a=1$ no transformation is applied, if $a>1$ individual forecasts are made more extreme, if $0 < a<1$ forecasts are made less extreme, what is shown on picture below (see Karmarkar, 1978; Baron et al, 2013).After such transformation forecasts are averaged (using arithmetic mean, median, weighted mean, or other method). If equations (1) or (2) were used results need to be back-transformed using inverse logit for (1) and inverse odds for (2). Alternatively, geometric mean can be used (see Genest and Zidek, 1986; cf. Dietrich and List, 2014)$$ \hat p = \frac{ \prod_{i=1}^N p_i^{w_i} }{ \prod_{i=1}^N p_i^{w_i} + \prod_{i=1}^N (1 - p_i)^{w_i} } \tag{4}$$or approach proposed by Satopää et al (2014)$$ \hat p = \frac{ \left[ \prod_{i=1}^N \left(\frac{p_i}{1-p_i} \right)^{w_i} \right]^a }{  1 + \left[ \prod_{i=1}^N \left(\frac{p_i}{1-p_i} \right)^{w_i} \right]^a } \tag{5}$$where $w_i$ are weights. In most cases equal weights $w_i = 1/N$ are used unless a priori information that suggests other choice exists. Such methods are used in averaging expert forecasts so to correct for under- or overconfidence. In other cases you should consider if transforming forecasts to more, or less extreme is justified since it can make resulting aggregate estimate fall out of the boundaries marked by the lowest and the greatest individual forecast.If you have a priori knowledge about rain probability you can apply Bayes theorem to update the forecasts given the a priori probability of rain in similar fashion as described in here. There is also a simple approach that could be applied, i.e. calculate weighted average of your $p_i$ forecasts (as described above) where prior probability $\pi$ is treated as additional data point with some prespecified weight $w_{\pi}$ as in this IMDB example (see also source, or here and here for discussion; cf. Genest and Schervish, 1985), i.e.$$ \hat p = \frac{ \left(\sum_{i=1}^N p_i w_i \right) + \pi w_{\pi} }{ \left(\sum_{i=1}^N w_i \right) + w_{\pi} } \tag{6}$$From your question however it does not follow that you have any a priori knowledge about your problem so you would probably use uniform prior, i.e. assume a priori $50\%$ chance of rain and this does not really change much in case of example that you provided.For dealing with zeros, there are several different approaches possible. First you should notice that $0\%$ chance of rain is not really reliable value, since it says that it is impossible that it will rain. Similar problems often occur in natural language processing when in your data you do not observe some values that possibly can occur (e.g. you count frequencies of letters and in your data some uncommon letter does not occur at all). In this case the classical estimator for probability, i.e.$$ p_i = \frac{n_i}{\sum_i n_i} $$where $n_i$ is a number of occurrences of $i$th value (out of $d$ categories), gives you $p_i = 0$ if $n_i = 0$. This is called zero-frequency problem. For such values you know that their probability is nonzero (they exist!), so this estimate is obviously incorrect. There is also a practical concern: multiplying and dividing by zeros leads to zeros or undefined results, so zeros are problematic in dealing with.The easy and commonly applied fix is, to add some constant $\beta$ to your counts, so that$$ p_i = \frac{n_i + \beta}{(\sum_i n_i) + d\beta} $$The common choice for $\beta$ is $1$, i.e. applying uniform prior based on Laplace's rule of succession, $1/2$ for Krichevsky-Trofimov estimate, or $1/d$ for Schurmann-Grassberger (1996) estimator. Notice however that what you do here is you apply out-of-data (prior) information in your model, so it gets subjective, Bayesian flavor. With using this approach you have to remember of assumptions you made and take them into consideration. The fact that we have strong a priori knowledge that there should not be any zero probabilities in our data directly justifies the Bayesian approach in here. In your case you do not have frequencies but probabilities, so you would be adding some very small value so to correct for zeros. Notice however that in some cases this approach may have bad consequences (e.g. when dealing with logs) so it should be used with caution.Schurmann, T., and P. Grassberger. (1996). Entropy estimation of symbol sequences. Chaos, 6, 41-427. Ariely, D., Tung Au, W., Bender, R.H., Budescu, D.V., Dietz, C.B., Gu, H., Wallsten, T.S. and  Zauberman, G. (2000). The effects of averaging subjective probability estimates between and within judges. Journal of Experimental Psychology: Applied, 6(2), 130.Baron, J., Mellers, B.A., Tetlock, P.E., Stone, E. and Ungar, L.H. (2014). Two reasons to make aggregated probability forecasts more extreme. Decision Analysis, 11(2), 133-145.Erev, I., Wallsten, T.S., and Budescu, D.V. (1994). Simultaneous over-and underconfidence: The role of error in judgment processes. Psychological review, 101(3), 519.Karmarkar, U.S. (1978). Subjectively weighted utility: A descriptive extension of the expected utility model. Organizational behavior and human performance, 21(1), 61-72.Turner, B.M., Steyvers, M., Merkle, E.C., Budescu, D.V., and Wallsten, T.S. (2014). Forecast aggregation via recalibration. Machine learning, 95(3), 261-289.Genest, C., and Zidek, J. V. (1986). Combining probability distributions: a
critique and an annotated bibliography. Statistical Science, 1, 114–135.Satopää, V.A., Baron, J., Foster, D.P., Mellers, B.A., Tetlock, P.E., and Ungar, L.H. (2014). Combining multiple probability predictions using a simple logit model. International Journal of Forecasting, 30(2), 344-356.Genest, C., and Schervish, M. J. (1985). Modeling expert judgments for Bayesian updating. The Annals of Statistics, 1198-1212.Dietrich, F., and List, C. (2014). Probabilistic Opinion Pooling. (Unpublished)"
"What is the difference between Metropolis-Hastings, Gibbs, Importance, and Rejection sampling?","
I have been trying to learn MCMC methods and have come across Metropolis-Hastings, Gibbs, Importance, and Rejection sampling. While some of these differences are obvious, i.e., how Gibbs is a special case of Metropolis-Hastings when we have the full conditionals, the others are less obvious, like when we want to use MH within a Gibbs sampler, etc. Does anyone have a simple way to see the bulk of the differences between each of these?
","['markov-chain-montecarlo', 'monte-carlo', 'gibbs', 'metropolis-hastings', 'importance-sampling']",
Gamma vs. lognormal distributions,"
I have an experimentally observed distribution that looks very similar to a gamma or lognormal distribution. I've read that the lognormal distribution is the maximum entropy probability distribution for a random variate $X$ for which the mean and variance of $\ln(X)$ are fixed. Does the gamma distribution have any similar properties?
","['density-function', 'gamma-distribution', 'lognormal-distribution']","As for qualitative differences, the lognormal and gamma are, as you say, quite similar.Indeed, in practice they're often used to model the same phenomena (some people will use a gamma where others use a lognormal). They are both, for example, constant-coefficient-of-variation models (the CV for the lognormal is $\sqrt{e^{\sigma^2} -1}$, for the gamma it's $1/\sqrt \alpha$).[How can it be constant if it depends on a parameter, you ask? It applies when you model the scale (location for the log scale); for the lognormal, the $\mu$ parameter acts as the log of a scale parameter, while for the gamma, the scale is the parameter that isn't the shape parameter (or its reciprocal if you use the shape-rate parameterization). I'll call the scale parameter for the gamma distribution $\beta$. Gamma GLMs model the mean ($\mu=\alpha\beta$) while holding $\alpha$ constant; in that case $\mu$ is also a scale parameter. A model with varying $\mu$ and constant $\alpha$ or $\sigma$ respectively will have constant CV.]You might find it instructive to look at the density of their logs, which often shows a very clear difference.The log of a lognormal random variable is ... normal. It's symmetric.The log of a gamma random variable is left-skew. Depending on the value of the shape parameter, it may be quite skew or nearly symmetric.Here's an example, with both lognormal and gamma having mean 1 and variance 1/4. The top plot shows the densities (gamma in green, lognormal in blue), and the lower one shows the densities of the logs:(Plotting the log of the density of the logs is also useful. That is, taking a log-scale on the y-axis above)This difference implies that the gamma has more of a tail on the left, and less of a tail on the right; the far right tail of the lognormal is heavier and its left tail lighter. And indeed, if you look at the skewness, of the lognormal and gamma, for a given coefficient of variation, the lognormal is more right skew ($\text{CV}^3+3\text{CV}$) than the gamma ($2\text{CV}$)."
Taking the expectation of Taylor series (especially the remainder),"
My question concerns trying to justify a widely-used method, namely taking the expected value of Taylor Series. Assume we have a random variable $X$ with positive mean $\mu$ and variance $\sigma^2$. Additionally, we have a function, say, $\log(x)$.
Doing Taylor Expansion of $\log X$ around the mean, we get
$$
\log X = \log\mu + \frac{X - \mu}{\mu} - \frac12 \frac{(X-\mu)^2}{\mu^2} + \frac13 \frac{(X - \mu)^3}{\xi_X^3},
$$
where, as usual, $\xi_X$ is s.t. $|\xi_X - \mu| < |X - \mu|$.
If we take an expectation, we will get an approximate equation which people usually refer to as something self-apparent (see the $\approx$ sign in the first equation here):
$$
\mathbb{E}\log X \approx \log \mu - \frac12 \frac{\sigma^2}{\mu^2}
$$
QUESTION: I'm interested in how to prove that the expected value of the remainder term is actually negligible, i.e. 
$$
\mathbb{E}\left[\frac{(X - \mu)^3}{\xi_X^3}\right] = o(\sigma^2)
$$
(or, in other words, $\mathbb{E}\bigl[o(X-\mu)^2\bigr] = o\bigl(\mathbb{E}\bigl[(X-\mu)^2\bigr]\bigr)$).
What I tried to do: assuming that $\sigma^2 \to 0$ (which, in turn, means $X \to \mu$ in $\mathbb{P}$), I tried to split the integral into two, surrounding $\mu$ with some $\varepsilon$-vicinity $N_\varepsilon$:
$$
\int_\mathbb{R} p(x)\frac{(x-\mu)^3}{\xi_x^3} \,dx = \int_{x \in N_\varepsilon} \ldots dx + \int_{x \notin N_\varepsilon} \ldots dx
$$
The first one can be bounded due to the fact that $0 \notin N_\varepsilon$ and thus $1/\xi^3$ doesn't bother. But with the second one we have two concurring facts: on the one hand
$$
\mathbb{P}(|X - \mu| > \varepsilon) \to 0
$$ 
(as $\sigma^2 \to 0$). But on the other hand, we don't know what to do with $1/\xi^3$.
Another possibility could be to try using the Fatou's lemma, but I can't figure out how.
Will appreciate any help or hint. I realize that this is sort of a very technical question, but I need to go through it in order to trust this ""Taylor-expectation"" method. Thanks!
P.S. I checked out here, but seems it's a bit of another stuff.
","['self-study', 'mathematical-statistics', 'expected-value']","You are right to be skeptical of this approach. The Taylor series method does not work in general, although the heuristic contains a kernel of truth. To summarize the technical discussion below,As Alecos's answer indicates, this suggests that the Taylor-series method should be scrapped if your data might have heavy tails. (Finance professionals, I'm looking at you.) As Elvis noted, key problem is that the variance does not control higher moments.  To see why, let's simplify your question as much as possible to get to the main idea.  Suppose we have a sequence of random variables $X_n$ with $\sigma(X_n)\to 0$ as $n\to \infty$.Q: Can we guarantee that $\mathbb{E}[|X_n-\mu|^3] = o(\sigma^2(X_n))$ as $n\to \infty?$Since there are random variables with finite second moments and infinite third moments, the answer is emphatically no.  Therefore, in general, the Taylor series method fails even for 3rd degree polynomials. Iterating this argument shows you cannot expect the Taylor series method to provide accurate results, even for polynomials, unless all moments of your random variable are well controlled.What, then, are we to do? Certainly the method works for bounded random variables whose support converges to a point, but this class is far too small to be interesting.  Suppose instead that the sequence $X_n$ comes from some highly concentrated family that satisfies (say)$$\mathbb{P}\left\{ |X_n-\mu|> t\right\} \le \mathrm{e}^{- C n t^2} \tag{1}$$for every $t>0$ and some $C>0$. Such random variables are surprisingly common. For example when $X_n$ is the empirical mean$$ X_n := \frac{1}{n} \sum_{i=1}^n Y_i$$of nice random variables $Y_i$ (e.g., iid and bounded), various concentration inequalities imply that $X_n$ satisfies (1). A standard argument (see p. 10 here) bounds the $p$th moments for such random variables:$$ \mathbb{E}[|X_n-\mu|^p] \le \left(\frac{p}{2 C n}\right)^{p/2}.$$Therefore, for any ""sufficiently nice"" analytic function $f$ (see below), we can bound the error $\mathcal{E}_m$ on the $m$-term Taylor series approximation using the triangle inequality$$ \mathcal{E}_m:=\left|\mathbb{E}[f(X_n)] - \sum_{p=0}^m \frac{f^{(p)}(\mu)}{p!} \mathbb{E}(X_n-\mu)^p\right|\le \tfrac{1}{(2 C n)^{(m+1)/2}} \sum_{p=m+1}^\infty |f^{(p)}(\mu)| \frac{p^{p/2}}{p!}$$when $n>C/2$.  Since Stirling's approximation gives $p! \approx p^{p-1/2}$, the error of the truncated Taylor series satisfies$$ \mathcal{E}_m = O(n^{-(m+1)/2}) \text{ as } n\to \infty\quad \text{whenever} \quad \sum_{p=0}^\infty p^{(1-p)/2 }|f^{(p)}(\mu)| < \infty \tag{2}.$$Hence, when $X_n$ is strongly concentrated and $f$ is sufficiently nice, the Taylor series approximation is indeed accurate. The inequality appearing in (2) implies that $f^{(p)}(\mu)/p! = O(p^{-p/2})$, so that in particular our condition requires that $f$ is entire.  This makes sense because (1) does not impose any boundedness assumptions on $X_n$. Let's see what can go wrong when $f$ is has a singularity (following whuber's comment). Suppose that we choose $f(x)=1/x$.  If we take $X_n$ from the $\mathrm{Normal}(1,1/n)$ distribution truncated between zero and two, then $X_n$ is sufficiently concentrated but $\mathbb{E}[f(X_n)] = \infty$ for every $n$.  In other words, we have a highly concentrated, bounded random variable, and still the Taylor series method fails when the function has just one singularity.A few words on rigor.  I find it nicer to present the condition appearing in (2) as derived rather than a deus ex machina that's required in a rigorous theorem/proof format.  In order to make the argument completely rigorous, first note that the right-hand side in (2)  implies that $$\mathbb{E}[|f(X_n)|] \le \sum_{i=0}^\infty \frac{|f^{(p)}(\mu)|}{p!} \mathbb{E}[|X_n-\mu|^p]< \infty$$by the growth rate of subgaussian moments from above.  Thus, Fubini's theorem provides$$ \mathbb{E}[f(X_n)] = \sum_{i=0}^\infty \frac{f^{(p)}(\mu)}{p!} \mathbb{E}[(X_n-\mu)^p]$$The rest of the proof proceeds as above."
Entropy of an image,"
What is the most information/physics-theoretical correct way to compute the entropy of an image? I don't care about computational efficiency right now - I want it theoretically as correct as possible.
Lets start with a gray-scale image. One intuitive approach is to consider the image as a bag of pixels and compute
$$
H = - \sum_k p_k log_2(p_k)
$$
where $K$ is the number of gray levels and $p_k$ is the probability associated with gray level $k$.
There are two problems with this definition:

It works for one band (i.e. gray-scale), but how should one extend it in a statistically correct way to multiple bands? For example, for 2 bands, should one base oneself on $(X_1,X_2)$ and thus on PMF using $P(X_1=x_1,X_2=x_2)$? If one has many ($B$>>2) bands then $P(X_1=x_1, ..., X_B=x_B) \sim 1/N^B \rightarrow H_{MAX}$, which seems wrong.
Spatial information is not taken into account. For example, the images below (custody of John Loomis) have the same $H$, although clearly they do not convey the same information. 


Anyone care to explain or give advice, or refer me to some decent reference material about the subject? I am mainly interested in a theoretically correct approach of the second problem (i.e. spatial information).
","['spatial', 'entropy', 'image-processing']",
Where to start with statistics for an experienced developer,"
During the first half of 2015 I did the coursera course of Machine Learning (by Andrew Ng, GREAT course). And learned the basics of machine learning (linear regression, logistic regression, SVM, Neuronal Networks...)
Also I have been a developer for 10 years, so learning a new programming language would not be a problem.
Lately, I have started learning R in order to implement machine learning algorithms.
However I have realized that if I want to keep learning I will need a more formal knowledge of statistics, currently I have a non-formal knowledge of it, but so limited that, for example, I could not properly determine which of several linear models would be better (normally I tend to use R-square for it, but apparently that is not a very good idea). 
So to me it seems pretty obvious that I need to learn the basics of statistics (I studied that in uni but forgot most of it), where should I learn, please note that I don't really need a fully comprehensive course, just something that within a month allows me to know enough so I can get eager and learn more :).
So far I have read about ""Statistics without tears"", any other suggestion?
","['r', 'regression', 'machine-learning', 'references']","I would suggest you a basic road-map about how to go about it:Bonus:A wonderful site for such road maps is Metacademy, which I personally would vouch as one of the best Data Science resources on the web.Gitxiv is another beautiful site, which connects the Arxiv research papers on Data Science with the relevant open source implementations/libraries. "
How are Random Forests not sensitive to outliers?,"
I've read in a few sources, including this one, that Random Forests are not sensitive to outliers (in the way that Logistic Regression and other ML methods are, for example).
However, two pieces of intuition tell me otherwise:

Whenever a decision tree is constructed, all of the points must be classified.  This means that even outliers will get classified, and hence will affect the decision trees where they were selected during boosting.
Bootstrapping is a part of how a RandomForest does sub-sampling.  Bootstrapping is susceptible to outliers.

Is there any way to reconcile my intuition about its sensitivity to outliers, with sources that disagree?    
","['random-forest', 'bootstrap', 'outliers', 'cart']",
Pitfalls in time series analysis,"
I am just starting out self-learning in time series analysis. I have noticed that there are a number of potential pitfalls that are not applicable to general statistics. So, building on What are common statistical sins?, I would like to ask:
What are common pitfalls or statistical sins in time series analysis?
This is intended as a community wiki, one concept per answer, and please, no repetition of more general statistical pitfalls that are (or should be) listed at What are common statistical sins?
","['time-series', 'self-study']",
Proof that the coefficients in an OLS model follow a t-distribution with (n-k) degrees of freedom,"
Background
Suppose we have an Ordinary Least Squares model where we have $k$ coefficients in our regression model,
$$\mathbf{y}=\mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$$
where $\mathbf{\beta}$ is an $(k\times1)$ vector of coefficients, $\mathbf{X}$ is the design matrix defined by
$$\mathbf{X} = \begin{pmatrix}
1 & x_{11} & x_{12} & \dots & x_{1\;(k-1)} \\
1 & x_{21} & \dots  &       &   \vdots \\
\vdots &   & \ddots &       &   \vdots \\
1 & x_{n1} & \dots  & \dots &   x_{n\;(k-1)}    
\end{pmatrix}$$
and the errors are IID normal, 
$$\mathbf{\epsilon} \sim \mathcal{N}\left(\mathbf{0},\sigma^2 \mathbf{I}\right) \;.$$
We minimize the sum-of-squared-errors by setting our estimates for $\mathbf{\beta}$ to be
$$\mathbf{\hat{\beta}}= (\mathbf{X^T X})^{-1}\mathbf{X}^T \mathbf{y}\;. $$ 
An unbiased estimator of $\sigma^2$ is 
$$s^2 = \frac{\left\Vert \mathbf{y}-\mathbf{\hat{y}}\right\Vert ^2}{n-p}$$
where $\mathbf{\hat{y}} \equiv \mathbf{X} \mathbf{\hat{\beta}}$ (ref). 
The covariance of $\mathbf{\hat{\beta}}$ is given by
$$\operatorname{Cov}\left(\mathbf{\hat{\beta}}\right) = \sigma^2 \mathbf{C}$$
where $\mathbf{C}\equiv(\mathbf{X}^T\mathbf{X})^{-1}$ (ref) .
Question
How can I prove that for $\hat\beta_i$,
$$\frac{\hat{\beta}_i - \beta_i} {s_{\hat{\beta}_i}} \sim t_{n-k}$$
where $t_{n-k}$ is a t-distribution with $(n-k)$ degrees of freedom, and the standard error of $\hat{\beta}_i$ is estimated by $s_{\hat{\beta}_i} = s\sqrt{c_{ii}}$.

My attempts
I know that for $n$ random variables sampled from $x\sim\mathcal{N}\left(\mu, \sigma^2\right)$, you can show that 
$$\frac{\bar{x}-\mu}{s/\sqrt{n}} \sim t_{n-1} $$
by rewriting the LHS as
$$\frac{ \left(\frac{\bar x - \mu}{\sigma/\sqrt{n}}\right)  }  
{\sqrt{s^2/\sigma^2}}$$
and realizing that the numertor is a standard normal distribution, and the denominator is square root of a Chi-square distribution with df=(n-1) and divided by (n-1) (ref).  And therefore it follows a t-distribution with df=(n-1) (ref).
I was unable to extend this proof to my question...
Any ideas?  I'm aware of this question, but they don't explicitly prove it, they just give a rule of thumb, saying ""each predictor costs you a degree of freedom"".
","['regression', 'linear-model', 'least-squares', 't-distribution']","Since
$$\begin{align*}
\hat\beta &= (X^TX)^{-1}X^TY \\
&= (X^TX)^{-1}X^T(X\beta + \varepsilon) \\
&= \beta + (X^TX)^{-1}X^T\varepsilon
\end{align*}$$
we know that
$$\hat\beta-\beta \sim \mathcal{N}(0,\sigma^2 (X^TX)^{-1})$$
and thus we know that for each component $k$ of $\hat\beta$,
$$\hat\beta_k -\beta_k \sim \mathcal{N}(0, \sigma^2 S_{kk})$$
where $S_{kk}$ is the $k^\text{th}$ diagonal element of $(X^TX)^{-1}$.
Thus, we know that 
$$z_k = \frac{\hat\beta_k -\beta_k}{\sqrt{\sigma^2 S_{kk}}} \sim \mathcal{N}(0,1).$$Take note of the statement of the Theorem for the Distribution of an Idempotent Quadratic Form in a Standard Normal Vector (Theorem B.8 in Greene):If $x\sim\mathcal{N}(0,I)$ and $A$ is symmetric and idempotent, then $x^TAx$ is distributed $\chi^2_{\nu}$ where $\nu$ is the rank of $A$.Let $\hat\varepsilon$ denote the regression residual vector and let 
$$M=I_n - X(X^TX)^{-1}X^T \text{,}$$
which is the residual maker matrix (i.e. $My=\hat\varepsilon$). It's easy to verify that $M$ is symmetric and idempotent. Let
$$s^2 = \frac{\hat\varepsilon^T \hat\varepsilon}{n-p}$$
be an estimator for $\sigma^2$.We then need to do some linear algebra. Note these three linear algebra properties:So 
$$\begin{align*}
\operatorname{rank}(M) = \operatorname{Tr}(M) &= \operatorname{Tr}(I_n - X(X^TX)^{-1}X^T) \\
&= \operatorname{Tr}(I_n) - \operatorname{Tr}\left( X(X^TX)^{-1}X^T) \right) \\
&= \operatorname{Tr}(I_n) - \operatorname{Tr}\left( (X^TX)^{-1}X^TX) \right) \\
&= \operatorname{Tr}(I_n) - \operatorname{Tr}(I_p) \\
&=n-p
\end{align*}$$Then 
$$\begin{align*}
V = \frac{(n-p)s^2}{\sigma^2} = \frac{\hat\varepsilon^T\hat\varepsilon}{\sigma^2} = \left(\frac{\varepsilon}{\sigma}\right)^T M \left(\frac{\varepsilon}{\sigma}\right).
\end{align*}$$Applying the Theorem for the Distribution of an Idempotent Quadratic Form in a Standard Normal Vector (stated above), we know that $V \sim \chi^2_{n-p}$.Since you assumed that $\varepsilon$ is normally distributed, then $\hat\beta$ is independent of $\hat\varepsilon$, and since $s^2$ is a function of $\hat\varepsilon$, then $s^2$ is also independent of $\hat\beta$. Thus, $z_k$ and $V$ are independent of each other.Then,
$$\begin{align*}
t_k = \frac{z_k}{\sqrt{V/(n-p)}}
\end{align*}$$
is the ratio of a standard Normal distribution with the square root of a Chi-squared distribution with the same degrees of freedom (i.e. $n-p$), which is a characterization of the $t$ distribution. Therefore, the statistic $t_k$ has a $t$ distribution with $n-p$ degrees of freedom.It can then be algebraically manipulated into a more familiar form.$$\begin{align*}
t_k &= \frac{\frac{\hat\beta_k -\beta_k}{\sqrt{\sigma^2 S_{kk}}}}{\sqrt{\frac{(n-p)s^2}{\sigma^2}/(n-p)}} \\
&= \frac{\frac{\hat\beta_k -\beta_k}{\sqrt{S_{kk}}}}{\sqrt{s^2}} = \frac{\hat\beta_k -\beta_k}{\sqrt{s^2 S_{kk}}} \\
&= \frac{\hat\beta_k -\beta_k}{\operatorname{se}\left(\hat\beta_k \right)}
\end{align*}$$"
Bayesian statistics tutorial,"
I am trying to get upto speed in Bayesian Statistics. I have a little bit of stats background (STAT 101) but not too much - I think I can understand prior, posterior, and likelihood :D. 
I don't want to read a Bayesian textbook just yet.
I'd prefer to read from a source (website preferred) that will ramp me up quickly. Something like this, but that has more details.
Any advice?
","['bayesian', 'references']",Here's a place to start:ftp://selab.janelia.org/pub/publications/Eddy-ATG3/Eddy-ATG3-reprint.pdfhttp://blog.oscarbonilla.com/2009/05/visualizing-bayes-theorem/http://yudkowsky.net/rational/bayeshttp://www.math.umass.edu/~lavine/whatisbayes.pdfhttp://en.wikipedia.org/wiki/Bayesian_inferencehttp://en.wikipedia.org/wiki/Bayesian_probabilityTutorial_on_Bayesian_Statistics_and_Clinical_Trials
First R packages source code to study in preparation for writing own package,"
I'm planning to start writing R packages.
I thought it would be good to study the source code of existing packages
in order to learn the conventions of package construction.
My criteria for good packages to study:

Simple statistical/technical ideas: The point is to learn about the mechanics of package construction. Understanding the package should not require detailed highly domain specific knowledge about the actual topic of the package.
Simple and conventional coding style: I'm looking for something a bit more than Hello World but not a whole lot more. Idiosyncratic tricks and hacks would be distracting when first learning R packages.
Good coding style: The code is well written. It reveals both an understanding of good coding, in general, and an awareness of the conventions of coding in R.

Questions:

Which packages would be good to study?
Why would the suggested package source code be good to study
relative either to the criteria mentioned above or any other criteria that might be relevant?

Update (13/12/2010)
Following Dirk's comments I wanted to make it clear that no doubt many packages would be good to study first. I also agree that packages will provide models for different things (e.g., vignettes, S3 classes, S4 classes, unit testing, Roxygen, etc.).
Nonetheless, it would be interesting to read concrete suggestions about good packages 
to start with and the reasons why they would be good packages to start with.
I've also updated the question above to refer to ""packages"" rather than ""package"".
",['r'],
Does the reciprocal of a probability represent anything?,"
I was wondering if the reciprocal of P(X = 1) represents anything in particular?
",['probability'],
McFadden's Pseudo-$R^2$ Interpretation,"
I have a binary logistic regression model with a McFadden's pseudo R-squared of 0.192 with a dependent variable called payment (1 = payment and 0 = no payment). What is the interpretation of this pseudo R-squared? 
Is it a relative comparison for nested models (e.g. a 6 variable model has a McFadden's pseudo R-squared of 0.192, whereas a 5 variable model (after removing one variable from the aforementioned 6 variable model), this 5 variable model has a pseudo R-squared of 0.131. Would we would want to keep that 6th variable in the model?) or is it an absolute quantity (e.g. a given model that has a McFadden's pseudo R-squared of 0.192 is better than any existing model with a McFadden's pseudo R-squared of 0.180 (for even non-nested models)? These are just possible ways to look at McFadden’s pseudo R-squared; however, I assume these two views are way off, thus the reason why I am asking this question here.
I have done a great deal of research on this topic, and I have yet to find the answer that I am looking for in terms of being able to interpret a McFadden's pseudo R-squared of 0.192. Any insight and/or references are greatly appreciated! Before answering this question, I am aware that this isn't the best measure to describe a logistic regression model, but I would like to have a greater understanding of this statistic regardless!
","['regression', 'self-study', 'logistic']","So I figured I'd sum up what I've learned about McFadden's pseudo $R^2$ as a proper answer. The seminal reference that I can see for McFadden's pseudo $R^2$ is: McFadden, D. (1974) “Conditional logit analysis of qualitative choice behavior.” Pp. 105-142 in P. Zarembka (ed.), Frontiers in Econometrics. Academic Press. http://eml.berkeley.edu/~mcfadden/travel.html
Figure 5.5 shows the relationship between $\rho^2$ and traditional $R^2$ measures from OLS. My interpretation is that larger values of $\rho^2$ (McFadden's pseudo $R^2$) are better than smaller ones.  The interpretation of McFadden's pseudo $R^2$ between 0.2-0.4 comes from a book chapter he contributed to: Bahvioural Travel Modelling. Edited by David Hensher and Peter Stopher. 1979. McFadden contributed Ch. 15 ""Quantitative Methods for Analyzing Travel Behaviour on Individuals: Some Recent Developments"". Discussion of model evaluation (in the context of multinomial logit models) begins on page 306 where he introduces $\rho^2$ (McFadden's pseudo $R^2$). McFadden states ""while the $R^2$ index is a more familiar concept to planner who are experienced in OLS, it is not as well behaved as the $\rho^2$ measure, for ML estimation. Those unfamiliar with $\rho^2$ should be forewarned that its values tend to be considerably lower than those of the $R^2$ index...For example, values  of 0.2 to 0.4 for $\rho^2$ represent EXCELLENT fit.""So basically, $\rho^2$ can be interpreted like $R^2$, but don't expect it to be as big. And values from 0.2-0.4 indicate (in McFadden's words) excellent model fit. "
Why is multiple comparison a problem?,"
I find it hard to understand what really is the issue with multiple comparisons.  With a simple analogy, it is said that a person who will make many decisions will make many mistakes. So very conservative precaution is applied, like Bonferroni correction, so as to make the probability that, this person will make any mistake at all, as low as possible. 
But why do we care about whether the person has made any mistake at all among all decisions he/she has made, rather than the percentage of the wrong decisions?  
Let me try to explain what confuses me with another analogy.  Suppose there are two judges, one is 60 years old, and the other is 20 years old.  Then Bonferroni correction tells the one which is 20 years old to be as conservative as possible, in deciding for execution, because he  will work for many more years as a judge, will make many more decisions, so he has to be careful.  But the one at 60 years old will possibly retire soon, will make fewer decisions, so he can be more careless compared to the other.  But actually, both judges should be equally careful or conservative, regardless of the total number of decisions they will make.  I think this analogy more or less translates to the real problems where Bonferroni correction is applied, which I find counterintuitive. 
","['hypothesis-testing', 'multiple-comparisons']",
Understanding regressions - the role of the model,"
How can a regression model be any use if you don't know the function you are trying to get the parameters for?
I saw a piece of research that said that mothers who breast fed their children were less likely to suffer diabetes in later life. The research was from a survey of some 1000 mothers and controlled for miscellaneous factors and a loglinear model was used.
Now does this mean that they reckon all the factors that determine the likelihood of diabetes fit in a nice function (exponential presumably) that translates neatly into a linear model with logs and that whether the woman breast fed turned out to be statistically significant?
I'm missing something I'm sure but, how the hell do they know the model? 
","['regression', 'modeling', 'epidemiology', 'log-linear']",
Is it valid to include a baseline measure as control variable when testing the effect of an independent variable on change scores?,"
I am attempting to run an OLS regression:

DV: Change in weight over a year (initial weight - end weight)
IV: Whether or not you exercise.

However, it seems reasonable that heavier people will lose more weight per unit of exercise than thinner people. Thus, I wanted to include a control variable:

CV: Initial starting weight.

However, now initial weight is used BOTH to calculate the dependent variable AND as a control variable.
Is this okay? Does this violate an assumption of OLS?
","['regression', 'repeated-measures', 'least-squares', 'pre-post-comparison']",
How to interpret mean of Silhouette plot?,"
Im trying to use silhouette plot to determine the number of cluster in my dataset. Given the dataset Train , i used the following matlab code 
Train_data = full(Train);  
Result = [];  
for num_of_cluster = 1:20  
    centroid = kmeans(Train_data,num_of_cluster,'distance','sqeuclid');  
    s = silhouette(Train_data,centroid,'sqeuclid');  
    Result = [ Result; num_of_cluster mean(s)];  
end  
plot( Result(:,1),Result(:,2),'r*-.');`

The resultant plot is given below with xaxis as number of cluster and yaxis mean of silhouette value. 
How do i interpret this graph? How do i determine the number of cluster from this? 

","['data-visualization', 'clustering', 'matlab']",
Lift measure in data mining,"
I searched many websites to know what exactly lift will do? The results that I found all were about using it in applications not itself.
I know about the support and confidence function. From Wikipedia, in data mining, lift is a measure of the performance of a model at predicting or classifying cases, measuring against a random choice model. But how? Confidence*support is the value of lift I searched another formulas too but I can't understand why the lift charts are important in accuracy of predicted values I mean I want to know what policy and reason is behind lift?
",['data-mining'],"I'll give an example of how ""lift"" is useful...Imagine you are running a direct mail campaign where you mail customers an offer in the hopes they respond. Historical data shows that when you mail your customer base completely at random about 8% of them respond to the mailing (i.e. they come in and shop with the offer). So, if you mail 1,000 customers you can expect 80 responders.Now, you decide to fit a logistic regression model to your historical data to find patterns that are predictive of whether a customer is likely to respond to a mailing. Using the logistic regression model each customer is assigned a probability of responding and you can assess the accuracy because you know whether they actually responded. Once each customer is assigned their probability, you rank them from highest to lowest scoring customer. Then you could generate some ""lift"" graphics like these:Ignore the top chart for now. The bottom chart is saying that after we sort the customers based on their probability of responding (high to low), and then break them up into ten equal bins, the response rate in bin #1 (the top 10% of customers) is 29% vs 8% of random customers, for a lift of 29/8 = 3.63. By the time we get to scored customers in the 4th bin, we have captured so many the previous three that the response rate is lower than what we would expect mailing people at random. Looking at the top chart now, what this says is that if we use the probability scores on customers we can get 60% of the total responders we'd get mailing randomly by only mailing the top 30% of scored customers. That is, using the model we can get 60% of the expected profit for 30% of the mail cost by only mailing the top 30% of scored customers, and this is what lift really refers to."
Is there an explanation for why there are so many natural phenomena that follow normal distribution?,"
I think this is a fascinating topic and I do not fully understand it. What law of physics makes so that so many natural phenomena have normal distribution? It would seem more intuitive that they would have uniform distribution.
It is so hard for me to understand this and I feel I am missing some information. Can somebody help me with a good explanation or link me to a book/video/article?
","['distributions', 'normal-distribution', 'normality-assumption', 'uniform-distribution']",
Dealing with singular fit in mixed models,"
Let's say we have a model
mod <- Y ~ X*Condition + (X*Condition|subject)

# Y = logit variable  
# X = continuous variable  
# Condition = values A and B, dummy coded; the design is repeated 
#             so all participants go through both Conditions  
# subject = random effects for different subjects 

summary(model)
Random effects:
 Groups  Name             Variance Std.Dev. Corr             
 subject (Intercept)      0.85052  0.9222                    
         X                0.08427  0.2903   -1.00            
         ConditionB       0.54367  0.7373   -0.37  0.37      
         X:ConditionB     0.14812  0.3849    0.26 -0.26 -0.56
Number of obs: 39401, groups:  subject, 219

Fixed effects:
                 Estimate Std. Error z value Pr(>|z|)    
(Intercept)       2.49686    0.06909   36.14  < 2e-16 ***
X                -1.03854    0.03812  -27.24  < 2e-16 ***
ConditionB       -0.19707    0.06382   -3.09  0.00202 ** 
X:ConditionB      0.22809    0.05356    4.26 2.06e-05 ***

Here we observe a singular fit, because the correlation between intercept and x random effects is -1. Now, according to this helpful link one way to deal with this model is to remove higher-order random effects (e.g., X:ConditionB) and see whether that makes a difference when testing for singularity. The other is to use the Bayesian approach, e.g., the blme package to avoid singularity.
What is the preferred method and why?
I am asking this because using the first or the second one leads to different results - in the first case, I will remove the X:ConditionB random effect and won't be able to estimate the correlation between X and X:ConditionB random effects. On the other hand, using blme allows me to keep X:ConditionB and to estimate the given correlation. I see no reason why I should even use the non-bayesian estimations and remove random effects when singular fits occur when I can estimate everything with the Bayesian approach.
Can someone explain to me the benefits and problems using either method to deal with singular fits?
","['lme4-nlme', 'glmm', 'overfitting', 'singular-matrix']","When you obtain a singular fit, this is often indicating that the model is overfitted – that is, the random effects structure is too complex to be supported by the data, which naturally leads to the advice to remove the most complex part of the random effects structure (usually random slopes).  The benefit of this approach is that it leads to a more parsimonious model that is not over-fitted.However, before doing anything, do you have a good reason for wanting X, Condition and their interaction, all to vary by subject in the first place ? Does the theory of how the data are generated suggest this ? If you desire to fit the model with the maximal random effects structure, and lme4 obtains a singular fit, then fitting the same model in a Bayesian framework might very well inform you why lme4 had problems, by inspecting trace plots and how well the various parameter estimates converge. The advantage in taking the Bayesian approach is that by doing so you may uncover a problem with original model ie. the reason why the maximum random effects structure isn’t supported by the data) or it might uncover why lme4 is unable to fit the model. I have encountered situations where a Bayesian model does not converge well, unless informative priors are used – which may or may not be OK. In short, both approaches have merit. However, I would always start from a place where the initial model is parsimonious and informed by expert domain knowledge to determine the most appropriate random effects structure. Specifying grouping variables is relatively easy, but random slopes usually don’t have to be included. Only include them if they make sound theoretical sense AND they are supported by the data. Edit:
It is mentioned in the comments that there are sound theoretical reasons to fit the maximal random effects structure. So, a relatively easy way to proceed with an equivalent Bayesian model is to swap the call to glmer with stan_glmer from the rstanarm package – it is designed to be plug and play.  It has default priors, so you can quickly get a model fitted. The package also has many tools for assessing convergence. If you find that all the parameters have converging to plausible values, then you are all good. However there can be a number of issues – for example a variance being estimated at or below zero, or an estimate that continues to drift. The mc-stan.org site has a wealth of information and a user forum."
How to sample from a normal distribution with known mean and variance using a conventional programming language?,"
I've never had a course in statistics, so I hope I'm asking in the right place here.
Suppose I have only two data describing a normal distribution: the mean $\mu$ and variance $\sigma^2$.  I want to use a computer to randomly sample from this distribution such that I respect these two statistics.
It's pretty obvious that I can handle the mean by simply normalizing around 0: just add $\mu$ to each sample before outputting the sample.  But I don't see how to programmatically generate samples to respect $\sigma^2$.
My program will be in a conventional programming language; I don't have access to any statistical packages.
","['normal-distribution', 'sampling', 'computational-statistics']","If you can sample from a given distribution with mean 0 and variance 1, then you can easily sample from a scale-location transformation of that distribution, which has mean $\mu$ and variance $\sigma^2$. If $x$ is a sample from a mean 0 and variance 1 distribution then 
$$\sigma x + \mu$$
is a sample with mean $\mu$ and variance $\sigma^2$. So, all you have to do is to scale the variable by the standard deviation $\sigma$ (square root of the variance) before adding the mean $\mu$. How you actually get a simulation from a normal distribution with mean 0 and variance 1 is a different story. It's fun and interesting to know how to implement such things, but whether you use a statistical package or programming language or not, I will recommend that you obtain and use a suitable function or library for the random number generation. If you want advice on what library to use you might want to add specific information on which programming language(s) you are using. Edit: In the light of the comments, some other answers and the fact that Fixee accepted this answer, I will give some more details on how one can use transformations of uniform variables to produce normal variables. At the end of the day, a correctly implemented method is not better than the uniform pseudo random number generator used. Personally, I prefer to rely on special purpose libraries that I believe are trustworthy. I almost always rely on the methods implemented in R either directly in R or via the API in C/C++. Obviously, this is not a solution for everybody, but I am not familiar enough with other libraries to recommend alternatives."
Rigorous definition of an outlier?,"
People often talk about dealing with outliers in statistics.  The thing that bothers me about this is that, as far as I can tell, the definition of an outlier is completely subjective.  For example, if the true distribution of some random variable is very heavy-tailed or bimodal, any standard visualization or summary statistic for detecting outliers will incorrectly remove parts of the distribution you want to sample from.  What is a rigorous definition of an outlier, if one exists, and how can outliers be dealt with without introducing unreasonable amounts of subjectivity into an analysis?
","['outliers', 'definition']","As long as your data comes from a known distribution with known properties, you can rigorously define an outlier as an event that is too unlikely to have been generated by the observed process (if you consider ""too unlikely"" to be non-rigorous, then all hypothesis testing is).However, this approach is problematic on two levels: It assumes that the data comes from a known distribution with known properties, and it brings the risk that outliers are looked at as data points that were smuggled into your data set by some magical faeries. In the absence of magical data faeries, all data comes from your experiment, and thus it is actually not possible to have outliers, just weird results. These can come from recording errors (e.g. a 400000 bedroom house for 4 dollars), systematic measurement issues (the image analysis algorithm reports huge areas if the object is too close to the border) experimental problems (sometimes, crystals precipitate out of the solution, which give very high signal), or features of your system (a cell can sometimes divide in three instead of two), but they can also be the result of a mechanism that no one's ever considered because it's rare and you're doing research, which means that some of the stuff you do is simply not known yet. Ideally, you take the time to investigate every outlier, and only remove it from your data set once you understand why it doesn't fit your model. This is time-consuming and subjective in that the reasons are highly dependent on the experiment, but the alternative is worse: If you don't understand where the outliers came from, you have the choice between letting outliers ""mess up"" your results, or defining some ""mathematically rigorous"" approach to hide your lack of understanding. In other words, by pursuing ""mathematical rigorousness"" you choose between not getting a significant effect and not getting into heaven. EDITIf all you have is a list of numbers without knowing where they come from, you have no way of telling whether some data point is an outlier, because you can always assume a distribution where all data are inliers. "
Statistical models cheat sheet,"
I was wondering if there is a statistical model ""cheat sheet(s)"" that lists any or more information:

when to use the model
when not to use the model
required and optional inputs
expected outputs
has the model been tested in different fields (policy, bio, engineering, manufacturing, etc)?
is it accepted in practice or research?
expected variation / accuracy / precision
caveats
scalability
deprecated model, avoid or don't use
etc ..

I've seen hierarchies before on various websites, and some simplistic model cheat sheets in various textbooks; however, it'll be nice if there is a larger one that encompasses various types of models based on different types of analysis and theories.
","['references', 'modeling']","I have previously found UCLA's ""Choosing the Correct Statistical Test"" to be helpful:
https://stats.idre.ucla.edu/other/mult-pkg/whatstat/It also gives examples of how to do the analysis in SAS, Stata, SPSS and R."
Optimized implementations of the Random Forest algorithm,"
I have noticed that there are a few implementations of random forest such as ALGLIB, Waffles and some R packages like randomForest.  Can anybody tell me whether these libraries are highly optimized?  Are they basically equivalent to the random forests as detailed in The Elements of Statistical Learning or have a lot of extra tricks been added?
I hope this question is specific enough.  As an illustration of the type of answer I am looking for, if somebody asked me whether the linear algebra package BLAS was highly optimized, I would say it was extremely highly optimized and mostly not worth trying to improve upon except in very specialized applications.
","['random-forest', 'algorithms', 'model-evaluation']","(Updated 6 IX 2015 with suggestions from comments, also made CW)There are two new, nice packages available for R which are pretty well optimised for a certain conditions:Other RF implementations:Ranger paper has some speed/memory comparisons, but there is no thorough benchmark."
whether to rescale indicator / binary / dummy predictors for LASSO,"
For the LASSO (and other model selecting procedures) it is crucial to rescale the predictors.  The general recommendation I follow is simply to use a 0 mean, 1 standard deviation normalization for continuous variables. But what is there to do with dummies?
E.g. some applied examples from the same (excellent) summer school I linked to rescales continuous variables to be between 0 and 1 (not great with outliers though), probably to be comparable to the dummies. But even that does not guarantee that the coefficients should be the same order of magnitude, and thus penalized similarly, the key reason for rescaling, no?
","['predictive-models', 'model-selection', 'lasso', 'normalization', 'standardization']","According Tibshirani (THE LASSO METHOD FOR VARIABLE SELECTION
IN THE COX MODEL, Statistics in Medicine, VOL. 16, 385-395 (1997)), who literally wrote the book on regularization methods, you should standardize the dummies.  However, you then lose the straightforward interpretability of your coefficients.  If you don't, your variables are not on an even playing field.  You are essentially tipping the scales in favor of your continuous variables (most likely).  So, if your primary goal is model selection then this is an egregious error.  However, if you are more interested in interpretation then perhaps this isn't the best idea. The recommendation is on page 394:The lasso method requires initial standardization of the regressors, so that the penalization scheme is fair to all regressors. For categorical regressors, one codes the regressor with dummy variables and then standardizes the dummy variables. As pointed out by a referee, however, the relative scaling between continuous and categorical variables in this scheme can be somewhat arbitrary."
How do I fit a constrained regression in R so that coefficients total = 1?,"
I see a similar constrained regression here:
Constrained linear regression through a specified point
but my requirement is slightly different. I need the coefficients to add up to 1. Specifically I am regressing the returns of 1 foreign exchange series against 3 other foreign exchange series, so that investors may replace their exposure to that series with a combination of exposure to the other 3, but their cash outlay must not change, and preferably (but this is not mandatory), the coefficients should be positive. 
I have tried to search for constrained regression in R and Google but with little luck. 
","['r', 'regression']","If I understand correctly, your model is
$$ Y = \pi_1 X_1 + \pi_2 X_2 + \pi_3 X_3 + \varepsilon, $$
with $\sum_k \pi_k = 1$ and $\pi_k\ge0$. You need to minimize
$$\sum_i \left(Y_i - (\pi_1 X_{i1} + \pi_2 X_{i2} + \pi_3 X_{i3}) \right)^2 $$
subject to these constraints. This kind of problem is known as quadratic programming.Here a few line of R codes giving a possible solution ($X_1, X_2, X_3$ are the columns of X, the true values of the $\pi_k$ are 0.2, 0.3 and 0.5).I don’t know any results on the asymptotic distribution of the estimators, etc. If someone has pointers, I’ll be curious to get some (if you wish I can open a new question on this)."
How to do community detection in a weighted social network/graph?,"
I'm wondering if someone could suggest what are good starting points when it comes to performing community detection/graph partitioning/clustering on a graph that has weighted, undirected edges. The graph in question has approximately 3 million edges and each edge expresses the degree of similarity between the two vertices it connects. In particular, in this dataset edges are individuals and vertices are a measure of the similarity of their observed behavior.
In the past I followed a suggestion I got here on stats.stackexchange.com and used igraph's implementation of Newman's modularity clustering and was satisfied with the results, but that was on a unweighted dataset.
Are there any specific algorithms I should be looking at?
","['clustering', 'data-visualization', 'networks', 'partitioning', 'modularity']","igraph  implementation of Newman's modularity clustering (fastgreedy function) can be used with weighted edges as well. Just add weight attribute to the edges and analyse as usual. In my experience, it run even faster with weights as there are less ties."
Why does Andrew Ng prefer to use SVD and not EIG of covariance matrix to do PCA?,"
I am studying PCA from Andrew Ng's Coursera course and other materials. In the Stanford NLP course cs224n's first assignment, and in the lecture video from Andrew Ng, they do singular value decomposition instead of eigenvector decomposition of covariance matrix, and Ng even says that SVD is numerically more stable than eigendecomposition.
From my understanding, for PCA we should do SVD of the data matrix of (m,n) size, not of the covariance matrix of (n,n) size. And eigenvector decomposition of covariance matrix.
Why do they do SVD of covariance matrix, not data matrix?
","['pca', 'linear-algebra', 'svd', 'eigenvalues', 'numerics']",
Why is the sampling distribution of variance a chi-squared distribution?,"
The statement
The sampling distribution of the sample variance is a chi-squared distribution with degree of freedom equals to $n-1$, where $n$ is the sample size (given that the random variable of interest is normally distributed).
Source
My intuition
It kinda makes intuitive sense to me 1) because a chi-square test looks like a sum of square and 2) because a Chi-squared distribution is just a sum of squared normal distribution. But still, I don't have a good understanding of it.
Question
Is the statement true? Why?
","['distributions', 'normal-distribution', 'sampling', 'sample-size', 'chi-squared-distribution']","[I'll assume from the discussion in your question that you're happy to accept as fact that if $Z_i, i=1,2,\ldots,k$ are independent identically distributed $N(0,1)$ random variables then $\sum_{i=1}^{k}Z_i^2\sim \chi^2_k$.]Formally, the result you need follows from Cochran's theorem.
 (Though it can be shown in other ways)Less formally, consider that if we knew the population mean, and estimated the variance about it (rather than about the sample mean): $s_0^2 = \frac{1}{n} \sum_{i=1}^{n}(X_i-\mu)^2$, then $s_0^2/\sigma^2 = \frac{1}{n} \sum_{i=1}^{n}\left(\frac{X_i-\mu}{\sigma}\right)^2=\frac{1}{n} \sum_{i=1}^{n}Z_i^2$,  ($Z_i=(X_i-\mu)/\sigma$) which will be $\frac{1}{n}$ times a $\chi^2_n$ random variable.The fact that the sample mean is used, instead of the population mean ($Z_i^*=(X_i-\bar{X})/\sigma$) makes the sum of squares of deviations smaller, but in just such a way that $\sum_{i=1}^{n}(Z_i^*)^2\,\sim\chi^2_{n-1}$ (about which, see Cochran's theorem). That is, rather than $ns_0^2/\sigma^2\sim \chi^2_n$ we now have $(n-1)s^2/\sigma^2\sim\chi^2_{n-1}$."
Neural Networks: weight change momentum and weight decay,"
Momentum $\alpha$ is used to diminish the fluctuations in weight changes over consecutive iterations:
$$\Delta\omega_i(t+1) = - \eta\frac{\partial E}{\partial w_i} + \alpha \Delta \omega_i(t),$$
where $E({\bf w})$ is the error function, ${\bf w}$ - the vector of weights, $\eta$ - learning rate.
Weight decay $\lambda$ penalizes the weight changes:
$$\Delta\omega_i(t+1) =- \eta\frac{\partial E}{\partial w_i} - \lambda\eta\omega_i$$
The question is if it makes sense to combine both tricks during the back-propagation and what effect it would have?
$$\Delta\omega_i(t+1) = - \eta\frac{\partial E}{\partial w_i} + \alpha \Delta \omega_i(t) - \lambda\eta\omega_i$$
","['neural-networks', 'optimization', 'regularization', 'gradient-descent']","Yes, it's very common to use both tricks.  They solve different problems and can work well together.One way to think about it is that weight decay changes the function that's being optimized, while momentum changes the path you take to the optimum.Weight decay, by shrinking your coefficients toward zero, ensures that you find a local optimum with small-magnitude parameters.  This is usually crucial for avoiding overfitting (although other kinds of constraints on the weights can work too).  As a side benefit, it can also make the model easier to optimize, by making the objective function more convex.Once you have an objective function, you have to decide how to move around on it.  Steepest descent on the gradient is the simplest approach, but you're right that fluctuations can be a big problem. Adding momentum helps solve that problem.  If you're working with batch updates (which is usually a bad idea with neural networks) Newton-type steps are another option. The new ""hot"" approaches are based on Nesterov's accelerated gradient and so-called ""Hessian-Free"" optimization.But regardless of which of these update rules you use (momentum, Newton, etc.), you're still working with the same objective function, which is determined by your error function (e.g. squared error) and other constraints (e.g. weight decay).  The main question when deciding which of these to use is how quickly you'll get to a good set of weights."
What is the adjusted R-squared formula in lm in R and how should it be interpreted?,"
What is the exact formula used in R lm() for the Adjusted R-squared? How can I interpret it?
Adjusted r-squared formulas
There seem to exist several formulas to calculate Adjusted R-squared. 

Wherry’s formula: $1-(1-R^2)\frac{(n-1)}{(n-v)}$
McNemar’s formula: $1-(1-R^2)\frac{(n-1)}{(n-v-1)}$
Lord’s formula: $1-(1-R^2)\frac{(n+v-1)}{(n-v-1)}$
Stein's formula: $1-\big[\frac{(n-1)}{(n-k-1)}\frac{(n-2)}{(n-k-2)}\frac{(n+1)}{n}\big](1-R^2)$

Textbook descriptions

According to Field's textbook, Discovering Statistics Using R (2012, p. 273) R uses Wherry's equation which ""tells us how much variance in Y would be accounted for if the model had been derived from the population from which the sample was taken"". He does not give the formula for Wherry. He recommends using Stein's formula (by hand) to check how well the model cross-validates.
Kleiber/Zeileis, Applied Econometrics with R (2008, p. 59) claim it's ""Theil's adjusted R-squared"" and don't say exactly how its interpretation varies from the multiple R-squared.
Dalgaard, Introductory Statistics with R (2008, p. 113) writes that ""if you multiply [adjusted R-squared] by 100%, it can be interpreted as '% variance reduction'"". He does not say to which formula this corresponds.

I had previously thought, and read widely, that R-squared penalizes for adding additional variables to the model. Now the use of these different formulas seems to call for different interpretations.  I also looked at a related question on Stack Overflow (What is the difference between Multiple R-squared and Adjusted R-squared in a single-variate least squares regression?), and the Wharton school's statistical dictionary at UPenn.
Questions

Which formula is used for adjusted r-square by R lm()?
How can I interpret it?

","['r', 'regression', 'r-squared', 'lm', 'regularization']",
Poisson regression to estimate relative risk for binary outcomes,"
Brief Summary 
Why is it more common for logistic regression (with odds ratios) to be used in cohort studies with binary outcomes, as opposed to Poisson regression (with relative risks)?
Background
Undergraduate and graduate statistics and epidemiology courses, in my experience, generally teach that logistic regression should be used for modelling data with binary outcomes, with risk estimates reported as odds ratios.
However, Poisson regression (and related: quasi-Poisson, negative binomial, etc.) can also be used to model data with binary outcomes and, with appropriate methods (e.g. robust sandwich variance estimator), it provides valid risk estimates and confidence levels. E.g.,

Greenland S., Model-based estimation of relative risks and other epidemiologic measures in studies of common outcomes and in case-control studies, Am J Epidemiol. 2004 Aug 15;160(4):301-5.
Zou G., A modified Poisson regression approach to prospective studies with binary data, Am J Epidemiol. 2004 Apr 1;159(7):702-6. 
Zou G.Y. and Donner A., Extension of the modified Poisson regression model to prospective studies with correlated binary data, Stat Methods Med Res. 2011 Nov 8.

From Poisson regression, relative risks can be reported, which some have argued are easier to interpret compared with odds ratios, especially for frequent outcomes, and especially by individuals without a strong background in statistics. See Zhang J. and Yu K.F., What's the relative risk? A method of correcting the odds ratio in cohort studies of common outcomes, JAMA. 1998 Nov 18;280(19):1690-1.
From reading the medical literature, among cohort studies with binary outcomes it seems that it is still far more common to report odds ratios from logistic regressions rather than relative risks from Poisson regressions. 
Questions
For cohort studies with binary outcomes:

Is there good reason to report odds ratios from logistic regressions rather than relative risks from Poisson regressions?
If not, can the infrequency of Poisson regressions with relative risks in the medical literature be attributed mostly to a lag between methodological theory and practice among scientists, clinicians, statisticians, and epidemiologists?
Should intermediate statistics and epidemiology courses include more discussion of Poisson regression for binary outcomes?
Should I be encouraging students and colleagues to consider Poisson regression over logistic regression when appropriate?

","['logistic', 'poisson-distribution', 'epidemiology', 'odds-ratio', 'relative-risk']","An answer to all four of your questions, preceeded by a note:It's not actually all that common for modern epidemiology studies to report an odds ratio from a logistic regression for a cohort study. It remains the regression technique of choice for case-control studies, but more sophisticated techniques are now the de facto standard for analysis in major epidemiology journals like Epidemiology, AJE or IJE. There will be a greater tendency for them to show up in clinical journals reporting the results of observational studies. There's also going to be some problems because Poisson regression can be used in two contexts: What you're referring to, wherein it's a substitute for a binomial regression model, and in a time-to-event context, which is extremely common for cohort studies. More details in the particular question answers:For a cohort study, not really no. There are some extremely specific cases where say, a piecewise logistic model may have been used, but these are outliers. The whole point of a cohort study is that you can directly measure the relative risk, or many related measures, and don't have to rely on an odds ratio. I will however make two notes: A Poisson regression is estimating often a rate, not a risk, and thus the effect estimate from it will often be noted as a rate ratio (mainly, in my mind, so you can still abbreviate it RR) or an incidence density ratio (IRR or IDR). So make sure in your search you're actually looking for the right terms: there are many cohort studies using survival analysis methods. For these studies, Poisson regression makes some assumptions that are problematic, notably that the hazard is constant. As such it is much more common to analyze a cohort study using Cox proportional hazards models, rather than Poisson models, and report the ensuing hazard ratio (HR). If pressed to name a ""default"" method with which to analyze a cohort, I'd say epidemiology is actually dominated by the Cox model. This has its own problems, and some very good epidemiologists would like to change it, but there it is.There are two things I might attribute the infrequency to - an infrequency I don't necessarily think exists to the extent you suggest. One is that yes - ""epidemiology"" as a field isn't exactly closed, and you get huge numbers of papers from clinicians, social scientists, etc. as well as epidemiologists of varying statistical backgrounds. The logistic model is commonly taught, and in my experience many researchers will turn to the familiar tool over the better tool.

The second is actually a question of what you mean by ""cohort"" study. Something like the Cox model, or a Poisson model, needs an actual estimate of person-time. It's possible to get a cohort study that follows a somewhat closed population for a particular period - especially in early ""Intro to Epi"" examples, where survival methods like Poisson or Cox models aren't so useful. The logistic model can be used to estimate an odds ratio that, with sufficiently low disease prevalence, approximates a relative risk. Other regression techniques that directly estimate it, like binomial regression, have convergence issues that can easily derail a new student. Keep in mind the Zou papers you cite are both using a Poisson regression technique to get around the convergence issues of binomial regression. But binomial-appropriate cohort studies are actually a small slice of the ""cohort study pie"".Yes. Frankly, survival analysis methods should come up earlier than they often do. My pet theory is that the reason this isn't so is that methods like logistic regression are easier to code. Techniques that are easier to code, but come with much larger caveats about the validity of their effect estimates, are taught as the ""basic"" standard, which is a problem. You should be encouraging students and colleagues to use the appropriate tool. Generally for the field, I think you'd probably be better off suggesting a consideration of the Cox model over a Poisson regression, as most reviewers would (and should) swiftly bring up concerns about the assumption of a constant hazard. But yes, the sooner you can get them away from ""How do I shoehorn my question into a logistic regression model?"" the better off we'll all be. But yes, if you're looking at a study without time, students should be introduced to both binomial regression, and alternative approaches, like Poisson regression, which can be used in case of convergence problems."
How can I test whether a random effect is significant?,"
I am trying to understand when to use a random effect and when it is unnecessary.  Ive been told a rule of thumb is if you have 4 or more groups/individuals which I do (15 individual moose).  Some of those moose were experimented on 2 or 3 times for a total of 29 trials.  I want to know if they behave differently when they are in higher risk landscapes than not.  So, I thought I would set the individual as a random effect.  However, I am now being told that there is no need to include the individual as a random effect because there is not a lot of variation in their response.  What I can't figure out is how to test if there really is something being accounted for when setting individual as a random effect.  Maybe an initial question is: What test/diagnostic can I do to figure out if Individual is a good explanatory variable and should it be a fixed effect - qq plots? histograms? scatter plots? And what would I look for in those patterns.
I ran the model with the individual as a random effect and without, but then I read Ben Bolker's GLMM FAQ where they state:

do not compare lmer models with the corresponding lm fits, or
glmer/glm; the log-likelihoods are not commensurate (i.e., they
include different additive terms)

And here I assume this means you can't compare between a model with random effect or without.  But I wouldn't really know what I should compare between them anyway.
In my model with the random effect I also was trying to look at the output to see what kind of evidence or significance the RE has:
lmer(Velocity ~ D.CPC.min + FD.CPC + (1|ID), REML=FALSE, family=gaussian, data=tv)

Linear mixed model fit by maximum likelihood 
Formula: Velocity ~ D.CPC.min + FD.CPC + (1 | ID) 
   Data: tv 
    AIC    BIC logLik deviance REMLdev
 -13.92 -7.087  11.96   -23.92   15.39
Random effects:
 Groups   Name        Variance Std.Dev.
 ID       (Intercept) 0.00000  0.00000 
 Residual             0.02566  0.16019 
Number of obs: 29, groups: ID, 15

Fixed effects:
              Estimate Std. Error t value
(Intercept)  3.287e-01  5.070e-02   6.483
D.CPC.min   -1.539e-03  3.546e-04  -4.341
FD.CPC       1.153e-04  1.789e-05   6.446

Correlation of Fixed Effects:
          (Intr) D.CPC.
D.CPC.min -0.010       
FD.CPC    -0.724 -0.437

You see that my variance and SD from the individual ID as the random effect equals 0.  How is that possible? What does 0 mean? Is that right? Then my friend who said ""since there is no variation, using ID as random effect is unnecessary"" is correct?  So, then would I use it as a fixed effect? But wouldn't the fact that there is so little variation suggest it isn't going to tell us much anyway?
","['mixed-model', 'lme4-nlme', 'random-effects-model', 'glmm']","The estimate ID's variance = 0, indicates that the level of
between-group variability is not sufficient to warrant incorporating random effects in the model; i.e., your model is degenerate.As you correctly identify yourself: most probably, yes; ID as a random effect is unnecessary. A few things spring to mind to test this assumption:Mind you, choices 1 & 2 have an issue: you are checking for something that is on the boundaries of the parameter space so actually they are not technically sound. Having said that, I don't think you'll get wrong insights from them and a lot of people use them (e.g., Douglas Bates, one of lme4's developers, uses them in his book but clearly states this caveat about parameter values being tested on the boundary of the set of possible values).Choice 3 is the most tedious but actually gives you the best idea about what is really going on. Some people are tempted to use non-parametric bootstrap also but I think that given the fact you are making parametric assumptions to start with you might as well use them."
Computing Cohen's Kappa variance (and standard errors),"
The Kappa ($\kappa$) statistic was introduced in 1960 by Cohen [1] to measure agreement between two raters. Its variance, however, had been a source of contradictions for quite a some time. 
My question is about which is the best variance calculation to be used with large samples.  I am inclined to believe the one tested and verified by Fleiss [2] would be the right choice, but this does not seem to be the only published one which seems to be correct (and used throughout fairly recent literature). 
Right now I have two concrete ways to compute its asymptotic large sample variance:

The corrected method published by Fleiss, Cohen and Everitt [2];
The delta method which can be found in the book by Colgaton, 2009 [4] (page 106).

To illustrate some of this confusion, here is a quote by Fleiss, Cohen and Everitt [2], emphasis mine:

Many human endeavors have been cursed with repeated failures before
  final success is achieved. The scaling of Mount Everest is one
  example. The discovery of the Northwest Passage is a second. The
  derivation of a correct standard error for kappa is a third.

So, here is a small summary of what happened:

1960: Cohen publishes his paper ""A coefficient of agreement for nominal scales"" [1] introducing his chance-corrected measure of agreement between two raters called $\kappa$. However, he publishes incorrect formulas for the variance calculations.
1968: Everitt attempts to correct them, but his formulas were incorrect as well.
1969: Fleiss, Cohen and Everitt publish the correct formulas in the paper ""Large Sample Standard Errors Of Kappa and Weighted Kappa"" [2].
1971: Fleiss publishes another $\kappa$ statistic (but a different one) under the same name, with incorrect formulas for the variances.
1979: Fleiss Nee and Landis publish the corrected formulas for Fleiss' $\kappa$.

At first, consider the following notation. This notation implies the summation operator  should be applied to all elements in the dimension over which the dot is placed:
$\ \ \ p_{i.} = \displaystyle\sum_{j=1}^{k} p_{ij}$
$\ \ \ p_{.j} = \displaystyle\sum_{i=1}^{k} p_{ij}$
Now, one can compute Kappa as:
$\ \ \ \hat\kappa = \displaystyle\frac{p_o-p_c}{1-p_e}$
In which 
$\ \ \ p_o = \displaystyle\sum_{i=1}^{k} p_{ii}$ is the observed agreement, and
$\ \ \ p_c = \displaystyle\sum_{i=1}^{k} p_{i.} p_{.i}$ is the chance agreement.
So far, the correct variance calculation for Cohen's $\kappa$ is given by:
$\ \ \ \newcommand{\var}{\mathrm{var}}\widehat{\var}(\hat{\kappa}) = \frac{1}{N(1-p_c)^4} \{ \displaystyle\sum_{i=1}^{k} p_{ii}[(1-p_o) - (p_{.i} + p_{i.})(1-p_o)]^2 \\ 
\ \ \ + (1-p_o)^2 \displaystyle\sum_{i=1}^{k} \displaystyle\sum_{j=1 \atop i\not=j}^{k} p_{ij} (p_{.i} + p_{j.})^2 - (p_op_c-2p_c+p_o)^2 \} $
and under the null hypothesis it is given by:
$\ \ \ \widehat{\var}(\hat{\kappa}) = \frac{1}{N(1-p_c)^2} \{ \displaystyle\sum_{i=1}^{k} p_{.i}p_{i.} [1- (p_{.i} + p_{i.})^2] + \displaystyle\sum_{i=1}^{k} \displaystyle\sum_{j=1, i\not=j}^{k} p_{.i}p_{j.}(p_{.i} + p_{j.})^2 - p_c^2 \} $
Congalton's method seems to be based on the delta method for obtaining variances (Agresti, 1990; Agresti, 2002); however I am not sure on what the delta method is or why it has to be used. The $\kappa$ variance, under this method, is given by:
$\ \ \ \widehat{\var}(\hat{\kappa}) = \frac{1}{n} \{ \frac{\theta_1 (1-\theta_1)}{(1-\theta_2)^2} + \frac{2(1-\theta_1)(2\theta_1\theta_2-\theta_3)}{(1-\theta_2)^3} + \frac{(1-\theta_1)^2(\theta_4-4\theta_2^2)}{(1-\theta_2)^4} \} $
in which
$\ \ \ \theta_1 = \frac{1}{n} \displaystyle\sum_{i=1}^{k} n_{ii}$
$\ \ \ \theta_2 = \frac{1}{n^2} \displaystyle\sum_{i=1}^{k} n_{i+}n_{+i}$
$\ \ \ \theta_3 = \frac{1}{n^2} \displaystyle\sum_{i=1}^{k} n_{ii}(n_{i+} + n_{+i})$
$\ \ \ \theta_4 = \frac{1}{n^3} \displaystyle\sum_{i=1}^{k} \displaystyle\sum_{j=1}^{k} n_{ij}(n_{j+} + n_{+i})^2$
(Congalton uses a $+$ subscript rather than a $.$, but it seems to mean the same thing. In addition, I am supposing that $n_{ij}$ should be a counting matrix, i.e. the confusion matrix before being divided by the number of samples as related by the formula $p_{ij} = \frac{n_{ij}}{\mathrm{samples}}$)
Another weird part is that Colgaton's book seems to refer the original paper by Cohen, but does not seems to cite the corrections to the Kappa variance published by Fleiss et al, not until he goes on to discuss weighted Kappa. Perhaps his first publication was written when the true formula for kappa was still lost in confusion?
Is somebody able to explain why those differences? Or why would someone use the delta method variance instead of the corrected version by Fleiss?
[1]: Fleiss, Joseph L.; Cohen, Jacob; Everitt, B. S.; Large sample standard errors of kappa and weighted kappa. Psychological Bulletin, Vol 72(5), Nov 1969, 323-327. doi: 10.1037/h0028106 
[2]: Cohen, Jacob (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement 20 (1): 37–46. DOI:10.1177/001316446002000104.
[3]: Alan Agresti, Categorical Data Analysis, 2nd edition. John Wiley and Sons, 2002.
[4]: Russell G. Congalton and Green, K.; Assessing the Accuracy of Remotely Sensed Data: Principles and Practices, 2nd edition. 2009.
","['estimation', 'variance', 'reliability', 'cohens-kappa']",
"How well can multiple regression really ""control for"" covariates?","
We’re all familiar with observational studies that attempt to establish a causal link between a nonrandomized predictor X and an outcome by including every imaginable potential confounder in a multiple regression model. By thus “controlling for” all the confounders, the argument goes, we isolate the effect of the predictor of interest.
I’m developing a growing discomfort with this idea, based mostly on off-hand remarks made by various professors of my statistics classes. They fall into a few main categories:
1. You can only control for covariates that you think of and measure.
    This is obvious, but I wonder if it is actually the most pernicious and insurmountable of all.
2. The approach has led to ugly mistakes in the past.
For example, Petitti & Freedman (2005) discuss how decades’ worth of statistically
    adjusted observational studies came to disastrously
    incorrect conclusions on the effect of hormone replacement therapy
    on heart disease risk. Later RCTs found nearly opposite effects.
3. The predictor-outcome relationship can behave strangely when you
    control for covariates.
Yu-Kang Tu, Gunnell, & Gilthorpe (2008)
    discuss some different manifestations, including Lord’s
    Paradox, Simpson’s Paradox, and suppressor variables. 
4. It is difficult for a single model (multiple regression) to
    adequately adjust for covariates and simultaneously model the
    predictor-outcome relationship. 
I’ve heard this given as a reason
    for the superiority of methods like propensity scores and stratification on confounders, but I'm not sure I really understand it.
5. The ANCOVA model requires the covariate and predictor of interest to be independent. 
Of course, we adjust for confounders precisely BECAUSE they're correlated with the predictor of interest, so, it seems, the model will be unsuccessful in the exact instances when we want it the most. The argument goes that adjustment is only appropriate for noise-reduction in randomized trials. Miller & Chapman, 2001 give a great review. 
So my questions are:

How serious are these problems and others I might not know of?
How afraid should I be when I see a study that ""controls for everything""?

(I hope this question isn't venturing too far into discussion territory and happily invite any suggestions for improving it.)
EDIT: I added point 5 after finding a new reference.
","['multiple-regression', 'ancova', 'observational-study']",
How to plot trends properly,"
I am creating a graph to show trends in death rates (per 1000 ppl.) in different countries and the story that should come from the plot is that Germany (light blue line) is the only one whose trend is increasing after 1932. 
This is my first (basic) try 

In my opinion, this graph is already showing what we want it to tell but it is not super intuitive. Do you have any suggestion to make it clearer that distinction among trends? I was thinking of plotting growth rates but I tried and it is not that better.
The data are the following 
year     de     fr      be       nl     den      ch     aut     cz       pl
1927    10.9    16.5    13      10.2    11.6    12.4    15      16      17.3
1928    11.2    16.4    12.8    9.6     11      12      14.5    15.1    16.4
1929    11.4    17.9    14.4    10.7    11.2    12.5    14.6    15.5    16.7
1930    10.4    15.6    12.8    9.1     10.8    11.6    13.5    14.2    15.6
1931    10.4    16.2    12.7    9.6     11.4    12.1    14      14.4    15.5
1932    10.2    15.8    12.7    9       11      12.2    13.9    14.1    15
1933    10.8    15.8    12.7    8.8     10.6    11.4    13.2    13.7    14.2
1934    10.6    15.1    11.7    8.4     10.4    11.3    12.7    13.2    14.4
1935    11.4    15.7    12.3    8.7     11.1    12.1    13.7    13.5    14
1936    11.7    15.3    12.2    8.7     11      11.4    13.2    13.3    14.2
1937    11.5    15      12.5    8.8     10.8    11.3    13.3    13.3    14

",['data-visualization'],Sometimes less is more. With less detail about the year-to-year variations and the country distinctions you can provide more information about the trends. Since the other countries are moving mostly together you can get by without separate colors.In using a smoother you're requiring the reader to trust that you haven't smoothed over any interesting variation.Update after getting a couple requests for code:I made this in JMP's interactive Graph Builder. The JMP script is:));
When to use simulations?,"
So this is a very simple and basic question. However, when I was in school, I paid very little attention to the whole concept of simulations in class and that's left me a little terrified of that process.

Can you explain the simulation process in laymen terms? (could be for generating data, regression coefficients, etc)
What are some practical situations/problems when one would use simulations?

I would prefer any examples given to be in R.
",['simulation'],"A quantitative model emulates some behavior of the world by (a) representing objects by some of their numerical properties and (b) combining those numbers in a definite way to produce numerical outputs that also represent properties of interest.In this schematic, three numerical inputs on the left are combined to produce one numerical output on the right.  The number lines indicate possible values of the inputs and output; the dots show specific values in use.  Nowadays digital computers usually perform the calculations, but they are not essential: models have been calculated with pencil-and-paper or by building ""analog"" devices in wood, metal, and electronic circuits.As an example, perhaps the preceding model sums its three inputs.  R code for this model might look likeIts output simply is a number,-0.1We cannot know the world perfectly: even if the model happens to work exactly the way the world does, our information is imperfect and things in the world vary.  (Stochastic) simulations help us understand how such uncertainty and variation in the model inputs ought to translate into uncertainty and variation in the outputs.  They do so by varying the inputs randomly, running the model for each variation, and summarizing the collective output.""Randomly"" does not mean arbitrarily.  The modeler must specify (whether knowingly or not, whether explicitly or implicitly) the intended frequencies of all the inputs.  The frequencies of the outputs provide the most detailed summary of the results.The same model, shown with random inputs and the resulting (computed) random output.The figure displays frequencies with histograms to represent distributions of numbers.  The intended input frequencies are shown for the inputs at left, while the computed output frequency, obtained by running the model many times, is shown at right.Each set of inputs to a deterministic model produces a predictable numeric output.  When the model is used in a stochastic simulation, however, the output is a distribution (such as the long gray one shown at right).  The spread of the output distribution tells us how the model outputs can be expected to vary when its inputs vary.The preceding code example might be modified like this to turn it into a simulation:Its output has been summarized with a histogram of all the numbers generated by iterating the model with these random inputs:Peering behind the scenes, we may inspect some of the many random inputs that were passed to this model:The output shows the first five out of $100,000$ iterations, with one column per iteration:Arguably, the answer to the second question is that simulations can be used everywhere.  As a practical matter, the expected cost of running the simulation should be less than the likely benefit.  What are the benefits of understanding and quantifying variability?  There are two primary areas where this is important:Seeking the truth, as in science and the law.  A number by itself is useful, but it is far more useful to know how accurate or certain that number is.Making decisions, as in business and daily life.  Decisions balance risks and benefits.  Risks depend on the possibility of bad outcomes.  Stochastic simulations help assess that possibility.Computing systems have become powerful enough to execute realistic, complex models repeatedly.  Software has evolved to support generating and summarizing random values quickly and easily (as the second R example shows).  These two factors have combined over the last 20 years (and more) to the point where simulation is routine.  What remains is to help people (1) specify appropriate distributions of inputs and (2) understand the distribution of outputs.  That is the domain of human thought, where computers so far have been little help."
Are there any good movies involving mathematics or probability?,"
Can you suggest some good movies which involve math, probabilities etc? One example is 21. I would also be interested in movies that involve algorithms (e.g. text decryption). In general ""geeky"" movies with famous scientific theories but no science fiction or documentaries. Thanks in advance!
","['probability', 'references']",Pi
What is your favorite statistical graph?,"
This is a favorite of mine

This example is in a humorous vein (credit goes to a former professor of mine, Steven Gortmaker), but I am also interested in graphs that you feel beautifully capture and communicate a statistical insight or method, along with your ideas about same.
One entry per answer. Of course, this question is along the same line as What is your favorite ""data analysis"" cartoon?
Kindly provide proper credit/citations with any images you provide.
",['data-visualization'],
Negative values for AICc (corrected Akaike Information Criterion),"
I have calculated AIC and AICc to compare two general linear mixed models; The AICs are positive with model 1 having a lower AIC than model 2.  However, the values for AICc are both negative (model 1 is still < model 2).  Is it valid to use and compare negative AICc values? 
","['mixed-model', 'model-selection', 'aic']","All that matters is the difference between two AIC (or, better, AICc) values, representing the fit to two models.  The actual value of the AIC (or AICc), and whether it is positive or negative, means nothing. If you simply changed the units the data are expressed in, the AIC (and AICc) would change dramatically. But the difference between the AIC of the two alternative models would not change at all.Bottom line: Ignore the actual value of AIC (or AICc) and whether it is positive or negative. Ignore also the ratio of two AIC (or AICc) values. Pay attention only to the difference."
What is the reason why we use natural logarithm (ln) rather than log to base 10 in specifying function in econometrics?,"
What is the reason why we use natural logarithm (ln) rather than log to base 10 in specifying functions in econometrics?
",['econometrics'],
How seriously should I think about the different philosophies of statistics?,"
I've just finished a module where we covered the different approaches to statistical problems – mainly Bayesian vs frequentist. The lecturer also announced that she is a frequentist. We covered some paradoxes and generally the quirks of each approach (long run frequencies, prior specification, etc). This has got me thinking – how seriously do I need to consider this? If I want to be a statistician, do I need to align myself with one philosophy? Before I approach a problem, do I need to specifically mention which school of thought I will be applying? And crucially, do I need to be careful that I don't mix frequentist and Bayesian approaches and cause contradictions/paradoxes?
","['bayesian', 'frequentist', 'philosophical']","I think that the main takeaway here is this: the mere fact that there are these different philosophies of statistics and disagreement over them implies that translating the ""hard numbers"" that one gets from applying statistical formulae into ""real world"" decisions is a non-trivial problem and is fraught with interpretive peril.Frequently, people use statistics to influence their decision-making in the real world.  For example, scientists aren't running randomized trials on COVID vaccines right now for funsies: it is because they want to make real world decisions about whether or not to administer a particular vaccine candidate to the populace.  Although it may be a logistical challenge to gather up 1000 test subjects and observe them over the course of the vaccine, the math behind all of this is well-defined whether you are a Frequentist or a Bayesian:  You take the data you gathered, cram it through the formulae and numbers pop out the other end.However, those numbers can sometimes be difficult to interpret:  Their relationship to the real world depends on many non-mathematical things – and this is where the philosophy bit comes in.  The real world interpretation depends on how we went about gathering those test subjects.  It depends on how likely we anticipated this vaccine to be effective a priori (did we pull a molecule out of a hat, or did we start with a known-effective vaccine-production method?).  It depends on (perhaps unintuitively) how many other vaccine candidates we happen to be testing.  It depends on etc., etc., etc.Bayesians have attempted to introduce additional mathematical frameworks to help alleviate some of these interpretation problems.  I think the fact that the Frequentist methods continue to proliferate shows that these additional frameworks have not been super successful in helping people translate their statistical computations into real world actions (although, to be sure, Bayesian techniques have led to many other advances in the field, not directly related to this specific problem).To answer your specific questions: you don't need to align yourself with one philosophy.  It may help to be specific about your approach, but it will generally be totally obvious that you are doing a Bayesian analysis the moment you start talking about priors.  Lastly, though, you should consider all of this very seriously, because as a statistician it will be your ethical duty to ensure that the numbers that you provide people are used responsibly – because correctly interpreting those numbers is a hard problem.  Whether you interpret your numbers through the lens of Frequentist or Bayesian philosophy isn't a huge deal, but interpretation of your numbers requires familiarity with the relevant philosophy."
Approximate $e$ using Monte Carlo Simulation,"
I've been looking at Monte Carlo simulation recently, and have been using it to approximate constants such as $\pi$ (circle inside a rectangle, proportionate area).
However, I'm unable to think of a corresponding method of approximating the value of $e$ [Euler's number] using Monte Carlo integration.
Do you have any pointers on how this can be done?
","['simulation', 'monte-carlo', 'algorithms', 'random-generation', 'numerical-integration']","The simple and elegant way to estimate $e$ by Monte Carlo is described in this paper. The paper is actually about teaching $e$. Hence, the approach seems perfectly fitting for your goal. The idea's based on an exercise from a popular Ukrainian textbook on probability theory by Gnedenko.
See ex.22 on p.183It happens so that $E[\xi]=e$, where $\xi$ is a random variable that is defined as follows. It's the minimum number of $n$ such that $\sum_{i=1}^n r_i>1$ and $r_i$ are random numbers from uniform distribution on $[0,1]$. Beautiful, isn't it?!Since it's an exercise, I'm not sure if it's cool for me to post the solution  (proof) here :) If you'd like to prove it yourself, here's a tip: the chapter is called ""Moments"", which should point you in right direction.If you want to implement it yourself, then don't read further!This is a simple algorithm for Monte Carlo simulation. Draw a uniform random, then another one and so on until the sum exceeds 1. The number of randoms drawn is your first trial. Let's say you got:Then your first trial rendered 3. Keep doing these trials, and you'll notice that in average you get $e$.MATLAB code, simulation result and the histogram follow.The result and the histogram:UPDATE:
I updated my code to get rid of the array of trial results so that it doesn't take RAM. I also printed the PMF estimation.Update 2:
Here's my Excel solution. Put a button in Excel and link it to the following VBA macro:Enter the number of trials, such as 1000, in the cell D1, and click the button.
Here how the screen should look like after the first run:UPDATE 3:
Silverfish inspired me to another way, not as elegant as the first one but still cool. It calculated the volumes of n-simplexes using Sobol sequences.Coincidentally he wrote the first book on Monte Carlo method I read back in high school. It's the best introduction to the method in my opinion.UPDATE 4:Silverfish in comments suggested a simple Excel formula implementation. This is the kind of result you get with his approach after about total 1 million random numbers and 185K trials:Obviously, this is much slower than Excel VBA implementation. Especially, if you modify my VBA code to not update the cell values inside the loop, and only do it once all stats are collected.UPDATE 5Xi'an's solution #3 is closely related (or even the same in some sense as per jwg's comment in the thread). It's hard to say who came up with the idea first Forsythe or Gnedenko. Gnedenko's original 1950 edition in Russian doesn't have Problems sections in Chapters. So, I couldn't find this problem at a first glance where it is in later editions. Maybe it was added later or buried in the text.As I commented in Xi'an's answer, Forsythe's approach is linked to another interesting area: the distribution of distances between peaks (extrema) in random (IID) sequences. The mean distance happens to be 3. The down sequence in Forsythe's approach ends with a bottom, so if you continue sampling you'll get another bottom at some point, then another etc. You could track the distance between them and build the distribution."
R - Confused on Residual Terminology,"

Root mean square error
residual sum of squares
residual standard error
mean squared error
test error

I thought I used to understand these terms but the more I do statistic problems the more I have gotten myself confused where I second guess myself. I would like some re-assurance & a concrete example
I can find the equations easily enough online but I am having trouble getting a 'explain like I'm 5' explanation of these terms so I can crystallize in my head the differences and how one leads to another.
If anyone can take this code below and point out how I would calculate each one of these terms I would appreciate it. R code would be great..
Using this example below:
summary(lm(mpg~hp, data=mtcars))

Show me in R code how to find:
rmse = ____
rss = ____
residual_standard_error = ______  # i know its there but need understanding
mean_squared_error = _______
test_error = ________

Bonus points for explaining like i'm 5 the differences/similarities between these.  example: 
rmse = squareroot(mss)

","['r', 'regression', 'residuals']","As requested, I illustrate using a simple regression using the mtcars data:The mean squared error (MSE) is the mean of the square of the residuals:Root mean squared error (RMSE) is then the square root of MSE:Residual sum of squares (RSS) is the sum of the squared residuals:Residual standard error (RSE) is the square root of (RSS / degrees of freedom):The same calculation, simplified because we have previously calculated rss:The term test error in the context of regression (and other predictive analytics techniques) usually refers to calculating a test statistic on test data, distinct from your training data.In other words, you estimate a model using a portion of your data (often an 80% sample) and then calculating the error using the hold-out sample.  Again, I illustrate using mtcars, this time with an 80% sampleEstimate the model, then predict with the hold-out data:Combine the original data and prediction in a data frameNow compute your test statistics in the normal way.  I illustrate MSE and RMSE:Note that this answer ignores weighting of the observations."
How to perform two-sample t-tests in R by inputting sample statistics rather than the raw data?,"
Let's say we have the statistics given below
gender mean sd n
f 1.666667 0.5773503 3
m 4.500000 0.5773503 4

How do you perform a two-sample t-test (to see if there is a significant difference between the means of men and women in some variable) using statistics like this rather than actual data?
I couldn't find anywhere on the internet how to do this. Most of the tutorials and even the manual deal with the test with the actual data set only.
","['r', 't-test']","You can write your own function based on what we know about the mechanics of the two-sample $t$-test. For example, this will do the job:Example usage:This matches the result of t.test:"
Evaluation measures of goodness or validity of clustering (without having truth labels) [duplicate],"







This question already has answers here:
                                
                            




How to select a clustering method? How to validate a cluster solution (to warrant the method choice)?

                                (3 answers)
                            

Closed 3 years ago.



I'm clustering a set of data but I don't have truth document that allow me to evaluate the result of clustering (I have unlabelled data), so I can not use an external evaluation measure. In this case, is there any efficient evaluation measures - internal cluster validity measures, that will allow me to incrementally evaluate the clustering result each time I cluster some new data to the current clustering?
","['clustering', 'unsupervised-learning']",
When should I use a variational autoencoder as opposed to an autoencoder?,"
I understand the basic structure of variational autoencoder and normal (deterministic) autoencoder and the math behind them, but when and why would I prefer one type of autoencoder to the other? All I can think about is the prior distribution of latent variables of variational autoencoder allows us to sample the latent variables and then construct the new image. What advantage does the stochasticity of variational autoencoder over the deterministic autoencoder?
","['deep-learning', 'autoencoders', 'variational-bayes']",
K-fold vs. Monte Carlo cross-validation,"
I am trying to learn various cross validation methods, primarily with intention to apply to supervised multivariate analysis techniques. Two I have come across are K-fold and Monte Carlo cross-validation techniques. I have read that K-fold is a variation on Monte Carlo but I'm not sure I fully understand what makes up the definition of Monte Carlo. Could someone please explain the distinction between these two methods?
","['cross-validation', 'monte-carlo']","Suppose you have 100 data points. For $k$-fold cross validation, these 100 points are divided into $k$ equal sized and mutually-exclusive 'folds'. For $k$=10, you might assign points 1-10 to fold #1, 11-20 to fold #2, and so on, finishing by assigning points 91-100 to fold #10. Next, we select one fold to act as the test set, and use the remaining $k-1$ folds to form the training data. For the first run, you might use points 1-10 as the test set and 11-100 as the training set. The next run would then use points 11-20 as the test set and train on points 1-10 plus 21-100, and so forth, until each fold is used once as the test set.Monte Carlo works somewhat differently. You randomly select (without replacement) some fraction of your data to form the training set, and then assign the rest of the points to the test set. This process is then repeated multiple times, generating (at random) new training and test partitions each time. For example, suppose you chose to use 10% of your data as test data. Then your test set on rep #1 might be points 64, 90, 63, 42, 65, 49, 10, 64, 96, and 48. On the next run, your test set might be 90, 60, 23, 67, 16, 78, 42, 17, 73, and 26. Since the partitions are done independently for each run, the same point can appear in the test set multiple times, which is the major difference between Monte Carlo and cross validation.Each method has its own advantages and disadvantages. Under cross validation, each point gets tested exactly once, which seems fair. However, cross-validation only explores a few of the possible ways that your data could have been partitioned. Monte Carlo lets you explore somewhat more possible partitions, though you're unlikely to get all of them--there are $\binom{100}{50} \approx 10^{28}$ possible ways to 50/50 split a 100 data point set(!). If you're attempting to do inference (i.e., statistically compare two algorithms), averaging the results of a $k$-fold cross validation run gets you a (nearly) unbiased estimate of the algorithm's performance, but with high variance (as you'd expect from having only 5 or 10 data points). Since you can, in principle, run it for as long as you want/can afford, Monte Carlo cross validation can give you a less variable, but more biased estimate.Some approaches fuse the two, as in the 5x2 cross validation (see Dietterich (1998) for the idea, though I think there have been some further improvements since then), or by correcting for the bias (e.g., Nadeau and Bengio, 2003)."
Does the beta distribution have a conjugate prior?,"
I know that the beta distribution is conjugate to the binomial. But what is the conjugate prior of the beta? Thank you.
","['beta-distribution', 'conjugate-prior']","It seems that you already gave up on conjugacy. Just for the record, one thing that I've seen people doing (but don't remember exactly where, sorry) is a reparameterization like this. If $X_1,\dots,X_n$ are conditionally iid, given $\alpha,\beta$, such that $X_i\mid\alpha,\beta\sim\mathrm{Beta}(\alpha,\beta)$, remember that
$$
  \mathbb{E}[X_i\mid\alpha,\beta]=\frac{\alpha}{\alpha+\beta} =: \mu
$$
and
$$
  \mathbb{Var}[X_i\mid\alpha,\beta] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} =: \sigma^2 \, .
$$
Hence, you may reparameterize the likelihood in terms of $\mu$ and $\sigma^2$ and use as a prior
$$
  \sigma^2\mid\mu \sim \mathrm{U}[0,\mu(1-\mu)] \qquad \qquad \mu\sim\mathrm{U}[0,1] \, .
$$
Now you're ready to compute the posterior and explore it by your favorite computational method."
Difference between LOESS and LOWESS,"
What is the difference between LOESS (locally estimated scatterplot smoothing) and LOWESS (locally weighted scatterplot smoothing)? From Wikipedia I can only see that LOESS is a generalization of LOWESS. Do they have slightly different parameters?
","['nonparametric', 'loess']","I think it is important to distinguish between methods and their implementations in software. The main difference with respect to the first is that lowess allows only one predictor, whereas loess can be used to smooth multivariate data into a kind of surface. It also gives you confidence intervals. In these senses, loess is a generalization. Both smooth by using tricube weighting around each point, and loess also adds an optional robustification option that re-weights residuals using biweight weighting.  Now for the implementation. In some software, lowess uses a linear polynomial, while loess uses a quadratic polynomial (though you can alter that). The defaults and shortcuts that the algorithms use are often quite different, so that it is hard to get the univariate outputs to match exactly. On the other hand, I am not aware of a case where the choice between the two made a substantive difference.  "
Fake uniform random numbers: More evenly distributed than true uniform data,"
I'm looking for a way to generate random numbers that appear to be uniform distributed -- and every test will show them to be uniform -- except that they are more evenly distributed than true uniform data.
The problem I have with the ""true"" uniform randoms is that they will occasionally cluster. This effect is stronger at a low sample size. Roughly said: when I draw two Uniform randoms in U[0;1], chances are around 10% that they are within a range of 0.1, and 1% that they are within 0.01.
So I'm looking for a good way to generate random numbers that are more evenly distributed than uniform randoms.
Use case example: say I'm doing a computer game, and I want to place treasure randomly on a map (not caring about any other thing). I don't want the treasure to be all in one place, it should be all over the map. With uniform randoms, if I place, say, 10 objects, the chances are not that low that there are 5 or so really close to each other. This may give one player an advantage over another. Think of minesweeper, chances (albeit low, if there are enough mines) are that you are really lucky and win with a single click.
A very naive approach for my problem is to divide the data into a grid. As long as the number is large enough (and has factors), one can enforce extra uniformness this way. So instead of drawing 12 random variables from U[0;1], I can draw 6 from U[0;.5] and 6 from U[0.5;1], or 4 from U[0;1/3] + 4 from U[1/3;2/3] + 4 from U[2/3; 1].
Is there any better way to get this extra evenness into the uniform? It probably only works for batch randoms (when drawing a single random, I obviously have to consider the whole range). In particular, I can shuffle the records again afterwards (so it's not the first four from the first third).
How about doing it incrementally? So the first is on U[0;1], then two from each halves, one from each third, one from each fourth? Has this been investigated, and how good is it?
I might have to be careful to use different generators for x and y to not get them correlated (the first xy would always be in the bottom half, the second in the left half and bottom third, the third in center third and top third... so at least some random bin permutation is also needed. and in the long run, it will be too even, I guess.
As a side node, is there a well-known test whether some distribution is too evenly distributed to be truly uniform? So testing ""true uniform"" vs. ""someone messed with the data and distributed the items more evenly"". If I recall correctly, Hopkins Statistic can measure this, but can it be used for testing, too? Also somewhat an inverse K-S-Test: if the largest deviation is below a certain expected threshold, the data is too evenly distributed?
","['distributions', 'random-generation', 'uniform-distribution', 'quasi-monte-carlo']","Yes, there are many ways to produce a sequence of numbers that are more evenly distributed than random uniforms. In fact, there is a whole field dedicated to this question; it is the backbone of quasi-Monte Carlo (QMC). Below is a brief tour of the absolute basics.Measuring uniformityThere are many ways to do this, but the most common way has a strong, intuitive, geometric flavor. Suppose we are concerned with generating $n$ points $x_1,x_2,\ldots,x_n$ in $[0,1]^d$ for some positive integer $d$. Define
$$\newcommand{\I}{\mathbf 1}
D_n := \sup_{R \in \mathcal R}\,\left|\frac{1}{n}\sum_{i=1}^n \I_{(x_i \in R)} - \mathrm{vol}(R)\right| \>,
$$
where $R$ is a rectangle $[a_1, b_1] \times \cdots \times [a_d, b_d]$ in $[0,1]^d$ such that $0 \leq a_i \leq b_i \leq 1$ and $\mathcal R$ is the set of all such rectangles. The first term inside the modulus is the ""observed"" proportion of points inside $R$ and the second term is the volume of $R$, $\mathrm{vol}(R) = \prod_i (b_i - a_i)$.The quantity $D_n$ is often called the discrepancy or extreme discrepancy of the set of points $(x_i)$. Intuitively, we find the ""worst"" rectangle $R$ where the proportion of points deviates the most from what we would expect under perfect uniformity.This is unwieldy in practice and difficult to compute. For the most part, people prefer to work with the star discrepancy,
$$
D_n^\star = \sup_{R \in \mathcal A} \,\left|\frac{1}{n}\sum_{i=1}^n \I_{(x_i \in R)} - \mathrm{vol}(R)\right| \>.
$$
The only difference is the set $\mathcal A$ over which the supremum is taken. It is the set of anchored rectangles (at the origin), i.e., where $a_1 = a_2 = \cdots = a_d = 0$.Lemma: $D_n^\star \leq D_n \leq 2^d D_n^\star$ for all $n$, $d$.
Proof. The left hand bound is obvious since $\mathcal A \subset \mathcal R$. The right-hand bound follows because every $R \in \mathcal R$ can be composed via unions, intersections and complements of no more than $2^d$ anchored rectangles (i.e., in $\mathcal A$).Thus, we see that $D_n$ and $D_n^\star$ are equivalent in the sense that if one is small as $n$ grows, the other will be too. Here is a (cartoon) picture showing candidate rectangles for each discrepancy.Examples of ""good"" sequencesSequences with verifiably low star discrepancy $D_n^\star$ are often called, unsurprisingly, low discrepancy sequences.van der Corput. This is perhaps the simplest example. For $d=1$, the van der Corput sequences are formed by expanding the integer $i$ in binary and then ""reflecting the digits"" around the decimal point. More formally, this is done with the radical inverse function in base $b$,
$$\newcommand{\rinv}{\phi}
\rinv_b(i) = \sum_{k=0}^\infty a_k b^{-k-1} \>,
$$
where $i = \sum_{k=0}^\infty a_k b^k$ and $a_k$ are the digits in the base $b$ expansion of $i$. This function forms the basis for many other sequences as well. For example, $41$ in binary is $101001$ and so $a_0 = 1$, $a_1 = 0$, $a_2 = 0$, $a_3 = 1$, $a_4 = 0$ and $a_5 = 1$. Hence, the 41st point in the van der Corput sequence is $x_{41} = \rinv_2(41) = 0.100101\,\text{(base 2)} = 37/64$.Note that because the least significant bit of $i$ oscillates between $0$ and $1$, the points $x_i$ for odd $i$ are in $[1/2,1)$, whereas the points $x_i$ for even $i$ are in $(0,1/2)$.Halton sequences. Among the most popular of classical low-discrepancy sequences, these are extensions of the van der Corput sequence to multiple dimensions. Let $p_j$ be the $j$th smallest prime. Then, the $i$th point $x_i$ of the $d$-dimensional Halton sequence is
$$
x_i = (\rinv_{p_1}(i), \rinv_{p_2}(i),\ldots,\rinv_{p_d}(i)) \>.
$$
For low $d$ these work quite well, but have problems in higher dimensions. Halton sequences satisfy $D_n^\star = O(n^{-1} (\log n)^d)$. They are also nice because they are extensible in that the construction of the points does not depend on an a priori choice of the length of the sequence $n$.Hammersley sequences. This is a very simple modification of the Halton sequence. We instead use
$$
x_i = (i/n, \rinv_{p_1}(i), \rinv_{p_2}(i),\ldots,\rinv_{p_{d-1}}(i)) \>.
$$
Perhaps surprisingly, the advantage is that they have better star discrepancy $D_n^\star = O(n^{-1}(\log n)^{d-1})$.Here is an example of the Halton and Hammersley sequences in two dimensions.Faure-permuted Halton sequences. A special set of permutations (fixed as a function of $i$) can be applied to the digit expansion $a_k$ for each $i$ when producing the Halton sequence. This helps remedy (to some degree) the problems alluded to in higher dimensions. Each of the permutations has the interesting property of keeping $0$ and $b-1$ as fixed points.Lattice rules. Let $\beta_1, \ldots, \beta_{d-1}$ be integers. Take
$$
x_i = (i/n, \{i \beta_1 / n\}, \ldots, \{i \beta_{d-1}/n\}) \>,
$$
where $\{y\}$ denotes the fractional part of $y$. Judicious choice of the $\beta$ values yields good uniformity properties. Poor choices can lead to bad sequences. They are also not extensible. Here are two examples.$(t,m,s)$ nets. $(t,m,s)$ nets in base $b$ are sets of points such that every rectangle of volume $b^{t-m}$ in $[0,1]^s$ contains $b^t$ points. This is a strong form of uniformity. Small $t$ is your friend, in this case. Halton, Sobol' and Faure sequences are examples of $(t,m,s)$ nets. These lend themselves nicely to randomization via scrambling. Random scrambling (done right) of a $(t,m,s)$ net yields another $(t,m,s)$ net. The MinT project keeps a collection of such sequences.Simple randomization: Cranley-Patterson rotations. Let $x_i \in [0,1]^d$ be a sequence of points. Let $U \sim \mathcal U(0,1)$. Then the points $\hat x_i = \{x_i + U\}$ are uniformly distributed in $[0,1]^d$.Here is an example with the blue dots being the original points and the red dots being the rotated ones with lines connecting them (and shown wrapped around, where appropriate).Completely uniformly distributed sequences. This is an even stronger notion of uniformity that sometimes comes into play. Let $(u_i)$ be the sequence of points in $[0,1]$ and now form overlapping blocks of size $d$ to get the sequence $(x_i)$. So, if $s = 3$, we take $x_1 = (u_1,u_2,u_3)$ then $x_2 = (u_2,u_3,u_4)$, etc. If, for every $s \geq 1$, $D_n^\star(x_1,\ldots,x_n) \to 0$, then $(u_i)$ is said to be completely uniformly distributed. In other words, the sequence yields a set of points of any dimension that have desirable $D_n^\star$ properties.As an example, the van der Corput sequence is not completely uniformly distributed since for $s = 2$, the points $x_{2i}$ are in the square $(0,1/2) \times [1/2,1)$ and the points $x_{2i-1}$ are in $[1/2,1) \times (0,1/2)$. Hence there are no points in the square $(0,1/2) \times (0,1/2)$ which implies that for $s=2$, $D_n^\star \geq 1/4$ for all $n$.Standard referencesThe Niederreiter (1992) monograph and the Fang and Wang (1994) text are places to go for further exploration."
What is the difference between finite and infinite variance,"
What is the difference between finite and infinite variance ? My stats knowledge is rather basic; Wikipedia / Google wasn't much help here.
","['variance', 'expected-value', 'intuition', 'partial-moments']",
What are best practices in identifying interaction effects?,"
Other than literally testing each possible combination of variable(s) in a model (x1:x2 or x1*x2 ... xn-1 * xn). How do you identify if an interaction SHOULD or COULD exist between your independent (hopefully) variables? 
What are best practices in attempting to identify interactions?
Is there a graphical technique that you could or do use?
","['regression', 'modeling', 'interaction']","Cox and Wermuth (1996) or Cox (1984) discussed some methods for detecting interactions. The problem is usually how general the interaction terms should be. Basically, we 
(a) fit (and test) all second-order interaction terms, one at a time, and (b) plot their corresponding p-values (i.e., the No. terms as a function of $1-p$). The idea is then to look if a certain number of interaction terms should be retained: Under the assumption that all interaction terms are null the distribution of the p-values should be uniform (or equivalently, the points on the scatterplot should be roughly distributed along a line passing through the origin).Now, as @Gavin said, fitting many (if not all) interactions might lead to overfitting, but it is also useless in a certain sense (some high-order interaction terms often have no sense at all). However, this has to do with interpretation, not detection of interactions, and a good review was already provided by Cox in Interpretation of interaction: A review (The Annals of Applied Statistics 2007, 1(2), 371–385)--it includes references cited above. Other lines of research worth to look at are study of epistatic effects in genetic studies, in particular methods based on graphical models (e.g., An efficient method for identifying statistical interactors in gene association networks)."
Why KL divergence is non-negative?,"
Why is KL divergence non-negative?
From the perspective of information theory, I have such an intuitive understanding:
Say there are two ensembles $A$ and $B$ which are composed of the same set of elements labeled by $x$. $p(x)$ and $q(x)$ are different probability distributions over ensemble $A$ and $B$ respectively. 
From the perspective of information theory, $\log_{2}(P(x))$ is the least amount of bits that required for recording an element $x$ for ensemble $A$. So that the expectation
$$\sum_{x \in  ensemble}-p(x)\ln(p(x))$$
can be interpreted as at least how many bits that we need for recording an element in $A$ on average. 
Since this formula puts a lower bound on the bits that we need on average, so that for a different ensemble $B$ which brings about a different probability distribution $q(x)$, the bound that it gives for each element $x$ will surely not bit that is given by $p(x)$, which means taking the expectation,
$$\sum_{x\in ensemble}-p(x)\ln(q(x))$$
this average length will surely be greater than the former one, which leads to
$$\sum_{x\in ensemble }p(x)\frac{\ln(p(x))}{\ln(q(x))} > 0$$ 
I don't put $\ge$ here since $p(x)$ and $q(x)$ are different.
This is my intuitive understanding, is there a purely mathematical way of proving KL divergence is non-negative? The problem can be stated as:
Given $p(x)$ and $q(x)$ are both positive over real line, and $\int_{-\infty}^{+\infty}p(x)dx = 1$, $\int_{-\infty}^{+\infty}q(x)dx = 1$. 
Prove $$\int_{-\infty}^{+\infty}p(x)\ln\frac{p(x)}{q(x)}$$ is non-negative.
How can this be proved? Or can this be proved without extra conditions?
","['information-theory', 'kullback-leibler']","Proof 1:First note that $\ln a \leq a-1$ for all $a \gt 0$.We will now show that $-D_{KL}(p||q) \leq 0$ which means that $D_{KL}(p||q) \geq 0$\begin{align}
-D(p||q)&=-\sum_x p(x)\ln \frac{p(x)}{q(x)}\\
&= \sum_x p(x)\ln \frac{q(x)}{p(x)}\\
&\stackrel{\text{(a)}}{\leq} \sum_x p(x)\left(\frac{q(x)}{p(x)}-1\right)\\
&=\sum_x q(x) - \sum_x p(x)\\
&= 1 - 1\\
&= 0
\end{align}For inequality (a) we used the $\ln$ inequality explained in the beginning.Alternatively you can start with Gibbs' inequality which states:
$$
-\sum_x p(x) \log_2 p(x) \leq -\sum_x p(x)\log_2 q(x)
$$Then if we bring the left term to the right we get:
$$
\sum_x p(x) \log_2 p(x) - \sum_x p(x)\log_2 q(x)\geq 0 \\
\sum_x p(x)\log_2 \frac{p(x)}{q(x)}\geq 0 
$$The reason I am not including this as a separate proof is because if you were to ask me to prove Gibbs' inequality, I would have to start from the non-negativity of KL divergence and do the same proof from the top.Proof 2:
We use the Log sum inequality:
$$
\sum_{i=1}^{n} a_i \log_2 \frac{a_i}{b_i} \geq \left(\sum_{i=1}^{n} a_i\right)\log_2\frac{\sum_{i=1}^{n} a_i}{\sum_{i=1}^{n} b_i}
$$Then we can show that $D_{KL}(p||q) \geq 0$:
\begin{align}
D(p||q)&=\sum_x p(x)\log_2 \frac{p(x)}{q(x)}\\
&\stackrel{\text{(b)}}{\geq} \left(\sum_x p(x)\right)\log_2\frac{\sum_x p(x)}{\sum_x q(x)}\\
&=1 \cdot \log_2 \frac{1}{1}\\
&=0
\end{align}where we have used the Log sum inequality at (b).Proof 3:(Taken from the book ""Elements of Information Theory"" by Thomas M. Cover and Joy A. Thomas)\begin{align}
-D(p||q)&=-\sum_x p(x)\log_2 \frac{p(x)}{q(x)}\\
&= \sum_x p(x)\log_2 \frac{q(x)}{p(x)}\\
&\stackrel{\text{(c)}}{\leq} \log_2 \sum_x p(x)\frac{q(x)}{p(x)}\\
&=\log_2 1\\
&=0
\end{align}where at (c) we have used Jensen's inequality and the fact that $\log$ is a concave function."
How is Naive Bayes a Linear Classifier?,"
I've seen the other thread here but I don't think the answer satisfied the actual question.  What I have continually read is that Naive Bayes is a linear classifier (ex: here) (such that it draws a linear decision boundary) using the log odds demonstration.
However, I simulated two Gaussian clouds and fitted a decision boundary and got the results as such (library e1071 in r, using naiveBayes())

As we can see, the decision boundary is non-linear.  Is it trying to say that the parameters (conditional probabilities) are a linear combination in the log space rather than saying the classifier itself separates data linearly?
","['classification', 'naive-bayes']","In general the naive Bayes classifier is not linear, but if the likelihood factors $p(x_i \mid c)$ are from exponential families, the naive Bayes classifier corresponds to a linear classifier in a particular feature space. Here is how to see this.You can write any naive Bayes classifier as*$$p(c = 1 \mid \mathbf{x}) = \sigma\left( \sum_i \log \frac{p(x_i \mid c = 1)}{p(x_i \mid c = 0)} + \log \frac{p(c = 1)}{p(c = 0)} \right),$$where $\sigma$ is the logistic function. If $p(x_i \mid c)$ is from an exponential family, we can write it as$$p(x_i \mid c) = h_i(x_i)\exp\left(\mathbf{u}_{ic}^\top \phi_i(x_i) - A_i(\mathbf{u}_{ic})\right),$$and hence$$p(c = 1 \mid \mathbf{x}) = \sigma\left( \sum_i \mathbf{w}_i^\top \phi_i(x_i) + b \right),$$where\begin{align}
\mathbf{w}_i &= \mathbf{u}_{i1} - \mathbf{u}_{i0}, \\
b &= \log \frac{p(c = 1)}{p(c = 0)} - \sum_i \left( A_i(\mathbf{u}_{i1}) - A_i(\mathbf{u}_{i0}) \right).
\end{align}Note that this is similar to logistic regression – a linear classifier – in the feature space defined by the $\phi_i$. For more than two classes, we analogously get multinomial logistic (or softmax) regression. If $p(x_i \mid c)$ is Gaussian, then $\phi_i(x_i) = (x_i, x_i^2)$ and we should have
\begin{align}
w_{i1} &= \sigma_1^{-2}\mu_1 - \sigma_0^{-2}\mu_0, \\
w_{i2} &= 2\sigma_0^{-2} - 2\sigma_1^{-2}, \\
b_i &= \log \sigma_0 - \log \sigma_1,
\end{align}assuming $p(c = 1) = p(c = 0) = \frac{1}{2}$.*Here is how to derive this result:\begin{align}
p(c = 1 \mid \mathbf{x})
&= \frac{p(\mathbf{x} \mid c = 1) p(c = 1)}{p(\mathbf{x} \mid c = 1) p(c = 1) + p(\mathbf{x} \mid c = 0) p(c = 0)} \\
&= \frac{1}{1 + \frac{p(\mathbf{x} \mid c = 0) p(c = 0)}{p(\mathbf{x} \mid c = 1) p(c = 1)}} \\
&= \frac{1}{1 + \exp\left( -\log\frac{p(\mathbf{x} \mid c = 1) p(c = 1)}{p(\mathbf{x} \mid c = 0) p(c = 0)} \right)} \\
&= \sigma\left( \sum_i \log \frac{p(x_i \mid c = 1)}{p(x_i \mid c = 0)} + \log \frac{p(c = 1)}{p(c = 0)} \right)
\end{align}"
Differences between Bhattacharyya distance and KL divergence,"
I'm looking for an intuitive explanation for the following questions:
In statistics and information theory, what's the difference between Bhattacharyya distance and KL divergence, as measures of the difference between two discrete probability distributions?
Do they have absolutely no relationships and measure the distance between two probability distribution in totally different way?
","['mathematical-statistics', 'information-theory', 'kullback-leibler', 'bhattacharyya']","The Bhattacharyya coefficient is defined as $$D_B(p,q) = \int \sqrt{p(x)q(x)}\,\text{d}x$$ and can be turned into a distance $d_H(p,q)$ as $$d_H(p,q)=\{1-D_B(p,q)\}^{1/2}$$ which is called the Hellinger distance. A connection between this Hellinger distance and the Kullback-Leibler divergence is
$$d_{KL}(p\|q) \geq 2 d_H^2(p,q) = 2 \{1-D_B(p,q)\}\,,$$
since
\begin{align*}
d_{KL}(p\|q) &= \int \log \frac{p(x)}{q(x)}\,p(x)\text{d}x\\
&= 2\int \log \frac{\sqrt{p(x)}}{\sqrt{q(x)}}\,p(x)\text{d}x\\
&= 2\int -\log \frac{\sqrt{q(x)}}{\sqrt{p(x)}}\,p(x)\text{d}x\\
&\ge 2\int \left\{1-\frac{\sqrt{q(x)}}{\sqrt{p(x)}}\right\}\,p(x)\text{d}x\\
&= \int \left\{1+1-2\sqrt{p(x)}\sqrt{q(x)}\right\}\,\text{d}x\\
&= \int \left\{\sqrt{p(x)}-\sqrt{q(x)}\right\}^2\,\text{d}x\\
&= 2d_H(p,q)^2
\end{align*}However, this is not the question: if the Bhattacharyya distance is defined as$$d_B(p,q)\stackrel{\text{def}}{=}-\log D_B(p,q)\,,$$then
\begin{align*}d_B(p,q)=-\log D_B(p,q)&=-\log \int \sqrt{p(x)q(x)}\,\text{d}x\\
&\stackrel{\text{def}}{=}-\log \int h(x)\,\text{d}x\\
&= -\log \int \frac{h(x)}{p(x)}\,p(x)\,\text{d}x\\
&\le \int -\log \left\{\frac{h(x)}{p(x)}\right\}\,p(x)\,\text{d}x\\
&= \int \frac{-1}{2}\log \left\{\frac{h^2(x)}{p^2(x)}\right\}\,p(x)\,\text{d}x\\
\end{align*}
Hence, the inequality between the two distances is
$${d_{KL}(p\|q)\ge 2d_B(p,q)\,.}$$
One could then wonder whether this inequality follows from the first one. It happens to be the opposite: since $$-\log(x)\ge 1-x\qquad\qquad 0\le x\le 1\,,$$
we have the complete ordering$${d_{KL}(p\|q)\ge 2d_B(p,q)\ge 2d_H(p,q)^2\,.}$$"
Test for bimodal distribution,"
I wonder if there is any statistical test to ""test"" the significance of a bimodal distribution. I mean, How much my data meets the bimodal distribution or not? If so, is there any test in the R program?
","['r', 'hypothesis-testing', 'distributions', 'bimodal']","Another possible approach to this issue is to think about what might be going on behind the scenes that is generating the data you see.  That is, you can think in terms of a mixture model, for example, a Gaussian mixture model.  For instance, you might believe that your data are drawn from either a single normal population, or from a mixture of two normal distributions (in some proportion), with differing means and variances.  Of course, you don't have to believe that there are only one or two, nor do you have to believe that the populations from which the data are drawn need to be normal.  There are (at least) two R packages that allow you to estimate mixture models.  One package is flexmix, and another is mclust. Having estimated two candidate models, I believe it may be possible to conduct a likelihood ratio test.  Alternatively, you could use the parametric bootstrap cross-fitting method (pdf).  "
How is softmax_cross_entropy_with_logits different from softmax_cross_entropy_with_logits_v2?,"
Specifically, I suppose I wonder about this statement:

Future major versions of TensorFlow will allow gradients to flow
      into the labels input on backprop by default.

Which is shown when I use tf.nn.softmax_cross_entropy_with_logits. In the same message it urges me to have a look at tf.nn.softmax_cross_entropy_with_logits_v2. I looked through the documentation but it only states that for tf.nn.softmax_cross_entropy_with_logits_v2:

Backpropagation will happen into both logits and labels. To disallow backpropagation into labels, pass label tensors through a stop_gradients before feeding it to this function.

as opposed to, tf.nn.softmax_cross_entropy_with_logits's:

Backpropagation will happen only into logits.

Being very new to the subject (I'm trying to make my way through some basic tutorials) those statements are not very clear. I have a shallow understanding of backpropagation but what does the previous statement actually mean? How are backpropagation and the labels connected? And how does this change how I work with tf.nn.softmax_cross_entropy_with_logits_v2 as opposed to the original?
","['machine-learning', 'supervised-learning', 'tensorflow', 'backpropagation']","You have every reason to be confused, because in supervised learning one doesn't need to backpropagate to labels. They are considered fixed ground truth and only the weights need to be adjusted to match them.But in some cases, the labels themselves may come from a differentiable source, another network. One example might be adversarial learning. In this case, both networks might benefit from the error signal. That's the reason why tf.nn.softmax_cross_entropy_with_logits_v2 was introduced. Note that when the labels are the placeholders (which is also typical), there is no difference if the gradient through flows or not, because there are no variables to apply gradient to."
Why is logistic regression a linear model?,"
I want to know why logistic regression is called a linear model. It uses a sigmoid function, which is not linear. So why is logistic regression a linear model?
","['regression', 'logistic', 'terminology']",
How to model non-negative zero-inflated continuous data?,"
I'm currently trying to apply a linear model (family = gaussian) to an indicator of biodiversity that cannot take values lower than zero, is zero-inflated and is continuous. Values range from 0 to a little over 0.25. As a consequence, there is quite an obvious pattern in the residuals of the model that I haven't managed to get rid of:

Does anyone have any ideas on how to solve this?
","['regression', 'zero-inflation', 'tobit-regression', 'tweedie-distribution']",
Mixed Effects Model with Nesting,"
I have data collected from an experiment organized as follows:
Two sites, each with 30 trees. 15 are treated, 15 are control at each site. From each tree, we sample three pieces of the stem, and three pieces of the roots, so 6 level 1 samples per tree which is represented by one of two factor levels (root, stem). Then, from those stem / root samples, we take two samples by dissecting different tissues within the sample, which is represented by one of two factor levels for tissue type (tissue type A, tissue type B). These samples are measured as a continuous variable. Total number of observations is 720; 2 sites * 30 trees * (three stem samples + three root samples) * (one tissue A sample + one tissue B sample). Data looks like this...
        ï..Site Tree Treatment Organ Sample Tissue Total_Length
    1        L  LT1         T     R      1 Phloem           30
    2        L  LT1         T     R      1  Xylem           28
    3        L  LT1         T     R      2 Phloem           46
    4        L  LT1         T     R      2  Xylem           38
    5        L  LT1         T     R      3 Phloem          103
    6        L  LT1         T     R      3  Xylem           53
    7        L  LT1         T     S      1 Phloem           29
    8        L  LT1         T     S      1  Xylem           21
    9        L  LT1         T     S      2 Phloem           56
    10       L  LT1         T     S      2  Xylem           49
    11       L  LT1         T     S      3 Phloem           41
    12       L  LT1         T     S      3  Xylem           30

I am attempting to fit a mixed effects model using R and lme4, but am new to mixed models. I'd like to model the response as the Treatment + Level 1 Factor (stem, root) + Level 2 Factor (tissue A, tissue B), with random effects for the specific samples nested within the two levels. 
In R, I am doing this using lmer, as follows
fit <- lmer(Response ~ Treatment + Organ + Tissue + (1|Tree/Organ/Sample)) 

From my understanding (...which is not certain, and why I am posting!) the term:
(1|Tree/Organ/Sample)

Specifies that 'Sample' is nested within the organ samples, which is nested within the tree. Is this sort of nesting relevant / valid? Sorry if this question is not clear, if so, please specify where I can elaborate. 
","['r', 'mixed-model', 'model', 'nested-data', 'lme4-nlme']",I think this is correct. You may find the glmm FAQ helpful (it's focused on GLMMs but does have stuff relevant to linear mixed models as well).
Interpreting the residuals vs. fitted values plot for verifying the assumptions of a linear model,"
Consider the following figure from Faraway's Linear Models with R (2005, p. 59).

The first plot seems to indicate that the residuals and the fitted values are uncorrelated, as they should be in a homoscedastic linear model with normally distributed errors. Therefore, the second and third plots, which seem to indicate dependency between the residuals and the fitted values, suggest a different model.
But why does the second plot suggest, as Faraway notes, a heteroscedastic linear model, while the third plot suggest a non-linear model?
The second plot seems to indicate that the absolute value of the residuals is strongly positively correlated with the fitted values, whereas no such trend is evident in the third plot. So if it were the case that, theoretically speaking, in a heteroscedastic linear model with normally distributed errors
$$
\mbox{Cor}\left(\mathbf{e},\hat{\mathbf{y}}\right) = \left[\begin{array}{ccc}1 & \cdots & 1 \\ \vdots & \ddots & \vdots \\ 1 & \cdots & 1\end{array}\right]
$$
(where the expression on the left is the variance-covariance matrix between the residuals and the fitted values) this would explain why the second and third plots agree with Faraway's interpretations.
But is this the case? If not, how else can Faraway's interpretations of the second and third plots be justified? Also, why does the third plot necessarily indicate non-linearity? Isn't it possible that it is linear, but that the errors are either not normally distributed, or else that they are normally distributed, but do not center around zero?
","['regression', 'residuals', 'assumptions']","Below are those residual plots with the approximate mean and spread of points (limits that include most of the values) at each value of fitted (and hence of $x$) marked in - to a rough approximation indicating the conditional mean (red) and conditional mean $\pm$ (roughly!) twice the conditional standard deviation (purple):The second plot shows the mean residual doesn't change with the fitted values (and so is doesn't change with $x$), but the spread of the residuals (and hence of the $y$'s about the fitted line) is increasing as the fitted values (or $x$) changes. That is, the spread is not constant. Heteroskedasticity.the third plot shows that the residuals are mostly negative when the fitted value is small, positive when  the fitted value is in the middle and negative when the fitted value is large. That is, the spread is approximately constant, but the conditional mean is not - the fitted line doesn't describe how $y$ behaves as $x$ changes, since the relationship is curved.Isn't it possible that it is linear, but that the errors are either not normally distributed, or else that they are normally distributed, but do not center around zero?Not really*, in those situations the plots look different to the third plot.(i) If the errors were normal but not centered at zero, but at $\theta$, say, then the intercept would pick up the mean error, and so the estimated intercept would be an estimate of $\beta_0+\theta$ (that would be its expected value, but it is estimated with error). Consequently, your residuals would still have conditional mean zero, and so the plot would look like the first plot above.(ii) If the errors are not normally distributed the pattern of dots might be densest somewhere other than the center line (if the data were skewed), say, but the local mean residual would still be near 0.Here the purple lines still represent a (very) roughly 95% interval, but it's no longer symmetric. (I'm glossing over a couple of issues to avoid obscuring the basic point here.)* It's not necessarily impossible -- if you have an ""error"" term that doesn't really behave like the usual assumptions about errors - say where $x$ and $y$ are related to them in just the right way - you might be able to produce patterns something like these. However, we make assumptions about the error term, such as that it's not related to $x$, for example, and has zero mean; we'd have to break at least some of those sorts of assumptions to do it. (In many cases you may have reason to conclude that such effects should be absent or at least relatively small.)"
OpenBugs vs. JAGS,"
I am about to try out a BUGS style environment for estimating Bayesian models.  Are there any important advantages to consider in choosing between OpenBugs or JAGS?  Is one likely to replace the other in the foreseeable future?  
I will be using the chosen Gibbs Sampler with R.  I don't have a specific application yet, but rather I am deciding which to intall and learn.
","['r', 'software', 'bugs', 'jags', 'gibbs']","BUGS/OpenBugs has a peculiar build system which made compiling the code difficult to impossible on some systems — such as Linux (and IIRC OS X) where people had to resort to Windows emulation etc.Jags, on the other hand, is a completely new project written with standard GNU tools and hence portable to just about anywhere — and therefore usable anywhere.So in short, if your system is Windows then you do have a choice, and a potential cost of being stuck to Bugs if you ever move.  If you are not on Windows, then Jags is likely to be the better choice."
Statistics published in academic papers,"
I read a lot of evolutionary/ecological academic papers, sometimes with the specific aim of seeing how statistics are being used 'in the real world' outside of the textbook. I normally take the statistics in papers as gospel and use the papers to help in my statistical learning. After all, if a paper has taken years to write and has gone through rigorous peer review, then surely the statistics are going to be rock solid? But in the past few days, I've questioned my assumption, and wondered how often the statistical analysis published in academic papers is suspect? In particular, it might be expected that those in fields such as ecology and evolution have spent less time learning statistics and more time learning their fields. 
How often do people find suspect statistics in academic papers?
","['publication-bias', 'academia']",
What are the differences between hidden Markov models and neural networks?,"
I'm just getting my feet wet in statistics so I'm sorry if this question does not make sense. I have used Markov models to predict hidden states (unfair casinos, dice rolls, etc.) and neural networks to study users clicks on a search engine.  Both had hidden states that we were trying to figure out using observations.
To my understanding they both predict hidden states, so I'm wondering when would one use Markov models over neural networks?  Are they just different approaches to similar problems?
(I'm interested in learning but I also have another motivation, I have a problem that I'm trying to solve using hidden Markov models but its driving me bonkers so I was interested in seeing if I can switch to using something else.)
","['data-mining', 'algorithms', 'neural-networks', 'markov-process']","What is hidden and what is observedThe thing that is hidden in a hidden Markov model is the same as the thing that is hidden in a discrete mixture model, so for clarity, forget about the hidden state's dynamics and stick with a finite mixture model as an example.  The 'state' in this model is the identity of the component that caused each observation.  In this class of model such causes are never observed, so 'hidden cause' is translated statistically into the claim that the observed data have marginal dependencies which are removed when the source component is known.  And the source components are estimated to be whatever makes this statistical relationship true.The thing that is hidden in a feedforward multilayer neural network with sigmoid middle units is the states of those units, not the outputs which are the target of inference.  When the output of the network is a classification, i.e., a probability distribution over possible output categories, these hidden units values define a space within which categories are separable.  The trick in learning such a model is to make a hidden space (by adjusting the mapping out of the input units) within which the problem is linear.  Consequently, non-linear decision boundaries are possible from the system as a whole.Generative versus discriminativeThe mixture model (and HMM) is a model of the data generating process, sometimes called a likelihood or 'forward model'.  When coupled with some assumptions about the prior probabilities of each state you can infer a distribution over possible values of the hidden state using Bayes theorem (a generative approach).  Note that, while called a 'prior', both the prior and the parameters in the likelihood are usually learned from data.In contrast to the mixture model (and HMM) the neural network learns a posterior distribution over the output categories directly (a discriminative approach).  This is possible because the output values were observed during estimation.  And since they were observed, it is not necessary to construct a posterior distribution from a prior and a specific model for the likelihood such as a mixture.  The posterior is learnt directly from data, which is more efficient and less model dependent. Mix and matchTo make things more confusing, these approaches can be mixed together, e.g. when mixture model (or HMM) state is sometimes actually observed.  When that is true, and in some other circumstances not relevant here, it is possible to train discriminatively in an otherwise generative model.  Similarly it is possible to replace the mixture model mapping of an HMM with a more flexible forward model, e.g., a neural network.  The questionsSo it's not quite true that both models predict hidden state.  HMMs can be used to predict hidden state, albeit only of the kind that the forward model is expecting.  Neural networks can be used to predict a not yet observed state, e.g. future states for which predictors are available.  This sort of state is not hidden in principle, it just hasn't been observed yet.When would you use one rather than the other?  Well, neural networks make rather awkward time series models in my experience.  They also assume you have observed output.  HMMs don't but you don't really have any control of what the hidden state actually is.  Nevertheless they are proper time series models."
"Empirical relationship between mean, median and mode","
For a unimodal distribution that is moderately skewed, we have the following empirical relationship between the mean, median and mode:
$$  
\text{(Mean - Mode)}\sim  3\,\text{(Mean - Median)}
$$
How was this relationship derived? 
Did Karl Pearson plot thousands of these relationships before forming this conclusion, or is there a logical line of reasoning behind this relationship?
","['distributions', 'mathematical-statistics', 'descriptive-statistics', 'history']","Denote $\mu$ the mean ($\neq$ average), $m$ the median, $\sigma$ the standard deviation and $M$ the mode. Finally, let $X$ be the sample, a realization of a continuous unimodal distribution $F$ for which the first two moments exist.It's well known that $$|\mu-m|\leq\sigma\label{d}\tag{1}$$This is a frequent textbook exercise:  \begin{eqnarray}
|\mu-m| &=&    |E(X-m)|          \\
        &\leq& E|X-m|            \\
        &\leq& E|X-\mu|          \\
        &=&    E\sqrt{(X-\mu)^2} \\
        &\leq& \sqrt{E(X-\mu)^2} \\
        &=&    \sigma
\end{eqnarray}
The first equality derives from the definition of the mean, the third comes about because the median is the unique minimiser (among all $c$'s) of $E|X-c|$ and the fourth from Jensen's inequality (i.e. the definition of a convex function). Actually, this inequality can be made tighter. In fact, for any $F$, satisfying the conditions above, it can be shown [3] that$$|m-\mu|\leq \sqrt{0.6}\sigma\label{f}\tag{2}$$Even though it is in general not true (Abadir, 2005) that any unimodal distribution must satisfy either one of
 $$M\leq m\leq\mu\textit{ or }M\geq m\geq \mu$$ 
it can still be shown that the inequality $$|\mu-M|\leq\sqrt{3}\sigma\label{e}\tag{3}$$holds for any unimodal, square integrable distribution (regardless of skew). This is proven formally in Johnson and  Rogers (1951) though the proof depends on many auxiliary lemmas that are hard to fit here. Go see the original paper.A sufficient condition for a distribution $F$ to satisfy $\mu\leq m\leq M$ is given in [2]. If $F$:$$F(m−x)+F(m+x)\geq 1 \text{ for all }x\label{g}\tag{4}$$then $\mu\leq m\leq M$. Furthermore, if $\mu\neq m$, then the inequality is strict. The Pearson Type I to XII distributions are one example of family of distributions satisfying $(4)$ [4] (for example, the Weibull is one common distribution for which $(4)$ does not hold, see [5]). Now assuming that $(4)$ holds strictly and w.l.o.g. that $\sigma=1$, we have that 
 $$3(m-\mu)\in(0,3\sqrt{0.6}] \mbox{ and } M-\mu\in(m-\mu,\sqrt{3}]$$and since the second of these two ranges is not empty, it's certainly possible to find distributions for which the assertion is true (e.g. when $0<m-\mu<\frac{\sqrt{3}}{3}<\sigma=1$) for some range of values of the distribution's parameters but it is not true for all distributions and not even for all distributions satisfying $(4)$."
Are pooling layers added before or after dropout layers?,"
I'm creating a convolutional neural network (CNN), where I have a convolutional layer followed by a pooling layer and I want to apply dropout to reduce overfitting. I have this feeling that the dropout layer should be applied after the pooling layer, but I don't really have anything to back that up. Where is the right place to add the dropout layer? Before or after the pooling layer?
","['deep-learning', 'conv-neural-network', 'dropout']",
Using R online - without installing it [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about statistics within the scope defined in the help center.


Closed 9 years ago.







                        Improve this question
                    



Is there a possibility to use R in a webinterface without the need to install it?
I have only one small script which I like to run but I just want to give it a shot without a long installation procedure.
Thank you.
",['r'],"Yes, there are some Rweb interface, like this one (dead as of September 2020), RDDR online REPL, or Repl.it.Note: Installation of the R software is pretty straightforward and quick, on any platform."
What is the difference between conditional and unconditional quantile regression?,"
The conditional quantile regression estimator by Koenker and Basset (1978) for the $\tau^{th}$ quantile is defined as
$$
\widehat{\beta}_{QR} = \min_{b} \sum^{n}_{i=1} \rho_\tau (y_i - X'_i b_\tau)
$$
where $\rho_\tau = u_i\cdot (\tau - 1(u_i<0))$ is a re-weighting function (called ""check""-function) of the residuals $u_i$.
In a paper by Firpo et al. (2009), the authors state that conditional quantile regression does not give interesting effects. They say that conditional results cannot be generalized to the population (in OLS we can always go from conditional to unconditional via the law of iterated expectations but this is not available for quantiles). This is because the $\tau^{th}$ unconditional quantile $y_i$ might not be the same as the $\tau^{th}$ conditional quantile $y_i |X_i$.
If I understand correctly, part of the problem is that which covariates are included in $X_i$ has an effect on the ranking variable $u_i$ because inclusion of covariates splits the error into observed and unobserved components. I just cannot quite understand why this causes problems.
Here are my questions:  

What makes conditional and unconditional quantile effects different from each other?
How can I interpret the coefficients of conditional quantile regressions?
Are conditional quantile regressions biased?

References:  

Koenker, R., & Bassett, G. (1978) ""Regression Quantiles"", Econometrica, Vol. 46(1), pp. 33-50.  
Firpo, S. et al. (2009) ""Unconditional Quantile Regressions"", Econometrica, Vol. 77(3), pp. 953-973.  

",['quantile-regression'],"Set-up
Suppose you have a simple regression of the form
$$\ln y_i = \alpha + \beta S_i + \epsilon_i $$
where the outcome are the log earnings of person $i$, $S_i$ is the number of years of schooling, and $\epsilon_i$ is an error term. Instead of only looking at the average effect of education on earnings, which you would get via OLS, you also want to see the effect at different parts of the outcome distribution.1) What is the difference between the conditional and unconditional setting
First plot the log earnings and let us pick two individuals, $A$ and $B$, where $A$ is in the lower part of the unconditional earnings distribution and $B$ is in the upper part.
It doesn't look extremely normal but that's because I only used 200 observations in the simulation, so don't mind that. Now what happens if we condition our earnings on years of education? For each level of education you would get a ""conditional"" earnings distribution, i.e. you would come up with a density plot as above but for each level of education separately.The two dark blue lines are the predicted earnings from linear quantile regressions at the median (lower line) and the 90th percentile (upper line). The red densities at 5 years and 15 years of education give you an estimate of the conditional earnings distribution. As you see, individual $A$ has 5 years of education and individual $B$ has 15 years of education. Apparently, individual $A$ is doing quite well among his pears in the 5-years of education bracket, hence she is in the 90th percentile.So once you condition on another variable, it has now happened that one person is now in the top part of the conditional distribution whereas that person would be in the lower part of the unconditional distribution - this is what changes the interpretation of the quantile regression coefficients. Why?You already said that with OLS we can go from $E[y_i|S_i] = E[y_i]$ by applying the law of iterated expectations, however, this is a property of the expectations operator which is not available for quantiles (unfortunately!). Therefore in general $Q_{\tau}(y_i|S_i) \neq Q_{\tau}(y_i)$, at any quantile $\tau$. This can be solved by first performing the conditional quantile regression and then integrate out the conditioning variables in order to obtain the marginalized effect (the unconditional effect) which you can interpret as in OLS. An example of this approach is provided by Powell (2014).2) How to interpret quantile regression coefficients?
This is the tricky part and I don't claim to possess all the knowledge in the world about this, so maybe someone comes up with a better explanation for this. As you've seen, an individual's rank in the earnings distribution can be very different for whether you consider the conditional or unconditional distribution.For conditional quantile regression
Since you can't tell where an individual will be in the outcome distribution before and after a treatment you can only make statements about the distribution as a whole. For instance, in the above example a $\beta_{90} = 0.13$ would mean that an additional year of education increases the earnings in the 90th percentile of the conditional earnings distribution (but you don't know who is still in that quantile before you assigned to people an additional year of education). That's why the conditional quantile estimates or conditional quantile treatment effects are often not considered as being ""interesting"". Normally we would like to know how a treatment affects our individuals at hand, not just the distribution.For unconditional quantile regression
Those are like the OLS coefficients that you are used to interpret. The difficulty here is not the interpretation but how to get those coefficients which is not always easy (integration may not work, e.g. with very sparse data). Other ways of marginalizing quantile regression coefficients are available such as Firpo's (2009) method using the recentered influence function. The book by Angrist and Pischke (2009) mentioned in the comments states that the marginalization of quantile regression coefficients is still an active research field in econometrics - though as far as I am aware most people nowadays settle for the integration method (an example would be Melly and Santangelo (2015) who apply it to the Changes-in-Changes model).3) Are conditional quantile regression coefficients biased?
No (assuming you have a correctly specified model), they just measure something different that you may or may not be interested in. An estimated effect on a distribution rather than individuals is as I said not very interesting - most of the times. To give a counter example: consider a policy maker who introduces an additional year of compulsory schooling and they want to know whether this reduces earnings inequality in the population.The top two panels show a pure location shift where $\beta_{\tau}$ is a constant at all quantiles, i.e. a constant quantile treatment effect, meaning that if $\beta_{10} = \beta_{90} = 0.8$, an additional year of education increases earnings by 8% across the entire earnings distribution.When the quantile treatment effect is NOT constant (as in the bottom two panels), you also have a scale effect in addition to the location effect. In this example the bottom of the earnings distribution shifts up by more than the top, so the 90-10 differential (a standard measure of earnings inequality) decreases in the population.
You don't know which individuals benefit from it or in what part of the distribution people are who started out in the bottom (to answer THAT question you need the unconditional quantile regression coefficients). Maybe this policy hurts them and puts them in an even lower part relative to others but if the aim was to know whether an additional year of compulsory education reduces the earnings spread then this is informative. An example of such an approach is Brunello et al. (2009).If you are still interested in the bias of quantile regressions due to sources of endogeneity have a look at Angrist et al (2006) where they derive an omitted variable bias formula for the quantile context."
How do you use the 'test' dataset after cross-validation?,"
In some lectures and tutorials I've seen, they suggest to split your data into three parts: training, validation and test. But it is not clear how the test dataset should be used, nor how this approach is better than cross-validation over the whole data set.
Let's say we have saved 20% of our data as a test set. Then we take the rest, split it into k folds and, using cross-validation, we find the model that makes the best prediction on unknown data from this dataset. Let's say the best model we have found gives us 75% accuracy.
Various tutorials and lots of questions on various Q&A websites say that now we can verify our model on a saved (test) dataset. But I still can't get how exactly is it done, nor what is the point of it.
Let's say we've got an accuracy of 70% on the test dataset.  So what do we do next? Do we try another model, and then another, until we will get a high score on our test dataset? But in this case it really looks like we will just find the model that fits our limited (only 20%) test set. It doesn't mean that we will find the model that is best in general.
Moreover, how can we consider this score as a general evaluation of the model, if it is only calculated on a limited data set? If this score is low, maybe we were unlucky and selected ""bad"" test data.
On the other hand, if we use all the data we have and then choose the model using k-fold cross-validation, we will find the model that makes the best prediction on unknown data from the entire data set we have.  
","['machine-learning', 'cross-validation', 'validation']","This is similar to another question I answered regarding cross-validation and test sets.  The key concept to understand here is independent datasets.  Consider just two scenarios:Now, if I am a researcher who isn't so fortunate what do I do?  Well, you can try to mimic that exact scenario:To address your other concerns:Let's say we've got an accuracy of 70% on test data set, so what do we do next? Do we try an other model, and then an other, untill we will get hight score on our test data set? Sort of, the idea is that you are creating the best model you can from your data and then evaluating it on some more data it has never seen before.  You can re-evaluate your cross-validation scheme but once you have a tuned model (i.e. hyper parameters) you are moving forward with that model because it was the best you could make.  The key is to NEVER USE YOUR TEST DATA FOR TUNING.  Your result from the test data is your model's performance on 'general' data.  Replicating this process would remove the independence of the datasets (which was the entire point).  This is also address in another question on test/validation data.And also, how can we consider this score as general evaluation of the model, if it is calculated on a limited data set? If this score is low, maybe we were unlucky to select ""bad"" test data.This is unlikely if you have split your data correctly.  You should be splitting your data randomly (although potentially stratified for class balancing).  If you dataset is large enough that you are splitting your data in to three parts, your test subset should be large enough that the chance is very low that you just chose bad data.  It is more likely that your model has been overfit."
Is standardisation before Lasso really necessary?,"
I have read three main reasons for standardising variables before something such as Lasso regression:
1) Interpretability of coefficients.
2) Ability to rank the coefficient importance by the relative magnitude of post-shrinkage coefficient estimates.
3) No need for intercept.
But I am wondering about the most important point. Do we have reason to think that standardisation would improve the out of sample generalisation of the model? Also I don't care if I don't need an intercept in my model; adding one doesn't hurt me.
","['normalization', 'lasso', 'standardization', 'regularization']",
Does down-sampling change logistic regression coefficients?,"
If I have a dataset with a very rare positive class, and I down-sample the negative class, then perform a logistic regression, do I need to adjust the regression coefficients to reflect the fact that I changed the prevalence of the positive class?
For example, let's say I have a dataset with 4 variables: Y, A, B and C.  Y, A, and B are binary, C is continuous.  For 11,100 observations Y=0, and for 900 Y=1:
set.seed(42)
n <- 12000
r <- 1/12
A <- sample(0:1, n, replace=TRUE)
B <- sample(0:1, n, replace=TRUE)
C <- rnorm(n)
Y <- ifelse(10 * A + 0.5 * B + 5 * C + rnorm(n)/10 > -5, 0, 1)

I fit a logistic regression to predict Y, given A, B and C. 
dat1 <- data.frame(Y, A, B, C)
mod1 <- glm(Y~., dat1, family=binomial)

However, to save time I could remove 10,200 non-Y observations, giving 900 Y=0, and 900 Y=1:
require('caret')
dat2 <- downSample(data.frame(A, B, C), factor(Y), list=FALSE)
mod2 <- glm(Class~., dat2, family=binomial)

The regression coefficients from the 2 models look very similar:
> coef(summary(mod1))
              Estimate Std. Error   z value     Pr(>|z|)
(Intercept) -127.67782  20.619858 -6.191983 5.941186e-10
A           -257.20668  41.650386 -6.175373 6.600728e-10
B            -13.20966   2.231606 -5.919353 3.232109e-09
C           -127.73597  20.630541 -6.191596 5.955818e-10
> coef(summary(mod2))
              Estimate  Std. Error     z value    Pr(>|z|)
(Intercept) -167.90178   59.126511 -2.83970391 0.004515542
A           -246.59975 4059.733845 -0.06074284 0.951564016
B            -16.93093    5.861286 -2.88860377 0.003869563
C           -170.18735   59.516021 -2.85952165 0.004242805

Which leads me to believe that the down-sampling did not affect the coefficients.  However, this is a single, contrived example, and I'd rather know for sure.
","['logistic', 'unbalanced-classes', 'case-control-study']","Down-sampling is equivalent to case–control designs in medical statistics—you're fixing the counts of responses & observing the covariate patterns (predictors). Perhaps the key reference is Prentice & Pyke (1979), ""Logistic Disease Incidence Models and Case–Control Studies"", Biometrika, 66, 3.They used Bayes' Theorem to rewrite each term in the likelihood for the probability of a given covariate pattern conditional on being a case or control as two factors; one representing an ordinary logistic regression (probability of being a case or control conditional on a covariate pattern), & the other representing the marginal probability of the covariate pattern. They showed that maximizing the overall likelihood subject to the constraint that the marginal probabilities of being a case or control are fixed by the sampling scheme gives the same odds ratio estimates as maximizing the first factor without a constraint (i.e. carrying out an ordinary logistic regression).The intercept for the population $\beta_0^*$ can be estimated from the case–control intercept $\hat{\beta}_0$ if the population prevalence $\pi$ is known:$$ \hat{\beta}_0^* = \hat{\beta}_0 - \log\left( \frac{1-\pi}{\pi}\cdot \frac{n_1}{n_0}\right)$$where $n_0$ & $n_1$ are the number of controls & cases sampled, respectively.Of course by throwing away data you've gone to the trouble of collecting, albeit the least useful part, you're reducing the precision of your estimates. Constraints on computational resources are the only good reason I know of for doing this, but I mention it because some people seem to think that ""a balanced data-set"" is important for some other reason I've never been able to ascertain."
what does the numbers in the classification report of sklearn mean?,"
I have below an example I pulled from sklearn 's sklearn.metrics.classification_report documentation. 
What I don't understand is why there are f1-score, precision and recall values for each class where I believe class is the predictor label? I thought the f1 score tells you the overall accuracy of the model. Also, what does the support column tell us? I couldn't find any info on that.
print(classification_report(y_true, y_pred, target_names=target_names))
             precision    recall  f1-score   support

    class 0       0.50      1.00      0.67         1
    class 1       0.00      0.00      0.00         1
    class 2       1.00      0.67      0.80         3

avg / total       0.70      0.60      0.61         5

","['machine-learning', 'python', 'scikit-learn', 'precision-recall']","The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes. The support is the number of samples of the true response that lie in that class. You can find documentation on both measures in the sklearn documentation.Support - http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.htmlF1-score - http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.htmlEDITThe last line gives a weighted average of precision, recall and f1-score where the weights are the support values. so for precision the avg is (0.50*1 + 0.0*1 + 1.0*3)/5 = 0.70. The total is just for total support which is 5 here."
Can somebody explain to me NUTS in english?,"
My understanding of the algorithm is the following:
No U-Turn Sampler (NUTS) is a Hamiltonian Monte Carlo Method. This means that it is not a Markov Chain method and thus, this algorithm avoids the random walk part, which is often deemed as inefficient and slow to converge.
Instead of doing the random walk, NUTS does jumps of length x. Each jump doubles as the algorithm continues to run. This happens until the trajectory reaches a point where it wants to return to the starting point.
My questions:
What is so special about the U-turn?
How does doubling the trajectory not skip the optimized point?
Is my above description correct?  
","['bayesian', 'monte-carlo', 'markov-process']","The no U-turn bit is how proposals are generated. HMC generates a hypothetical physical system: imagine a  ball with a certain kinetic energy rolling around a landscape with valleys and hills (the analogy breaks down with more than 2 dimensions) defined by the posterior you want to sample from. Every time you want to take a new MCMC sample, you randomly pick the kinetic energy and start the ball rolling from where you are. You simulate in discrete time steps, and to make sure you explore the parameter space properly you simulate steps in one direction and the twice as many in the other direction, turn around again etc. At some point you want to stop this and a good way of doing that is when you have done a U-turn (i.e. appear to have gone all over the place).At this point the proposed next step of your Markov Chain gets picked (with certain limitations) from the points you have visited. I.e. that whole simulation of the hypothetical physical system was ""just"" to get a proposal that then gets accepted (the next MCMC sample is the proposed point) or rejected (the next MCMC sample is the starting point).The clever thing about it is that proposals are made based on the shape of the posterior and can be at the other end of the distribution. In contrast Metropolis-Hastings makes proposals within a (possibly skewed) ball, Gibbs sampling only moves along one (or at least very few) dimensions at a time. "
Intuitive explanation of convergence in distribution and convergence in probability,"
What is the intuitive difference between a random variable converging in probability versus a random variable converging in distribution?
I've read numerous definitions and mathematical equations, but that does not really help. (Please keep in mind, I am undergraduate student studying econometrics.)
How can a random variable converge to a single number, but also converge to a distribution?
","['distributions', 'random-variable', 'convergence', 'intuition']","How can a random number converge to a constant?Let's say you have $N$ balls in the box. You can pick them one by one. After you picked $k$ balls, I ask you: what's the mean weight of the balls in the box? Your best answer would be $\bar x_k=\frac{1}{k}\sum_{i=1}^kx_i$. You realize that $\bar x_k$ itself is the random value? It depends on which $k$ balls you picked first.Now, if you keep pulling the balls, at some point there'll be no balls left in the box, and you'll get $\bar x_N\equiv\mu$. So, what we've got is the random sequence $$\bar x_1,\dots,\bar x_k, \dots, \bar x_N ,\bar x_N, \bar x_N, \dots $$ which converges to the constant $\bar x_N = \mu$. So, the key to understanding your issue with convergence in probability is realizing that we're talking about a sequence of random variables, constructed in a certain way.Next, let's get uniform random numbers $e_1,e_2,\dots$, where $e_i\in [0,1]$. Let's look at the random sequence $\xi_1,\xi_2,\dots$, where $\xi_k=\frac{1}{\sqrt{\frac{k}{12}}}\sum_{i=1}^k \left(e_i- \frac{1}{2} \right)$. The $\xi_k$ is a random value, because all its terms are random values. We can't predict what is $\xi_k$ going to be. However, it turns out that we can claim that the probability distributions of $\xi_k$ will look more and more like the standard normal $\mathcal{N}(0,1)$. That's how the distributions converge."
Are there any examples of where the central limit theorem does not hold?,"
Wikipedia says - 

In probability theory, the central limit theorem (CLT) establishes that, in most situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a ""bell curve"") even if the original variables themselves are not normally distributed...

When it says ""in most situations"", in which situations does the central limit theorem not work?
","['probability', 'mathematical-statistics', 'normal-distribution', 'central-limit-theorem']","To understand this, you need to first state a version of the Central Limit Theorem.  Here's the ""typical"" statement of the central limit theorem:Lindeberg–Lévy CLT. Suppose ${X_1, X_2, \dots}$ is a sequence of i.i.d.
  random variables with $E[X_i] = \mu$ and $Var[X_i] = \sigma^2 < \infty$. 
  Let $S_{n}:={\frac {X_{1}+\cdots +X_{n}}{n}}$.  Then as
  $n$ approaches infinity, the random variables $\sqrt{n}(S_n − \mu)$ converge
  in distribution to a normal $N(0,\sigma^2)$ i.e.$${\displaystyle {\sqrt {n}}\left(\left({\frac {1}{n}}\sum
 _{i=1}^{n}X_{i}\right)-\mu \right)\ {\xrightarrow {d}}\ N\left(0,\sigma ^{2}\right).}$$So, how does this differ from the informal description, and what are the gaps?  There are several differences between your informal description and this description, some of which have been discussed in other answers, but not completely.  So, we can turn this into three specific questions:Taking these one at a time, Not identically distributed, The best general results are the Lindeberg and Lyaponov versions of the central limit theorem.  Basically, as long as the standard deviations don't grow too wildly, you can get a decent central limit theorem out of it. Lyapunov CLT.[5] Suppose ${X_1, X_2, \dots}$ is a sequence of independent
  random variables, each with finite expected value $\mu_i$ and variance $\sigma^2$
  Define: $s_{n}^{2}=\sum _{i=1}^{n}\sigma _{i}^{2}$If for some $\delta > 0$, Lyapunov’s
  condition
  ${\displaystyle \lim _{n\to \infty }{\frac {1}{s_{n}^{2+\delta }}}\sum_{i=1}^{n}\operatorname {E} \left[|X_{i}-\mu _{i}|^{2+\delta }\right]=0}$ is satisfied, then a sum
  of  $X_i − \mu_i / s_n$  converges in distribution to a standard normal
  random variable, as n goes to infinity:${{\frac {1}{s_{n}}}\sum _{i=1}^{n}\left(X_{i}-\mu_{i}\right)\ {\xrightarrow {d}}\ N(0,1).}$Infinite Variance  Theorems similar to the central limit theorem exist for variables with infinite variance, but the conditions are significantly more narrow than for the usual central limit theorem.  Essentially the tail of the probability distribution must be asymptotic to $|x|^{-\alpha-1}$ for $0 < \alpha < 2$.  In this case, appropriate scaled summands converge to a Levy-Alpha stable distribution.Importance of Independence There are many different central limit theorems for non-independent sequences of $X_i$.  They are all highly contextual.  As Batman points out, there's one for Martingales.  This question is an ongoing area of research, with many, many different variations depending upon the specific context of interest.  This Question on Math Exchange is another post related to this question."
Is it OK to remove outliers from data?,"
I looked for a way to remove outliers from a dataset and I found this question.
In some of the comments and answers to this question, however, people mentioned that it is bad practice to remove outliers from the data. 
In my dataset I have several outliers that very likely are just due to measurement errors. Even if some of them are not, I have no way of checking it case by case, because there are too many data points. Is it statistically valid than just to remove the outliers? Or, if not, what could be another solution? 
If I just leave those points there, they influence e.g. the mean in a way that does not reflect reality (because most of them are errors anyway). 
EDIT: I am working with skin conductance data. Most of the extreme values are due to artifacts like somebody pulling on the wires. 
EDIT2: My main interest in analyzing the data is to determine if there is a difference between two groups
","['outliers', 'faq']",
What exactly is Big Data?,"
I have been asked on several occasions the question:

What is Big-Data?

Both by students and my relatives that are picking up the buzz around statistics and ML. 
I found this CV-post. And I feel that I agree with the only answer there. 
The Wikipedia page also has some comments on it, but I am not sure if I really agree with everything there.
EDIT: (I feel that the Wikipedia page lacks in explaining the methods to tackle this and the paradigm I mention below).
I recently attended a lecture by Emmanuel Candès, where he introduced the Big-Data paradigm as 

Collect data first $\Rightarrow$ Ask questions later

This is the main difference from hypothesis-driven research, where you first formulate a hypothesis and then collect data to say something about it.
He went a lot into the issues of quantifying reliability of hypotheses generated by data snooping. The main thing I took out of his lecture was that we really need to start to control the FDR and he presented the knockoff method to do so.
I think that CV should have a question on what is Big-Data and what is your definition on it. I feel that there are so many different ""definitions"", that it is hard to really grasp what it is, or explain it to others, if there is not a general consensus on what it consists of.
I feel that the ""definition/paradigm/description"" provided by Candès is the closest thing I agree on, what are your thoughts?
EDIT2: I feel that the answer should provide something more than just an explanation of the data itself. It should be a combination of data/methods/paradigm.
EDIT3: I feel that this interview with Michael Jordan could add something to the table as well.
EDIT4: I decided to choose the highest voted answer as the correct one. Although I think that all the answers add something to the discussion and I personally feel that this is more a question of a paradigm of how we generate hypotheses and work with data. I hope this question will serve as a pool of references for those that go looking for what Big-Data is. I hope that the Wikipedia page will be changed to further emphasize the multiple comparison problem and control of FDR.
",['large-data'],"I had the pleasure of attending a lecture given by Dr. Hadley Wickham, of RStudio fame. He defined it such thatHadley also believes that most data can at least be reduced to managable problems, and that a very small amount is actually true big data. He denotes this as the ""Big Data Mirage"". Slides can be found here. "
What best practices should I follow when preparing plots?,"
I usually make my own idiosyncratic choices when preparing plots. However, I wonder if there are any best practices for generating plots. 
Note: Rob's comment to an answer to this question is very relevant here. 
","['data-visualization', 'references']",The Tufte principles are very good practices when preparing plots. See also his book Beautiful EvidenceThe principles include:The term to search for is Information Visualization
How does cross-validation overcome the overfitting problem?,"
Why does a cross-validation procedure overcome the problem of overfitting a model?
","['regression', 'cross-validation', 'model-selection', 'overfitting']","I can't think of a sufficiently clear explanation just at the moment, so I'll leave that to someone else; however cross-validation does not completely overcome the over-fitting problem in model selection, it just reduces it.  The cross-validation error does not have a negligible variance, especially if the size of the dataset is small; in other words you get a slightly different value depending on the particular sample of data you use.  This means that if you have many degrees of freedom in model selection (e.g. lots of features from which to select a small subset, many hyper-parameters to tune, many models from which to choose) you can over-fit the cross-validation criterion as the model is tuned in ways that exploit this random variation rather than in ways that really do improve performance, and you can end up with a model that performs poorly.  For a discussion of this, see Cawley and Talbot ""On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"", JMLR, vol. 11, pp. 2079−2107, 2010Sadly cross-validation is most likely to let you down when you have a small dataset, which is exactly when you need cross-validation the most.  Note that k-fold cross-validation is generally more reliable than leave-one-out cross-validation as it has a lower variance, but may be more expensive to compute for some models (which is why LOOCV is sometimes used for model selection, even though it has a high variance)."
"Ridge, lasso and elastic net","
How do ridge, LASSO and elasticnet regularization methods compare? What are their respective advantages and disadvantages? Any good technical paper, or lecture notes would be appreciated as well.  
","['references', 'lasso', 'regularization', 'ridge-regression', 'elastic-net']","In The Elements of Statistical Learning book, Hastie et al. provide a very insightful and thorough comparison of these shrinkage techniques. The book is available online (pdf). The comparison is done in section 3.4.3, page 69.The main difference between Lasso and Ridge is the penalty term they use. Ridge uses $L_2$ penalty term which limits the size of the coefficient vector. Lasso uses $L_1$ penalty which imposes sparsity among the coefficients and thus, makes the fitted model more interpretable. Elasticnet is introduced as a compromise between these two techniques, and has a penalty which is a mix of $L_1$ and $L_2$ norms. "
Mean absolute percentage error (MAPE) in Scikit-learn [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question appears to be off-topic because EITHER it is not about statistics, machine learning, data analysis, data mining, or data visualization, OR it focuses on programming, debugging, or performing routine operations within a statistical computing platform. If the latter, you could try the support links we maintain.


Closed 6 years ago.







                        Improve this question
                    



How can we calculate the Mean absolute percentage error (MAPE) of our predictions using Python and scikit-learn?
From the docs, we have only these 4 metric functions for Regressions:

metrics.explained_variance_score(y_true, y_pred)
metrics.mean_absolute_error(y_true, y_pred)  
metrics.mean_squared_error(y_true, y_pred)
metrics.r2_score(y_true, y_pred)

","['predictive-models', 'python', 'scikit-learn', 'mape']","As noted (for example, in Wikipedia), MAPE can be problematic. Most pointedly, it can cause division-by-zero errors. My guess is that this is why it is not included in the sklearn metrics. However, it is simple to implement. Use like any other metric...: (Note that I'm multiplying by 100 and returning a percentage.) ... but with caution: "
Should covariates that are not statistically significant be 'kept in' when creating a model?,"
I have several covariates in my calculation for a model, and not all of them are statistically significant.  Should I remove those that are not?
This question discusses the phenomenon, but does not answer my question:
How to interpret non-significant effect of a covariate in ANCOVA?
There is nothing in the answer to that question that suggests that non-significant covariates be taken out, though, so right now I am inclined to believe that they should stay in.  Before even reading that answer, I was thinking the same since a covariate can still explain some of the variance (and thus help the model) without necessarily explaining an amount beyond some threshold (the significance threshold, which I see as not applicable to covariates).
There is another question somewhere on CV for which the answer seems to imply that covariates should be kept in regardless of significance, but it is not clear on that.  (I want to link to that question, but I was not able to track it down again just now.)
So...   Should covariates that do not show as statistically significant be kept in the calculation for the model?  (I have edited this question to clarify that covariates are never in the model output by the calculation anyway.)
To add complication, what if the covariates are statistically significant for some subsets of the data (subsets which have to be processed separately).  I would default to keeping such a covariate, otherwise either different models would have to be used or you would have a statistically significant covariate missing in one of the cases.  If you also have an answer for this split case, though, please mention it.
","['regression', 'statistical-significance', 'model', 'ancova', 'faq']","You have gotten several good answers already. There are reasons to keep covariates and reasons to drop covariates. Statistical significance should not be a key factor, in the vast majority of cases.If you are in a very exploratory mode and the covariate is not important in the literature and the effect size is small and the covariate has little effect on your model and the covariate was not in your hypothesis, then you could probably delete it just for simplicity."
Do Bayesian priors become irrelevant with large sample size?,"
When performing Bayesian inference, we operate by maximizing our likelihood function in combination with the priors we have about the parameters. Because the log-likelihood is more convenient, we effectively maximize $\sum \ln (\text{prior}) + \sum \ln (\text{likelihood})$ using an MCMC or otherwise which generates the posterior distributions (using a pdf for each parameter's prior and each data point's likelihood).
If we have a lot of data, the likelihood from that is going to overwhelm any information that the prior provides, by simple mathematics. Ultimately, this is good and by design; we know that the posterior will converge to just the likelihood with more data because it is supposed to.
For problems defined by conjugate priors, this is even provable exactly.
Is there a way to decide when priors don't matter for a given likelihood function and some sample size?
","['bayesian', 'prior']","It is not that easy. Information in your data overwhelms prior information not only when your sample size is large, but also when your data provides enough information to overwhelm the prior information. Uninformative priors get easily persuaded by data, while strongly informative ones may be more resistant. In extreme case, with ill-defined priors, your data may not be able at all to overcome it (e.g. zero density over some region).Recall that by Bayes theorem we use two sources of information in our statistical model, out-of-data, prior information, and information conveyed by data in likelihood function:$$ \color{violet}{\text{posterior}} \propto \color{red}{\text{prior}} \times \color{lightblue}{\text{likelihood}} $$When using uninformative prior (or maximum likelihood), we try to bring minimal possible prior information into our model. With informative priors we bring substantial amount of information into the model. So both, the data and prior, inform us what values of estimated parameters are more plausible, or believable. They can bring different information and each of them can overpower the other one in some cases.Let me illustrate this with very basic beta-binomial model (see here for detailed example). With ""uninformative"" prior, pretty small sample may be enough to overpower it. On the plots below you can see priors (red curve), likelihood (blue curve), and posteriors (violet curve) of the same model with different sample sizes.On another hand, you can have informative prior that is close to the true value, that would also be easily, but not that easily as with weekly informative one, persuaded by data.The case is very different with informative prior, when it is far from what the data says (using the same data as in first example). In such case you need larger sample to overcome the prior.So it is not only about sample size, but also about what is your data and what is your prior. Notice that this is a desired behavior, because when using informative priors we want to potentially include out-of-data information in our model and this would be impossible if large samples would always discard the priors.Because of complicated posterior-likelihood-prior relations, it is always good to look at the posterior distribution and do some posterior predictive checks (Gelman, Meng and Stern, 1996; Gelman and Hill, 2006; Gelman et al, 2004). Moreover, as described by Spiegelhalter (2004), you can use different priors, for example ""pessimistic"" that express doubts about large effects, or ""enthusiastic"" that are optimistic about estimated effects. Comparing how different priors behave with your data may help to informally assess the extent how posterior was influenced by prior.Spiegelhalter, D. J. (2004). Incorporating Bayesian ideas into health-care evaluation. Statistical Science, 156-174.Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (2004). Bayesian data analysis. Chapman & Hall/CRC.Gelman, A. and Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.Gelman, A., Meng, X. L., and Stern, H. (1996). Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, 733-760."
Quantile regression: Loss function,"
I am trying to understand the quantile regression, but one thing that makes me suffer is the choice of the loss function.  
$\rho_\tau(u) = u(\tau-1_{\{u<0\}})$
I know that the minimum of the expectation of $\rho_\tau(y-u)$ is equal to the $\tau\%$-quantile, but what is the intuitive reason to start off with this function? I don't see the relation between minimizing this function and the quantile.
Can somebody explain it to me?
","['quantiles', 'loss-functions', 'quantile-regression']","I understand this question as asking for insight into how one could come up with any loss function that produces a given quantile as a loss minimizer no matter what the underlying distribution might be.  It would be unsatisfactory, then, just to repeat the analysis in Wikipedia or elsewhere that shows this particular loss function works. Let's begin with something familiar and simple.What you're talking about is finding a ""location"" $x^{*}$ relative to a distribution or set of data $F$.  It is well known, for instance, that the mean $\bar x$ minimizes the expected squared residual; that is, it is a value for which$$\mathcal{L}_F(\bar x)=\int_{\mathbb{R}} (x - \bar x)^2 dF(x)$$is as small as possible.  I have used this notation to remind us that $\mathcal{L}$ is derived from a loss, that it is determined by $F$, but most importantly it depends on the number $\bar x$.The standard way to show that $x^{*}$ minimizes any function begins by demonstrating the function's value does not decrease when $x^{*}$ is changed by a little bit.  Such a value is called a critical point of the function.What kind of loss function $\Lambda$ would result in a percentile $F^{-1}(\alpha)$ being a critical point?  The loss for that value would be$$\mathcal{L}_F(F^{-1}(\alpha)) = \int_{\mathbb{R}} \Lambda(x-F^{-1}(\alpha))dF(x)=\int_0^1\Lambda\left(F^{-1}(u)-F^{-1}(\alpha)\right)du.$$For this to be a critical point, its derivative must be zero.  Since we're just trying to find some solution, we won't pause to see whether the manipulations are legitimate: we'll plan to check technical details (such as whether we really can differentiate $\Lambda$, etc.) at the end.  Thus$$\eqalign{0 &=\mathcal{L}_F^\prime(x^{*})= \mathcal{L}_F^\prime(F^{-1}(\alpha))= -\int_0^1 \Lambda^\prime\left(F^{-1}(u)-F^{-1}(\alpha)\right)du \\
&= -\int_0^{\alpha} \Lambda^\prime\left(F^{-1}(u)-F^{-1}(\alpha)\right)du  -\int_{\alpha}^1 \Lambda^\prime\left(F^{-1}(u)-F^{-1}(\alpha)\right)du.\tag{1}
}$$On the left hand side, the argument of $\Lambda$ is negative, whereas on the right hand side it is positive.  Other than that, we have little control over the values of these integrals because $F$ could be any distribution function.  Consequently our only hope is to make $\Lambda^\prime$ depend only on the sign of its argument, and otherwise it must be constant. This implies $\Lambda$ will be piecewise linear, potentially with different slopes to the left and right of zero.  Clearly it should be decreasing as zero is approached--it is, after all, a loss and not a gain.  Moreover, rescaling $\Lambda$ by a constant will not change its properties, so we may feel free to set the left hand slope to $-1$.  Let $\tau \gt 0$ be the right hand slope.  Then $(1)$ simplifies to $$0 = \alpha - \tau (1 - \alpha),$$whence the unique solution is, up to a positive multiple,$$\Lambda(x) = \cases{-x, \ x \le 0 \\ \frac{\alpha}{1-\alpha}x, \ x \ge 0.}$$Multiplying this (natural) solution by $1-\alpha$, to clear the denominator, produces the loss function presented in the question.Clearly all our manipulations are mathematically legitimate when $\Lambda$ has this form.  "
Assumptions of linear models and what to do if the residuals are not normally distributed,"
I am a little bit confused on what the assumptions of linear regression are.
So far I checked whether:

all of the explanatory variables correlated linearly with the response variable. (This was the case)
there was any collinearity among the explanatory variables. (there was little collinearity).
the Cook's distances of the datapoints of my model are below 1 (this is the case, all distances are below 0.4, so no influence points).
the residuals are normally distributed. (this may not be the case)

But I then read the following:

violations of normality often arise either because (a) the
distributions of the dependent and/or independent variables are
themselves significantly non-normal, and/or (b) the linearity
assumption is violated.

Question 1
This makes it sound as if the independent and depend variables need to be normally distributed, but as far as I know this is not the case. My dependent variable as well as one of my independent variables are not normally distributed. Should they be?
Question 2
My QQnormal plot of the residuals look like this:

That slightly differs from a normal distribution and the shapiro.test also rejects the null hypothesis that the residuals are from a normal distribution:
> shapiro.test(residuals(lmresult))
W = 0.9171, p-value = 3.618e-06

The residuals vs fitted values look like:

What can I do if my residuals are not normally distributed? Does it mean the linear model is entirely useless?
","['linear-model', 'residuals', 'assumptions', 'normality-assumption']","First off, I would get yourself a copy of this classic and approachable article and read it: Anscombe FJ. (1973) Graphs in statistical analysis. The American Statistician. 27:17–21.On to your questions:Answer 1: Neither the dependent nor independent variable needs to be normally distributed. In fact they can have all kinds of loopy distributions. The normality assumption applies to the distribution of the errors ($Y_{i} - \widehat{Y}_{i}$).Answer 2: You are actually asking about two separate assumptions of ordinary least squares (OLS) regression:One is the assumption of linearity. This means that the trend in $\overline{Y}$ across $X$ is expressed by a straight line (Right? Straight back to algebra: $y = a +bx$, where $a$ is the $y$-intercept, and $b$ is the slope of the line.) A violation of this assumption simply means that the relationship is not well described by a straight line (e.g., $\overline{Y}$ is a sinusoidal function of $X$, or a quadratic function, or even a straight line that changes slope at some point). My own preferred two-step approach to address non-linearity is to (1) perform some kind of non-parametric smoothing regression to suggest specific nonlinear functional relationships between $Y$ and $X$ (e.g., using LOWESS, or GAMs, etc.), and (2) to specify a functional relationship using either a multiple regression that includes nonlinearities in $X$, (e.g., $Y \sim X + X^{2}$), or a nonlinear least squares regression model that includes nonlinearities in parameters of $X$ (e.g., $Y \sim X + \max{(X-\theta,0)}$, where $\theta$ represents the point where the regression line of $\overline{Y}$ on $X$ changes slope).Another is the assumption of normally distributed residuals. Sometimes one can validly get away with non-normal residuals in an OLS context; see for example, Lumley T, Emerson S. (2002) The Importance of the Normality Assumption in Large Public Health Data Sets. Annual Review of Public Health. 23:151–69. Sometimes, one cannot (again, see the Anscombe article).However, I would recommend thinking about the assumptions in OLS not so much as desired properties of your data, but rather as interesting points of departure for describing nature. After all, most of what we care about in the world is more interesting than $y$-intercept and slope. Creatively violating OLS assumptions (with the appropriate methods) allows us to ask and answer more interesting questions."
Why is RSS distributed chi square times n-p?,"
I would like to understand why, under the OLS model, the RSS (residual sum of squares) is distributed $$\chi^2\cdot (n-p)$$ ($p$ being the number of parameters in the model, $n$ the number of observations).
I apologize for asking such a basic question, but I seem to not be able to find the answer online (or in my, more application oriented, textbooks).
","['regression', 'distributions', 'least-squares']","I consider the following linear model: ${y} = X \beta + \epsilon$.The vector of residuals is estimated by$$\hat{\epsilon} = y - X \hat{\beta} 
                     = (I - X (X'X)^{-1} X') y
                     = Q y
                     = Q (X \beta + \epsilon) = Q \epsilon$$where $Q = I - X (X'X)^{-1} X'$.Observe that $\textrm{tr}(Q) = n - p$ (the trace is invariant under cyclic permutation) and that $Q'=Q=Q^2$. The eigenvalues of $Q$ are therefore $0$ and $1$ (some details below). Hence, there exists a unitary matrix $V$ such that (matrices are diagonalizable by unitary matrices if and only if they are normal.)$$V'QV = \Delta = \textrm{diag}(\underbrace{1, \ldots, 1}_{n-p \textrm{ times}}, \underbrace{0, \ldots, 0}_{p \textrm{ times}})$$Now, let $K = V' \hat{\epsilon}$.Since $\hat{\epsilon} \sim N(0, \sigma^2 Q)$, we have $K \sim N(0, \sigma^2 \Delta)$ and therefore $K_{n-p+1}=\ldots=K_n=0$. Thus$$\frac{\|K\|^2}{\sigma^2} = \frac{\|K^{\star}\|^2}{\sigma^2} \sim \chi^2_{n-p}$$with $K^{\star} = (K_1, \ldots, K_{n-p})'$.Further, as $V$ is a unitary matrix, we also have$$\|\hat{\epsilon}\|^2 = \|K\|^2=\|K^{\star}\|^2$$Thus $$\frac{\textrm{RSS}}{\sigma^2} \sim \chi^2_{n-p}$$Finally, observe that this result implies that $$E\left(\frac{\textrm{RSS}}{n-p}\right) = \sigma^2$$Since $Q^2 - Q =0$, the minimal polynomial of $Q$ divides the polynomial $z^2 - z$. So, the eigenvalues of $Q$ are among $0$ and $1$. Since $\textrm{tr}(Q) = n-p$ is also the sum of the eigenvalues multiplied by their multiplicity, we necessarily have that $1$ is an eigenvalue with multiplicity $n-p$ and zero is an eigenvalue with multiplicity $p$."
Difference between naive Bayes & multinomial naive Bayes,"
I've dealt with Naive Bayes classifier before. I've been reading about Multinomial Naive Bayes lately.
Also Posterior Probability = (Prior * Likelihood)/(Evidence).
The only prime difference (while programming these classifiers) I found between Naive Bayes & Multinomial Naive Bayes is that
Multinomial Naive Bayes calculates likelihood to be count of an word/token (random variable) and Naive Bayes calculates likelihood to be following:

Correct me if I'm wrong!
","['bayesian', 'classification', 'text-mining', 'naive-bayes']","The general term Naive Bayes refers the the strong independence assumptions in the model, rather than the particular distribution of each feature. A Naive Bayes model assumes that each of the features it uses are conditionally independent of one another given some class. More formally, if I want to calculate the probability of observing features $f_1$ through $f_n$, given some class c, under the Naive Bayes assumption the following holds:$$ p(f_1,..., f_n|c) = \prod_{i=1}^n p(f_i|c)$$This means that when I want to use a Naive Bayes model to classify a new example, the posterior probability is much simpler to work with:$$ p(c|f_1,...,f_n) \propto p(c)p(f_1|c)...p(f_n|c) $$Of course these assumptions of independence are rarely true, which may explain why some have referred to the model as the ""Idiot Bayes"" model, but in practice Naive Bayes models have performed surprisingly well, even on complex tasks where it is clear that the strong independence assumptions are false.Up to this point we have said nothing about the distribution of each feature. In other words, we have left $p(f_i|c)$ undefined. The term Multinomial Naive Bayes simply lets us know that each $p(f_i|c)$ is a multinomial distribution, rather than some other distribution. This works well for data which can easily be turned into counts, such as word counts in text.The distribution you had been using with your Naive Bayes classifier is a Guassian p.d.f., so I guess you could call it a Guassian Naive Bayes classifier.In summary, Naive Bayes classifier is a general term which refers to conditional independence of each of the features in the model, while Multinomial Naive Bayes classifier is a specific instance of a Naive Bayes classifier which uses a multinomial distribution for each of the features.References:Stuart J. Russell and Peter Norvig. 2003. Artificial Intelligence: A Modern Approach (2 ed.). Pearson Education. See p. 499 for reference to ""idiot Bayes"" as well as the general definition of the Naive Bayes model and its independence assumptions"
"Is machine learning less useful for understanding causality, thus less interesting for social science?","
My understanding of the difference between machine learning/other statistical predictive techniques vs. the kind of statistics that social scientists (e.g., economists) use is that economists seem very interested in understanding the effect of a single or several variables -- both in terms of magnitude and detecting whether the relationship is causal. For this, you end up concerning yourself with experimental and quasi-experimental methods, etc.
Machine learning or statistical modeling that is predictive often entirely neglects this aspect and in many cases doesn't give you a specific degree to which one variable affects the outcome (logit and probit do seem to do both).
A related question is to what extent do theoretically inspired economic or behavioral models have an advantage over atheoretical models when predicting to new domains? What would a machine learning or prediction-oriented statistician say to the criticism that without an economic model, you wouldn't be able to correctly predict new samples where covariates were very different.
I'd be really happy to hear people's take on this from all perspectives.
","['machine-learning', 'econometrics']",
Softmax layer in a neural network,"
I'm trying to add a softmax layer to a neural network trained with backpropagation, so I'm trying to compute its gradient.
The softmax output is $h_j = \frac{e^{z_j}}{\sum{e^{z_i}}}$ where $j$ is the output neuron number.
If I derive it then I get
$\frac{\partial{h_j}}{\partial{z_j}}=h_j(1-h_j)$
Similar to logistic regression.
However this is wrong since my numerical gradient check fails.
What am I doing wrong? I had a thought that I need to compute the cross derivatives as well (i.e. $\frac{\partial{h_j}}{\partial{z_k}}$) but I'm not sure how to do this and keep the dimension of the gradient the same so it will fit for the back propagation process.
",['neural-networks'],"I feel a little bit bad about providing my own answer for this because it is pretty well captured by amoeba and juampa, except for maybe the final intuition about how the Jacobian  can be reduced back to a vector.You correctly derived the gradient of the diagonal of the Jacobian matrix, which is to say that $ {\partial h_i \over \partial z_j}= h_i(1-h_j)\;\;\;\;\;\;: i = j $and as amoeba stated it, you also have to derive the off diagonal entries of the Jacobian, which yield$ {\partial h_i \over \partial z_j}= -h_ih_j\;\;\;\;\;\;: i \ne j $These two concepts definitions can be conveniently combined using a construct called the Kronecker Delta, so the definition of the gradient becomes$ {\partial h_i \over \partial z_j}= h_i(\delta_{ij}-h_j) $So the Jacobian is a square matrix $ \left[J \right]_{ij}=h_i(\delta_{ij}-h_j) $All of the information up to this point is already covered by amoeba and juampa. The problem is of course, that we need to get the input errors from the output errors that are already computed. Since the gradient of the output error $\nabla h_i$ depends on all of the inputs, then the gradient of the input $x_i$ is$[\nabla x]_k = \sum\limits_{i=1} \nabla h_{i,k}  $Given the Jacobian matrix defined above, this is implemented trivially as the product of the matrix and the output error vector:$ \vec{\sigma_l} = J\vec{\sigma_{l+1}} $If the softmax layer is your output layer, then combining it with the cross-entropy cost model simplifies the computation to simply$ \vec{\sigma_l} = \vec{h}-\vec{t} $where $\vec{t}$ is the vector of labels, and $\vec{h}$ is the output from the softmax function. Not only is the simplified form convenient, it is also extremely useful from a numerical stability standpoint."
How to interpret F- and p-value in ANOVA?,"
I am new to statistics and I currently deal with ANOVA. I carry out an ANOVA test in R using
aov(dependendVar ~ IndependendVar)

I get – among others – an F-value and a p-value. 
My null hypothesis ($H_0$) is that all group means are equal. 
There is a lot of information available on how F is calculated, but I don't know how to read an F-statistic and how F and p are connected. 
So, my questions are:

How do I determine the critical F-value for rejecting $H_0$?
Does each F have a corresponding p-value, so they both mean basically the same? (e.g., if $p<0.05$, then $H_0$ is rejected) 

","['r', 'anova', 'interpretation']","To answer your questions:You find the critical F value from an F distribution (here's a table).  See an example.  You have to be careful about one-way versus two-way, degrees of freedom of numerator and denominator.Yes."
Relationship between Binomial and Beta distributions,"
I'm more of a programmer than a statistician, so I hope this question isn't too naive.
It happens in sampling program executions at random times. If I take N=10 random-time samples of the program's state, I could see function Foo being executed on, for example, I=3 of those samples. I'm interested in what that tells me about the actual fraction of time F that Foo is in execution.
I understand that I is binomially distributed with mean F*N. I also know that, given I and N, F follows a beta distribution. In fact I've verified by program the relationship between those two distributions, which is
cdfBeta(I, N-I+1, F) + cdfBinomial(N, F, I-1) = 1

The problem is I don't have an intuitive feel for the relationship. I can't ""picture"" why it works.
EDIT: All the answers were challenging, especially @whuber's, which I still need to grok, but bringing in order statistics was very helpful. Nevertheless I've realized I should have asked a more basic question: Given I and N, what is the distribution for F? Everyone has pointed out that it's Beta, which I knew. I finally figured out from Wikipedia (Conjugate prior) that it appears to be Beta(I+1, N-I+1). After exploring it with a program, it appears to be the right answer. So, I would like to know if I'm wrong. And, I'm still confused about the relationship between the two cdfs shown above, why they sum to 1, and if they even have anything to do with what I really wanted to know.
","['binomial-distribution', 'beta-binomial-distribution', 'beta-distribution']","Consider the order statistics $x_{[0]} \le x_{[1]} \le \cdots \le x_{[n]}$ of $n+1$ independent draws from a uniform distribution.  Because order statistics have Beta distributions, the chance that $x_{[k]}$ does not exceed $p$ is given by the Beta integral$$\Pr[x_{[k]} \le p] = \frac{1}{B(k+1, n-k+1)} \int_0^p{x^k(1-x)^{n-k}dx}.$$(Why is this?  Here is a non-rigorous but memorable demonstration.  The chance that $x_{[k]}$ lies between $p$ and $p + dp$ is the chance that out of $n+1$ uniform values, $k$ of them lie between $0$ and $p$, at least one of them lies between $p$ and $p + dp$, and the remainder lie between $p + dp$ and $1$.  To first order in the infinitesimal $dp$ we only need to consider the case where exactly one value (namely, $x_{[k]}$ itself) lies between $p$ and $p + dp$ and therefore $n - k$ values exceed $p + dp$.  Because all values are independent and uniform, this probability is proportional to $p^k (dp) (1 - p - dp)^{n-k}$.  To first order in $dp$ this equals $p^k(1-p)^{n-k}dp$, precisely the integrand of the Beta distribution.  The term $\frac{1}{B(k+1, n-k+1)}$ can be computed directly from this argument as the multinomial coefficient ${n+1}\choose{k,1, n-k}$ or derived indirectly as the normalizing constant of the integral.)By definition, the event $x_{[k]} \le p$ is that the $k+1^\text{st}$ value does not exceed $p$.  Equivalently, at least $k+1$ of the values do not exceed $p$: this simple (and I hope obvious) assertion provides the intuition you seek. The probability of the equivalent statement is given by the Binomial distribution,$$\Pr[\text{at least }k+1\text{ of the }x_i \le p] = \sum_{j=k+1}^{n+1}{{n+1}\choose{j}} p^j (1-p)^{n+1-j}.$$In summary, the Beta integral breaks the calculation of an event into a series of calculations: finding at least $k+1$ values in the range $[0, p]$, whose probability we normally would compute with a Binomial cdf, is broken down into mutually exclusive cases where exactly $k$ values are in the range $[0, x]$ and 1 value is in the range $[x, x+dx]$ for all possible $x$, $0 \le x \lt p$, and $dx$ is an infinitesimal length.  Summing over all such ""windows"" $[x, x+dx]$--that is, integrating--must give the same probability as the Binomial cdf."
When to use a GAM vs GLM,"
I realize this may be a potentially broad question, but I was wondering whether there are assumptions that indicate the use of a GAM (Generalized additive model) over a GLM (Generalized linear model)?
Someone recently told me that GAMs should only be used when I assume the data structure to be ""additive"", i.e. I expect additions of x to predict y.
Another person pointed out that a GAM does a different type of regression analysis than a GLM, and that a GLM is preferred when linearity can be assumed.
In the past I have been using a GAM for ecological data, e.g.:

continuous time-series
when the data did not have a linear shape
I had multiple x to predict my y that I thought to have some nonlinear interaction that I could visualize using ""surface plots"" together with a statistical test

I obviously don't have a great understanding of what a GAM does different than a GLM. I believe it's a valid statistical test, (and I see an increase in the use GAMs, at least in ecological journals), but I need to know better when its use is indicated over other regression analyses.
","['regression', 'generalized-linear-model', 'generalized-additive-model']","The main difference imho is that while ""classical"" forms of linear, or generalized linear, models assume a fixed linear or some other parametric form of the relationship between the dependent variable and the covariates, GAM do not assume a priori any specific form of this relationship, and can be used to reveal and estimate non-linear effects of the covariate on the dependent variable. 
More in detail, while in (generalized) linear models the linear predictor is a weighted sum of the $n$ covariates, $\sum_{i=1}^n \beta_i x_i$, in GAMs this term is replaced by a sum of smooth function, e.g. $\sum_{i=1}^n \sum_{j=1}^q \beta_i \, s_j \left( x_i \right)$, where the $s_1(\cdot),\dots,s_q(\cdot)$ are smooth basis functions (e.g. cubic splines) and $q$ is the basis dimension. By combining the basis functions GAMs can represent a large number of functional relationship (to do so they rely on the assumption that the true relationship is likely to be smooth, rather than wiggly). They are essentially an extension of GLMs, however they are designed in a way that makes them particularly useful for uncovering nonlinear effects of numerical covariates, and for doing so in an ""automatic"" fashion (from Hastie and Tibshirani original article, they have 'the advantage of being completely automatic, i.e. no ""detective"" work is needed on the part of the statistician')."
"Analysis with complex data, anything different?","
Say for example you are doing a linear model, but the data $y$ is complex.
$ y = x \beta + \epsilon $
My data set is complex, as in all the numbers in $y$ are of the form $(a + bi)$.  Is there anything procedurally different when working with such data?
I ask because, you will end up getting complex covariance matrices, and test statistics which are complex valued..
Do you need to use a conjugate transposes instead of transposes when doing least squares?  is a complex valued covariance meaningful?  
","['regression', 'anova', 'data-transformation', 'complex-numbers']","The generalization of least-squares regression to complex-valued variables is straightforward, consisting primarily of replacing matrix transposes by conjugate transposes in the usual matrix formulas.  A complex-valued regression, though, corresponds to a complicated multivariate multiple regression whose solution would be much more difficult to obtain using standard (real variable) methods.  Thus, when the complex-valued model is meaningful, using complex arithmetic to obtain a solution is strongly recommended.  This answer also includes some suggested ways to display the data and present diagnostic plots of the fit.For simplicity, let's discuss the case of ordinary (univariate) regression, which can be written$$z_j = \beta_0 + \beta_1 w_j + \varepsilon_j.$$I have taken the liberty of naming the independent variable $W$ and the dependent variable $Z$, which is conventional (see, for instance, Lars Ahlfors, Complex Analysis).  All that follows is straightforward to extend to the multiple regression setting.This model has an easily visualized geometric interpretation: multiplication by $\beta_1$ will rescale $w_j$ by the modulus of $\beta_1$ and rotate it around the origin by the argument of $\beta_1$.  Subsequently, adding $\beta_0$ translates the result by this amount.  The effect of $\varepsilon_j$ is to ""jitter"" that translation a little bit.  Thus, regressing the $z_j$ on the $w_j$ in this manner is an effort to understand the collection of 2D points $(z_j)$ as arising from a constellation of 2D points $(w_j)$ via such a transformation, allowing for some error in the process.  This is illustrated below with the figure titled ""Fit as a Transformation.""Note that the rescaling and rotation are not just any linear transformation of the plane: they rule out skew transformations, for instance.  Thus this model is not the same as a bivariate multiple regression with four parameters.To connect the complex case with the real case, let's write $z_j = x_j + i y_j$ for the values of the dependent variable and$w_j = u_j + i v_j$ for the values of the independent variable.Furthermore, for the parameters write $\beta_0 = \gamma_0 + i \delta_0$ and $\beta_1 = \gamma_1 +i \delta_1$.  Every one of the new terms introduced is, of course, real, and $i^2 = -1$ is imaginary while $j=1, 2, \ldots, n$ indexes the data.OLS finds $\hat\beta_0$ and $\hat\beta_1$ that minimize the sum of squares of deviations,$$\sum_{j=1}^n ||z_j - \left(\hat\beta_0 + \hat\beta_1 w_j\right)||^2
= \sum_{j=1}^n \left(\bar z_j - \left(\bar{\hat\beta_0} + \bar{\hat\beta_1} \bar w_j\right)\right) \left(z_j - \left(\hat\beta_0 + \hat\beta_1 w_j\right)\right).$$Formally this is identical to the usual matrix formulation: compare it to $\left(z - X\beta\right)'\left(z - X\beta\right).$  The only difference we find is that the transpose of the design matrix $X'$ is replaced by the conjugate transpose $X^* = \bar X '$.  Consequently the formal matrix solution is$$\hat\beta = \left(X^*X\right)^{-1}X^* z.$$At the same time, to see what might be accomplished by casting this into a purely real-variable problem, we may write the OLS objective out in terms of the real components:$$\sum_{j=1}^n \left(x_j-\gamma_0-\gamma_1u_j+\delta_1v_j\right)^2 
+ \sum_{j=1}^n\left(y_j-\delta_0-\delta_1u_j-\gamma_1v_j\right)^2.$$Evidently this represents two linked real regressions: one of them regresses $x$ on $u$ and $v$, the other regresses $y$ on $u$ and $v$; and we require that the $v$ coefficient for $x$ be the negative of the $u$ coefficient for $y$ and the $u$ coefficient for $x$ equal the $v$ coefficient for $y$. Moreover, because the total squares of residuals from the two regressions are to be minimized, it will usually not be the case that either set of coefficients gives the best estimate for $x$ or $y$ alone.  This is confirmed in the example below, which carries out the two real regressions separately and compares their solutions to the complex regression.This analysis makes it apparent that rewriting the complex regression in terms of the real parts (1) complicates the formulas, (2) obscures the simple geometric interpretation, and (3) would require a generalized multivariate multiple regression (with nontrivial correlations among the variables) to solve.  We can do better.As an example, I take a grid of $w$ values at integral points near the origin in the complex plane.  To the transformed values $w\beta$ are added iid errors having a bivariate Gaussian distribution: in particular, the real and imaginary parts of the errors are not independent.It is difficult to draw the usual scatterplot of $(w_j, z_j)$ for complex variables, because it would consist of points in four dimensions.  Instead we can view the scatterplot matrix of their real and imaginary parts.Ignore the fit for now and look at the top four rows and four left columns: these display the data.  The circular grid of $w$ is evident in the upper left; it has $81$ points.  The scatterplots of the components of $w$ against the components of $z$ show clear correlations.  Three of them have negative correlations; only the $y$ (the imaginary part of $z$) and $u$ (the real part of $w$) are positively correlated.For these data, the true value of $\beta$ is $(-20 + 5i, -3/4 + 3/4\sqrt{3}i)$.  It represents an expansion by $3/2$ and a counterclockwise rotation of 120 degrees followed by translation of $20$ units to the left and $5$ units up.  I compute three fits: the complex least squares solution and two OLS solutions for $(x_j)$ and $(y_j)$ separately, for comparison.It will always be the case that the real-only intercept agrees with the real part of the complex intercept and the imaginary-only intercept agrees with the imaginary part fo the complex intercept.  It is apparent, though, that the real-only and imaginary-only slopes neither agree with the complex slope coefficients nor with each other, exactly as predicted.Let's take a closer look at the results of the complex fit.  First, a plot of the residuals gives us an indication of their bivariate Gaussian distribution.  (The underlying distribution has marginal standard deviations of $2$ and a correlation of $0.8$.)  Then, we can plot the magnitudes of the residuals (represented by sizes of the circular symbols) and their arguments (represented by colors exactly as in the first plot) against the fitted values: this plot should look like a random distribution of sizes and colors, which it does.Finally, we can depict the fit in several ways.  The fit appeared in the last rows and columns of the scatterplot matrix (q.v.) and may be worth a closer look at this point.  Below on the left the fits are plotted as open blue circles and arrows (representing the residuals) connect them to the data, shown as solid red circles.  On the right the $(w_j)$ are shown as open black circles filled in with colors corresponding to their arguments; these are connected by arrows to the corresponding values of $(z_j)$.  Recall that each arrow represents an expansion by $3/2$ around the origin, rotation by $120$ degrees, and translation by $(-20, 5)$, plus that bivariate Guassian error.These results, the plots, and the diagnostic plots all suggest that the complex regression formula works correctly and achieves something different than separate linear regressions of the real and imaginary parts of the variables.The R code to create the data, fits, and plots appears below.  Note that the actual solution of $\hat\beta$ is obtained in a single line of code.  Additional work--but not too much of it--would be needed to obtain the usual least squares output: the variance-covariance matrix of the fit, standard errors, p-values, etc."
What is the difference between a population and a sample?,"
What is the difference between a population and a sample? What common variables and statistics are used for each one, and how do those relate to each other? 
","['standard-deviation', 'variance', 'sample', 'population']","The population is the set of entities under study. For example, the mean height of men. This is a hypothetical population because it includes all men that have lived, are alive and will live in the future. I like this example because it drives home the point that we, as analysts, choose the population that we wish to study. Typically it is impossible to survey/measure the entire population because not all members are observable (e.g. men who will exist in the future). If it is possible to enumerate the entire population it is often costly to do so and would take a great deal of time. In the example above we have a population ""men"" and a parameter of interest, their height.Instead, we could take a subset of this population called a sample and use this sample to draw inferences about the population under study, given some conditions. Thus we could measure the mean height of men in a sample of the population which we call a statistic and use this to draw inferences about the parameter of interest in the population. It is an inference because there will be some uncertainty and inaccuracy involved in drawing conclusions about the population based upon a sample. This should be obvious - we have fewer members in our sample than our population therefore we have lost some information.There are many ways to select a sample and the study of this is called sampling theory. A commonly used method is called Simple Random Sampling (SRS). In SRS each member of the population has an equal probability of being included in the sample, hence the term ""random"". There are many other sampling methods e.g. stratified sampling, cluster sampling, etc which all have their advantages and disadvantages.It is important to remember that the sample we draw from the population is only one from a large number of potential samples. If ten researchers were all studying the same population, drawing their own samples then they may obtain different answers. Returning to our earlier example, each of the ten researchers may come up with a different mean height of men i.e. the statistic in question (mean height) varies of sample to sample -- it has a distribution called a sampling distribution. We can use this distribution to understand the uncertainty in our estimate of the population parameter.The sampling distribution of the sample mean is known to be a normal distribution with a standard deviation equal to the sample standard deviation divided by the sample size. Because this could easily be confused with the standard deviation of the sample it more common to call the standard deviation of the sampling distribution the standard error."
PCA objective function: what is the connection between maximizing variance and minimizing error?,"
The PCA algorithm can be formulated in terms of the correlation matrix (assume the data $X$ has already been normalized and we are only considering projection onto the first PC). The objective function can be written as:
$$ \max_w (Xw)^T(Xw)\; \: \text{s.t.} \: \:w^Tw = 1. $$
This is fine, and we use Lagrangian multipliers to solve it, i.e. rewriting it as:
$$ \max_w [(Xw)^T(Xw) - \lambda w^Tw], $$
which is equivalent to 
$$ \max_w \frac{ (Xw)^T(Xw) }{w^Tw},$$
and hence (see here on Mathworld) seems to be equal to $$\max_w \sum_{i=1}^n \text{(distance from point $x_i$ to line $w$)}^2.$$
But this is saying to maximize the distance between point and line, and from what I've read here, this is incorrect -- it should be $\min$, not $\max$. Where is my error?
Or, can someone show me the link between maximizing variance in projected space and minimizing distance between point and line?
","['pca', 'optimization']","Let $\newcommand{\X}{\mathbf X}\X$ be a centered data matrix with $n$ observations in rows. Let $\newcommand{\S}{\boldsymbol \Sigma}\S=\X^\top\X/(n-1)$ be its covariance matrix. Let $\newcommand{\w}{\mathbf w}\w$ be a unit vector specifying an axis in the variable space. We want $\w$ to be the first principal axis.According to the first approach, first principal axis maximizes the variance of the projection $\X \w$ (variance of the first principal component). This variance is given by the $$\mathrm{Var}(\X\w)=\w^\top\X^\top \X \w/(n-1)=\w^\top\S\w.$$According to the second approach, first principal axis minimizes the reconstruction error between $\X$ and its reconstruction $\X\w\w^\top$, i.e. the sum of squared distances between the original points and their projections onto $\w$. The square of the reconstruction error is given by
\begin{align}\newcommand{\tr}{\mathrm{tr}}
\|\X-\X\w\w^\top\|^2
&=\tr\left((\X-\X\w\w^\top)(\X-\X\w\w^\top)^\top\right) \\
&=\tr\left((\X-\X\w\w^\top)(\X^\top-\w\w^\top\X^\top)\right) \\
&=\tr(\X\X^\top)-2\tr(\X\w\w^\top\X^\top)+\tr(\X\w\w^\top\w\w^\top\X^\top) \\
&=\mathrm{const}-\tr(\X\w\w^\top\X^\top) \\
&=\mathrm{const}-\tr(\w^\top\X^\top\X\w) \\
&=\mathrm{const} - \mathrm{const} \cdot \w^\top \S \w. \end{align}Notice the minus sign before the main term. Because of that, minimizing the reconstruction error amounts to maximizing $\w^\top \S \w$, which is the variance. So minimizing reconstruction error is equivalent to maximizing the variance; both formulations yield the same $\w$."
What is meant by 'weak learner'?,"
Can anyone tell me what is meant by the phrase 'weak learner'? Is it supposed to be a weak hypothesis? I am confused about the relationship between a weak learner and a weak classifier. Are both the same or is there some difference? 
In the adaboost algorithm, T=10. What is meant by that? Why do we select T=10?
","['classification', 'svm', 'terminology', 'adaboost', 'pac-learning']",
"If only prediction is of interest, why use lasso over ridge?","
On page 223 in An Introduction to Statistical Learning, the authors summarise the differences between ridge regression and lasso. They provide an example (Figure 6.9) of when ""lasso tends to outperform ridge regression in terms of bias, variance, and MSE"". 
I understand why lasso can be desirable: it results in sparse solutions since it shrinks many coefficients to 0, resulting in simple and interpretable models. But I do not understand how it can outperform ridge when only predictions are of interest (i.e. how is it getting a substantially lower MSE in the example?). 
With ridge, if many predictors have almost no affect on the response (with a few predictors having a large effect), won't their coefficients simply be shrunk to a small number very close to zero... resulting in something very similar to lasso? So why would the final model have worse performance than lasso?
","['machine-learning', 'prediction', 'lasso', 'regularization', 'ridge-regression']","You are right to ask this question.  In general, when a proper accuracy scoring rule is used (e.g., mean squared prediction error), ridge regression will outperform lasso.  Lasso spends some of the information trying to find the ""right"" predictors and it's not even great at doing that in many cases.  Relative performance of the two will depend on the distribution of true regression coefficients.  If you have a small fraction of nonzero coefficients in truth, lasso can perform better.  Personally I use ridge almost all the time when interested in predictive accuracy."
Understanding input_shape parameter in LSTM with Keras,"
I'm trying to use the example described in the Keras documentation named ""Stacked LSTM for sequence classification"" (see code below) and can't figure out the input_shape parameter in the context of my data.
I have as input a matrix of sequences of 25 possible characters encoded in integers to a padded sequence of maximum length 31. As a result, my x_train has the shape (1085420, 31) meaning (n_observations, sequence_length).
from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np

data_dim = 16
timesteps = 8
num_classes = 10

# expected input data shape: (batch_size, timesteps, data_dim)
model = Sequential()
model.add(LSTM(32, return_sequences=True,
               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32
model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32
model.add(LSTM(32))  # return a single vector of dimension 32
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Generate dummy training data
x_train = np.random.random((1000, timesteps, data_dim))
y_train = np.random.random((1000, num_classes))

# Generate dummy validation data
x_val = np.random.random((100, timesteps, data_dim))
y_val = np.random.random((100, num_classes))

model.fit(x_train, y_train,
          batch_size=64, epochs=5,
          validation_data=(x_val, y_val))

In this code x_train has the shape (1000, 8, 16), as for an array of 1000 arrays of 8 arrays of 16 elements. There I get completely lost on what is what and how my data can reach this shape.
Looking at Keras doc and various tutorials and Q&A, it seems I'm missing something obvious. Can someone give me a hint of what to look for ?
Thanks for your help !
","['lstm', 'keras', 'dimensions']","LSTM shapes are tough so don't feel bad, I had to spend a couple days battling them myself:If you will be feeding data 1 character at a time your input shape should be (31,1) since your input has 31 timesteps, 1 character each. You will need to reshape your x_train from (1085420, 31) to (1085420, 31,1) which is easily done with this command :"
What exactly is a Bayesian model?,"
Can I call a model wherein Bayes' Theorem is used a ""Bayesian model""? I am afraid such a definition might be too broad.
So what exactly is a Bayesian model?
","['machine-learning', 'bayesian']","In essence, one where inference is based on using Bayes theorem to obtain a posterior distribution for a quantity or quantities of interest form some model (such as parameter values) based on some prior distribution for the relevant unknown parameters and the likelihood from the model.i.e. from a distributional model of some form, $f(X_i|\mathbf{\theta})$, and a prior $p(\mathbf{\theta})$, someone might seek to obtain the posterior $p(\mathbf{\theta}|\mathbf{X})$.A simple example of a Bayesian model is discussed in this question, and in the comments of this one - Bayesian linear regression, discussed in more detail in Wikipedia here. Searches turn up discussions of a number of Bayesian models here. But there are other things one might try to do with a Bayesian analysis besides merely fit a model - see, for example, Bayesian decision theory.  "
Why are non zero-centered activation functions a problem in backpropagation?,"
I read here the following:


Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on
  this soon) would be receiving data that is not zero-centered. This has
  implications on the dynamics during gradient descent, because if the
  data coming into a neuron is always positive (e.g. $x > 0$
  elementwise in $f = w^Tx + b$)), then the gradient on the weights
  $w$ will during backpropagation become either all be positive, or
  all negative (depending on the gradient of the whole expression
  $f$). This could introduce undesirable zig-zagging dynamics in the
  gradient updates for the weights. However, notice that once these
  gradients are added up across a batch of data the final update for the
  weights can have variable signs, somewhat mitigating this issue.
  Therefore, this is an inconvenience but it has less severe
  consequences compared to the saturated activation problem above.


Why would having all $x>0$ (elementwise) lead to all-positive or all-negative gradients on $w$?

","['neural-networks', 'deep-learning', 'backpropagation']","$$f=\sum w_ix_i+b$$ $$\frac{df}{dw_i}=x_i$$ $$\frac{dL}{dw_i}=\frac{dL}{df}\frac{df}{dw_i}=\frac{dL}{df}x_i$$because $x_i>0$, the gradient $\dfrac{dL}{dw_i}$ always has the same sign as $\dfrac{dL}{df}$ (all positive or all negative).Update
Say there are two parameters $w_1$ and $w_2$. If the gradients of two dimensions are always of the same sign (i.e., either both are positive or both are negative), it means we can only move roughly in the direction of northeast or southwest in the parameter space.If our goal happens to be in the northwest, we can only move in a zig-zagging fashion to get there, just like parallel parking in a narrow space. (forgive my drawing)Therefore all-positive or all-negative activation functions (relu, sigmoid) can be difficult for gradient based optimization. To solve this problem we can normalize the data in advance  to be zero-centered as in batch/layer normalization.Also another solution I can think of is to add a bias term for each input so the layer becomes
$$f=\sum w_i(x_i+b_i).$$
The gradients is then
$$\frac{dL}{dw_i}=\frac{dL}{df}(x_i-b_i)$$
the sign won't solely depend on $x_i$."
Why is Laplace prior producing sparse solutions?,"
I was looking through the literature on regularization, and often see paragraphs that links L2 regulatization with Gaussian prior, and L1 with Laplace centered on zero.
I know how these priors look like, but I don't understand, how it translates to, for example, weights in linear model. 
In L1, if I understand correctly, we expect sparse solutions, i.e. some weights will be pushed to exactly zero. And in L2 we get small weights but not zero weights.
But why does it happen?
Please comment if I need to provide more information or clarify my path of thinking.
","['regression', 'bayesian', 'prior', 'regularization', 'laplace-distribution']","The relation of Laplace distribution prior with median (or L1 norm) was found by Laplace himself, who found that using such prior you estimate median rather than mean as with Normal distribution (see Stingler, 1986 or Wikipedia). This means that regression with Laplace errors distribution estimates the median (like e.g. quantile regression), while Normal errors refer to OLS estimate.The robust priors you asked about were described also by Tibshirani (1996) who noticed that robust Lasso regression in Bayesian setting is equivalent to using Laplace prior. Such prior for coefficients are centered around zero (with centered variables) and has wide tails - so most regression coefficients estimated using it end up being exactly zero. This is clear if you look closely at the picture below, Laplace distribution has a peak around zero (there is a greater distribution mass), while Normal distribution is more diffuse around zero, so non-zero values have greater probability mass. Other possibilities for robust priors are Cauchy or $t$- distributions.Using such priors you are more prone to end up with many zero-valued coefficients, some moderate-sized and some large-sized (long tail), while with Normal prior you get more moderate-sized coefficients that are rather not exactly zero, but also not that far from zero.(image source Tibshirani, 1996)Stigler, S.M. (1986). The History of Statistics: The Measurement of Uncertainty Before 1900. Cambridge, MA: Belknap Press of Harvard University Press.Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 267-288.Gelman, A., Jakulin, A., Pittau, G.M., and Su, Y.-S. (2008). A weakly informative default prior distribution for logistic and other regression models. The Annals of Applied Statistics, 2(4), 1360-1383.Norton, R.M. (1984). The Double Exponential Distribution: Using Calculus to Find a Maximum Likelihood Estimator. The American Statistician, 38(2): 135-136."
Why does increasing the sample size lower the (sampling) variance?,"
Big picture:
I'm trying to understand how increasing the sample size increases the power of an experiment.
My lecturer's slides explain this with a picture of 2 normal distributions, one for the null-hypothesis and one for the alternative-hypothesis and a decision threshold c between them.
They argue that increasing sample size will lower variance and thereby cause a higher kurtosis, reducing the shared area under the curves and so the probability of a type II error.
Small picture:
I don't understand how a bigger sample size will lower the variance.
I assume you just calculate the sample variance and use it as a parameter in a normal distribution.
I tried:

googling, but most accepted answers have 0 upvotes or are merely examples
thinking: By the law of big numbers every value should eventually stabilize around its probable value according to the normal distribution we assume. And the variance should therefore converge to the variance of our assumed normal distribution. But what is the variance of that normal distribution and is it a minimum value i.e. can we be sure our sample variance decreases to that value?

","['variance', 'sampling', 'statistical-power']",
References for survival analysis,"
I am looking for a good book/tutorial to learn about survival analysis. I am also interested in references on doing survival analysis in R.
","['r', 'survival', 'references']","I like:The first does a good job of straddling theory and model building issues.  It's mostly focused on semi-parametric techniques, but there is reasonable coverage of parametric methods.  It doesn't really provide any R or other code examples, if that's what you're after.The second is heavy with modeling on the Cox PH side (as the title might indicate).  It's by the author of the survival package in R and there are plenty of R examples and mini-case studies.  I think both books complement each other, but I'd recommend the first for getting started.  A quick way to get started in R is David Diez's guide."
Using LASSO from lars (or glmnet) package in R for variable selection,"
Sorry if this question comes across a little basic.
I am looking to use LASSO variable selection for a multiple linear regression model in R. I have 15 predictors, one of which is categorical(will that cause a problem?). After setting my $x$ and $y$ I use the following commands:
model = lars(x, y)
coef(model)

My problem is when I use coef(model). This returns a matrix with 15 rows, with one extra predictor added each time. However there is no suggestion as to which model to choose. Have I missed something? Is there a way I can get the lars package to return just one ""best"" model?
There are other posts suggesting using glmnet instead but this seems more complicated. An attempt is as follows, using the same $x$ and $y$. Have I missed something here?:  
cv = cv.glmnet(x, y)
model = glmnet(x, y, type.gaussian=""covariance"", lambda=cv$lambda.min)
predict(model, type=""coefficients"")

The final command returns a list of my variables, the majority with a coefficient although some are =0. Is this the correct choice of the ""best"" model selected by LASSO? If I then fit a linear model with all my variables which had coefficients not=0 I get very similar, but slightly different, coefficient estimates. Is there a reason for this difference? Would it be acceptable to refit the linear model with these variables chosen by LASSO and take that as my final model? Otherwise I cannot see any p-values for significance. Have I missed anything?  
Does 
type.gaussian=""covariance"" 

ensure that that glmnet uses multiple linear regression?
Does the automatic normalisation of the variables affect the coefficients at all? Is there any way to include interaction terms in a LASSO procedure?
I am looking to use this procedure more as a demonstration of how LASSO can be used than for any model that will actually be used for any important inference/prediction if that changes anything.
Thank you for taking the time to read this. Any general comments on LASSO/lars/glmnet would also be greatly appreciated.
","['feature-selection', 'lasso', 'glmnet', 'lars']",
Intuition behind tensor product interactions in GAMs (MGCV package in R),"
Generalized additive models are those where
$$
y = \alpha + f_1(x_1) + f_2(x_2) + e_i
$$
for example.  the functions are smooth, and to be estimated.  Usually by penalized splines.  MGCV is a package in R that does so, and the author (Simon Wood) writes a book about his package with R examples.  Ruppert, et al. (2003) write a far more accessible book about simpler versions of the same thing.
My question is about interactions within these sorts of models.  What if I want to do something like the following:
$$
y = \alpha + f_1(x_1) + f_2(x_2) + f_3(x_1\times x_2) + e_i
$$
if we were in OLS land (where the $f$ is just a beta), I'd have no problem with interpreting $\hat{f}_3$.  If we estimate via penalized splines, I also have no problem with interpretation in the additive context.
But the MGCV package in GAM has these things called ""tensor product smooths"".  I google ""tensor product"" and my eyes immediately glaze over trying to read the explanations that I find.  Either I'm not smart enough or the math isn't explained very well, or both.
Instead of coding
normal = gam(y~s(x1)+s(x2)+s(x1*x2))

a tensor product would do the same (?) thing by
what = gam(y~te(x1,x2))

when I do
plot(what)

or
vis.gam(what)

I get some really cool output.  But I have no idea what is going on inside the black box that is te(), nor how to interpret the aforementioned cool output.  Just the other night I had a nightmare that I was giving a seminar.  I showed everyone a cool graph, they asked me what it meant, and I didn't know.
Could anyone help both me, and posterity, by giving a bit of mechanics and intuition on what is going on underneath the hood here?  Ideally by saying a bit about the difference between the normal additive interaction case and the tensor case?
","['r', 'nonparametric', 'interaction', 'splines', 'intuition']","I'll (try to) answer this in three steps: first, let's identify exactly what we mean by a univariate smooth. Next, we will describe a multivariate smooth (specifically, a smooth of two variables). Finally, I'll make my best attempt at describing a tensor product smooth.Let's say we have some response data $y$ that we conjecture is an unknown function $f$ of a predictor variable $x$ plus some error $ε$. The model would be:$$y=f(x)+ε$$Now, in order to fit this model, we have to identify the functional form of $f$. The way we do this is by identifying basis functions, which are superposed in order to represent the function $f$ in its entirety. A very simple example is a linear regression, in which the basis functions are just $β_2x$ and $β_1$, the intercept. Applying the basis expansion, we have$$y=β_1+β_2x+ε$$In matrix form, we would have:$$Y=Xβ+ε$$Where $Y$ is an n-by-1 column vector, $X$ is an n-by-2 model matrix, $β$ is a 2-by-1 column vector of model coefficients, and $ε$ is an n-by-1 column vector of errors. $X$ has two columns because there are two terms in our basis expansion: the linear term and the intercept.The same principle applies for basis expansion in MGCV, although the basis functions are much more sophisticated. Specifically, individual basis functions need not be defined over the full domain of the independent variable $x$. Such is often the case when using knot-based bases (see ""knot based example""). The model is then represented as the sum of the basis functions, each of which is evaluated at every value of the independent variable. However, as I mentioned, some of these basis functions take on a value of zero outside of a given interval and thus do not contribute to the basis expansion outside of that interval. As an example, consider a cubic spline basis in which each basis function is symmetric about a different value (knot) of the independent variable -- in other words, every basis function looks the same but is just shifted along the axis of the independent variable (this is an oversimplification, as any practical basis will also include an intercept and a linear term, but hopefully you get the idea).To be explicit, a basis expansion of dimension $i-2$ could look like:$$y=β_1+β_2x+β_3f_1(x)+β_4f_2(x)+...+β_if_{i-2} (x)+ε$$where each function $f$ is, perhaps, a cubic function of the independent variable $x$.The matrix equation $Y=Xβ+ε$ can still be used to represent our model. The only difference is that $X$ is now an n-by-i matrix; that is, it has a column for every term in the basis expansion (including the intercept and linear term). Since the process of basis expansion has allowed us to represent the model in the form of a matrix equation, we can use linear least squares to fit the model and find the coefficients $β$.This is an example of unpenalized regression, and one of the main strengths of MGCV is its smoothness estimation via a penalty matrix and smoothing parameter. In other words, instead of:$$β=(X^TX)^{-1}X^TY$$we have:$$β=(X^TX+λS)^{-1}X^TY$$where $S$ is a quadratic $i$-by-$i$ penalty matrix and $λ$ is a scalar smoothing parameter. I will not go into the specification of the penalty matrix here, but it should suffice to say that for any given basis expansion of some independent variable and definition of a quadratic ""wiggliness"" penalty (for example, a second-derivative penalty), one can calculate the penalty matrix $S$.MGCV can use various means of estimating the optimal smoothing parameter $λ$. I will not go into that subject since my goal here was to give a broad overview of how a univariate smooth is constructed, which I believe I have done.The above explanation can be generalized to multiple dimensions. Let's go back to our model that gives the response $y$ as a function $f$ of predictors $x$ and $z$. The restriction to two independent variables will prevent cluttering the explanation with arcane notation. The model is then:$$y=f(x,z)+ε$$Now, it should be intuitively obvious that we are going to represent $f(x,z)$ with a basis expansion (that is, a superposition of basis functions) just like we did in the univariate case of $f(x)$ above. It should also be obvious that at least one, and almost certainly many more, of these basis functions must be functions of both $x$ and $z$ (if this was not the case, then implicitly $f$ would be separable such that $f(x,z)=f_x(x)+f_z(z)$). A visual illustration of a multidimensional spline basis can be found here. A full two dimensional basis expansion of dimension $i-3$ could look something like:$$y=β_1+β_2x+β_3z+β_4f_1(x,z)+...+β_if_{i-3} (x,z)+ε$$I think it's pretty clear that we can still represent this in matrix form with:$$Y=Xβ+ε$$by simply evaluating each basis function at every unique combination of $x$ and $z$. The solution is still:$$β=(X^TX)^{-1}X^TY$$Computing the second derivative penalty matrix is very much the same as in the univariate case, except that instead of integrating the second derivative of each basis function with respect to a single variable, we integrate the sum of all second derivatives (including partials) with respect to all independent variables. The details of the foregoing are not especially important: the point is that we can still construct penalty matrix $S$ and use the same method to get the optimal value of smoothing parameter $λ$, and given that smoothing parameter, the vector of coefficients is still:$$β=(X^TX+λS)^{-1}X^TY$$Now, this two-dimensional smooth has an isotropic penalty: this means that a single value of $λ$ applies in both directions. This works fine when both $x$ and $z$ are on approximately the same scale, such as a spatial application. But what if we replace spatial variable $z$ with temporal variable $t$? The units of $t$ may be much larger or smaller than the units of $x$, and this can throw off the integration of our second derivatives because some of those derivatives will contribute disproportionately to the overall integration (for example, if we measure $t$ in nanoseconds and $x$ in light years, the integral of the second derivative with respect to $t$ may be vastly larger than the integral of the second derivative with respect to $x$, and thus ""wiggliness"" along the $x$ direction may go largely unpenalized). Slide 15 of the ""smooth toolbox"" I linked has more detail on this topic.It is worth noting that we did not decompose the basis functions into marginal bases of $x$ and $z$. The implication here is that multivariate smooths must be constructed from bases supporting multiple variables. Tensor product smooths support construction of multivariate bases from univariate marginal bases, as I explain below.Tensor product smooths address the issue of modeling responses to interactions of multiple inputs with different units. Let's suppose we have a response $y$ that is a function $f$ of spatial variable $x$ and temporal variable $t$. Our model is then:$$y=f(x,t)+ε$$What we'd like to do is construct a two-dimensional basis for the variables $x$ and $t$. This will be a lot easier if we can represent $f$ as:$$f(x,t)=f_x(x)f_t(t)$$In an algebraic / analytical sense, this is not necessarily possible. But remember, we are discretizing the domains of $x$ and $t$ (imagine a two-dimensional ""lattice"" defined by the locations of knots on the $x$ and $t$ axes) such that the ""true"" function $f$ is represented by the superposition of basis functions. Just as we assumed that a very complex univariate function may be approximated by a simple cubic function on a specific interval of its domain, we may assume that the non-separable function $f(x,t)$ may be approximated by the product of simpler functions $f_x(x)$ and $f_t(t)$ on an interval—provided that our choice of basis dimensions makes those intervals sufficiently small!Our basis expansion, given an $i$-dimensional basis in $x$ and $j$-dimensional basis in $t$, would then look like:\begin{align}
y = &β_{1} + β_{2}x + β_{3}f_{x1}(x)+β_{4}f_{x2}(x)+...+  \\
    &β_{i}f_{x(i-3)}(x)+ β_{i+1}t + β_{i+2}tx + β_{i+3}tf_{x1}(x)+β_{i+4}tf_{x2}(x)+...+  \\
    &β_{2i}tf_{x(i-3)}(x)+ β_{2i+1}f_{t1}(t) + β_{2i+2}f_{t1}(t)x + β_{2i+3}f_{t1}(t)f_{x1}(x)+β_{i+4}f_{t1}(t)f_{x2}(x){\small +...+}  \\
    &β_{2i}f_{t1}(t)f_{x(i-3)}(x)+\ldots+  \\
    &β_{ij}f_{t(j-3)}(t)f_{x(i-3)}(x) + ε
\end{align}Which may be interpreted as a tensor product. Imagine that we evaluated each basis function in $x$ and $t$, thereby constructing n-by-i and n-by-j model matrices $X$ and $T$, respectively. We could then compute the $n^2$-by-$ij$ tensor product $X \otimes T$ of these two model matrices and reorganize into columns, such that each column represented a unique combination $ij$. Recall that the marginal model matrices had $i$ and $j$ columns, respectively. These values correspond to their respective basis dimensions. Our new two-variable basis should then have dimension $ij$, and therefore the same number of columns in its model matrix.NOTE: I'd like to point out that since we explicitly constructed the tensor product basis functions by taking products of marginal basis functions, tensor product bases may be constructed from marginal bases of any type. They need not support more than one variable, unlike the multivariate smooth discussed above.In reality, this process results in an overall basis expansion of dimension $ij-i-j+1$ because the full multiplication includes multiplying every $t$ basis function by the x-intercept $β_{x1}$ (so we subtract $j$) as well as multiplying every $x$ basis function by the t-intercept $β_{t1}$ (so we subtract $i$), but we must add the intercept back in by itself (so we add 1). This is known as applying an identifiability constraint.So we can represent this as:$$y=β_1+β_2x+β_3t+β_4f_1(x,t)+β_5f_2(x,t)+...+β_{ij-i-j+1}f_{ij-i-j-2}(x,t)+ε$$Where each of the multivariate basis functions $f$ is the product of a pair of marginal $x$ and $t$ basis functions. Again, it's pretty clear having constructed this basis that we can still represent this with the matrix equation:$$Y=Xβ+ε$$Which (still) has the solution:$$β=(X^TX)^{-1}X^TY$$Where the model matrix $X$ has $ij-i-j+1$ columns. As for the penalty matrices $J_x$ and $J_t$, these are are constructed separately for each independent variable as follows:$$J_x=β^T I_j \otimes S_x β$$and,$$J_t=β^T S_t \otimes I_i β$$This allows for an overall anisotropic (different in each direction) penalty (Note: the penalties on the second derivative of $x$ are added up at each knot on the $t$ axis, and vice versa). The smoothing parameters $λ_x$ and $λ_t$ may now be estimated in much the same way as the single smoothing parameter was for the univariate and multivariate smooths. The result is that the overall shape of a tensor product smooth is invariant to rescaling of its independent variables.I recommend reading all the vignettes on the MGCV website, as well as ""Generalized Additive Models: and introduction with R."" Long live Simon Wood."
Reviewing statistics in papers [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            



Closed 2 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






For some of us, refereeing papers is part of the job. When refereeing statistical methodology papers, I think advice from other subject areas is fairly useful, i.e. computer science and Maths.
This question concerns reviewing more applied statistical papers. By this I mean, the paper is submitted to a non-statistical/mathematical journal and statistics is just mentioned in the ""methods"" section.
Some particular questions:

How much effort should we put in to understand the application area?
How much time should I spend on a report?
How picky are you when looking at figures/tables.
How do you cope with the data not being available.
Do you try and rerun the analysis used.
What's the maximum number of papers your would review in a year?

Have a missed any questions? Feel free to edit or add a comment.
Edit
I coming to this question as a statistician reviewing a biology paper, but I'm interested in the statistical review of any non-mathematical discipline.

I'm not sure if this should be a CW. On one hand it's a bit open, but on the other I can see myself accepting an answer. Also, answers will probably be fairly long.
","['references', 'referee']","I am not sure about which area of science you are referring to (I'm sure the answer would be really different if dealing with biology vs physics for instance...)Anyway, as a biologist, I will answer from a ""biological"" point of view:How much effort should we put in to understand the application area?I tend at least to read the previous papers from the same authors and look for a few review on the subject if I am not too familiar with it. This is especially true when dealing with new techniques I don't know, because I need to understand if they did all the proper controls etc. How much time should I spend on a report?As much as needed (OK, dumb answer, I know! :P)
In general I would not like someone reviewing my paper to do an approximative job just because he/she has other things to do, so I try not to do it myself.How picky are you when looking at figures/tables.Quite picky. Figures are the first thing you look at when browsing through a paper. They need to be consistent (e.g. right titles on the axes, correct legend etc.). On occasion I have suggested to use a different kind of plot to show data when I thought the one used was not the best. This happens a lot in biology, a field that is dominated by the ""barplot +/- SEM"" type of graph.
I'm also quite picky on the ""materials and methods"" section: a perfect statistical analysis on a inherently wrong biological model is completely useless.How do you cope with the data not being available.You just do and trust the Authors, I guess. In many cases in biology there's not much you can do, especially when dealing with things like imaging or animal behaviour and similar. Unless you want people to publish tons of images, videos etc (that you most likely would not go through anyways), but that may be very unpractical. If you think the data are really necessary ask for the authors to provide them as supplementary data/figures.Do you try and rerun the analysis used.Only if I have serious doubts on the conclusions drawn by the authors.
In biology there's often a difference between what is (or not) ""statistically significant"" and what is ""biologically significant"". I prefer a thinner statistical analysis with good biological reasoning then the other way around. But again, in the very unlikely event that I were to review a bio-statistics paper (ahah, that would be some fun!!!) I would probably pay much more attention to the stats than to the biology in there. "
PCA and Correspondence analysis in their relation to Biplot,"
Biplot is often used to display results of principal component analysis (and of related techniques). It is a dual or overlay scatterplot showing component loadings and component scores simultaneously. I was informed by @amoeba today that he has given an answer departing from my comment to a question which asks about how biplot coordinates are produced/scaled; and his answer consideres several ways in some detail. And @amoeba asked if I would share my experience with biplot.
My experience (both theoretical and by experimenting), although very modest, nevetherless highlights two things that are not so frequently recognized: (1) biplot should be classified as analytic technique rather than subsidiary scatterplot; (2) PCA, correspondence analysis (and some other well known techniques) are actually particular cases of the biplot. Or, at least, they both are almost twins to biplot. If you can do biplot you can do the other two.
My question to you is: how are they (PCA, CA, Biplot) connected for you? Please, share your thoughts. Meanwile I'm posting my own account about it. I would like to ask to add more answers and to make critical remarks.
","['pca', 'multivariate-analysis', 'svd', 'correspondence-analysis', 'biplot']","Singular-value decomposition is at the root of the three kindred techniques. Let $\bf X$ be $r \times c$ table of real values. SVD is $\bf X = U_{r\times r}S_{r\times c}V_{c\times c}'$. We may use just $m$ $[m \le\min(r,c)]$ first latent vectors and roots to obtain $\bf X_{(m)}$ as the best $m$-rank approximation of $\bf X$: $\bf X_{(m)} = U_{r\times m}S_{m\times m}V_{c\times m}'$. Further, we'll notate $\bf U=U_{r\times m}$, $\bf V=V_{c\times m}$, $\bf S=S_{m\times m}$.Singular values $\bf S$ and their squares, the eigenvalues, represent scale, also called inertia, of the data. Left eigenvectors $\bf U$ are the coordinates of the rows of the data onto the $m$ principal axes; while right eigenvectors $\bf V$ are the coordinates of the columns of the data onto those same latent axes. The entire scale (inertia) is stored in $\bf S$ and so the coordinates $\bf U$ and $\bf V$ are unit-normalized (column SS=1).In PCA, it is agreed upon to consider rows of $\bf X$ as random observations (which can come or go), but to consider columns of $\bf X$ as fixed number of dimensions or variables. Hence it is appropriate and convenient to remove the effect of the number of rows (and only rows) on the results, particularly on the eigenvalues, by svd-decomposing of $\mathbf Z=\mathbf X/\sqrt{r}$ instead of $\bf X$. Note that this corresponds to eigen-decomposition of $\mathbf {X'X}/r$, $r$ being the sample size n. (Often, mostly with covariances - to make them unbiased - we'll prefer to divide by $r-1$, but it is a nuance.)The multiplication of $\bf X$ by a constant affected only $\bf S$; $\bf U$ and $\bf V$ remain to be the unit-normalized coordinates of rows and of columns.From here and everywhere below we redefine $\bf S$, $\bf U$ and $\bf V$ as given by svd of $\bf Z$, not of $\bf X$; $\bf Z$ being a normalized version of $\bf X$, and the normalization varies between types of analysis.By multiplying $\mathbf U\sqrt{r}=\bf U_*$ we bring the mean square in the columns of $\bf U$ to 1. Given that rows are random cases to us, it is logical. We've thus obtained what is called in PCA standard or standardized principal component scores of observations, $\bf U_*$. We do not do the same thing with $\bf V$ because variables are fixed entities.We then can confer rows with all the inertia, to obtain unstandardized row coordinates, also called in PCA raw principal component scores of observations: $\bf U_*S$. This formula we'll call ""direct way"". The same result is returned by $\bf XV$; we'll label it ""indirect way"".Analogously, we can confer columns with all the inertia, to obtain unstandardized column coordinates, also called in PCA the component-variable loadings: $\bf VS'$ [may ignore transpose if $\bf S$ is square], - the ""direct way"". The same result is returned by $\bf Z'U$, - the ""indirect way"". (The above standardized principal component scores can also be computed from the loadings as $\bf X(AS^{-1/2})$, where $\bf A$ are the loadings.)Consider biplot in a sense of a dimensionality reduction analysis on its own, not simply as ""a dual scatterplot"". This analysis is very similar to PCA. Unlike PCA, both rows and columns are treated, symmetrically, as random observations, which means that $\bf X$ is being seen as a random two-way table of varying dimensionality. Then, naturally, normalize it by both $r$ and $c$ before svd: $\mathbf Z=\mathbf X/\sqrt{rc}$.After svd, compute standard row coordinates as we did it in PCA: $\mathbf U_*=\mathbf U\sqrt{r}$. Do the same thing (unlike PCA) with column vectors, to obtain standard column coordinates: $\mathbf V_*=\mathbf V\sqrt{c}$. Standard coordinates, both of rows and of columns, have mean square 1.We may confer rows and/or columns coordinates with inertia of eigenvalues like we do it in PCA. Unstandardized row coordinates: $\bf U_*S$ (direct way). Unstandardized column coordinates: $\bf V_*S'$ (direct way). What's about the indirect way? You can easily deduce by substitutions that the indirect formula for the unstandardized row coordinates is $\mathbf {XV_*}/c$, and for the unstandardized column coordinates is $\mathbf {X'U_*}/r$.PCA as a particular case of Biplot. From the above descriptions you probably learned that PCA and biplot differ only in how they normalize $\bf X$ into $\bf Z$ which is then decomposed. Biplot normalizes by both the number of rows and the number of columns; PCA normalizes only by the number of rows. Consequently, there is a little difference between the two in the post-svd computations. If in doing biplot you set $c=1$ in its formulas you will get exactly PCA results. Thus, biplot can be seen as a generic method and PCA as a particular case of biplot.[Column centering. Some user may say: Stop, but doesn't PCA require also and first of all the centering of the data columns (variables) in order it to explain variance? While biplot may not do the centering? My answer: only PCA-in-narrow-sense does the centering and explains variance; I'm discussing linear PCA-in-general-sense, PCA which explains some sort sum of squared deviations from the origin chosen; you might choose it to be the data mean, the native 0 or whatever you like. Thus, the ""centering"" operation isn't what could distinguish PCA from biplot.]In biplot or PCA, you can set some rows and/or columns to be passive, or supplementary. Passive row or column does not influence the SVD and therefore does not influence the inertia or the coordinates of other rows/columns, but receives its coordinates in the space of principal axes produced by the active (not passive) rows/columns.To set some points (rows/columns) to be passive, (1) define $r$ and $c$ be the number of active rows and columns only. (2) Set to zero passive rows and columns in $\bf Z$ before svd. (3) Use the ""indirect"" ways to compute coordinates of passive rows/columns, since their eigenvector values will be zero.In PCA, when you compute component scores for new incoming cases with the help of loadings obtained on old observations (using the score coefficient matrix), you actually doing the same thing as taking these new cases in PCA and keeping them passive. Similarly, to compute correlations/covariances of some external variables with the component scores produced by a PCA is equivalent to taking those variables in that PCA and keeping them passive.The column mean squares (MS) of standard coordinates are 1. The column mean squares (MS) of unstandardized coordinates are equal to the inertia of the respective principal axes: all the inertia of eigenvalues was donated to eigenvectors to produce the unstandardized coordinates.In biplot: row standard coordinates $\bf U_*$ have MS=1 for each principal axis. Row unstandardized coordinates, also called row principal coordinates $\mathbf {U_*S} = \mathbf {XV_*}/c$ have MS = corresponding eigenvalue of $\bf Z$. The same is true for column standard and unstandardized (principal) coordinates.Generally, it is not required that one endows coordinates with inertia either in full or in none. Arbitrary spreading is allowed, if needed for some reason. Let $p_1$ be the proportion of inertia which is to go to rows. Then the general formula of row coordinates is: $\bf U_*S^{p1}$ (direct way) = $\mathbf {XV_*S^{p1-1}}/c$ (indirect way). If $p_1=0$ we get standard row coordinates, whereas with $p_1=1$ we get principal row coordinates.Likewise $p_2$ be the proportion of inertia which is to go to columns. Then the general formula of column coordinates is: $\bf V_*S^{p2}$ (direct way) = $\mathbf {X'U_*S^{p2-1}}/r$ (indirect way). If $p_2=0$ we get standard column coordinates, whereas with $p_2=1$ we get principal column coordinates.The general indirect formulas are universal in that they allow to compute coordinates (standard, principal or in-between) also for the passive points, if there are any.If $p_1+p_2=1$ they say the inertia is distributed between row and column points. The $p_1=1,p_2=0$, i.e. row-principal-column-standard, biplots are sometimes called ""form biplots"" or ""row-metric preservation"" biplots.
The $p_1=0,p_2=1$, i.e. row-standard-column-principal, biplots are often called  within PCA literature ""covariance biplots"" or ""column-metric preservation"" biplots; they display variable loadings (which are juxtaposed to covariances) plus standardized component scores, when applied within PCA.In correspondence analysis, $p_1=p_2=1/2$ is often used and is called ""symmetric"" or ""canonical"" normalization by inertia - it allows (albeit at some expence of euclidean geometric strictness) compare proximity between row and column points, like we can do on multidimensional unfolding map.Two-way (=simple) correspondence analysis (CA) is biplot used to analyze a two-way contingency table, that is, a non-negative table which entries bear the meaning of some sort of affinity between a row and a column. When the table is frequencies chi-square model correspondence analysis is used. When the entries is, say, means or other scores, a simplier Euclidean model CA is used.Euclidean model CA is just the biplot described above, only that the table $\bf X$ is additionally preprocessed before it enters the biplot operations. In particular, the values are normalized not only by $r$ and $c$ but also by the total sum $N$.The preprocessing consists of centering, then normalizing by the mean mass. Centering can be various, most often: (1) centering of columns; (2) centering of rows; (3) two-way centering which is the same operation as computation of frequency residuals; (4) centering of columns after equalizing column sums; (5) centering of rows after equalizing row sums. Normalizing by the mean mass is dividing by the mean cell value of the initial table. At preprocessing step, passive rows/columns, if exist, are standardized passively: they are centered/normalized by the values computed from active rows/columns.Then usual biplot is done on the preprocessed $\bf X$, starting from $\mathbf Z=\mathbf X/\sqrt{rc}$.Imagine that the activity or importance of a row or a column can be any number between 0 and 1, and not only 0 (passive) or 1 (active) as in the classic biplot discussed so far. We could weight the input data by these row and column weights and perform weighted biplot. With weighted biplot, the greater is the weight the more influential is that row or that column regarding all the results - the inertia and the coordinates of all the points onto the principal axes.The user supplies row weights and column weights. These and those are first normalized separately to sum to 1. Then the normalization step is $\mathbf{Z_{ij} = X_{ij}}\sqrt{w_i w_j}$, with $w_i$ and $w_j$ being the weights for row i and column j. Exactly zero weight designates the row or the column to be passive.At that point we may discover that classic biplot is simply this weighted biplot with equal weights $1/r$ for all active rows and equal weights $1/c$ for all active columns; $r$ and $c$ the numbers of active rows and active columns.Perform svd of $\bf Z$. All operations are the same as in classic biplot, the only difference being that $w_i$ is in place of $1/r$ and $w_j$ is in place of $1/c$. Standard row coordinates: $\mathbf {U_{*i}=U_i}/\sqrt{w_i}$ and standard column coordinates: $\mathbf {V_{*j}=V_j}/\sqrt{w_j}$. (These are for rows/columns with nonzero weight. Leave values as 0 for those with zero weight and use the indirect formulas below to obtain standard or whatever coordinates for them.)Give inertia to coordinates in the proportion you want (with $p_1=1$ and $p_2=1$ the coordinates will be fully unstandardized, or principal; with $p_1=0$ and $p_2=0$ they will stay standard). Rows: $\bf U_*S^{p1}$ (direct way) = $\bf X[Wj]V_*S^{p1-1}$ (indirect way). Columns: $\bf V_*S^{p2}$ (direct way) = $\bf ([Wi]X)'U_*S^{p2-1}$ (indirect way). Matrices in brackets here are the diagonal matrices of the column and the row weights, respectively. For passive points (that is, with zero weights) only the indirect way of computation is suited. For active (positive weights) points you may go either way.PCA as a particular case of Biplot revisited. When considering unweighted biplot earlier I mentioned that PCA and biplot are equivalent, the only difference being that biplot sees columns (variables) of the data as random cases symmetrically to observations (rows). Having extended now biplot to more general weighted biplot we may once again claim it, observing that the only difference is that (weighted) biplot normalizes the sum of column weights of input data to 1, and (weighted) PCA - to the number of (active) columns. So here is the weighted PCA introduced. Its results are proportionally identical to those of weighted biplot. Specifically, if $c$ is the number of active columns, then the following relationships are true, for weighted as well as classic versions of the two analyses:This is technically a weighted biplot where weights are being computed from a table itself rather then supplied by the user. It is used mostly to analyze frequency cross-tables. This biplot will approximate, by euclidean distances on the plot, chi-square distances in the table. Chi-square distance is mathematically the euclidean distance inversely weighted by the marginal totals. I will not go further in details of Chi-square model CA geometry.The preprocessing of frequency table $\bf X$ is as follows: divide each frequency by the expected frequency, then subtract 1. It is the same as to first obtain the frequency residual and then to divide by the expected frequency. Set row weights to $w_i=R_i/N$ and column weights to $w_j=C_j/N$, where $R_i$ is the marginal sum of row i (active columns only), $C_j$ is the marginal sum of column j (active rows only), $N$ is the table total active sum (the three numbers come from the initial table).Then do weighted biplot: (1) Normalize $\bf X$ into $\bf Z$. (2) The weights are never zero (zero $R_i$ and $C_j$ are not allowed in CA); however you can force rows/columns to become passive by zeroing them in $\bf Z$, so their weights are ineffective at svd. (3) Do svd. (4) Compute standard and inertia-vested coordinates as in weighted biplot.In Chi-square model CA as well as in Euclidean model CA using two-way centering one last eigenvalue is always 0, so the maximal possible number of principal dimensions is $\min(r-1,c-1)$.See also a nice overview of chi-square model CA in this answer.Here is some data table.Several dual scatterplots (in 2 first principal dimensions) built on analyses of these values follow. Column points are connected with the origin by spikes for visual emphasis. There were no passive rows or columns in these analyses.The first biplot is SVD results of the data table analyzed ""as is""; the coordinates are the row and the column eigenvectors.Below is one of possible biplots coming from PCA. PCA was done on the data ""as is"", without centering the columns; however, as it is adopted in PCA, normalization by the number of rows (the number of cases) was done initially. This specific biplot displays principal row coordinates (i.e. raw component scores) and principal column coordinates (i.e. variable loadings).Next is biplot sensu stricto: The table was initially normalized both by the number of rows and the number of columns. Principal normalization (inertia spreading) was used for both row and column coordinates - as with PCA above. Note the similarity with the PCA biplot: the only difference is due to the difference in the initial normalization.Chi-square model correspondence analysis biplot. The data table was preprocessed in the special manner, it included two-way centering and a normalization using marginal totals. It is a weighted biplot. Inertia was spread over the row and the column coordinates symmetrically - both are halfway between ""principal"" and ""standard"" coordinates.The coordinates displayed on all these scatterplots:"
How to set up neural network to output ordinal data?,"
I have a neural network set up to predict something where the output variable is ordinal.  I will describe below using three possible outputs A < B < C.  
It is pretty obvious how to use a neural network to output categorical data: the output is just a softmax of the last (usually fully connected) layer, one per category, and the predicted category is the one with the largest output value (this is the default in many popular models).  I have been using the same setup for ordinal values.  However, in this case the outputs often don't make sense, for example the network outputs for A and C are high but B is low: this is not plausible for ordinal values.
I have one idea for this, which is to calculate loss based on comparing the outputs with 1 0 0 for A, 1 1 0 for B, and 1 1 1 for C.  The exact thresholds can be tuned later using another classifier (eg Bayesian) but this seems to capture the essential idea of an ordering of inputs, without prescribing any specific interval scale.
What is the standard way of solving this problem?  Is there any research or references that describe the pros and cons of different approaches?
","['neural-networks', 'ordinal-data', 'softmax']","I think the approach to only encode the ordinal labels asclass 1 is represented as [0 0 0 0 ...]class 2 is represented as [1 0 0 0 ...]class 3 is represented as [1 1 0 0 ...]and use binary cross-entropy as the loss function is suboptimal. As mentioned in the comments, it might happen that the predicted vector is for example [1 0 1 0 ...]. This is undesirable for making predictions.The paper Rank-consistent ordinal regression for neural networks describes how to restrict the neural network to make rank-consistent predictions. You have to make sure that the last layer shares its weights, but should have different biases. You can implement this in Tensorflow by adding the following as the last part of the network (credits for https://stackoverflow.com/questions/59656313/how-to-share-weights-and-not-biases-in-keras-dense-layers):Note that the number of ordinal classes here is 5, hence the $K-1$ biases.I tested the difference in performance on actual data, and the predictive accuracy improved substantially. Hope this helps."
Are smaller p-values more convincing?,"
I've been reading up on $p$-values, type 1 error rates, significance levels, power calculations, effect sizes and the Fisher vs Neyman-Pearson debate. This has left me feeling a bit overwhelmed. I apologise for the wall of text, but I felt it was necessary to provide an overview of my current understanding of these concepts, before I moved on to my actual questions.

From what I've gathered, a $p$-value is simply a measure of surprise, the probability of obtaining a result at least as extreme, given that the null hypothesis is true. Fisher originally intended for it to be a continuous measure.
In the Neyman-Pearson framework, you select a significance level in advance and use this as an (arbitrary) cut-off point. The significance level is equal to the type 1 error rate. It is defined by the long run frequency, i.e. if you were to repeat an experiment 1000 times and the null hypothesis is true, about 50 of those experiments would result in a significant effect, due to the sampling variability. By choosing a significance level, we are guarding ourselves against these false positives with a certain probability. $P$-values traditionally do not appear in this framework.
If we find a $p$-value of 0.01 this does not mean that the type 1 error rate is 0.01, the type 1 error is stated a priori. I believe this is one of the major arguments in the Fisher vs N-P debate, because $p$-values are often reported as 0.05*, 0.01**, 0.001***. This could mislead people into saying that the effect is significant at a certain $p$-value, instead of at a certain significance value.
I also realise that the $p$-value is a function of the sample size. Therefore, it cannot be used as an absolute measurement. A small $p$-value could point to a small, non-relevant effect in a large sample experiment. To counter this, it is important to perform an power/effect size calculation when determining the sample size for your experiment. $P$-values tell us whether there is an effect, not how large it is. See Sullivan 2012.
My question:
How can I reconcile the facts that the $p$-value is a measure of surprise (smaller = more convincing) while at the same time it cannot be viewed as an absolute measurement?
What I am confused about, is the following: can we be more confident in a small $p$-value than a large one? In the Fisherian sense, I would say yes, we are more surprised. In the N-P framework, choosing a smaller significance level would imply we are guarding ourselves more strongly against false positives. 
But on the other hand, $p$-values are dependent on sample size. They are not an absolute measure. Thus we cannot simply say 0.001593 is more significant than 0.0439. Yet this what would be implied in Fisher's framework: we would be more surprised to such an extreme value. There's even discussion about the term highly significant being a misnomer: Is it wrong to refer to results as being ""highly significant""?
I've heard that $p$-values in some fields of science are only considered important when they are smaller than 0.0001, whereas in other fields values around 0.01 are already considered highly significant.
Related questions:

Is the ""hybrid"" between Fisher and Neyman-Pearson approaches to statistical testing really an ""incoherent mishmash""?
When to use Fisher and Neyman-Pearson framework?
Is the exact value of a 'p-value' meaningless?
Frequentist properties of p-values in relation to type I error
Confidence intervals vs P-values for two means
Why are lower p-values not more evidence against the null? Arguments from Johansson 2011 (as provided by @amoeba)

","['hypothesis-testing', 'statistical-significance', 'confidence-interval', 'p-value', 'effect-size']","Are smaller $p$-values ""more convincing""? Yes, of course they are.In the Fisher framework, $p$-value is a quantification of the amount of evidence against the null hypothesis. The evidence can be more or less convincing; the smaller the $p$-value, the more convincing it is. Note that in any given experiment with fixed sample size $n$, the $p$-value is monotonically related to the effect size, as @Scortchi nicely points out in his answer (+1). So smaller $p$-values correspond to larger effect sizes; of course they are more convincing!In the Neyman-Pearson framework, the goal is to obtain a binary decision: either the evidence is ""significant"" or it is not. By choosing the threshold $\alpha$, we guarantee that we will not have more than $\alpha$ false positives. Note that different people can have different $\alpha$ in mind when looking at the same data; perhaps when I read a paper from a field that I am skeptical about, I would not personally consider as ""significant"" results with e.g. $p=0.03$ even though the authors do call them significant. My personal $\alpha$ might be set to $0.001$ or something. Obviously the lower the reported $p$-value, the more skeptical readers it will be able to convince! Hence, again, lower $p$-values are more convincing.The currently standard practice is to combine Fisher and Neyman-Pearson approaches: if $p<\alpha$, then the results are called ""significant"" and the $p$-value is [exactly or approximately] reported and used as a measure of convincingness (by marking it with stars, using expressions as ""highly significant"", etc.); if $p>\alpha$ , then the results are called ""not significant"" and that's it. This is usually referred to as a ""hybrid approach"", and indeed it is hybrid. Some people argue that this hybrid is incoherent; I tend to disagree. Why would it be invalid to do two valid things at the same time?Further reading:Is the ""hybrid"" between Fisher and Neyman-Pearson approaches to statistical testing really an ""incoherent mishmash""? -- my question about the ""hybrid"". It generated some discussion, but I am still not satisfied with any of the answers, and plan to get back to that thread at some point.Is it wrong to refer to results as being ""highly significant""? -- see my yesterday's answer, which is essentially saying: it isn't wrong (but perhaps a bit sloppy).Why are lower p-values not more evidence against the null? Arguments from Johansson 2011 -- an example of an anti-Fisher paper arguing that $p$-values do not provide evidence against the null; the top answer by @Momo does a good job in debunking the arguments. My answer to the title question is: But of course they are."
Compendium of cross-validation techniques,"
I'm wondering if anybody knows of a compendium of cross-validation techniques with a discussion of the differences between them and a guide on when to use each of them. Wikipedia has a list of the most common techniques, but I'm curious if there are other techniques, and if there are any taxonomies for them.
For example, I just run into a library that allows me to choose one of the following strategies:

Hold out
Bootstrap
K Cross-validation
Leave one out
Stratified Cross Validation
Balanced Stratified Cross Validation
Stratified Hold out
Stratified Bootstrap

and I am trying to understand what stratified and balanced mean in bootstrapping, hold out or CV.
We can also turn this post into a community wiki if people want ,and collect a discussion of techniques or taxonomies here. 
",['cross-validation'],"You can add to that list: I don't really have a lot of advice as far as how to use these techniques or when to use them.  You can use the caret package in R to compare CV, Boot, Boot632, leave-one-out, leave-group-out, and out-of-bag cross-validation.In general, I usually use the boostrap because it is less computationally intensive than repeated k-fold CV, or leave-one-out CV.  Boot632 is my algorithm of choice because it doesn't require much more computation than the bootstrap, and has show to be better than cross-validation or the basic bootstap in certain situations.I almost always use out-of-bag error estimates for random forests, rather than cross-validation.  Out-of-bag errors are generally unbiased, and random forests take long enough to compute as it is."
What are correct values for precision and recall when the denominators equal 0?,"
Precision is defined as:

p = true positives / (true positives + false positives)

What is the value of precision if (true positives + false positives) = 0? Is it just undefined?
Same question for recall:

r = true positives / (true positives + false negatives)

In this case, what is the value of recall if (true positives + false negatives) = 0?
P.S. This question is very similar to the question What are correct values for precision and recall in edge cases?.
",['precision-recall'],"The answers to the linked earlier question apply here too.If (true positives + false negatives) = 0 then no positive cases in the input data, so any analysis of this case has no information, and so no conclusion about how positive cases are handled.  You want N/A or something similar as the ratio result, avoiding a division by zero errorIf (true positives + false positives) = 0 then all cases have been predicted to be negative: this is one end of the ROC curve. Again, you want to recognise and report this possibility while avoiding a division by zero error.  "
What are the practical differences between the Benjamini & Hochberg (1995) and the Benjamini & Yekutieli (2001) false discovery rate procedures?,"
My statistics program implements both the Benjamini & Hochberg (1995) and Benjamini & Yekutieli (2001) false discovery rate (FDR) procedures.  I have done my best to read through the later paper, but it is quite mathematically dense and I am not reasonably certain I understand the difference between the procedures.  I can see from the underlying code in my statistics program that they are indeed different and that the latter includes a quantity q that I have seen referred to in regards to FDR, but also don't quite have a grasp of.
Is there any reason to prefer the Benjamini & Hochberg (1995) procedure versus the Benjamini & Yekutieli (2001) procedure?  Do they have different assumptions?  What are the practical differences between these approaches?
Benjamini, Y., and Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society Series B, 57, 289–300.
Benjamini, Y., and Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. Annals of Statistics 29, 1165–1188.
The 1999 paper referenced in the comments below: Yekutieli, D., & Benjamini, Y. (1999). Resampling-based false discovery rate controlling multiple test procedures for correlated test statistics. Journal of Statistical Planning and Inference, 82(1), 171-196.
","['post-hoc', 'false-discovery-rate']","Benjamini and Hochberg (1995)  introduced the false discovery rate. Benjamini and Yekutieli (2001)  proved that the estimator is valid under some forms of dependence.  Dependence can arise as follows. Consider the continuous variable used in a t-test and another variable correlated with it; for example, testing if BMI differs in two groups and if waist circumference differs in these two groups. Because these variables are correlated, the resulting p-values will also be correlated. Yekutieli and Benjamini (1999) developed another FDR controlling procedure, which can be used under general dependence by resampling the null distribution. Because the comparison is with respect to the null permutation distribution, as the total number of true positives increases, the method becomes more conservative.  It turns out that BH 1995 is also conservative as the number of true positives increases. To improve this, Benjamini  and Hochberg (2000)  introduced the adaptive FDR procedure. This required estimation of a parameter, the null proportion, which is also used in Storey's pFDR estimator.  Storey gives comparisons and argues that his method is more powerful and emphasizes the conservative nature of 1995 procedure.  Storey also has results and simulations under dependence. All of the above tests are valid under independence. The question is what kind of departure from independence can these estimates deal with. My current thinking is that if you don't expect too many true positives the BY (1999) procedure is nice because it incorporates distributional features and dependence. However, I'm unaware of an implementation. Storey's method was designed for many true positives with some dependence. BH 1995 offers an alternative to the family-wise error rate and it is still conservative. Benjamini, Y and Y Hochberg. On the Adaptive Control of the False Discovery Rate in Multiple Testing with Independent Statistics. Journal of Educational and Behavioral Statistics, 2000."
When and how to use standardized explanatory variables in linear regression,"
I have 2 simple questions about linear regression:

When is it advised to standardize the explanatory variables?
Once estimation is carried out with standardized values, how can one predict with new values (how one should standardize the new values)? 

Some references would be helpful. 
","['regression', 'predictive-models', 'references', 'standardization', 'predictor']","Although terminology is a contentious topic, I prefer to call ""explanatory"" variables, ""predictor"" variables.I also think that relying on standardised variables may take attention away from the fact that we have not thought about how to make the metric of a variable more meaningful to the reader.Andrew Gelman has a fair bit to say on the topic.
See his page on standardisation for example and Gelman (2008, Stats Med, FREE PDF) in particular."
Why do I get a 100% accuracy decision tree?,"
I'm getting a 100% accuracy for my decision tree. What am I doing wrong?
This is my code:
import pandas as pd
import json
import numpy as np
import sklearn
import matplotlib.pyplot as plt


data = np.loadtxt(""/Users/Nadjla/Downloads/allInteractionsnum.csv"", delimiter=',')


x = data[0:14]
y = data[-1]


from sklearn.cross_validation import train_test_split

x_train = x[0:2635]
x_test = x[0:658]
y_train = y[0:2635]
y_test = y[0:658]


from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier()
tree.fit(x_train.astype(int), y_train.astype(int))


from sklearn.metrics import accuracy_score

y_predicted = tree.predict(x_test.astype(int))
accuracy_score(y_test.astype(int), y_predicted)

","['machine-learning', 'python', 'cart', 'accuracy']","Your test sample is a subset of your training sample:This means that you evaluate your model on a part of your training data, i.e., you are doing in-sample evaluation. In-sample accuracy is a notoriously poor indicator to out-of-sample accuracy, and maximizing in-sample accuracy can lead to overfitting. Therefore, one should always evaluate a model on a true holdout sample that is completely independent of the training data.Make sure your training and your testing data are disjoint, e.g., "
What is the minimum recommended number of groups for a random effects factor?,"
I'm using a mixed model in R (lme4) to analyze some repeated measures data. I have a response variable (fiber content of feces) and 3 fixed effects (body mass, etc.). My study only has 6 participants, with 16 repeated measures for each one (though two only have 12 repeats). The subjects are lizards that were given different combinations of food in different 'treatments'. 
My question is: can I use subject ID as a random effect? 
I know this is the usual course of action in longitudinal mixed effects models, to take account of the randomly sampled nature of the subjects and the fact that observations within subjects will be more closely correlated than those between subjects. But, treating subject ID as a random effect involves estimating a mean and variance for this variable. 

Since I have only 6 subjects (6 levels of this factor), is this enough to get an accurate characterization of the mean and variance? 
Does the fact that I have quite a few repeated measurements for each subject help in this regard (I don't see how it matters)? 
Finally, If I can't use subject ID as a random effect, will including it as a fixed effect allow me to control for the fact that I have repeated measures?

Edit: I'd just like to clarify that when I say ""can I"" use subject ID as a random effect, I mean ""is it a good idea to"". I know I can fit the model with a factor with just 2 levels, but surely this would be in-defensible? I'm asking at what point does it become sensible to think about treating subjects as random effects? It seems like the literature advises that 5-6 levels is a lower bound. It seems to me that the estimates of the mean and variance of the random effect would not be very precise until there were 15+ factor levels.
","['mixed-model', 'sample-size']","Short answer: Yes, you can use ID as random effect with 6 levels.Slightly longer answer: The @BenBolker's GLMM FAQ says (among other things) the following under the headline ""Should I treat factor xxx as fixed or random?"":One point of particular relevance to 'modern' mixed model estimation
  (rather than 'classical' method-of-moments estimation) is that, for
  practical purposes, there must be a reasonable number of
  random-effects levels (e.g. blocks) — more than 5 or 6 at a minimum.So you are at the lower bound, but on the right side of it."
Bound for the correlation of three random variables,"
There are three random variables, $x,y,z$. The three correlations between the three variables are the same. That is,
$$\rho=\textrm{cor}(x,y)=\textrm{cor}(x,z)=\textrm{cor}(y,z)$$
What is the tightest bound you can give for $\rho$?
","['correlation', 'correlation-matrix']",
What does negative R-squared mean?,"
Let's say I have some data, and then I fit the data with a model (a non-linear regression). Then I calculate the R-squared ($R^2$).
When R-squared is negative, what does that mean? Does that mean my model is bad? I know the range of $R^2$ can be [-1,1]. When $R^2$ is 0, what does that mean as well?
","['regression', 'goodness-of-fit', 'r-squared', 'curve-fitting', 'negative-r-squared']",
Why law of large numbers does not apply in the case of Apple share price?,"
Here is the article in NY times called ""Apple confronts the law of large numbers"". It tries to explain Apple share price rise using law of large numbers. What statistical (or mathematical) errors does this article make? 
","['probability', 'central-limit-theorem', 'law-of-large-numbers', 'statistics-in-media']","Here is the rub: Apple is so big, it’s running up against the law of
  large numbers.Also known as the golden theorem, with a proof attributed to the
  17th-century Swiss mathematician Jacob Bernoulli, the law states that
  a variable will revert to a mean over a large sample of results. In
  the case of the largest companies, it suggests that high earnings
  growth and a rapid rise in share price will slow as those companies
  grow ever larger.This muddled jumble actually refers to three different phenomena!The (various) Laws of Large Numbers are fundamental in probability theory for characterizing situations where it is reasonable to expect large samples to give increasingly better information about a process or population being sampled.  Indeed, Jacob Bernoulli was the first to recognize the need to state and prove such a theorem, which appeared in his posthumous Ars Conjectandi in 1713 (edited by nephew Nicholas Bernoulli).There is no apparent valid application of such a law to Apple's growth.Regression toward the mean was first recognized by Francis Galton in the 1880's.  It has often been underappreciated among business analysts, however.  For example, at the beginning of 1933 (during the depths of a Great Depression), Horace Secrist published his magnum opus, the Triumph of Mediocrity in Business.  In it, he copiously examined business time series and found, in every case, evidence of regression toward the mean.  But, failing to recognize this as an ineluctable mathematical phenomenon, he maintained that he had uncovered a basic truth of business development!  This fallacy of mistaking a purely mathematical pattern for the result of some underlying force or tendency (now often called the ""regression fallacy"") is reminiscent of the quoted passage.(It is noteworthy that Secrist was a prominent statistician, author of one of the most popular statistics textbooks published at the time.  On JSTOR, you can find a lacerating review of Triumph... by Harold Hotelling published in JASA in late 1933.  In a subsequent exchange of letters with Secrist, Hotelling wrote My review ... was chiefly devoted to warning readers not to conclude that business firms have a tendency to become mediocre ...  To ""prove"" such a mathematical result by a costly and prolonged numerical study ... is analogous to proving the multiplication table by arranging elephants in rows and columns, and then doing the same for numerous other kinds of animals.  The performance, though perhaps entertaining, and having a certain pedagogical value, is not an important contribution either to zoology or to mathematics.[JASA Vol. 29, No. 186 (June 1934), pp 198 and 199].)The NY Times passage seems to make the same mistake with Apple's business data.If we read on in the article, however, we soon uncover the author's intended meaning:If Apple’s share price grew even 20 percent a year for the next decade, which is far below its current blistering pace, its \$500 billion market capitalization would be more than \$3 trillion by 2022.This, of course, is a statement about extrapolation of exponential growth.  As such it contains echoes of Malthusian population predictions.  The hazards of extrapolation are not confined to exponential growth, however.  Mark Twain (Samuel Clements) pilloried wanton extrapolators in Life on the Mississippi (1883, chapter 17):Now, if I wanted to be one of those ponderous scientific people, and 'let on' to prove ... what will occur in the far future by what has occurred in late years, what an opportunity is here! ... Please observe:-- In the space of one hundred and seventy-six years the Lower Mississippi has shortened itself two hundred and forty-two miles. That is an average of a trifle over one mile and a third per year. Therefore, any calm person, who is not blind or idiotic, can see that in the “Old Oolitic Silurian Period,” just a million years ago next November, the Lower Mississippi River was upwards of one million three hundred thousand miles long, and stuck out over the Gulf of Mexico like a fishing-rod. And by the same token any person can see that seven hundred and forty-two years from now the Lower Mississippi will be only a mile and threequarters long, and Cairo and New Orleans will have joined their streets together, and be plodding comfortably along under a single mayor and a mutual board of aldermen. There is something fascinating about science. One gets such wholesale returns of conjecture out of such a trifling investment of fact.”(Emphasis added.)  Twain's satire compares favorably to the article's quotation of business analyst Robert Cihra:If you extrapolate far enough out into the future, to sustain that growth Apple would have to sell an iPhone to every man, woman, child, animal and rock on the planet.(Unfortunately, it appears Cihra does not heed his own advice: he rates this stock a ""buy.""  He might be right, not on the merits, but by virtue of the greater fool theory.)If we take the article to mean ""beware of extrapolating previous growth into the future,"" we will get much out of it.  Investors who think this company is a good buy because its PE ratio is low (which includes several of the notable money managers quoted in the article) are no better than the ""ponderous scientific people"" Twain skewered over a century ago.A better acquaintance with Bernoulli, Hotelling, and Twain would have improved the accuracy and readability of this article, but in the end it seems to have gotten the message right."
How do DAGs help to reduce bias in causal inference?,"
I have read in several places that the use of DAGs can help to reduce bias due to 

Confounding
Differential Selection
Mediation
Conditioning on a collider

I also see the term “backdoor path” a lot.
How do we use DAGs to reduce these biases, and how does it relate to backdoor paths ?
Extra points (I will award a bounty) for real world examples of the above
","['inference', 'causality', 'dag']","A DAG is a Directed Acyclic Graph.A “Graph” is a structure with nodes (which are usually variables in statistics) and arcs (lines) connecting nodes to other nodes. “Directed” means that all the arcs have a direction, where one end of the arc has an arrow head, and the other does not, which usually refers to causation. “Acyclic” means that the graph is not cyclic – that means there can be no path from any node that leads back to the same node.  In statistics a DAG is a very powerful tool to aid in causal inference – to estimate the causal effect of one variable (often called the main exposure) on another (often called the outcome) in the presence of other variables which may be competing exposures, confounders or mediators.  The DAG can be used to identify a minimal sufficient set of variables to be used in a multivariable regression model for the estimation of said causal effect.  For example it is usually a very bad idea to condition on a mediator (a variable that lies on the causal path between the main exposure and the outcome), while it is usually a very good idea to condition on a confounder (a variable that is a cause, or a proxy for a cause, of both the main exposure and the outcome). It is also a bad idea to condition on a collider (to be defined below).But first, what is the problem we want to overcome?  This is what a multivariable regression model looks like to your statistical software:The software does not “know” which variables are our main exposure, competing exposures, confounders or mediators. It treats them all the same. In the real world it is far more common for the variables to be inter-related. For example, knowledge of the particular area of research may indicate a structure such as:Note that it is the researchers job to specify the causal paths, using expert knowledge about the subject at hand. DAGs represent a set of (often abstracted) causal beliefs pertinent to specific causal relationships. One researcher's DAG may be different to another researcher's DAG, for the same relationship(s), and that is completely OK. In the same way, a researcher may have more than one DAG for the same causal relationships, and using DAGs in a principled way as described below is one way to gather knowledge about, or support for a particular hypothesis.Let’s suppose that our interest is in the causal effect of $X7$ on $Y$. What are we to do? A very naive approach is simply to put all the variables into a regression model, and take the estimated coefficient for $X7$ as our “answer”. This would be a big mistake. It turns out that the only variable that should be adjusted for in this DAG is $X3$, because it is a confounder.  But what if our interest was in the effect of $X3$, not $X7$ ? Do we simply use the same model (also containing $X7$) and just take the estimate of $X3$ as our “answer”? No! In this case, we do not adjust for $X7$ because it is a mediator. No adjustment is needed at all. In both cases, we may also adjust for $X1$ because this is a competing exposure and will improve the precision of our casual inferences in both models. In both models we should not adjust for $X2$, $X4$, $X5$ and $X6$ because all of them are mediators for the effect of $X7$ on $Y$.So, getting back to the question, how do DAGs actually enable us to do this?  First we need to establish a few ground truths.A collider is a variable which has more than 1 cause – that is, at least 2 arrows are pointing at it (hence the incoming arrows “collide”). $X5$ in the above DAG is a colliderIf there are no variables being conditioned on, a path is blocked if and only if it contains a collider. The path $X4 \rightarrow X5 \leftarrow X6$ is blocked by the collider $X5$.Note: when we talk about ""conditioning"" on a variable this could refer to a few things, for example stratifying, but perhaps more commonly including the variable as a covariate in a multivariable regression model. Other synonymous terms are ""controlling for"" and ""adjusting for"".Any path that contains a non-collider that has been conditioned on is blocked. The path $Y \leftarrow X3 \rightarrow X7$ will be blocked if we condition on $X3$.A collider (or a descendant of a collider) that has been conditioned on does not block a path. If we condition on $X5$ we will open the path $X4 \rightarrow X5 \leftarrow X6$A backdoor path is a non-causal path between an outcome and a cause. It is non-causal because it contains an arrow pointing at both the cause and the outcome. For example the path $Y \leftarrow X3 \rightarrow X7$ is a backdoor path from $Y$ to $X3$.Confounding of a causal path occurs where a common cause for both variables is present. In other words confounding occurs where an unblocked backdoor path is present. Again, $Y \leftarrow X3 \rightarrow X7$ is such a path.So, armed with this knowledge, let’s see how DAGs help us with removing bias:The definition of confounding is 6 above.  If we apply 4 and condition on the confounder we will block the backdoor path from the outcome to the cause, thereby removing confounding bias.  The example is the association of carrying a lighter and lung cancer:Carrying a lighter has no causal effect on lung cancer, however, they share a common cause - smoking - so applying rule 5 above, a backdoor path from Lung cancer to carrying a lighter is present which induces an association between carrying a lighter and Lung cancer. Conditioning on Smoking will remove this association, which can be demonstrate with a simple simulation where I use continuous variables for simplicity:which shows the spurious association between Lighter and Cancer, but now when we condition on Smoking:...the bias is removed.A mediator is a variable that lies on the causal path between the cause and the outcome.  This means that the outcome is a collider. Therefore, applying rule 3 means that we should not condition on the mediator otherwise the indirect effect of the cause on the outcome (i.e., that mediated by the mediator) will be blocked. A good example example is the grades of a student and their happiness. A mediating variable is self-esteem:Here, Grades has a direct effect on Happiness, but it also has an indirect effect mediated by self-esteem. We want to estimate the total causal effect of Grades on Happiness. Rule 3 says that a path that contains a non-collider that has been conditioned on is blocked. Since we want the total effect (i.e., including the indirect effect) we should not condition on self-esteem otherwise the mediated path will be blocked, as we can see in the following simulation:So the total effect should be 2:which is what we do find. But if we now condition on self esteem:only the direct effect for grades is estimated, due to blocking the indirect effect by conditioning on the mediator SelfEsteem.This is probably the most difficult one to understand, but with the aid of a very simple DAG we can easily see the problem:Here, there is no causal path between X and Y. However, both cause C, the collider. If we condition on C, then applying rule 4 above we will invoke collider bias by opening up the (non causal) path between X, and Y. This may be a little hard to grasp at first, but it should become apparent by thinking in terms of equations. We have X + Y = C.  Let X and Y be binary variables taking the values 1 or zero. Hence, C can only take the values of 0, 1 or 2. Now, when we condition on C we fix its value. Say we fix it at 1. This immediately means that if X is zero then Y must be 1, and if Y is zero then X must be one. That is, X = -Y, so they are perfectly (negatively) correlated, conditional on C= 1. We can also see this in action with the following simulation:So, X and Y are independent so we should find no association:and indeed no association is found. But now condition on Cand now we have a spurious association between X and Y.Now let’s consider a slightly more complex situation:Here we are interested in the causal effect of Activity on Cervical Cancer. Hypochondria is an unmeasured variable which is a psychological condition that is characterized by fears of minor and sometimes non-existent medical symptoms being an indication of major illness. Lesion is also an unobserved variable that indicates the presence of a pre-cancerous lesion. Test is a diagnostic test for early stage cervical cancer. Here we hypothesise that both the unmeasured variables affect Test, obviously in the case of Lesion, and by making frequent visits to the doctor in the case of Hypochondria. Lesion also (obviously causes Cancer) and Hypochondria causes more physical activity (because persons with hypochondria are worried about a sedentary lifestyle leading to disease in later life.First notice that if the collider, Test, was removed and replace with an arc either from Lesion to Hypochondria or vice versa, then our causal path of interest, Activity to Cancer, would be confounded, but due to rule 2 above, the collider blocks the backdoor path $\text{Cancer}\leftarrow \text{Lesion} \rightarrow \text{Test} \leftarrow \text{Hypochondria} \rightarrow \text{Activity}$, as we can see with a simple simulation:where we hypothesize a much smaller effect of Activity on Cancer than Lesion on CancerAnd indeed we obtain a reasonable estimate.Now, also observe the association of Activity and Cancer with Test (due to their common, but unmeasured causes:The traditional definition of confounding is that a confounder is variable that is associated with both the exposure and the outcome. So, we might mistakenly think that Test is a confounder and condition on it. However, we then open up the backdoor path $\text{Cancer}\leftarrow \text{Lesion} \rightarrow \text{Test} \leftarrow \text{Hypochondria} \rightarrow \text{Activity}$, and introduce confounding which would otherwise not be present, as we can see from:Now not only is the estimate for Activity biased, but it is of larger magnitude and of the opposite sign!The preceding example can also be used to demonstrate selection bias. A researcher may identify Test as a potential confounder, and then only conduct the analysis on those that have tested negative (or positive).So for those that test positive we obtain a very small positive effect, that is not statistically significant at the 5% levelAnd for those that test negative we obtain a very small negative association which is also not significant."
Tiny (real) datasets for giving examples in class?,"
When teaching an introductory level class, the teachers I know tend to invent some numbers and a story in order to exemplify the method they are teaching. 
What I would prefer is to tell a real story with real numbers.  However, these stories needs to relate to a very tiny dataset, which enables manual calculations.
Any suggestions for such datasets will be very welcomed.
Some sample topics for the tiny datasets:

correlation/regression (basic)
ANOVA (1/2 ways)
z/t tests - one/two un/paired samples
comparisons of proportions - two/multi way tables

","['dataset', 'references', 'teaching']","The data and story library is an "" online library of datafiles and stories that illustrate the use of basic statistics methods"".This site seems to have what you need, and you can search it for particular data sets."
Functions of Independent Random Variables,"
Is the claim that functions of independent random variables are themselves independent, true? 
I have seen that result often used implicitly in some proofs, for example in the proof of independence between the sample mean and the sample variance of a normal distribution, but I have not been able to find justification for it. It seems that some authors take it as given but I am not certain that this is always the case.   
","['probability', 'self-study', 'random-variable', 'independence']","The most general and abstract definition of independence makes this assertion trivial while supplying an important qualifying condition: that two random variables are independent means the sigma-algebras they generate are independent.  Because the sigma-algebra generated by a measurable function of a sigma-algebra is a sub-algebra, a fortiori any measurable functions of those random variables have independent algebras, whence those functions are independent.(When a function is not measurable, it usually does not create a new random variable, so the concept of independent wouldn't even apply.)Let's unwrap the definitions to see how simple this is.  Recall that a random variable $X$ is a real-valued function defined on the ""sample space"" $\Omega$ (the set of outcomes being studied via probability).A random variable $X$ is studied by means of the probabilities that its value lies within various intervals of real numbers (or, more generally, sets constructed in simple ways out of intervals: these are the Borel measurable sets of real numbers).Corresponding to any Borel measurable set $I$ is the event $X^{*}(I)$ consisting of all outcomes $\omega$ for which $X(\omega)$ lies in $I$.The sigma-algebra generated by $X$ is determined by the collection of all such events.The naive definition says two random variables $X$ and $Y$ are independent ""when their probabilities multiply.""  That is, when $I$ is one Borel measurable set and $J$ is another, then$\Pr(X(\omega)\in I\text{ and }Y(\omega)\in J) = \Pr(X(\omega)\in I)\Pr(Y(\omega)\in J).$But in the language of events (and sigma algebras) that's the same as$\Pr(\omega \in X^{*}(I)\text{ and }\omega \in Y^{*}(J)) = \Pr(\omega\in X^{*}(I))\Pr(\omega\in Y^{*}(J)).$Consider now two functions $f, g:\mathbb{R}\to\mathbb{R}$ and suppose that $f \circ X$ and $g\circ Y$ are random variables.  (The circle is functional composition: $(f\circ X)(\omega) = f(X(\omega))$.  This is what it means for $f$ to be a ""function of a random variable"".)  Notice--this is just elementary set theory--that$$(f\circ X)^{*}(I) = X^{*}(f^{*}(I)).$$In other words, every event generated by $f\circ X$ (which is on the left) is automatically an event generated by $X$ (as exhibited by the form of the right hand side).  Therefore (5) automatically holds for $f\circ X$ and $g\circ Y$: there's nothing to check!NB You may replace ""real-valued"" everywhere by ""with values in $\mathbb{R}^d$"" without needing to change anything else in any material way.  This covers the case of vector-valued random variables."
What is an instrumental variable?,"
Instrumental variables are becoming increasingly common in applied economics and statistics. For the uninitiated, can we have some non-technical answers to the following questions:

What is an instrumental variable?
When would one want to employ an instrumental variable?
How does one find or choose an instrumental variable?

","['regression', 'econometrics', 'instrumental-variables']",
Why shouldn't the denominator of the covariance estimator be n-2 rather than n-1?,"
The denominator of the (unbiased) variance estimator is $n-1$ as there are $n$ observations and only one parameter is being estimated.
$$
\mathbb{V}\left(X\right)=\frac{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}}{n-1}
$$
By the same token I wonder why shouldn't the denominator of covariance be $n-2$ when two parameters are being estimated?
$$
\mathbb{Cov}\left(X, Y\right)=\frac{\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)\left(Y_{i}-\overline{Y}\right)}{n-1}
$$
","['self-study', 'variance', 'covariance', 'descriptive-statistics', 'unbiased-estimator']",
Were generative adversarial networks introduced by Jürgen Schmidhuber?,"
I read on https://en.wikipedia.org/wiki/Generative_adversarial_networks :

[Generative adversarial networks] were introduced by Ian Goodfellow et al in 2014.

but Jurgen Schmidhuber claims to have performed similar work earlier in that direction (e.g., there was some debate at NIPS 2016 during the generative adversarial networks tutorial: https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks see 1h03min). 
Was the idea behind generative adversarial networks first publicly introduced by Jürgen Schmidhuber? If not, how similar were Jürgen Schmidhuber's ideas?
","['neural-networks', 'history', 'gan']",
Can you explain Parzen window (kernel) density estimation in layman's terms?,"
Parzen window density estimation is described as
$$ p(x)=\frac{1}{n}\sum_{i=1}^{n} \frac{1}{h^2} \phi \left(\frac{x_i - x}{h} \right) $$
where $n$ is number of elements in the vector, $x$ is a vector, $p(x)$ is a probability density of $x$, $h$ is dimension of the Parzen Window, and $\phi$ is a window function.
My questions are:

What is the basic difference between a Parzen Window Function and other density functions like Gaussian Function and so on?
What is the role of the Window Function ($\phi$) in finding the density of $x$?
Why can we plug other density functions in place of the Window Function?
What is the role of $h$ in in finding the density of $x$?

","['density-function', 'kernel-smoothing', 'intuition', 'density-estimation']","1) My understanding is that users have a choice of functions to use for $\phi$, and that the Gaussian function is a very common choice.2) The density at $x$ is the mean of the different values of $\phi_h(x_i - x)$ at $x$. For example, you might have $x_1=1$, $x_2 = 2$, and a Gaussian distribution with $\sigma=1$ for $\phi_h$. In this case, the density at $x$ would be $\frac{\mathcal{N}_{1, 1}(x) + \mathcal{N}_{2, 1}(x)}{2}$.3) You can plug in any density function you like as your window function.4) $h$ determines the width of your chosen window function."
Is LSTM (Long Short-Term Memory) dead?,"
From my own experience, LSTM has a long training time, and does not improve performance significantly in many real world tasks.
To make the question more specific, I want to ask when LSTM will work better than other deep NN (may be with real world examples)? I know LSTM captures the sequential relationship in data, but is it really necessary?
Most demos on related topic are meaningless. They just focus on toy data e.g., IMDB review, where simple logistic regression will get very good results. I do not see any value of using LSTM which has huge computational cost but marginal improvements (if there are any).
Even with these toy examples, I did not find any good use cases that LSTM can solve very well but other models cannot.
","['machine-learning', 'natural-language', 'lstm']","Maybe. But RNNs aren't.Transformers learn ""pseudo-temporal"" relationships; they lack the true recurrent gradient that RNNs have, and thus extract fundamentally different features. This paper, for example, shows that the standard transformers are difficult to optimize in reinforcement learning settings, especially in memory-intensive environments. They do, however, eventually design a variant surpassing LSTMs.Where are RNNs still needed?Long memory tasks. Very long memory. IndRNNs have show ability to remember for 5000 timesteps, where LSTM barely manages 1000. A transformer is quadratic in time-complexity whereas RNNs are linear, meaning good luck processing even a single iteration of 5000 timesteps. If that isn't enough, the recent Legendre Memory Units have demonstrated memory of up to 512,000,000 timesteps; I'm unsure the world's top supercomputer could fit the resultant 1E18 tensor in memory.Aside reinforcement learning, signal applications are memory-demanding - e.g. speech synthesis, video synthesis, seizure classification. While CNNs have shown much success on these tasks, many utilize RNNs inserted in later layers; CNNs learn spatial features, RNNs temporal/recurrrent. An impressive 2019 paper's network manages to clone a speaker's voice from a only a 5 second sample, and it uses CNNs + LSTMs.Memory vs. Feature Quality:One doesn't warrant the other; ""quality"" refers to information utility for a given task. For sentences with 50 words, for example, model A may classify superior to model B, but fail dramatically with 100 where B would have no trouble. This exact phenomenon is illustrated in the recent Bistable Recurrent Cell paper, where the cell shows better memory for longer sequences, but is outdone by LSTMs on shorter sequences. An intuition is, LSTMs' four-gated networking permits for greater control over information routing, and thus richer feature extraction.Future of LSTMs?My likeliest bet is, some form of enhancement - like a Bistable  Recurrent Cell, maybe with attention, and recurrent normalization (e.g. LayerNorm or Recurrent BatchNorm). BRC's design is based on control theory, and so are LMUs; such architectures enjoy self-regularization, and there's much room for further innovation. Ultimately, RNNs cannot be ""replaced"" by non-recurrent architectures, and will thus perform superior on some tasks that demand explicitly recurrent features.Recurrent TransformersIf we can't do away with recurrence, can't we just incorporate it with transformers somehow? Yes: Universal Transformers. Not only is there recurrence, but variable input sequences are supported, just like in RNNs. Authors go so far as to argue that UTs are Turing complete; whether that's true I haven't verified, but even if it is, it doesn't warrant practical ability to fully harness this capability.Bonus: It helps to visualize RNNs to better understand and debug them; you can see their weights, gradients, and activations in action with See RNN, a package of mine (pretty pics included).Update 6/29/2020: new paper redesigns transformers to operate in time dimension with linear, O(N), complexity: Transformers are RNNs. Mind the title though; from section 3.4: ""we consider recurrence with respect to time and not depth"". So they are a kind of RNN, but still differ from 'traditional' ones. I've yet to read it, seems promising; a nice video explanation here."
How trustworthy are the confidence intervals for lmer objects through effects package?,"
Effects package provides a very fast and convenient way for plotting linear mixed effect model results obtained through lme4 package. The effect function calculates confidence intervals (CIs) very quickly, but how trustworthy are these confidence intervals?
For example:
library(lme4)
library(effects)
library(ggplot)

data(Pastes)

fm1  <- lmer(strength ~ batch + (1 | cask), Pastes)
effs <- as.data.frame(effect(c(""batch""), fm1))
ggplot(effs, aes(x = batch, y = fit, ymin = lower, ymax = upper)) + 
  geom_rect(xmax = Inf, xmin = -Inf, ymin = effs[effs$batch == ""A"", ""lower""],
        ymax = effs[effs$batch == ""A"", ""upper""], alpha = 0.5, fill = ""grey"") +
  geom_errorbar(width = 0.2) + geom_point() + theme_bw()


According to CIs calculated using effects package, batch ""E"" does not overlap with batch ""A"".
If I try the same using confint.merMod function and the default method:
a <- fixef(fm1)
b <- confint(fm1)
# Computing profile confidence intervals ...
# There were 26 warnings (use warnings() to see them)

b <- data.frame(b)
b <- b[-1:-2,]

b1 <- b[[1]]
b2 <- b[[2]]

dt <- data.frame(fit   = c(a[1],  a[1] + a[2:length(a)]), 
                 lower = c(b1[1],  b1[1] + b1[2:length(b1)]), 
                 upper = c(b2[1],  b2[1] + b2[2:length(b2)]) )
dt$batch <- LETTERS[1:nrow(dt)]

ggplot(dt, aes(x = batch, y = fit, ymin = lower, ymax = upper)) +
  geom_rect(xmax = Inf, xmin = -Inf, ymin = dt[dt$batch == ""A"", ""lower""], 
        ymax = dt[dt$batch == ""A"", ""upper""], alpha = 0.5, fill = ""grey"") + 
  geom_errorbar(width = 0.2) + geom_point() + theme_bw()


I see that all of the CIs overlap. I also get warnings indicating that the function failed to calculate trustworthy CIs. This example, and my actual dataset, makes me to suspect that effects package takes shortcuts in CI calculation that might not entirely be approved by statisticians. How trustworthy are the CIs returned by effect function from effects package for lmer objects?
What have I tried: Looking into the source code, I noticed that effect function relies on Effect.merMod function, which in turn directs to Effect.mer function, which looks like this:  
effects:::Effect.mer
function (focal.predictors, mod, ...) 
{
    result <- Effect(focal.predictors, mer.to.glm(mod), ...)
    result$formula <- as.formula(formula(mod))
    result
}
<environment: namespace:effects>

mer.to.glm function seems to calculate Variance-Covariate Matrix from the lmerobject: 
effects:::mer.to.glm

function (mod) 
{
...
mod2$vcov <- as.matrix(vcov(mod))
...
mod2
}

This, in turn, is probably used in Effect.default function to calculate CIs (I might have misunderstood this part):
effects:::Effect.default
...
     z <- qnorm(1 - (1 - confidence.level)/2)
        V <- vcov.(mod)
        eff.vcov <- mod.matrix %*% V %*% t(mod.matrix)
        rownames(eff.vcov) <- colnames(eff.vcov) <- NULL
        var <- diag(eff.vcov)
        result$vcov <- eff.vcov
        result$se <- sqrt(var)
        result$lower <- effect - z * result$se
        result$upper <- effect + z * result$se
...

I do not know enough about LMMs to judge whether this is a right approach, but considering the discussion around confidence interval calculation for LMMs, this approach appears suspiciously simple.
","['r', 'mixed-model', 'confidence-interval', 'effects', 'lme4-nlme']","All of the results are essentially the same (for this particular example).  Some theoretical differences are:I think all of these approaches are reasonable (some are more approximate than others), but in this case it barely makes any difference which one you use.  If you're concerned, try out several contrasting methods on your data, or on simulated data that resemble your own, and see what happens ...(PS: I wouldn't put too much weight on the fact that the confidence intervals of A and E don't overlap.  You'd have to do a proper pairwise comparison procedure to make reliable inferences about the differences between this particular pair of estimates ...)95% CIs:Comparison code:"
Calculating confidence intervals for a logistic regression,"
I'm using a binomial logistic regression to identify if exposure to has_x or has_y impacts the likelihood that a user will click on something. My model is the following:
fit = glm(formula = has_clicked ~ has_x + has_y, 
          data=df, 
          family = binomial())

This the output from my model:
Call:
glm(formula = has_clicked ~ has_x + has_y, 
    family = binomial(), data = active_domains)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9869  -0.9719  -0.9500   1.3979   1.4233  

Coefficients:
                      Estimate Std. Error z value Pr(>|z|)    
(Intercept)          -0.504737   0.008847 -57.050  < 2e-16 ***
has_xTRUE -0.056986   0.010201  -5.586 2.32e-08 ***
has_yTRUE  0.038579   0.010202   3.781 0.000156 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 217119  on 164182  degrees of freedom
Residual deviance: 217074  on 164180  degrees of freedom
AIC: 217080

Number of Fisher Scoring iterations: 4

As each coefficient is significant, using this model I'm able to tell what the value of any of these combinations is using the following approach:
predict(fit, data.frame(has_x = T, has_y=T), type = ""response"")

I don't understand how I can report on the Std. Error of the prediction. 

Do I just need to use $1.96*SE$? Or do I need to convert the
$SE$ using an approach described here? 
If I want to understand the standard-error for both variables
how would I consider that?

Unlike this question, I am interested in understanding what the upper and lower bounds of the error are in a percentage. For example, of my prediction shows a value of 37% for True,True can I calculate that this is $+/- 0.3%$ for a $95\% CI$? (0.3% chosen to illustrate my point)
","['regression', 'logistic', 'standard-error', 'logit']","Your question may come from the fact that you are dealing with Odds Ratios and Probabilities which is confusing at first. Since the logistic model is a non linear transformation of $\beta^Tx$ computing the confidence intervals is not as straightforward.Recall that for the Logistic regression modelProbability of $(Y = 1)$: $p = \frac{e^{\alpha + \beta_1x_1 + \beta_2 
x_2}}{1 + e^{ \alpha + \beta_1x_1 + \beta_2 x_2}}$Odds of $(Y = 1)$: $ \left( \frac{p}{1-p}\right) = e^{\alpha + \beta_1x_1 + \beta_2 
x_2}$Log Odds of $(Y = 1)$: $ \log \left( \frac{p}{1-p}\right) = \alpha + \beta_1x_1 + \beta_2 
x_2$Consider the case where you have a one unit increase in variable $x_1$, i.e. $x_1 + 1$, then the new odds are$$ \text{Odds}(Y = 1) = e^{\alpha + \beta_1(x_1 + 1) + \beta_2x_2}  = e^{\alpha + \beta_1 x_1 + \beta_1 + \beta_2x_2 } $$$$ \frac{\text{Odds}(x_1 + 1)}{\text{Odds}(x_1)} = \frac{e^{\alpha + \beta_1(x_1 + 1) + \beta_2x_2}  }{e^{\alpha + \beta_1 x_1 + \beta_2x_2}} = e^{\beta_1}  $$Log Odds Ratio =  $\beta_1$Relative risk or (probability ratio) = $\frac{ \frac{e^{\alpha + \beta_1x_1 + \beta_1 + \beta_2 
x_2}}{1 + e^{ \alpha + \beta_1x_1 + \beta_1 + \beta_2 x_2}}}{ \frac{e^{\alpha + \beta_1x_1 + \beta_2 
x_2}}{1 + e^{ \alpha + \beta_1x_1 + \beta_2 x_2}}}$How would you interpret the coefficient value $\beta_j$ ? Assuming that everything else remains fixed:Do I just need to use $1.96∗SE$? Or do I need to convert the SE using an approach described here?Since the parameter $\beta_j$ is estimated using Maxiumum Likelihood Estimation, MLE theory tells us that it is asymptotically normal and hence we can use the large sample Wald confidence interval to get the usual$$ \beta_j \pm z^* SE(\beta_j)$$Which gives a confidence interval on the log-odds ratio. Using the invariance property of the MLE allows us to exponentiate to get
$$ e^{\beta_j \pm z^* SE(\beta_j)}$$which is a confidence interval on the odds ratio. Note that these intervals are for a single parameter only.If I want to understand the standard-error for both variables how would I consider that?If you include several parameters you can use the Bonferroni procedure, otherwise for all parameters you can use the confidence interval for probability estimatesIf $g$ parameters are to be estimated with family confidence coefficient of approximately $1 - \alpha$, the joint Bonferroni confidence limits are$$ \beta_g \pm z_{(1 - \frac{\alpha}{2g})}SE(\beta_g)$$The logistic model outputs an estimation of the probability of observing a one and we aim to construct a frequentist interval around the true probability $p$ such that $Pr(p_{L} \leq p \leq p_{U}) = .95$One approach called endpoint transformation does the following:Since $Pr(x^T\beta) = F(x^T\beta)$ is a monotonic transformation of $x^T\beta$$$ [Pr(x^T\beta)_L \leq Pr(x^T\beta) \leq Pr(x^T\beta)_U] = [F(x^T\beta)_L \leq F(x^T\beta) \leq F(x^T\beta)_U] $$Concretely this means computing $\beta^Tx \pm z^* SE(\beta^Tx)$ and then applying the logit transform to the result to get the lower and upper bounds:$$[\frac{e^{x^T\beta - z^* SE(x^T\beta)}}{1 + e^{x^T\beta - z^* SE(x^T\beta)}}, \frac{e^{x^T\beta + z^* SE(x^T\beta)}}{1 + e^{x^T\beta + z^* SE(x^T\beta)}},] $$The estimated approximate variance of $x^T\beta$ can be calculated using the covariance matrix of the regression coefficients using$$ Var(x^T\beta) = x^T \Sigma x$$The advantage of this method is that the bounds cannot be outside the range $(0,1)$There are several other approaches as well, using the delta method, bootstrapping etc.. which each have their own assumptions, advantages and limits.My favorite book on this topic is ""Applied Linear Statistical Models"" by Kutner, Neter, Li, Chapter 14Otherwise here are a few online sources:"
What is the significance of logistic regression coefficients?,"
I am currently reading a paper concerning voting location and voting preference in the 2000 and 2004 election. In it, there is a chart which displays logistic regression coefficients.  From courses years back and a little reading up, I understand logistic regression to be a way of describing the relationship between multiple independent variables and a binary response variable. What I'm confused about is, given the table below, because the South has a logistic regression coefficient of .903, does that mean that 90.3% of Southerners vote republican? Because of the logistical nature of the metric, that this direct correlation does not exist. Instead, I assume that you can only say that the south, with .903, votes Republican more than the Mountains/plains, with the regression of .506.  Given the latter to be the case, how do I know what is significant and what is not and is it possible to extrapolate a percentage of republican votes given this logistic regression coefficient.

As a side note, please edit my post if anything is stated incorrectly
","['regression', 'logistic', 'interpretation']","That the author has forced someone as thoughtful as you to have ask a question like this is compelling illustration of why the practice -- still way too common -- of confining reporting of regression model results to a table like this is so unacceptable. You can, as pointed out, try to transform the logit coefficient into some meaningful indication of the effect being estimated for the predictor in question but that's cumbersome and doesn't convey information about the precision of the prediction, which is usually pretty important in a logistic regression model (on voting in particular). Also, the use of multiple asterisks to report ""levels"" of significance reinforces the misconception that p-values are some meaningful index of effect size (""wow--that one has 3 asterisks!!""); for crying out loud, w/ N's of 10,000 to 20,000, completely trivial differences will be ""significant"" at p < .001 blah blah.There is absolutely no need to mystify in this way. The logistic regression model is an equation that can be used (through determinate calculation or better still simulation) to predict probability of an outcome conditional on specified values for predictors, subject to measurement error. So the researcher should report what the impact of predictors of interest are on the probability of the outcome variable of interest, & associated CI, as measured in units the practical importance of which can readily be grasped.  To assure ready grasping, the results should be graphically displayed. Here, for example, the researcher could report that being a rural as opposed to an urban voter increases the likelihood of voting Republican, all else equal, by X pct points (I'm guessing around 17 in 2000; ""divide by 4"" is a reasonable heuristic) +/- x% at 0.95 level of confidence-- if that's something that is useful to know. The reporting of pseudo R^2 is also a sign that the modeler is engaged in statistical ritual rather than any attempt to illuminate. There are scores of ways to compute ""pseudo R^2""; one might complain that the one used here is not specified, but why bother? All are  next to meaningless.  The only reason anyone uses pseudo R^2 is that they or the reviewer who is torturing them learned (likely 25 or more yrs ago) that OLS linear regression is the holy  grail of statistics & thinks the only thing one is ever trying to figure out is ""variance explained."" There are plenty of defensible ways to assess the adequacy of overall model fit for logistic analysis, and likelihood ratio conveys meaningful information for comparing models that reflect alternative hypotheses. King, G. How Not to Lie with Statistics. Am. J. Pol. Sci. 30, 666-687 (1986).If you read a paper in which reporting is more or less confined to a table like this don't be confused, don't be intimidated, & definitely don't be impressed; instead be angry & tell the researcher he or she is doing a lousy job (particularly if he or she is polluting your local intellectual environment w/ mysticism & awe--amazing how many completely mediocre thinkers trick smart people into thinking they know something just b/c they can produce a table that the latter can't understand). For smart, & temperate, expositions of these ideas, see King, G., Tomz, M. & Wittenberg., J. Making the Most of Statistical Analyses: Improving Interpretation and Presentation. Am. J. Pol. Sci. 44, 347-361 (2000); and Gelman, A., Pasarica, C. & Dodhia, R. Let's Practice What We Preach: Turning Tables into Graphs. Am. Stat. 56, 121-130 (2002)."
What is the purpose of characteristic functions?,"
I'm hoping that someone can explain, in layman's terms, what a characteristic function is and how it is used in practice. I've read that it is the Fourier transform of the pdf, so I guess I know what it is, but I still don't understand its purpose. If someone could provide an intuitive description of its purpose and perhaps an example of how it is typically used, that would be fantastic!
Just one last note: I have seen the Wikipedia page, but am apparently too dense to understand what is going on. What I'm looking for is an explanation that someone not immersed in the wonders of probability theory, say a computer scientist, could understand.
","['probability', 'mathematical-statistics', 'characteristic-function']","Back in the day, people used logarithm tables to multiply numbers faster.  Why is this?  Logarithms convert multiplication to addition, since $\log(ab) = \log(a) + \log(b)$.  So in order to multiply two large numbers $a$ and $b$, you found their logarithms, added the logarithms, $z = \log(a) + \log(b)$, and then looked up $\exp(z)$ on another table.Now, characteristic functions do a similar thing for probability distributions.  Suppose $X$ has a distribution $f$ and $Y$ has a distribution $g$, and $X$ and $Y$ are independent.  Then the distribution of $X+Y$ is the convolution of $f$ and $g$, $f * g$.Now the characteristic function is an analogy of the ""logarithm table trick"" for convolution, since if $\phi_f$ is the characteristic function of $f$, then the following relation holds:$$
\phi_f \phi_g = \phi_{f * g}
$$Furthermore, also like in the case of logarithms,it is easy to find the inverse of the characteristic function: given $\phi_h$ where $h$ is an unknown density, we can obtain $h$ by the inverse Fourier transform of $\phi_h$.The characteristic function converts convolution to  multiplication for density functions the same way that logarithms convert multiplication into addition for numbers. Both transformations convert a relatively complicated operation into a relatively simple one."
10-fold Cross-validation vs leave-one-out cross-validation,"
I'm doing nested cross-validation. I have read that leave-one-out cross-validation can be biased (don't remember why).
Is it better to use 10-fold cross-validation or leave-one-out cross-validation apart from the longer runtime for leave-one-out cross-validation?
","['machine-learning', 'cross-validation']","Just to add slightly to the answer of @SubravetiSuraj (+1)Cross-validation gives a pessimistically biased estimate of performance because most statistical models will improve if the training set is made larger.  This means that k-fold cross-validation estimates the performance of a model trained on a dataset $100\times\frac{(k-1)}{k}\%$ of the available data, rather than on 100% of it.  So if you perform cross-validation to estimate performance, and then use a model trained on all of the data for operational use, it will perform slightly better than the cross-validation estimate suggests.Leave-one-out cross-validation is approximately unbiased, because the difference in size between the training set used in each fold and the entire dataset is only a single pattern.  There is a paper on this by Luntz and Brailovsky (in Russian).Luntz, Aleksandr, and Viktor Brailovsky. ""On estimation of characters obtained in statistical procedure of recognition."" Technicheskaya Kibernetica 3.6 (1969): 6–12.see alsoLachenbruch,Peter A.,  and Mickey, M. Ray. ""Estimation of Error Rates in Discriminant Analysis."" Technometrics 10.1 (1968): 1–11.However, while leave-one-out cross-validation is approximately unbiased, it tends to have a high variance (so you would get very different estimates if you repeated the estimate with different initial samples of data from the same distribution).  As the error of the estimator is a combination of bias and variance, whether leave-one-out cross-validation is better than 10-fold cross-validation depends on both quantities.Now the variance in fitting the model tends to be higher if it is fitted to a small dataset (as it is more sensitive to any noise/sampling artifacts in the particular training sample used).  This means that 10-fold cross-validation is likely to have a high variance (as well as a higher bias) if you only have a limited amount of data, as the size of the training set will be smaller than for LOOCV.  So k-fold cross-validation can have variance issues as well, but for a different reason.  This is why LOOCV is often better when the size of the dataset is small.However, the main reason for using LOOCV in my opinion is that it is computationally inexpensive for some models (such as linear regression, most kernel methods, nearest-neighbour classifiers, etc.), and unless the dataset were very small, I would use 10-fold cross-validation if it fitted in my computational budget, or better still, bootstrap estimation and bagging."
Why is a likelihood-ratio test distributed chi-squared?,"
Why is the test statistic of a likelihood ratio test distributed chi-squared?
$2(\ln \text{ L}_{\rm alt\ model} - \ln \text{ L}_{\rm null\ model} ) \sim \chi^{2}_{df_{\rm alt}-df_{\rm null}}$
","['distributions', 'chi-squared-test', 'likelihood-ratio']","As mentioned by @Nick this is a consequence of Wilks' theorem. But note that the test statistic is asymptotically $\chi^2$-distributed, not $\chi^2$-distributed.I am very impressed by this theorem because it holds in a very wide context. Consider a statistical model with likelihood $l(\theta \mid y)$ where $y$ is the vector observations of  $n$ independent replicated observations from a distribution with parameter $\theta$ belonging to a submanifold $B_1$ of $\mathbb{R}^d$ with dimension $\dim(B_1)=s$. Let $B_0 \subset B_1$ be a submanifold with dimension $\dim(B_0)=m$. Imagine you are interested in testing $H_0\colon\{\theta \in B_0\}$. The likelihood ratio is 
$$lr(y) = \frac{\sup_{\theta \in B_1}l(\theta \mid y)}{\sup_{\theta \in B_0}l(\theta \mid y)}. $$
Define the deviance $d(y)=2 \log \big(lr(y)\big)$. Then Wilks' theorem says that, under usual regularity assumptions, $d(y)$ is asymptotically $\chi^2$-distributed with $s-m$ degrees of freedom when $H_0$ holds true. It is proven in Wilk's original paper mentioned by @Nick. I think this paper is not easy to read. Wilks published a book later, perhaps with an easiest presentation of his theorem. A short heuristic proof is given in Williams' excellent book."
"What is elastic net regularization, and how does it solve the drawbacks of Ridge ($L^2$) and Lasso ($L^1$)?","
Is elastic net regularization always preferred to Lasso & Ridge since it seems to solve the drawbacks of these methods? What is the intuition and what is the math behind elastic net?
","['regression', 'lasso', 'regularization', 'ridge-regression', 'elastic-net']",
Manually calculated $R^2$ doesn't match up with randomForest() $R^2$ for testing new data,"
I know this is a fairly specific R question, but I may be thinking about proportion variance explained, $R^2$, incorrectly. Here goes.
I'm trying to use the R package randomForest. I have some training data and testing data. When I fit a random forest model, the randomForest function allows you to input new testing data to test. It then tells you the percentage of variance explained in this new data. When I look at this, I get one number.
When I use the predict() function to predict the outcome value of the testing data based on the model fit from the training data, and I take the squared correlation coefficient between these values and the actual outcome values for the testing data, I get a different number. These values don't match up. 
Here's some R code to demonstrate the problem.
# use the built in iris data
data(iris)

#load the randomForest library
library(randomForest)

# split the data into training and testing sets
index <- 1:nrow(iris)
trainindex <- sample(index, trunc(length(index)/2))
trainset <- iris[trainindex, ]
testset <- iris[-trainindex, ]

# fit a model to the training set (column 1, Sepal.Length, will be the outcome)
set.seed(42)
model <- randomForest(x=trainset[ ,-1],y=trainset[ ,1])

# predict values for the testing set (the first column is the outcome, leave it out)
predicted <- predict(model, testset[ ,-1])

# what's the squared correlation coefficient between predicted and actual values?
cor(predicted, testset[, 1])^2

# now, refit the model using built-in x.test and y.test
set.seed(42)
randomForest(x=trainset[ ,-1], y=trainset[ ,1], xtest=testset[ ,-1], ytest=testset[ ,1])

","['r', 'correlation', 'predictive-models', 'random-forest', 'r-squared']","The reason that the $R^2$ values are not matching is because randomForest is reporting variation explained as opposed to variance explained. I think this is a common misunderstanding about $R^2$ that is perpetuated in textbooks. I even mentioned this on another thread the other day. If you want an example, see the (otherwise quite good) textbook Seber and Lee, Linear Regression Analysis, 2nd. ed.A general definition for $R^2$ is
$$
R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2} .
$$That is, we compute the mean-squared error, divide it by the variance of the original observations and then subtract this from one. (Note that if your predictions are really bad, this value can go negative.)Now, what happens with linear regression (with an intercept term!) is that the average value of the $\hat{y}_i$'s matches $\bar{y}$. Furthermore, the residual vector $y - \hat{y}$ is orthogonal to the vector of fitted values $\hat{y}$. When you put these two things together, then the definition reduces to the one that is more commonly encountered, i.e.,
$$
R^2_{\mathrm{LR}} = \mathrm{Corr}(y,\hat{y})^2 .
$$
(I've used the subscripts $\mathrm{LR}$ in $R^2_{\mathrm{LR}}$ to indicate linear regression.)The randomForest call is using the first definition, so if you doyou'll see that the answers match."
Understanding the parameters inside the Negative Binomial Distribution,"
I was trying to fit my data into various models and figured out that the fitdistr function from library MASS of R gives me Negative Binomial as the best-fit. Now from the wiki page, the definition is given as:

NegBin(r,p) distribution describes the probability of k failures and r
  successes in k+r Bernoulli(p) trials
  with success on the last trial.

Using R to perform model fitting gives me two parameters mean and dispersion parameter. I am not understanding how to interpret these because I cannot see these parameters on the wiki page. All I can see is the following formula:

where k is the number of observations and r=0...n. Now how do I relate these with the parameters given by R? The help file does not provide much information either. 
Also, just to say a few words about my experiment: In a social experiment that I was conducting, I was trying to count the number of people each user contacted in a period of 10 days. The population size was 100 for the experiment. 
Now, if the model fits the Negative Binomial, I can blindly say that it follows that distribution but I really want to understand the intuitive meaning behind this. What does it mean to say that the number of people contacted by my test subjects follows a negative binomial distribution? Can someone please help clarify this?
","['r', 'distributions', 'modeling', 'negative-binomial-distribution']","You should look further down the Wikipedia article on the NB, where it says ""gamma-Poisson mixture"". While the definition you cite (which I call the ""coin-flipping"" definition since I usually define it for classes as ""suppose you want to flip a coin until you get $k$ heads"") is easier to derive and makes more sense in an introductory probability or mathematical statistics context, the gamma-Poisson mixture is (in my experience) a much more generally useful way to think about the distribution in applied contexts. (In particular, this definition allows non-integer values of the dispersion/size parameter.) In this context, your dispersion parameter describes the distribution of a hypothetical Gamma distribution that underlies your data and describes unobserved variation among individuals in their intrinsic level of contact. In particular, it is the shape parameter of the Gamma, and it may be helpful in thinking about this to know that the coefficient of variation of a Gamma distribution with shape parameter $\theta$ is $1/\sqrt{\theta}$; as $\theta$ becomes large the latent variability disappears and the distribution approaches the Poisson."
Measures of variable importance in random forests,"
I've been playing around with random forests for regression and am having difficulty working out exactly what the two measures of importance mean, and how they should be interpreted.
The importance() function gives two values for each variable: %IncMSE and IncNodePurity.
Is there simple interpretations for these 2 values?
For IncNodePurity in particular, is this simply the amount the RSS increase following the removal of that variable?
","['r', 'machine-learning', 'random-forest', 'importance']","The first one can be 'interpreted' as follows: if a predictor is important in your current model, then assigning other values for that predictor randomly but 'realistically' (i.e.: permuting this predictor's values over your dataset), should have a negative influence on prediction, i.e.: using the same model to predict from data that is the same except for the one variable, should give worse predictions.So, you take a predictive measure (MSE) with the original dataset and then with the 'permuted' dataset, and you compare them somehow. One way, particularly since we expect the original MSE to always be smaller, the difference can be taken. Finally, for making the values comparable over variables, these are scaled.For the second one: at each split, you can calculate how much this split reduces node impurity (for regression trees, indeed, the difference between RSS before and after the split). This is summed over all splits for that variable, over all trees.Note: a good read is Elements of Statistical Learning by Hastie, Tibshirani and Friedman..."
Standard error clustering in R (either manually or in plm),"
I am trying to understand standard error ""clustering"" and how to execute in R (it is trivial in Stata). In R I have been unsuccessful using either plm or writing my own function. I'll use the diamonds data from the ggplot2 package.
I can do fixed effects with either dummy variables
> library(plyr)
> library(ggplot2)
> library(lmtest)
> library(sandwich)
> # with dummies to create fixed effects
> fe.lsdv <- lm(price ~ carat + factor(cut) + 0, data = diamonds)
> ct.lsdv <- coeftest(fe.lsdv, vcov. = vcovHC)
> ct.lsdv

t test of coefficients:

                      Estimate Std. Error  t value  Pr(>|t|)    
carat                 7871.082     24.892  316.207 < 2.2e-16 ***
factor(cut)Fair      -3875.470     51.190  -75.707 < 2.2e-16 ***
factor(cut)Good      -2755.138     26.570 -103.692 < 2.2e-16 ***
factor(cut)Very Good -2365.334     20.548 -115.111 < 2.2e-16 ***
factor(cut)Premium   -2436.393     21.172 -115.075 < 2.2e-16 ***
factor(cut)Ideal     -2074.546     16.092 -128.920 < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

or by de-meaning both left- and right-hand sides (no time invariant regressors here) and correcting degrees of freedom.
> # by demeaning with degrees of freedom correction
> diamonds <- ddply(diamonds, .(cut), transform, price.dm = price - mean(price), carat.dm = carat  .... [TRUNCATED] 
> fe.dm <- lm(price.dm ~ carat.dm + 0, data = diamonds)
> ct.dm <- coeftest(fe.dm, vcov. = vcovHC, df = nrow(diamonds) - 1 - 5)
> ct.dm

t test of coefficients:

         Estimate Std. Error t value  Pr(>|t|)    
carat.dm 7871.082     24.888  316.26 < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

I can't replicate these results with plm, because I don't have a ""time"" index (i.e., this isn't really a panel, just clusters that could have a common bias in their error terms).
> plm.temp <- plm(price ~ carat, data = diamonds, index = ""cut"")
duplicate couples (time-id)
Error in pdim.default(index[[1]], index[[2]]) : 

I also tried to code my own covariance matrix with clustered standard error using Stata's explanation of their cluster option (explained here), which is to solve $$\hat V_{cluster} = (X'X)^{-1} \left( \sum_{j=1}^{n_c} u_j'u_j \right) (X'X)^{-1}$$ where $u_j = \sum_{cluster~j} e_i * x_i$, $n_c$ si the number of clusters, $e_i$ is the residual for the $i^{th}$ observation and $x_i$ is the row vector of predictors, including the constant (this also appears as equation (7.22) in Wooldridge's Cross Section and Panel Data). But the following code gives very large covariance matrices. Are these very large values given the small number of clusters I have? Given that I can't get plm to do clusters on one factor, I'm not sure how to benchmark my code.
> # with cluster robust se
> lm.temp <- lm(price ~ carat + factor(cut) + 0, data = diamonds)
> 
> # using the model that Stata uses
> stata.clustering <- function(x, clu, res) {
+     x <- as.matrix(x)
+     clu <- as.vector(clu)
+     res <- as.vector(res)
+     fac <- unique(clu)
+     num.fac <- length(fac)
+     num.reg <- ncol(x)
+     u <- matrix(NA, nrow = num.fac, ncol = num.reg)
+     meat <- matrix(NA, nrow = num.reg, ncol = num.reg)
+     
+     # outer terms (X'X)^-1
+     outer <- solve(t(x) %*% x)
+ 
+     # inner term sum_j u_j'u_j where u_j = sum_i e_i * x_i
+     for (i in seq(num.fac)) {
+         index.loop <- clu == fac[i]
+         res.loop <- res[index.loop]
+         x.loop <- x[clu == fac[i], ]
+         u[i, ] <- as.vector(colSums(res.loop * x.loop))
+     }
+     inner <- t(u) %*% u
+ 
+     # 
+     V <- outer %*% inner %*% outer
+     return(V)
+ }
> x.temp <- data.frame(const = 1, diamonds[, ""carat""])
> summary(lm.temp)

Call:
lm(formula = price ~ carat + factor(cut) + 0, data = diamonds)

Residuals:
     Min       1Q   Median       3Q      Max 
-17540.7   -791.6    -37.6    522.1  12721.4 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
carat                 7871.08      13.98   563.0   <2e-16 ***
factor(cut)Fair      -3875.47      40.41   -95.9   <2e-16 ***
factor(cut)Good      -2755.14      24.63  -111.9   <2e-16 ***
factor(cut)Very Good -2365.33      17.78  -133.0   <2e-16 ***
factor(cut)Premium   -2436.39      17.92  -136.0   <2e-16 ***
factor(cut)Ideal     -2074.55      14.23  -145.8   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 1511 on 53934 degrees of freedom
Multiple R-squared: 0.9272, Adjusted R-squared: 0.9272 
F-statistic: 1.145e+05 on 6 and 53934 DF,  p-value: < 2.2e-16 

> stata.clustering(x = x.temp, clu = diamonds$cut, res = lm.temp$residuals)
                        const diamonds....carat..
const                11352.64           -14227.44
diamonds....carat.. -14227.44            17830.22

Can this be done in R? It is a fairly common technique in econometrics (there's a brief tutorial in this lecture), but I can't figure it out in R. Thanks!
","['r', 'panel-data', 'standard-error', 'fixed-effects-model', 'clustered-standard-errors']","Edit as of December 2021:Probably the easiest way to get clustered standard errors in R now is via the the feols function in the fixest package or felm function in the lfe package:feols in fixest: Clustering syntax and standard error computational procedurefelm in lfe: CRAN documentationOriginal answers and some subsequent edits:For White standard errors clustered by group with the plm framework trywhere model.plm is a plm model.See also this linkhttp://www.inside-r.org/packages/cran/plm/docs/vcovHC or the plm package documentationEDIT:For two-way clustering (e.g. group and time) see the following link:http://people.su.se/~ma/clustering.pdfHere is another helpful guide for the plm package specifically that explains different options for clustered standard errors:http://www.princeton.edu/~otorres/Panel101R.pdfClustering and other information, especially for Stata, can be found here:http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htmEDIT 2:Here are examples that compare R and stata: http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/Also, the multiwayvcov may be helpful. This post provides a helpful overview: http://rforpublichealth.blogspot.dk/2014/10/easy-clustered-standard-errors-in-r.htmlFrom the documentation:"
Who invented stochastic gradient descent?,"
I'm trying to understand the history of Gradient descent and Stochastic gradient descent. Gradient descent was invented in Cauchy in 1847.Méthode générale pour la résolution des systèmes d'équations simultanées. pp. 536–538 For more information about it see here.
Since then gradient descent methods kept developing and I'm not familiar with their history. In particular I'm interested in the invention of stochastic gradient descent. 
A reference that can be used in an academic paper in more than welcomed.
","['references', 'gradient-descent', 'history', 'stochastic-gradient-descent']","Stochastic Gradient Descent is preceded by Stochastic Approximation as first described by Robbins and Monro in their paper, A Stochastic Approximation Method. Kiefer and Wolfowitz subsequently published their paper, *Stochastic Estimation of the Maximum of a Regression Function* which is more recognizable to people familiar with the ML variant of Stochastic Approximation (i.e Stochastic Gradient Descent), as pointed out by Mark Stone in the comments. The 60's saw plenty of research along that vein -- Dvoretzky, Powell, Blum all published results that we take for granted today. It is a relatively minor leap to get from the Robbins and Monro method to the Kiefer Wolfowitz method, and merely a reframing of the problem to then get to Stochastic Gradient Descent (for regression problems). The above papers are widely cited as being the antecedents of Stochastic Gradient Descent, as mentioned in  this review paper by Nocedal, Bottou, and Curtis, which provides a brief historical perspective from a Machine Learning point of view.I believe that Kushner and Yin in their book Stochastic Approximation and Recursive Algorithms and Applications suggest that the notion had been used in control theory as far back as the 40's, but I don't recall if they had a citation for that or if it was anecdotal, nor do I have access to their book to confirm this.Herbert Robbins and Sutton Monro A Stochastic Approximation Method
The Annals of Mathematical Statistics, Vol. 22, No. 3. (Sep., 1951), pp. 400-407, DOI: 10.1214/aoms/1177729586J. Kiefer and J. Wolfowitz Stochastic Estimation of the Maximum of a Regression Function Ann. Math. Statist. Volume 23, Number 3 (1952), 462-466, DOI: 10.1214/aoms/1177729392Leon Bottou and Frank E. Curtis and Jorge Nocedal Optimization Methods for Large-Scale Machine Learning, Technical Report, arXiv:1606.04838"
Are mixed models useful as predictive models?,"
I am a bit confused about advantages of mixed models in regard to predictive modelling. Since predictive models are usually meant to predict values of previously unknown observations then it seems obvious to me that the only way a mixed model may be useful is through its ability to provide population-level predictions (that is without adding any random effects). However, the problem is that so far in my experience population-level predictions based on mixed models are significantly worse than predictions based on standard regression models with fixed effects only.
So what is the point of mixed models in regard to prediction problems?
EDIT.
The problem is the following: I fitted a mixed model (with both fixed and random effects) and standard linear model with fixed effects only. When I do cross-validation I get a following hierarchy of predictive accuracy: 1) mixed models when predicting using fixed and random effects (but this works of course only for observations with known levels of random effects variables, so this predictive approach seems not to be suitable for real predictive applications!); 2) standard linear model; 3) mixed model when using population-level predictions (so with random effects thrown out). Thus, the only difference between standard linear model and mixed model are somewhat different value of coefficients due to different estimation methods (i.e. there are the same effects/predictors in both models, but they have different associated coefficients).
So my confusion boils down to a question, why would I ever use a mixed model as a predictive model, since using mixed model to generate population-level predictions seems to be an inferior strategy in comparison to a standard linear model.
","['mixed-model', 'predictive-models']","It depends on the nature of the data, but in general I would expect the mixed model to outperform the fixed-effects only models. Let's take an example: modelling the relationship between sunshine and the height of wheat stalks. We have a number of measurements of individual stalks, but many of the stalks are measured at the same sites (which are similar in soil, water and other things that may affect height). Here are some possible models: 1) height ~ sunshine2) height ~ sunshine + site3) height ~ sunshine + (1|site)We want to use these models to predict the height of new wheat stalks given some estimate of the sunshine they will experience. I'm going to ignore the parameter penalty you would pay for having many sites in a fixed-effects only model, and just consider the relative predictive power of the models. The most relevant question here is whether these new data points you are trying to predict are from one of the sites you have measured; you say this is rare in the real world, but it does happen. If so, models #2 and #3 will outperform #1. They both use more relevant information (mean site effect) to make predictions. I would still expect model #3 to outperform #1 and #2, for the following reasons. (i) Model #3 vs #1: Model #1 will produce estimates that are biased in favour of overrepresented sites. If you have similar numbers of points from each site and a reasonably representative sample of sites, you should get similar results from both. (ii) Model #3 vs. #2: Why would model #3 be better that model #2 in this case? Because random effects take advantage of shrinkage - the site effects will be 'shrunk' towards zero. In other words, you will tend to find less extreme values for site effects when it is specified as a random effect than when it is specified as a fixed effect. This is useful and improves your predictive ability when the population means can reasonably be thought of as being drawn from a normal distribution (see Stein's Paradox in Statistics). If the population means are not expected to follow a normal distribution, this might be a problem, but it's usually a very reasonable assumption and the method is robust to small deviations. [Side note: by default, when fitting model #2, most software would use one of the sites as a reference and estimate coefficients for the other sites that represent their the deviation from the reference. So it may appear as though there is no way to calculate an overall 'population effect'. But you can calculate this by averaging across predictions for all of the individual sites, or more simply by changing the coding of the model so that coefficients are calculated for every site.]"
Time series 'clustering' in R,"
I have a set of time series data. Each series covers the same period, although the actual dates in each time series may not all 'line up' exactly.
That is to say, if the Time series were to be read into a 2D matrix, it would look something like this:
date     T1   T2   T3 .... TN
1/1/01   100  59   42      N/A
2/1/01   120  29   N/A     42.5
3/1/01   110  N/A  12      36.82
4/1/01   N/A  59   40      61.82
5/1/01    05  99   42      23.68
...
31/12/01  100  59   42     N/A

etc 

I want to write an R script that will segregate the time series {T1, T2, ... TN} into 'families' where a family is defined as a set of series which ""tend to move in sympathy"" with each other. 
For the 'clustering' part, I will need to select/define a kind of distance measure. I am not quite sure how to go about this, since I am dealing with time series, and a pair of series that may move in sympathy over one interval, may not do so in a subsequent interval.
I am sure there are far more experienced/clever people than me on here, so I would be grateful for any suggestions, ideas on what algorithm/heuristic to use for the distance measure and how to use that in clustering the time series. 
My guess is that there is NOT an established robust statistic method for doing this, so I would be very interested to see how people approach/solve this problem - thinking like a statistician.
","['r', 'time-series', 'clustering', 'cointegration']","In data streaming and mining of time series databases, a common approach is to transform the series to a symbolic representation, then use a similarity metric, such as Euclidean distance, to cluster the series.  The most popular representations are SAX (Keogh & Lin) or the newer iSAX (Shieh & Keogh):The pages above also contain references to distance metrics and clustering.  Keogh and crew are into reproducible research and pretty receptive to releasing their code.  So you could email them and ask.  I believe they tend to work in MATLAB/C++ though.  There was a recent effort to produce a Java and R implementation:I don't know how far along it is -- it's geared towards motif finding, but, depending on how far they've gotten, it should have the necessary bits you need to put something together for your needs (iSAX and distance metrics: since this part is common to clustering and motif finding)."
How to interpret glmnet?,"
I am trying to fit a multivariate linear regression model with approximately 60 predictor variables and 30 observations, so I am using the glmnet package for regularized regression because p>n.
I have been going through documentation and other questions but I still can't interpret the results, here's a sample code (with 20 predictors and 10 observations to simplify):
I create a matrix x with num rows = num observations and num cols = num predictors and a vector y which represents the response variable
> x=matrix(rnorm(10*20),10,20)
> y=rnorm(10)

I fit a glmnet model leaving alpha as default (= 1 for lasso penalty)
> fit1=glmnet(x,y)
> print(fit1)

I understand I get different predictions with decreasing values of lambda (i.e. penalty)
Call:  glmnet(x = x, y = y) 

        Df    %Dev   Lambda
  [1,]  0 0.00000 0.890700
  [2,]  1 0.06159 0.850200
  [3,]  1 0.11770 0.811500
  [4,]  1 0.16880 0.774600
   .
   .
   .
  [96,] 10 0.99740 0.010730
  [97,] 10 0.99760 0.010240
  [98,] 10 0.99780 0.009775
  [99,] 10 0.99800 0.009331
 [100,] 10 0.99820 0.008907

Now I predict my Beta values choosing, for example, the smallest lambda value given from glmnet
> predict(fit1,type=""coef"", s = 0.008907)

21 x 1 sparse Matrix of class ""dgCMatrix""
                  1
(Intercept) -0.08872364
V1           0.23734885
V2          -0.35472137
V3          -0.08088463
V4           .         
V5           .         
V6           .         
V7           0.31127123
V8           .         
V9           .         
V10          .         
V11          0.10636867
V12          .         
V13         -0.20328200
V14         -0.77717745
V15          .         
V16         -0.25924281
V17          .         
V18          .         
V19         -0.57989929
V20         -0.22522859

If instead I choose lambda with 
cv <- cv.glmnet(x,y)
model=glmnet(x,y,lambda=cv$lambda.min)

All of the variables would be (.).
Doubts and questions:

I am not sure about how to choose lambda.
Should I use the non (.) variables to fit another model? In my case I would like to keep as much variables as possible.
How do I know the p-value, i.e. which variables significantly predict the response?

I apologize for my poor statistical knowledge! And thank you for any help.
","['r', 'regression', 'regularization', 'glmnet']","Here's an unintuitive fact - you're not actually supposed to give glmnet a single value of lambda. From the documentation here: Do not supply a single value for lambda (for predictions after CV use predict() instead).
  Supply instead a decreasing sequence of lambda values. glmnet relies on its
  warms starts for speed, and its often faster to ﬁt a whole path than compute a
  single ﬁt.cv.glmnet will help you choose lambda, as you alluded to in your examples. The authors of the glmnet package suggest cv$lambda.1se instead of cv$lambda.min, but in practice I've had success with the latter.After running cv.glmnet, you don't have to rerun glmnet! Every lambda in the grid (cv$lambda) has already been run. This technique is called ""Warm Start"" and you can read more about it here. Paraphrasing from the introduction, the Warm Start technique reduces running time of iterative methods by using the solution of a different optimization problem (e.g., glmnet with a larger lambda) as the starting value for a later optimization problem (e.g., glmnet with a smaller lambda).To extract the desired run from cv.glmnet.fit, try this:Revision (1/28/2017)No need to hack to the glmnet object like I did above; take @alex23lemm's advice below and pass the s = ""lambda.min"", s = ""lambda.1se"" or some other number (e.g., s = .007) to both coef and predict. Note that your coefficients and predictions depend on this value which is set by cross validation. Use a seed for reproducibility! And don't forget that if you don't supply an ""s"" in coef and predict, you'll be using the default of s = ""lambda.1se"". I have warmed up to that default after seeing it work better in a small data situation. s = ""lambda.1se"" also tends to provide more regularization, so if you're working with alpha > 0, it will also tend towards a more parsimonious model. You can also choose a numerical value of s with the help of plot.glmnet to get to somewhere in between (just don't forget to exponentiate the values from the x axis!)."
How to quasi match two vectors of strings (in R)?,"
I am not sure how this should be termed, so please correct me if you know a better term.
I've got two lists.  One of 55 items (e.g: a vector of strings), the other of 92.  The item names are similar but not identical.
I wish to find the best candidates in the 92 list to the items in the 55 list (I will then go through it and pick the correct fitting).
How can it be done?
Ideas I had where to:

See all the ones that match (using something list ?match)
Try a distance matrix between the strings vectors, but I am not sure how to best define it (number of identical letters, what about order of strings?)

So what package/functions/field-of-research deals with such a task, and how?
Update: Here is an example of the vectors I wish to match
vec55 <- c(""Aeropyrum pernix"", ""Archaeoglobus fulgidus"", ""Candidatus_Korarchaeum_cryptofilum"", 
""Candidatus_Methanoregula_boonei_6A8"", ""Cenarchaeum_symbiosum"", 
""Desulfurococcus_kamchatkensis"", ""Ferroplasma acidarmanus"", ""Haloarcula_marismortui_ATCC_43049"", 
""Halobacterium sp."", ""Halobacterium_salinarum_R1"", ""Haloferax volcanii"", 
""Haloquadratum_walsbyi"", ""Hyperthermus_butylicus"", ""Ignicoccus_hospitalis_KIN4"", 
""Metallosphaera_sedula_DSM_5348"", ""Methanobacterium thermautotrophicus"", 
""Methanobrevibacter_smithii_ATCC_35061"", ""Methanococcoides_burtonii_DSM_6242""
)
vec91 <- c(""Acidilobus saccharovorans 345-15"", ""Aciduliprofundum boonei T469"", 
""Aeropyrum pernix K1"", ""Archaeoglobus fulgidus DSM 4304"", ""Archaeoglobus profundus DSM 5631"", 
""Caldivirga maquilingensis IC-167"", ""Candidatus Korarchaeum cryptofilum OPF8"", 
""Candidatus Methanoregula boonei 6A8"", ""Cenarchaeum symbiosum A"", 
""Desulfurococcus kamchatkensis 1221n"", ""Ferroglobus placidus DSM 10642"", 
""Halalkalicoccus jeotgali B3"", ""Haloarcula marismortui ATCC 43049"", 
""Halobacterium salinarum R1"", ""Halobacterium sp. NRC-1"", ""Haloferax volcanii DS2"", 
""Halomicrobium mukohataei DSM 12286"", ""Haloquadratum walsbyi DSM 16790"", 
""Halorhabdus utahensis DSM 12940"", ""Halorubrum lacusprofundi ATCC 49239"", 
""Haloterrigena turkmenica DSM 5511"", ""Hyperthermus butylicus DSM 5456"", 
""Ignicoccus hospitalis KIN4/I"", ""Ignisphaera aggregans DSM 17230"", 
""Metallosphaera sedula DSM 5348"", ""Methanobrevibacter ruminantium M1"", 
""Methanobrevibacter smithii ATCC 35061"", ""Methanocaldococcus fervens AG86"", 
""Methanocaldococcus infernus ME"", ""Methanocaldococcus jannaschii DSM 2661"", 
""Methanocaldococcus sp. FS406-22"", ""Methanocaldococcus vulcanius M7"", 
""Methanocella paludicola SANAE"", ""Methanococcoides burtonii DSM 6242"", 
""Methanococcus aeolicus Nankai-3"", ""Methanococcus maripaludis C5"", 
""Methanococcus maripaludis C6"", ""Methanococcus maripaludis C7"", 
""Methanococcus maripaludis S2"", ""Methanococcus vannielii SB"", 
""Methanococcus voltae A3"", ""Methanocorpusculum labreanum Z"", 
""Methanoculleus marisnigri JR1"", ""Methanohalobium evestigatum Z-7303"", 
""Methanohalophilus mahii DSM 5219"", ""Methanoplanus petrolearius DSM 11571"", 
""Methanopyrus kandleri AV19"", ""Methanosaeta thermophila PT"", 
""Methanosarcina acetivorans C2A"", ""Methanosarcina barkeri str. Fusaro"", 
""Methanosarcina mazei Go1"", ""Methanosphaera stadtmanae DSM 3091"", 
""Methanosphaerula palustris E1-9c"", ""Methanospirillum hungatei JF-1"", 
""Methanothermobacter marburgensis str. Marburg"", ""Methanothermobacter thermautotrophicus str. Delta H"", 
""Nanoarchaeum equitans Kin4-M"", ""Natrialba magadii ATCC 43099"", 
""Natronomonas pharaonis DSM 2160"", ""Nitrosopumilus maritimus SCM1"", 
""Picrophilus torridus DSM 9790"", ""Pyrobaculum aerophilum str. IM2"", 
""Pyrobaculum arsenaticum DSM 13514"", ""Pyrobaculum calidifontis JCM 11548"", 
""Pyrobaculum islandicum DSM 4184"", ""Pyrococcus abyssi GE5"", ""Pyrococcus furiosus DSM 3638"", 
""Pyrococcus horikoshii OT3"", ""Staphylothermus hellenicus DSM 12710"", 
""Staphylothermus marinus F1"", ""Sulfolobus acidocaldarius DSM 639"", 
""Sulfolobus islandicus L.D.8.5"", ""Sulfolobus islandicus L.S.2.15"", 
""Sulfolobus islandicus M.14.25"", ""Sulfolobus islandicus M.16.27"", 
""Sulfolobus islandicus M.16.4"", ""Sulfolobus islandicus Y.G.57.14"", 
""Sulfolobus islandicus Y.N.15.51"", ""Sulfolobus solfataricus P2"", 
""Sulfolobus tokodaii str. 7"", ""Thermococcus gammatolerans EJ3"", 
""Thermococcus kodakarensis KOD1"", ""Thermococcus onnurineus NA1"", 
""Thermococcus sibiricus MM 739"", ""Thermofilum pendens Hrk 5"", 
""Thermoplasma acidophilum DSM 1728"", ""Thermoplasma volcanium GSS1"", 
""Thermoproteus neutrophilus V24Sta"", ""Thermosphaera aggregans DSM 11486"", 
""Vulcanisaeta distributa DSM 14429"", ""uncultured methanogenic archaeon RC-I""
) 

","['r', 'text-mining']","I've had similar problems. (seen here: https://stackoverflow.com/questions/2231993/merging-two-data-frames-using-fuzzy-approximate-string-matching-in-r)Most of the recommendations that I received fell around: pmatch(), and agrep(), grep(), grepl() are three functions that if you take the time to look through will provide you with some insight into approximate string matching either by approximate string or approximate regex.Without seeing the strings, it's hard to provide you with hard example of how to match them. If you could provide us with some example data I'm sure we could come to a solution. Another option that I found works well is to flatten the strings, tolower(), looking at the first letter of each word within the string and then comparing. Sometimes that works without a hitch. Then there are more complicated things like the distances mentioned in other answers. Sometimes these work, sometimes they're horrible - it really depends on the strings. Can we see them?It looks like agrep() will do the trick for most of these. Note that agrep() is just R's implementation of Levenshtein distance. Some don't compute although, I'm not even sure if Ferroplasm acidaramus is the same as Ferroglobus placidus DSM 10642, for example: I think you may be a bit SOL for some of these and perhaps creating an index from scratch is the best bet. ie,. Create a table with id numbers for vec55, and then manually create a reference to the id's in vec55 in vec91. Painful, I know, but a lot of it can be done with agrep(). "
Improve classification with many categorical variables,"
I'm working on a dataset with 200,000+ samples and approximately 50 features per sample: 10 continuous variables and the other ~40 are categorical variables (countries, languages, scientific fields etc.). For these categorical variables, you have for example 150 different countries, 50 languages, 50 scientific fields etc...
So far my approach is:  

For each categorical variable with many possible value, take only the one having more than 10000 sample that takes this value. This reduces to 5-10 categories instead of 150.
Build dummy variable for each categorical one (if 10 countries then for each sample add a binary vector of size 10).
Feed a random forest classifier (cross-validate the parameters etc...) with this data.

Currently with this approach, I only manage to get 65% accuracy and I feel like more can be done. Especially I'm not satisfied with my 1) since I feel like I shouldn't arbitrarily remove the ""least relevant values"" according the the number of sample they have, because these less represented values could be more discriminative. On the other hand, my RAM can't afford adding 500 columns * 200000 rows to the data by keeping all possible values.
Would you have any suggestion to cope with this much categorical variables?
","['machine-learning', 'classification', 'categorical-data', 'random-forest', 'many-categories']","Random forests should be able to handle categorical values natively so look for a different implementation so you don't have to encode all of those features and use up all your memory.The problem with high cardinality categorical features is that it is easy to over fit with them. You may have enough data that this isn't an issue but watch out for it.I suggest looking into random forest based feature selection using either the method Breiman proposed or artificial contrasts. The artificial contrasts method (ACE) is interesting because it compares the importance of the feature to the importance of a shuffled version of itself which fights some of the high cardinality issues. There is a new paper ""Module Guided Random Forests"" which might be interesting if you had many more features as it uses a feature selection method that is aware of groups of highly correlated features.Another sometime used option is to tweak the algorithm so it uses the out of bag cases to do the final feature selection after fitting the splits on the in bag cases which sometimes helps fight overfitting.There is an almost complete ace implementation here and I have a more memory efficient/fast RF implementation that handles categorical variables natively here...the -evaloob option supports option 4 I'm working on adding support for ACE and a couple of other RF based feature selection methods but it isn't done yet."
How to interpret the output of the summary method for an lm object in R? [duplicate],"







This question already has answers here:
                                
                            




Interpretation of R's lm() output

                                (2 answers)
                            

Closed 10 years ago.



I am using sample algae data to understand data mining a bit more. I have used the following commands:
data(algae)
algae <- algae[-manyNAs(algae),]
clean.algae <-knnImputation(algae, k = 10)
lm.a1 <- lm(a1 ~ ., data = clean.algae[, 1:12])
summary(lm.a1)

Subsequently I received the results below. However I can not find any good documentation which explains what most of this means, especially Std. Error,t value and Pr. 
Can someone please be kind enough to shed some light please? Most importantly, which variables should I look at to ascertain on whether a model is giving me good prediction data?
Call:
lm(formula = a1 ~ ., data = clean.algae[, 1:12])

Residuals:
  Min      1Q  Median      3Q     Max 
  -37.679 -11.893  -2.567   7.410  62.190 

  Coefficients:
                Estimate Std. Error t value Pr(>|t|)   
  (Intercept)  42.942055  24.010879   1.788  0.07537 . 
  seasonspring  3.726978   4.137741   0.901  0.36892   
  seasonsummer  0.747597   4.020711   0.186  0.85270   
  seasonwinter  3.692955   3.865391   0.955  0.34065   
  sizemedium    3.263728   3.802051   0.858  0.39179   
  sizesmall     9.682140   4.179971   2.316  0.02166 * 
  speedlow      3.922084   4.706315   0.833  0.40573   
  speedmedium   0.246764   3.241874   0.076  0.93941   
  mxPH         -3.589118   2.703528  -1.328  0.18598   
  mnO2          1.052636   0.705018   1.493  0.13715   
  Cl           -0.040172   0.033661  -1.193  0.23426   
  NO3          -1.511235   0.551339  -2.741  0.00674 **
  NH4           0.001634   0.001003   1.628  0.10516   
  oPO4         -0.005435   0.039884  -0.136  0.89177   
  PO4          -0.052241   0.030755  -1.699  0.09109 . 
  Chla         -0.088022   0.079998  -1.100  0.27265   
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 17.65 on 182 degrees of freedom
  Multiple R-squared:  0.3731,    Adjusted R-squared:  0.3215 
  F-statistic: 7.223 on 15 and 182 DF,  p-value: 2.444e-12

","['r', 'regression', 'data-mining']","It sounds like you need a decent basic statistics text that covers at least basic location tests, simple regression and multiple regression.Std. Error,t value and Pr. Std. Error is the standard deviation of the sampling distribution
of the estimate of the coefficient under the standard regression
assumptions. Such standard deviations are called standard errors
of the corresponding quantity (the coefficient estimate in this
case).In the case of simple regression, it's usually denoted $s_{\hat
\beta}$, as here. Also see thisFor multiple regression, it's a little more complicated, but if you
don't know what these things are it's probably best to understand
them in the context of simple regression first.t value is the value of the t-statistic for testing whether the
corresponding regression coefficient is different from 0.The formula for computing it is given at the first link above.Most importantly, which variables should I look at to ascertain on whether a model is giving me good prediction data?What do you mean by 'good prediction data'? Can you make it clearer what you're asking?The Residual standard error, which is usually called $s$, represents the standard deviation of the residuals.  It's a measure of how close the fit is to the points.The Multiple R-squared, also called the coefficient of determination is the proportion of the variance in the data that's explained by the model. The more variables you add - even if they don't help - the larger this will be. The Adjusted one reduces that to account for the number of variables in the model.The $F$ statistic on the last line is telling you whether the regression as a whole is performing 'better than random' - any set of random predictors will have some relationship with the response, so it's seeing whether your model fits better than you'd expect if all your predictors had no relationship with the response (beyond what would be explained by that randomness). This is used for a test of whether the model outperforms 'noise' as a predictor. The p-value in the last row is the p-value for that test, essentially comparing the full model you fitted with an intercept-only model.Where do the data come from? Is this in some package?"
How does R handle missing values in lm?,"
I'd like to regress a vector B against each of the columns in a matrix A. This is trivial if there are no missing data, but if matrix A contains missing values, then my regression against A is constrained to include only rows where all values are present (the default na.omit behavior). This produces incorrect results for columns with no missing data. I can regress the column matrix B against individual columns of the matrix A, but I have thousands of regressions to do, and this is prohibitively slow and inelegant. The na.exclude function seems to be designed for this case, but I can't make it work. What am I doing wrong here? Using R 2.13 on OSX, if it matters.
A = matrix(1:20, nrow=10, ncol=2)
B = matrix(1:10, nrow=10, ncol=1)
dim(lm(A~B)$residuals)
# [1] 10 2 (the expected 10 residual values)

# Missing value in first column; now we have 9 residuals
A[1,1] = NA  
dim(lm(A~B)$residuals)
#[1]  9 2 (the expected 9 residuals, given na.omit() is the default)

# Call lm with na.exclude; still have 9 residuals
dim(lm(A~B, na.action=na.exclude)$residuals)
#[1]  9 2 (was hoping to get a 10x2 matrix with a missing value here)

A.ex = na.exclude(A)
dim(lm(A.ex~B)$residuals)
# Throws an error because dim(A.ex)==9,2
#Error in model.frame.default(formula = A.ex ~ B, drop.unused.levels = TRUE) : 
#  variable lengths differ (found for 'B')

","['r', 'missing-data', 'linear-model']","Edit: I misunderstood your question. There are two aspects:a) na.omit and na.exclude both do casewise deletion with respect to both predictors and criterions. They only differ in that extractor functions like residuals() or fitted() will pad their output with NAs for the omitted cases with na.exclude, thus having an output of the same length as the input variables.b) The real issue is not with this difference between na.omit and na.exclude, you don't seem to want casewise deletion that takes criterion variables into account, which both do.The regression results depend on the matrices $X^{+} = (X' X)^{-1} X'$ (pseudoinverse of design matrix $X$, coefficients $\hat{\beta} = X^{+} Y$) and the hat matrix $H = X X^{+}$, fitted values $\hat{Y} = H Y$). If you don't want casewise deletion, you need a different design matrix $X$ for each column of $Y$, so there's no way around fitting separate regressions for each criterion. You can try to avoid the overhead of lm() by doing something along the lines of the following:Note that there might be numerically better ways to caculate $X^{+}$ and $H$, you could check a $QR$-decomposition instead. The SVD-approach is explained here on SE. I have not timed the above approach with big matrices $Y$ against actually using lm()."
Training loss goes down and up again. What is happening?,"
My training loss goes down and then up again. It is very weird. The cross-validation loss tracks the training loss. What is going on? 
I have two stacked LSTMS as follows (on Keras):
model = Sequential()
model.add(LSTM(512, return_sequences=True, input_shape=(len(X[0]), len(nd.char_indices))))
model.add(Dropout(0.2))
model.add(LSTM(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(nd.categories)))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adadelta')

I train it for a 100 Epochs:
model.fit(X_train, np.array(y_train), batch_size=1024, nb_epoch=100, validation_split=0.2)


Train on 127803 samples, validate on 31951 samples

And that is what the loss looks like:

","['machine-learning', 'neural-networks', 'loss-functions', 'lstm']","Your learning rate could be to big after the 25th epoch. This problem is easy to identify. You just need to set up a smaller value for your learning rate. If the problem related to your learning rate than NN should reach a lower error despite that it will go up again after a while. The main point is that the error rate will be lower in some point in time.If you observed this behaviour you could use two simple solutions. First one is a simplest one. Set up a very small step and train it. The second one is to decrease your learning rate monotonically. Here is a simple formula:$$
\alpha(t + 1) = \frac{\alpha(0)}{1 + \frac{t}{m}}
$$Where $a$ is your learning rate, $t$ is your iteration number and $m$ is a coefficient that identifies learning rate decreasing speed. It means that your step will minimise by a factor of two when $t$ is equal to $m$."
"Variance of $K$-fold cross-validation estimates as $f(K)$: what is the role of ""stability""?","
TL,DR: It appears that, contrary to oft-repeated advice, leave-one-out cross validation (LOO-CV) -- that is, $K$-fold CV with $K$ (the number of folds) equal to $N$ (the number of training observations) -- yields estimates of the generalization error that are the least variable for any $K$, not the most variable, assuming a certain stability condition on either the model/algorithm, the dataset, or both (I'm not sure which is correct as I don't really understand this stability condition). 

Can someone clearly explain what exactly this stability condition is?
Is it true that linear regression is one such ""stable"" algorithm, implying that in that context, LOO-CV is strictly the best choice of CV as far as bias and variance of the estimates of generalization error are concerned?


The conventional wisdom is that the choice of $K$ in $K$-fold CV follows a bias-variance tradeoff, such lower values of $K$ (approaching 2) lead to estimates of the generalization error that have more pessimistic bias, but lower variance, while higher values of $K$ (approaching $N$) lead to estimates that are less biased, but with greater variance. The conventional explanation for this phenomenon of variance increasing with $K$ is given perhaps most prominently in The Elements of Statistical Learning (Section 7.10.1):

With K=N, the cross-validation estimator is approximately unbiased for the true (expected) prediction error, but can have high variance because the N ""training sets"" are so similar to one another.

The implication being that the $N$ validation errors are more highly correlated so that their sum is more variable. This line of reasoning has been repeated in many answers on this site (e.g., here, here, here, here, here, here, and here) as well as on various blogs and etc. But a detailed analysis is virtually never given, instead only an intuition or brief sketch of what an analysis might look like.
One can however find contradictory statements, usually citing a certain ""stability"" condition that I don't really understand. For example, this contradictory answer quotes a couple paragraphs from a 2015 paper which says, among other things, ""For models/modeling procedures with low instability, LOO often has the smallest variability"" (emphasis added). This paper (section 5.2) seems to agree that LOO represents the least variable choice of $K$ as long as the model/algorithm is ""stable."" Taking even another stance on the issue, there is also this paper (Corollary 2), which says ""The variance of $k$ fold cross validation [...] does not depend on $k$,"" again citing a certain ""stability"" condition.
The explanation about why LOO might be the most variable $K$-fold CV is intuitive enough, but there is a counter-intuition. The final CV estimate of the mean squared error (MSE) is the mean of the MSE estimates in each fold. So as $K$ increases up to $N$, the CV estimate is the mean of an increasing number of random variables. And we know that the variance of a mean decreases with the number of variables being averaged over. So in order for LOO to be the most variable $K$-fold CV, it would have to be true that the increase in variance due to the increased correlation among the MSE estimates outweighs the decrease in variance due to the greater number of folds being averaged over. And it is not at all obvious that this is true.
Having become thoroughly confused thinking about all this, I decided to run a little simulation for the linear regression case. I simulated 10,000 datasets with $N$=50 and 3 uncorrelated predictors, each time estimating the generalization error using $K$-fold CV with $K$=2, 5, 10, or 50=$N$. The R code is here. Here are the resulting means and variances of the CV estimates across all 10,000 datasets (in MSE units):
         k = 2 k = 5 k = 10 k = n = 50
mean     1.187 1.108  1.094      1.087
variance 0.094 0.058  0.053      0.051

These results show the expected pattern that higher values of $K$ lead to a less pessimistic bias, but also appear to confirm that the variance of the CV estimates is lowest, not highest, in the LOO case.
So it appears that linear regression is one of the ""stable"" cases mentioned in the papers above, where increasing $K$ is associated with decreasing rather than increasing variance in the CV estimates. But what I still don't understand is:

What precisely is this ""stability"" condition? Does it apply to models/algorithms, datasets, or both to some extent?
Is there an intuitive way to think about this stability?
What are other examples of stable and unstable models/algorithms or datasets?
Is it relatively safe to assume that most models/algorithms or datasets are ""stable"" and therefore that $K$ should generally be chosen as high as is computationally feasible?

","['regression', 'machine-learning', 'variance', 'cross-validation', 'predictive-models']","This answer follows up on my answer in Bias and variance in leave-one-out vs K-fold cross validation that discusses why LOOCV does not always lead to higher variance. Following a similar approach, I will attempt to highlight a case where LOOCV does lead to higher variance in the presence of outliers and an ""unstable model"".  The topic of algorithmic stability is a recent one and several classic, infuential results have been proven in the past 20 years. Here are a few papers which are often citedThe best page to gain an understanding is certainly the wikipedia page which provides a excellent summary written by a presumably very knowledgeable user. Intuitively, a stable algorithm is one for which the prediction does not change much when the training data is modified slightly. Formally, there are half a dozen versions of stability, linked together by technical conditions and hierarchies, see this graphic from here for example: The objective however is simple, we want to get tight bounds on the generalization error of a specific learning algorithm, when the algorithm satisfies the stability criterion. As one would expect, the more restrictive the stability criterion, the tighter the corresponding bound will be. The following notation is from the wikipedia article, which itself copies the Bousquet and Elisseef paper: Perhaps the strongest notion of stability that an interesting learning algorithm might be expected to obey is that of uniform stability: Uniform stability
An algorithm has uniform stability $\beta$ wth respect to the loss function $V$ if the following holds:$$\forall S \in Z^m \ \ \forall i \in \{ 1,...,m\}, \ \ \sup | V(f_s,z) - V(f_{S^{|i},z})  |\  \ \leq \beta$$Considered as a function of $m$, the term $\beta$ can be written as $\beta_m$. We say the algorithm is stable when $\beta_m$ decreases as $\frac{1}{m}$. A slightly weaker form of stability is: Hypothesis stability $$\forall i \in \{ 1,...,m\}, \ \ \mathbb{E}[\ | V(f_s,z) - V(f_{S^{|i},z})  |\ ] \ \leq \beta$$If one point is removed, the difference in the outcome of the learning algorithm is measured by the averaged absolute difference of the losses ($L_1$ norm). Intuitively: small changes in the sample can only cause the algorithm to move to nearby hypotheses.The advantage of these forms of stability is that they provide bounds for the bias and variance of stable algorithms. In particular, Bousquet proved these bounds for Uniform and Hypothesis stability in 2002. Since then, much work has been done to try to relax the stability conditions and generalize the bounds, for example in 2011, Kale, Kumar, Vassilvitskii argue that mean square stability  provides better variance quantitative variance reduction bounds.  The following algorithms have been shown to be stable and have proven generalization bounds:Repeating the experiment from the previous thread (see here), we now introduce a certain ratio of outliers in the data set. In particular:As the $3$ order polynomial model is not regularized, it will be heavily influenced by the presence of a few outliers for small data sets. For larger datasets, or when there are more outliers, their effect is smaller as they tend to cancel out. See below for two models for 60 and 200 data points. Performing the simulation as previously and plotting the resulting average MSE and variance of the MSE gives results very similar to Experiment 2 of the Bengio & Grandvalet 2004 paper. Left Hand Side: no outliers. Right Hand Side: 3% outliers. (see the linked paper for explanation of the last figure) Quoting Yves Grandvalet's answer on the other thread:Intuitively, [in the situation of unstable algorithms], leave-one-out CV may be blind to instabilities that exist, but may not be triggered by changing a single point in the training data, which makes it highly variable to the realization of the training set.In practice it is quite difficult to simulate an increase in variance due to LOOCV. It requires a particular combination of instability, some outliers but not too many, and a large number of iterations. Perhaps this is expected since linear regression has been shown to be quite stable. An interesting experiment would be to repeat this for higher dimensional data and a more unstable algorithm (e.g. decision tree) "
