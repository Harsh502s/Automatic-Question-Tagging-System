Head,Body,Tags,First Answer
how to work with multi-labels or two inputs and a output,"
I’m in this problem and haven’t found a sound solution to it. Been like 20 days now. I have a dataset that looks like this: 
X=image

Y1= current_zoom (0,25,50,75)

Y2= predicted_zoom (0,25,50,75)

y1 will have equal images for all classes. Also, I will know X and y1 when I test the model.
y2 will have variation with it because it has the predicted zoom level.
I tried to train on MTL model, I used two outputs from the model - y1 and y2. Now, y2 overfits (still not sure why, but my best guess is class imbalance). And y1 accuracy is around 0.99 in validation. Now the thing is when I deploy this on production, I’ll always have the current zoom (y1) with the image. So, I wanted to incorporate this to my model. First, I tried with two inputs and one output model, but loss was too much. Here, I concatenated the y1 to the output of last conv layer before it flattens and goes to dense. And second I tried was to concatenate y1 after flattening the output from last conv. Both didn’t work.
Any ideas on how can I with data like this.
Models used: resnet-18, vgg, alexnet
size of data: approx 7000 images in total.
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']",
What are the technological challenges that AI faces today? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I am writing a field report on AI. I was wondering what the technological challenges are that AI is facing today. I have written the following so far.

AI needs common sense like a human common
AI needs curiosity
AI needs to understand cause and effect
AI needs to be creative

Are there any other hardware-tech-related obstacles?
",['reference-request'],
Understanding alpha-beta pruning for simplified NIM,"
This is a simple version of NIM: Two players alternately remove one, two or three coins from a stack initially containing 5 coins. The player who picks up the last coin loses.
What does alpha-beta pruning look like on the game tree for this game?
","['game-ai', 'minimax']",
Reinforcement Learning with more actions than states,"
I have read a lot about RL recently. As far as I understood, most RL applications have much more states than there are actions to choose from.
I am thinking about using RL for a problem where I have got a lot of actions to choose from, but only very few states. 
To give a handy example:
The algo should render (for whatever reason) a sentence with three words. I always want to have a sentence with three words, but I have many words to choose from. After choosing the words, I get some sort of reward.
Are RL algorithms an efficient way to solve this? 
I am thinking about using policy gradients with an ε-greedy algorithm to explore a lot of the possible actions before exploiting the knowledge gained.
","['reinforcement-learning', 'policy-gradients', 'greedy-ai']","As far as I understood, most RL applications have much more states than there are actions to choose from.Yes, this is quite common, but in no way required by the underlying theory of Markov Decision Processes (MDPs). The most extreme version of the opposite thing - with one state (or effectively no state, as state is not relevant)  - are k-armed bandit problems, where an agent tries to find a single best long-term action in general from a selection of actions. These problems typically would not use RL algorithms such as Q-learning or policy-gradients. However, that is partly because they are described with different goals in mind (e.g. minimising ""regret"" or simply gaining as much reward as possible during the learning process), and RL algorithms will work to solve them, albeit less efficiently than algorithms designed to work on bandit problems directly.I am thinking about using RL for a problem where I have got a lot of actions to choose from, but only very few states.That should work, provided your problem is still a MDP. That means for instance that the state evolves according to rules depending on which action was taken in which starting state. If the state evolution is instead arbitrary or random, then you may have a contextual bandit problem instead.There is an important difference here between:andThe former will require lots of exploration, since any specific combination of action and state could be the ideal. With the latter, you can use that fact that numerical values that are similar will often give similar results, which will make generalisation via function approximation (e.g. neural networks) work efficiently in your favour.The algo should render (for whatever reason) a sentence with three words. I always want to have a sentence with three words, but I have many words to choose from. After choosing the words, I get some sort of rewardThis seems more like the first bullet-point above, although that may depend if a natural language model could be applied for example. E.g. if ""This is good"" and ""This is great"" would produce similar rewards in a specific state, then there is maybe some benefit to generalisation, although I am not quite sure where you would fit this knowledge - possibly in a generator for a sentence vector as the ""raw"" action and then have a LSTM-based language model produce the actual action from that vector, similar to seq2seq translation models.Are RL algorithms an efficient way to solve this?Yes, but whether or not it is the most efficient will depend on other factors, such as:Whether the environment is stochastic or deterministic with regard to both reward and state progression. Whether state progression is key to obtaining the best rewards in the long term (e.g. there is some ""goal"" or ""best"" state that can only be reached by a certain route).What the actual size of the MDP is $|\mathcal{S}| \times |\mathcal{A}|$. Small MDPs can be fully enumerated, allowing you to estimate action values in a simple table. If you have 10 states and 1,000,000 discrete actions, with no real pattern of actions mapping to results, then a big 10 million entry table will actually be reasonably efficient.Competitive algorithms to RL here might be global search and optimisation ones, such as genetic algorithms. The more arbitrary and deterministic your environment is, the more likely it is that an exhaustive search will find your optimal policy faster than RL.I am thinking about using policy gradients with an ε-greedy algorithm to explore a lot of the possible actions before exploiting the knowledge gained.This should be fine. Exploration here is definitely important, but finding the sweet spot for the right amount of it will be hard, and depend on other traits of the environment. You may want to use something like upper confidence bound action selection or simply optimistic initial values, in order to ensure exploration does not miss certain actions. An epsilon greedy approach will miss a certain fraction of actions over time, and the expectation of that fraction grows smaller progressively more slowly, so it may be possible to miss an important action for a long time if you rely on being able to randomly select it.To give a handy example: The algo should render (for whatever reason) a sentence with three words. I always want to have a sentence with three words, but I have many words to choose from. After choosing the words, I get some sort of reward.I would consider modelling this as sequences of 3 actions, each of which chooses a word, with the state being a start token (whatever the state you already have) plus the sentence so far, and on every 3 words the environment is consulted to reset the state to the next start token and to gather a reward (rewards for interim steps would be zero).Doing this immediately makes the state space much larger than the action space, as your state includes history of up to two actions. If you had 10 different start states, and 100 word choices, then your state space would be 101,010 and action space 100.This will fit available designs of RL algorithms, and allow for learning some internal language modelling if it is relevant. It will reduce your need to model sentence construction outside of the agent. Most importantly, if ""good"" or ""bad"" sentences tend to start or end with certain words, and you use function approximation, then the algorithm may discover combinations more efficiently than iterating over all sentences as if they were completely independent."
Reinforcement learning for inventory management with dynamic changes to available products,"
Consider a shop owner who has to deal with having to buy for one week from a different supplier with several different brands. Another week a brand is removed or added from the market. Yet another week, the manager decides to skip three assortments of fruit soda and exchange them with a different selection of three assortments of fruit sods, and so on and so forth.
Is dealing with such issues possible with existing public implementations of reinforcement learning?
The only research that I saw dealing with dynamically changing stuff like this is the DeepMind FTW bot playing Quake capture the flag. It deals with changing layouts to the map being played, but the implementation is not public and it doesn't resemble the inventory management situation I outlined.
","['machine-learning', 'reinforcement-learning']",
Is making lot of 1 versus other model efficient?,"
I've got classification problem on image, I have 10 classes and when I fine tuned my model on it (I tried VGG, Xception, resnet etc) I have approximatly 83% validation accuracy.
I was wondering if doing lot of binary model with 1 class represented and the other as 'other' and then using them to classify my image would be good and efficient ? (I obtain more than 90% val acc for each class doing this)
Except for memory consumption and training time does this method have drawback ?
","['deep-learning', 'convolutional-neural-networks', 'classification']",
Why don't we perform classification of crowd density?,"
For the case of crowd density estimation using CNN, using datasets like shanhaiTech or UCF, why there hasn't been attempts to tackle this type of task as a classification problem? All current papers I've seen are related to regression, not classification.
For example, if I have the crowd images labeled based on their crowd density (low, moderate, high), and I'm not interested in the count, but the density class (low, moderate, high), can't I train the network to classify the data based on these classes as a classification network?
","['convolutional-neural-networks', 'classification']","For example, if I have the crowd images labeled based on their crowd density (low, moderate, high), and I'm not interested in the count, but the density class (low, moderate, high), can't I train the network to classify the data based on these classes as a classification network?Yes you can, all you need is enough correctly labelled training data.A good rule of thumb is if a human expert can assign the correct label from an image (and purely from the image, not using extra information) then it is a realistic goal to train a CNN to perform the same labelling. why there hasn't been attempts to tackle this type of task as a classification problem.Probably because there are no natural, and likely no widely accepted, classes in this case (I may be wrong, maybe some international society has defined classes you could use). If you use a regression, you can map it to a particular problem case - e.g. sending an alert to someone responsible for traffic and safety when crowd density hits some threshold - by setting numerical boundaries to your classes. Using classification and mapping back the other way is harder."
Why doesn't my image classification network get better with training?,"
I am attempting to train a network to do something I thought would be a relatively simple case to learn with: identify whether the back of a scanned vintage postcard has one of 'no postage stamp', a '1 cent stamp', or a '2 cent stamp.' The images are 250px by about 150px, RGB color, and there are about two thousand of them. Ballpark 75% of them are no-stamp, 20% 1-cent, and 10% 2-cent.
When I attempt to train the network it seems like it is starting at 70 +/- 1 % accurate and hovers in that range for 50 epochs, never improving. I'm not sure I'm reading the metrics correctly, though, as this doesn't seem quite right.
I set this up by following the tutorial on the Keras blog: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html 
I haven't implemented the latter part of the tutorial, where a pre-trained network is used, because I haven't found one that seems like it would be a similar problem.
My training and validation sets are here: https://drive.google.com/open?id=1-TxEKVVvP7RuFC7kFgH7Wt5A8z8QGTR3 
And my Google Colab Jupyter notebook is here: https://colab.research.google.com/drive/1UuKDF1wDwYlXszB2ahIrygRnfcs2D_sD 
","['convolutional-neural-networks', 'image-recognition', 'classification', 'keras']",
What determines the values of weights in a neural network?,"
I am trying to understand how weights are actually gotten. What is generating them in a neural network? What is the algorithm that gives them certain values?
","['neural-networks', 'machine-learning', 'neurons']","Typically, weights are randomly initialized. Then, as the model is optimized for its given task, those weights are steadily made ""better"" as determined by the network's loss function. This is also referred to as ""training"" the neural network. By far the most popular way of updating weights in a neural net is the backpropagation algorithm, most simply with stochastic gradient descent (SGD). Essentially, the algorithm determines how much each individual weight contributed to the network's loss. It then updates that weight in the direction that would reduce the loss.I recommend going through Michael Nielsen's online book to learn the basics."
Can machine learning be used to improve the average case complexity of an algorithm?,"
I am developing an algorithm that, in certain moment, must explore an exponential number of objects derived from a graph:
for o in my_graph.getDerivedObjects():
  if hasPropertyX(o):
    process(o)
    break;

If one of the derived objects has property $X$, then the algorithm process it and then stops. The theory ensures that at least one of these derived objects has property $X$. Now, I strongly suspect that there is a strong correlation between some topological aspects of the graph, and which derived objects actually have property $X$. I want to predict some of the derived objects that have property $X$ using Machine Learning. So, the idea is:

Predict a derived object $o$ that supposedly has property $X$ - or maybe predict $n$ of them for some number $n$.
If any of them is useful, I use them. If not, I run the exponential algorithm.

Of course, this isn't an optimization in the worst-case complexity of the algorithm. Also, I suppose I should also develop some statistical tests in order to show that the prediction algorithm actually works.
Is this type of optimizations common? Could you please provide some examples? The literature on the subject would also be greatly appreciated.
","['algorithm', 'applications', 'time-complexity']",
"In tic-tac-toe, what is the effect of the starting state on the state and action value function?","
I am simulating a Tic-Tac-Toe game with a human opponent. The way the RL trains is through policy/value iterations for a fixed number of iterations all specified by the user. Now, whether the human player has turn 1 or turn 2 will decide the starting state (1st move by human or empty). The starting states for the 1st case can differ as the human can make 9 different moves.
So, my questions are:

In tic-tac-toe, what is the effect of the starting state on the state and action value function?
Does it converge to the same stable value for all starting states?
Will the value functions change if the starting players are changed? (human vs RL to RL vs human)

NOTE: I will be enumerating all states since there are approximately 20000 states which I believe is not a big number and thus convergence should not be a problem.
","['reinforcement-learning', 'value-functions', 'value-iteration', 'policy-iteration', 'tic-tac-toe']","My understanding - from comments on the question - is that you are looking to train a Reinforcement Learning agent on the game of Tic Tac Toe (perhaps just in theory), where the agent should learn to play against a ""human"" opponent. In practice you may want a model of a human opponent.In this case, the RL agent will be presented with a board state, it will take an action (to put its mark on an empty place in the grid) and either:Win immediately, receiving a positive reward. It is common to use +1 in a game like Tic Tac Toe, so I will assume that later.Lose on opponent's turn, receiving a negative reward (assumed -1 later in the answer), as opponent makes a move that causes it to win. This is effectively ""immediately"" in terms of time steps, the agent does not get to act afterwards.Receive zero reward, and a new board state that includes the opponent's moveReceive zero reward, and the game ends in a drawIn all cases, the opponent is considered part of the environment. That makes the opponent behaviour critical to the value function and choice of optimal play. Training versus different opponents can result in very different state values.For training to be stable, the opponent should behave with the same probability of action choices for each interim state that it observes. That includes it behaving deterministically, even optimally, purely randomly or anything in-between provided the probability distribution is fixed.With the above context, it is possible to give sound answers to your questions:In tic-tac-toe, what is the effect of the starting state on the state and action value function?Each state of the board, or each state/action pair if you want to track action values, should converge to a value, depending on the agent's estimated, expected result at the end of the episode. As there is only a single reward possible at the end of each game, this will vary between -1 and +1. If either the agent or the opponent can make mistakes at random, then non-zero values between -1 and +1 are possible.Does it converge to the same stable value for all starting states?That depends on behaviour of the opponent. In the scenario you are working with, the agent may not learn to play optimally in general, instead it will learn to gain optimal results against the supplied opponent. If the opponent can make mistakes, then moves which take advantage of that fact will have higher values.Without a detailed description of the opponent it is not possible to make many statements about the actual state values and action values.Against a perfect opponent, with the RL going second, it should converge to state values which are all zero and action values which are all zero or -1 for moves which would be mistakes.Against a completely random opponent, I would have to run it to be sure, but I would expect state values to have 3 different values, all slightly positive, depending if opponent chose middle, edge or corner cases - each of these would have slightly different chance of leading to a win for the agent going forward.Will the value functions change if the starting players are changed?Due to the turn-based nature of the game, all the states observed would be different depending on who was the first player. When the agent goes first it will get to score the empty grid and action values of any position it would like to make a first mark in - it gets to see state after turn 0 on time step 1, after turn 2 on time step 2, after turn 4 on time step 3 etc. When the agent goes second it will get to see and evaluate the outpt of other turns - turn 1 at t=1, turn 3 at t=2, turn 5 at t=3 etc.That means the sets of states and state/action pairs for each case (RL first or RL second) are disjoint, and you cannot ask if one agent has the same value for a specific state as the other agent - it simply won't know about the other agent's values.If you train a single agent, sometimes starting first, sometimes starting second, the two sets of values never interact with each other directly - in an enumerated table, as per the question, this is not at all, but if an agent uses function approximation such as neural networks, then they can affect each other."
What is the difference between latent and embedding spaces?,"
In general, the word ""latent"" means ""hidden"" and ""to embed"" means ""to incorporate"". In machine learning, the expressions ""hidden (or latent) space"" and ""embedding space"" occur in several contexts. More specifically, an embedding can refer to a vector representation of a word. An embedding space can refer to a subspace of a bigger space, so we say that the subspace is embedded in the bigger space. The word ""latent"" comes up in contexts like hidden Markov models (HMMs) or auto-encoders.
What is the difference between these spaces? In some contexts, do these two expressions refer to the same concept?
","['machine-learning', 'terminology', 'word-embedding', 'latent-variable', 'embeddings']","Due to Machine Learning's recent and rapid renaissance, and the fact that it draws from many distinct areas of mathematics, statistics, and computer science, it often has a number of different terms for the same or similar concepts.""Latent space"" and ""embedding"" both refer to an (often lower-dimensional) representation of high-dimensional data:For example, in this ""Swiss roll"" data, the 3d data on the left is sensibly modelled as a 2d manifold 'embedded' in 3d space. The function mapping the 'latent' 2d data to its 3d representation is the embedding, and the underlying 2d space itself is the latent space (or embedded space):Depending on the specific impression you wish to give, ""embedding"" often goes by different terms:However this is not a hard-and-fast rule, and they are often completely interchangeable."
Is there a RNN that can predict the next substitute in a floorball match?,"
Floorball is a type of floor hockey. During the game, substitutions can be made.

The team is also allowed to change players any time in the game; usually, they change the whole team. Individual substitution happens sometimes, but it usually happens when a player is exhausted or is hurt. 

I would like to use an RNN to predict when the next substitution will happen for a team. However, I have no pre-existing dataset to train on. Is there a way that I can start predicting without a dataset and continually improve accuracy as more games are played?
","['neural-networks', 'training', 'datasets']",
Why don't we decorrelate transitions for policy-based data?,"
I'm implementing PPO myself strictly follow the steps:

sample transitions
randomly shuffle the sampled transitions
compute gradients and update networks using the sampled transitions
drop transitions and repeat the above steps 

I observe a strange phenomenon that randomly shuffling transitions makes the algorithm perform significantly worse than keeping it as it is. This is very strange to me. To my best understanding, neural networks perform badly when the input data are correlated. To decorrelate transitions, algorithms like DQN introduce replay buffer and randomly sample from it. But this seems not the same story to policy-based methods. I'm wondering why policy-based methods do not require to decorrelate the input data?
","['reinforcement-learning', 'proximal-policy-optimization']","We do decorrelate training experience, even for policy gradient methods. This is because decorrelation helps training data be more like IID data, which helps with the convergence of SGD-like optimizers. The shuffling is done on line 151 of OpenAI's ""baselines"" implementation of PPO.I'm going to guess that there's a bug somewhere in your implementation. If you're using the same truncated advantage estimation as in the PPO paper, $$\begin{align}
&\hat{A}_t = \delta_t + (\gamma\lambda)\delta_{t+1}+\dots+(\gamma\lambda)^{T-t+1}\delta_{T-1}\\
&\text{where}\quad\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)
\end{align}$$make sure you're not shuffling experience before computing your advantage estimates $\hat{A}_1,\dots,\hat{A}_T$. Each of these estimates is a function of several unbroken steps of experience. After you compute your advantage estimates, then create the tuples $(s_t, a_t, \hat{A}_t)$ and shuffle and sample from those. I think that's all the information you need  per time step for constructing their objective function."
Facial Recognition + Database + Compare & Identify - is it complicated? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



The last week I've been looking for freelancers who are able to do this project for me but they weren't that experienced in it, so I would like to know whether my idea is complicated or is it their lake of experience.
Scenario:
1. The facial recognition system will be installed on a vertical screen where a camera would be attached to it and it will be assigned on the entrance of the room.
2. Once a visitor come to the entrance, and looks at the screen, a text on the screen would say: "" Welcome! It seems like it's your first visit! Please enter your name then a keyboard should popup""
3. The visitor would enter his first name into the keyboard, and it would be saved alongside his face in the database, and it would say thank you, {name}.
4. If the same visitor visits again, the system should say: "" Welcome back {name}, happy to see you again""!
","['machine-learning', 'facial-recognition', 'pattern-recognition']",
Is Reinforcement Learning the future of Natural Language Processing?,"
I was reading about the grounding problem after seeing it mentioned in another answer today. The article states that, in order to avoid the ""infinite regress"" of defining all words with other words, we must ground the meaning of some words in the ""sensorimotor."" 

To be grounded, the symbol system would have to be augmented with nonsymbolic, sensorimotor capacities—the capacity to interact autonomously with that world of objects, events, actions, properties and states that its symbols are systematically interpretable (by us) as referring to.

Obviously, this made me think of Reinforcement Learning. But I'm not exactly sure what counts as ""interaction."" Would this necessarily imply an MDP-like formulation with rewards, state transitions, etc? Or could some form of grounding be accomplished with supervised learning?
This seems like a pretty fundamental problem of AI. Does anyone know of research being done on grounding words/symbols within an RL agent?
","['reinforcement-learning', 'natural-language-processing']",
Which matrix represents the similarity between words when using SVD?,"
Two words can be similar if they co-occur ""a lot"" together. They can also be similar if they have similar vectors. This similarity can be captured using cosine similarity. Let $A$ be a $n \times n$ matrix counting how often $w_i$ occurs with $w_k$ for $i,k = 1, \dots, n$. Since computing the cosine similarity between $w_i$ and $w_k$ might be expensive, we approximate $A$ using truncated SVD with $k$ components as: $$A \approx W_k \Sigma W^{T}_{k} = CD$$
where $$C = W_{k} \Sigma \\ D = W^{T}_{k}$$
Where are the cosine similarities between the words $w_i$ and $w_k$ captured? In the $C$ matrix or the $D$ matrix?
","['machine-learning', 'natural-language-processing', 'math']","You can find some material here and here but the idea (at least in this case) is the following: consider the full SVD decomposition of the symmetric matrix $A = W \Delta W^T$. We want to calculate the cosine similarity between the $i$-th column (aka word) $a_i$ and the $j$-th column $a_j$ of $A$. Then $a_k = A e_k$, where $e_k$ is the $k$-th vector of the canonical basis of $\mathbb{R}$. Let's call $\cos(a_i,a_j)$ the cosine between $a_i,a_j$. Then
$$\cos(a_i,a_j) = \cos(Ae_i,Ae_j) = \cos(W \Delta W^T e_i,W \Delta W^T e_j) = \cos(\Delta W^T e_i,\Delta W^T e_j)$$where the last equality holds because $W$ is an orthogonal matrix (and so $W$ is conformal, i.e. it preserves angles). So you can calculate the cosine similarity between the columns of $\Delta W^T$. A $k$-truncated SVD gives a well-enough approximation. In general, columns of $W \Delta$ and rows of $W$ have different meanings!"
How can we use linear programming to solve an MDP?,"
Apparently, we can solve an MDP (that is, we can find the optimal policy for a given MDP) using a linear programming formulation. What's the basic idea behind this approach? I think you should start by explaining the basic idea behind a linear programming formulation and which algorithms can be used to solve such constrained optimisation problems.
","['reinforcement-learning', 'optimization', 'markov-decision-process', 'linear-programming']","This question seems to be addressed directly in these slides.The basic idea is:$$V^*(s) = r + \gamma \max_{a \in A}\sum_{s' \in S} P(s' | s,a) \cdot V^*(s')$$That is, the true value of the state is the reward we accrue for being in it, plus the expected future rewards of acting optimally from now until infinitely far into the future, discounted by the factor $\gamma$, which captures the idea that reward in the future is less good than reward now.In Linear Programming, we find the minimum or maximum value of some function, subject to a set of constraints. We can do this efficiently if the function can take on continuous values, but the problem becomes NP-Hard if the values are discrete. You would usually do this using something like the Branch & Bound algorithm. These are widely available in fast implementations. GLPK is a decent free library. IBM's CPLEX is faster, but expensive.We can represent the problem of finding the value of a given state as:
$$\text{minimize}_V \ V(s)$$
subject to the constraints:
$$V(s) \geq r + \gamma\sum_{s' \in S} P(s' | s,a)*V(s'),\; \forall a\in A, s \in S$$
It should be apparent that if we find the smallest value of $V(s)$ that matches this requirement, then that value would make exactly one of the constraints tight.If you formulate your linear program by writing a program like the one above for every state and then minimize $\sum_{s\in S} V(s)$, subject to the union of all the constraints from all these sub-problems you have reduced the problem of learning a value function to solving the LP."
Why is the max a non-expansive operator?,"
In certain reinforcement learning (RL) proofs, the operators involved are assumed to be non-expansive. For example, on page 6 of the paper Generalized Markov Decision Processes: Dynamic-programming and Reinforcement-learning Algorithms (1997), Csaba Szepesvari and Michael L. Littman state

When $0 \leq \gamma < 1$ and $\otimes$ and $\oplus$ are non-expansions, the generalized Bellman equations have a unique optimal solution, and therefore, the optimal value function is well defined.

On page 7 of the same paper, the authors say that max is non-expansive. Moreover, on page 33, the authors assume $\otimes$ and $\oplus$ are non-expansions.
What is a non-expansive operator? Why is the $\max$ (and the $\min$), which is, for example, used in Q-learning, a non-expansive operator?
","['reinforcement-learning', 'q-learning', 'math', 'proofs', 'papers']",
What is a generalized MDP?,"
What is a generalized MDP?
Here are a few sub-questions.

How is it different than a ""regular"" MDP?
How does it generalize the notion of an MDP?
Why do we need a generalized MDP?
Do generalized MDPs have some practical usefulness or they are just theoretical tools?

","['reinforcement-learning', 'reference-request', 'definitions', 'markov-decision-process']","(This answer is based on info that you can find in the paper $\varepsilon $-MDPs: Learning in Varying Environments, 2002, by István Szita et al. and [Szepesvári and Littman(1996)], the paper that proposed generalised MDPs. I just adapted the notation to be more consistent with Sutton & Barto's book and provided additional info and links).Let's first start with the usual definition of an MDP.An MDP is defined by the tuple $\langle \mathcal{S}, \mathcal{A}, R, \color{blue}{P} , \gamma \rangle$, whereGiven this formulation of a decision problem, the goal is to maximize the expected reward. The objective function can be defined as follows
$$
\mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^{t} R_{t} \right] = \mathbb{E}\left[ G_{t} \right],
$$
where $R_{t}$ is the reward that we get at time step $t$.There are multiple ways to solve this problem. The most common is probably Q-learning.In Q-learning, we try to estimate the state-action (or action) value function. The optional value function is defined as follows$$
Q^{*}(s, a)=\sum_{s'} \color{blue}{P}(s, a, s')\left(R(s, a, s')+\gamma \color{red}{\max} _{a^{\prime}} Q^{*}(s', a^{\prime})\right) \label{1}\tag{1}, 
$$
for all $s \in \mathcal{S}$.So, it's a function of the form $Q^{*}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$. To emphasize the main purpose of this function, it can be denoted/defined as $Q^{*}(s, a) = \mathbb{E}\left[G_t \mid a, s \right]$, so it's the expected return ($G_t$ is the return or cumulative reward) for taking action $a$ in state $s$ under the optimal policy $\pi^*$ (hence the $*$).The equation (\ref{1}) is a recursive equation because $Q^{*}(s, a)$ is defined in terms of itself, i.e. we use $Q^{*}(s', a^{\prime})$ to define it. In this context, this type of recursive equation is also known as Bellman equation (due to Richard Bellman which contributed to the theory of dynamic programming, which is related to MDPs, as dynamic programming algorithms can be used to solve MDPs, given a transition model).One thing to keep in mind is that the optimal policy can be derived from the optimal state-action value function by acting greedily with respect to it.Now, there's one reason why I colored the transition model and the max operation in the Bellman equation \ref{1} in $\color{blue}{\text{blue}}$ and $\color{red}{\text{red}}$, respectively, and this is related to generalized MDPs.The Bellman equation in \ref{1} is defined with respect tothe transition model $\color{blue}{P}$, which describes the dynamics of the environment,$\color{red}{\max}$, which is related to the assumption that the optimal agent acts greedily with respect to the optimal value function (keep in mind that the max operation is non-expansive)A generalised MDP [Szepesvári and Littman(1996)] is a generalisation of multiple versions/definitions of MDPs that circulate around in the literature, including the definition above. More specifically, 2 concepts are generalised: transition model and the max operator.Mathematically, we can define a generalised MDP as a tuple $\langle \mathcal{S}, \mathcal{A}, R, \color{blue}{\oplus}, \color{red}{\otimes}, \gamma \rangle$, where $\mathcal{S}, \mathcal{A}$ and $R$ are defined as above and$\color{blue}{\oplus}: (\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}) \rightarrow(\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R})$ is a ""expected value-type"" operator that generalises $\color{blue}{P}$; the intuition of this operator is the same as the input of $\color{blue}{P}$ that I described above$\color{red}{\otimes}:(\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}) \rightarrow(\mathcal{S} \rightarrow \mathbb{R})$ is a ""maximization-type"" operator that generalises $\color{red}{\max}$; the intuition of $\color{red}{\otimes}$ is also the same as the intuition of $\color{red}{\max}$ (i.e. it represents how the optimal agent would behave).So, these are operators (like the gradient operator or the Bellman operator) because they take as input a function and produce another function.Now, if we set$(\color{blue}{\oplus} U)(s, a)=\sum_{s'} \color{blue}{P}(s, a, s') U(s, a, s')$ (so, in this case, it takes as input the function $U$; note that, in this case, $\color{blue}{\oplus}$ is an expected value operator, with respect to the probability distirbution $\color{blue}{P}(s, a, s')$)$(\color{red}{\otimes} Q)(s)=\color{red}{\max}_{a} Q(s, a)$ (note that this just uses the $\color{red}{\max}$)Then we get the usual expected-reward MDP model.In the literature, there are different versions of MDPs, such asGeneralised MDPs generalise all these MDPs for different values of $\color{blue}{\oplus}$ and $\color{red}{\otimes}$.The paper [Szepesvári and Littman(1996)] (that introduced GMDPs) provides more info about how we should set $\color{red}{\otimes}$ and $\color{blue}{\oplus}$ to get these different MDPs (see table 1).The Bellman equation for the state value function can be expressed as follows in the context of GMDP$$
V^{*}=\color{red}{\otimes} \color{blue}{\oplus} \left(R + \gamma V^{*}\right)
$$For the state-action value function, you can have the following Bellman equation$$
K Q=\color{blue}{\oplus} (R+\gamma \color{red}{\otimes} Q)
$$or the one that relates $Q^{*}$ to $V^{*}$$$
Q^{*}=\color{blue}{\oplus} \left(R+\gamma V^{*}\right)
$$"
"Why would adding all the possible embeddings be ""worse"" than using 1D-convolutions?","
Suppose we are using word2vec and have embeddings of individual words $w_1, \dots, w_{10}$. Let's say we wanted to analyze $2$ grams or $3$ grams.
Why would adding all the possible embeddings, $\binom{10}{2}$ or $\binom{10}{3}$, be ""worse"" than using 1D-convolutions?
","['natural-language-processing', 'word-embedding', 'word2vec', '1d-convolution', 'n-gram']",
Why does the BERT encoder have an intermediate layer between the attention and neural network layers with a bigger output?,"
I am reading the BERT paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
As I look at the attention mechanism, I don't understand why in the BERT encoder we have an intermediate layer between the attention and neural network layers with a bigger output ($4*H$, where $H$ is the hidden size).
Perhaps it is the layer normalization, but, by looking at the code, I'm not certain. 
","['deep-learning', 'natural-language-processing', 'papers', 'attention', 'bert']",
How to perform unsupervised anomaly detection from log file with mostly textual data?,"
I have a log file of the format,

Index, Date, Timestamp, Module, App, Context, Session, Verbosity level, Description

The log file can be considered as a master log, which consists of individual logs from several modules constituting a distributed system. The individual log can be identified using the corresponding Module+App+Context tags. The verbosity level(Info, Warn, Error, …) and the descriptions(system generated + print statements added by developers) contain further information on the log events necessary for debugging. I need to perform an unsupervised anomaly detection with the log file as input. The output should be the functionality and timestamp of the identified anomalies. 
Since the log is mostly textual, I plan to use NLP algorithm (Bag of words/TF-IDF) to convert the data into word vectors and then perform a generative learning method to identify the normal pattern. Can someone suggest if my approach in the right direction? Which headers of the log file would be relevant for the word-vector representation and further analysis?
","['neural-networks', 'machine-learning', 'natural-language-processing', 'word2vec', 'anomaly-detection']",
Comparison and understanding of different version of DDQN?,"
There are several version of DDQN floating around. Sutton gives one that is a simple symmetric random update of the two Q functions. I think other papers (Silver paper for example) use a kind of delayed and split update rule. 
Is there anything systematic describing the properties of the bias corrections and their respective advantages?
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']",
Is there any GUI for per-neuron editing,"
I couldn't find GUI for precise ""artificial neural-network alike"" structures, which could supports neuron naming, synapse naming, import of external functions or code fragments and debugging. It would be ideal if synapses also could pass not only float values, but user-defined structures too. Optimization, GPU-computing is irrelevant (and probably impossible with such features).
Is such thing exists? I'm thinking about writing one myself, for my needs. And most probably I will...
Why do I need such features? For testing concept of construction of kind-of-neural logic programming. I'm sure there is also should exists programming languages with such paradigm, but I also don't know how to find them, google would mostly give me info about common artificial neural networks.
","['neural-networks', 'programming-languages']",
transform Location data into int which will be used as input to ML model [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I have features like state, city and location. Currently i am inserting this in respective tables in DB and tranform it using its primary key.
eg. country,state,city = IN,Maharastra,Pune = 2,5,10 (primary key in DB)
transforming it as 002005010
Is this approach correct? If not suggest me correct one
","['machine-learning', 'datasets']","In order to accurately input location data into a machine learning model it really depends on what your goal is and what type of algorithm you are working with.  If you are working with a strictly numerical algorithm and your data seems to be spread far apart, it might be easier to convert your country-state-city location to a longitude, latitude feature where the exact value is the centroid location of the given city.  This kaggle post has a good writeup of how to run build features for geo-spatial data.However, if you have only a small number of different locations per identifier (country, city, state) you could just represent them as seperate location classes and subclasses like you would for machine learning detection model.  You could think ""vehicle"" -> ""bike"" -> ""road-bike"" similarly to ""country"" -> ""state"" -> ""city"".  In this case, you would want to look at hierachical methods for machine learning and see some ways they represent their data.  Although I would say this method is more frowned upon for larger datasets, for smaller datasets it might be a better option."
What is non-Euclidean data?,"
What is non-Euclidean data?
Here are some sub-questions

Where does this type of data arise? I have come across this term in the context of geometric deep learning and graph neural networks.

Apparently, graphs and manifolds are non-Euclidean data. Why exactly is that the case?

What is the difference between non-Euclidean and Euclidean data?

How would a dataset of non-Euclidean data look like?


","['deep-learning', 'definitions', 'geometric-deep-learning', 'graph-neural-networks', 'non-euclidean-data']","I presume this question was prompted by the paper Geometric deep learning:
going beyond Euclidean data (2017). If we look at its abstract:Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric
  data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However,
  these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them.We see that the authors use the term ""non-Euclidean data"" to refer to data whose underlying structure is non-Euclidean.Since Euclidean spaces are prototypically defined by $\mathbb{R}^n$ (for some dimension $n$), 'Euclidean data' is data which is sensibly modelled as being plotted in $n$-dimensional linear space, for example image files (where the $x$ and $y$ coordinates refer to the location of each pixel, and the $z$ coordinate refers to its colour/intensity).However some data does not map neatly into $\mathbb{R}^n$, for example, a social network modelled by a graph. You can of course embed the physical shape of a graph in 3-d space, but you will lose information such as the quality of edges, or the values associated with nodes, or the directionality of edges, and there isn't an obvious sensible way of mapping these attributes to higher dimensional Euclidean space. And depending on the specific embedding, you may introduce spurious correlations (e.g. two unconnected nodes appearing closer to each other in the embedding than to nodes they are connected to).Methods such as Graph Neural Networks seek to adapt existing Machine Learning technologies to directly process non-Euclidean structured data as input, so that this (possibly useful) information is not lost in transforming the data into a Euclidean input as required by existing techniques."
What are the current open source text-to-audio libraries? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 7 months ago.







                        Improve this question
                    



I am not new to AI and did some work for few months but completely new to text to audio. Yes I used text to audio tools a decade back... but I would like to know where exactly we stand in terms of text to audio today.
I already did some research, it seems like traditional way of text to audio is fading away and speech cloning seems to be emerging but my impression on this might be completely wrong.
What are the current open source text-to-audio libraries?
",['audio-processing'],
Do we have to use CNN for Deep Q Learning?,"
I read top articles on Google Search about Deep Q-Learning:

https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8
https://skymind.ai/wiki/deep-reinforcement-learning
https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
Q-Learning page on wikipedia*

and then I noticed that they all use CNN as approximator. If deep learning has a broader definition than just CNN, can we use the term ""Deep Q-Learning"" on our model if we don't use CNN? or is there a more appropriate definition for that kind of Q-Learning model? for example, if my model only using deep fully-connected layer.
*it doesn't say explicitly Deep RL means CNN on RL, but it uses the DeepMind (that uses CNN) as an example on Deep Q-Learning 
","['reinforcement-learning', 'definitions', 'deep-rl']","No. DQN and other deep RL methods work well with fully connected layers. Here's an implementation of DQN which doesn't use CNNs: github.com/keon/deep-q-learning/blob/master/dqn.pyDeepMind mostly use CNN because they use image as input state, and that because they tried to evaluate performance of their methods vs humans performance. Humane performance is easy to measure at games with image as input state, and that's why CNN based methods present so promptly in RL now. "
Which unsupervised anomaly detection algorithms are there?,"
I need to create model which will find suspicious entries or anomalies in a network, whose characteristics or features are the asset_id, user_id, IP accessed from and time_stamp.
Which unsupervised anomaly detection algorithms or models should I use to solve this task?
","['unsupervised-learning', 'anomaly-detection']","If you are OK to use python, thy novelty-detection with sklearn:https://scikit-learn.org/stable/modules/outlier_detection.html"
"How to translate algorithm from logic to equation, and back?","
I just recently got into machine learning, and have been hitting a lot of obstacles understanding the algorithms involved in the programmings. My issue isnt with the programming, but how they're translated from math to code. ML is popular with python, and that's okay, but i dont like python, and i dont want to have to learn it to be able to use the programming language of my choice, to do the exact same thing but in a way i feel comfortable (i dont care if python is popular for math majors, because it's easier for them to understand -- it isnt for me, when nothing being done is explained thoroughly.). 
I'm trying to decipher this model

this is the breakdown for the algorithm model

this is the math i was able to decipher for this particular model

(Left is terminology and their usage, the middle in black was something to do with programming arrays... below it is the equation used in bottom left, but more elaborate, and underneath that is a image that says the same thing the algorithm is doing.. because sometimes pictures are easier to understand that words VectorArray(Value) * VectorArray(Weight) + SingleUnit(Bias) = Neuron(Node))
But then everything stops at the middle layer of the second image. How do i get the full output to give me a yes or no response?
How do i enter in the variables and tables to go thru the math steps?
Is my understanding correct, or am i lacking somewhere?
This user is also sharing the same algorithm but our math dont look the same
How do i go from what i have, to what [s]he has?
At the end of all of these questions, i'm going to write everything into a programming script, that'll use a different language from python (and i would need to manually create resources from scratch, because no one else thinks machine learning should be done in other languages -- it seems...). I want to be able to understand the process itself, without just doing cookie-cutter actions (tools made by users for those too lazy to do the work -- which circumvent the learning/understanding process of what's going on behind scenes).
","['neural-networks', 'machine-learning', 'algorithm']",
Should I ignore the actions RIGHTFIRE and LEFTFIRE in the SpaceInvaders environment? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I'm trying to replicate the DeepMind DQN paper. I'm using OpenAI's Gym. I'm trying to get a decent score with Space Invaders (using SpaceInvaders-v4 environment). I checked the actions available with env.unwrapped.get_action_meanings(), and I get this:
['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']

Checking the number of actions with env.action_space.n gives me a number of 6 actions.
The RIGHTFIRE and LEFTFIRE actions, I suppose, aren't used, given that they seem to do the same as LEFT and RIGHT, am I right?
If so, restricting the action size to the 4 first actions would improve my learning?
","['reinforcement-learning', 'dqn', 'deep-rl', 'gym', 'action-spaces']","I found that we shouldn't eliminate those actions, because RIGHTFIRE and LEFTFIRE do RIGHT + FIRE and LEFT + FIRE, respectively. So they are different actions. I post this just to clarify in case someone faces the same doubt."
Negative counterfactual regret,"
I am reading the paper Regret Minimization in Games with Incomplete
Information on CFR algorithm.
On page 4, the paper defines $R^{T,+}_{i,\text{imm}}=\max\{R^{T}_{i,\text{imm}}, 0\}$ after equation (5). I am confused why it is necessary? It seems to me that since in the definition of $R^{T}_{i,\text{imm}}$ the regret is computed with respect to the optimal action.

As everything is in expectation, is mixed-action going to make any difference?

Isn't $R^{T}_{i,\text{imm}}$ always nonzero already?
","['reinforcement-learning', 'game-ai', 'game-theory', 'poker']",
Which fields of AI are actively researching consciousness?,"
Max Tegmark discusses the topic of consciousness in his book Life 3.0 and comes to the conclusion, that consciousness is substrate independent. If his analysis is correct, it should be possible to create artificial consciousness. The integrated information theory (IIT), while currently only just a theory, also points in this direction.
This leads me to the question: which fields of AI research, if any, are currently actively engaged in this domain?
So far, I've only found research concerning consciousness in neuroscience and discussions of experts in philosophy.
Are there any projects publicly known concerning artificial consciousness or organizations that are active in this regard?
","['reference-request', 'research', 'artificial-consciousness', 'ai-field']",
How to reduce over-fitting on training set?,"
Currently I'm feeding spectrogram of audio to the CNN with 3 convolution.
Each convolution is followed by a max pool of filter size 2.
First -> 5x5x4
Second - > 5x5x8
Third - > 5x5x16
and final layer is a fully connected with 512 unit.
But while training with dropout of 0.25, getting train accuracy of 0.97 with 150 iterations.
and on test data accuracy is just 0.60.
Tell me how to improve the results.
Yes both train and test data come from same distribution.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks']",
How create an AI that continuously adapts to different users?,"
I want to make an AI with deep learning which can adapt itself from user to user. 
Let's say we have food combiner AI which suggests a food to eat with another food as you give as input. This is the most personalized case I found to ask here. For example the AI suggested a food for me. However, the food AI suggested for me might not be good choice for another person. So another person will let the AI know like ""I don't like that food to eat with this. Etc. When the user let the AI know that, It should affect AI's further combination food suggestions. 
How can I build that AI? Where should I start from? which area or topics should I research? 
","['machine-learning', 'deep-learning']",
"How many parameter would there be in a logistic regression model used to classify reviews into ""good"" or ""bad""?","
Suppose we want to classify a review as good ($1$) or bad ($0$). We have a training data set of $10,000$ reviews. Also, suppose we have a vocabulary of $100,000$ words $w_1, \dots, w_{100,000}$. So the data is a matrix of dimension $100,000 \times 10,000$. Let's represent each of the words in the reviews using a bag-of-words approach over tf-idf values. Also, we normalize the rows such that they sum to $1$.
In a logistic regression approach, would we have $10,000$ different logistic regression models as follows:
$$ \log \left(\frac{p}{1-p} \right)_{1} = \beta_{0_{1}} + \beta_{1_{1}}w_{11} + \dots + \beta_{100,000_{1}}w_{100,000} \\ \vdots \\ \log \left(\frac{p}{1-p} \right)_{10,000} = \beta_{0_{10,000}} + \beta_{1_{10,000}}w_{11} + \dots + \beta_{100,000_{10,000}}w_{100,000}$$
So are we estimating $100,000 \times 10,000$ coefficients?
","['natural-language-processing', 'weights', 'logistic-regression', 'binary-classification', 'bag-of-words']",
Are there communities dealing with costs-vs-accuracy tradeoffs in Machine Learning?,"
My group is working on a ML model that can work with little data (and bad accuracy) as long as there actually is little data available but can easily be extended as soon as said data becomes available (Think of interpolating existing models and then creating an individual predictor). This is due to a business requirement of the relevant application (new plants get installed over time but need to be integrated seamlessly). Transforming from a coarse yet cheap prediction to an accurate but expensive prediction takes labor effort that the company can invest as desired.
Are there research areas that take into account such ""evolutionary transformation processes"" which enable a tradeoff between costs and accuracy? What are the right keywords to look for? 
I am looking for keywords/papers/communities.
","['machine-learning', 'applications', 'reference-request']",
What would be a good comprehensive source about the different forms of classical learning in mammals?,"
I am looking for a source that really discusses the classic rules of learning in depth. So classical conditioning, operant conditioning, imitation learning... I have found an infinite number of books that supposedly discuss these topics, but have not yet found anything that summarizes all the important findings on this topic. I am familiar with the basics, but I am explicitly interested in a detailed presentation of the topic.
Can anyone tell me a good source about the different forms of classical learning in mammals? I consider ""Reinforcement Learning: An Introduction"" comprehensive and detailed regarding RL. A comparable book on biological learning systems would be great. Sources in German and English would fit.
","['reinforcement-learning', 'reference-request', 'biology']",
What features should a dataset to predict monthly retail sales for a motorcycle spare parts shop have?,"
I am making an AI model to predict monthly retail sales of a motor cycle spare parts shop, for that to be possible I have to first create a dataset. The problem I am facing is what features should the dataset have?
I already did some research on some other datasets but still I want to know specifically what features should it have other than Date, Product Name, Quantity, Net amount, Gross amount..?
","['datasets', 'feature-selection', 'data-mining']","This is the prototypical problem in AI/ML known as feature-selection. If you do not have too many features, typically one would just use them all(with the exception of features that are known to to be correlated), in this case you would want to perform feature engineering wherever possible as well. One the other hand, if you have lots of features, and some are likely to be useless, you would use a feature-selection technique.There are many many methods for doing this. One can simply use their intuitive grasp as their problem domain at the simplest level. However, there are also many algorithmic ways of performing it. Some examples of this are a low variance filter, or using a ensemble(classifier or regression depending on the problem) which can then be used to order features according to their derived importance(I personally like this).A search of ""feature selection methods in ML"" will yield many potential ways to accomplish your goal.As far as what to grab if you don't have many, as many as possible wrt domain knowledge."
Unable to understand the second iteration update in value iteration algorithm for solving MDP,"
I am trying to understand the value iteration method for Markov Decision Process(MDP) and I was referring to UC Berkeley's slides titled Markov Decision Processes and Exact Solution Methods
On slide no. 9, we start with the first step :

Ok! So, we have the information about the transition function (described elaborately in slide no. 5 as well), the resting reward is 0 and discount of 0.9.
Using this, I am able to compute the utility value of the cell left to terminal state with R = +1 (Green cell). The action that is going to be most rewarding at this cell is moving forward, so putting the values in the equation as:
$$0.0 + 0.9 (0.8*1 + 0.1*0 + 0.1*0) =0.72$$
which seems to be correct:

Now, using the same algorithm I am able to compute the value of the cells adjacent to this newly obtained utility cell value. However, I really do not know how did they update the value from

0.72 -> 0.78

in the next slide:

I have tried searching at various sites and seen some videos but most of them stop at the first iteration assuming the next step is the same, as it is a recursive equation (And it should have been so!), but I am stuck at this!
","['reinforcement-learning', 'markov-decision-process', 'value-iteration']","First thing to know is that, in this case, values for the gridworld in new iteration are completely calculated with respect to the old values from the previous iteration. Value of $0.78$ is got like this:  $0.9 \cdot (0.8 \cdot 1 + 0.1 \cdot 0.72 + 0.1 \cdot 0) = 0.7848 \approx 0.78$ term $0.8 \cdot 1$ is for going to the right with probability of $0.8$ and getting reward of $1$.  term $0.1 \cdot 0.72$ is for going up with probability of $0.1$, we hit the wall and stay in the same field which value is $0.72$ (from previous iteration)  term $0.1 \cdot 0$ is for going down with probability of $0.1$, even though value of that field in the image is $0.43$ we take the value from previous iteration which is $0$.  "
How do I detect similar objects in an image?,"
I want to tackle the problem of detecting similar objects in an image. To illustrate the problem consider this photo of some Lego bricks as my ""input"":

The detection routine should identify similar objects. So for the given input, it should e.g. identify the following output:

So an object might appear none to multiple times in the input image. For example, there are only two bricks marked with a blue cross, but three bricks marked with a red cross.
It can be assumed that all objects are of similar kind, so e.g. only Lego bricks or a heap of sweets.
My initial idea is to apply a two-phased approach:

Extract all objects in input image.
Compare those extracted objects and find similar ones.

Is this a valid approach or is there already some standard way of solving this kind of problem? Can you give me some pointers how to solve this problem?
",['object-recognition'],
What do the vectors of the center and outside word look like in word2vec?,"
In word2vec, the task is to learn to predict which words are most likely to be near each other in some long corpus of text. For each word $c$ in the corpus, the model outputs the probability distribution $P(O=o|C=c)$ of how likely each other word $o$ in the vocabulary is to be within a certain number of words away from $c$. We call $c$ the ""center word"" and $o$ the ""outside word"".
We choose the softmax distribution as the output of our model: $$P(O=o|C=c) = \frac{\exp(\textbf{u}_{0}^{T} \textbf{v}_{c})}{\sum_{w \in \text{Vocab}} \exp(\textbf{u}_{w}^{T} \textbf{v}_c)}$$
where $\textbf{u}_0$ and $\textbf{v}_c$ are vectors that represent the outside and center words respectively. 

Question. What do the vectors  $\textbf{u}_0$ and $\textbf{v}_c$ look like? Are they just one-hot-encodings? Do we need to learn them
  too? Why is this useful?

","['natural-language-processing', 'word2vec', 'word-embedding']",
2 Player Games in OpenAI Retro,"
I have been using OpenAI Retro for awhile, and I wanted to experiment with two player games. By two player games, I mean co-op games like ""Tennis-Atari2600"" or even Pong, where 2 agents are present in one environment.
There is a parameter for players in the OpenAI documentation, but setting this variable to 2 does nothing in terms of the game.
How do you properly implement this? Can this even be done? 
The end goal is to have 2 separate networks per one environment.
","['machine-learning', 'deep-learning', 'reinforcement-learning', 'python', 'open-ai']","OpenAI Retro is an extension to OpenAI Gym.As such, it does not support multiple agents in multi-player environments. Instead, the environment is always presented from the perspective of a single agent that needs to solve a control problem.When there is more than one agent in a Gym problem, everything other than the first agent is considered part of the environment. Typically player 2 in 2-player problems is controlled by whatever game AI already exists.You can work around this in a couple of ways, both involve you modifying existing environments, requiring a solid understanding of the integration and wrapper layers in Gym so that you can make correct edits. However you might find environments where either of these are done for you.In fact, within the Retro environments, some multiplayer environments are supported using the second option below.You could modify the environment code so that it accepts a parameter which is the agent used to control other players. Typically such an agent would not be set up to learn, it could just be some earlier version of the agent that is learning. You would need to code the interface between the supplied agent and the game engine - taking care to allow for its potentially different view of success.If a Gym environment has been set up for you (or any human) to compete against your own agent using the keyboard, that is already halfway there to this solution. You could take a look at where the Gym environment has been set up for this, and modify the code to allow for automated input as opposed to action selection by keyboard.This approach probably works best for competitive games where you can use a random selection of previous agents inserted into the environment to train a next ""generation"".Provided the game emulator allows you to do this, you can edit the code so that multiple players are controlled at once. The Gym environment does not specify how you construct your action choices, so you are then free to split the action vector up so that parts of it are chosen by one trained agent and parts of it by another.You may need to work on different views of the environment and/or reward signal in this case. For instance in a competitive game (or a co-operative game with competitive elements such as personal scores) then you will need to add custom code to re-interpret reward values compared to single player game. For something simple like Pong, that could just be that Player 2 receives the negative of Player 1's reward signal.This approach probably works best for cooperative games where success is combined. In that case, you have the choice of writing any number of separate agents that control different aspects of the action space - that would include having agents that separately controlled P1 and P2 game controller inputs. Each agent would still effectively treat its partner as a part of the environment, so you may want to mix training scenarios so that agents for each controller did not get too co-dependent. For instance, if you hope to step in to control P1, you probably don't want the AI running P2 to have too specific expectations of P1's role and behaviour in the game.This second option is what OpenAI Retro interface does. For instance Pong supports this:I verified this using the sample code in the documentation, which controls both paddles, and receives the reward signal as a Python list mostly [0.0, 0.0] but whenever a player scores it becomes [-1.0, 1.0] or [1.0, -1.0] depending on which player got the point.This will only be supported on retro games where someone has made the effort to make it work as above. If you have a game which does not do this, but in theory supports multiplayer on the ROM, you will need to look at and alter either the AI retro wrapper or the emulator or both."
Why is $M_t$ (the emphasis) helping in correcting for the state distribution in the Emphatic TD algorithm?,"
The book by Sutton and Barto discusses in section 11.8 that the convergence of off-policy TD function approximation can be improved by correcting for the distribution of states encountered. The section seems to be written in haste and doesn't do a good job in explaining why will $M_t$, the emphasis, help in getting a state distribution closer to the target policy.
My understanding of on-policy distribution is not clear at the moment. I think it is the distribution of states encountered under the target policy (the policy for which we want to state-action/state values).
The importance sampling ratio corrects for update distribution (by multiplying the correction term with the ratio), but how is $M_t$ helping in correcting for the state distribution?
","['reinforcement-learning', 'temporal-difference-methods', 'off-policy-methods', 'sutton-barto']",
How can the convolution operation be implemented as a matrix multiplication?,"
How can the convolution operation used by CNNs be implemented as a matrix-vector multiplication? We often think of the convolution operation in CNNs as a kernel that slides across the input. However, rather than sliding this kernel (e.g. using loops), we can perform the convolution operation ""in one step"" using a matrix-vector multiplication, where the matrix is a circulant matrix containing shifted versions of the kernel (as rows or columns) and the vector is the input. 
How exactly can this operation be performed? I am looking for a detailed step-by-step answer that shows how the convolution operation (as usually presented) can be performed using a matrix-vector multiplication.
Is this the usual way the convolution operations are implemented in CNNs?
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'implementation', 'convolution']","To show how the convolution (in the context of CNNs) can be viewed as matrix-vector multiplication, let's suppose that we want to apply a $3 \times 3$ kernel to a $4 \times 4$ input, with no padding and with unit stride.Here's an illustration of this convolutional layer (where, in blue, we have the input, in dark blue, the kernel, and, in green, the feature map or output of the convolution).Now, let the kernel be defined as follows$$
\mathbf{W} = 
\begin{bmatrix}
w_{0, 0} & w_{0, 1} & w_{0, 2} \\
w_{1, 0} & w_{1, 1} & w_{1, 2} \\
w_{2, 0} & w_{2, 1} & w_{2, 2}
\end{bmatrix} 
\in
\mathbb{R}^{3 \times 3}
$$Similarly, let the input be defined as$$
\mathbf{I} = 
\begin{bmatrix}
i_{0, 0} & i_{0, 1} & i_{0, 2} & i_{0, 3} \\
i_{1, 0} & i_{1, 1} & i_{1, 2} & i_{1, 3} \\
i_{2, 0} & i_{2, 1} & i_{2, 2} & i_{2, 3} \\
i_{3, 0} & i_{3, 1} & i_{3, 2} & i_{3, 3} \\ 
\end{bmatrix} 
\in
\mathbb{R}^{4 \times 4}
$$Then the convolution above (without padding and with stride 1) can be computed as a matrix-vector multiplication as follows. First, we redefine the kernel $\mathbf{W}$ as a sparse matrix $\mathbf{W}' \in \mathbb{R}^{4 \times 16}$ (which is a circulant matrix because of its circular nature) as follows.$$
{\scriptscriptstyle
\mathbf{W}' = 
\begin{bmatrix}
w_{0, 0} & w_{0, 1} & w_{0, 2} & 0 & w_{1, 0} & w_{1, 1} & w_{1, 2} & 0 &  w_{2, 0} & w_{2, 1} & w_{2, 2} & 0 & 0 & 0 & 0 & 0 \\
0 & w_{0, 0} & w_{0, 1} & w_{0, 2} & 0 & w_{1, 0} & w_{1, 1} & w_{1, 2} & 0 & w_{2, 0} & w_{2, 1} & w_{2, 2} & 0 & 0 & 0 & 0  \\
0 & 0 & 0 & 0 & w_{0, 0} & w_{0, 1} & w_{0, 2} & 0 & w_{1, 0} & w_{1, 1} & w_{1, 2} & 0 &  w_{2, 0} & w_{2, 1} & w_{2, 2} & 0 \\
0 & 0 & 0 & 0 & 0 & w_{0, 0} & w_{0, 1} & w_{0, 2} & 0 & w_{1, 0} & w_{1, 1} & w_{1, 2} & 0 &  w_{2, 0} & w_{2, 1} & w_{2, 2}
\end{bmatrix} 
}
$$
Similarly, we reshape the input $\mathbf{I}$ as a 16-dimensional vector $\mathbf{I}' \in  \mathbb{R}^{16}$.$$
{\scriptstyle
\mathbf{I}' = 
\begin{bmatrix}
i_{0, 0} & i_{0, 1} & i_{0, 2} & i_{0, 3} & i_{1, 0} & i_{1, 1} & i_{1, 2} & i_{1, 3} & i_{2, 0} & i_{2, 1} & i_{2, 2} & i_{2, 3} & i_{3, 0} & i_{3, 1} & i_{3, 2} & i_{3, 3} 
\end{bmatrix}^T
}
$$Then the convolution of $\mathbf{W}$ and $\mathbf{I}$, that is$$\mathbf{W} \circledast \mathbf{I} = \mathbf{O} \in \mathbb{R}^{2 \times 2},$$
where $\circledast$ is the convolution operator, is equivalently defined as $$\mathbf{W}' \cdot \mathbf{I}' = \mathbf{O}' \in \mathbb{R}^{4},$$ where $\cdot$ is the matrix-vector multiplication operator. The produced vector $\mathbf{O}'$ can then be reshaped as a $2 \times 2$ feature map.You can easily verify that this representation is correct by multiplying e.g. the 16-dimensional input vector $\mathbf{I}'$ with the first row of $\mathbf{W}'$ to obtain the top-left entry of the feature map.$$w_{0, 0} i_{0, 0} + w_{0, 1} i_{0, 1} + w_{0, 2} i_{0, 2} + 0 i_{0, 3} + w_{1, 0} i_{1, 0} + w_{1, 1} i_{1, 1} + w_{1, 2}i_{1, 2} + 0 i_{1, 3} +  w_{2, 0} i_{2, 0} + w_{2, 1}i_{2, 1} + w_{2, 2} i_{2, 2} + 0 i_{2, 3} + 0 i_{3, 0} + 0 i_{3, 1} + 0 i_{3, 2} + 0 i_{3, 3} = \\
w_{0, 0} i_{0, 0} + w_{0, 1} i_{0, 1} + w_{0, 2} i_{0, 2} + w_{1, 0} i_{1, 0} + w_{1, 1} i_{1, 1} + w_{1, 2}i_{1, 2} + w_{2, 0} i_{2, 0} + w_{2, 1}i_{2, 1} + w_{2, 2} i_{2, 2} = \\
\mathbf{O}'_{0}  \in \mathbb{R} ,$$ which is equivalent to an element-wise multiplication of $\mathbf{W}$ with the top-left $3 \times 3$ sub-matrix of the input followed by a summation over all elements (i.e. convolution), that is$$
\sum \left(
\begin{bmatrix}
w_{0, 0} & w_{0, 1} & w_{0, 2} \\
w_{1, 0} & w_{1, 1} & w_{1, 2} \\
w_{2, 0} & w_{2, 1} & w_{2, 2}
\end{bmatrix} 
\odot
\begin{bmatrix}
i_{0, 0} & i_{0, 1} & i_{0, 2} \\
i_{1, 0} & i_{1, 1} & i_{1, 2}  \\
i_{2, 0} & i_{2, 1} & i_{2, 2}
\end{bmatrix} 
\right) = \mathbf{O}_{0, 0} = \mathbf{O}'_{0} \in \mathbb{R},
$$
where $\odot$ is the element-wise multiplication and $\sum$ is the summation over all elements of the resulting matrix.The advantage of this representation (and computation) is that back-propagation can be computed more easily by just transposing $\mathbf{W}'$, i.e. with $\mathbf{W}'^T$.Take also a look at this Github repository that explains how the convolution can be implemented as matrix multiplication."
What is a graph neural network?,"
What is a graph neural network (GNN)?
Here are some sub-questions

How is a GNN different from a NN?
How exactly is a GNN related to graphs?
What are the components of a GNN? What are the inputs and outputs of GNNs?
How can GNNs be trained? Can we also use gradient descent with back-propagation to train GNNs?

","['deep-learning', 'definitions', 'geometric-deep-learning', 'graph-neural-networks']",
What is geometric deep learning?,"
What is geometric deep learning (GDL)?
Here are a few sub-questions

How is it different from deep learning?
Why do we need GDL?
What are some applications of GDL?

","['deep-learning', 'definitions', 'geometric-deep-learning', 'graphs', 'graph-neural-networks']","The article Geometric deep learning: going beyond Euclidean data (by Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst) provides an overview of this relatively new sub-field of deep learning. It answers all the questions asked above (and more). If you are familiar with deep learning, graphs, linear algebra and calculus, you should be able to follow this article.What is geometric deep learning (GDL)?This article describes GDL as followsGeometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds.So, the inputs to these GDL models are graphs (or representations of graphs), or, in general, any non-Euclidean data. To be more concrete, the input to these models (e.g. graph neural networks) are e.g. feature vectors associated with the nodes of the graphs and matrices which describe the graph structure (e.g. the adjacency matrix of the graphs).Why are e.g. graphs non-Euclidean data?A graph is a non-Euclidean structure because e.g. distances between nodes are not well defined. Yes, you can have graphs with weights associated with the edges, but not all graphs have this property.What classes of problems does GDL address?In GDL, there are two classes of problems that are often tackled:These classes of problems are related, given that the structure of the graph imposes certain properties on the functions that can be defined on it. Furthermore, these properties of these functions can also convey information about the structure of the graph.What are applications of GDL?An example of an application where this type of data (graphs) arises is in the context of social networks, where each user can be associated with a vertex of the social graph and the characteristics (or features) of each user (e.g. number of friends) can be represented as a feature vector (which can then be associated with the corresponding vertex of a graph). In this context, the goal might e.g. be to determine different groups of users in the social network (i.e. clustering).Why can't we simply use deep learning methods (like CNNs) when the data is non-Euclidean?There are several problems that arise when dealing with non-Euclidean data. For example, operations like convolution are not (usually) defined on non-Euclidean data. More concretely, the relative position of nodes is not defined on graphs (but this would be required to perform the usual convolution operation): in other words, it is meaningless to talk about a vertex that is e.g. on the left of another vertex. In practice, it means that we can't simply use the usual CNN when we are given non-Euclidean data. There have been attempts to generalise the convolution operation to graphs (or to approximate it). The field is still quite new, so there will certainly be new developments and breakthroughs."
How could we estimate the square footage of a room from an image?,"
I wonder if it would be possible to know the size of a room using image, I don't see anything about this subject, do you have some idea how it could be done?
","['machine-learning', 'deep-learning', 'computer-vision', 'papers']","Welcome to AI.SE Hadrien!A possible approach is:In practice, the network is quite likely to pick up on any patterns in the images that correlate with square footage, not necessarily the ones you want. For example, it might pick up on, say, the fact that humans in the picture are usually a good object to guess at the scale of the rest of the room."
Should we multiply the target of actor by the importance sampling ratio when prioritized replay is applied to DDPG?,"
According to PER, we have to multiply the $Q$ error $\delta_i$ by the importance sampling ratio to correct the bias introduced by the imbalance sampling of PER, where importance sampling ratio is defined
$$
w_i=\left({1\over N}{1\over P(i)}\right)^\beta
$$
in which $1/N$ is the probability of drawing a sample uniformly from the buffer, and $P(i)$ is the probability of drawing a sample from PER.
I'm wondering if we have to do the same to the target of the actor when we apply PER to DDPG. That is, multiplying $-Q(s_i, \mu(s_i))$ by $w_i$, where $\mu$ is the outcome of the actor.
In my opinion, it is necessary. And I've done some experiments in the gym environment BipedalWalker-v2. The results, however, is quite confusing: I constantly get better performance when I do not apply importance sampling to the actor. Why would this be the case?
","['reinforcement-learning', 'actor-critic-methods', 'experience-replay', 'ddpg']",
What is the difference between backpropagation and predictive coding?,"
Reading the high-level descriptions of backpropagation and predictive coding, they don't sound so drastically different. What is the key difference between these techniques?
I am currently reading the following paper if that helps ground the explanation:
Predictive Coding-based Deep Dynamic Neural Network for Visuomotor
Learning
","['neural-networks', 'machine-learning', 'backpropagation', 'comparison']",
Is it possible to combine k-fold cross-validation and oversampling for a multi-class text classification task with imbalanced data?,"
I am dealing with an intent classification task on an Italian customer service data set.
I've more or less 1.5k sentences and 29 classes (imbalanced).
According to the literature, a good choice is to generate synthetic data, oversampling, or undersampling the training data, using for example the SMOTE algorithm.
I also want to use a cross-validation mechanism (stratified k-fold) to be more confident in the obtained result.
I also know that accuracy is not the right metric to take into account, I should use precision, recall, and confusion matrix.
Is it possible to combine k-fold cross-validation and oversampling (or undersampling) techniques?
","['imbalanced-datasets', 'multiclass-classification', 'k-fold-cv']","It is straightforward to combine k-fold cross-validation with a technique like oversampling or undersampling. First, apply the balance-restoration technique to your training data. Then parametrize a model using k-fold cross-validation on the re-balanced training data. In Scikit learn, I believe you can even bundle these actions together into a single 'pipeline' object to make it easier to manipulate.Precision/recall is probably a fine starting place for measuring performance."
Extract personal information about a person from a list of documents and summarize it,"
I need to extract personal information about a person from a list of documents and summarize it to the user. If there are 2 people with the same name, the correct person should be identified. If the person has a nickname, that also needs to be identified. The input to the program can be the name of the person, address, organization name etc.
I have extracted named entities like person, org, location etc from the text using NLTK library. 
The output after extracting the named entities is mentioned below,
[('Michael', 'NNP', 'B-PERSON'), ('Joseph', 'NNP', 'B-PERSON'), ('Jackson', 'NNP', 'I-PERSON'), ('was', 'VBD', 'O'), ('born', 'VBN', 'O'), ('in', 'IN', 'O'), ('Gary', 'NNP', 'B-GPE'), (',', ',', 'O'), ('Indiana', 'NNP', 'B-GPE')....
Now, I want to extract relationships between those entities.
",['natural-language-processing'],
Siamese Network for unknown object,"
I am currently trying to create a One-Shot network using the Siamese architecture for an object that isn't a face. 
My problem is, in normal Face Recognition the detecting gadget (e.g. Smartphone) knows which image it should compare the face, currently trying to unlock the tool, to.
In my case, I don't know the object's identity so I would have to compare every other object in the database to it.
Is there a better, more efficient way of checking the identity of the object without comparing it to any other object. A normal classification doesn't work in my case, because new unknown objects can be added any time.
","['convolutional-neural-networks', 'image-recognition', 'classification', 'computer-vision']",
How to back-propagate illegal actions for policy gradient learning,"
When training a AI RL agent to play a game there'll be situations where the AI cannot perform certain actions lest they violate the game rules. That's easy to handle, and I can set illegal actions to some large negative amount so when doing an argmax they won't be selected.  Or if I use softmax I can set probabilities of illegal actions to zero and then re-calculate softmax on the remaining legal states. Indeed, I believe this is what David Silver was referring to when asked this question at a presentation/lecture of AlphaZero:
https://www.youtube.com/watch?v=Wujy7OzvdJk&t=2404s
But doing so changes the output from the network and surely changes things when performing the backprop once a reward is known.
How does one handle that?
Would I set the illegal actions to the mean of the legal actions, or zero...?
","['machine-learning', 'reinforcement-learning', 'game-ai', 'backpropagation']",
How to apply or extend the $Q(\lambda)$ algorithm to semi-MDPs?,"
I want to model an SMDP such that time is discretized and the transition time between the two states follows an exponential distribution and there would be no reward between the transition. 
Can I know what are the differences between $Q(\lambda)$ and Q-learning for this problem (SMDP)? I actually want to extend the pseudo-code presented here to an SMDP problem with discretization of time horizon.
","['reinforcement-learning', 'q-learning', 'semi-mdp', 'eligibility-traces']",
Which neural network can count the number of objects in an image?,"
I'm looking for a neural network architecture that excels in counting objects. For example, CNN that can output the number of balls (or any other object) in a given image.
I already found articles about crowd counting. I'm looking for articles about different types of objects.
","['deep-learning', 'convolutional-neural-networks', 'object-detection', 'model-request']","If you want to count the number of objects using a neural network, you can use pretrained YOLO with the bottom prediction layer removed, and feed the features to a classification feed forward layer of let's say 1000 class representing 0-999 objects in the image. You can then train it and propagate the gradients through it. For example, in the pytorch code for YOLO,(source:https://github.com/eriklindernoren/PyTorch-YOLOv3)
You can add a nn.Linear and use cross entropy loss to classify the number of images. You can also change the architecture completely. Maybe you can try adding layers to reset or other classifying network to count the number of objects. Hope this can help you and have a nice day!"
Is there any research on models that make predictions by also taking into account the previous predictions?,"
With the recent revelation of severe limitations in some AI domains, such as self-driving cars, I notice that neural networks behave with the same sort of errors as in simpler models, i.e. they may be ~100% accurate on test data, but, if you throw in a test sample that is slightly different from anything it's been trained on, it can throw the neural network off completely. This seems to be the case with self-driving cars, where neural networks are miss-classifying modified/grafitied Stop Signs, unable to cope with rain or snowflakes, or birds appearing on the road, etc. Something it's never seen before in a unique climate may cause it to make completely unpredictable predictions. These specific examples may be circumvented by training on modified Stop Signs, or with rain and birds, but that avoids the point: that NN's seem very limited when it comes to generalizing to an environment that is completely unique to its training samples. And this makes sense of course given the way NNs train.
The current solution seems to be to manually find out these new things that confuse the network and label them as additional training data. But that isn't an AI at all. That isn't ""true"" generalization.
I think part of the problem to blame is the term ""AI"" in and of itself. When all we're doing is finding a global minimum to a theoretical ideal function at some perfect point before over-fitting our training data, it's obvious that the neural network cannot generalize anymore than what is possible within its training set.
I thought one way that might be possible to get around this is: rather than being static ""one unique calculation at a time"", neural networks could remember the last one or two predictions they made, and then their current prediction, and use the result of that to then make a more accurate prediction. In other words, a very basic form of short-term memory.
By doing this, perhaps, the neural network could see that the raindrops or snowflakes aren't static objects, but are simply moving noise. It could determine this by looking at its last couple of predictions and see how those objects move. Certainly, this would require immense additional computation overhead, but I'm just looking to the future in terms of when processing power increases how NNs might evolve further. Similar to how neural networks were already defined many decades ago, but they were not widely adopted due to the lack of computational power, could this be the same case with something like short-term memory? That we lack the practical computational power for it but that perhaps we could theoretically implement it somehow for some time in the future when we do have it.
Of course, this short-term memory thing would only be useful when a classification is related to the prior classifications, like with self-driving cars. It's important to know what was observed a few seconds ago in real life when driving. Likewise, it could be important for a neural network to know what was predicted a few live classifications ago. This might also have a use in object detection: perhaps, a representation could be learned for a moving figure in the distance. Speed could now become a representation in the hidden layers and be used in assistance for the identification of distant objects, something not possible when using a set of single static weights.
Of course, this whole thing would involve somehow getting around the problem of training weights on a live model for the most recent sample. Or, alternatively, perhaps the weights could still remain static but we'd use two or three different models of weights to represent time somehow.
Nevertheless, I can't help but see a short-term memory of some form as being a requirement, if AI is to not be ""so stupid"", when it observes something unique, and if it's to ever classify things based on time and recent observations.
I'm curious if there's any research papers or other sources that explore any aspect of incorporating time using multiple recent classifications or something else that could be considered a short-term memory of sorts to help reach a more general form of generalization that the neural network doesn't necessarily see in its training, i.e. making it able to avoid noise using time as a feature to help it do so, or using time from multiple recent classifications as a way to estimate speed and use that as a feature?
I'd appreciate the answer to include some specific experiment or methodology as to how this sort of thing might be added to neural networks (or list it as a source), or if this is not an area of active research, why not.
","['reference-request', 'recurrent-neural-networks', 'adversarial-ml', 'generalization', 'model-request']","What you're describing is called a recurrent neural network. There are a large number of designs in this family that all have the ability to remember recent inputs and use them in the processing of future inputs. The ""Long Short Term Memory"" or LSTM architecture was one of the most successful in this family.These are actually very widely used in things like self-driving cars too, so, on their own, they are not enough to overcome the brittleness of current models."
How to encode card game state into neural network input,"
I'm trying create neural network to predict moves in a card game.  I am looking for recommendations on encoding the game state to my input layer.  It's a complex turn based collectible card game (think Magic the Gathering).  I need to represent cards being in various areas of the game board (deck, discard pile, hand, etc).  It seems difficult to assign cards to these areas because the number of cards in these areas is never constant.
I was thinking of an approach where I assign each card in the game to being in a specific card area.  The number of total cards in the game should be constant (let's assume that).  This approach I feel should give me a potentially less sparse input.
Also, With this approach what is the best way to handle card duplicates?  Let's say I have 3 copies of the exact same card in my deck.  Maybe 1 of the copies is in my hand and 2 in my discard pile.  It does not matter which of the 3 copies is in my hand because they are all the same exact card.  There is now multiple ways to represent this same exact game state in my network because each individual card has it's own state.  To me this does not seem good.  How much will this effect my neural network's ability to learn the game?
","['neural-networks', 'game-ai']",
Backpropagation: how to take into account different samples quality,"
I have a NN I'd like to train using supervised learning. Some samples of the training set, however, have better ""quality"" than others, so I'd like the algorithm to pay ""special attention"" to them. As general question, how to take this into account in implementation?
Being more specific, I'm working with OpenCV and noticed that the train method apparently have such parameter:
cv2.ANN_MLP.train(inputs, outputs, sampleWeights[, sampleIdx[, params[, flags]]]) → retval

Where:

sampleWeights – (RPROP only) Optional floating-point vector of weights for each sample. Some samples may be more important than
  others for training. You may want to raise the weight of certain
  classes to find the right balance between hit-rate and false-alarm
  rate, and so on.

However OpenCV documentation is unclear on this, so how to handle this parameter?
","['machine-learning', 'supervised-learning']",
How would an AI visualize a story written in natural language?,"
Can AI transform natural language text describing real scenarios to visual images and videos ? How does as AI interprets say a Harry Potter story if it has to reproduce it in form of videos ? Would be useful if anyone can help me with the required literature for understanding text to image transformation by AI 
","['natural-language-processing', 'image-generation']",
Can AI come up with scientific theories of past when provided with sufficient data available at that time?,"
I would love to know if an AI model could come up with certain theories of the old like Pythagoras' theorem, Euclid's formulations,
Newton's gravity, Einstein's theories if provided and trained with sufficient amount of observable data available at those period of time. If this is possible can unsolved conjectures be proved by AI? Or even better can AI develop new theories or will it  fail to come up with even basic mathematical operations by itself?
","['machine-learning', 'philosophy', 'automated-theorem-proving']",
Use AI to auto-correlate the words of human-translated texts?,"
There are many, many literary works in the public domain, along with human translations, many of which have entered the public domain as well. (Public domain = easily available)
In order for me to advance my knowledge of e.g. Japanese, I'd like to read texts in Japanese using a yet to be written tool, and when I encounter an unknown word/phrase/sentence, I'd like to just click on a position in the text and be transferred to the corresponding position in the e.g. English translation (or original) of the text in question.
Let's also assume we have access to a dictionary that translates a fair amount of words between those two languages (and there are of course free dictionaries for many language pairs).
What ways are there to use AI toolkits plus some wiring and perhaps scripting/programming, to auto-correlate the positions of two versions of the same text, in two languages? The results do not - and in fact in many cases cannot - be perfect, but they should be roughly correct.
I'm aware that this is still not a straightforwards task, as there are complicating factors like inflection of verbs and other grammatical properties that make the use of dictionary tools much harder. Also, translators will often translate to words that don't have that mapping in any dictionary. Then there is the fact that words aren't delimited by spaces in languages like my example language Japanese - (but if it is easier to work with only space-separated languages like, say, Spanish, or Russian, I'd like to hear answers to this simpler problem as well). Also the order of words and even hole sub-clauses differs from language to language.
A simple, non-AI approximation would be to

figure out at what relative position in the source language text the user clicked (e.g. at the character on position 50.234%)
then go to that same relative position 50.234% in the target language text

This approximation could perhaps be used as the starting point for the AI, which would then use words and dictionaries to make the results more accurate.
","['algorithm', 'learning-algorithms']",
Game AI design for a multiplayer random board game?,"
I'm writing an AI for a board game, and previously I would just create a value maximizing state machine and tune the factors one at a time.
However, the issues with this is getting apparent. My last AI did not look into the future, hurting it's long term chances, and manual tuning of weights is proving to be a chore.
I've looked into minimax algorithms, but with a non perfect information game and elements of random chance I'm not too confident it will be effective.
I've also looked into traditional neural networks, but evaluating board states is tricky and the game does not split into moves well.
The game I'm writing the AI for is Ticket To Ride, but I would appreciate tips for any board game with similar mechanics.
",['game-ai'],
ANN’s single output representing value,"
Can ANN with only one neuron in output layer be trained in a way that output neuron’s value (0-1) can be representation of some real value, like for example height.
In other words,can neural network given the inputs predict the height of a person by outputing values from 0 to 1. Zero being the 50 cm and 1 being 250cm.Or it will always gravitate to 0 or 1?Can it predict a height of 150cm (0.5) ?
",['neural-networks'],
What characteristics make it difficult for a Neural Network to approximate a function?,"
What are the characteristics which make a function difficult for the Neural Network to approximate? 
Intuitively, one might think uneven functions might be difficult to approximate, but uneven functions just contain some high frequency terms (which in case of sigmoid is easy to approximate $\frac{1} {(1 + e ^ {-(w*x + b)})}$, by increasing the value of $w$). So uneven data might not be diffcult to approximate.
So my question is what makes a function truly difficult for approximation?
NOTE: By approximation I do not mean things which can be changed by changing the  training method (changing training set size, methods, optimisers). By approximation I mean things which require hyperparameters (size, structure, etc) of a NN to be changed to approximate to a certain level significantly easily.
","['neural-networks', 'machine-learning', 'deep-learning', 'math', 'function-approximation']",
How much the dialects recognition and speech recognition are relevant?,"
In this tutorial, they build a speech recognition model to classify a one-second audio clip as one of ten predefined words. Suppose that we modified this problem as the following: Given an Arabic dataset, we aim to build a dialects recognition model to classify a two-second audio clip as one of $n$ local dialects using ten predefined sentences. I.e. for each of these ten sentences, there are $x$ different phrases and idioms which refer to the same meaning$^*$. Now how can I take advantage of the mentioned tutorial to solve the modified problem?
$*$ The $x$ different phrases and idioms for each sentence are not predefined.
","['deep-learning', 'natural-language-processing', 'classification', 'voice-recognition']","The tutorials you link are not much relevant, there are already existing implementations of your exact problem.You can use https://github.com/swshon/dialectID_e2e, there are many other similar implementations on github."
Does the observation function for POMDP always add up to 1?,"
I was reading in the article A tutorial on partially observable Markov decision processes (p. 120), by Michael L. Littman, that $\sum_{z \in Z}O(a, s',z) =1$, where $a$ is the action, $s'$ the next possible state and $z$ a certain/specific observation.
How come that the observation function $O(a, s', z)$ adds up to $1$ in POMDP?
","['reinforcement-learning', 'markov-decision-process', 'pomdp']","$O(a, s', z) = \mathbb{P}(z \mid a, s')$ is a conditional probability distribution, so it always needs to sum up to $1$. You should interpret $O(a, s', z)$ as the probability of observation $z$, given that the agent took action $a$ and landed in state $s'$. $O(a, s', z)$ is thus not a joint distribution, even though the notation $O(a, s', z)$ might suggest it. In this case, $O(a, s', z)$ simply means that $O$ is a function of $a$, $z$ and $s'$.If you want to see a proof that conditional probability distributions sum up to 1, have a look at this post."
What is the difference between a problem representation and problem modelling?,"
As far as I know, a problem representation is the formulation of the problem in a way that it can be programmed and therefore solved (for example, you can represent the $N$-queens problem by using an array of $N \times N$). 
What does problem modelling mean? What is the difference between a problem representation and problem modelling?
","['comparison', 'terminology', 'norvig-russell']",
Is Intelligence a naturally occurring function of Information Technology?,"
Here intelligence is defined as any analytic or decision making process, regardless of strength (utility), and, potentially, any process of computation that produces output, regardless of the medium.
The idea that AI is part of an evolutionary process, with humans as merely a vehicle of the next dominant species, has been a staple of Science Fiction for many decades.  It informs our most persistent mythologies related to AI.  (Recent examples include Terminator/Skynet, Westworld, Ex Machina, and Alien:Covenant).
What I'm driving at here is that, although the concept of Neural Networks have been around since the 1940's, they have only recently demonstrated strong utility, so it's not an unreasonable assumption to identify Moore's Law as the limiting factor.  (i.e. it is only recently that we have had sufficient processing power and memory to achieve this utility.)
But the idea of AI is ingrained into information technology once automation becomes possible.  Babbage's Difference Engine led to the idea of an Analytic Engine, and the game of Tic-Tac-Toe was proposed as a means of demonstrating intelligence.
What I'm driving at here is that the idea of analysis and decision making are so fundamental in regard to information technology, that it is difficult to see functions that don't involve them.  And, if the strength of analysis and decision making is largely a function of computing power:
Can intelligence be understood as a naturally occurring function of information technology?
","['philosophy', 'mythology-of-ai', 'intelligence']",
Is there a way to run other platforms (other than Atari) in an OpenAI's Gym-like environment? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






Is there a way to run C64, Nintendo Gameboy, or other platforms in an OpenAI's Gym-like environment? It seems that we can run only Atari games.
","['reinforcement-learning', 'open-ai', 'gym']",
What should the range of the output layer be when performing classification?,"
I am working on a MLP neural networks, using supervised learning (2 classes and multi-class classification problems). For the hidden layers, I am using $\tanh$ (which produces an output in the range $[-1, 1]$) and for the output layer a softmax (which gives the probability distribution between $0$ and $1$). As I am working with supervised learning, should be my targets output between 0 and 1, or $-1$ and $1$ (because of the $\tanh$ function), or it does not matter?
The loss function is quadratic (MSE).
","['neural-networks', 'classification', 'activation-functions', 'supervised-learning']",
Translate product names with AI,"
I frequently need to translate product names for hundreds of similar products -- and I have a list of past product names. Is it possible to train AI to review past translations and translate? It doesn't have any special grammar, simply the name (with some industry-specific usage that a general machine translator can't do.) What would I need to do to get started?
","['natural-language-processing', 'machine-translation']",
Dialects classification using deep learning,"
Dialects differ a lot between cities in my country, Syria. People sometimes express themselves using different local phrases and idioms which refer to the same topic. So, I came up with the idea of creating an Android application shows a limited set of sentences or expressions while asking you to express them in the local dialect of your region orally, after that this application tries to figure out what your dialect is. For a short period of time, I'm going to launch an Android application in order to collect the needed dataset which will be a new contribution. First of all, I need some helpful answers to my questions:

In general, is a period of 6 months enough for such a project to be done by only one student who is a beginner in this field or it is harder than it seems?
Are the libraries and tools needed to do this project available for free?
I know that more training data leads to more accurate results. In order to obtain good results, what is the estimated minimum number of training data needed for this model?
How do you advise me to begin?
How much is my suggested project relevant to the project attached in this link?

kindly write down your suggested edits and recommendations if any.
Edit for the 5th question: also see this paper.
","['deep-learning', 'natural-language-processing', 'classification', 'applications', 'voice-recognition']",
"Given specific rewards, how can I calculate the returns for each time step?","
Let's use Excercise 3.8 from Sutton, Barto - Introduction to RL:

Suppose $\gamma = 0.5$ and following sequence of rewards is received
  $R_1=-1$ , $R_2=2$ , $R_3=6$ , $R_4=3$ , $R_5=2$ , with $T=5$ . What
  are $G_0, G_1, ..., G_5?$

There isn't $G_5$ because $R_5$ is last reward. Am I understanding it right? 
So:
$G_4 = 2$
$G_3 = 3 + 0.5*2 = 4$
$G_2 = 6+0.5*4 = 8$
$G_1 = 2+0.5*8 = 6$
$G_0 = -1 +0.5*6 = 2$
","['reinforcement-learning', 'rewards', 'return']","Perfect. To back up your intuition about there not being a $G_5$, refer to the definition of discounted return in the periodic case (3.11). 
$$G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1} R_k$$You'll see that $G_5$ would be written as a sum with no terms in it, since $T=5$."
Teaching a neural network to play a card game with two phases,"
I'm writing a virtual environment for a 4-player card game named estimation, and will use deep reinforcement learning to teach an agent to play it. 
Each player gets a hand of 13 cards, and the first phase is for each player to estimate the number of tricks they will collect. The highest player gets to start first,and then after each round, the player who collects the trick starts the next. So basically the first phase is for bidding and the next phase consists of 13 rounds. 
The state input I'll use will include all the cards that have been played, the goal and collected tricks, and the available cards. The output for each round will be a vector of length 54, containing all the cards and then the available card with the highest probability would be played.
At first I thought that the bidding phase should use the same input but with zeros everywhere except the available hand, and the output would exclude all the cards with no numbers like king, queen or jack. But then the ability to dash (estimate that you'll collect 0 tricks) wouldn't be available. Also I don't think it would work really well.
Should I just use two NNs for each phase, or what should I do? Also if anyone has any advice on things I need to watch out for, I'd really appreciate it if they shared them.
","['neural-networks', 'reinforcement-learning']",
"Difference in continuing and episodic cases in Sutton and Barto - Introduction to RL, exercise 3.5","

Excercise 3.5 The equastions in Section 3.1 are for the continuing
  case and need to be modified (very slightly) to apply to episodic
  tasks. Show that you know the modifications needed by giving the
  modified version of (3.3).

$\displaystyle\sum_{s^{\prime} \in S} \displaystyle\sum_{r \in R} = p(s^{\prime}, r 
| s,a) = 1$ , for all $s\in S, a \in A(s)$    (3.3)
Is it just about final states? So for $s \in S$ when S is not final? 
","['reinforcement-learning', 'markov-decision-process', 'sutton-barto', 'markov-chain']","Is it just about final states? So for $s \in S$ when S is not final? You are thinking the right way, but to represent what you mean you don't need to write out ""when $s$ is not final"" - although that would be fine (and is used in some places), there is a more concise way of saying that given to you by the book.As this is a formal exercise from the book, I don't want to write out an answer that could be cut&paste for all students.Instead I suggest you take a look at the notations section at the beginning of the book, and find how Sutton & Barto use different set labels for all states including terminal states, and all states excluding terminal states. Also, check carefully which of those sets needs to be summed over."
Finding anomaly detection by pattern matching in a set of continous data,"
I have series of sensors (around 4k) and each sensor will measure the amplitudes at each point.Suppose I train the neural network with sufficent set of 4k values (N * 4k shape). The machine will find a pattern in the series of values.If the values stray away from the pattern (that is anomaly)  it can detect the point and will be able to say that anomaly is in the 'X'th  sensor.Is this possible.If so what kind of neural network should I use?
","['neural-networks', 'machine-learning', 'deep-learning', 'pattern-recognition']",
Why does a fully connected layer only accept a fixed input size?,"
I'm studying how SPP (Spatial, Pyramid, Pooling) works. SPP was invented to tackle the fix input image size in CNN. According to the original paper https://arxiv.org/pdf/1406.4729.pdf, the authors say: 

convolutional layers do not require a fixed image size and can
  generate feature maps of any sizes. On the other hand, the
  fully-connected layers need to have fixed size/length input by their
  definition. Hence, the fixed size constraint comes only from the
  fully-connected layers, which exist at a deeper stage of the network.

Why does a fully connected layer only accepts a fixed input size (but convolutional layers don't)? What's the real reason behind this definition?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']","A convolutional layer is a layer where you slide a kernel or filter (which you can think of as a small square matrix of weights, which need to be learned during the learning phase) over the input. In practice, when you need to slide this kernel, you will often need to specify the ""padding"" (around the input) and ""stride"" (with which you convolve the kernel on the input), in order to obtain the desired output (size). So, even if you receive inputs of different sizes, you can change these values, like the padding or the stride, in order to produce a valid output (size). In this sense, I think, we can say that convolutional layers accept inputs of (almost) any size.The number of feature maps does not depend on the kernel or input (sizes). The number of features maps is determined by the number of different kernels that you will use to slide over the input. If you have $K$ different kernels, then you will have $K$ different feature maps. The number of kernels is often a hyper-parameter, so you can change it (as you please).A fully connected (FC) layer requires a fixed input size by design. The programmer decides the number of input units (or neurons) that the FC layer will have. This hyper-parameter often does not change during the learning phase. So, yes, FC often accept inputs of fixed size (also because they do not adopt techniques like ""padding"")."
Is there any grid world dataset or generator for reinforcement learning?,"
I would like to start programming a multi task reinforcement learning model. For this, I need not just one maze or grid world (or just model-based), but many with different reward functions. So, I am wondering if exists a dataset or a generator for such thing, or do I need to code everything by my self?
","['reinforcement-learning', 'q-learning', 'model-based-methods', 'value-functions']",
Automation the import of files to Database,"
I don't know if this it's possible but nowadays as almost everything is possible I am asking to see if anyone has any idea.
The problem is:

Regularly I have to import files (CSV, XML, Excel, ...) to an SQL Server Database and after I import the files I have to map the content of the columns of the imported files in a .sql Script to run and insert in a table in another DB (this table is the same for all the files imported).
I spend a lot of time on these mappings as you can imagine, I want to know if it's possible through Artificial Intelligence or Machine Learning one solution that can make the import of the initial data to the final DB, the initial import I can make it automatically the problem is only the import of the imported files to the final DB.
Note: The initial files don't have the same columns names, each file can have different columns names.

Sorry for the long post but I don't know if it is possible something like that. I searched and don't find anything.
","['machine-learning', 'automation']",
A few questions regarding the difference between policy iteration and value iteration [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



The question already has some answer. But I am still finding it quite unclear (also does $\pi(s)$ here mean $q(s,a)$ ?):

The few things I do not understand are:

Why the difference between 2 iterations if we are acting greedily in each of them? As per many sources 'Value Iteration' does not have an explicit policy, but here we can see the policy is to act greedily to current $v(s)$
What exactly does Policy Improvement mean? Are we acting greedily only at a particular state at a particular iteration OR once we act greedily on a particular state we keep on acting greedily on that state and other states are added iteratively until in all states we act greedily?
We can intuitively understand that acting greedily w.r.t $v(s)$ will lead to $v^*(s)$ eventually, but does using Policy Iteration eventually lead to $v^*(s)$?

NOTE: I have been thinking of all the algorithms in context of Gridworld, but if you think there is a better example to illustrate the difference you are welcome.
","['reinforcement-learning', 'policies', 'value-iteration']","$\pi(s)$ does not mean $q(s,a)$ here. $\pi(s)$ is a policy that represents probability distribution over action space for a specific state. $q(s,a)$ is a state-action pair value function that tells us how much reward do we expect to get by taking action $a$ in state $s$ onwards.  For the value iteration on the right side with this update formula:  $v(s) \leftarrow \max_\limits{a} \sum_\limits{s'}p(s'\mid s, a)[r(s, a, s') + \gamma v(s')]$ we have an implicit greedy deterministic policy that updates value of state $s$ based on the greedy action that gives us the biggest expected return. When the value iteration converges to its values based on greedy behaviour after $n$ iterations we can get the explicit optimal policy with:  $\pi(s) = \arg \max_\limits{a} \sum_\limits{s'} p(s'\mid s, a)[r(s, a, s') + \gamma v(s')]$ here we are basically saying that the action that has highest expected reward for state $s$ will have probability of 1, and all other actions in action space will have probability of 0For the policy evaluation on the left side with this update formula:  $v(s) \leftarrow \sum_\limits{s'}p(s'\mid s, \pi(s))[r(s, \pi(s), s') + \gamma v(s')]$ we have an explicit policy $\pi$ that is not greedy in general case in the beginning. That policy is usually randomly initialized so the actions that it takes will not be greedy, it means we can start with policy that takes some pretty bad actions. It also does not need to be deterministic but I guess in this case it is. Here we are updating value of state $s$ according to the current policy $\pi$.
After policy evaluation step ran for $n$ iterations we start with the policy improvement step:  $\pi(s) = \arg \max_\limits{a} \sum_\limits{s'} p(s'\mid s, a)[r(s, a, s') + \gamma v(s')]$ here we are greedily updating our policy based on the values of states that we got through policy evaluation step. It is guaranteed that our policy will improve but it is not guaranteed that our policy will be optimal after only one policy improvement step. After improvement step we do the evaluation step for new improved policy and after that we again do the improvement step and so on until we converge to the optimal policy"
What is the best machine learning algorithm to select best 3 variable combinations?,"
I have 10 variables as like below

$V_1=1$, $V_2=2$, $V_3=3$, $V_4=4$, $V_5=5$, $V_6=6$, $V_7=7$, $V_8=8$, $V_9=9$ and $V_{10}=10$

Note: Each variable can have any value
Now I want to select the best 3 variables combination as like below

$V_1V_3V_4$ or $V_{10}V_1V_7$ or $V_5V_3V_9$ etc.

The best combination is nothing but the sum of the values of 3 variables in the combination.
Example:

Combination 1($V_1V_2V_3$) : 1+2+3 $\Rightarrow$ 6
Combination 2($V_8V_9V_{10}$) : 8+9+10 $\Rightarrow$ 27

In the above example Combination 2($V_8V_9V_{10}$) has the highest sum value. So the Combination 2($V_8V_9V_{10}$) is the best combination here.
Like this, if I have a large number of variables means which machine learning algorithm selects the best combination in all the sense.
Suggest me the best machine learning algorithm for selecting the best variable combinations. Thanks in advance.
",['machine-learning'],
What is the Bellman operator in reinforcement learning?,"
In mathematics, the word operator can refer to several distinct but related concepts. An operator can be defined as a function between two vector spaces, it can be defined as a function where the domain and the codomain are the same, or it can be defined as a function from functions (which are vectors) to other functions (for example, the differential operator), that is, a high-order function (if you are familiar with functional programming).

What is the Bellman operator in reinforcement learning (RL)?
Why do we even need it?
How is the Bellman operator related to the Bellman equations in RL?

","['reinforcement-learning', 'terminology', 'math', 'bellman-equations', 'bellman-operators']","The notation I'll be using is from two different lectures by David Silver and is also informed by these slides.The expected Bellman equation is
$$v_\pi(s) = \sum_{a\in \cal{A}} \pi(a|s) \left(\cal{R}_s^a + \gamma\sum_{s' \in \cal{S}} \cal{P}_{ss'}^a v_\pi(s')\right) \tag 1$$If we let
$$\cal{P}_{ss'}^\pi = \sum\limits_{a \in \cal{A}} \pi(a|s)\cal{P}_{ss'}^a \tag 2$$
and
$$\cal{R}_{s}^\pi = \sum\limits_{a \in \cal{A}} \pi(a|s)\cal{R}_{s}^a \tag 3$$
then we can rewrite $(1)$ as$$v_\pi(s) = \cal{R}_s^\pi + \gamma\sum_{s' \in \cal{S}} \cal{P}_{ss'}^\pi v_\pi(s') \tag 4$$This can be written in matrix form$$\left.
\begin{bmatrix}
v_\pi(1) \\
\vdots \\
v_\pi(n)
\end{bmatrix}=
\begin{bmatrix}
\cal{R}_1^\pi \\
\vdots \\
\cal{R}_n^\pi
\end{bmatrix}
+\gamma
\begin{bmatrix}
\cal{P}_{11}^\pi & \dots & \cal{P}_{1n}^\pi\\
\vdots & \ddots & \vdots\\
\cal{P}_{n1}^\pi & \dots & \cal{P}_{nn}^\pi
\end{bmatrix}
\begin{bmatrix}
v_\pi(1) \\
\vdots \\
v_\pi(n)
\end{bmatrix}
\right. \tag 5$$Or, more compactly,$$v_\pi = \cal{R}^\pi + \gamma \cal{P}^\pi v_\pi \tag 6$$Notice that both sides of $(6)$ are $n$-dimensional vectors. Here $n=|\cal{S}|$ is the size of the state space. We can then define an operator $\cal{T}^\pi:\mathbb{R}^n\to\mathbb{R}^n$ as$$\cal{T^\pi}(v) = \cal{R}^\pi + \gamma \cal{P}^\pi v \tag 7$$for any $v\in \mathbb{R}^n$. This is the expected Bellman operator.Similarly, you can rewrite the Bellman optimality equation$$v_*(s) = \max_{a\in\cal{A}} \left(\cal{R}_s^a + \gamma\sum_{s' \in \cal{S}} \cal{P}_{ss'}^a v_*(s')\right) \tag 8$$as the Bellman optimality operator$$\cal{T^*}(v) = \max_{a\in\cal{A}} \left(\cal{R}^a + \gamma \cal{P}^a v\right) \tag 9$$The Bellman operators are ""operators"" in that they are mappings from one point to another within the vector space of state values, $\mathbb{R}^n$.Rewriting the Bellman equations as operators is useful for proving that certain dynamic programming algorithms (e.g. policy iteration, value iteration) converge to a unique fixed point. This usefulness comes in the form of a body of existing work in operator theory, which allows us to make use of special properties of the Bellman operators.Specifically, the fact that the Bellman operators are contractions gives the useful results that, for any policy $\pi$ and any initial vector $v$,$$\lim_{k\to\infty}(\cal{T}^\pi)^k v = v_\pi \tag{10}$$$$\lim_{k\to\infty}(\cal{T}^*)^k v = v_* \tag{11}$$where $v_\pi$ is the value of policy $\pi$ and $v_*$ is the value of an optimal policy $\pi^*$. The proof is due to the contraction mapping theorem."
Does humanism grant moral consideration to sentient artificial general intelligences?,"
Some formulations of humanism already grant moral consideration to sentient non-human animals (e.g. https://humanists.international/what-is-humanism/). Does humanism also extend to granting rights to AGIs, should they become sentient?
Sentientism is a closely related philosophy that makes this explicit https://en.wikipedia.org/wiki/Sentientism.
","['philosophy', 'ethics']",
"How to deal with a huge action space, where, at every step, there is a variable number of legal actions?","
I am working on creating an RL-based AI for a certain board game. Just as a general overview of the game so that you understand what it's all about: It's a discrete turn-based game with a board of size $n \times n$ ($n$ depending on the number of players). Each player gets an $m$ number of pieces, which the player must place on the board. In the end, the one who has the least number of pieces wins. There are of course rules as to how the pieces can be placed so that not all placements are legal at every move.
I have the game working in an OpenAI's gym environment (i.e. control by step function), have the board representation as the observation, and I have defined the reward function.
The thing I am struggling with right now is to meaningfully represent the action space.
I looked into how AlphaZero tackles chess. The action space there is $8*8*73 = 4672$: for every possible tile on the board, there are 73 movement-related modalities. So, for every move, the algorithm comes up with 4672 values, the illegal ones are set to zero and non-zero ones are re-normalized.
Now, I am not sure if such an approach would be feasible for my use-case, as my calculations show that I have a theoretical cap of ~30k possible actions ($n * n * m$) if using the same way of calculation. I am not sure if this would still work on, especially considering that I don't have the DeepMind computing resources at hand.
Therefore, my question: Is there any other way of doing it apart from selecting the legal actions from all theoretically possible ones?
The legal actions would be just a fraction of the ~30k possible ones. However, at every step, the legal actions would change because every new piece determines the new placement possibilities (also, the already placed pieces are not available anymore, i.e. action space generally gets smaller with every step).
I am thinking of games, like Starcraft 2, where action space must be larger still and they demonstrate good results, not only by DeepMind but also by private enthusiasts with for example DQN.
I would appreciate any ideas, hints, or readings!
","['reinforcement-learning', 'dqn', 'game-ai', 'action-spaces', 'board-games']",
Is there a way to train an RL agent without any environment?,"
Following Deep Q-learning from Demonstrations, I'd like to avoid potentially unsafe behavior during early learning by making use of supervised learning with demonstration data. However, the implementation I'm following still uses an environment. Can I train my agent without an environment at all?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'environment']","There are many techniques for training an RL agent without explicitly interacting with an environment, some of which are cited in the paper you linked. Heck, even using experience replay like in the foundational DQN paper is a way of doing this. However, while many models utilize some sort of pre-training for the sake of safety or speed, there are a couple of reasons why an environment is also used whenever possible.Eventually, your RL agent will be placed in an environment to take its own actions. This is why we train RL agents. I'm assuming that, per your question, learning does not happen during this phase. Maybe your agent encounters a novel situation
Hopefully, the experience your agent learns from is extensive enough to include every possible state-action pair $(s,a)$ that your agent will ever encounter. If it isn't, your agent won't have learned about these situations, and it will always perform suboptimally in them. This lack of coverage over the state-action space could be caused by stochasticity or nonstationarity in the environment.Maybe the teacher isn't perfect
If you don't allow your agent to learn from its own experience, it will only ever perform as well as the agent that collected the demonstration data. That's an upper bound on performance that we have no reason to set for ourselves."
Isolate the speech of two people in an audio record with two people only,"
I would like to find a way to isolate the speech of each of the people in an audio record so I can create a file of that form :
[
   {
       ""voice_fingerprint"": ""701066EDD3A0A40A2F53F61EAFD0E6AB"",
       ""sentences"": {
           {
               ""sentence"": ""do you like red apples"",
               ""position"": 1.39 // Seconds. Time position in the audio record
           },
           {
               ""sentence"": ""and how do you feel about time shifts"",
               ""position"": 7.21
           }
       }
   },
   {
       ""voice_fingerprint"": ""8FFEA051AF3E3FB9A80A51A98FE05896"",
       ""sentences"": {
           {
               ""sentence"": ""yes I do like them"",
               ""position"": 4.54
           },
           {
               ""sentence"": ""i feel well about traveling"",
               ""position"": 10.18
           }
       }
   }
]

This may be an interview record.
The problem IS NOT the Speech to Text, but to isolate the two people's sentences. Preferably in Python.
Have you ever worked on this ? Do you have any hints ?
","['machine-learning', 'natural-language-processing', 'python', 'audio-processing']",
Where can I find an implementation of the wake-sleep algorithm?,"
I'm looking to build from scratch an implementation of the wake-sleep algorithm, for unsupervised learning with neural networks. I plan on doing this in Python in order to better understand how it works. In order to facilitate my task, I was wondering if anyone could point me to an existing (open-source) implementation of this concept. I'm also looking for articles or, in general, resources that could facilitate this task.
","['neural-networks', 'python', 'unsupervised-learning', 'resource-request', 'wake-sleep-algorithm']","I found the following detailed and well documented Python notebook, which uses only NumPy."
What is the purpose of the new neurons in the constrained neural network?,"
I would like to train a constrained neural network. I found a paper on this: https://papers.nips.cc/paper/4-constrained-differential-optimization.pdf.
However, I don't really understand how to change my feedforward neural network. I don't understand exactly the role of the new output neurons, which serve as lagrangian multipliers. Can someone explain this to me in more detail? 
Which steps in backpropagation do I have to change?
","['neural-networks', 'optimization']",
Learning similarities between customers and offers representation,"
I am interested in a framework for learning the similarity of different input representations based on some common context. I have looked into word2vec, SVD and siamese networks, all of which are similar to what I want. 
For example, suppose we have some customers we are sending different advertisements to, and I would like to create a system to map offers to customers. I am thinking in the lines of creating a customer representation, and a representation of the offers, and feeding them in parallel to a neural network that has a label of whether they acted on the advertisement or not. The idea is that I should be able to locate the best offer for any customer given these representations.
I have looked into siamese networks and word2vec, both are close to what I want. The problem differs slightly in that for the siamese networks, it tends to be identical parallel networks, which I don't want because my inputs are not equivalent. And for word2vec the vectors tend to be in the same domain, while I want to apply this in a more general setting. 
If anyone has any resources on a similar problem statement, I would be very interested in it.
","['deep-learning', 'knowledge-representation', 'word2vec', 'recommender-system']","The idea is that I should be able to locate the best offer for any customer given these representations.I think you need a Recommender System. As you want to map the offers to customers based one their representation you can check Content-Based Recommender System.The method is by taking pattern from a customer history and try to find similarities with the new offers. You can use many techniques for a recommender system, from a simple TF-IDF or Deep Learning for more complex problems."
Which pathfinding algorithms can be applied on coloured graphs?,"
Are there any (well validated) approaches for applying pathfinding algorithms on a graph following specific rules?
To be more specific: I want to introduce a graph with coloured edges. The idea is to apply a well known pathfinding algorithm (such as Dijkstra) on the graph given the rule: ""only black and red edges"".
",['path-finding'],
What does 'acting greedily' mean?,"
I wanted to clarify the term 'acting greedily'. What does it mean? Does it correspond to the immediate reward, future reward or both combined? I want to know the actions that will be taken in 2 cases:

$v_\pi(s)$ is known and $R_s$ is also known (only).
$q_{\pi}(s, a)$ is known and $R_s^a$ is also known (only).

","['reinforcement-learning', 'greedy-ai']","In RL, the phrase ""acting greedily"" is usually short for ""acting greedily with respect to the value function"". Greedy local optimisation turns up in other contexts, and it is common to specify what metric is being maximised or minimised. The value function is most often the discounted sum of expected future reward, and also the metric used when defining a policy as ""acting greedily"".It is possible to define a policy as acting greedily with respect to immediate expected reward, but not the norm. As a special case, when the discount factor $\gamma = 0$ then it is the same as acting greedily with respect to the value function.When rewards are sparse (a common situation in RL), acting greedily with respect to expected immediate reward is not very useful. There is not enough information to make a decision.I want to know the actions that will be taken in 2 cases:To act greedily in RL, you would use the value function $v_\pi(s')$ - the value function of the next states. To do so, you need to know the environment dynamics - to go with the notation $R_s$ you should also know the transition matrix $P_{ss'}^a$ - the probability of transitioning from $s$ to $s'$ whilst taking action $a$:$$\pi'(s) = \text{argmax}_a \sum_{s'} P_{ss'}^a(R_{s} + \gamma v_{\pi}(s'))$$Notes:This assumes $R_{s}$ is your immediate reward for leaving state $s$. Substitute $R_{s'}$ if the reward matrix is for entering state $s'$ instead.The policy gained from acting greedily with respect to $v_\pi(s)$ is not $\pi$, it is (a usually improved) policy $\pi'$To act greedily in RL, you would use the value function $q_{\pi}(s, a)$, and this is much simpler:$$\pi'(s) = \text{argmax}_a q_{\pi}(s, a)$$Again, the policy gained from acting greedily with respect to $q_\pi(s,a)$ is not $\pi$, it is (a usually improved) policy $\pi'$"
Why is a constant plane of ones added into the input features of AlphaGo?,"
In the paper Mastering the game of Go with deep neural networks and tree search, the input features of the networks of AlphaGo contains a plane of constant ones and a plane of constant zeros, as following.
Feature       #of planes Description 
Stone colour  3          Player stone/opponent stone/empty 
Ones          1          A constant plane ﬁlled with 1 
Turns since   8          How many turns since a move was played 
Liberties     8          Number of liberties (empty adjacent points) 
Capture size  8          How many opponent stones would be captured 
Self-atari size 8        How many of own stones would be captured 
Liberties after move 8   Number of liberties after this move is played 
Ladder capture 1         Whether a move at this point is a successful ladder capture 
Ladder escape 1          Whether a move at this point is a successful ladder escape 
Sensibleness  1          Whether a move is legal and does not ﬁll its own eyes 
Zeros         1          A constant plane ﬁlled with 0 
Player color  1          Whether current player is black

I wonder why these features are necessary, because I think a constant plane contains no information and it makes the the network larger and consequently harder to train.
What's more, I don't understand the sharp sign here. Does it mean ""the number""? But one number is enough to represent ""the number of turns since a move was played"", why eight?
Thank you very much.
","['machine-learning', 'alphago']",
Would the people of the 19th Century call our conventional software today artificial intelligence? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            



Closed 1 year ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






It is possible that the view of what is impressive enough in computer behavior to be called intelligence changes with each decade as we adjust to what capabilities are made available in products and services.
","['philosophy', 'social', 'history']",
Attempting to solve a optical character recognition task using a feed-forward network,"
I am doing some experimentation on neural networks, and for that I am trying to program a plain OCR task. I have learned CNNs are the best choice ,but for the time being and due to my inexperience, I wanna go step by step and start with feedforward nets.
So my training data is a set of roughly 400 16*16 images extracted from a script that draws every alphabet char in a tiny image for a small set of fonts registered in my computer.
Then the test data set is extracted from the same procedure, but for all fonts in my computer.
Well, results are quite bad. Get an accuracy of aprox. 45-50%, which is very poor... but that's not my question.
The point is that I can't get the MSE below 0.0049, no matter what hidden layer distribution I apply to the net. I have tried with several architectures and all comes down to this figure. Does that mean the net cannot learn any further given the data?
This MSE value however throws this poor results too.
I am using Tensorflow API directly, no keras or estimators and for a list of 62 recognizable characters these are examples of the architectures I have used: [256,1860,62] [256,130,62] [256,256, 128 ,62] [256,3600,62] ....
But never get the MSE below 0.0049, and still results are not over 50%.
Any hints are greatly appreciated.
","['feedforward-neural-networks', 'optical-character-recognition']",
What are examples of applications of the Fourier transform to AI?,"
The (discrete and continuous) Fourier transform (FT) is used in signal processing in order to convert a signal (or function) in a certain domain (e.g. the time domain) to another domain (e.g., frequency domain). There are several resources on the web that attempt to explain the FT at different levels of complexity. See e.g. this answer or this and this Youtube videos.
What are examples of (real-world) applications of the Fourier transform to AI? I am looking for answers that explain the reason behind the use of the FT in the given application. I suppose that there are several applications of the FT to e.g. ML (data analysis) and robotics. I am looking for specific examples.
","['machine-learning', 'applications', 'robotics', 'fourier-transform']",
How accurate are neuroevolution algorithms in modelling organism evolution?,"
How accurate are neuro-evolution algorithms (such as NEAT) in modelling real organism evolution?
","['evolutionary-algorithms', 'neat', 'neuroevolution']","Modelling genetic code persistence, mutation, and meiosis for neurological features or any other biological features is not the greatest of challenges. Modelling the rest of the organism that can support neural growth and change, the electrochemistry of the many types of neurons, and the environment that favors the emergence of learned neural behaviors is challenging and still far beyond the current level of technology."
How do the sine and cosine functions encode position in the transformer?,"
After going through both the ""Illustrated Transformer"" and ""Annotated Transformer"" blog posts, I still don't understand how the sinusoidal encodings are representing the position of elements in the input sequence. 
Is it the fact that since each row (input token) in a matrix (entire input sequence) has a unique waveform as its encoding, each of which can be expressed as a linear function of any other element in the input sequence, then the transformer can learn relations between these rows via linear functions? 
","['deep-learning', 'natural-language-processing', 'transformer', 'attention', 'positional-encoding']",
How do two perceptrons produce different linear decision boundaries when learning?,"
I've learned that you can use two perceptrons to ultimately create a classifier for non-linearly separable data. I'm trying to understand how / if  these two perceptrons converge to two different decision boundaries though.

Source: https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html
I don't understand how the second perceptron creates a different decision boundary when it has the same input as the first perceptron? I know the weights can be initialized differently but does this second perceptron classify something else? Shouldn't the decision boundaries be converge to be the the same ultimately after training?

Source: https://tdb-alcorn.github.io/2017/12/17/seeing-like-a-perceptron.html
","['neural-networks', 'activation-functions', 'perceptron']",
How can/should I use AI to populate a game (in the game theory sense) from text input,"
I'm wanting to conduct game theoretic analyses of ongoing conflict situations (e.g. the US/North Korea negotiations; Syrian conflict; etc)  as reported in the news media. I believe that AI may help me do this by helping me to pick out from the text: the parties involved; the issues over which they are in conflict; the choices they have; their preferences.  However I'm not sure whether to approach this using 'modern' 'deep learning' approaches or to try something along the lines of the classic work by Schank, deJong etc. who used the notion of scripts (and sketchy scripts) in their work with conceptual dependency approaches.  Does anyone have comments, suggestions that may guide my work please?
","['deep-learning', 'natural-language-processing', 'game-theory']",
Can particle swarm optimization be used to train neural networks with more than one hidden layer?,"
I've been thinking about the idea of replacing the classic gradient descent algorithm with an algorithm that is less sensitive to a local optimum. I was thinking about particle swarm optimization (PSO), which thus tries to select the best weights and biases for the model.
But I've seen everywhere that only one hidden layer is used (no one explains why just one layer is being used) and all those codes break when I try to use more than one hidden layer, so the questions are:

Can't PSO be used to optimize an Artificial Neural Network with more than one hidden layer?

In that case, why is that?


","['neural-networks', 'deep-learning', 'training', 'hidden-layers', 'particle-swarm-optimization']","Particle Swarm Optimization can be used to optimize a neural network with more than one hidden layer. Instead of optimizing a single weight matrix, and two bias vectors, you are just optimizing more of them.However, PSO is not often used for larger neural networks, because, particle swarm optimization is not all that efficient at working with a large amount of data, which you need to train larger neural networks. Particle Swarm Optimization involves taking many particles, and evaluating the fitness in all of them. If you have, let's say, 1,000,000 training examples, you are doing a lot more error calculations, even if you used a small batch size, than if you used another technique lake back-propagation or a related technique. Backpropagation and related techniques are used much more in larger neural networks because they are more efficient at working with larger sets of data.As for why the code that you have been using breaks with networks with more than one hidden layer, I cannot explain, but it is fairly easy to write your own basic implementation of PSO to train multi-layer neural networks."
How do I use GPT-2 to summarise text?,"
In section 3.6 of the OpenAI GPT-2 paper it mentions summarising text based relates to this, but the method is described in very high-level terms:

To induce summarization behavior we add the text TL;DR: after the article and generate 100 tokens with Top-k random sampling (Fan et al., 2018) with k=2 which reduces repetition and encourages more abstractive summaries than greedy decoding. We use the first 3 generated sentences in these 100 tokens as the summary.

Given a corpus of text, in concrete code terms (python preferred), how would I go about generating a summary of it?
","['open-ai', 'text-summarization', 'text-generation', 'gpt-2']",
Is there a neural network with a varying number of neurons?,"
Is there some type of neural network that changes the number of neurons while training? 
Using this idea, the network can increase or decrease the number of neurons when the complexity of the inputs increases or decreases.
","['neural-networks', 'neat', 'neuroevolution']","Yes, NEAT (NeuroEvolution of Augmenting Topologies) increases the number of neurons during training. More specifically, NEAT uses evolution to introduce new neurons and connections during training, and - just as evolution - if the mutation performs poorly, gets eliminated after a few generations. This way overall performance increases over time while it keeps your network size (and computing power to run it) minimal. There's also a way to add convolution to this algorithm.In the paper that introduced NEAT, the authors mention a completely different algorithm they tried, optimizing network hyper-parameters by choosing reasonable random values and training it multiple times. That could decrease that number as well.Also, there is a trick to temporarily turn off specific parts of a network, which supposedly helps with overfitting.A ReLU+backpropagation based network can ""turn off"" parts of the network during training, because constant 0's derivative is constant 0. In practice, you decrease the number of neurons (that is considered bad though, that's when leaky ReLU and PReLU is used instead)."
What would be the steps to create an sentiment analysis chatbot?,"
We have been assigned a project, in which we have to create a chatbot which will ask question, take the replies, analyse them and give an approximate assessment of the current emotional state of the person. There are two aspects of the project, 

Training the bot to choose the next question based on the previous response 
And analysing the responses individually, to detect . the sentiment.

What technology would we have to use and what would be the steps to accomplishing the tasks?
Thanks.
","['machine-learning', 'sentiment-analysis']",
Why is MSE used over other quadratic loss functions?,"
So I was wondering, why I have only encountered square loss function also known as MSE. The only nice property of MSE I am so far aware of is its convex nature. But then all equations of the form $x^{2n}$ where $n$ is an integer belongs to the same family.
My question is what makes MSE the most suitable candidate among this entire family of curves? Why do other curves in the family, even though having steeper slopes, $(x >1) $, which might result in better optimisation, not used?
Here is a picture to what I mean where red is $x^4$ and green is $x^2$:

","['neural-networks', 'machine-learning', 'deep-learning', 'math', 'objective-functions']","I can comment on several properties of MSE and related losses.As you mentioned MSE (aka $l_2$-loss) is convex which is a great property in optimization in which one can find a single global optimum. MSE is used in linear and non-linear least squares problems which form the basis of many widely used statistical methods. I would imagine the math and implementation would be more difficult if one would use a higher-order loss (e.g. $x^3$) and that would also prove to be futile because MSE already possesses great statistical and optimization properties on its own.
Another important aspect, one wouldn't use higher-order loss functions in regression is because it would be extremely prone to outliers. MSE on its own would weigh the outliers much more than l1-loss would! And in real world data there is always noise and outliers present. In comparison l1 loss is more difficult in optimization, one reason for which is it's not differentiable at zero.Other interesting losses you might want to read about are $l_0$ and $l_{inf}$ loss, all of which have their own trade-offs in optimization-sense."
Can A3C update the policy / critic on a local machine without needing to copy?,"
To make A2C into A3C you make it asynchronous. From what I understand the 'correct' way to do that is to thread off workers with a copy of the policy and critic, and then return the state/action/reward tuples to the main thread, which then performs the gradients updates on the main policy and critic, and then repeat the process.
I understand why the copying would be necessary in a distributed environment, but if I were to always run it locally then could I just perform the updates on a global variable of the policy and critic, i.e. avoid the need for copying? Provided the concurrency of the updates was handled correctly, would that be fine?
","['reinforcement-learning', 'actor-critic-methods']",
Predicting sine using LSTM: Small output range and delayed output?,"
I have coded a very basic LSTM with forget gates (no libraries used). I'm trying to predict $0.5*sin(t + N)$ given $0.5*sin(t)$ as an exercise.
I have tweaked the model, changing the output layer activation function, weight initialization, number of memory blocks/cells, etc. However, I still couldn't manage to correct the output.
The problem is that the output range is much smaller than desired, $[-0.2, 0.2]$ instead of $[-0.5, 0.5]$. The output also is slightly delayed, meaning it is predicting $sin(t + N - 1)$ for example.
Is there something that I'm missing?
As an example, for output layer activation function as a centered logistic from $(-1, 1)$, the validation output looks like

Training output looks like

Additional information:

Topology: 1 input layer, 1 hidden layer each with 5 memory blocks each with 1 cell, 1 output layer each with 1 regular neuron.
Alpha: 1
Weights: generated with normal distribution, from $[-1, 1]$
Output layer activation function used: logistic $[0, 1]$, centered logistic, tanh, ReLU, leaky ReLU, $f(x) = x$ (identity)

","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'long-short-term-memory']",
Why would someone use NEAT over other machine learning algorithms?,"
Why would someone use a neuroevolution algorithm, such as NEAT, over other machine learning algorithms? What situation would only apply to an algorithm such as NEAT, but no other machine learning algorithm?
","['machine-learning', 'neat', 'comparison', 'neuroevolution']","The main difference leading to strengths and weaknesses of NEAT algorithm, is that it does not use any gradient calculations. That means for NEAT, neither the cost function, nor the activation functions of the neurons are required to be differentiable. In some circumstances - e.g. where agents directly compete, and you can select them for next generation if they win versus one or more opponents - you may not even need a cost function to optimise. Quite often the cost function can be a simple number generated from a complex evaluation of the network. Therefore, NEAT can be used in situations where the formula for a cost function, in terms of single feed-forward runs of the network, is not very clear. It can also be used to explore activation functions such as step functions or stochastic firing neurons, where gradient based methods are difficult to impossible to apply.NEAT can perform well for simple control scenarios, as a policy network that outputs actions given some sensor inputs. For example, it is popular to use it to create agents for racing games or simulated robotics controllers. When used as a policy-based controller, NEAT is in competition with Reinforcement Learning (RL), and has basic similarities with policy gradient methods - the nature of the controller is similar and often you could use the same reward/fitness function.As NEAT is already an evolution-inspired algorithm, it also fits well with a-life code as the ""brains"" of simulated creatures.The main disadvantage of NEAT is slow convergence to optimal results, especially in complex or challenging environments. Gradient methods can be much faster, and recent advances in Deep RL Policy Gradients (algorithms like A3C and DDPG) means that RL can tackle much more complex environments than NEAT. I would suggest to use NEAT when:The problem is easy to assess - either via a measurement of performance or via competition between agents - but might be hard to specify as a loss functionA relatively small neural network should be able to approximate the target functionThere is a goal to assess a non-differentiable activation functionIf you are looking at a sequential control problem, and could use a standard feed-forward neural network to approximate a policy, it is difficult to say in advance whether NEAT or some form of Deep RL would be better. "
Can a sentence have different parse trees?,"
I just read about the concept of a parse tree.
In my understanding, a valid parse tree of a sentence needs to be validated by a linguistic expert. So, I concluded, a sentence only has one parse tree.
But, is that correct? Is it possible a sentence has more than one valid parse tree (e.g. constituency-based)?
",['natural-language-processing'],"But, is that correct? Is it possible a sentence has more than one valid parse tree (e.g. constituency-based)?The fact that a single sequence of words can be parsed in different ways depending on context (or ""grounding"") is a common basis of miscommunication, misunderstanding, innuendo and jokes.One classic NLP-related ""joke"" (around longer than modern AI and NLP) is:Time flies like an arrow.Fruit flies like a banana.There are actually several valid parse trees for even these simple sentences. Which ones come ""naturally"" will depend on context - anecdotally I only half got the joke when I was younger, because I did not know there were such things as fruit flies, so I was partly confused by literal (but still validly parsed, and somewhat funny) meaning that all fruit can fly about as well as a banana does.Analysing these kinds of ambiguous sentences leads to the grounding problem - the fact that without some referent for symbols, a grammar is devoid of meaning, even if you know the rules and can construct valid sequences. For instance, the above joke works partly because the nature of time, when referred in a particular way (singular noun, not as a possession or property of another object), leads to a well-known metaphorical reading of the first sentence.A statistical ML parser could get both sentences correct through training on many relevant examples (or trivially by including the examples themselves with correct parse trees). This has not solved the grounding problem, but may be of practical use for any machine required to handle natural language input and map it to some task.I did check a while ago though, and most Parts Of Speech taggers in Pythons NLTK get both sentences wrong - I suspect because resolving sentences like those above and AI ""getting language jokes"" is not a high priority compared to more practical uses for chatbots/summarisers, etc."
Which local minima to choose according to the shape of the error surface?,"
The following plot shows error function output based on system weights.
Two equal local minima are shown in green pointers. Note that the red dots are not related to the question.
Considering the amount of convex in the local minima, is there any way to opt between these two local minima?

","['neural-networks', 'backpropagation', 'optimization', 'gradient-descent', 'objective-functions']","Just sharing my thoughts on this: A local minimum in the loss function is just a synthetic measure expressing how well the model is performing on the target dataset (I'm assuming it's the test set)Letting aside the absolute values, you could also consider the steepness of that region as follows: the first LM is at the bottom of steep region which means the related parametrization is quite unstable as if you change it slightly the performance drop quickly (loss increases quickly) so you could interpret this 
as a parametrization with not great generalization capability (it is not robust against small changes): always remember that what you are actually looking for is model able to generalize well when in production (i.e. after you have deployed it so when training, validation, test is finished) and test set is typically a small proxy for the data observed in production (but it really depends on the final application itself)the second LM is related to a much less steep region: even if you change the parametrization slightly you do not observe a big performance change which could be interpreted as a more stable parametrization hence with better possibility to generalize on unseen data "
Are there Python packages for recent Bayesian optimization methods? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I want to try and compare different optimization methods in some datasets. I know that in scikit-learn there are some corresponding functions for the grid and random search optimizations. However, I also need a package (or multiple ones) for different recent Bayesian optimization methods. 
Are there any good and stable ones to use? Which packages do you recommend? (If any recent for grid/random search, it is also okay.)
","['python', 'hyperparameter-optimization', 'resource-request', 'bayesian-optimization']","Apart from the Scikit-Optimize package related to Scikit-Learn, following are some of the packages related to Bayesian optimization:"
Does a data structure that models the encoding specificity principle in memory exist?,"
I find myself wondering if there exists a data structure with the following properties:

Stores information
Conforms to the encoding specificity principle

","['classification', 'biology', 'memory']","Beyond the basic requirement of storage, it seems you are looking for some explicitly defined data organization providing a query interface which allows specifying, alongside the fundamental query params, 
a context and is able to use both these information items (instead of the first one only) to boost the query.If this interpretation of mine is correct, the context-aware query database comes to my mind, here are some examples:In this paper, we propose a logical model and an abstract query language as a foundation of context-aware database management systems. The model is a natural extension of the relational model in which contexts are first class citizens and can be described at different levels of granularity.Querying Context-Aware Databases"
Could error surface shape be useful to detect which local minima is better for generalization?,"
The following plot shows error function output based on system weights.
Two equal local minima are shown in green pointers. Note that the red dots are not related to the question.
Does the right one generalize better compared to the left one? 
My assumption is that for the right minimum if the weights change, the overall error increases less compared to the left minimum point. Would this somehow mean the system does better generalization if the right one is chosen as the optimum minimum?

","['neural-networks', 'backpropagation', 'optimization', 'gradient-descent', 'objective-functions']","In general I agree with @nbro answer, nevertheless sticking strictly to this specific question I'd like to share some speculations: what the author of the question provides us with is the Loss Function Shape so I'll try to use the full information here to compare the 2 minima looking at the LF steepness we observe the Left LM is in a steeper region than the Right LM: how can this be interpreted? I'd interpret the LF Steepness as a Measure of Parametrization Stability: in fact perturbating the Parametrization slightly has a bigger effect on Left LM observed performance rather than the Right LM one when the NN is running ""in production"" typically the parametrization is fixed (in most typical application, weights are not changed out of the training phase) so you should not be concerned about parametrization stability, 
however I'm one of those believing the Flat Minima - Schmidhuber 1997 idea that Local Minima Flatness is connected to generalization property However it is important to observe this is still a very open and interesting question as in Sharp Minima Can Generalize For Deep Nets Dinh 2017 et al. demonstrated 
it is not just about flatness, because reparametrization of the NN model, though preserving minima locations, change its shape so basically sharp minima might be transformed into flat ones without affecting network performance "
Is reinforcement learning possible for chatbots?,"
The reinforcement learning paradigm has the aim to determine the optimal actions for a robot. A typical example is a maze finding robot, but reinforcement learning can also be used for training a robot to play the pong game. The principle is based on a reward function. If the robot is able to solve a problem, he gets a score from the underlying game engine. The score can be positive, if the robot reaches the end of a maze, or it can be negative, if he is colliding with an obstacle. The principle itself is working quite well, that means for simpler applications it is possible to train a robot to play a game with reinforcement learning.
Chatbots are a different category of artificial intelligence. They are working not with actions but with natural language. Person #1 is opening a dialogue with “Hi, I'm Alice”, while person #2 is responding with “Nice to meet you”. What is missing here is an underlying game which is played. There is no reward available for printing out a certain sentence. In some literature the problem of language grounding was discussed seriously, but with an unclear result. It seems, that a classical game for example pong, and a chatbot conversation doesn't have much in common.
Is it possible to combine Reinforcement Learning with chatbot design? The problem is, that a speech-act should be connected to a reward. That means, a well formulated sentence gets +10 points but a weak sentence gets -10 points. How can this be evaluated?
","['reinforcement-learning', 'chat-bots']","The problem is indeed that the 'rules' of conversations are not as fixed as the rules for games. However, you can make use of  descriptive formalism from Discourse Analysis, called adjacency pairs. These describe regularities between utterances on a local level, for example greeting/reply, which would match your ""Hi, I'm Alice"" and ""Nice to meet you"".You will need to be able to classify utterances by your chat bot according to a set of possible responses, and then you can see if a valid response is produced for any given utterance. If the user asks a question, then a greeting will not be a good answer, but a statement could be, if it was a response to the question. This is leaving aside the content and focuses merely on the formal characteristics of the utterance.If you want to know more about the topic, have a look at Conversation Analysis, which is the linguistic field dealing with the subject."
Does IBM Watson use machine learning?,"
I was reading an article on Medium and wanted to make it clear whether a bot created on IBM Watson is an intelligent one or unintelligent.

Simply put, there are 2 types of chatbots — unintelligent ones that act using predefined conversation flows (algorithms) written by developers building them and intelligent ones that use machine learning to interact with users.

","['machine-learning', 'chat-bots', 'watson']","Yes, it does and at many parts of the solution. For one of the core components - intent detection - Intento did a benchmark comparing IBM Watson and other similar products.Outside of intent detection, there are other areas where AI techniques help - e.g. disambiguation, bootstrapping a bot from chat logs etc. Specifically for IBM Watson, you can learn more here."
Are there neural networks with very few nodes that decently solve non-trivial problems?,"
I'm interested in knowing whether there exist any neural network, that solves (with  >=80% accuracy) any nontrivial problem, that uses very few nodes (where 20 nodes is not a hard limit). I want to develop an intuition on sizes of neural networks.
",['neural-networks'],"Even if it’s impossible to answer this question properly, as non trivial is not well defined (maybe the author will edit this questions later, to specify it better), I take the opportunity to point out this paper which looks interesting to me Smallest Neural Network to Learn the Ising CriticalityAssuming you have a general idea of the Ising Model I think the problem of identifying the critical temperature from a data driven perspective can be considered as non trivial and the paper shows how the authors have improved the performance related to solve this task with NN passing from 100 Hidden Neurons, as performed in this paper Machine learning phases of matter from 2017, to only 2 Hidden Neurons Just my cents: "
How to study the correlation between GAN's input vector and output image,"
A generative adversarial network (GAN) takes a vector of numbers as input and generates an image, based on the input. Each element of the vector causes some feature of the image to change, but the mapping between input and output is not clear, as often happens in deep learning. What is the best way to study the correlation between the vector elements and the output image features? The first approach that comes to mind is to manually change every element and check the result, however I am not sure that this is the best solution.
","['convolutional-neural-networks', 'generative-adversarial-networks']",
Machine Learning papers for matching packets to request flows,"
I'm currently doing a research project related to Distributed Tracing. My research has led me to a point where I think ML might be suited for our problem.
I'm looking for papers that are similar to this (even if they have other applications):
I want to match packets exiting a black-box system (outputs) to packets that enter a black box (inputs). I can do that easily in a non concurrent setting which should help  me grow a training set (maybe for supervised learning), but I need an algorithm that, in a concurrent setting, can separate the different request ""flows"" if you will.
I hope this makes sense.
The closest thing to what I'm looking for is ""Aguilera, Marcos K., et al. ""Performance debugging for distributed systems of black boxes."" ACM SIGOPS Operating Systems Review. Vol. 37. No. 5. ACM, 2003."" but it's mostly suited for finding the dependency graph of the system, which I already know.
Thank you
",['machine-learning'],
How do I choose the activation function of the output layer of a neural network (based on theoretical motivations)?,"
Is there a method (not a table of recommendations!) that could tell me what activation function to choose if the outputs of the neural network have some interpretation? For example, these can be the mean of some normal distribution, probabilities in multinomial distribution, parameters of the exponential distribution, and so on.
","['neural-networks', 'activation-functions']",
identifying pattern in datasets,"
i am new to machine learning. i'm trying to identify driving pattern through accelerometer and gyroscope sensor. i have been collecting the data of both the sensors and have been storing them in .csv extension. i am not able to identify a pattern in the datasets since it has a lot of datas. i have three independent variables of accelerometer and with that i need to identify the sudden acceleration and sudden breaking and i have three independent variables of gyroscope and i need to identify aggressive turns. can you suggest as to how i need to analyse the pattern and find a algorithm which suits my requirement. this is how the dataset is

","['classification', 'datasets', 'unsupervised-learning', 'autonomous-vehicles', 'supervised-learning']",
"Is it possible to compute $P( F \mid S )$ given $P(F \mid S,A)$ and $P(F \mid S, \lnot A)$ in Bayesian network?","
I have a bayesian network, which has the following data: 
$P(S) = 0.07$
$P(A) = 0.01$
$P(F \mid S,A) = 1.0$
$P(F \mid S, \lnot A) = 0.7$
$P(F \mid \lnot S, A) = 0.9$
$P(F \mid \lnot S, \lnot A) = 0.1$
And I'm asked to get $P(F \mid S)$. Is it possible? How can I deduce it? 
","['bayesian-networks', 'probability-theory']",
"In DQN, updating target network every N steps or slowly update every step is better?","
The use of target network is to reduce the chance of value divergence which could happen with off-policy samples trained with semi-gradient objectives. In Deep Q network, semi-gradient TD is used and with experience replay the training could diverge. 
Target network is a slow changing network designed to slowly track the main value network. In Mnih 2013, it was designed to match the main network every $N$ steps. There is another way which slowly updates the weight in the direction to match the main network every step. To someone, the latter is called Polyak updates. 
I have done some very limited experiments and seen that with the same update rate, e.g. $N=10$, Polyak update would update with the rate of 0.1, I usually see Polyak updates to give smoother progress and converge faster. My experiments are by no means conclusive. 
I would thence ask if it is known which one to perform better, converge faster or has smoother progress, in a wider range of tasks and settings?
","['reinforcement-learning', 'dqn', 'deep-rl']",
"How would a quantum computer potentially facilitate artificial consciousness, assuming it is possible?","
How would a quantum computer potentially facilitate artificial consciousness, assuming it is possible? 
","['philosophy', 'research', 'artificial-consciousness', 'quantum-computing']",
What is the fundamental difference between neural networks for classifying and generating data,"
What is the fundamental difference between NN for classifying data and generating data?
Most examples show how neural networks can be used to classify data. Like is it an image of a dog or a cat. However, there are applications where NN are used to create images and even write short stories. 
",['neural-networks'],"The formal name for this difference is ""generative"" vs ""discriminative"" models.By default, a supervised learning process using a simple feed-forward neural network and a set of training data with expected answers will produce a discriminative model. It is hard to use such a model to generate content directly.Differences between discriminative and generative models tend to be at the architecture level or deeper, although a few generative techniques are simple variants or re-purposing of discriminative networks. There is no single ""fundamental"" difference in terms of NN design. About the only common theme is that generative models are more complex and harder to work with than discriminative ones.Probably the most popular currently for image generation are Generative Adversarial Networks or GANs. These train a Generator to create an image from random inputs, alongside a Discriminator that tries to spot fakes compared to real images. Training them together results in a generator network that gets progressively better at creating fake data.Also suitable for image generation are Variational Autoencoders (VAEs) which learn essentially by compressing a set of images down into a small representation, and as a result become able to ""uncompress"" similar representations.There are also VAEGANs, which combine VAEs and GANsThere are other models which generate examples from a dataset. For example, Restricted Boltzmann Machines (RBMs) are similar to neural networks, but the neurons fire randomly, and RBMs require a different training process to NNs.GANs, VAEs, VAEGANs, RBMs can be used to generate data in any simple non-sequential datasets, they are not restricted to just images, but recent work with GANs for example has excelled there.For a different kind of generator, you can look at how Deep Dream works. The interesting thing here is that it is a modification of a feed forward network that has been trained by supervised learning. Essentially to run Deep Dream, you take an existing image and change it so that it maximises some internal part(s) of the existing neural network, ""training"" the image as if it were a set of weights.DeepMind's WaveNet architecture is similar to a CNN predicting the next samples of audio from a given input. It can be used in speech or music generation.LSTMs are a popular choice here, as well as for language models.One important caveat. Nearly all these models are hard to understand, work with and train, compared to simpler supervised techniques. Probably the easiest to get to grips with if you want to understand them well enough to implement your own versions, are Deep Dream, VAEs and LSTMs.It is also worth noting that there are many other ways to generate content than neural networks. For instance, sound and music generation has a long history completely independent of recent AI developments."
Using convnet to classify language of text contained in images,"
I hope this question is not too broad or general. I have a very large set of images all of which contain text (some have more, some less). All of them have been tagged as containing, say, English text or Korean. I wonder if convolutional neural networks would be a good approach to classify these images as containing English vs. Korean. Or is there any existing literature/method that does this already. Crucially though, I am not interested in ""understanding"" the text, so this is not an NLP task but, I suppose, a task of classifying orthographies in the images. 
","['convolutional-neural-networks', 'image-recognition', 'classification']",
How do you efficiently choose the hyper-parameters of a neural network?,"
How do you efficiently choose the hyper-parameters of a neural network (e.g. the learning rate, number of layer, weights, etc.)?
","['neural-networks', 'machine-learning', 'deep-learning', 'hyperparameter-optimization', 'hyper-parameters']",
"Calculating tangent vector of curve s(P,$\alpha$) at given point $\alpha$ = 0","
I am reading the paper ""Transformation Invariance in Pattern Recognition – Tangent Distance and Tangent Propagation"", where the tangent vector is calculated for the given curve $s(P,\alpha)$ at $\alpha=0$ by differentiating with respect to $\alpha$, that is, $\frac{\partial s(P,\alpha)}{\partial\alpha}$. For the curve, I have taken one $2D$ image and I am rotating it with matrix $R=\left[\matrix{cos(\alpha)\space -sin(\alpha)\\sin(\alpha) \space\space\space\space\space cos(\alpha)} \right]$. 
As my image is fixed, the curve is just a function of $\alpha$. Therefore, to find the tangent vector, what I am doing is as follows:

I am rotating the image by the matrix $R^{'}$ which is $R^{'}=\left[\matrix{-sin(\alpha)\space -cos(\alpha)\\cos(\alpha) \space\space\space\space\space -sin(\alpha)} \right]$ 
This rotates the image by $90$ degree, which is not the expected result.

I have done the same exercise by differentiating numerically and I am getting the expected answer which is as follows:$\alpha$=0"">
$\alpha$=0"">
Please, help me to understand my mistake in taking derivative of the matrix and multiplying it with image.
","['convolutional-neural-networks', 'computer-vision', 'math']",
How to make machine learning model that reports ambiguity of the input?,"
Suppose I want to build a neural network regression model that takes one input and return one output.
Here's the training data:
0.1 => 0.1
0.2 => 0.2
0.1 => -0.1

You will see that there are 2 inputs 0.1 that matches to different output values 0.1 and -0.1. So what will happen with most machine learning models is that they will predict the average when 0.1 is fed to the model. E.g. the output of 0.1 will be (0.1 + (-0.1))/2 = 0.
But this 0 as an average answer is an incorrect answer. I want the model to be telling me that the input is ambiguous/insufficient to infer the output. Ideally, the model would report it as a form of confidence.
How do I report predictability confidence from the input?
The application that I find very useful in many areas is that I could then later ask the model to show me inputs that are easy to predict and inputs that are ambiguous. This would make me able to collect the data that are making sense.
One way I know is to train the model then check the error on each training data, if it's high then it probably means that the input is ambiguous. But if you know any other papers or better techniques, I would be appreciated to know that!
","['neural-networks', 'machine-learning', 'linear-regression', 'probability']",
"How can I solve part b of exercise 3.6 from the book ""Artificial Intelligence: A Modern Approach""?","
I am trying to solve part b of the exercise 3.6 (page 113) from the book Artificial Intelligence: A Modern Approach.
More specifically, I need to give a complete problem formulation (that is precise enough to be implemented) for the following problem.

A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot ceiling. He would like to get the bananas. The room contains two stackable, movable, climbable 3-foot-high crates.
Give the initial state, goal test, successor function, and cost function for each of the following. Choose a formulation that is precise enough to be implemented.

","['ai-design', 'search', 'homework', 'norvig-russell']",
Is there an alternative to the use of target network?,"
In the context of Deep Q Network, a target network is usually utilized. The target network is a slow changing network with a changing rate as its hyperparameter. This includes both replacement update every $N$ iterations and slowly update every iteration.
Since the rate is hard to fine tune manually, is there an alternative technique that can eliminate the use of target network or at least makes it less susceptible to the changing rate?
","['reinforcement-learning', 'deep-rl']",
What is the difference between DQN and AlphaGo Zero?,"
I have already implemented a relatively simple DQN on Pacman.
Now I would like to clearly understand the difference between a DQN and the techniques used by AlphaGo zero/AlphaZero and I couldn't find a place where the features of both approaches are compared. 
Also sometimes, when reading through blogs, I believe different terms might in fact be the same mathematical tool which adds to the difficulty of clearly understanding the differences. For example, variations of DQN e.g. Double DQN also uses two networks like alpha zero.
Has someone a good reference regarding this question ? Be it a book or an online ressource.
","['reinforcement-learning', 'dqn', 'alphazero', 'deep-rl', 'alphago-zero']","DQN and AlphaZero do not share much in terms of implementation.However, they are based on the same Reinforcement Learning (RL) theoretical framework. If you understand terms like MDP, reward, return, value, policy, then these are interchangeable between DQN and AlphaZero. When it comes to implementation, and what each part of the system is doing, then this is less interchangeable. For instance two networks you have read about in AlphaZero are the policy network and value network. Whilst double DQN alternates between two value networks.Probably the best resource that summarises both DQN and AlphaZero, and explains how they extend the basic RL framework in different ways is Sutton & Barto's Reinforcement Learning: An Introduction (second edition) - Chapter 16 sections 5 and 6 cover the designs of DQN Atari, AlphaGo and AlphaZero in some depth.In brief:"
Why are the nodes (or neurons) in neural networks depicted as circles?,"
Why are the nodes (or neurons) in neural networks depicted as circles?
What is the difference between a circle and a box in diagrams of neural networks?
","['neural-networks', 'machine-learning', 'deep-learning', 'hidden-layers']",
New face generator from Nvidia,"
I have just found the paper and documentation about GAN 2.0, the new face creator from Nvidia.
On the website https://thispersondoesnotexist.com/ they have used this approach to create realistic faces. Unfortunately, the website does not exist anymore.
Is there another webpage demonstrating the new face creator from Nvidia?
","['neural-networks', 'machine-learning']",
How to make chatbot using NLP like Dialogflow? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I want to apply the concept that exists in the Dialogflow API in my e-commerce website.
I get some references in this regard :

Tokenization
Part Of Speech
Named Entity Recognition
Rule based 

I just saw that I just didn't understand how to implement it on the website. 
so I still don't know how the truth is.
please give me a method or explanation that can help me create a chatbot for ecommerce that can give action when a user asks for a product and wants to place an order or something else.
Please give me some explanation or method or references :(
","['chat-bots', 'natural-language-processing']",
Presence of object (highly occluded vehicle) in a scene,"
How to detect presence of object (highly occluded) in a scene?
There are specific features (small patterns, etc), which allow to say that object is present, but it is not enough for detection for YOLO or RPCNN.
How to detect small specific pattern in a whole image efficiently?
","['image-recognition', 'computer-vision', 'object-recognition', 'pattern-recognition']",
"What are the differences between learning by analogy, inductive learning and explanation based learning?","
I have heard of the concepts of learning by analogy (which is quite self-explanatory), inductive learning and explanation-based learning. I tried to learn about inductive learning and explanation-based learning, but I don't understand them.
How would you explain all these three concepts? What are the differences between them?
A link to some explanatory article/notes/blog post are appreciated too.
","['machine-learning', 'terminology']",
How do we know if GPT-2 is a better language model?,"
You may have heard of GPT2, a new language model. It has recently attracted attention from the general public as the foundation that published the paper, OpenAI, ironically refused to share the whole model fearing dangerous implications. Along the paper, they also published a manifesto to justify their choice: ""Better Language Models and Their Implications"". And soon a lot of media were publishing articles discussing the choice and its effectiveness to actually prevent bad implications. I am not here to discuss the ethical components of this choice but the actual performance of the model.
The model got my attention too and I downloaded the small model to play with. To be honest I am far from impressed by the results. Some times the first paragraph of the produced text appears to make sense, but nine times out of ten it is giberish by the first or the second sentence. Exemples given in the paper seems to be ""Lucky"" outputs, cherry picked by human hands. Overall, the paper may suffer from a very strong publication bias.
However, most article we can read on the internet seems to take its powerfulness for granted. The MIT technology review wrote:

The language model can write like a human

The Guardian wrote

When used to simply generate new text, GPT2 is capable of writing plausible passages that match what it is given in both style and subject. It rarely shows any of the quirks that mark out previous AI systems, such as forgetting what it is writing about midway through a paragraph, or mangling the syntax of long sentences.

The model appears generally qualified as a ""breakthrough"". These writings do not match my personal experimentation as produced texts are rarely consistent / syntactically correct.
My question is: without the release of the whole model for ethical reasons, how do we know if the model is really that powerful?
","['natural-language-processing', 'transformer', 'gpt']",
What are daily life examples of SAT problems?,"
What are (good) daily life examples of SAT problems?
I've thought about this one. The problem of placing a bunch of different kinds of glasses in a shared cabinet in such a way that some constraints would be satisfied, such as putting the longer ones in the back of the shorter ones, so it will be easier to take them when we need. 
Is it a good one, or can you think of any other better one?
","['problem-solving', 'satisfiability', 'constraint-satisfaction-problems']",
How do I model the blocked N queens problem as a search problem?,"
The blocked N-queens is a variant of the N-queens problem. In the blocked N-queens problem, we also have a NxN chess board and N queens. Each square can hold at most one queen. Some squares on the board are blocked and can not hold any queens. Conditionality is that queens do not dare to attack each other. At the entrance to this problem are the queues and blocked areas. 
How do I model the blocked N queens problem as a search problem, so that I can apply search algorithms like BFS?
","['search', 'problem-solving', 'breadth-first-search']",
Anomaly Detection in distributed system using generated log file,"
I am developing an AI tool for anomaly detection in a distributed system.  The system supports an interface that combines several individual logs into a single log file generating approx. 7000 entries/min. The logs entries are partially system generated (d-Bus, IPC, ….)  and human written statements (Status not received, initialized successfully, ….). The developers use the generated log for debugging. The entries have been configured to have a similar format depending on the generated system (timestamp, ids, component, context, verbosity level, description, ….). 
Background:
1. The history of the identified anomalies is minimal and not archived.
2. Not many similar event templates in log files.
3. Software execution rules are not clearly documented.
4. The log events are co-related.  
What are the recommended algorithms (Statistical, NLP, ML, Neural networks) that can be used to efficiently perform pattern extraction on the entries and identify existing and new anomalous behavior?
","['neural-networks', 'machine-learning', 'natural-language-processing', 'pattern-recognition']",
Understanding an execution of the Monte Carlo tree search algorithm,"
I have the execution of the Monte Carlo Tree Search (MCTS) below. I need to expand it, but I don't understand steps 1 and 2. 
Why does it go to the first node and then make a new node, instead of going to the deepest left leaf? I thought it needs to go to the most probable leaf.

In step 1 of MCTS, it added a new node. Now, there are 9 cases.

In step 2 of MCTS, it added a new node. Now, there are 2 cases under the first node:

","['algorithm', 'monte-carlo-tree-search']",
Genetic programming with Objective-C [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I have a complex wargame already developed in a aging Objective-C and I would like to improve the AI
I have built the logic for self-play, fitness evaluation and evolution
The hard-time is the ability to run a lot of experiences of self-play with limited ressources (single Mac). Time is a factor but also memory. I am facing some random crashed after a given number of games
I was wondering 
- if people have faced the same issues with a large number of runs with objective-C 
- if other people have tried to use Genetic Programming or Reinforcement Learning with Objective-C or C#
","['game-ai', 'genetic-programming']",
Why isn't Nilsson's Sequence Score an admissible heuristic function?,"
I understand what an admissible heuristic is, I just don't know how to tell whether one heuristic is admissible or not. So, in this case, I'd like to know why Nilsson's sequence score heuristic function isn't admissible.
","['proofs', 'admissible-heuristic', 'heuristic-functions', '8-puzzle-problem', 'nilssons-heuristic-function']","I will use the 8-puzzle game to show you why Nilson's sequence score heuristic function is not admissible.  In the 8-puzzle game, you have a $3 \times 3$ board of (numbered) squares as follows.The numbers in these squares are just used to refer to the specific squares (to avoid saying the ""middle square"" or the ""upper-left square""). So, when I say square 1, I refer to the upper-left square. In these games, you have 8 ""tiles"". Let's denote these tiles by $A, B, C, D, E, F, G$ and $H$. So, in this game, there is always one square which is free (or empty), given that there are $9$ squares. The goal of this game is to reach the following configuration of tilesNote that, in the case of the 8-puzzle game, a ""state"" is a configuration of the board. So, the following two board configurations are two distinct statesand The rules of the 8-puzzle are simple. You can move one tile (at a time) from its current position (or square) to another position, provided that the destination square is free. You can only move a tile horizontally and vertically (and one square at a time).I will not explain in this answer how the Nilson's sequence score heuristic works. Here is a explanation of how is used in the case of the 8-puzzle game. You should read this explanation and make sure you understand how Nilson's heuristic works before proceeding! You can also find an explanation of how this heuristic works in the book ""Principles of Artificial Intelligence"" (at page 85), by Nils J. Nilsson (1982).Why isn't then Nilson's sequence score admissible?A heuristic function $h$ is admissible if $h(n) \leq h^*(n)$, for all states $n$ in the state space, where $h^*(n)$ is the actual distance to reach the goal (which is often unknown in practice, hence the need to use heuristics to estimate such distance).Note that admissibility is a property that must hold for all states. So, if we find a state where the condition above is not satisfied for the Nilson's sequence score heuristic function, then we show that the Nilson's sequence score is not admissible. Let us create the following stateNote that this state only differs from the goal state by one move: if we move the tile $H$ to square $7$, then we reach the goal state. So, the actual distance to reach the goal state is $1$ move. But what does the Nilson's score function tell us regarding the distance of this state to the goal state? You can see from the algorithm presented in this answer to compute the Nilson's sequence score (of a board configuration) that the score of the configuration (or state) above would be more than $1$ (you can immediately see this because you need to multiply by $3$). Therefore, Nilson's sequence score overestimated the distance to the goal (at least for one state), thus it cannot be admissible (by definition)."
How can my Neural Network categorize message strings?,"
Abstract
I wish to design a neural network that will categorize messages based on criteria I have predefined. It should feature the ability to be proactively trained as it continues its lifecycle. This means a human can intervene in its categorization attempts and determine whether or not it was correct and have it adjust its weights accordingly (without having to retrain all over again).
Inputs
It is know that ALL input will follow these rules:

Always of $N$ length
All messages are transformed to eliminate unnecessary complexity

Here's a brief overview of how an example message might be processed.
Starting with a message $M$:

That's an interesting perspective. I think that you should consider adding more details to your point about the cat being too silly.

The text is then transformed so that extra details are removed:

thats an interesting perspective i think that you should consider adding more details to your point about the cat being too silly

Then it's converted into a vector (appending $0$ to reach length $N$) ready to be processed by the neural network:
[116, 104, 97, 116, 115, 32, 97, . . . , 0, 0, 0, 0]
Ouputs
In my network, I wish all the outputs to be weighted on how well they fit in each category. I need multiple outputs. I'm not really focusing on one particular category per-say, but how well the message fits in all of them. 
Following the input $M$ I used as an example, I'd expect the outputs to look something like this after my vector has fed-forward:
Suggestive:  0.89042Opinionated: 0.68703
The weight values for each output indicate the strength of the category in the overall message.
From message $M$:

That's an interesting perspective.

Would weigh the opinionated category as $0.68703$.
And:

I think that you should consider adding more details to your point about the cat being too silly.

Would weigh the suggestive category as $0.89042$.
Summary and Questions
I'm interested in the architectural design choices of a network that would support my feature set. The main goal is to be able to train my network to categorize messages based on pre-trained (and live-trained) data. I'd like to know things like:

What type of Neural Network I should use for this purpose? I've researched LSTM & Recurrent networks; which have been mentioned to be good at processing sequences (ie. messages).
What considerations should I account for when creating this network?.
How can the overall network support live-training so I can tell my network when its wrong and have it 'correct' itself without having to retrain completely? 

","['neural-networks', 'machine-learning', 'recurrent-neural-networks']",
What is the motivation for row-wise convolution and folding in Kalchbrenner et al. (2014)?,"
I was reading the paper by Kalchbrenner et al. titled A Convolutional Neural Network for Modelling Sentences and I am struggling to understand their definition of convolutional layer.
First, let's take a step back and describe what I'd expect the 1D convolution to look like, just as defined in Yoon Kim (2014).

sentence. A sentence of length n (padded where necessary) is represented as
$x_{1:n} = x_1 \oplus  x_2 \oplus \dots ⊕ x_n,$ (1)
where $\oplus$ is the concatenation operator. In general, let $x_{i:i+j}$ refer to the concatenation of words $x_i, x_{i+1}, \dots, x_{i+j}$. A convolution operation involves a filter $w \in \mathbb{R}^{hk}$, which is applied to a window of h words to produce a new feature. For example, a feature ci is generated from a window of words $x_{i:i+h−1}$ by
$c_i = f(w \cdot x_{i:i+h−1} + b)$ (2).
Here $b \in \mathbb{R}$ is a bias term and $f$ is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sentence $\{x_{1:h}, x_{2:h+1}, \dots, x_{n−h+1:n}\}$ to produce a feature map
$c = [c_1, c_2, \dots, c_{n−h+1}]$, (3)
with $c \in \mathbb{R}^{n−h+1}$.

Meaning a single feature detector transforms every window from the input sequence to a single number, resulting in $n-h+1$ activations.
Whereas in Kalchbrenner's paper, the convolution is described as follows:

If we temporarily ignore the pooling layer, we may state how one computes each d-dimensional column a in the matrix a resulting after the convolutional and non-linear layers. Define $M$ to be the matrix of diagonals:
$M = [diag(m:,1), \dots, diag(m:,m)]$ (5)
where $m$ are the weights of the d filters of the wide convolution. Then after the first pair of a convolutional and a non-linear layer, each column $a$ in the matrix a is obtained as follows, for some index $j$:

Here $a$ is a column of first order features. Second order features are similarly obtained by applying Eq. 6 to a sequence of first order features $a_j, \dots, a_{j+m'−1}$ with another weight matrix $M'$. Barring pooling, Eq. 6 represents a core aspect of the feature extraction function and has a rather general form that we return to below. Together with pooling, the feature function induces position invariance and makes the range of higher-order features variable.

As described in this question, the matrix $M$ has dimensionalty of $d$ by $d * m$ and the vector of concatenated $w$'s has dimensionality $d * m$. Thus the multiplication produces a vector of dimensionality d (for a single convolution of a single window!).
Architecture visualization from the paper seems to confirm this understanding:

The two matrices in the second layer represent two feature maps. Each feature map has dimensionality $(s + m - 1) \times d$, and not $(s + m - 1)$ as I would expect.
Authors refer to a ""conventional"" model where feature maps have only one dimension as Max-TDNN and differentiate it from their own.
As the authors point out, feature detectors in different rows are fully independent from each other until the top layer. Thus they introduce the Folding layer, which merges each pair of rows in the penultimate layer (by summation), reducing their number in half (from $d$ to $d/2$).

Sorry for the prolonged introduction, here are my two main questions:

What is the possible motivation for this definition of convolution (as opposed to Max-TDNN or e.g. Yoon Kim's model)

In the Folding layer, why is it satisfying to only have dependence between pairs of corresponding rows? I don't understand the gain over no dependence at all.


","['machine-learning', 'deep-learning', 'natural-language-processing']",
Why exactly do neural networks require i.i.d. data?,"
In reinforcement learning, successive states (actions and rewards) can be correlated. An experience replay buffer was used, in the DQN architecture, to avoid training the neural network (NN), which represents the $Q$ function, with correlated (or non-independent) data. In statistics, the i.i.d. (independently and identically distributed) assumption is often made. See e.g. this question. This is another related question. In the case of humans, if consecutive data points are correlated, we may learn slowly (because the differences between those consecutive data points are not sufficient to infer more about the associated distribution).
Mathematically, why exactly do (feed-forward) neural networks (or multi-layer perceptrons) require i.i.d. data (when being trained)? Is this only because we use back-propagation to train NNs? If yes, why would back-propagation require i.i.d. data? Or is actually the optimisation algorithm (like gradient-descent) which requires i.i.d. data? Back-propagation is just the algorithm used to compute the gradients (which is e.g. used by GD to update the weights), so I think that back-propagation isn't really the problem.
When using recurrent neural networks (RNNs), we apparently do not make this assumption, given that we expect consecutive data points to be highly correlated. So, why do feed-forward NNs required the i.i.d. assumption but not RNNs?
I'm looking for a rigorous answer (ideally, a proof) and not just the intuition behind it. If there is a paper that answers this question, you can simply link us to it.
","['neural-networks', 'reinforcement-learning', 'statistical-ai', 'experience-replay', 'iid']",
Having trouble understanding some of the details of R-CNN (first one),"
Here is what I understand (what I think I understand).
We first train out model on our images using transfer learning.
So now we have a pre-trained model.
For each image in out dataset, we compute selective search on it, which makes 2000 region proposals.
    These 2000 region proposals are feed through our pre-trained NN ,
However we only collect the output (feature maps) from the last convolution layer. These outputs are saved to a hard-disk.
These feature maps are fed into a SVM for another round of training, were another label, ""no object"" is added.
We also have regression model that trains based on the window coordinates that we also annotated.
So we have SVN and a regression model (two models) that we train.
1)Is the above correct?
2) Are each of these 2000 region proposals hand-labeled (correct label (cat, dog etc) or no-object) before feeding it into the SVM?
3) Is the regression model tied into the SVM model? Basically out loss is a combination of both regression coords and SVM classification?
","['deep-learning', 'convolutional-neural-networks', 'object-recognition']",
My DQN is stuck and can't see where the problem is,"
I'm trying to replicate the DeepMind paper results, so I implemented my own DQN. I left it training for more than 4 million frames (more than 2000 episodes) on SpaceInvaders-v4 (OpenAI-Gym) and it couldn't finish a full episode. I tried two different learning rates (0.0001 and 0.00125) and seems to work better with 0.0001, but the median score never raises above 200.
I'm using a double DQN.
Here is my code and some photos of the graphs I'm getting each session. 
Between sessions I'm saving the network weights; I'm updating the target network every 1000 steps.
I can't see if I'm doing something wrong, so any help would be appreciated. I'm using the same CNN construction as the DQN paper.
Here's the action selection function; it uses a batch  of 4  80x80 processed experiences in grayscale to select the action (s_batch means for state batch):
    def action_selection(self, s_batch):
        action_values = self.parallel_model.predict(s_batch)
        best_action = np.argmax(action_values)
        best_action_value = action_values[0, best_action]
        random_value = np.random.random()

        if random_value < AI.epsilon:
            best_action = np.random.randint(0, AI.action_size)
        return best_action, best_action_value

Here is my training function. It uses the past experiences as training; I tried to implement that if it lose any life, it wouldn't get any extra rewards, so in theory, the agent would try to not die:
    def training(self, replay_s_batch, replay_ns_batch):
        Q_values = []
        batch_size = len(AI.replay_s_batch)
        Q_values = np.zeros((batch_size, AI.action_size))

        for m in range(batch_size):

            Q_values[m] = self.parallel_model.predict(AI.replay_s_batch[m].reshape(AI.batch_shape))
            new_Q = self.parallel_target_model.predict(AI.replay_ns_batch[m].reshape(AI.batch_shape))
            Q_values[m, [item[0] for item in AI.replay_a_batch][m]] = AI.replay_r_batch[m]

            if np.all(AI.replay_d_batch[m] == True):
                Q_values[m, [item[0] for item in AI.replay_a_batch][m]] = AI.gamma * np.max(new_Q)    

        if lives == 0:
            loss = self.parallel_model.fit(np.asarray(AI.replay_s_batch).reshape(batch_size,80,80,4), Q_values, batch_size=batch_size, verbose=0)

        if AI.epsilon > AI.final_epsilon:
            AI.epsilon -= (AI.initial_epsilon-AI.final_epsilon)/AI.epsilon_decay

replay_s_batch it's a batch of (batch_size) experience replay states (packs of 4 experiences), and replay_ns_batch it's full of 4 next states. The batch size is 32.
And here are some results, after training:

In blue, the loss (I think it's correct; it's near-zero). Red dots are the different match scores (as you can see, it does sometimes really good matches). In green, the median (near 190 in this training, with learning rate = 0.0001)
 
Here is the last training, with lr = 0.00125; the results are worse (it's median it's about 160). Anyway the line it's almost straight, I don't see any variation in any case.
So anyone can point me to the right direction? I tried a similar approach with pendulum and it worked properly. I know that with Atari games it takes more time but a week or so I think it's enough, and it seems to be stuck.
In case someone need to see another part of my code just tell me.
Edit: With the suggestions provided, I modified the action_selection function. Here it is:
def action_selection(self, s_batch):
    if np.random.rand() < AI.epsilon:
        best_action = env.action_space.sample()
    else:
        action_values = self.parallel_model.predict(s_batch)
        best_action = np.argmax(action_values[0])
    return best_action

To clarify my last edit: with action_values you get the q values; with best_action you get the action which corresponds to the max q value. Should I return that or just the max q value?
","['deep-learning', 'reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']","After some research and reading this post, I see where my problem was: I was introducing a full consecutive batch of experiences, selected randomly, yes, but the experiences in the batch were consecutives.
After redoing my experience selection method, my DQN is actually working and has reached about +200 points after 400000 experiences (about 500 episodes; only 2-3 hours or training!). Before I couldn't reach that score after days of training.
I'll let it train to see if there are something I can improve. Thanks to everyone who tried to help me! I let this answer here just in case someone has the same problem as me. "
Which AI algorithm to use to identify a subject from many unknown factors,"
Say I have a set of data generated by someone. It could be either bytes from a photo, or readings from bio-sensors, and I have a huge amount of said information, from many people or subjects. Which AI algorithms could be used to learn that a set of data belongs to a subject. I would have the information map that a huge set of data belongs to  Bob, and another belongs to Alice to train the system.
",['machine-learning'],"You have a set of different types of data available for each of your subjects, and given one set you'd like to classify which subject it belongs to. This looks like a supervised classificiation problem.The most popular classifiers for supervised learning are neural networks. Now given the heterogenous nature of your data types, a simple approach would be to use separate classifiers for each type of data. For example, a convolution neural net for the image data, and a simple feed forward net for the biosensor data. Another thing you try is a multi channel approach, where towards the input side you have multiple channels for the different types of data, and the final few layers are fully connected.
The image has only CNNs for the multi channel part but you could have one channel as a simple feed forward net while another one as having conv layers.Also, if you wish to classify the data as belonging to a subject on the basis of just one of the data types from the set, then you should have separate classifiers for all types. In that case it might be worthwhile to look into classifiers other than neural nets like multi class logistic regression which might be simpler to work with for a particular data type. "
How can I train a neural network to grade the user's answers to a questionnaire?,"
I have a questionnaire consisting of over 10 questions. The questionnaire is being answered by a lot of people, which I have manually graded. Each question can give the user up to 10 points depending on how they have answered. 
Let's say that my dataset is big enough, how would I go about using a neural network to automatically grade these questions for me?
I have used a convolutional neural network before for image classification, but, when dealing with text classification, where should I start? Is there some sort of tutorial out there that covers this with a similar example?
","['neural-networks', 'machine-learning', 'natural-language-processing', 'classification']",
What is happening when a reinforcement learning agent trains itself out of desired behavior?,"
I have a reinforcement learning agent with both a positive and a negative terminal state.  After each episode during training, I am recording whether a success or failure occurred, and then I can compute a running ratio of success to failure.
I am seeing a phenomenon where, at some point in time, my agent achieves a reasonably high success rate (~80%) for a 100-episode running average.  However, with further training, it seems to 'train itself out' of this behavior and ends the training sequence with a very low success rate (~10-20%).
I am using an epsilon-greedy strategy whereby epsilon decays linearly from 1.0 to 0.1 for the first 10% of episodes and then remains at 0.1 for the remaining 90%.  As such, the 'training out' appears to occur some time where exploration only occurs with 10% probability.
What could be causing this undesirable behavior? How can I combat it?
","['reinforcement-learning', 'q-learning']","This is a common problem and does have a name. It is called ""catastrophic forgetting"" (link is just to a paper I found randomly when searching for the term).What could be causing this undesirable behavior? It happens only when using function approximation for value functions (e.g. a neural network trained to learn Q values), and is caused by the agent's own success. After a while the only samples that you will train with will be near-optimal high return cases, and a neural network will optimise the Q value approximation only for that recent data in order to get the best loss. This will usually mean much poorer predictions on unseen but different data, as the network overfits to states and actions only seen in the optimal policy. Eventually the agent will explore into an area where its predictions are way off. Then, because Q learning also uses its own predictions to bootstrap new Q values, this can start a runaway feedback process where the agent starts to choose a suboptimal path through the environment. Inside the hidden layers and weights, the neural network may have lost the ability to differentiate well between the states on the old, worse path and the newer better ones. It didn't need to differentiate, because it stopped needing to reduce loss on any data about the old states. So it will also start incorrectly associating the now poor results with the more optimal paths. It will behave at least partially as if the correct policy was set by the overfit Q predictions, but the values need adjusting - so as well as (correctly) reducing its value predictions of the suboptimal paths it has just encountered, it will also (incorrectly) reduce the value predictions of the optimal paths. Sometimes, the swing back to receiving lower returns during this process is so strong and/or incorrectly associated with the high return states along with the low ones, that the agent never properly recovers. Other times, the agent can re-learn fully and you get random oscillations over time between optimal and non-optimal agent behaviour.How can I combat it?This is still an ongoing area of research. Here are a couple of things that have worked for me:Low learning rates, and defences against sudden large gradients (e.g. gradient clipping).Regularisation. Sadly dropout seems not to work in RL, but weight decay is still useful to prevent over-fitting, and it also helps combat catastrophic forgetting because it prevents bootstrap estimates of long-unseen state/action combinations from returning radically different Q values to the rest of the system.Keep some early experience around from when the agent was still performing badly - this allows the agent to still train with some bad cases and prevents the Q function predicting that ""everything is awesome"" because it still has examples to learn from where this is not the case.For simple environments, such as inverted pendulum, just keeping some very early fully random behaviour in the experience replay table is enough. For instance if you have a table with 10000 observations (of $s, a, r, s'$), keep 1000 of the first experiences in that table and don't discard them when the table is full. For more complex environments, this is not so useful, as the early random behaviour is too far removed from what the agent learns. The DQN ""rainbow"" paper uses prioritized experience replay to focus on areas where Q value predictions from the NN are not matching the observations. "
Problem with Proposition 1 of Google Deepmind's 'Weight uncertainty in Neural Networks',"
I'm going through the paper Weight Uncertainty in Neural Networks by Google Deepmind. In the final line of the proof of proposition 1, the integral and the derivative are swapped. Then the derivative is taken. But this somehow yields 2 derivatives of $f$ with respect to $\theta$. I thought that this was the result of a product rule applied to $q(\epsilon)$ and $f(w,\theta)$ and then the chain rule. But that does not yield the same outcome as $\frac{\partial q(\epsilon)}{\partial \theta} = 0 $. My question is: does anyone understand how the equation in the last line comes about?

","['neural-networks', 'bayesian-networks', 'expectation']",
What is the difference between First-Visit Monte-Carlo and Every-Visit Monte-Carlo Policy Evaluation?,"
I came across these 2 algorithms, but I cannot understand the difference between these 2, both in terms of implementation as well as intuitionally.
So, what difference does the second point in both the slides refer to?


","['reinforcement-learning', 'comparison', 'monte-carlo-methods', 'policy-evaluation']","The first-visit and the every-visit Monte-Carlo (MC) algorithms are both used to solve the prediction problem (or, also called, ""evaluation problem""), that is, the problem of estimating the value function associated with a given (as input to the algorithms) fixed (that is, it does not change during the execution of the algorithm) policy, denoted by $\pi$. In general, even if we are given the policy $\pi$, we are not necessarily able to find the exact corresponding value function, so these two algorithms are used to estimate the value function associated with $\pi$.Intuitively, we care about the value function associated with $\pi$ because we might want or need to know ""how good it is to be in a certain state"", if the agent behaves in the environment according to the policy $\pi$.For simplicity, assume that the value function is the state value function (but it could also be e.g. the state-action value function), denoted by $v_\pi(s)$, where $v_\pi(s)$ is the expected return (or, in other words, expected cumulative future discounted reward), starting from state $s$ (at some time step $t$) and then following (after time step $t$) the given policy $\pi$. Formally, $v_\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ]$, where $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$ is the return (after time step $t$). In the case of MC algorithms, $G_t$ is often defined as $\sum_{k=0}^T  R_{t+k+1}$, where $T \in \mathbb{N}^+$ is the last time step of the episode, that is, the sum goes up to the final time step of the episode, $T$. This is because MC algorithms, in this context, often assume that the problem can be naturally split into episodes and each episode proceeds in a discrete number of time steps (from $t=0$ to $t=T$). As I defined it here, the return, in the case of MC algorithms, is only associated with a single episode (that is, it is the return of one episode). However, in general, the expected return can be different from one episode to the other, but, for simplicity, we will assume that the expected return (of all states) is the same for all episodes.To recapitulate, the first-visit and every-visit MC (prediction) algorithms are used to estimate $v_\pi(s)$, for all states $s \in \mathcal{S}$. To do that, at every episode, these two algorithms use $\pi$ to behave in the environment, so that to obtain some knowledge of the environment in the form of sequences of states, actions and rewards. This knowledge is then used to estimate $v_\pi(s)$. How is this knowledge used in order to estimate $v_\pi$? Let us have a look at the pseudocode of these two algorithms.$N(s)$ is a ""counter"" variable that counts the number of times we visit state $s$ throughout the entire algorithm (i.e. from episode one to $num\_episodes$). $\text{Returns(s)}$ is a list of (undiscounted) returns for state $s$. I think it is more useful for you to read the pseudocode (which should be easily translatable to actual code) and understand what it does rather than explaining it with words. Anyway, the basic idea (of both algorithms) is to generate trajectories (of states, actions and rewards) at each episode, keep track of the returns (for each state) and number of visits (of each state), and then, at the end of all episodes, average these returns (for all states). This average of returns should be an approximation of the expected return (which is what we wanted to estimate).The differences of the two algorithms are highlighted in $\color{red}{\text{red}}$. The part ""If state $S_t$ is not in the sequence $S_0, S_1, \dots, S_{t-1}$"" means that the associated block of code will be executed only if $S_t$ is not part of the sequence of states that were visited (in the episode sequence generated with $\pi$) before the time step $t$. In other words, that block of code will be executed only if it is the first time we encounter $S_t$ in the sequence of states, action and rewards: $S_0, A_0, R_1, S_1, A_1, R_2 \ldots, S_{T-1}, A_{T-1}, R_T$ (which can be collectively be called ""episode sequence""), with respect to the time step and not the way the episode sequence is processed.  Note that a certain state $s$ might appear more than once in $S_0, A_0, R_1, S_1, A_1, R_2 \ldots, S_{T-1}, A_{T-1}, R_T$: for example, $S_3 = s$ and $S_5 = s$. Do not get confused by the fact that, within each episode, we proceed from the time step $T-1$ to time step $t = 0$, that is, we process the ""episode sequence"" backwards. We are doing that only to more conveniently compute the returns (given that the returns are iteratively computed as follows $G \leftarrow G + R_{t+1}$).So, intuitively, in the first-visit MC, we only update the $\text{Returns}(S_t)$ (that is, the list of returns for state $S_t$, that is, the state of the episode at time step $t$) the first time we encounter $S_t$ in that same episode (or trajectory). In the every-visit MC, we update the list of returns for the state $S_t$ every time we encounter $S_t$ in that same episode. For more info regarding these two algorithms (for example, the convergence properties), have a look at section 5.1 (on page 92) of the book ""Reinforcement Learning: An Introduction"" (2nd edition), by Andrew Barto and Richard S. Sutton."
Any guidance on learning rate / batch size for noisy data (high Bayes error rate)?,"
Is there any guidance available for training on very noisy data, when Bayes error rate (lowest possible error rate for any classifier) is high? For example, I wonder if deliberately (not due to memory or numerical stability limitations) lowering the batch size or learning rate could produce a better classifier.
I found so far some general recommendations, not specific for noisy data: Tradeoff batch size vs. number of iterations to train a neural network
","['neural-networks', 'machine-learning', 'deep-learning', 'classification', 'optimization']",
Detect named entities inside words using spaCy,"
I am using rasa nlu for training an NLU system to detect intents and slots. Now, some languages have word endings with their nouns (like Finnish, e.g. ""in Berlin"" -> ""Berliinissä""). I have tried to annotate the characters in the training data as entities, but then I run the model, it doesn't detect the characters inside the word. When those characters are a separate word, only then they're detected. I am unable to think of an implementation to effectively detect named entities within a word. Suggestions needed.
","['natural-language-processing', 'chat-bots']",
Pseudocode for CNN with Bounding Box and Classifier,"
I've been looking at various bounding box algorithms, like the three versions of RCNN, SSD and YOLO, and I have noticed that not even the original papers include pseudocode for their algorithms. I have built a CNN classifier and I am attempting to incorporate bounding box regression, though I am having difficulties in implementation. I was wondering if anyone can whip up some pseudocode for any bounding box classifier or a link to one (unsuccessful in my search) to aid my endeavor.
Note: I do know that there are many pre-built and pre-trained versions of these object classifiers that I can download from various sources, I am interested in building it myself.
","['convolutional-neural-networks', 'object-recognition', 'pseudocode']",
"Expected SARSA vs SARSA in ""RL: An Introduction"" [closed]","







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



Sutton and Barto state in the 2018-version of ""Reinforcement Learning: An Introduction"" in the context of Expected SARSA (p. 133) the following sentences:

Expected SARSA is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of $A_{t+1}$. Given the same amount of experience we might expect it to perform slightly better than Sarsa, and indeed it generally does.

I have three questions concerning this statement:

Why is the action selection random with Sarsa? Isn't it on-policy and therefore $\epsilon$-greedy?
Because Expected-Sarsa is off-policy the experience it learns from can be from any policy that at least explores everything in the limit e.g. random action-selection with equal probabilities for every action. How can Exected-Sarsa learning from such policy be generally better than normal Sarsa learning from an $\epsilon$-greedy policy, especially with the same amount of experience?
Probably more general: How can on-policy and off-policy algorithms be compared in such way (e.g. through variance) even though their concepts and assumptions are so different?

","['reinforcement-learning', 'sutton-barto', 'sarsa', 'expected-sarsa']","Why is the action selection random with Sarsa? A policy could be stochastic. In the case of SARSA, it is stochastic because of the use of $\epsilon$-greedy.Isn't it on-policy and therefore ϵ-greedy?I don't quite understand the question. SARSA is on-policy evaluation with $\epsilon$-greedy policy. Q-learning is off-policy evaluation with $\epsilon$-greedy policy. $\epsilon$-greedy is just a way to turn an action-value function into a policy.Because Expected-Sarsa is off-policy the experience it learns from can be from any policy ... How can Exected-Sarsa learning from such policy be generally better than normal Sarsa learning from an ϵ-greedy policy, especially with the same amount of experience?It is unfair to compare different natures of experiences because off-policy experience contains less useful information. Thus, both SARSA and Expected SARSA should use their own on-policy experience for comparison. While Expected SARSA update step guarantees to reduce the expected TD error, SARSA could only achieve that in expectation (taking many updates with sufficiently small learning rate). Through this perspective, there is little doubt that Expected SARSA should be better.Probably more general: How can on-policy and off-policy algorithms be compared in such way (e.g. through variance) even though their concepts and assumptions are so different?Same as the previous answer, it is unfair to compare them without the same quality of experiences."
How do I avoid an agent to tend to terminate in a negative state when time needs to be taken into account?,"
In an unknown environment, how do I avoid an agent to tend to terminate its trajectory in a negative state when time needs to be taken into account?
Suppose the following example to make my question clear:

A mouse (M) starts in the bottom left of its world
Its goal is to reach the cheese (C) in the top right (+5 reward) while also avoiding the trap (T) (-5 reward)
It should do this as quickly as possible, so for every timestep it also receives a penalty (-1 reward)

If the grid world is sufficiently large, it may actually take the mouse many actions to reach the cheese. 
Is there a scenario where the mouse may choose to prefer the trap (-1*small + -5 cumulative reward) versus the cheese (-1*large + 5 cumulative reward)?  Is this avoidable?  How does this translate to an unknown environment where the number of time steps required to reach the positive terminal state is unknown?
","['reinforcement-learning', 'rewards']","This is a common problem in reward shaping. You want a certain behavior from you agent but its challenging to describe it completely in terms of rewards. This situation you are describing is challenging specifically because as the grid world grows, the chance of randomly stumbling onto the goal state becomes less likely AKA the problem of exploration.
There are a few techniques that can be used to address this problem though, here are some.0) This is an emergent property of the environment and gamma (0 based because its more immediate to your problem:p)If gamma is small, your agent will value rewards that are in its immediate future more highly whereas as gamma approaches 1, the agent values rewards that are further in its future. In your grid world, the the size of the grid affects how this gamma affects your agent. Like in your example, if your grid was 100x100, if the trap was close to the agent, you would have to have a gamma closer to 0 in order for your agent to avoid the trap because its worse than moving to a cell that isn't a trap. This is interesting because the whole purpose of gamma is to increase the weighted value of temporally distant rewards but when you make the trap more favorable than the goal, going to the trap is the optimal strategy. :)1) Include more observation data to your modelThis isn't always a possibility but depending on your application and what you have available, you may be able to give your agent missing information that might be necessary to its ability to solve your task. For instance, in your infinite grid example, you may include the distance between the agent and the goal or the direction of the goal.2) Include a reward that helps to shape the direction of progress.One could easily create an infinite grid world where there isn't actually a goal but rather a continuing task where the agent has to cover distance in a desired direction while avoiding obstacles. How would you approach this problem? Perhaps a reward that specifically looks at the number of cells visited by the agent in the desired direction over time aka the discrete analog to velocity. Of course this is still dependent on a know direction but thinking about how to one might handle the ""limit"" of your environment as it grows (e.g., adding more and more cells to the grid world) helps to give an intuition of what your agent could be missing.3) Use curiosity based approaches
Following from 2), if the direction isn't known, one thing to consider is rather than giving a penalty for each timestep, instead incentivize the agent to be faster, rewarding visiting ""infrequently visited"" states. As the task requires that the agent performs as quickly as possible, remaining or returning to a previously visited state clearly doesn't benefit the agent. Taking this notion of rewarding ""visiting infrequently/unyet visited/ states"" further results in the recent research topic of using curiosity to have RL agents that have novel exploration strategies.Although there are many (often debated) ways to define curiosity, they all share an idea of giving the agent a bonus when it has entered a state that has not been visited before. A paper that gives a good recap of curiosity methods and also introduces a novel approach is Random Network Distillation from OpenAI."
Judging a genetic algorithm's priority-based schedules by how far ahead the higher priority things are done,"
I'm creating a schedule for a summer camp. Because of the high risk of rain, the higher priority activities need to be attempted first, so there is more time for later attempts if need be (temporarily ignoring the schedule in that situation). 
Camp takes place over four days. My current idea is to map the days to a set of numbers (4, 3, 2, 1), and get a correlation between these numbers and the priorities of activities. But I'm not certain this is the best way to do it, nor what the best way of correlating the two are. I'm also not sure how I would factor this correlation in with the fitness function, along with the priorities themselves.
How should I proceed?
",['genetic-algorithms'],
"How is equation 8 derived in the paper ""Self-critical sequence training for image captioning""?","
In the paper ""Self-critical sequence training for image captioning"", on page 3, they define the loss function (of the parameters $\theta$) of an image captioning system as the negative expected reward of a generated sequence of words (Equation (3)): 
$$L(\theta) = - \mathbb{E}_{w^s \sim p_{\theta}}[r(w^s)],$$
where $w^s = (w_1^s,..., w_T^s)$ and $w^s_t$ is the word sampled from the model at time step $t$.
The derivation of the gradient of $L(\theta)$ concludes with Equation (7), where the gradient of $L(\theta)$ is approximated with a single sample $w^s \sim p_\theta$: 
$$\nabla_{\theta}L(\theta) \approx -(r(w^s) - b) \ \triangledown_{\theta}  log \ p_{\theta}(w^s),$$
where $b$ is a reward baseline and $p_\theta(w^s)$ is the probability that sequence $w^s$ is sampled from the model. Up until here I understand what's going on. However, then they proceed with defining the partial derivative of $L(\theta)$ w.r.t. the input of the softmax function $s_t$ (final layer):
$$\nabla_{\theta}L(\theta) = \sum^T_{t=1} \frac{\partial L(\theta)}{\partial s_t} \frac{\partial s_t}{\partial \theta}$$ 
I still understand the equation above.
And Equation (8):
$$\frac{\partial L(\theta)}{\partial s_t} \approx (r(w^s) - b) (p_\theta(w_t| h_t) - 1_{w^s_t}),$$
where $1_{w^s_t}$ is $0$ everywhere, but $1$ at the $w^s_t$'th entry. How do you arrive at Equation (8)? 
I'm happy to provide more information if necessary. In the paper ""Sequence level training with recurrent neural networks"", on page 7, they derive a similar result.
","['neural-networks', 'reinforcement-learning', 'objective-functions', 'papers', 'reinforce']",
Face liveness detection using face landmark points,"
How to detect liveness of face using face landmark points? 
I am getting face landmarks from android camera frames. And I want to detect liveness using these landmark points. 
How to tell if a human is making a specific movement that can be useful for liveness detection? 
","['machine-learning', 'object-detection', 'object-recognition', 'face-recognition']",
How to handle varying length of inputs that represent dependencies and recursivity in deep neural networks in case of regression?,"
I wanna solve a problem of regression to predict a factor. I decide to go with Deep Neural Networks as solution for my problem.
The features in this problem represent loop characteristic such us loop nest level, loop sizes. The loops hold also instruction (operations) that in itself represent many characteristics like number of variables, loads, stores, etc.
Those instructions maybe positions in the innermost loop or in the middle or under the outermost loop. 
We extract here characteristics of Computations in Tiramisu language.
For example, if we have two iterator variables:
var i(""i"", 0, 20), j(""j"", 0, 30);

and we have the following computation declaration:
computation S(""S"", {i,j}, 4);

This is equivalent to writing the following C code:
 for (i=0; i<20; i++)
      for (j=0; j<30; j++)
         S(i,j) = 4;

The aspect of receptivity here we can have something like this: 
 computation S(""S"", {i,j}, 4+M); 

where ""M"" is computation also.
We considered those features to represent Computations in Tiramisu language.
/** Computations=loops **/
   ""nest_level"" : 3,   // Number of nest levels
   ""perfect_nested"" : 1 ,  // 1 if the loop is perfectly nested , 0 instead
   ""loops_sizes"" : [200,100,300] // Sizes of for loops 
   ""lower_bound"" : [5,0,0], // Bounds of the iterator (in this e.g [2, 510])
   ""upper_bound"" : [205,100,300], 
   ""nb_intern_if"" : 1000, //number of if statements in the nest
   ""nb_exec_if"" : 300, // Estimation of number if 
   ""prec_if"" : 1,  // 1=true if the nest is preceded by if statement  
   ""nb_dependencies_intern"" : 5, // number of dependencies between loops levels in the nest 
   // ""dependencies_extern"" : , // number of extern nest dependencies  
    ""nb_computations"" : 3,  // number of operations (computations) in the nest 
    //std::map<std::string, computation_features *> computations_features; // list of operations Features in the nest

And this to represent operations: 
/** Instructions **/
""n"" : 1, <-- Number of computations
    ""compt_array"" : [
      {
              // Should we add to which level should belong the instructions ?

              ""comp_id"" : 1,  // Unique id for the instructions
              ""nb_var"" : 5,   // Number of the variables in the instructions
              ""nb_const"" : 2, // Number of constantes in the instructions
              ""nb_operands"" : 3, // Number of operands of the operatiion ( including direct values)
              ""histograme_loads"" :  [2,1,5,8], // number of load ops. i.e. acces to inputs per type
              ""histograme_stores"" :  [2,1,5,8], // number of load ops. i.e. acces to inputs per type
              ""nb_library_call"" : 5;  // number of the computation library_calls 
              ""wait_library_argument"" : 2, // number of ar 
              ""operations_histogram"" : [ // number of arithmetic operations per type
                    [0, 2, 0, 0],  // p_int32
                    [0, 0, 0, 0],  // float, for example
                    [0, 0, 0, 0],  // ...
                    [0, 0, 0, 0],
                    [0, 0, 0, 0],
                    [0, 0, 0, 0], // ...
                    [0, 0, 0, 0]  // boolean    
              ]              
      }
  ]

We may also represent iterator as a characteristic of computation.
The problem in those features we have: 

Loops (Computations) can hold many operations ==> the size of operation vector is variable.
Instructions (Operations) can be in the level 2, 3 under the innermost 
I mean we can have this situation: 


for (i=0; i < 20; i++)
    S(i, j) = 4;
for (j=0; j < 30; j++)
    ...


or this one:

for (i=0; i<20; i++)
      for (j=0; j<30; j++)
         S(i,j) = 4;


Or many other situations with many instructions ==> their is dependencies between the position of the instruction and the level (iterator) in which it is, in the other way operation hold the id of the iterator :§.

The operation on itself can be composed with another Computation(Loop nest) which on itself hold instruction and so forth ==> Resistivity.

After some research i have found that that DNN has fixed input size. RNN, recursive NN can handle with varying length of inputs. But what about others 
how should I present that as input?
","['neural-networks', 'deep-neural-networks']",
Training a neural network to output the conditional probability of an event when the training data output is only binary,"
I have a dataset with hundreds of thousands of training examples. There are 27 input variables and one output variable which is always a 0 or a 1, based on whether an event happened or not.
My network therefore has 27 inputs and 1 output. I want the network's output to be a confidence guess of how likely the event is to happen, for example if the output is 0.23 then that represents that the network thinks the event has a 23% chance of happening.
I am using back propagation to train the neural network. It does appear to work well and the network outputs a higher number when the event is more likely and a lower number when the event is less likely.
Would it be a valid concern that my training data only has 0 or 1 values as outputs, when this is not truly what I want the network to output?
My concern comes from the fact that back propagation attempts to reduce the square of the error between the network's output, and the value of the output in the training data, which is always a 0 or a 1. Because it is the square of the error it is trying to reduce, I'm concerned that it's probability output may not be a linear mapping to the true probability of the event happening based on the 27 inputs it is seeing.
Is this a valid concern? And are there any techniques I can use to get a neural network to output a linear confidence guess between 0 and 1 when my test data only has outputs of 0 or 1?
I am using the sigmoid activation function for all of my neurons, would there be a better choice of activation function for this problem?
Edit: Thanks to Xpector's answer, I now understand that not all back propagation aims to reduce the square of the error, it depends on the loss function used. I am including a part of the back propagation code I have used here which calculates the error:
var neuronOutput = layerOutputs[i];
var error = (neuronOutput - desiredOutput[i]);
errors[i] = error * Maths.SigmoidDerivative(neuronOutput);

This is from an open source RProp implementation. I am not sure what loss function is being used here.
",['neural-networks'],
"Is the definition of machine learning by Mitchell in his book ""Machine Learning"" valid?","
Mitchell's definition of machine learning is as follows:

A computer program is said to learn from experience E with respect to some task T and performance measure P, if its performance at task T, as measured by P, improves with experience E.

Here, we talk about what it means for a program to learn rather than a machine, but a program and a machine aren't equivalent, so this is a definition about ""program learning"" rather than ""machine learning"". How is this consistent?
","['machine-learning', 'terminology', 'definitions']",
How do neural networks weigh multiple inputs/features of different dimensionality?,"
I am confused about how neural networks weigh different features or inputs.
Consider this example. I have 3 features/inputs: an image, a dollar amount, and a rating.  However, since one feature is an image, I need to represent it with very high dimensionality, for example with $128 \times 128 = 16384$ pixel values.  (I am just using 'image' as an example, my question holds for any feature that needs high dimensional representation: word counts, one-hot encodings, etc.)
Will the $16384$ 'features' representing the image completely overwhelm the other 2 features that are the dollar amount and rating?  Ideally, I would think the network would consider each of the three true features relatively equally.  Would this issue naturally resolve itself in the training process? Would training become much more difficult of a task?
","['neural-networks', 'deep-learning', 'features']","As stated in your example, the three features are: an image, a price, a rating. Now, you want to build a model that uses all of these features and the simplest way to do is to feed them directly into the neural network, but it's inefficient and fundamentally flawed, due to the following reasons:In the first dense layer, the neural network will try to combine raw pixel values linearly with price and rating, which will produce features that are meaningless for inference.It could perform well just by optimizing the cost function, but the model performance will be nowhere as good as it could perform with a good architecture.So, the neural network doesn't care if the data is a raw pixel value, price, or rating: it would just optimize it to produce the desired output. That is why it is necessary to design a suitable architecture for the given problem.Possible architecture for your given example :Separate your raw features, i.e. pixel value, and high-level data, i.e. price and ratingStack 2-3 dense layers for raw features (to find complex patterns in images)Stack 1-2 dense layers for high-level featuresCombine them together in a final dense layerIf you want to de-emphasize the importance of the image, just connect the first dense layer 16,384 to another layer having fewer connections, say 1024, and have more connections from the high-level data, say 2048.So, again, here's the possible architecture"
What is the difference between a non-stationary policy and a state that stores time?,"
This question is related to What does ""stationary"" mean in the context of reinforcement learning?, but I have a more specific question to clarify the difference between a non-stationary policy and a state that includes time.
My understanding is that, in general, a non-stationary policy is a policy that doesn't change. My first (probably incorrect) interpretation of that was that it meant that the state shouldn't contain time. For example, in the case of game, we could encode time as the current turn, which increases every time the agent takes an action. However, I think even if we include the turn in the state, the policy is still non-stationary so long as sending the same state (including turn) to the policy produces the same action (in case of a deterministic policy) or the same probability distribution (stochastic policy).
I believe the notion of stationarity assumes an additional implicit background state that counts the number of times we have evaluated the policy, so a more precise way to think about a policy (I'll use a deterministic policy for simplicity) would be:
$$ \pi : \mathbb{N} \times S \rightarrow \mathbb{N} \times A $$
$$ \pi : (i, s_t) \rightarrow (i + 1, s_{t+1}) $$
instead of $\pi : S \rightarrow A$.
So, here is the question: Is it true that a stationary policy must satisfy this condition?
$$ \forall i, j \in \mathbb{N}, s \in S, \pi (i, s) = \pi(j, s) $$
In other words, the policy must output the same result no matter when we evaluate it (either the ith or jth time). Even if the state $S$ contains a counter of the turn, the policy would still be non-stationary because for the same state (including turn), no matter how many times you evaluate it, it will return the same thing. Correct?
As a final note, I want to contrast the difference between a state that includes time, with the background state I called $i$ in my definition of $\pi$. For example, when we run an episode of 3 steps, the state $S$ will contain 0, 1, 2, and the background counter of number of the policy $i$ will also be set to 2. Once we reset the environment to evaluate the policy again, the turn, which we store in the state, will go back to 0, but the background number of evaluations won't reset and it will be 3. My understanding is that in this reset is when we could see the non-stationarity of the policy in action. If we get a different result here it's a non-stationary policy, and if we get the same result it's a stationary policy, and such property is independent of whether or not we include the turn in the state. Correct?
","['reinforcement-learning', 'policies']",
Why am I getting spikes in the values of the loss function during training?,"
I trained a neural network on the UNSW-NB15 dataset, but, during training, I am getting spikes in the loss function. The algorithms see part of this UNSW dataset a single time. The loss function is plotted after every batch.

For other datasets, I don't experience this problem.  I've tried different optimizers and loss functions, but this problem remains with this dataset.
I'm using the fit_generator() function from Keras. Is there anyone experience this problem using Keras with this function? 
","['neural-networks', 'machine-learning', 'keras', 'objective-functions']",
Loss/accuracy on Synthetic data [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am trying to understand if there is any difference in the the interpretation of accuracy and loss on synthetic data vs real data.
",['data-science'],
Why is the learning rate is already very small (1e-05) while the model convergences too fast?,"
I am training a video prediction model.
According to the loss plots, the model convergences very fast while the final loss is not small enough and the generation is not good.
Actually, I have test the lr=1e-04and lr=1e-05, the loss plots drop down a little more slowly, but it's still not ideal. But I think lr=1e-05should be small enough, isn't it?
How should I fix my model or the hyper parameters?
","['machine-learning', 'deep-learning', 'computer-vision']",
How can it be shown clearly and transparently that the outcomes of data-driven health and care technology are validated?,"
The National Health Service (NHS) wrote down several principles in a document Code of conduct for data-driven health and care technology (updated 18 July 2019). I am concerned with principle 7.

Show what type of algorithm is being developed or deployed, the ethical examination of how the data is used, how its performance will be validated and how it will be integrated into health and care provision
Demonstrate the learning methodology of the algorithm being built.
Aim to show in a clear and transparent way how outcomes are validated.

But how exactly can outcomes be shown in a clear and transparent way how outcomes are validated?
","['social', 'ethics', 'healthcare']",
Can (trained) neural networks be combined with symbolic AI to perform operations like AND?,"
Does anyone work out ways of relating trained neural networks by symbolic AI?
For example, if I train a network on pictures of dogs, and I train a network on pictures of shirts. You could imagine that the simplest way (without going through the process from scratch) of identifying ""dog AND shirt"" would be to perform an AND operation on the last output of the individual cat & dog neural nets. So, ""dog AND shirt"" would amount to AND'ing the output of two nets.
But this operation AND could be replaced with a more complicated operation. And, in principle, I could ""train"" a neural network to act as one of these operations.  For instance, maybe I could figure out the net that describes some changeable output ""X"" being ""on the shirt."" This would be sort of like a ""functional"" in mathematics (in which we are operating are considering the behavior of a network whos input could be any network). If I can figure out this ""functional"", then I would be able to use it symbolically and determine queries like ""dog on the shirt""? - ""cat on the shirt""?
It seems like to me there's a lot of sense to turn specific neural networks into more ""abstract"" objects - and that there would be a lot of power in doing so.
","['neural-networks', 'reference-request', 'logic', 'symbolic-ai', 'neurosymbolic-ai']",
How do I apply the value iteration algorithm when there are two goal states?,"
I am working through the famous RL textbook by Sutton & Barto. Currently, I am on the value iteration chapter. To gain better understanding, I coded up a small example, inspired by this article.
The problem is the following

There is a rat (R) in a grid. Every square is accessible. The goal is to find the cheese (C). However, there is also a trap (T). The game is over whenever the rat either find the cheese or is trapped (these are my terminal states). 


The rat can move up, down, left, and right (always by one square). 
I modeled the reward as follows:
-1 for every step
5 for finding the cheese
-5 for getting trapped

I used value iteration for this and it worked out quite nice.
However, now I would like to add another cheese to the equation. In order to win the game, the rat has to collect both cheese pieces.

I am unsure how to model this scenario. I don't think it will work when I use both cheese squares and the trap square as terminal states, with rewards for both cheese squares.
How can I model this scenario? Should I somehow combine the two cheese states into one?
","['reinforcement-learning', 'sutton-barto', 'value-iteration']","What you could do is to trigger environment termination when rat either:  The problem with such setup is that, when the rat picks a single piece, it would move one step to the side, and then it would come back to the same cheese spot so it would keep exploiting the same spot indefinitely. The solution to that would be to simply remove the cheese piece once the rat picks it up, so that it can't exploit it indefinitely. Sadly, another problem would arise which is partial observability: Markov property wouldn't be fulfilled because the current action wouldn't depend on the current state solely, it would be important whether cheese piece was picked before or not. The solution to that would be to make environment fully observable. You could accomplish that by expanding the amount of information about your current state. Before, only your position on the grid was important, but now you would also add state features that tell you whether cheese piece at specific position was picked or not. You would basically add a flag for each cheese piece that has value of 1 if piece was picked, or value of 0 if it wasn't. That way you could remove cheese piece it rat picks it, and you would still have full information. I believe this setup would work."
Modelling gut-feeling/subconscious knowledge of stock market traders,"
Some (stock market) traders have the ability to produce a high percentage of winning trades (80%+, positive return) over years. I had the chance to look into real money trades of two such traders and I also got trading instructions from them for research. 
Now the interesting part is that if you strictly follow their rules then you usually end up with more losers than winners on the long run. But after a while you get some kind of subconscious ""feeling"" for winners which also shows in the results. I assume that this ""feeling"" is a hidden function which can be modeled.
My question is: Is there work about how to model such ""gut feeling"" and subconscious knowledge by means of machine learning (especially with little training data sets)? Is there relevant literature about this topic?
Regards,
","['neural-networks', 'machine-learning', 'reinforcement-learning']",
Why does Deep Q Network outputs multiple Q values?,"
I am learning Deep RL following this tutorial: https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8
I understand everything but one detail:
This image shows the difference between a classic Q learning table and a DNN.
It states that a Q table needs a state-action pair as input and outputs the corresponding Q value whereas a Deep Q network needs the state as feature input and outputs the Q value for each action that can be made in that state.
But shouldn't the state AND the action together be the input to the network and the network just outputs a single Q value?

","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'action-spaces']","I think this was just a ""clever"" design choice. You can actually design a neural network (NN), to represent your Q function, which receives as input the state and an action and outputs the corresponding Q value. However, to obtain $\max_aQ(s', a)$ (which is a term of the update rule of the Q-learning algorithm) you would need a ""forward pass"" of this network for each possible action from $s'$. By having a NN that outputs the Q value for each possible action from a given $s'$, you will just need one forward pass of the NN to obtain $\max_aQ(s', a)$, that is, you pick the highest Q value among the outputs of your NN.In the paper A Brief Survey of Deep Reinforcement Learning (by Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage and Anil Anthony Bharath), at page 7, section ""Value functions"" (and subsection ""Function Approximation and the DQN""), it's written It was designed such that the final fully connected layer outputs $Q^\pi(s,\cdot)$ for all action values in a discrete set of actions — in this case, the various directions of the joystick and the fire button. This not only enables the best action, $\text{argmax}_a Q^\pi(s, a)$, to be chosen after a single forward pass of the network, but also allows the network to more easily encode action-independent knowledge in the lower, convolutional layers."
"Is it possible to use a trained neural network to predict a feature, given other features and output?","
I have a neural network that is already trained to predict two continuous outputs from a set of 7 continuous features. 
Is there any way to apply the network to predict one of the input features, given other 6 features and the two outputs?
","['matlab', 'deep-learning']",
Is fuzzy logic connected to neural networks?,"
Fuzzy logic is typically used in control theory and engineering applications, but is it connected fundamentally to classification systems?
Once I have a trained neural network (multiple inputs, one output), I have a nonlinear function that will turn a set of inputs  into a number that will estimate how close my set of given inputs are to the trained set. 
Since my output number characterizes ""closeness"" to the training set as a continuous number, isn't this kind of inherently some sort of fuzzy classifier? 
Is there a deep connection here in the logic, or am I missing something? 
","['neural-networks', 'fuzzy-logic']",
Is policy learning and online system identification the same?,"
In some newer robotics literature, the term system identification is used in a certain meaning. The idea is not to use a fixed model, but to create the model on the fly. So it is equal to a model-free system identification. Perhaps a short remark for all, who doesn't know what the idea is. System identification means, to create a prediction model, better known as a forward numerical simulation. The model takes the input and calculates the outcome. It's not exactly the same like a physics engine, but both are operating with a model in the loop which is generating the output in realtime.
But what is policy learning? Somewhere, I've read that policy learning is equal to online system identification. Is that correct? And if yes, then it doesn't make much sense, because reinforcement learning has the goal to learn a policy. A policy is something which controls the robot. But if the aim is to do system identification, than the policy is equal to the prediction model. Perhaps somebody can lower the confusion about the different terms ...
Example Q-learning is a good example for reinforcement learning. The idea is to construct a q-table and this table controls the robot movements. But, if online-system-identification is equal to policy learning and this is equal to q-learning, then the q-table doesn't contains the servo signals for the robot, but it provides only the prediction of the system. That means, the q-table is equal to a box2d physics engine which can say, what x/y coordinates the robot will have. This kind of interpretation doesn't make much sense. Or does it make sense and the definition of a policy is quite different?
",['control-problem'],"From the book Reinforcement Learning, An Introduction (R. Sutton, A. Barto):The term system identification is used in adaptive control for what we
  call model-learning (e.g., Goodwin and Sin, 1984; Ljung and S
  ̈oderstrom, 1983; Young, 1984).Model-learning refers to the act of learning the model (environment). Reinforcement Learning can be divided into two types:Model-based - first we build a model of an environment and then do the control.Model-free - we do not try to model the behaviour of the environment. Policy learning is the act of learning optimal policy. You can do it in two ways:On-policy learning - learn about the policy $π$ by sampling from the same policy.Off-policy learning - learn about the policy $π$ from the experience sampled from some other policy (e.g. watching different agent playing a game)."
Azure ML studio pull directly from sharepoint,"
I am toying around with creating a probability of win calculator for proposals that we do. the information on each proposal is housed in our corporate SharePoint (which I am the admin) 
Is there a way to pull directly from SharePoint as the data source rather than have to export to xls then upload each time the data updates?
",['machine-learning'],
Why is gradient ascent necessary when training Actor Critic agents?,"
I have read a lot on Actor Critic and I'm not convinced that there is a qualitative difference doing direct gradient updates on the network and slightly adjusting a soft-max output in the direction of the advantage function and doing gradient descent on the error. 
Can anyone explain why updating the gradient directly is necessary? 
","['reinforcement-learning', 'actor-critic-methods']",
Robot Arm Deep Q Learning Actions,"
Hello I am new to reinforcement learning and robotics. So far I have an understanding of the concept on 2D world. You can make agent move one step in one direction. However, how do you define movement action of a robot arm? I am a bit lost over here. Any useful links or keywords would be very appreciated! :)
","['deep-learning', 'reinforcement-learning', 'q-learning', 'robotics']","It depends a lot on the hardware of your robot arm. 
Assuming that your servos have encoder information, if you have access to servos that have limited control like ""rotate left/rotate right"" functionality, you can phrase the your action space to be [""move left"", ""stop"", ""move right""]. In this way you can implement a discrete action space with 3 actions per servo and have an agent learn to move the servos around the space. If your servos are connected to each other in an elbow/shoulder configuration, you can have a 9 discrete action setup essentially making a box of cardinal directions:Up+Left----------Up-------Up+RightLeft--------------Stop---------RightDown+Left-----Down-----Down+RightIf you have 3 or more servos, you can still use the same idea of discrete actions but the number of discrete actions grows by a factor of 3 with each servo as your action space is now the cross product of all of the other servos. Alternatively you can use a ""multi-headed"" agent where each head chooses actions for a certain servo but there are pros and cons for both depending on your usecase.If you have more advanced servos like Dynamixels which have high quality encoders, you'll have access to more advanced controls schemes. For instance, Dynamixels allow you to give actions in encoder space, angle space, and even velocity space. For example, you could give the action of ""go to encoder value of 500"" or ""go to 90 degrees"" or ""move .5 radians/second"". All of these approaches are useful for certain tasks. For humans controlling the arm using a joystick, the velocity based is the most intuitive and, in my experience, the same is true for RL agents using continuous control.If you are using continuous control, you should normalize all of your action spaces within your agent then ""unnormalize (?)"" them before giving the actions to your servos.
For instance if your servo velocity ranges from -3.5 rad/sec to +3.5 rad/sec, have your agent select actions in the range of [-1,1] then multiply by 3.5 to get the velocity.In either case, one thing that should be noted is that you give your robotic arm enough time to actually perform the action that the agent selected. If not you will see your robot ""jitter"" back and forth quickly as your agent selects actions randomly. This is bad for a few reasons but most importantly because it might break your servos. To overcome this issue, give each action a little more time to be ""actualized"" by your robot. This can either come in the form of adding a delay in your code while your servos do ""the thing"" or by using an ""iterations since last action update"" counter to get a new action from the agent after a certain number of iterations. Not only is this better for your hardware, this also leads to better exploration of your state space as your agent can move through the state space encountering similar states more frequently.A last thing to be aware of is to set hard coded limits on the servos so that your agent doesn't kill your servos by banging the arm against a table or something. Finding these safe bounds isn't always easy as multiple jointed configurations can have multiple ways of hitting the bounds and it might require a forward dynamics model to find these limits. 
If you have only 2 servos, it should be pretty easy to find though :)."
To what does the number of hidden layers in a neural network correspond?,"
In a neural network, the number of neurons in the hidden layer corresponds to the complexity of the model generated to map the inputs to output(s).  More neurons creates a more complex function (and thus the ability to model more nuanced decision barriers) than a hidden layer with less nodes.
But what of the hidden layers?  What do more hidden layers correspond to in terms of the model generated?
","['neural-networks', 'models', 'hidden-layers']","This is a very interesting question that you ask. I believe, this post and this post (written by me) well address your question. However, it deserves an explanation here.1. Fully connected networksThe more layers you add, the more ""nonlinear"" your network becomes. For instance, in the case of two spirals problem, which requires a ""highly nonlinear separation"", the first known architecture to solve the problem was pretty advanced for that time: it had 3 hidden layers and also it had skip-connections (very early ResNet in 1988). Back then, computing was a way less powerful and training methods with momentum were not known. Nevertheless, thanks to the multilayer architecture, the problem was solved. Here, however, I was able to train a single-hidden layer network to solve the spirals problem using Adam.2. Convolutional nets (CNNs)An interesting partial case of neural networks are CNNs. They restrict the architecture of the first layers, known as convolutional layers, so that there is a much smaller number of trainable parameters due to the weights sharing. What we have learned from computer vision, moving towards the end of CNNs layers, their receptive fields become larger. That means that the subsequent CNN layers ""see"" more than their predecessors. Conceptually, first CNN layers can recognize simpler features such as edges and textures, whereas final CNN layers contain information about more abstract objects such as trees or faces.3. Recurrent nets (RNNs)RNNs are networks with layers which receive some of their outputs as inputs. Technically, a single recurrent layer is equivalent to an infinite (or at least large) number of ordinary layers. Thanks to that recurrence, RNNs retain an internal state (memory). Therefore, it is much more difficult to answer your question in the case of recurrent nets. What is known, due to their memory, RNNs are more like programs, and thus are in principle more complex than other neural networks. Please let me know if you find an answer to your question in the last case.To conclude, the higher number of hidden layers may help to structure a neural network. Thanks to the recent developments such as ResNets and backpropagation through time, it is possible to train neural networks with a large number of hidden layers."
Is there more than one Q-matrix update formula?,"
I asked a question a while ago here and since then I've been solving the issues within my code but I have just one question... This is the formula for updating the Q-Matrix in Q-Learning:
$$Q(s_t, a_t) = Q(s_t, a_t) + \alpha \times (R+Q(s_{t+1}, max_a)-Q(s_t, a_t))$$
However, I saw a Q-Learning example that uses a different formula, which I'm applying to my own problem and I'm getting good results:
$$Q(s_t, a_t) = R(s_t,a_t) + \alpha \times Q(s_{t+1}, max_a)$$
Is this valid?
","['reinforcement-learning', 'python', 'q-learning']",
Choosing more than one action in a parameterized policy,"
I would like to implement a variant of policy iteration that can choose one or more actions in each state. An example would be to heal and move in the game of Doom.
Parameterizing the power set of all single actions would be one idea, but I was wondering if somebody achieved good results on a similar problem, perhaps by simply defining some lower bound on the output layer and taking all actions with values larger than that bound (i.e. with actions and activation values {shoot=0.2, heal=0.51, move=0.6, jump=0.4} I would choose heal and move if the bound was 0.5)
Another idea was to collect these actions iteratively, i.e. choosing an action from a softmax output based on the state $s$ (taking action ""healing"") and then constructing and using some temporary state $s_t$ to evaluate that state to find another action (e.g. ""moving""). This would require some dummy action that is just used to signal the end of that iteration procedure (i.e. choosing action $n+1$ will not add any other action to the set $\{ \text{healing}, \text{moving} \}$, but it will lead to the execution of those two actions and transition to the next state $s'$.
","['reinforcement-learning', 'policy-iteration']",
What are the main benefits of using Bayesian networks?,"
I have some trouble understanding the benefits of Bayesian networks.
Am I correct that the key benefit of the network is that one does not need to use the chain rule of probability in order to calculate joint distributions?
So, using the chain rule:
$$
P(A_1, \dots, A_n) = \prod_{i=1}^n (A_i \mid \cap_{j=1}^{i-1} A_j)
$$
leads to the same result as the following (assuming the nodes are structured by a Bayesian network)?
$$
P(A_1, \dots, A_n) = \prod_{i=1}^n P(A_i \mid \text{parents}(A_i))
$$
","['applications', 'probability-distribution', 'probability-theory', 'bayesian-networks']",
Which model should I use to find (only) the object location (in terms of coordinates) in an image?,"
I am generating images that consist of points, where the object's location is where the most overlap of points occurs.

In this example, the object location is $(25, 51)$.
I am trying to train a model to just finds the location, so I don't care about the classification of the object. Additionally, the shape of the overlapping points where the object is located never changes and will always be that shape.
What is a good model for this objective?
Many of the potential models I've been looking at (CNN, YOLO, and R-CNN) are more concerned with classification than location. Should I search the image for the overlapping dots, create a bound box around them, then retrieve the boxes' coordinates?
","['neural-networks', 'machine-learning', 'computer-vision', 'object-recognition', 'model-request']",
Can we define the AI singularity mathematically?,"
The ""AI Singularity"" or ""Technological Singularity"" is a vague term that roughly seems to refer to the idea of:

Humans can design algorithms
Humans can improve algorithms
Eventually algorithms we design might end up being as good as humans at designing and improving algorithms
This might lead to these algorithms designing better versions of themselves, eventually becoming far more intelligent than humans. This improvement would continue to grow at an increasing rate until we reach a ""singularity"" where an AI is capable of making technological progress at a rate far faster than we could ever imagine

Also known as an Intelligence Explosion. This rough idea has been heavily debated as to its feasibility, how fast it'll take (if it does happen), etc.
However I'm not aware of any formal definitions of the concept of ""singularity"". Are there any? If not, do we have close approximations?
I have seen AIXI and the Gödel machine, but these both require some ""reward signal"" — it is unclear to me what reward signal one should choose to bring about a singularity, or really how those models are even relevant here. Because even if we had an oracle that can solve any formal problem given to it, it's unclear to me how we could use that to cause a singularity to happen (see this question for more discussion on that note).
","['math', 'learning-algorithms', 'singularity']",
Is a very powerful oracle sufficient to trigger the AI singularity?,"
Lets say we have a oracle $S$ that, given any function $F$ and desired output $y$,  can find an input $x$ that causes $F$ to output $y$ if it exists, or otherwise returns nil. I.e.:
$$S(F, y) = x \implies F(x) = y$$
$$S(F, y) = nil \implies !\exists  x \hspace{10px}s.t.\hspace{10px} F(x) = y$$
And $S$ takes $1$ millisecond to run (plus the amount of time it takes to read the input and write the output), regardless of $F$ or $y$. $F$ is allowed to include calls to $S$ in itself.
Clearly with this we can solve any NP-Complete problem in constant time (plus the amount of time it takes to read the input and write the output), and in fact we can go further and efficiently solve any optimization problem:
def IsMin(Cost, MeetsConstraints, x):
  def HasSmaller(y):
    return MeetsConstraints(x) and Cost(y) < Cost(x) and y != x
  return MeetsConstraints(x) and S(HasSmaller, True) == nil

def FindMin(Cost, MeetsConstraints):
  def Helper(x):
    return IsMin(Cost, MeetsConstraints, x)
  return S(Helper, True)

Which means we can do something like:
def FindSmallestRecurrentNeuralNetworkThatPerfectlyFitsData(Data):
  def MeetsConstraints(x):
    return IsRecurrentNeuralNetwork(x) and Error(x, Data) == 0
  return FindMin(NumParamaters, MeetsConstraints)

And something similar for any other kind of model (random forest, random ensemble of functions, etc.). We can even solve the halting problem with this, which probably means that there is some proof similar to the halting problem proof that shows such an oracle could not exist. Lets assume this exists anyway, as a thought experiment. 
But I'm not sure how to take it from here to something that achieves endless self improvement. What exactly the ""singularity"" even means I suppose is tricky to define formally, but I'm interested in any simple definitions, even if they don't quite capture it.
A sidenote, here is one more function we can do:
IsEquivalent(G, H):
    def Helper(x):
      return G(x) != H(x)
    return P(Helper, True) == nil

","['algorithm', 'optimization', 'singularity']",
How do you implement NEAT by taking into account the loops?,"
I'm working on my own implementation of NEAT algorithm based on the original 2002 paper called ""Efficient Reinforcement Learning through Evolving Neural Network Topologies"" (by Kenneth O. Stanley and Risto Miikkulainen). The way the algorithm is designed it may generate loops in connection of hidden layer. Which obviously will cause difficulties in calculating the output.
I have searched and came across two types of approaches. One set like this example claim that the value should be calculated like a time series usually seen in RNNs and the circular nodes should use ""old"" values as their ""current"" output. But, this seems wrong since the training data is not always ordered and the previous value has nothing to do with current one.
A second group like this example claim that the structure should be pruned with some method to avoid loops and cycles. This approach apart from being really expensive to do, is also against the core idea of the algorithm. Deleting connections like this may cause later structural changes.
I my self have so far tried setting the unknown forward values as 0 and this hides the connection (as whatever weight it has will have no effect on the result) but have failed also for two reasons. One is my networks get big quickly destroying the ""smallest network required"" idea and also not good results.
What is the correct approach? 
",['neat'],
Should I model my problem as a semi-MDP?,"
I have a system (like a bank) that people (customers) are entered into the systems by a Poisson process, so the time between the arrival of people (two consecutive customers) will be a random variable. The state of the problem is related to just the system (bank), and the action, made inside the system, can be e.g. offering the customer promotion or not (just based on the state of the system, not the status of customers). 
To model the problem through RL, 1) it is possible to discretize time horizon into very short time interval (for example 5 minutes as a stage) such that in each time interval, just a single customer enter to our system. On the other hand, 2) it is possible that stages are defined as the time when a customer enters our system. 
My questions are:

Is the second approach an semi-MDP (SMDP)? If I want to solve it with RL, should I use hierarchical RL?
In the first approach, if a customer enters in a time interval, it is easy to update the Q values. However, what should we do, if we are in state $S$ and take action $A$, but no customer enters our system, so we do not receive any reward for the pair of $(S, A)$? There would be no difference if we would take action $A_{1}$, $A_{2}$, and so on. This can happen for several consecutive time intervals. I think it is more challenging when we consider eligibility traces.

","['reinforcement-learning', 'markov-decision-process', 'semi-mdp', 'hierarchical-rl']",
Why is the number of output channels 16 in the hidden layer of this CNN?,"
In this tutorial from Jeremy Howard: What is torch.nn really? he has an example towards the end where he creates a CNN for MNIST. In nn.Conv2d, he makes the in_channels and out_channels: (1,16), (16,16), (16,10).
I get that the last one has to be 10 because there are 10 classes and we want 'probabilities' of each class. But why go up to 16 first? How do you choose this value? And why not just go from 1 to 10, 10 to 10, and 10 to 10? Does this have to do with the kernel_size and stride?
All of the images are 28x28, so I can't see any correlation between these values and 16 either.
class Mnist_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)

    def forward(self, xb):
        xb = xb.view(-1, 1, 28, 28)
        xb = F.relu(self.conv1(xb))
        xb = F.relu(self.conv2(xb))
        xb = F.relu(self.conv3(xb))
        xb = F.avg_pool2d(xb, 4)
        return xb.view(-1, xb.size(1))

","['convolutional-neural-networks', 'pytorch', 'hyper-parameters', 'filters']","I understand your question as: ""How did the author select the number of neurons in their hidden layer?""The number of neurons in the hidden layer is how you can control the complexity of the function you are trying to generate to map the inputs to an output. The more neurons in the hidden layer the more complex the function thus you can capture more intricate decision barriers. However, the more complex function is harder to optimize and will lead to lower performance scores. The goal here is to find the right tradeoff to maximize your performance. You can tune the number of hidden neurons as a hyper-parameter using cross-validation.There isn't any formula to determine the number of neurons you will need, however you can get an intuition based on the number of inputs and outputs you will have. Generally, you want more hidden neurons than input and output neurons. Since most people are programmers who are writing neural networks, we are used to working with units in $2^n$. Thus, 16 is chosen over 10, and 32 would be chosen over 28."
What is self-supervised learning in machine learning?,"
What is self-supervised learning in machine learning? How is it different from supervised learning?
","['machine-learning', 'comparison', 'supervised-learning', 'self-supervised-learning', 'representation-learning']","The term self-supervised learning (SSL) has been used (sometimes differently) in different contexts and fields, such as representation learning [1], neural networks, robotics [2], natural language processing, and reinforcement learning. In all cases, the basic idea is to automatically generate some kind of supervisory signal to solve some task (typically, to learn representations of the data or to automatically label a dataset).I will describe what SSL means more specifically in three contexts: representation learning, neural networks and robotics.The term self-supervised learning has been widely used to refer to techniques that do not use human-annotated datasets to learn (visual) representations of the data (i.e. representation learning).In [1], two patches are randomly selected and cropped from an unlabelled image and the goal is to predict the relative position of the two patches. Of course, we have the relative position of the two patches once you have chosen them (i.e. we can keep track of their centers), so, in this case, this is the automatically generated supervisory signal. The idea is that, to solve this task (known as a pretext or auxiliary task in the literature [3, 4, 5, 6]), the neural network needs to learn features in the images. These learned representations can then be used to solve the so-called downstream tasks, i.e. the tasks you are interested in (e.g. object detection or semantic segmentation).So, you first learn representations of the data (by SSL pre-training), then you can transfer these learned representations to solve a task that you actually want to solve, and you can do this by fine-tuning the neural network that contains the learned representations on a labeled (but smaller dataset), i.e. you can use SSL for transfer learning.This example is similar to the example given in this other answer.Some neural networks, for example, autoencoders (AE) [7] are sometimes called self-supervised learning tools. In fact, you can train AEs without images that have been manually labeled by a human. More concretely, consider a de-noising AE, whose goal is to reconstruct the original image when given a noisy version of it. During training, you actually have the original image, given that you have a dataset of uncorrupted images and you just corrupt these images with some noise, so you can calculate some kind of distance between the original image and the noisy one, where the original image is the supervisory signal. In this sense, AEs are self-supervised learning tools, but it's more common to say that AEs are unsupervised learning tools, so SSL has also been used to refer to unsupervised learning techniques.In [2], the training data is automatically but approximately labeled by finding and exploiting the relations or correlations between inputs coming from different sensor modalities (and this technique is called SSL by the authors). So, as opposed to representation learning or auto-encoders, in this case, an actual labeled dataset is produced automatically.Consider a robot that is equipped with a proximity sensor (which is a short-range sensor capable of detecting objects in front of the robot at short distances) and a camera (which is long-range sensor, but which does not provide a direct way of detecting objects). You can also assume that this robot is capable of performing odometry. An example of such a robot is Mighty Thymio.Consider now the task of detecting objects in front of the robot at longer ranges than the range the proximity sensor allows. In general, we could train a CNN to achieve that. However, to train such CNN, in supervised learning, we would first need a labelled dataset, which contains labelled images (or videos), where the labels could e.g. be ""object in the image"" or ""no object in the image"". In supervised learning, this dataset would need to be manually labelled by a human, which clearly would require a lot of work.To overcome this issue, we can use a self-supervised learning approach. In this example, the basic idea is to associate the output of the proximity sensors at a time step $t' > t$ with the output of the camera at time step $t$ (a smaller time step than $t'$).More specifically, suppose that the robot is initially at coordinates $(x, y)$ (on the plane), at time step $t$. At this point, we still do not have enough info to label the output of the camera (at the same time step $t$). Suppose now that, at time $t'$, the robot is at position $(x', y')$. At time step $t'$, the output of the proximity sensor will e.g. be ""object in front of the robot"" or ""no object in front of the robot"". Without loss of generality, suppose that the output of the proximity sensor at $t' > t$ is ""no object in front of the robot"", then the label associated with the output of the camera (an image frame) at time $t$ will be ""no object in front of the robot""."
How the actor use the output from the critic to make action in actor-critic network?,"
I am reading about the actor-critic architecture. I am confused about how the actor determines the action using the value (or future reward) from the critic network.
Below you have the most popular picture of actor-critic network. 

It looks like the input of the actor network is only the ""state"" variable ($s_t$), it has nothing to do with the critic network.
However, from the equation below

the actor seems to be related to critic network. 
I have a few questions

Does actor network has two inputs, state variable and future reward (output from critic network), or only the state variable?
If the actor network does take future reward as input, how does it use it? Only during the training stage, or in the action making stage?
Is there a ""policy iteration"" procedure happens during decision making stage, i.e. for every state $s_t$, policy network will make several attempts with critic network, and output the best policy? 

","['reinforcement-learning', 'actor-critic-methods']",
Complex systems constituting an entity unto itself,"
Introduction:
The notion that various social complex systems (e.g. society, family, business company, state, etc) could be regarded as ones exhibiting consistent traits of behaviour of their own - suggesting that they are entities unto themselves, some sort of organisms on their own or even intelligent entities on their own - is not new. I have personally stumbled upon papers of scholars who straightforwardly speak of such systems as if they were already proven to be singular entities.
That kind of assumption has entered the vernacular, as well, long time ago - e.g. ""the state wants to..."" , ""society responds to conflict by..."", ""the family dynamics seeks balance through..."", etc. 
Therefore, we could even assume at one point, that such social systems are not only organisms of their own, but even some sort of artificial intelligence entities (as long as we could see them as an artificial product of human activity).
We are generally used to seeing ourselves as conscious entities and we are also good at exploring entities of less complexity than ourselves. But when it comes to entities which consists of us as mere components, we are not ready to mentally process that idea - it sounds as either too abstract or too sci-fi (think of Stanisław Lem's work).
Question:

While the average Joe could easily say ""The state wants to..."" or ""Society responds to..."", etc, how exactly do we prove (or at least gather some sort of supporting evidence) that a complex social system really exhibits a behaviour of its own? 
Under what conditions could we regard it as some sort of spontaneously born artificial intelligence? 
If that were true, how could we predict if that AI would procreate and bring about other social system forms which are also entities unto themselves? How could we possibly become aware if that has already happened?

","['philosophy', 'social', 'artificial-consciousness']",
How can we estimate the transition model and reward function?,"
In reinforcement learning (RL), there are model-based and model-free algorithms. In short, model-based algorithms use a transition model $p(s' \mid s, a)$ and the reward function $r(s, a)$, even though they do not necessarily compute (or estimate) them. On the other hand, model-free algorithms do not use such a transition model or reward function, but they directly estimate a value function or policy by interacting with the environment, which allows the agent to infer the dynamics of the environment.
Given that model-based RL algorithms do not necessarily estimate or compute the transition model or reward function, in the case these are unknown, how can they be computed or estimated (so that they can be used by the model-based algorithms)? In general, what are examples of algorithms that can be used to estimate the transition model and reward function of the environment (represented as either an MDP, POMDP, etc.)?
","['reinforcement-learning', 'reference-request', 'algorithm-request', 'model-based-methods', 'model-free-methods']","Given that model-based RL algorithms do not necessarily estimate or compute the transition model or reward function, in the case these are unknown, how can they be computed or estimated (so that they can be used by the model-based algorithms)?A generally reliable approach to creating learned models from interacting with the environment, then using those models internally for planning or explicitly model-based learning, is still something of a holy grail in RL. An agent that can do this across multiple domains might be considered a significant step in autonomous AI. Sutton & Barto write in Reinforcement Learning: An Introduction (Chapter 17.5):More work is needed before planning with learned models can be effective. For example,the learning of the model needs to be selective because the scope of a model strongly affects planning efficiency. If a model focuses on the key consequences of the most important options, then planning can be efficient and rapid, but if a model includes details of unimportant consequences of options that are unlikely to be selected, then planning may be almost useless. Environment models should be constructed judiciously with regard to both their states and dynamics with the goal of optimizing the planning process. The various parts of the model should be continually monitored as to the degree to which they contribute to, or detract from, planning efficiency. The field has not
yet addressed this complex of issues or designed model-learning methods that take into account their implications.[Emphasis mine]This was written in 2019, so as far as I know still stands as a summary of state-of-the-art. There is ongoing research into this - for instance, the paper Model-Based Reinforcement Learning via Meta-Policy Optimization considers using multiple learned models to assess reliability. I have seen a similar recent paper which also assesses the reliability of the learned model and chooses how much it should trust it over a simpler model-free prediction, but cannot recall the name or find it currently.One very simple form of a learned model is to memorise transitions that have been experienced already. This is functionally very similar to the experience replay table used in DQN. The classic RL algorithm for this kind of model is Dyna-Q, where the data stored about known transitions is used to perform background planning. In its simplest form, the algorithm is almost indistinguishable from experience replay in DQN. However, this memorised set of transition records is a learned model, and is used as such in Dyna-Q.The basic Dyna-Q approach creates a tabular model. It does not generalise to predicting outcomes from previously unseen state, action pairs. However, this is relatively easy to fix - simply feed experience so far as training data into a function approximator and you can create a learned model of the environment that attempts to generalise to new states. This idea has been around for a long time. Unfortunately, it has problems - planning accuracy is strongly influenced by the accuracy of the model. This applies for both background planning and looking forward from the current state. Approximate models like this to date typically perform worse than simple replay-based approaches.This general approach - learn the model statistically from observations - can be refined and may work well if there is any decent prior knowledge that restricts the model. For example, if you want to model a physical system that is influenced by current air pressure and local gravity, you could have free parameters for those unknowns starting with some standardised guesses, and then refine the model of dynamics when observations are made, with strong constraints about the form it will take.Similarly, in games of chance with hidden state, you may be able to model the unknowns within a broader well-understood model, and use e.g. Bayesian inference to add constraints and best guesses. This is typically what you would do for a POMDP with a ""belief state"".Both of the domain-specific approaches in the last two paragraphs can be made to work better than model-free algorithms alone, but they require deep understanding/analysis of the problem being solved by the researcher to set up a parametric model that is both flexible enough to match the environment being learned, but also constrained enough that it cannot become too inaccurate."
Do I need an encoder-decoder architecture to predict the next item of a sequence?,"
I am trying to understand how RNNs are used for sequence modelling.
On a tutorial here, it mentions that if you want to translate say a sentence from English to French you can use an encoder-decoder set-up as they described.
However what if you want to do a sequence to sequence modelling where your inputs and outputs are of the same domain but you just want to predict the next output of a sequence.
For example if I want to use sequence modelling to learn the sine function. So say I have 20 y-coordinates from $y = sin(x)$ from 20 evenly spaced out x-coordinates and I want to predict the next 10 or so y-coordinates. Would I use an encoder-decoder setup here? 
","['deep-learning', 'natural-language-processing', 'recurrent-neural-networks', 'machine-translation']","You don't need to Encoder-Decoder here. When using seq2seq learning for text (for example, for translation) you need encoder-decoder to encode the words into the numeric vectors and decode the vectors into the words. Therefore, for your numerical case, you don't need an encoder or decoder to train the RNN."
"What is ""planning"" in the context of reinforcement learning, and how is it different from RL and SL?","
This is an excerpt taken from Sutton and Barto (pg. 3):

Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment. This is in contrast with many approaches that address subproblems without addressing how they might fit into a larger picture. For example, we have mentioned that much of machine learning research is concerned with supervised learning without explicitly specifying how such an ability would finally be useful. Other researchers have developed theories of planning with general goals, but without considering planning's role in real-time decision-making, or the question of where the predictive models necessary for planning would come from. Although these approaches have yielded many useful results, their focus on
isolated subproblems is a significant limitation.

I have an idea of supervised learning (SL), but what exactly does the author mean by planning? And how is the RL approach different from planning and SL?
(Illustration with an example would be nice).
","['reinforcement-learning', 'comparison', 'terminology', 'supervised-learning', 'planning']","The concept of ""planning"" is not just related to RL. In general (as the name suggests), planning consists in creating a ""plan"" which you will use to reach a ""goal"". The goal depends on the context or problem. For example, in robotics, you can use a ""planning algorithm"" (e.g. Dijkstra's algorithm) in order to find the path between two points on a map (given e.g. the map as a graph).In RL, planning usually refers to the use of a model of the environment in order to find a policy that hopefully will help the agent to behave optimally (that is, obtain the highest amount of return or ""future cumulative discounted reward""). In RL, the problem (or environment) is usually represented as a Markov Decision Process (MDP). The ""model"" of the environment (or MDP) refers to the transition probability distribution (and reward function) associated with the MDP. If the transition model (and reward function) is known, you can use an algorithm that exploits it to (directly or indirectly) find a policy. This is the usual meaning of planning in RL. A common planning algorithm in RL is e.g. value iteration (which is a dynamic programming algorithm).Other researchers have developed theories of planning with general goals, but without considering planning's role in real-time decision- making, or the question of where the predictive models necessary for planning would come from.Planing is often performed ""offline"", that is, you ""plan"" before executing. While you're executing the ""plan"", you often do not change it. However, often this is not desirable, given that you might need to change the plan because the environment might also have changed. Furthermore, the authors also point out that planning algorithms often have a few limitations: in the case of RL, a ""model"" of the environment is required to plan.For example, we have mentioned that much of machine learning research is concerned with supervised learning without explicitly specifying how such an ability would finally be useful.I think the authors simply want to say that supervised learning is usually used to solve specific problems. The solutions to supervised problems often are not directly applicable to other problems, so this makes them limited.Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment.In RL, there is the explicit notion of a ""goal"": there is an agent that interacts with an environment in order to achieve its goal. The goal is often to maximize the ""return"" (or ""future cumulative discounted reward"", or, simply, the reward in the long run).How is RL different from planning and supervised learning?RL and planning (in RL) are quite related. In RL, the problem is similar to the one in planning (in RL). However, in RL, the transition model and reward function of the MDP (which represents the environment) are usually unknown. Therefore, the only way of finding or estimating an optimal policy that will allow the agent to (near-optimally) behave in this environment is to interact with the environment and gather some info regarding its ""dynamics"".RL and supervised learning (SL) are quite different. In SL, there isn't usually the explicit concept of ""agent"" or ""environment"" (and their interaction), even though it might be possible to describe supervised learning in that way (see this question). In supervised learning, during the training or learning phase, a set of inputs and the associated expected outputs is often provided. Then the ""objective"" is to find a map between inputs and outputs, which generalizes to inputs (and corresponding outputs) that have not been observed during the learning phase. In RL, there isn't such a set of inputs and associated expected outputs. In RL, there is just a scalar signal emitted by the environment, at each time step, which roughly indicates how well the agent is currently performing. However, the goal of the agent is not just to obtain rewards, but to behave optimally (in the long run).In short, in RL, there is the explicit notion of agent, environment and goal, and the reward is the only signal which tells the agent how well it is performing, but the reward does not tell the agent which actions it should take at each time step. In supervised learning, the objective is to find a function that maps inputs to the corresponding outputs, and this function is learned by providing explicit examples of such mappings during the training phase.There are some RL algorithms (like the temporal-difference ones), which could roughly be thought of as self-supervised learning algorithms, where the agent learns from itself (or from the experience it has gained by interacting with the environment). However, even in these cases, the actions that the agent needs to take are not explicitly taught."
How does backpropagation with unbounded activation functions such as ReLU work?,"
I am in the process of writing my own basic machine learning library in Python as an exercise to gain a good conceptual understanding. I have successfully implemented backpropagation for activation functions such as $\tanh$ and the sigmoid function. However, these are normalised in their outputs. A function like ReLU is unbounded so its outputs can blow up really fast. In my understanding, a classification layer, usually using the SoftMax function, is added at the end to squash the outputs between 0 and 1. 
How does backpropagation work with this? Do I just treat the SoftMax function as another activation function and compute its gradient? If so, what is that gradient and how would I implement it? If not, how does the training process work? If possible, a pseudocode answer is preferred.
","['neural-networks', 'backpropagation', 'relu']","Backprop through ReLU is easier than backprop through sigmoid activations. For positive activations, you just pass through the input gradients as they were. For negative activations you just set the gradients to 0. Regarding softmax, the easiest approach is to consider it a part of the negative log-likelihood loss. In other words, I am suggesting to directly derive gradients of that loss with respect to the softmax input. The result is very elegant and extremely easy to implement. Try to derive that yourself!"
Using AI to guess a mathematical pattern of certain polynomials in four variables: practical challenge,"
I'd like to use machine learning to guess a mathematical pattern: the input are certain polynomials in four variables $q_1,q_2,q_3,q_4$, the output can be zero or one.
Allowed polynomials are such that (i) all their non-zero coefficients are equal to one, (ii) they do not contain monomials of the form $q_1^j$ for $j \geq 0$, and (iii) if an allowed polynomial contains a monomial $m=q_1^a q_2^b q_3^c q_4^d$ for some non-negative integers $a,b,c,d$, then it also contains $m'=q_1^{a-1} q_2^b q_3^c q_4^d$, provided this does not violate (ii) and $a \geq 1$; similarly for $b \to b-1$, $c \to c-1$, and $d \to d-1$.
Here's an example batch, given by pairs {input, output}: $\{q_2,1\},\{q_3,1\},\{q_4,1\}$
Here's a second batch: $\{q_2+q_1 q_2,0\},\{q_2+q_2^2,0\},\{q_2+q_3,1\},\{q_3+q_1 q_3,0\},\{q_3+q_3^2,0\},\{q_2+q_4,1\},\{q_3+q_4,1\},\{q_4+q_1 q_4,0\},\{q_4+q_4^2,1\}$
I can construct larger and larger batches using Mathematica, and I'd like to know how to practically go from here, to instructing an AI to guess a simple function of $q$'s that reproduces the behavior, namely that can guess the correct output for previously unknown admissible polynomials.
What are the typical batch size and computational power required for such a program to succeed?
My idea is to use a function $\phi$ from the space of allowed polynomials $\mathcal P$ to the set $\mathbb Z_2=\{0,1\}$, of the form $\phi:\mathcal P \to \mathbb Z_2$, $p=\sum_{i \in I} m_i \mapsto \phi(p):= \sum_{i \in I' \subset I} m_i|_1 \mod 2$, where $m_i|_1$ means the $i$-th monomial inside $p$ evaluated at $q_1=q_2=q_3=q_4=1$, and come up with the form of $I'$ as function of $I$.
Notice there's no linear structure on $\mathcal P$.
Remark: of course instead of polynomials one could use punctured solid partitions.
","['neural-networks', 'machine-learning', 'deep-learning', 'python', 'implementation']",
"How is it that AI can become biased, and what are the proposals to mitigate this?","
This is not meant to be negative or a joke but rather looking for a productive solution on AI development, engineering and its impact on human life:
Lately with my Google searches, the AI model keeps auto filling the ending of my searches with:
“...in Vietnamese”
And 
“...in a Vietnamese home”
The issue is I have never searched for that but because of my last name the model is creating this context. 
The other issue is that I’m a halfy and my dad is actually third generation, I grew up mainstream American and don’t even speak Vietnamese. I’m not even sure what a Vietnamese home means. 
My buddy in a similar situation of South Asian and noticed the same exact thing more so with YouTube recommended videos. 
We already have enough issues in the US with racism, projections of who others expect us to be based on any number of things, stereotyping and putting people in boxes to limit them - I truly believe AI is adding to the problem, not helping. 
How can we fix this. Moreover, how can we use AI to bring out peoples true self, talents and empower and free them them to create their life how they like ?
There is huge potential here to harness AI in ways that can bring us more freedom, joy and beauty so people can be the whole of themselves and with who they really are. Then meet peoples needs, wishes, dreams and hope. Given them shoulders to stand on to create their reality, not live someone else's projection of themselves.
","['philosophy', 'social', 'ethics', 'human-like', 'algorithmic-bias']","Lately with my Google searches, the AI model keeps auto filling the ending of my searches with:“...in Vietnamese”I can see how this would be annoying.I don't think Google's auto-complete algorithm and training data is publicly available. Also it changes frequently as they work to improve the service. As such, it is hard to tell what exactly is leading it to come up with this less-than-useful suggestion.Your suspicion that it has something to do with Google's service detecting your heritage seems plausible.The whole thing is based around statistical inference. At no point does any machine ""know"" what Vietnamese - or in fact any of the words in your query  - actually means. This is a weakness of pretty much all core NLP work in AI, and is called the grounding problem. It is why, for instance, that samples of computer generated text produce such surreal and comic material. The rules of grammar are followed, but semantics and longer term coherence are a mess.Commercial chatbot systems work around this with a lot of bespoke coding around some subject area, such as booking tickets, shopping etc. These smaller domains are possible for human developers to ""police"", connecting them back to reality, and avoiding the open-ended nature of the whole of human language. Search engine text autocomplete however, cannot realistically use this approach.Your best bets are probably:Wait it out. The service will improve. Whatever language use statistics are at work here are likely change over time. Your own normal use of the system without using the suggestions will be part of that data stream of corrections.Send a complaint to Google. Someone, somewhere in Google will care about these results, and view them as errors to be fixed.Neither of these approaches guarantee results in any time frame sadly.We already have enough issues in the US with racism, projections of who others expect us to be based on any number of things, stereotyping and putting people in boxes to limit them - I truly believe AI is adding to the problem, not helping.You are not alone in having these worries. The statistics-driven nature of machine learning algorithms and use of ""big data"" to train them means that machines are exposing bias and prejudice that are long buried in our language. These biases are picked up by machinery then used by companies that don't necessarily want to reflect those attitudes.A similar example occurs in natural language processing models with word embeddings. A very interesting feature of LSTM neural networks that learn statistical language models is that you can look at word embeddings, mathematical representations of words, and do ""word math"":$$W(king) - W(man) + W(woman) \approx W(queen)$$$$W(he) - W(male) + W(female) \approx W(she)$$This is very cool, and implies that the learned embeddings really are capturing semantics up to some depth. However, the same model can produce results like this:$$W(doctor) - W(male) + W(female) \approx W(nurse)$$This doesn't reflect modern sensibilities of gender equality. There is obviously a deep set reason for this, as it has appeared from non-prejudiced statistical analysis of billions of words of text from all sorts of sources. Regardless of this though, engineers responsible for these systems would prefer that their models did not have these flaws.How can we fix this. Moreover, how can we use AI to bring out peoples true self, talents and empower and free them them to create their life how they like ?Primarily by recognising that statistical ML and AI doesn't inherently have prejudice or any agenda at all. It is reflecting back ugliness already in the world. The root problem is to fix people (beyond scope of this answer, if I had solid ideas about this I would not be working in software engineering, but in something more people-focussed).However, we can remove some of the unwanted bias from AI systems. Broadly the steps toward this go:Recognise that a particular AI system has captured and is using unwanted gender, racial, religious etc bias.Reach a consensus about how an unbiased model should behave. It must still be useful for purpose.Add the desired model behaviour into the training and assessment routines of the AI.For instance in your case, there are possibly some users of Google's system who would prefer to read articles in Vietnamese, or have English translated into Vietnamese, and are finding it awkward that the default assumption is that everything should be presented in English. These users don't necessarily need to use the search text for this, but presumably are for some reason. A reasonable approach is to figure out how their needs could be met without spamming ""in Vietnamese"" on the end of every autocomplete suggestion, and perhaps in general move suggestions to localise searches by cultural differences out of autocomplete into a different part of the system.For the case of gender bias in NLP systems, Andrew Ng's Coursera course on RNNs shows how this can be achieved using the embeddings themselves. Essentially it can be done by identifying a bias direction from a set of words (e.g. ""he/she"", ""male/female""), and removing deviations in that direction for most other words, preserving it only for words where it is inherently OK to reflect the differences (such as ""king"" and ""queen"" for gender bias).Each case of unwanted bias though needs to be discovered by people and oversight of this as a political and social issue, not primarily a technical one."
Is the optimal policy always stochastic if the environment is also stochastic?,"
Is the optimal policy always stochastic (that is, a map from states to a probability distribution over actions) if the environment is also stochastic? 
Intuitively, if the environment is deterministic (that is, if the agent is in a state $s$ and takes action $a$, then the next state $s'$ is always the same, no matter which time step), then the optimal policy should also be deterministic (that is, it should be a map from states to actions, and not to a probability distribution over actions).
","['reinforcement-learning', 'stochastic-policy', 'deterministic-policy', 'policies', 'environment']",
"What is the difference between an episode, a trajectory and a rollout?","
I often see the terms episode, trajectory, and rollout to refer to basically the same thing, a list of (state, action, rewards). Are there any concrete differences between the terms or can they be used interchangeably?
In the following paragraphs, I'll summarize my current slightly vague understanding of the terms. Please point any inaccuracy or missing details in my definitions.
I think episode has a more specific definition in that it begins with an initial state and finishes with a terminal state, where the definition of whether or not a state is initial or terminal is given by the definition of the MDP. Also, I understand an episode as a sequence of $(s, a, r)$ sampled by interacting with the environment following a particular policy, so it should have a non-zero probability of occurring in the exact same order.
With trajectory, the meaning is not as clear to me, but I believe a trajectory could represent only part of an episode and maybe the tuples could also be in an arbitrary order; even if getting such sequence by interacting with the environment has zero probability, it'd be ok, because we could say that such trajectory has zero probability of occurring.
I think rollout is somewhere in between since I commonly see it used to refer to a sampled sequence of $(s, a, r)$ from interacting with the environment under a given policy, but it might be only a segment of the episode, or even a segment of a continuing task, where it doesn't even make sense to talk about episodes.
","['reinforcement-learning', 'terminology', 'comparison']",
Why is the state value function sufficient to determine the policy if a model is available?,"
In section ""5.2 Monte Carlo Estimation of Action Values"" of the second edition of the reinforcement learning book by Sutton and Barto, this is stated:

If a model is not available, then it is particularly useful to estimate action values (the values of state– action pairs) rather than state values. With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP.

However, I don't see how this is true in practice. I can see how it'd work trivially for discrete state and action spaces with deterministic environment dynamics, because we could compute $\pi(s) = \underset{a}{\text{argmax}}\ V(\text{step}(s, a))$ by just looking at all possible actions and choosing the best one. As soon as I think about continuous state and action spaces with stochastic environment dynamics, computing the $\text{argmax}$ seems to be become very complicated and impractical. For the particular case of continuous states and discrete actions, I think estimating an action value might be more practical to do even if a forward model of the environment dynamics is available, because the $\text{argmax}$ becomes easier (I'm especially thinking of the approach taken in deep Q learning). 
Am I correct in thinking this way or is it true that if a model is available it's not useful to estimate action values if state values are already available?
","['reinforcement-learning', 'value-functions']",
how to normalize the state space for articulated robot environments?,"
What is the common representation used for the state in articulated robot environments? My first guess is that it's a set of the angles of every joint. Is that correct? My question is motivated by the fact that one common trick that helps training neural nets in general is to normalize the inputs, like setting mean = 0 and std dev = 1, or scaling all the input values to $[0, 1]$, which could be easily done in this case too if all the inputs are angles in $[0, 2 \pi]$. But, what about distances? Is it common to, for example, use as input some distance of the agent to the ground, or a distance to some target position? In that case, the scale of the distances can be arbitrary and vary a lot. What are some common ways to deal with that?
","['reinforcement-learning', 'robotics']","This paper might provide some answers https://arxiv.org/pdf/1810.05762.pdfFor the observations / states they used not only angles, but also velocities, heights and positions (Table 2).In 4.2 Learning algorithm you can see that they mention this, which is related to your question about normalization:Additionally, for stability we whiten the current observations by
  maintaining online statistics of mean and standard deviation from the
  history of past observations."
"How do temporal-difference and Monte Carlo methods work, if they do not have access to model?","
In value iteration, we have a model of the environment's dynamics, i.e $p(s', r \mid s, a)$, which we use to update an estimate of the value function.
In the case of temporal-difference and Monte Carlo methods, we do not use $p(s', r \mid s, a)$, but then how do these methods work?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'model-based-methods', 'model-free-methods']",
"How to make episode ending ""good"" in reinforcement learning?","
TL;DR: read the bold. The rest are details
I am trying to implement Reinforcement Learning:An Introduction, section 13.5 myself:

on OpenAi's cartpole
The algorithm seems to be learning something useful (and not random), as shown in these graphs (different zoom on the same run):



Which show the reward per episode (y axis is the ""time alive"", x axis is episode number).
However, as can be seen, 

The learning does not seem to stabilize.
It looks like every time the reward maxes out (200), it immediately drops.


My relevant code for reference (inspired by pytorch's actor critic)
note: in this question, xp_batch is ONLY THE VERY LAST (s, a, r, s'), meaning experience replay is not in use in this code!
The actor and critic are both distinct neural networks which 
def learn(self, xp_batch):#in this question, xp_batch is ONLY THE VERY LAST (s, a, r, s')
    for s_t, a_t, r_t, s_t1 in xp_batch:
        expected_reward_from_t = self.critic_nn(s_t)
        probs_t = self.actor_nn(s_t)
        expected_reward_from_t1 = torch.tensor([[0]], dtype=torch.float)
        if s_t1 is not None:  # s_t is not a terminal state, s_t1 exists.
            expected_reward_from_t1 = self.critic_nn(s_t1)

        m = Categorical(probs_t)
        log_prob_t = m.log_prob(a_t)

        delta = r_t + self.args.gamma * expected_reward_from_t1 - expected_reward_from_t

        loss_critic = delta * expected_reward_from_t
        self.critic_optimizer.zero_grad()
        loss_critic.backward(retain_graph=True)
        self.critic_optimizer.step()

        delta.detach()
        loss_actor = delta * log_prob_t
        self.actor_optimizer.zero_grad()
        loss_actor.backward()
        self.actor_optimizer.step()

def select_action(self, state):
    probs = self.actor_nn(state)
    m = Categorical(probs)
    action  = m.sample()
    return action


My questions are:

Am I doing something wrong, or is this to be expected?
I know this can be improved with eligibility traces/experience replay+off policy learning. Before making those upgrades, I want to make sure the current results make sense.

","['machine-learning', 'reinforcement-learning', 'pytorch']",
"Apart from the state and state-action value functions, what are other examples of value functions used in RL?","
In reinforcement learning, we often define two functions, the state-value function
$$V^\pi(s) = \mathbb{E}_{\pi} \left[\sum_{k=0}^{\infty} 
\gamma^{k}R_{t+k+1} \Bigg| S_t=s \right]$$
and the state-action-value function
$$Q^\pi(s,a) = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}\Bigg|S_t=s, A_t=a \right]$$
where $\mathbb{E}_{\pi}$ means that these functions are defined as the expectation with respect to a fixed policy $\pi$ of what is often called the return, $\sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}$, where $\gamma$ is a discount factor and $R_{t+k+1}$ is the reward received from the environment (while the agent interacts with it) from time $t$ onwards. 
So, both the $V$ and $Q$ functions are defined as expectations of the return (or the cumulative future discounted reward), but these expectations have different ""conditions"" (or are conditioned on different variables). The $V$ function is the expectation (with respect to a fixed policy $\pi$) of the return given that the current state (the state at time $t$) is $s$. The $Q$ function is the expectation (with respect to a fixed policy $\pi$) of the return conditioned on the fact that the current state the agent is in is $s$ and the action the agent takes at $s$ is $a$.
Furthermore, the Bellman optimality equation for $V^*$ (the optimal value function) can be expressed as the Bellman optimality equation for $Q^{\pi^*}$ (the optimal state-action value function associated with the optimal policy $\pi^*$) as follows
$$
V^*(s) = \max_{a \in \mathcal{A}(s)} Q^{\pi^*}(s, a)
$$
This is actually shown (or proved) at page 76 of the book ""Reinforcement Learning: An Introduction"" (1st edition) by Andrew Barto and Richard S. Sutton.
Are there any other functions, apart from the $V$ and $Q$ functions defined above, in the RL context? If so, how are they related?  
For example, I've heard of the ""advantage"" or ""continuation"" functions. How are these functions related to the $V$ and $Q$ functions? When should one be used as opposed to the other? Note that I'm not just asking about the ""advantage"" or ""continuation"" functions, but, if possible, any existing function that is used in RL that is similar (in purpose) to these mentioned functions, and how they are related to each other. 
","['reinforcement-learning', 'comparison', 'reference-request', 'value-functions', 'bellman-equations']","Advantage function: $A(s,a) = Q(s,a) - V(s)$More interesting is the General Value Function (GVF), the expected sum of the (discounted) future values of some arbitrary signal, not necessarily reward. It is therefore a generalization of value function $V(s)$. The GVF is defined on page 459 of the 2nd edition of Sutton and Barto's RL book as
$$v_{\pi,\gamma,C}(s) =\mathbb{E}\left[\left.\sum_{k=t}^\infty\left(\prod_{i=t+1}^k \gamma(S_i)\right)C_{k+1}\right\rvert S_t=s, A_{t:\infty}\sim\pi\right]$$
where $C_t \in \mathbb{R}$ is the signal being summed over time.$\gamma(S_t)$ is a function $\gamma: \cal{S}\to[0,1]$ allowing the discount rate to depend upon the state. Sutton and Barto call it the termination function. Some call it the continuation function.Also of note are the differential value functions. These are used in the continuing, undiscounted setting. Because there is no discounting, the expected sum of future rewards is unbounded. Instead, we optimize the expected differential reward $R_{t+1}-r(\pi)$, where $r(\pi)$ is the average reward under policy $\pi$.$$v_{\pi,\,diff}(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)\left[r-r(\pi)+ v_{\pi,\,diff}(s')\right]$$
$$v_{*,\,diff}(s) = \max_a \sum_{s',r} p(s',r|s,a)\left[r-\max_\pi r(\pi)+ v_{*,\,diff}(s')\right]$$The differential value functions assume that a single fixed value of $r(\pi)$ exists. That is, they assume the MDP is ""ergodic."" See section 10.3 of Sutton and Barto for details."
Parameters to calculate affluence in localities of Metro city,"
I have to calculate the affluence in localities of Metro city. To calculate affluence, I am considering a parameter per capita income. 
Where I can get a dataset of it? What are other parameters I should consider for the problem? 
Any guidance will be fruitful for me.
","['datasets', 'data-science']","Affluence could encompass several parameters:
Income;
Wealth (property ownership);
Life expectancy;
Access to services such as education and health;
Access to clean natural resources;
Low levels of criminality.Property prices in each locality might be easy to obtain from real estate agent sources
Ratings for schools or medical facilities in each area might be publishedGenerally, where public statistics are collected on a locality, they will be related in one way or another to affluence. A useful strategy might be to collect as many of these diverse data sets as possible, and to learn a composite affluence score from the data. It is very likely that all of these parameters will be correlated to a greater or lesser degree, and so you could accurately learn about affluence from a small number of these parameters."
Feature visualization on neural networks which are not for classification,"
Feature visualization allows to better understand neural networks by generating images that maximize the activation of a specific neuron, and therefore understand what are the abstract features that produce a high activation.
The examples that I saw so far are related to classification tasks. So my question is: can these concepts be applied to other convolutional neural network tasks, like semantic segmentation or image embedding (triplet loss)? What can I expect if I apply visualization algorithms to these networks?
","['neural-networks', 'convolutional-neural-networks', 'optimization']",
What is the most biologically plausible representation for the actor and critic?,"
Which representation is most biologically plausible for actor nodes? For example, actions represented across several output nodes which may be either

mutually exclusive with each other (e.g., go north, go south, etc),
achieved by winner-takes-all. 
NOT mutually exclusive with each other (e.g. left leg forward, right leg forward); these actions may occur concurrently. To go north, the correct combination of nodes must be active.

Similarly which representation is most plausible for critic output nodes?

A single output node that outputs a real number representing the
reward. 
A set of output nodes each representing a separate value, achieved by winner-takes-all.

Or do other representations better align with real brains ?
","['neural-networks', 'reinforcement-learning', 'actor-critic-methods']",
Where are reinforcement algorithms used in financial services?,"
One of the most common misconceptions about reinforcement learning (RL) applications is that, once you deploy them, they continue to learn. And, usually, I'm left having to explain this. As part of my explanations, I like to show where it is being used and where not.
I've done a little bit of research on the topic, but the descriptions seem fairly academic, and I'm left with the opinion that reinforcement learning is not really suitable for financial services in regulated markets.
Am I wrong? If so, I would like to know where RL is being used? Also, in those cases, are these RL algorithms adapting to new data over time? How do you ensure they are not picking up on data points or otherwise making decisions that are considered to be unacceptable?
","['reinforcement-learning', 'applications', 'ai-safety']",
Are A2C or A3C suitable for episodic tasks where the reward is delivered only at the end of the episode?,"
My understanding of the main idea behind A2C / A3C is that we run small segments of an episode to estimate the return using a trainable value function to compensate for the unseen final steps of the episode.
While I can see how this could work in continuing tasks with relatively dense rewards, where you can still get some useful immediate rewards from a small experience segment, does this approach work for episodic tasks where the reward is only delivered at the end? For example, in a game where you only know if you win or lose at the end of the game, does it still make sense to use the A2C / A3C approach?
It's not clear to me how the algorithm could get any useful signal to learn anything if almost every experience segment has zero reward, except for the last one. This would not be a problem in a pure MC approach for example, except for the fact that we might need a lot of samples. However, it's not clear to me that arbitrarily truncating episode segments like in A2C / A3C is a good idea in this case.
","['reinforcement-learning', 'actor-critic-methods']","My understanding of the main idea behind A2C / A3C is that we run small segments of an episode to estimate the return using a trainable value function to compensate for the unseen final steps of the episode.This seems fairly accurate. The important thing to note is that the trainable value function is trained to predict values (specifically, advantage values of state-action pairs in the case of A2C / A3C, where the first A stands for ""advantage""). These value estimates can intuitively be understood as estimates of long-term (discounted) rewards, they're not just short-term rewards.So yes, initially when the agent only observes a reward at the end of a long trajectory, only state-action pairs close to the end will receive credit for that reward. For example, when using $n$-step returns, approximately only the last $n$ state-action pairs receive credit. However, in the next episode, that longer-term reward will already become ""visible"" in the form of an advantage value prediction when you're still $n$ steps away from the end, and then that update can again get propagated back $n$ steps further into the history of state-action pairs.My explanation above is very informal... there are all kinds of nuances that I skipped over. Use of function approximation is likely to speed up the propagation of reward observations through the space of state-action pairs even more, and of course in reality things won't be as ""clean"" as getting the propagation to get $n$ steps further in the next episode in comparison to the previous episode, since selected actions and random state transitions can be different... but hopefully it gets the idea across."
How do I write a genetic algorithm to solve the knapsack problem?,"
I am trying to write a genetic algorithm that generates 100 items, assigning random weights and utilities to them. And then try to pick items how out these 100 items while maximising the utility and not picking items over 500ks. The program should return an array of boolean values, where true represents items to be picked and false represents item not to be picked. 
Can someone help with this or point me to a link of something that has been written like this before?
","['genetic-algorithms', 'java']",
"Logic questions: reasoning pattern, Infer literals, unit resolution, and-elimination etc","

Show which literals can be inferred from the following knowledge bases, using both reasoning patterns and truth tables. Show all steps in your reasoning and explain your answers.
1) P & Q
  2) Q →R v S
  3) P → ~R 

This is from my reasoning pattern tutorial, my text book shows similar question except that there is a single literal with workings, so I can somehow read through but I'm not familiar with some terms. I don't understand how I can infer all literals with the above information. I also don't fully understand what is and-elimination, modus ponens and unit resolution. 
Is there anyone who is kind enough to use the above question as an example so that I can have a clearer picture?
",['logic'],
How do self-driving cars construct paths?,"
I wonder how self-driving cars determine the path to follow. Yes, there's GPS, but GPS can have hiccups and a precision larger than expected. Suppose the car is supposed to turn right at an intersection on the inner lane, how is the exact path determined? How does it determine the trajectory of the inner lane?
","['autonomous-vehicles', 'path-planning', 'path-finding']","As you say, GPS is not precise enough for the purpose (until recently it was only accurate within 5m or so, since 2018 there are receivers that have an accuracy of about 30cm). Instead, autonomous vehicles have a multitude of sensors, mostly cameras and radar, which record the surrounding area and monitor the road ahead. Due to them being flat, mostly one colour, and often with lines or other markers on them, roads are usually fairly easy to spot, which is why most success has been made driving on roads as opposed to off-road. Once you know exactly where you are and where you want to go, computing the correct trajectory is then just a matter of maths and physics.For an academic paper on the subject of trajectory planning see Local Trajectory Planning and Tracking of Autonomous Vehicles, Using Clothoid Tentacles Method.It quickly becomes more complex when other road users and obstacles are taken into account; here machine learning is used to identify stationary and movable objects at high speed from the sensor input. Reacting to the input is a further problem, and one reason why there aren't any self-driving cars on the roads today.This is all on driving automation level 2 and above; on the lower levels things are somewhat easier. For example, the latest model Nissan LEAF has an automatic parking mode, where the car self-steers, guided by camera images and sonar, but still requires the driver to indicate the final position of the vehicle. Apart from that, it is fully automatic."
Calculating gradient for log policy when variance is not constant,"
I've noticed that when modelling a continuous action space, the default thing to do is to estimate a mean and a variance where each is parameterized by a neural network or some other model. 
I also often see that it is one network $\theta$ models both. The REINFORCE objective can be written as 
$$\nabla \mathcal{J}(\theta) = \mathbb{E}_{\pi} [\nabla_\theta \log \pi(a_t|s_t) * R_t] $$
For discrete action space this makes sense since the output of the network is determined by a softmax. However, if we explicitly model the output of the network as a Gaussian, then the gradient of the log likelihood is of a different form,
$$\pi_\theta(a_t|s_t) = Normal(\mu_\theta(s_t), \Sigma_\theta(s_t))$$
and the log is:
$$\log \pi_\theta(a_t | s_t) =  -\frac{1}{2} (a_t-\mu_\theta)^\top \Sigma^{-1}_\theta(a_t-\mu_\theta) + \log 2 \pi \det({\Sigma_\theta})$$
In the slides provided here (slide 18):
http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf
IF the variance is held constant, then we can solve this analytically:
$$\nabla_\theta \log \pi_\theta(a_t|s_t) = (a_t - \mu_\theta) \Sigma^{-1} \phi(s)$$
But, are things always modelled assuming a constant variance? If it's not constant then we have to account for the inverse of the covariance matrix 
as well as the determinant? 
I've taken a look at code online and from what I've seen, most of them assume the variance is constant. 

@NielSlater
Using the reparameterization trick we would use a normal distribution with fixed parameters 0 and 1. 
$$ a_t \sim \mu_\theta(s_t) + \Sigma_\theta(s_t) * Normal(0, 1) $$ 
Which is the same as if we had actually sampled directly from a distribution, $ \pi_\theta(a_t | s_t) = Normal(\mu_\theta(s_t), \Sigma_\theta(s_t))$ and let's us calculate the corresponding $\log \pi_\theta(a_t|s_t)$ and $\nabla_\theta \log \pi_\theta(a_t | s_t)$ without having to differentiate through the actual density.
","['reinforcement-learning', 'policy-gradients']",
Is it a great misconception that the softmax is an activation function?,"
An activation function is a function from $R \rightarrow R$. It takes as input the inner products of weights and activations in the previous layer. It outputs the activation. 
A softmax however, is a function that takes input from $R^p$, where $p$ is the number of possible outcomes that need to be classified. Therefore, strictly speaking, it cannot be an activation function.
Yet everywhere on the net it says the softmax is an activation function. Am I wrong or are they?
","['neural-networks', 'artificial-neuron', 'activation-functions']",
What is the gradient of the objective function in the Soft Actor-Critic paper?,"
In the paper Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, they define the loss function for the policy network as
$$
J_\pi(\phi)=\mathbb E_{s_t\sim \mathcal D}\left[D_{KL}\left(\pi_\phi(\cdot|s_t)\Big\Vert {\exp(Q_\theta(s_t,\cdot)\over Z_\theta(s_t)}\right)\right]
$$
Applying the reparameterization trick, let $a_t=f_\phi(\epsilon_t;s_t)$, then the objective could be rewritten as
$$
J_\pi(\phi)=\mathbb E_{s_t\sim \mathcal D, \epsilon \sim\mathcal N}[\log \pi_\phi(f_\phi(\epsilon_;s_t)|s_t)-Q_\theta(s_t,f_\phi(\epsilon_t;s_t))]
$$
They compute the gradient of the above objective as follows
$$
\nabla_\phi J_\pi(\phi)=\nabla_\phi\log\pi_\phi(a_t|s_t)+(\nabla_{a_t}\log\pi_\phi(a_t|s_t)-\nabla_{a_t}Q(s_t,a_t))\nabla_\phi f_\phi(\epsilon_t;s_t)
$$
The thing confuses me is the first term in the gradient, where does it come from? To my best knowledge, the second large term is already the gradient we need, why do they add the first term?
","['reinforcement-learning', 'papers', 'gradient-descent', 'actor-critic-methods', 'soft-actor-critic']","I'll give it a go here and try to answer your question, I'm not sure if this is entirely correct, so if someone thinks that it isn't please correct me.
I'll disregard expectation here to make things simpler. First, note that policy $\pi$ depends on parameter vector $\phi$ and function $f_\phi(\epsilon_t;s_t)$, and value function $Q$ depends on parameter vector $\theta$ and same function $f_\phi(\epsilon_t;s_t)$. Also, one important thing that authors mention in the paper and you didn't mention is that this solution is approximate gradient not the true gradient.
Our goal is to calculate gradient of objective function $J_\pi$ with respect to $\phi$, so disregarding the expectation we have:  $\nabla_\phi J_\pi (\phi) = \nabla_\phi \log\pi(\phi,f_\phi (\epsilon_t;s_t)) - \nabla_\phi Q(s_t,\theta,f_\phi (\epsilon_t;s_t))$ Let's see the gradient of first term on right hand side. To get the full gradient we need to calculate derivative w.r.t to both variables, $\phi$ and $f_\phi (\epsilon_t;s_t)$, so we have:  $\nabla_\phi \log\pi(\phi,f_\phi (\epsilon_t;s_t)) = \frac {\partial \log\pi(\phi,f_\phi (\epsilon_t;s_t))}{\partial \phi} + \frac{\partial \log\pi(\phi,f_\phi (\epsilon_t;s_t))}{\partial f_\phi(\epsilon_t;s_t)} \frac{\partial f_\phi(\epsilon_t;s_t)}{\partial \phi}$This is where approximation comes, they replace $f_\phi (\epsilon_t;s_t)$ with $a_t$ in some places and we have:  $\nabla_\phi \log\pi(\phi,f_\phi (\epsilon_t;s_t)) \approx \frac {\partial \log\pi(\phi,a_t)}{\partial \phi} + \frac{\partial \log\pi(\phi,a_t)}{\partial a_t} \frac{\partial f_\phi(\epsilon_t;s_t)}{\partial \phi}$
$\nabla_\phi \log\pi(\phi,f_\phi (\epsilon_t;s_t)) \approx \nabla_\phi \log\pi(\phi,a_t) + \nabla_{a_t} \log\pi(\phi,a_t) \nabla_\phi f_\phi (\epsilon_t;s_t)$ For the second term in first expression on right hand side we have:  $\nabla_\phi Q(s_t,\theta,f_\phi (\epsilon_t;s_t)) = \frac {\partial Q(s_t,\theta,f_\phi (\epsilon_t;s_t))}{\partial \phi} + \frac{\partial Q(s_t,\theta,f_\phi (\epsilon_t;s_t))}{\partial f_\phi(\epsilon_t;s_t)} \frac{\partial f_\phi(\epsilon_t;s_t)}{\partial \phi}$
$\nabla_\phi Q(s_t,\theta,f_\phi (\epsilon_t;s_t)) \approx \frac {\partial Q(s_t,\theta,a_t)}{\partial \phi} + \frac{\partial Q(s_t,\theta,a_t)}{\partial a_t} \frac{\partial f_\phi(\epsilon_t;s_t)}{\partial \phi}$ Fist term on right hand side is 0 because $Q$ does not depend on $\phi$ so we have:  $\nabla_\phi Q(s_t,\theta,f_\phi (\epsilon_t;s_t)) \approx \nabla_{a_t}Q(s_t, \theta,a_t)\nabla_\phi f_\phi(\epsilon_t;s_t)$ Now you add up things and you get the final result."
Does fp32 & fp64 performance of GPU affect deep learning model training? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I am purchasing Titan RTX GPU. Everything seems fine with that except float32 & float64 performance which seems lower vis-a-vis some of its counter parts. I wanted to understand if single precision and double precision performance of GPU affect deep learning training or efficiency ? We work mostly with images, however not limited to that.
","['deep-learning', 'hardware', 'hardware-evaluation', 'gpu']","First off I would like to post this comprehensive blog which makes comparison between all kinds of NVIDIA GPU's.The most popular deep learning library TensorFlow by default uses 32 bit floating point precision. The choice is made as it helps in 2 causes:64 bit is only marginally better than 32 bit as very small gradient values will also be propagated to the very earlier layers. But the trade-off for the gain in performance vs (the time for calculations + memory requirements + time for running through so many epochs so that those small gradients actually do something) is not worth it. There are state of art CNN architectures, which insert gradients midpoint and has very good performance. So overall 32 bit performance is the one which should really matter for deep learning, unless you are doing a very very high precision job (which still would hardly matter as small differences due to 64 bit representation is literally erased by any kind of softmax or sigmoid). So 64 bit might increase your accuracy classification by $<< 1 {\%}$ and will only become significant over very large datasets.As far as raw specs go the TITAN RTX in comparison to 2080Ti, TITAN will perform better than 2080Ti in fp64 (as its memory is double than 2080Ti and has higher clock speeds, BW, etc) but a more practical approach would be to use 2 2080Ti's coupled together, giving a much better performance for price.Side Note: Good GPU's require good CPU's. It is difficult to tell whether a given CPU will bottleneck a GPU as it entirely depends how the training is being performed (whether data is fully loaded in GPU then training occurs, or continuous feeding from CPU takes place.)
Here are a few links explaining the problem:CPU and GPU Bottleneck: A Detailed ExplanationA Full Hardware Guide to Deep Learning"
Should you reload the optimizer for transfer learning?,"
For example, you train on dataset 1 with an adaptive optimizer like Adam. Should you reload the learning schedule, etc., from the end of training on dataset 1 when attempting transfer to dataset 2? Why or why not?
","['neural-networks', 'deep-learning', 'transfer-learning']",
Is there a way to understand the type of a sentence?,"
I am a beginner, just started studying around NLP, specifically various language models. So far, my understanding is that -  the goal is to understand/produce natural language.
So far the methods I have studied speak about correlation of words, using correct combination to make a meaningful sentence. I also have the sense that the language modeling does not really care about the punctuation marks (or did I miss it?)
Thus I am curious is there a way they can classify sentence types such as Declarative, Imperative, Interrogative or Exclamatory?
","['natural-language-processing', 'classification', 'computational-linguistics', 'semantics']",
Are there profitable hedge funds using AI? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



Is there any research in this area?
","['machine-learning', 'research', 'algorithmic-trading']",
How to find AI specialists interested in additive Manufacturing [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 1 year ago.







                        Improve this question
                    



We recently founded a company in the area of additive manufacturing. Our development focuses on making the process easier and liberate time to the user, which I personally believe in as the ultimate promise of any computerised process, including AI, for the public. 
We have gone through the usual channels including regular job offers and personal contacts, visiting universities and meetups. However there appears to be a lack of participation of real specialists in most of those gatherings as the ones interested in learning about AI are doing just that, and don't show up somewhere where we can find them, although we believe to have a very interesting task at hand.
It appears to be remarkably hard to find specialists in the field, as AI specialists are either employed by huge multinational companies (instead of startups), have no interest in additive manufacturing (because it implies something else than only code) and the challenge of machines (making machines) or just do not pop up anywhere.
How would a specialist of AI look for someone else in the specific field of additive manufacturing/motion planning/creative strategy and its employment? (Or course in the case they do not already know someone.)
",['ai-field'],"You should probably compile a list of Universities/Instituitions that specialise in Additive Manufacturing, and target graduating students or researchers looking for a new challenge. This may prove more fruitful than targeting the normal channels. You may have to search worldwide, but of course this adds the issue of work visa etc.
There is no easy way to solve this....Yo can try posting on the Academia forum too."
"In online one step actor critic, why does the weights update become less significant as the episode progresses?","
The Reinforcement Learning Book by Richard Sutton et al, section 13.5 shows an online actor critic algorithm.

Why do the weights updates depend on the discount factor via $I$?
It seems that the more we get closer to the end of the episode, the less we value our newest experience $\delta$.
This seems odd to me. I thought discounting in the  recursive formula of $\delta$ itself is enough.
Why does the weights update become less significant as the episode progresses?
Note this is not eligibility traces, as those are discussed separately, later in the same episode.
","['machine-learning', 'reinforcement-learning', 'discount-factor', 'actor-critic-methods']","This ""decay"" of later values is a direct consequence of the episodic formula for the objective function for REINFORCE:$$J(\theta) = v_{\pi_\theta}(s_0)$$That is, the expected return from the first state of the episode. This is equation 13.4 in the book edition that you linked in the question.In other words, if there is any discounting, we care less about rewards seen later in the episode. We mainly care about how well the agent will do from its starting position.This is not true for all formulations of policy gradients. There are other, related, choices of objective function. We can formulate the objective function as caring about the returns from any distribution of states, but in order to define it well, we do need to describe the weighting/distribution somehow, it should be relevant to the problem, and we want to be able to get approximate samples of $\nabla J(\theta)$ for policy gradient to work. The algorithm you are asking about is specifically for improving policy for episodic problems. Note you can set $\gamma = 1$ for these problems, so the decay is not necessarily required.As an aside (because someone is bound to ask): Defining $J(\theta)$ with respect to all states equally weighted could lead to difficulties  e.g. the objective would take less account of a policy's ability to avoid undesirable states, and it would require a lot of samples from probably irrelevant states in order to estimate it. These difficulties would turn up as a hard  to calculate (or maybe impossible) expectation for $\nabla J(\theta)$"
"How to obtain a formula for loss, when given an iterative update rule in gradient descent?","
From the reinforcement learning book section 13.3:

Using pytorch, I need to calculate a loss, and then the gradient is calculated internally.
How to obtain the loss from equations which are stated in the form of an iterative update with respect to the gradient?
In this case:
$\theta \leftarrow \theta + \alpha\gamma^tG\nabla_{\theta}ln\pi(A_t|S_t,\theta)$
What would be the loss?
And in general, what would be the loss if the update rule were
$\theta \leftarrow \theta + \alpha C\nabla_{\theta}g(x|\theta)$
for some general (derivable) function $g$ parameterized by theta?
","['machine-learning', 'reinforcement-learning', 'gradient-descent', 'objective-functions']","You can find an implementation of the REINFORCE algorithm (as defined in your question) in PyTorch at the following URL: https://github.com/JamesChuanggg/pytorch-REINFORCE/. First of all, I would like to note that a policy can be represented or implemented as a neural network, where the input is the state (you are currently in) and the output is a ""probability distribution over the actions you can take from that state received as input"". In the Python module https://github.com/JamesChuanggg/pytorch-REINFORCE/blob/master/reinforce_discrete.py, the policy is defined as a neural network with 2 linear layers, where the first linear layer is followed by a ReLU activation function, whereas the second is followed by a soft-max. In that same Python module, the author also defines another class called REINFORCE, which creates a Policy object (in the __init__ method) and defines it as property of that class. The class REINFORCE also defines two methods select_action and update_parameters. These two methods are called from the main.py module, where the main loop of the REINFORCE algorithm is implemented. In that same main loop, the author declares lists entropies, log_probs and rewards. Note that these lists are re-initialized at ever episode. A ""log_prob"" and an ""entropy"" is returned from the select_action method, whereas a ""reward"" is returned from the environment after having executed one environment step. The environment is provided by the OpenAI's Gym library. The lists entropies, log_probs and rewards are then used to update the parameters, i.e. they are used by the method update_parameters defined in the class REINFORCE. Let's see better now what these methods, select_action and update_parameters, actually do. select_action first calls the forward method of the class Policy, which returns the output of the forward pass of the NN (i.e. the output of the soft-max layer), so it returns the probabilities of selecting each of the available actions (from the state given as input). It then selects the probability associated with the first action (I guess, it picks the probabilities associated with the action with the highest probabilities), denoted by prob (in the source code). Essentially, what I've described so far regarding this select_action method is the computation of $\pi(A_t \mid S_t, \theta)$ (as shown in the pseudocode of your question). Afterwards, in the same method select_action, the author also computes the log of that probability I've just mentioned above (i.e. the one associated with the  action with the highest probability, i.e. the log of prob), denoted by log_prob. In that same method, the entropy (as defined in this answer) is calculated. In reality, the author calculates the entropy using only one distribution (instead of two): more specifically, the entropy is calculated as follows entropy = -(probs*probs.log()).sum(). In fact, the entropy loss function usually requires the ground-truth labels (as explained in the answer I linked you to above), but, in this case, we do not have ground-truth labels (given that we are performing RL and not supervised learning). Nonetheless, I can't really tell you why the entropy is calculated like this, in this case. Finally, the method select_action then return action[0], log_prob, entropy.First of all, I would like to note that the method update_parameters is called only at the end of each episode (in the main.py module). In that same method, a variable called loss is first initialized to zero. In that method, we then iterate the list of rewards for the current episode. Inside that loop of the update_parameters method, the return, R is calculated. R is also multiplied by $\gamma$. On each time step, the loss is then calculated as followsThe loss is calculated by subtracting the previous loss with where log_probs are the log probabilities calculated in the select_action method. log_probs is the part $\log \pi(A_t \mid S_t, \theta)$ of the update rule of your pseudocode. log_probs are then multiplied by the return R. We then sum the result of this multiplication over all elements of the vector. We then subtract this just obtained result by the entropies multiplied by 0.0001. I can't really tell you why the author decided to implement the loss in this way. I would need to think about it a little more.The following article may also be useful: https://pytorch.org/docs/stable/distributions.html."
Can the rewards be stochastic when the transition model is deterministic?,"
Suppose we have a deterministic environment where knowing $s,a$ determines $s'$. Is it possible to get two different rewards $r\neq r'$ in some state $s_{\text{fixed}}$? Assume that $s_{\text{fixed}}$ is a fixed state I get to after taking the action $a$. Note that we can have situations where in multiple iterations we have: $$(s,a) \to (s_1, r_1) \\ (s,a) \to (s_{\text{fixed}}, r_1) \\ (s,a) \to (s_{\text{fixed}}, r_2) \\ (s,a) \to (s_3, r_3) \\ \vdots$$
My question is, would $r_1 =r_2$?
","['reinforcement-learning', 'markov-decision-process', 'reward-design', 'reward-functions']",
Is the next state drawn from the joint distribution of the previous state and action?,"
Suppose $G_t$, the discounted return at time $t$ is defined as: $$ G_t \triangleq R_t+\gamma R_{t+1}+\gamma^{2}R_{t+2} + \cdots = \sum_{j=1}^{\infty} \gamma^{k}R_{t+k}$$
where $R_t$ is the reward at time $t$ and $0 < \gamma < 1$ is a discount factor. Let the state-value function $v(s)$ be defined as: $$v_{\pi}(s) \triangleq \mathbb{E}[G_t|S_{t}=s]$$
In other words, it is the expected discounted return given that we start in state $s$ with some policy $\pi$. Then  $$v_{\pi}(s) = \mathbb{E}_{\pi}[R_t+\gamma G_{t+1}|S_{t}=s]$$
$$ = \sum_{a} \pi(a|s) \sum_{s',r} p(r,s'|s,a)[r+\ \gamma v_{\pi}(s')]$$

Question 1. Are the states $s'$ drawn from a from a joint probability distribution $P_{sa}$? In other words, if you are in an
  initial state $s$, take an action $\pi(s)$, then $s'$ is the random
  state you would end up in according to the probability distribution
  $P_{sa}$?

Also let $q_{\pi}(s,a)$, the action-value function be defined as: $$q_{\pi}(s,a) \triangleq \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]$$ 
$$=\sum_{s',r} p(r,s'|s,a)[r+\ \gamma v_{\pi}(s')]$$

Question 2. What are the advantages of looking at $q_{\pi}(s,a)$ versus $v_{\pi}(s)$?

","['reinforcement-learning', 'math', 'markov-decision-process']",
What is the difference between automatic transcription and automatic speech recognition?,"
What is the difference between automatic transcription and automatic speech recognition? Are they the same?
Is my following interpretation correct?
Automatic transcription: it converts the speech to text by looking at the whole spoken input 
Automatic speech recognition: it converts the speech to text by looking into word by word choices
","['natural-language-processing', 'comparison', 'speech-synthesis']",
Can we derive the distribution of a random variable based on a dependent random variable's distribution?,"
In the diagram below, there are three variables: X3 is a function of (depends on) X1 and X2, X2 also depends on X1. More specifically, X3 = f(X1, x2) and X2 = g(X1). Therefore, X3 = f(X1, g(X1)).

If the probabilistic distribution of X1 is known, is it possible to derive the probabilistic distribution of X3?
","['machine-learning', 'probability-distribution', 'statistical-ai', 'bayesian-networks']",
How to train chat bot on infinite non-stationary data?,"
I have continual simulated data of million sentences of two simulated persons talking to each other in a room and I want to model one of the persons speech. Now, during this period things in the room can change. Let's say, one of them says ""Where is the book?"" The other one responds ""I placed the book on the bookshelf"". Now during time, the position of the book changes, so the question Where is the book? does not have stationary answer i.e the answer changes during time. However, in general the answer has to be ""The book is at some_location"" and not something else. Also, the mentioning that the book is placed on the bookshelf can be sometimes 10, 100 or 1000 sentences before the question ""Where is the book?""
How do you approach this kind of problem? Since the window can be too large I can not split data into training samples of 10, 100 or 1000 sentences. My guess is that I should use BPTT + LSTM and train in one shot without shuffling the data. I am not sure this is feasible, so I will greatly appreciate your help! I have also my doubts what if ""Where is the book?"" appears 20 sentences after (instead of 10,100 and 1000) in the test set (which is not same as the training set)? Also, should I use Reinforcement Learning (since I can generate the data) or Supervised learning?
Thanks a lot!
","['deep-learning', 'reinforcement-learning', 'natural-language-processing', 'long-short-term-memory']",
Can traditional neural networks be combined with spiking neural networks?,"
Can traditional neural networks be combined with spiking neural networks? And can there be training algorithms for such hybrid network? Does such hybrid network model biological brains? 
As I understand, brains contain only spiking networks and traditional networks are more or less crude approximation of them. But we can imagine that evolutionary computing can surpass the biological evolution and so the new structure can be created that are better than mind. And that is why the question about such tradition-spiking hybrid neural networks should be interesting.
","['neural-networks', 'spiking-neural-networks']",
What is the relation between a policy which is the solution to a MDP and a policy like $\epsilon$-greedy?,"
In the context of reinforcement learning, a policy, $\pi$, is often defined as a function from the space of states, $\mathcal{S}$, to the space of actions, $\mathcal{A}$, that is, $\pi : \mathcal{S} \rightarrow \mathcal{A}$. This function is the ""solution"" to a problem, which is represented as a Markov decision process (MDP), so we often say that $\pi$ is a solution to the MDP. In general, we want to find the optimal policy $\pi^*$ for each MDP $\mathcal{M}$, that is, for each MDP $\mathcal{M}$, we want to find the policy which would make the agent behave optimality (that is, obtain the highest ""cumulative future discounted reward"", or, in short, the highest ""return"").
It is often the case that, in RL algorithms, e.g. Q-learning, people often mention ""policies"" like $\epsilon$-greedy, greedy, soft-max, etc., without ever mentioning that these policies are or not solutions to some MDP. It seems to me that these are two different types of policies: for example, the ""greedy policy"" always chooses the action with the highest expected return, no matter which state we are in; similarly, for the ""$\epsilon$-greedy policy""; on the other hand, a policy which is a solution to an MDP is a map between states and actions.
What is then the relation between a policy which is the solution to an MDP and a policy like $\epsilon$-greedy? Is a policy like $\epsilon$-greedy a solution to any MDP? How can we formalise a policy like $\epsilon$-greedy in a similar way that I formalised a policy which is the solution to an MDP?
I understand that ""$\epsilon$-greedy"" can be called a policy, because, in fact, in algorithms like Q-learning, they are used to select actions (i.e. they allow the agent to behave), and this is the fundamental definition of a policy.
","['reinforcement-learning', 'definitions', 'markov-decision-process', 'policies', 'exploration-strategies']","for example, the ""greedy policy"" always chooses the action with the highest expected return, no matter which state we are inThe ""no matter which state we are in"" there is generally not true; in general, the expected return depends on the state we are in and the action we choose, not just the action.In general, I wouldn't say that a policy is a mapping from states to actions, but a mapping from states to probability distributions over actions. That would only be equivalent to a mapping from states to actions for deterministic policies, not for stochastic policies.Assuming that our agent has access to (estimates of) value functions $Q(s, a)$ for state-action pairs, the greedy and $\epsilon$-greedy policies can be described in precisely the same way.Let $\pi_g (s, a)$ denote the probability assigned to an action $a$ in a state $s$ by the greedy policy. For simplicity, I'll assume there are no ties (otherwise it would in practice be best to randomize uniformly across the actions leading to the highest values). This probability is given by:$$
\pi_g (s, a) = 
\begin{cases}
1, & \text{if } a = \arg\max_{a'} Q(s, a') \\
0, & \text{otherwise}
\end{cases}
$$Similarly, $\pi_{\epsilon} (s, a)$ could denote the probability assigned by an $\epsilon$-greedy strategy, with probabilities given by:$$
\pi_{\epsilon} (s, a) = 
\begin{cases}
(1 - \epsilon) + \frac{\epsilon}{\vert \mathcal{A}(s) \vert}, & \text{if } a = \arg\max_{a'} Q(s, a') \\
\frac{\epsilon}{\vert \mathcal{A}(s) \vert}, & \text{otherwise}
\end{cases}
$$
where $\vert \mathcal{A}(s) \vert$ denotes the size of the set of legal actions in state $s$."
Deep Q-learning is not performing well when there are several enemies,"
I am playing with a deep Q-learning algorithm in my own environment. The network can perform well as long as there is only one enemy. My agent can perform the following actions:

do_nothing
prepare_for(e)
attack(e) 

where e is some enemy. 
In the case of two enemies, the action vector has 5 elements:
|   0       |      1          |      2      |        3         |     4      |
-----------------------------------------------------------------------------
|do_nothing | prepare_for(e1) |  attack(e1) |  prepare_for(e2) | attack(e2) |
-----------------------------------------------------------------------------

After a couple of episodes, the agent always starts picking the first do_nothing action, which is not desired. Changing reward for do_nothing action is not helping, even using significantly higher negative reward, than for other actions. 
There is no problem with the environment with only one enemy. (Only using columns 0, 1, 2). I feel like my action encoding can be the issue, but I can't figure it out, how to fix it. Any suggestions?
","['deep-learning', 'reinforcement-learning', 'deep-rl']",
What is the next state for a two-player board game?,"
I'm using Q-learning to train an agent to play a board game (e.g. chess, draughts or go). 
The agent takes an action while in state $S$, but then what is the next state (that is,  $S'$)? Is $S'$ now the board with the piece moved as a result of taking the action, or is $S'$ the state the agent encounters after the other player has performed his action (i.e. it's this agent's turn again)?
","['reinforcement-learning', 'q-learning']","If your opponent has fixed knowledge (it doesn't learn), then the next state after your agent did an action is the state when your turn is back. So the actions of other players are considered as an environment reaction to your actions.But if your opponent can learn, you may create a Multi-agent Reinforcement Learning"
How do updates in SARSA and Q-learning differ in code?,"
The update rules for Q-learning and SARSA each are as follows:
Q Learning:
$$Q(s_t,a_t)←Q(s_t,a_t)+α[r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t)]$$
SARSA:
$$Q(s_t,a_t)←Q(s_t,a_t)+α[r_{t+1}+γQ(s_{t+1},a_{t+1})−Q(s_t,a_t)]$$
I understand the theory that SARSA performs 'on-policy' updates, and Q-learning performs 'off-policy' updates.
At the moment I perform Q-learning by calculating the target thusly: 
target = reward + self.y * np.max(self.action_model.predict(state_prime))

Here you can see I pick the maximum for the Q-function for state prime (i.e. greedy selection as defined by maxQ in the update rule). If I were to do a SARSA update and use the same on-policy as used when selecting an action, e.g. ϵ-greedy, would I basically change to this:
if np.random.random() < self.eps:
    target = reward + self.y * self.action_model.predict(state_prime)[random.randint(0,9)]
else:
    target = reward + self.y * np.max(self.action_model.predict(state_prime))

So sometimes it will pick a random future reward based on my epsilon greedy policy?
","['reinforcement-learning', 'q-learning']",Picking actions and making updates should be treated as separate things. For Q-learning you also need to explore by using some exploration strategy (e.g. $\epsilon$-greedy).Steps for Q-learning:Steps for Sarsa:
What is the relation between online (or offline) learning and on-policy (or off-policy) algorithms?,"
In the context of RL, there is the notion of on-policy and off-policy algorithms. I understand the difference between on-policy and off-policy algorithms. Moreover, in RL, there's also the notion of online and offline learning.
What is the relation (including the differences) between online learning and on-policy algorithms? Similarly, what is the relation between offline learning and off-policy algorithms?
Finally, is there any relation between online (or offline) learning and off-policy (or on-policy) algorithms? For example, can an on-policy algorithm perform offline learning? If yes, can you explain why?
","['reinforcement-learning', 'off-policy-methods', 'on-policy-methods', 'online-learning', 'offline-reinforcement-learning']",
How are vectors and matrices multiplied in supervised machine learning?,"
I've recently started reading a book about deep learning. The book is titled ""Grokking Deep Learning"" (by Andrew W Trask). In chapter 3 (pages 44 and 45), it talks about multiplying vectors using dot product and element-wise multiplication. For instance, taking 3 scalar inputs (vector) and 3 vector weights (matrix) and multiplying. 
From my understanding, when multiplying vectors the size needs to be identical. The concept I have a hard time understanding is multiplying vectors by a matrix. The book gives an example of an 1x4 vector being multiplied by 4x3 matrix. The output is an 1x3 vector. I'm am confused because I assumed multiplying vector by matrix needs the same number of columns, but I have read that the matrices need rows equal to the vectors columns. 
If I do not have an equal number of columns, how does my deep learning algorithm multiply each input in my vector by a corresponding weight?
","['deep-learning', 'math']",
When is a knowledge base consistent?,"
I am studying a knowledge base (KB) from the book ""Artificial Intelligence: A Modern Approach"" (by Stuart Russell and Peter Norvig) and from this series of slides.
A formula is satisfiable if there is some assignment to the variables that makes the formula evaluate to true. For example, if we have the boolean formula $A \land B$, then the assignments $A=\text{true}$ and $B=\text{true}$ make it satisfiable. Right?
But what does it mean for a KB to be consistent? The definition (given at slide 14 of this series of slides) is:

a KB is consistent with formula $f$ if $M(KB \cup \{ f \})$ is non-empty (there is a world in which KB is true and $f$ is also true).

Can anyone explain this part to me with an example?
","['definitions', 'logic', 'knowledge-representation', 'norvig-russell', 'knowledge-base']","I will first recapitulate the key concepts which you need to know in order to understand the answer to your question (which will be very simple, because I will just try to clarify what is given as a ""definition"").In logic, a formula is e.g. $f$, $\lnot f$, $f \land g$, where $f$ can be e.g. the proposition (or variable) ""today it will rain"".  So, in a (propositional) formula, you have propositions, i.e. sentences like ""today it will rain"", and logical connectives, i.e. symbols like $\land$ (i.e. logical AND), which logically connect these sentences. The propositions like ""today it will rain"" can often be denoted by a single (capital) letter like $P$. $f \land g$ is the combination of two formulae (where formulae is the plural of formula). So, for example, suppose that $f$ is composed of the propositions ""today it will rain"" (denoted by $P$) or ""my friend will visit me"" (denoted by $Q$) and $f$ is defined as ""I will play with my friend"" (denoted by $S$). Then the formula $f \land g = (P \lor Q) \land S$. In general, you can combine formulae in any logically appropriate way.In this context, a model is an assignment to each variable in a formula. For example, suppose $f = P \lor Q$, then $w = \{ P=0, Q = 1\}$ is a model for $f$, that is, each variable (e.g. $P$) is assigned either ""true"" ($1$) or ""false"" ($0$) but not both. (Note that the word model may be used to refer to different concepts depending on the context; again, in this context, you can simply think of a model as an assignment of values to the variables in a formula.)Suppose now we define $I(f, w)$ to be a function that receives the formula $f$ and the model $w$ as input, and $I$ returns either ""true"" ($1$) or ""false"" ($0$). In other words, $I$ is a function that automatically tells us if $f$ is evaluated to true or false given the assignment $w$.You can now define $M(f)$ to be a set of assignments (or models) to the formula $f$ such that $f$ is true. So, $M$ is a set and not just an assignment (or model). This set can be empty, it can contain one assignment or it can contain any number of assignments: it depends on the formula $f$: in some cases, $M$ is empty and, in other cases, it may contain say $n$ valid assignments to $f$, where by ""valid"" I mean that these assignments make $f$ evaluate to ""true"". For example, suppose we have formula $f = A \land \lnot A$. Then you can try to assign any value to $A$, but $f$ will never evaluate to true. In that case, $M(f)$ is an empty set, because there is no assignment to the variables (or propositions) of $f$ which make $f$ evaluate to true.A knowledge base is a set of formulae $\text{KB} = \{ f_1, f_2, \dots, f_n \}$. So, for example, $f_2 = $ ""today it will rain"" and $f_3 = $ ""I will go to school AND I will have lunch"".We can now define $M(\text{KB})$ to be the set of assignments to the formulae in the knowledge base $\text{KB}$ such that all formulae are true. If you think of the formulae in $KB$ as ""facts"", $M(\text{KB})$ is an assignment to these formulae in $KB$ such that these facts hold or are true.In this context, we then say that a particular knowledge base (i.e., a set of formulae as defined above), denoted by $\text{KB}$, is consistent with formula $f$ if $M(\text{KB} \cup \{ f \})$ is a non-empty set, where $\cup$ means the union operation between sets: note that (as we defined it above) $\text{KB}$ is a set, and $\{ f \}$ means that we are making a set out of the formula $f$, so we are indeed performing an union operation on sets.So, what does it mean for a knowledge base to be consistent? First of all, the consistency of a knowledge base $\text{KB}$ is defined with respect to another formula $f$. Recall that a knowledge base is a set of formulae, so we are defining the consistency of a set of formulae with respect to another formula.When is then a knowledge base $\text{KB}$ consistent with a formula $f$? When $M(\text{KB} \cup \{ f \})$ is a non-empty set. Recall that $M$ is an assignment to the variables in its input such that its inputs evaluate to true. So, $\text{KB}$ is consistent with $f$ when there is a set of assignments of values to the formulae in $\text{KB}$ and an assignment of values to the variables in $f$ such that both $\text{KB}$ and $f$ are true. In other words, $\text{KB}$ is consistent with $f$ when both all formulae in $\text{KB}$ and $f$ can be true at the same time."
What is the intuition behind the calculation of the similarity between encoder and decoder states?,"
Suppose that we are doing machine translation. We have a conditional language model  with attention where we are are trying to predict a sequence $y_1, y_2, \dots, y_J$ from $x_1, x_2, \dots x_I$: $$P(y_1, y_2, \dots, y_{J}|x_1, x_2, \dots x_I) = \prod_{j=1}^{J} p(y_j|v_j, y_1, \dots, y_{j-1})$$ where $v_j$ is a context vector that is different for each $y_j$. Using an RNN with a encoder-decoder structure, each element $x_i$ of the input sequence and $y_j$ of the output sequence is converted into an embedding $h_i$ and $s_j$ respectively: $$h_i = f(h_{i-1}, x_i) \\ s_j = g(s_{j-1},[y_{j-1}, v_j])$$ where $f$ is some function of the previous input state $h_{i-1}$ and the current input word $x_i$ and $g$ is some function of the previous output state $s_{j-1}$, the previous output word $y_{j-1}$ and the context vector $v_j$.
Now, we want the process of predicting $s_j$ to ""pay attention"" to the correct parts of the encoder states (context vector $v_j$). So: $$v_j = \sum_{i=1}^{I} \alpha_{ij} h_i$$ where $\alpha_{ij}$ tells us how much weight to put on the $i^{th}$ state of the source vector when predicting the $j^{th}$ word of the output vector. Since we want the $\alpha_{ij}$s to be probabilities, we use a softmax function on the similarities between the encoder and decoder states: $$\alpha_{ij} = \frac{\exp(\text{sim}(h_i, s_{j-1}))}{\sum_{i'=1}^{I} \exp(\text{sim}(h_i, s_{j-1}))}$$
Now, in additive attention, the similarities of the encoder and decoder states are computed as: $$\text{sim}(h_i, s_{j}) = \textbf{w}^{T} \text{tanh}(\textbf{W}_{h}h_{i} +\textbf{W}_{s}s_{j})$$
where $\textbf{w}$, $\textbf{W}_{h}$ and $\textbf{W}_{s}$ are learned attention parameters using a  one-hidden layer feed-forward network. 
What is the intuition behind this definition? Why use the $\text{tanh}$ function? I know that the idea is to use one layer of a neural network to predict the similarities. 
Added. This description of machine translation/attention is based on the Coursera course Natural Language Processing.
","['recurrent-neural-networks', 'attention', 'machine-translation']",
"A neural network for digits recognition doesn't work (MNIST, Numpy) [closed]","







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 4 years ago.







                        Improve this question
                    



I'm a beginner in machine learning and I was trying to make a test neural network for digits recognition from scratch using Numpy. I used MNIST dataset for training and testing. Input layer have 28*28 neurons which correspond to each pixel of image that must be recognized. Output layer have 10 neurons which correspond to each digit (0-9), and return values from 0 to 1 which mean chance that the corresponding digit is displayed on the image. Class Layer represents a separate layer and contains links to previous and next layer (if prevLayer is None, the current layer is input layer; if nextLayer is None, the current layer is output). The forward() method is responsible for passing data through neural network. The backprop() method is responsible for training of neural network via Backpropagation algorithm. A Layer object contains weights (W) between previous and current layer (input layer object doesn't contain weights). 'data_in' property contains a vector of calculated values before passing them into activation function. Property 'data' contains the values after activation function. But, unfortunately, it doesn't work: returned value of loss function doesn't decrease during training and neural network returns the same result during testing. I assume that bugs might be associated with backprop() and softmax_derivatime() methods. I tried in vain to find all bugs. Here's my code:
import numpy as np

def ReLU(x):
    return np.maximum(0, x)

def ReLU_derivative(x):
    return np.greater(x, 0).astype(int)

def softmax(x):
    shift = x - np.max(x)
    return np.exp(shift) / np.sum(np.exp(shift))

def softmax_derivative(x):
    sm_array = softmax(x)
    J = np.zeros((x.size, x.size))
    for i in range(x.size):
        for j in range(x.size):
            delta = np.equal(i, j).astype(int)
            J[j, i] = sm_array[0][i] * (delta - sm_array[0][j])
    return J

class Layer:
    def __init__(self, size, prev_layer=None):
        self.size = size
        self.prevLayer = prev_layer
        self.nextLayer = None
        self.data = None
        self.data_in = None
        if prev_layer is not None:
            self.prevLayer.nextLayer = self
            self.W = np.random.random((self.prevLayer.size, size))
            self.W_bias = np.array([np.random.random(size)])
        else:
            self.W = None
            self.W_bias = None

    def forward(self):
        if self.prevLayer is not None:
            self.data_in = np.dot(self.prevLayer.data, self.W)
            self.data_in += np.dot([[1]], self.W_bias)
            if self.nextLayer is not None:
                self.data = ReLU(self.data_in)
                self.nextLayer.forward()
            else:
                self.data = softmax(self.data_in)
        else:
            self.nextLayer.forward()

    def backprop(self, expected_output=None, prev_delta=None):
        if prev_delta is None:
            #print(self.data_in)
            delta = np.dot(-(expected_output - self.data), softmax_derivative(self.data_in))
            delta_bias = delta
        else:
            delta = np.dot(prev_delta, self.nextLayer.W.T) * ReLU_derivative(self.data_in)
            delta_bias = np.dot(prev_delta, self.nextLayer.W_bias.T) * ReLU_derivative(self.data_in)
        training_velocity = 0.1
        W_dif = np.dot(self.prevLayer.data.T, delta) * training_velocity
        W_bias_dif = np.dot([[1]], delta_bias) * training_velocity
        if self.prevLayer.prevLayer is not None:
            self.prevLayer.backprop(prev_delta=delta)
        self.W -= W_dif
        self.W_bias -= W_bias_dif

f_images = open(""train-images.idx3-ubyte"", ""br"")
f_images.seek(4)
f_labels = open(""train-labels.idx1-ubyte"", ""br"")
f_labels.seek(8)
images_number = int.from_bytes(f_images.read(4), byteorder='big')
rows_number = int.from_bytes(f_images.read(4), byteorder='big')
cols_number = int.from_bytes(f_images.read(4), byteorder='big')

input_layer = Layer(rows_number*cols_number)
hidden_layer1 = Layer(rows_number*cols_number*7//10, input_layer)
hidden_layer2 = Layer(rows_number*cols_number*7//10, hidden_layer1)
output_layer = Layer(10, hidden_layer2)
digits = np.array([np.zeros(10)])

input_image = np.array([np.zeros(rows_number * cols_number)])
for k in range(images_number):
    for i in range(rows_number):
        for j in range(cols_number):
            input_image[0][i*cols_number+j] = int.from_bytes(f_images.read(1), byteorder='big') / 255.0 * 2 - 1
    input_layer.data = input_image
    input_layer.forward()
    current_digit = int.from_bytes(f_labels.read(1), byteorder='big')
    digits[0][current_digit] = 1
    output_layer.backprop(expected_output=digits)
    print(np.sum((digits - output_layer.data)**2)/2)
    digits[0][current_digit] = 0
    if((k+1) % 1000 == 0):
        print(str(k+1) + "" / "" + str(images_number))
f_images.close()
f_labels.close()

f_images = open(""t10k-images.idx3-ubyte"", ""br"")
f_images.seek(4)
f_labels = open(""t10k-labels.idx1-ubyte"", ""br"")
f_labels.seek(8)
images_number = int.from_bytes(f_images.read(4), byteorder='big')
rows_number = int.from_bytes(f_images.read(4), byteorder='big')
cols_number = int.from_bytes(f_images.read(4), byteorder='big')

for k in range(images_number):
    for i in range(rows_number):
        for j in range(cols_number):
            input_image[0][i*cols_number+j] = int.from_bytes(f_images.read(1), byteorder='big')
    input_layer.data = input_image
    input_layer.forward()
    current_digit = int.from_bytes(f_labels.read(1), byteorder='big')
    print(output_layer.data)

f_images.close()
f_labels.close()

I would appreciate for any help. Thanks in advance!
","['neural-networks', 'machine-learning', 'backpropagation', 'object-recognition', 'perceptron']","It seems I've solved the issue. There was several mistakes:
1. I've generated random weights from 0 to 1. As a result, too big numbers passed through softmax function (>10000), and the function wasn't calculated correctly. I divided each initial weight on the number of neurons in previous layer and solved the issue.
2. I've calculated separate delta for biases while delta must be the same for main weights and biases.
If anyone is interested, here is the correct code (83% and 89% precision after first and second launch):"
Bubble Chamber Image Analysis Using Neural Network,"
I have a data analysis problem that I can reduce to one similar to analyzing the trajectories in the images below. These images show the tracks of subatomic particles interacting in a bubble chamber. 
It's pretty obvious that by eye, easily discernible patterns can be seen. I want very much to know more about how classification and segmentation can be done using neural networks for this type of image. 
These images are binary. The trajectory is either at a point in the image or it isn't. As can be seen, trajectories cross over one another, Some data appears to be missing in otherwise smooth curves, at arbitrary points along those curves. (My data may be more sparse in this respect.)
A typical paper on bubble chamber analysis that I would find deals with the analysis of the physics after trajectories have been classified and segmented. 
Can anyone identify some papers that address this or something similar in the context of neural networks? I am not able to find anything recent on automated methods at all, but my google fu may not be up to the challenge. (By the way, I am less interested in some of the parametric methods like Hough Transforms. I'd like to focus on the neural approach.)
(I posted this previous question which wasn't quite as specific as this one. I hope there is some available research in this area related to physics that might give me some insights that are more directly related to my problem.)


","['image-recognition', 'classification', 'pattern-recognition']",
Experiment shows that LSTM does worse than Random Forest... Why?,"
LSTM is supposed to be the right tool to capture path-dependency in time-series data.
I decided to run a simple experiment (simulation) to assess the extent to which LSTM is better able to understand path-dependency.
The setting is very simple. I just simulate a bunch (N=100) paths coming from 4 different data generating processes. Two of these processes represent a real increase and a real decrease, while the other two fake trends that eventually revert to zero.
The following plot shows the simulated paths for each category:

The candidate machine learning algorithm will be given the first 8 values of the path ( t in [1,8] ) and will be trained to predict the subsequent movement over the last 2 steps. 
In other words:

the feature vector is X = (p1, p2, p3, p4, p5, p6, p7, p8)
the target is y = p10 - p8

I compared LSTM with a simple Random Forest model with 20 estimators. Here are the definitions and the training of the two models, using Keras and scikit-learn:
# LSTM
model = Sequential()
model.add(LSTM((1), batch_input_shape=(None, H, 1), return_sequences=True))
model.add(LSTM((1), return_sequences=False))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
history = model.fit(train_X_LS, train_y_LS, epochs=100, validation_data=(vali_X_LS, vali_y_LS), verbose=0)

# Random Forest
RF = RandomForestRegressor(random_state=0, n_estimators=20)
RF.fit(train_X_RF, train_y_RF);

The results are the summarized by the following scatter plots:

As you can see, the Random Forest model is clearly outperforming the LSTM. The latter seems to be not able to distinguish between the real and the fake trends.
Do you have any idea to explain why this is happening?
How would you modify the LSTM model to make it better at this problem?
Some remarks:

The data points are divided by 100 to make sure gradients do not explode
I tried to increase the sample size, but I noticed no differences
I tried to increase the number of epochs over which the LSTM is trained, but I noticed no differences (the loss becomes stagnant after a bunch of epochs)
You can find the code I used to run the experiment here 

","['machine-learning', 'deep-learning', 'long-short-term-memory']",
How do I convert table-based to neural network-based Q-learning?,"
I've used a table to represent the Q function, while an agent is being trained to catch the cheese without touching the walls.
The first and last row (and column) of the matrix are associated with the walls. I placed in last cell a cheese that agent must catch while being training.
So far, I've done it with dynamic states and, when necessary, I resized matrix with new states. I've used four actions (up, left, right and down). 
I would like now to use an ANN to represent my Q function. How do I do that? What should be the input and output of such neural network?
","['neural-networks', 'reinforcement-learning', 'q-learning']",
How to handle infeasibility caused due to crossover and mutation in genetic algorithm for optimization?,"
I have chromosomes with floating-point representation with values between $0$ and $1$. For example
Let $p_1 = [0.1, 0.2, 0.3]$ and $p_2 = [0.5, 0.6, 0.7]$ be two parents. Both comply with the set of constraints. In my case, the major constraint is $$ p_1[1]*p_1[2] - k*p_1[0] \geq 0 $$ for any chromosome $p_1$. For the example above we can take $k=0.3$, which renders $c_2$ infeasible.
However, the children produced by 1 point crossover, we get $c_1 = [0.1, 0.6, 0.7]$ and $c_2 = [0.5, 0.2, 0.3]$ out of which 1 or both may not comply with the given constraints.
A similar scenario can also occur with a small perturbation of values due to mutation strategy. Correct me if I am wrong in the belief that such kind of scenarios might arise irrespective of the strategy employed for crossover and mutation.
What are the options to handle such kinds of cases?
","['genetic-algorithms', 'crossover-operators', 'mutation-operators', 'constrained-optimization']",
How to perform structure learning for Bayesian network given already partially constructed Bayesian network?,"
Let's assume that we have a dataset of variables (random events)I apriori would like to set dependency conditions between some of them and perform structure learning to figure out the rest of the Bayesian network.
How can this be done practically (e.g. some libraries, like bnlearn) or, at least, in theory?
I was trying to google it, but haven't found anything related.
","['reference-request', 'algorithm-request', 'bayesian-inference', 'bayesian-networks']",
Can Q-learning be used to derive a stochastic policy?,"
In my understanding, Q-learning gives you a deterministic policy. However, can we use some technique to build a meaningful stochastic policy from the learned Q values? I think that simply using a softmax won't work. 
","['reinforcement-learning', 'q-learning', 'stochastic-policy', 'deterministic-policy']","No it is not possible to use Q-learning to build a deliberately stochastic policy, as the learning algorithm is designed around choosing solely the maximising value at each step, and this assumption carries forward to the action value update step $Q_{k+1}(S_t,A_t) = Q_k(S_t,A_t) + \alpha(R_{t+1} +\gamma\text{max}_{a'}Q_k(S_{t+1},a') - Q_k(S_t,A_t))$ - i.e. the assumption is that the agent will always choose the highest Q value, and that in turn is used to calculate the TD target values. If you use a stochastic policy as the target policy, then the assumption is broken and the Q table (or approximator) would not converge to estimates of action value for the policy*.The policy produced by Q-learning can only be treated as stochastic when there is more than one maximum action value in a particular state - in which case you can select equivalent maximising values using any distribution.In theory you could use the Q values to derive various distributions, such as a Boltzmann distribution, or softmax as you suggest (you will want to include some weighting factor to make softmax work in general). These can work nicely for the behaviour policy, for further training, and as an alternative to the more common $\epsilon$-greedy approach. However, they are not optimal policies, and the training algorithm will not adjust the probabilities in any meaningful way related to the problem you want to solve. You can set a value for e.g. $\epsilon$ for $\epsilon$-greedy, or have more sophisticated action choice with more parameters, but no value-based method can provide a way to change those parameters to make action choice optimal. In cases where a stochastic policy would perform better - e.g. Scissor, Paper, Stone versus an opponent exploiting patterns in the agent's behaviour - then value based methods provide no mechanism to learn a correct distribution, and they typically fail to learn well in such environments. Instead you need to look into policy gradient methods, where the policy function is learned directly and can be stochastic. The most basic policy gradient algorithm is REINFORCE, and variations on Actor-Critic such as A3C are quite popular.* You could get around this limitation by using an estimator that does work with a stochastic target policy, e.g. SARSA or Expected SARSA. Expected SARSA can even be used off-policy to learn one stochastic policy's Q values whilst behaving differently. However, neither of these provide you with the ability to change the probability distribution towards an optimal one."
How to use CNN for making predictions on non-image data?,"
I have a dataset which I have loaded as a data frame in Python. It consists of 21392 rows (the data instances, each row is one sample) and 1972 columns (the features). The last column i.e. column 1972 has string type labels (14 different categories of target labels). I would like to use a CNN to classify the data in this case and predict the target labels using the available features. This is a somewhat unconventional approach though it seems possible. However, I am very confused on how the methodology should be as I could not find any sample code/ pseudo code guiding on using CNN for Classifying non-image data, either in Tensorflow or Keras. Any help in this regard will be highly appreciated. Cheers!
","['convolutional-neural-networks', 'tensorflow']","You can use CNN on any data, but it's recommended to use CNN only on data that have spatial features (It might still work on data that doesn't have spatial features, see DuttaA's comment below).For example, in the image, the connection between pixels in some area gives you another feature (e.g. edge) instead of a feature from one pixel (e.g. color). So, as long as you can shaping your data, and your data have spatial features, you can use CNN.For Text classification, there are connections between characters (that form words) so you can use CNN for text classification in character level.For Speech recognition, there is also a connection between frequencies from one frame with some previous and next frames, so you can also use CNN for speech recognition.If your data have spatial features, just reshape it to a 1D array (for example in text) or 2D array (for example in Audio). Tensorflow's function conv1d and conv2d are general function that can be used on any data. It look the data as an array of floating-point, not as image/audio/text.But if your data doesn't have spatial features, for example, your features are price, salary, status_marriage, etc. I think you don't need CNN, and using CNN won't help."
Is a dataset of roughly 700 sentences of an average length of 15 words enough for text classification?,"
I'm building a customer assistant chatbot in Python. So, I am modelling this problem as a text classification task. I have available more or less 7 hundred sentences of an average length of 15 words (unbalanced class).
What do you think, knowing that I have to do an oversampling, is this dataset large enough?
","['natural-language-processing', 'classification', 'datasets', 'chat-bots', 'text-classification']","It depends on the number of classes; we are getting good results with about 40 training examples per class.A good way to get an idea about this is to run a test with an increasing set of training data, evaluating the result as you go along. Obviously, with a small set (eg 3 sentences per class), it will be very poor, but the accuracy should quickly increase and then stabilise at a higher level. With larger amounts of data you will probably only find a small increase or no change at all.Collecting this data would not only give you confidence in your conclusion, it would also be a good supporting argument when you have to ask for more training data, or have to justify the poor performance of the classifier if you do find the data set is too small.So, set up an automated 10-fold cross validation, feed an increasing amount of your available data into it, sit back, and graph the results."
"How are the reward functions $R(s)$, $R(s, a)$ and $R(s, a, s')$ equivalent?","
In this video, the lecturer states that $R(s)$, $R(s, a)$ and $R(s, a, s')$ are equivalent representations of the reward function. Intuitively, this is the case, according to the same lecturer, because $s$ can be made to represent the state and the action. Furthermore, apparently, the Markov decision process would change depending on whether we use one representation or the other.
I am looking for a formal proof that shows that these representations are equivalent. Moreover, how exactly would the Markov decision process change if we use one representation over the other? Finally, when should we use one representation over the other and why are there three representations? I suppose it is because one representation may be more convenient than another in certain cases: which cases? How do you decide which representation to use?
","['reinforcement-learning', 'markov-decision-process', 'proofs', 'notation', 'reward-functions']",
What should be saved in SARSA prioritized sweeping?,"
In the book ""Reinforcement Learning: An Introduction"", by Sutton and Barto, they provided the ""Q-learning prioritized sweeping"" algorithm, in which the model saves the next state and the immediate reward, for each state and action, that is, $Model(S_{t},A_{t}) \leftarrow S_{t+1}, R_{t+1}$. 
If we want to use ""SARSA prioritized sweeping"", should we save ""next state, immediate reward, and next action"", that is, $Model(S_{t},A_{t}) \leftarrow  S_{t+1}, R_{t+1}, A_{t+1}$?
",['reinforcement-learning'],
Key Point Extraction the best method?,"
I have been researching about determining some key points on an image, in this case I'm gonna use cloth (top side of human body) pictures. I want to detect some corner points on those. 
Example:

I have two solutions on my mind. One CNN with transpose layers resulting in heatmap where I can get points. The second is to get 24 number as output from the model meaning 12(x,y) point. I don't know which one will be better. 
In face point detection, they use the second method. In human pose estimation, they use method one. So what do you suggest me to use? or do you have any new ideas? Thanks
","['deep-learning', 'feature-extraction']","The 2nd method would make sense only if your object is at the same position in all test images. You would have such situation if you operated on crops located by a separate object detection algorithm. This happens to be the case in facial key-point detection. The 1st method would be much more robust to various object poses since it is translation covariant by design. If a keypoint is detected at location A, it will be equally well detected at any other position with the same set of parameters."
Should I choose a model with the smallest loss or highest accuracy?,"
I have two Machine Learning models (I use LSTM) that have a different result on the validation set (~100 samples data):

Model A: Accuracy: ~91%, Loss: ~0.01
Model B: Accuracy: ~83%, Loss: ~0.003

The size and the speed of both models are almost the same. So, which model should I choose?
","['deep-learning', 'long-short-term-memory', 'loss', 'accuracy', 'validation']","You should note that both your results are consistent with a ""true"" probability of 87% accuracy, and your measurement of a difference between these models is not statistically significant. With an 87% accuracy applied at random, then there is approx 14% chance of getting the two extremes of accuracy you have observed by chance if samples are chosen randomly from the target population, and models are different enough make errors effectively at random. This last assertion is usually not true though, so you can relax a little - that is, unless you took different random slices for cross-validation in each case.100 test cases is not really enough to discern small differences between models. I would suggest using k-fold cross-validation in order to reduce errors in your accuracy and loss estimates.Also, it is critical to check that the cross-validation split was identical in both cases here. If you have used auto-splitting with a standard tool and not set the appropriate RNG seed, then you may have got a different set each time, and your results are just showing you variance due to the validation split which could completely swamp any differences between the models.However, assuming the exact same dataset was used each time, and it was representative sample of your target population, then on average you should expect the one with the best metric to have the highest chance of being the best model.What you should really do is decide which metric to base the choice on in advance of the experiment. The metric should match some business goal for the model.Now you are trying to choose after the fact, you should go back to the reason you created the model in the first place and see if you can identify the correct metric. It might not be either accuracy or loss."
How do we create a good agent that does not outperform humans?,"
A lot of research has been done to create the optimal (or ""smartest"") RL agent, using methods such as A2C. An agent can now beat humans at playing Go, Chess, Poker, Atari Games, DOTA, etc. But I think these kind of agents will never be a friend of humans, because humans won't play with a agent that always beats them. 
How could we create an agent that doesn't outperform humans, but it has the human level skill, so that when it plays agains a human, the human is still motivated to beat it?
","['reinforcement-learning', 'philosophy', 'intelligent-agent']","You basically have to degrade the result, assuming that the machine always finds the best move. There are a number of possibilities:restrict the depth of searching. In early chess programs I believe that was the main way of regulating the difficulty. You stop the evaluation of moves after a particular depth in your search tree has been reached. This would be equivalent to only looking ahead two moves instead of twenty.set a time limit. This is somewhat similar the restricting the depth of the search, but more generally applicable. If your algorithm accumulates candidate moves, and the general tendency is to get to the better moves after first finding a number of weaker ones, then you can stop at a given point in time and return what you have found then.distort available information. This might not be that applicable to games such a chess, but you could restrict the information the machine has available for evaluating moves. Something like the ""Fog of War"" often used in strategy games. With incomplete information it is harder to find a good move, though it is not impossible, which makes it more challenging than, say, restricting the depth of search too much.sub-optimal evaluation function. If you have a function that evaluates the quality of a move, simply fudge that function to not return the best value. Perhaps add a random offset to the return value to make it less deterministic/predictable.There are probably other methods as well; the tricky part is to tread the fine line between appearing to be a weaker (but consistent) player, and just being a random number generator."
Can neuroevolution be used for solving tasks other than games?,"
I'm seeing a lot of examples of neuroevolution techniques involving games or robot problems. Can neuroevolution be used for solving tasks other than games? For example, how could you transform a CSV file of psychological data to determine the best life actions you can get from a self-report questionnaire?
","['neural-networks', 'applications', 'evolutionary-algorithms', 'neuroevolution']",
Batch PTA stopping condition,"
I am reviewing my Neural Network lectures and I have a doubt: My book's (Haykin) batch PTA describes a cost function which is defined over the set of the misclassified inputs.
I have always been taught to use MSE < X as a stopping condition for the training process. Is the batch case different? Should I use as stopping condition size(misclassified) < Y (and as a consequence when the weight change is very little)?
Moreover, the book uses the same symbol for both the training set and the misclassified input set. Does this mean that my training set changes each epoch?
","['neural-networks', 'objective-functions', 'perceptron']",
Will Q-learning converge to the optimal state-action function when the reward periodically changes?,"
Imagine that the agent receives a positive reward upon reaching a state 𝑠. Once the state 𝑠 has been reached the positive reward associated with it vanishes and appears somewhere else in the state space, say at state 𝑠′. The reward associated to 𝑠′ also vanishes when the agent visits that state once and re-appears at state 𝑠. This goes periodically forever. Will discounted Q-learning converge to the optimal policy in this setup? Is yes, is there any proof out there, I couldn't find anything.
","['reinforcement-learning', 'q-learning']","No, it will not converge in the general case (maybe it might in extremely convenient special cases, not sure, didn't think hard enough about that...). Practically everything in Reinforcement Learning theory (including convergence proofs) relies on the Markov property; the assumption that the current state $s_t$ includes all relevant information, that the history leading up to $s_t$ is no longer relevant. In your case, this property is violated; it is important to remember whether or not we visited $s$ more recently than $s'$. I suppose if you ""enhance"" your states such that they include that piece of information, then it should converge again. This means that you'd essentially double your state-space. For every state that you have in your ""normal"" state space, you'd have to add a separate copy that would be used in cases where $s$ was visited more recently than $s'$."
Meaning of Actor Output in Actor Critic Reinforcement Learning,"
In actor critic, The equations for calculating the loss in actor critic are an 
actor loss (parameterized by $\theta$) 
$$log[\pi_\theta(s_t,a_t)]Q_w(s_t,a_t)$$ 
and a critic loss (parameterized by $w$) 
$$r(s_t,a_t) + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_{t}, a_t).$$
This is bootstrapping in experience replay:
$$
L_i(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \left[ \left(r + \gamma \max_{a'} Q(s', a'; \theta_i^-) - Q(s, a; \theta_i)\right)^2 \right]
$$
It is clear that bootstrapping is comparable to the critic loss, except that the $max$ operation is lacking from the critic.
As i see it, (correct me if I'm wrong):
$Q(s_t,a_t) = V(s_{t+1}) + r_t$ where $a_t$ is the actual action that had been taken.
The critic, as I understand, estimates $V(s)$
My question:
What exactly is the critic calculating?
What In actor critic outputs $Q(s_{t+1},a_{t+1})$?
It seems to me like the critic calculates the average next state $s_{t+1}$ value, over all possible actions, with their corresponding probabilities, yielding
$Q(s_t, a_t) = r_t + \sum_{a_{t+1} \in A}P(a_{t+1}|s_t)V(s_{t+1})$
Which would mean that in order to get $Q(s_{t+1}, a_{t+1})$ for the above formula, I would need to calculate
$Q(s_{t+1}, a_{t+1}) = r_{t+1} + \sum_{a_{t+2} \in A}P(a_{t+2}|s_{t+1})V(s_{t+2})$
Where $V(s_{t+2})$ is the critic output on $s_{t+2}$, a state we get to by taking action $a_{t+1}$ from state $s_{t+1}$ but I am not sure that is indeed the meaning of the critic output and still it is unclear to me how to get $Q(s_{t+1}, a_{t+1})$ from actor critic.
If indeed that is what's being calculated, then why is it mathematically true that an improvement is being made? Or why does it make sense (even if not mathematically always true)?

Practical use:
I want to use actor critic with experience replay in an environment with a large action space (could be continuous). Therefore, I cannot use the $max$ term. I need to understand the correct equation for the critic loss, and why it works.
","['machine-learning', 'reinforcement-learning', 'q-learning', 'policy-gradients', 'actor-critic-methods']","When using the loss function for the critic described in your question, the Actor-Critic is an on-policy approach (as are most Actor-Critic methods). Your intuition as to what it is learning seems to be quite close, but the notation/terminology is not quite on point.First it's important to realize that the $Q(s, a)$ critic is an estimator, we're training it to estimate state-action values. You could say that we are training it such that it can hopefully provide accurate estimates of:$$Q_w^{\pi} (s_t, a_t) \approx \mathbb{E}_{\pi} \left[ r_t + \gamma V^{\pi}(s_{t+1}) \right].$$You'll notice I've added quite a number of symbols there in comparison to your $Q(s_t, a_t) = r_t + V(s_{t+1})$:So, the critic is trained to estimate $Q^{\pi}(s, a)$, which can intuitively be interpreted as the long-term discounted rewards that we expect to collect when executing $a$ in $s$, and selecting actions according to the distribution $\pi$ subsequently. It definitely still is trying to estimate $Q(s, a)$ values for state-action pairs, not just $V(s)$ values for states alone.What In actor critic outputs $Q(s_{t+1},a_{t+1})$?In practice, when using the loss function described in your question, $a_{t+1}$ really simply is a single action selected in an actual trajectory of experience by the policy $\pi$. The trained network simply takes $s_{t+1}$ as input, and the output corresponding to a single action $a_{t+1}$ as selected by the policy is used as the value for $Q(s_{t+1},a_{t+1})$ in the update rule. The update rule does not involve any sum over all actions, multiplied with their probabilities. The ""trick"" is that we do not just run the update rule a single time, but we expect to generate lots (sometimes millions) of trajectories as experience, and we repeatedly run the update rule. In different trajectories, we'll experience the different actions $a_t$ as samples with approximately the correct frequencies, and in expectation we'll have proper update targets (except for potential bias resulting from function approximation).I want to use actor critic with experience replay in an environment with a large action spaceThe Actor-Critic method in your question is, as I mentioned above, on-policy. This means that the experience used in update rules has to be generated according to exactly the same policy for which you are also learning value estimates. This is incompatible with the idea of experience replay, because old trajectories stored in a replay buffer were generated by older versions of your policy.There are off-policy Actor-Critic methods which can correct for the mismatch in distributions and use experience replay, but these are going to be quite a bit more complicated. Examples are ACER and IMPALA."
How to train CNN such it eliminate dependent features and focuses on independent ones?,"
How we should train a CNN model when training dataset contains only limited number of cases, and the trained model is supposed to predict class (label) for several other cases, which has not seen before? 
Supposing there was hidden independent features describing the label repeated in previously seen cases of dataset.
For example let's consider we want to train a model to movement time series signals so it can predict some sort of activities (labels), and we have long record of movement signals (e.g. hours) for limited number of persons (e.g. 30) during various type of activities (e.g. 5), we may say these signals carry three type of hidden features:

Noise-features: Common features between every persons/activities 
Case-features: features mostly correlated with persons
Class-features: features mostly correlated with activities

We want to train the model such it learn mostly Class-features and eliminate 1st and 2nd types of features. 
In conventional types of supervised-learning CNN learns all features how dataset represents them. In my test the model learned those 30 person activities very well, but on new persons it only predict randomly (i.e. 20% success). Over-fitted?
It seems there are three straight workaround to this:

Extracting class-features and using a shallow classifier.
Increasing dataset wideness by recording signal on other persons: it can get so expensive or impossible in some situations. 
Signal augmentation: by augmenting signals such it does not change Class-features, and making augmented Case-features. it seems to me harder than 1st workaround. 

Is there any other workaround on this type of problem?
For example specific type of training to use, to learn the model how different cases similarly follow class-features during class changes, eliminating case-features which varies case by case.
Sorry for very long question!
","['convolutional-neural-networks', 'training', 'datasets', 'overfitting']",
GPFlow: Gaussian Process Uncertainty Quantification,"
I trained some Gaussian process model with the Python library GPFlow on a dataset consisting of $(X, Y)$, inputs and outputs, in a regression setting. This model gives me pretty good predictions in the sense that the relative error is small almost everywhere. I want to use the uncertainty as well, which is given in a GPFlow setting in the form of a standard deviation (STD) associated with every prediction. Here's my problem: I normalised both inputs and outputs before training (separately) using sklearn's StandardScaler (effectively making the data normally distributed with $0$ mean and unit STD). So the STD given by the model pertains to the scaled data. How do I ""rescale"" the uncertainty estimates of the GP to the actual data? Using the inverse_transform function of the output scaler makes little sense. This issue might be easier solvable if I scaled with a MinMaxScaler (squishing all data points into the unit interval) by dividing by the length of the range of the original output set (at least I think it works that way). But how about the case of the StandardScaler? Any insights will be appreciated!
","['machine-learning', 'python']",
What are the aspects that most impact on the inference time for neural networks in embedded systems?,"
I work with neural networks for real-time image processing on embedded softwares and I tested different architectures (Googlenet, Mobilenet, Resnet, custom networks...) and different hardware solutions (boards, processors, AI accelerators...). I noticed that the performance of the system, in terms of inference time, does not depend only on the processor but also on other factors. 
For example, I have two boards from different manifacturers, B1 (with a cheap processor) and B2 (with a better processor), and two neural networks, N1 (very light with regular convolutions and fully connected layers) and N2 (very large, with inception modules and many layers). The inference time for N1 is better on B1, while for N2 it is better on N2. Moreover, it happens that, as the software is executed, the inference time changes over time.
So my question is: in an embedded system, what are the aspects that impact on the inference time, and how? I am interested not only in the hardware features but also in the neural network architecture (convolutional filter size, types of layers and so on).
","['neural-networks', 'hardware', 'performance', 'embedded-design']",
How do probabilistic graphical models factor into modern machine learning?,"
I just finished the three-part series of Probabilistic Graphical Models courses from Stanford over on Coursera. I got into them because I realized there is a certain class of problem for which the standard supervised learning approaches don't apply, for which graph search algorithms don't work, problems that don't look like RL control problems, that don't even exactly look like the kind of clustering I came to call ""unsupervised learning"".
In my AI courses in the Institute, we talked briefly about Bayes Nets, but it was almost as if professors considered that preamble to hotter topics like Neural Nets. Meanwhile, I heard about ""Expectation Maximization"" and ""Inference"" and ""Maximum Likelihood Estimation"" all the time, like I was supposed to know what they were talking about. It frustrated me not to be able to remember statistics well enough to feel these things, so I decided to fill the hole by delving deeper into PGMs.
Throughout, Koller gives examples of how to apply PGMs to things like image segmentation and speech recognition, examples that seem completely dated now because we have CNNs and LSTMs, even deep nets that encode notions of uncertainty about their beliefs.
I gather PGMs are good when:

You know the structure of the problem and can encode domain knowledge that way.

You need a generative model.

You want to learn more than just one $X \rightarrow Y$ mapping, when you instead need a more general-purpose model that can be queried from several sides to answer different kinds of questions.

You want to feed the model inputs that look more like probability distributions than like samples.


What else are they good for?
Here are a few more related questions.

Have they not been outstripped by more advanced methods for lots of problems now?

In which domains or for which specific kinds of problem are they still the preferred approach?

How are they complementary to modern advanced methods?


","['machine-learning', 'applications', 'probabilistic-graphical-models']",
My Gaussian Naive Bayes classifier is too slow,"
I am trying to build a film review classifier where I determine if a given review is positive or negative (w/ Python). I'm trying to avoid any other ML libraries so that I can better understand the processes. Here is my approach and the problems that I am facing:

I mine thousands of film reviews as training sets and classify them as positive or negative.
I parse through my training set and for each class, I build an array of unique words.
For each document, I build a vector of TF-IDF values where the vector size is my number of unique words.
I use a Gaussian classifier to determine: $$P(C_i|w)=P(C_i)P(w|C)=P(C_i)*\dfrac{1}{\sqrt{2\pi}\sigma_i}e^{-(1/2)(w-\mu_i)^T\sigma_i^{-1}(w-\mu_i)}$$ where $w$ is the my document in a vector, $C_i$ is a particular class, $\mu_i$ is the mean vector and $\sigma_i$ is my covariance matrix.

This approach seems to make sense. My problem is that my algorithm is much too slow. As an example, I have sampled over 1,500 documents and I have determined over 40,000 unique words. This means that each of my document vectors has 40,000 entries and if I were to build a covariance matrix, it would have dimensions 40,000 by 40,000. Even I were able to generate the entirety of $\sigma_i$, but then I would have to compute the matrix product in the exponent, which will take an extraordinarily long time just to classify one document.
I have experimented with a multinomial approach, which is working well. I am very curious about how to make this work more efficiently. I realise the matrix multiplication runtime can't be improved, and I was hoping for insight on how others are able to do this.
Some things I have tried:

Filtered any stop words (but this still leaves me with tens of thousands of words)
Estimated $\sigma_i$ by summing over a couple of documents.

","['python', 'naive-bayes']",
Which neuron represents which part of the input?,"
In a neural network, each neuron represents some part of the input. For example, in the case of a MNIST digit, consider the stem of the number 9. Each neuron in the NN represents some part of this digit.

What determines which neuron will represent which part of the digit?
Is it possible that if we pass in the same input multiple times, each neuron can represent different parts of the digit?
How is this related to the back-propagation algorithm and chain rule? Is it the case that, before training the neural network, each neuron doesn't really represent anything of the input, and, as training proceeds, neurons start to represent some part of the input?

","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'backpropagation', 'features']",
Input for the Env.step() in the 'Pendulum-v0' environment,"
I want to customize the 'Pendulum-v0' environment such that the action (the torque) from previous time step as well as from the current timestep serve as the inputs in the Env.step() function.
My problem statement is that I want to generate torque from the controller which has a white Gaussian noise of magnitude 1 and then filter it with the torque generated in the previous timestep as follows:
tor_ = tor_c + a*WGN ;
tor(t) = lambda*tor_ + (1-lambda)*tor(t-1) ;
https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py#L37
We can see that the 'u' is an array of some numbers as the input which is afterward clipped from -max_torque to max_torque and then only the first element is taken as the torque value to calculate the states and the reward function for the given time-step.
My question is what does the value of the other elements signify? Are they
torque values from the previous time steps or is it that the length of the 'u' array is just 1 and its value is restricted between -max_torque to max_torque?
In conclusion, I just wanna access the action (the torque value) from the previous time-step. Is it possible ? If yes, how?
",['reinforcement-learning'],
Why isn't the ElliotSig activation function widely used?,"
The Softsign (a.k.a. ElliotSig) activation function is really simple: 
$$ f(x) = \frac{x}{1+|x|} $$
It is bounded $[-1,1]$, has a first derivative, it is monotonic, and it is computationally extremely simple (easy for, e.g., a GPU).
Why it is not widely used in neural networks? Is it because it is not infinitely derivable?
","['machine-learning', 'activation-functions', 'performance']","I can't speak for individual researchers, but I can guess why the community as a whole hasn't adopted this activation function.ReLU is just so incredibly cheap. This benefit continues to grow as networks grow deeper. Also, they work reasonably well. As pointed out in Searching for Activation Functions, the performance improvements of the other activation functions tend to be inconsistent across different models and datasets.Even if a new activation function did provide a meager improvement in performance across the board, I wouldn't be surprised if ReLU were still commonly used. It's the default for a lot of machine learning software already.Also research isn't ordinarily about eeking out one more percentage point in accuracy on a specific task. If I were entering in a competition, I might experiment with activation functions. But even then I'd rather use ReLU and save a little time while prototyping my architecture.As pointed out by @DuttaA in comments, softsign could potentially replace sigmoid and tanh in situations where a bounded output is desired. I haven't seen anyone compare them before, but softsign would at least be much faster. I'm guessing this replacement hasn't happened because of tradition and exposure. Not because of softsign's lack of infinite derivatives. I don't know if this happening would make softsign ""widely used"", but it would be something."
Game-based or nasty chatbot for Facebook Messenger or Skype,"
Are there chatbots for Facebook Messenger or Skype available which are game-based, i.e. with which it is possible to play a short funny game? It should be possible to play the game for at least 10 minutes in sequence and it should be writing based and not based on clicking at boxes. That means the agent should be pretty clever, like Microsoft Zo bot, but instead of conducting random smalltalk a game should be played.
Second, are there bots for Facebook Messenger or Skype available which are nasty and unfriendly, i.e. which are offending?
Thank you a lot in advance. I'm thankful for any help.
","['game-ai', 'intelligent-agent', 'chat-bots']",
Sample from a distribution inside a NN layer,"
Is it possible to sample from a distribution inside a neural network forward function? Assume that there is a NN and a sample is needed to be derived from it at every forward-pass to randomly set a layer-specific hyper-parameter.
Is this operation differentiable
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
How to create meaningful multiple object detection evaluation comparison graph?,"
I have got multi-class object detector. One model accuracy evaluation of detection consists of: mAP, FP, FN, TP for each class divided to two graphs and looks like this (I've used this repo for evaluation).

Now, I've got many of these evaluations (multiple times these two graphs for different models) and I would like to easily compare all these trained models (results) and put them to one graph.
I've searched through the whole Internet, but wasn't able to find suitable method of placing all the values to one graph. Also, the values of these three classes can be put together (eg. result mAP for this evaluation would be (75+ 68+ 66) / 3 = ~70%), so I would have just single value of each mAP, FN, FP, TP for one whole model evaluation.
What comes to my mind is the following graph (or maybe some kind of plot):

Note: It may not make sense to place mAP together with TP, etc. into one graph, but I would like to have all these values together to easily compare all the model evaluations. Also I am not really looking for a script, I can do the graph manually from values, but script would be more helpful. What really matters is, how to create meaningful graph with all the data :). If the post is more suitable for different kind of site, please, let me know.
","['neural-networks', 'graphs']",
Reduce receptive field size of CNN while keeping its capacity?,"
I have a convolutional encoder (a CNN) consisting of DenseBlocks and a total of 50 layers (cf. FC-DenseNet103). The receptive field of the encoder (after last layer) is 660 according to Tensorflow function compute_receptive_field_from_graph_def(..)) whereas the input image is 64x64 pixels. Obviously the receptive field is way too big. 
How can the receptive field be reduced to say 46 but the capacity of the encoder be more or less kept at the same level? By capacity I simply mean the number of parameters of the model. The capacity requirement is justified due to the complex dataset to be processed.
Using less layers or smaller kernels reduces the receptive field size but also the capacity. Should I then just increase the number of filters in the remaining layers in order to keep the capacity?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks']",
Which field to study to learn & create a.i generated simulations?,"
I wasn't sure how to title this question so pardon me please.
You may have seen at least one video of those ""INSANE A.I created simulation of {X} doing {Y & Z} like the following ones:
A.I learns how to play Mario
A.I swaps faces of {insert celebrity} in this video after 16hrs.
etc...
I want to know what I have to learn to be able to create for example a program that takes xyz-K images of a person as training data and changes it with another person's face in a video.
Or create a program that on a basic level creates a simulation of 2 objects orbiting /attracting each other /colliding like this:

What field/topic is that?
 I suspect deep learning but I'm not sure. I'm currently learning machine learning with Python.
I'm struggling because linear regression & finances /stock value prediction is really not interesting compared to teaching objects in games to do archive something or create a program that tries to read characters from images.
","['neural-networks', 'machine-learning', 'deep-learning', 'data-science', 'ai-field']","You need to define ""simulation"" more specific. Playing Mario, Swapping face on image/video, or generating simulation of objects that are orbiting use different techniques.Playing Mario or ""AI that playing game"": the AI agent trained on available environment (Mario game, so the environment is not generated) and learn the best sequential actions to achieve the goal. It runs the game thousand times, when it did a wrong action then it gets ""penalties"" that improve its knowledge. The algorithm that can be used is Reinforcement Learning, but some earlier paper use Genetic Algorithm to generate the best actionFace swap: It's close to computer vision area, some methods that I know use Style Transfer principle (Convolutional Neural Network) to make transformation of face of one image to another image. You can read the basic of style transfer here.Generating physical movement: I don't know too much about this topic but I know there are some papers talk about this, Fluid Net from Google workers and this paper from TU Munchen. At a glance they also use CNN to improve the result but the main simulation came from Euler Fluid Equation. So if you need to generate object that orbiting, I think you need to find equations that models that movement.Hope it helps!"
How sigmoid funtion helps us in reducing error in neural networks?,"
As in sigmoid function when input x is very large or very small the curve is flat that means low gradient descent but when it is in between the slope is more so,
My question is how this thing helps us in neural network.
",['neural-networks'],
"If we collected a very large labelled dataset from multiple sensors, then train a neural network with that data, could that lead to an AGI?","
I've come up with an idea on how we could use a combination of Deep Learning and body sensors to create a walking talking living humanoid. Here goes:
First, we will recruit 1 billion people and have them wear a special full face mask and suit. This suit will contain touch sensors along the skin, cameras, smell sensors, taste sensors on the mask, basically every data and information that a human receives will be collected electronically, whether it is what they see, what they smell what they feel and so on.
These suits will also have potentiometers and other sensors to measure the movement made by the person. Every hand movement, leg movement, muscle movement will be recorded and saved in a database as well.
After 50 years or so, all collected input and output data from every single person who participated in this experiment will be saved in a computer. We then create a neural network and then train it on the input and output data from the database.
Next, we create a robot that has motorized muscles and hand/leg joints that are the same specs as to our previous suits and also the touch, smell, sight and other sensors integrated inside of this robot.
Once everything is ready, we will load the trained neural network onto the robot and switch it on. During inference, the neural network will take data from sensors all over the robot's body and translate it into movement in legs, hands, and muscles.
Could these techniques in conjunction with the data collection I describe, produce an AGI?
Essentially, how feasible is current technology to produce a robot that will behave, speak, live, move like a normal human being?
","['neural-networks', 'agi', 'supervised-learning']",
Is the expert system still in use today?,"
In my country, the Expert System class is mandatory, if you want to take the AI specialization in most universities. In class, I learned how to make a rule-based system, forward chaining, backward chaining, Prolog, etc.
However, I have read somewhere on the web that expert systems are no longer used.
Is that true? If yes, why? If not, where are they being used? With the rise of machine learning, they may not be as used as before, but is there any industry or company that still uses them today?
Please, provide some references to support your claims.
","['reference-request', 'applications', 'symbolic-ai', 'expert-systems', 'rule-based-systems']","I would say Expert Systems is still being taught. For instance, if you look at some of the open courses like MIT's, there are still lectures on it.Also, looking at the CLIPS documentation, you will find a couple of examples of usage from 2005. What I suspect is that Expert Systems are now embedded with ""normal systems"" in practice. Hence it may be difficult to distinguish from systems used on a daily basis for diagnostics, etc. and not as popular as before."
Can TD($\lambda$) be used with deep reinforcement learning?,"
TD lambda is a way to interpolate between TD(0) - bootstrapping over a single step, and, TD(max), bootstrapping over the entire episode length, or, Monte Carlo.
Reading the link above, I see that an eligibility trace is kept for each state in order to calculate its ""contribution to the future"".
But, if we use an approximator, and not a table for state-values, then can we still use eligibility traces? If so, how would the loss (and thus the gradients) be calculated? Specifically, I would like to use actor-critic (or advantage actor-critic).
","['reinforcement-learning', 'deep-rl', 'temporal-difference-methods', 'eligibility-traces', 'td-lambda']","Eligibility traces is a method of weighting between temporal-difference ""targets"" and Monte-Carlo ""returns"". In practice, for example, instead of using the one-step TD target, $r_t + \gamma V (s_{t+1})$, as in the temporal difference update $V (s_t) \leftarrow V (s_t) + \alpha (r_t + \gamma V (s_{t+1}) − V (s_t))$, you use the so-called ""lambda"" ($\lambda$) target, which is a target that balances between the TD target and the Monte Carlo return. So, in practice and intuitively, eligibility traces is just a way of using a more ""appropriate"" target while learning. In general, you need to perform these updates (e.g., the TD update above) ""online"", i.e. while you explore or exploit the environment.In theory, you could use a deep neural network to represent your value function (or your policy), while using eligibility traces. It would be similar to not using them: you would just use a different target.However, deep RL (that is, RL which uses deep neural networks to represent e.g. value functions) training needs to be performed using i.i.d. data, in order to prevent overfitting, which often means that they can't be trained online or need to use ""tricks"" like the ""experience replay"" (used in the paper Human-level control through deep reinforcement learning). Note that, in RL, successive states are often very correlated (e.g. two successive frames of a video would be very correlated).In theory and similarly, you would still be able to use eligibility traces with the actor-critic method, but not with the asynchronous advantage actor-critic method. See the section 2.3 of the paper ""Efficient Eligibility Traces for Deep Reinforcement Learning"" (2018) by Brett Daley and Christopher Amato, for more info.In this same paper, an approach is introduced to efficiently combine eligibility traces with deep neural networks. The authors propose DQN($\lambda$), which is the DQN architecture combined with eligibility traces, where the $\lambda$ return is computed in an ""efficient"" (and recursive) way, instead of the ""usual"" way. Since they use a DQN, they also use an ""experience replay"" buffer (or memory), where they also store the efficiently computed $\lambda$ target (in addition to the usual rewards). Furthermore, they also eliminate the need for the ""target"" network used in the standard DQN. You can have a look at algorithm 1 of the same paper to see how they improve the parameters of the network, which represents the Q function, in the case of the DQN($\lambda$) model. See the section 3.1 of the same paper for more details regarding this model.They also introduce A3C($\lambda$), which combines asynchronous advantage actor-critic (A3C) with eligibility traces. See the section 3.2 for more details.Note that there have been other proposals for combining eligibility traces with deep learning. You can have a look at the literature."
Which Openai Gym environment should I use to test a Temporal Difference RL Agent?,"
Up to now, I have been using (my version of) open AI's code, with the suggested CartPole.
I have been using Monte Carlo methods, which, for cartpole, seemed to work fine.
Trying to move to temporal difference, Cartpole seems to fail to learn (with simple TD method) (or I stopped it too soon, but still unacceptable).
I assume that is the case because in Cartpole, for every timestamp, we get a reward of 1, which has very little immediate information about weather or not the action was good or not.
Which gym environment is the simplest that would probably work with TD learning?
by simplest I mean that there is no need for a large NN to solve it. No conv nets, no RNNS. just a few small layers of a fully connected NN, just like in cartpole. something I can train on my home cpu, just to see it starting to converge.
","['machine-learning', 'reinforcement-learning', 'open-ai']",
What is the definition of a heuristic function in the BayesChess paper?,"
I am reading BayesChess: A computer chess program based on Bayesian networks (Fernandez, Salmeron; 2008)
It is a chess-playing engine using Bayesian networks. The following is mentioned about the heuristic function in section 3.

Here the heuristic is defined in terms of 838 parameters.
There are 5 parameters indicating the value of each piece (pawn,
queen, rook, knight, and bishop -the king is not evaluated, as it must
always be on the board), 1 parameter for controlling whether the king
is under check, 64 parameters for evaluating the location of each
piece on each square on the board (i.e., a total of 786 parameters,
corresponding to 64 squares × 6 pieces each colour × 2 colours) and
finally 64 more parameters that are used to evaluate the position of
the king on the board during the endgame.

The above sentence contains the parameters used by the heuristic function. But I didn't find the actual definition. What is the actual definition of the heuristic function?
","['definitions', 'papers', 'heuristics', 'chess', 'bayesian-networks']",
When are Q values calculated in experience replay?,"
In experience replay, the update rule follows the loss:

$$
L_i(\theta_i) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim U(D)} \left[ \left(r_t + \gamma \max_{a_{t+1}} Q(s_{t+1}, a_{t+1}; \theta_i^-) - Q(s_t, a_t; \theta_i)\right)^2 \right]
$$

I can't get my head around the order of calculation of the terms in that equation:
An experience element is

$(s_t, a_t, r_t, s_{t+1} )$

where

$s_t$ is the state at time $t$
$a_t$ is the action taken from $s_t$ at time $t$
$r_t$ is the reward received by taking that action from $s_t$ at time
$t$
$s_{t+1}$ is the next state

In the on policy case, as I understand it, Q of the equation above is the same Q, which is the only approximator.
As I understand the algorithm, at time $t$ we save an experience

$(s_t, a_t, r_t, s_{t+1} )$.

Then, later, at time $t+x$, we attempt to learn from that experience.
However, at the time of saving the experience, $Q(s_t,a_t)$ was something different than at the time of attempting to learn from that experience, because the parameters $\theta$ of $Q$ have since changed. This could actually be written as

$Q_t(s_t, a_t) \neq Q_{t+x}(s_t,a_t)$

Because the Q value is different, I don't see how the reward signal at time $t$ is of any relevance for $Q_{t+x}(s_t,a_t)$ at $t+x$, the time of learning.
Also, it is likely that following a policy that is derived from $Q_t$ would lead to $a_{t}$, whereas following a policy that is derived from $Q_{t+x}$ would not.
I don't see in the experience replay algorithm that the Q value $Q_t(s_t, a_t)$ is saved, so I must assume that is not.
Why does calculating the Q value again at a later time make sense FOR THE SAME SAVED REWARD AND ACTION?
","['machine-learning', 'reinforcement-learning', 'q-learning', 'experience-replay']","Because the Q value is different, I don't see how the reward signal at time $t$ is of any relevance for $Q_{t+x}(s_t,a_t)$ at $t+x$, the time of learning.The $r_t$ value for any single step is not dependent on $Q$ or the current policy. It is purely dependent on $(s_t,a_t)$. That means you can use the Q update equation to use it to calculate new TD targets, by combining your knowledge of the old reward signal and the value functions of the latest policy.The value $r_t + \gamma \max_{a_{t+1}} Q(s_{t+1}, a_{t+1}; \theta_i^-)$ is the TD target in the loss function you show. There are many possible variations to calculate the TD target value in RL, with different properties. The one you are using is biased towards initial random values unrelated to the problem, and also deliberately biased towards older Q values to prevent runaway feedback. It is also low variance compared to other possibilities, and simpler to calculate.Also, it is likely that following a policy which is derived from $Q_t$ would lead to $a_{t}$, whereas following a policy which is derived from $Q_{t+x}$ would not. Yes that is correct. However, you don't care about that for a single step, you just care about calculating a better estimate for $Q(s,a)$ regardless of whether you would choose $a$ in state $s$ with the current policy. That is what an action value measures - the utility of taking action $a$ in state $s$ and thereafter following a given policy.This is a strength of experience replay, that you are constantly refining your estimates of off-policy action values to determine which is the best.This does become more of an issue when you want to use longer trajectories in the update step (which you may want to do to reduce bias of your TD target estimate). A series of steps in history $s_t,a_t,r_t ... s_{t+1}, a_{t+1},r_{t+1}...s_{t+n}$ may not have the same chance of occurring under the latest policy as it did when it was stored in memory. For the first step $s_t,a_t$ again you don't care if it is one you would currently take because the point is to refine your estimate of that action value. However, if you want to use $r_{t+1}, r_{t+2}$ etc plus $s_{t+n}$ to create a TD target, then you have to care whether your current policy and the one used to populate the history table would be different.It is a problem if you want to use more sophisticated estimates of TD target that use multiple sample steps along with experience replay. There are some approaches you can take to allow for this, such as importance sampling. For a single step update mechanism you don't need to worry about it.I don't see in the experience replay algorithm that the Q value $Q_t(s_t, a_t)$ is saved, so I must assume that is is not.This is correct. You must re-calculate the TD target from a more up-to-date policy to get better estimates of the action value. With experience replay, you are not interested in collecting a history of values of Q. Instead you are interested in the history of state transitions and rewards.Why does calculating the Q value again at a later time make sense FOR THE SAME SAVED REWARD AND ACTION?Because it will change, due to the learning process summarising effects of state transitions.As a toy example, consider a maze solver with traps (negative rewards) and treasures (positive rewards). At one point in history, the agent finds itself in a location and its policy told it to move into a trap on the next step. The agent would initially score location and steps leading up to it with negative Q values. Later it discovers through exploration that there is also some treasure if it takes a different turning towards the end of the the same series of steps. With experience replay, and re-calculating Q values each time, it can figure out which section of that path should be converted to high scores because they lead to treasure as well as the trap and now the agent has a better policy for the end of the path it has better estimates of value there."
How to use SLAM on other sensor other than camera?,"
I have a sensor that reads electromagnetic field strength from each position.
And the field is stable and unique for each position. So the reading is simply a function of the position like this: reading = emf(x,y,z)
The reading consists of 3 numbers (not position).
I want to find the inverse function of emf function. This means I want to find function pos that is defined like this: x,y,z = pos(reading)
I don't have access to both emf and pos function. I think that I want to gradually estimate the pos function using a neural network.
So, I have the input reading and acceleration ax,ay,az of the sensor through space from an IMU. The acceleration is not so accurate. I want to use these 2 inputs to help me figure out the position of the sensor over time. You can assume that the starting position is at 0,0,0 on the first reading.
In short, input is reading and ax,ay,az on each timestep, the output will be an adjustment on the weights of pos function or output will be position directly.
I've been reading about SLAM (simultaneous localization and mapping) algorithm and I think that it might help in my case because my problem is probabilistic. If I know accurately the acceleration, I would not need any probability, but the acceleration is not accurate.
So, I want to know how do I model this problem in terms of SLAM?
I don't have a camera to do vision-based SLAM though.
Why do I think this is tractable?
If the first reading is 1,1,1 and the position is at origin 0,0,0, and I move the sensor, the position can drift because the sensor has never seen other reading before, but after I go back to the origin, the reading will be 1,1,1 again so the sensor should report the origin 0,0,0 as output. During the movement of the sensor, the algorithm should filter the acceleration so that all the previous positions make sense.
","['machine-learning', 'deep-learning', 'slam']",
predict customer visit,"
Suppose we have a data set consists of columns

TransactionId, CardNo, TransactionDate

then how can we calculate the customer purchase interval (means if customer A purchased on Jan 1st and after 10 days he again purchased, and then he again purchased after 15 days.) and how to predict the next visit of customer A by analyzing the purchasing intervals of customer A.
Any help will be appreciated.
","['python', 'forecasting']",
How does DeepMind perform reinforcement learning on a TPU?,"
I've watched this video of the recent contest of AlphaStar Vs Pro players of StarCraft2, and during the discussion David Silver of DeepMind said that they train AlphaStar on TPUs.
My question is, how is it possible to utilise a GPU or TPU for reinforcement learning when the agent would need to interact with an environment, in this case is the StarCraft game engine?
At the moment with my training of a RL agent I need to run it on my CPU, but obviously I'd love to utilise the GPU to speed it up. Does anyone know how they did it?
Here's the part where they talk about it, if anyone is interested:
https://www.youtube.com/watch?v=cUTMhmVh1qs&t=7030s
","['machine-learning', 'reinforcement-learning', 'deepmind']",
What role the activation function plays in the forward pass and how it is different from backpropagation [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Is the role played by activation function significant only during the training of neural network or they play their role during testing (after training we supply data for prediction) the network.
I understand that a linear line cannot separate data scattered in complex manner but
Then why we don't used simple polynomials.
why specifically sigmoid, or tanh or ReLu what exactly they are doing ?
What Activation functions do when we are supplying data during training and 
And when we supply test data once we have trained the network and we input test data for prediction?
",['neural-networks'],
Tips for keeping the distribution of weights normal,"
I am working on a project where the Neural Network weights must be quantized on 8 or 16 bits for an embedded platform, thus I will lose some precision.
Since our platform does not have floating point arithmetic we need to quantize the weights. By quantizing i mean taking the max absolute value of the weights and divide it by the maximum signed number representable on 8 or 16 bits. this operation will give us a quantization factor $(qf)$.
the final quantized weights will be integer$(value * qf)$.
If my weights are very sparse and have a very bad distribution, I lose more precision.
For example, to the left here is the distribution of weights for one layer, and to the right is the distribution of weights after I added to the loss function the Kurtosis and skew measures of the weights, and it improved a bit the shape of the distribution while keeping the same accuracy, even a bit higher.
Does anybody have any other suggestions? Has anyone tackled this problem before?
","['neural-networks', 'hardware']",
What information should be cached in experience replay for actor-critic?,"
Experience replay is buffer (or a ""memory"") of transactions $e_t = (s_t, a_t, r_t, s_{t+1})$.
The equations for calculating the loss in actor critic are an 
actor loss (parameterized by $\theta$) $$log[\pi_\theta(s_t,a_t)]Q_w(s_t,a_t)$$ and a critic loss (parameterized by $w$) $$r(s_t,a_t) + \gamma Q_w(s_{t+1},a_{t+1}) - Q_w(s_{t},a_{t}).$$
As I see it, there are two more elements that need to be saved for later use:

The expected Q value at the time $t$: $Q_w(s_{t},a_{t})$
The log probability for action $a_t$: $log[\pi_\theta(s_t,a_t)]$

If they are not saved, how will we be able to later calculate the loss for learning? I didn't see anywhere stating to save those, and I must be missing something.
Do these elements need to be saved or not?
","['machine-learning', 'reinforcement-learning', 'policy-gradients', 'experience-replay']","The loss function is estimated in every batch training cycle. Gradients of the loss are computed and propagation back to the network happens in every cycle. This means that you use a small batch (e.g. 100 instances) from the replay memory, and by having the states you can input them to the respective network and have the $Q(s)$ for every state in your batch. Then you estimate the loss and run backpropagation and the networks' weights are getting updated. You continue gathering experience by interacting with the environment and after a threshold that you specify you repeat the cycle by re-sampling a new batch from the memory.Just a suggestion: Start moving towards Asynchronous/synchronous methods for RL and use one network with different ""heads"" for Actor and Critic. Then you use one loss function plus the experience that now is collected from multiple instances of your agent-environment interaction (in contrast to your description in which one agent collects and stores experience from one instance)."
Why is this PyTorch implementation of the actor-critic algorithm inconsistent with the mathematical formulas?,"
This PyTorch implementation of the actor-critic algorithm calculates the losses like so:
actor_loss = -log_prob * discounted_reward
policy_loss = F.smooth_l1_loss(value, torch.tensor([discounted_reward]))

Both are different from the regular formulas which are, in the case of the actor loss (parameterized by $\theta$):
$$log[\pi_\theta(s_t,a_t)]Q_w(s_t,a_t)$$
and, in the case of the critic loss (parameterized by $w$):
$$r(s_t,a_t) + \gamma Q_w(s_{t+1},a_{t+1}) - Q_w(s_{t},a_{t}),$$
where $r(s_t,a_t)$ is the immediate reward following taking the action.
For the actor, ""the immediate critic evaluation of the transition"" was replaced with ""the discounted reward"". For the critic, the discounted evaluation of the value from the next state $r(s_t,a_t) + \gamma Q_w(s_{t+1},a_{t+1})$ was replaced by ""the discounted reward"". The $L_1$ loss is then calculated, effectively discarding the sign of the (equation) loss.
Questions:

Why did they make these changes?

Why is the sign discarded for the critic loss?


","['reinforcement-learning', 'objective-functions', 'pytorch', 'actor-critic-methods', 'implementation']",
How to deal with the terminal state in SARSA in a multi-agent setting?,"
I'm training a SARSA agent to update a Q function, but I'm confused about how you handle the final state. In this case, when the game ends and there is no $S'$.
For example, the agent performed an action based on the state $S$, and, because of that, the agent won or lost and there is no $S'$ to transition to.
So, how do you update the Q function with the very last reward in that scenario, given that the state hasn't actually changed? That case $S'$ would equal $S$ even though an action was performed and the agent received a reward (they ultimately won or lost, so quite important update to make!).
Do I add extra inputs to the state ""agent won"" and ""game finished"", and that's the difference between $S$ and $S'$ for the final Q update?
To make clear, this is in reference to a multi-agent/player system.  So, the final action the agent takes could have a cost/reward associated with it, but the subsequent actions other agents then take could further determine a greater gain or loss for this agent and whether it wins or loses. So, the final state and chosen action, in effect, could generate different rewards without the agent taking further actions.
","['reinforcement-learning', 'implementation', 'multi-agent-systems', 'sarsa']","The SARSA update rule looks like:$$Q(S, A) \gets Q(S, A) + \alpha \left[ R + \gamma Q(S', A') \right].$$Very similar, the $Q$-learning update rule looks like:$$Q(S, A) \gets Q(S, A) + \alpha \left[ R + \gamma \max_{A'} Q(S', A') \right].$$Both of these update rules are formulated for single-agent Markov Decision Processes. Sometimes you can make them work reasonably ok in multi-agent settings, but it is crucial to remember that these update rules should still always be implemented ""from the perspective"" of a single learning agent, who is oblivious to the presence of other agents and pretends them to be a part of the environment.What this means is that the states $S$ and $S'$ that you provide in update rules really must both be states in which the learning agent is allowed to make the next move (with the exception being that $S'$ is permitted to be a terminal game state.So, suppose that you have three subsequent states $S_1$, $S_2$, and $S_3$, where the learning agent gets to select actions in states $S_1$ and $S_3$, and the opponent gets to select an action in state $S_2$. In the update rule, you should completely ignore $S_2$. This means that you should take $S = S_1$, and $S' = S_3$.Following the reasoning I described above literally may indeed lead to a tricky situation with rewards from transitioning into terminal states, since technically every episode there will be only one agent that directly causes the transition into a terminal state. This issue (plus also some of my explanation above being repeated) is discussed in the ""How to see terminal reward in self-play reinforcement learning?"" question on this site."
Deep Ranking/Best way to classify book covers?,"
I recently came across a paper on Deep Ranking. I was wondering whether this could be used to classify book covers as book titles. (For example, if I had a picture for the cover of the second HP book, the classifier would return Harry Potter and the Chamber of Secrets.)
For example, say I have a dataset of book covers along with the book titles in text. Could that data set be used for this deep ranking algorithm, or is there a much better way to approach my problem? I'm quite new to this whole thing, and this one of my first projects in this field.
What I'm trying to create is a mobile app where people can take a picture of a book cover, have an algorithm/neural net classify the title of the book, and then have some other algorithm connect that to the book's Goodreads page.
Thanks for the help!
",['image-recognition'],
Using GAN's to generate dataset for CNN training,"
I'm doing bachaleor thesis on traffic sign detection using single shot detector called YOLO. These single shot detectors can perform detection of objects in image and so they have specific way of training, ie. training on full images. Thats quite problem for me, because the biggest real dataset with full traffic sign images is Belgian one with 9000 images in 210 classes, which is unfortunately not enough to train good detector.
To overcome this problem, I've created DatasetGenerator, which does quite good job in generating synthetic datasets, you can see in the results directory.
Recently I came across GAN's which can (besides others) generate or extend existing dataset and I would like to use these networks to compare with my dataset generator. I've tried this introduction to GANs succesfully.
The problem is it's unsupervised learning and so there are no annotations. It means it's able to extend my dataset of traffic signs, but the generated dataset won't be annotated at all, which is problem.
So my question is: Is there any way how to use GAN's to extend my dataset of full traffic sign images with annotations of traffic sign class and position? Actually the class is not important, because I can do it separately for each class, but what matters is the position of traffic sign in generated image.
","['convolutional-neural-networks', 'datasets', 'generative-adversarial-networks']","I think you'll enjoy this work from Apple on improving the realism of synthetic images.  Essentially what you need to do is generate a synthetic image then have your GAN modify the synthetic image so that a 1) a discriminator thinks it is real while also 2) not changing the gross structure of the image very much (so the traffic sign doesn't move) - yes, this loss function is going to take a little work!  Making synthetic data realistic enough to allow models to generalize successfully in the real world is a very active and exciting area of research, not least with respect to robotics, and so the work you are doing now should make you very attractive indeed to the right employer."
Can I use deterministic policy gradient methods for stochastic policy learning?,"
Can I treat a stochastic policy (over a finite action space of size $n$) as a deterministic policy (in the set of probability distribution in $\mathbb{R}^n$)? 
It seems to me that nothing is broken by making this mental translation, except that the ""induced environment"" now has to take a stochastic action and spit out the next state, which is not hard using on the original environment. Is this legit? If yes, how does this ""deterministify then DDPG"" approach compare to, for example, A2C?
","['reinforcement-learning', 'policy-gradients', 'ddpg']",
How can VAE have near perfect reconstruction but still output junk when using random noise input,"
I am creating a VAE for time series data using CNNs. The data has 4800 timesteps and 4 features. It is standardized and normalized. The network I am using is implemented in Keras as follows. I have used a MSE reconstruction error:
# network parameters
(_, seq_len, feat_init) = X_train.shape
input_shape = (seq_len, feat_init)
intermediate_dim = 512
batch_size = 128
latent_dim = 10
epochs = 10
img_chns = 3
filters = 32
num_conv = (2, 2)
epsilon_std = 1

inputs = Input(shape=input_shape)
conv1 = Conv1D(16, 3, 2, padding='same', activation = 'relu', data_format = 'channels_last')(inputs)
conv2 = Conv1D(32, 2, 2, padding='same', activation = 'relu', data_format = 'channels_last')(conv1)
conv3 = Conv1D(64, 2, 2, padding='same', activation = 'relu', data_format = 'channels_last')(conv2)
flat = Flatten()(conv3)
hidden = Dense(intermediate_dim, activation='relu')(flat)
z_mean = Dense(latent_dim, name = 'z_mean')(hidden)
z_log_var = Dense(latent_dim, name = 'z_log_var')(hidden)

def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),
                              mean=0., stddev=epsilon_std)
    return z_mean + K.exp(z_log_var) * epsilon

z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])

decoder_hid = Dense(intermediate_dim, activation='relu')(z)
decoder_upsample = Dense(38400, activation='relu')(decoder_hid)
decoder_reshape = Reshape((600,64))(decoder_upsample)

deconv1 = Conv1D(filters=32, kernel_size=2, strides=1,
             activation=""relu"", padding='same', name='conv-decode1')(decoder_reshape)
upsample1 = UpSampling1D(size=2, name='upsampling1')(deconv1)
deconv2 = Conv1D(filters=16, kernel_size=2, strides=1,
             activation=""relu"", padding='same', name='conv-decode2')(upsample1)
upsample2 = UpSampling1D(size=2, name='upsampling2')(deconv2)
deconv3 = Conv1D(filters=8
                 , kernel_size=2, strides=1,
             activation=""relu"", padding='same', name='conv-decode3')(upsample2)
upsample3 = UpSampling1D(size=2, name='upsampling3')(deconv3)
x_decoded_mean_squash = Conv1D(filters=4
                 , kernel_size=4, strides=1,
             activation=""relu"", padding='same', name='conv-decode4')(upsample3)

class CustomVariationalLayer(Layer):
    def __init__(self, **kwargs):
        self.is_placeholder = True
        super(CustomVariationalLayer, self).__init__(**kwargs)

    def vae_loss(self, x, x_decoded_mean_squash):
        x = K.flatten(x)
        x_decoded_mean_squash = K.flatten(x_decoded_mean_squash)
        xent_loss = mse(x, x_decoded_mean_squash)
        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
        return K.mean(xent_loss + kl_loss)

    def call(self, inputs):
        x = inputs[0]
        x_decoded_mean_squash = inputs[1]
        loss = self.vae_loss(x, x_decoded_mean_squash)
        self.add_loss(loss, inputs=inputs)
        return x

outputs = CustomVariationalLayer()([inputs, x_decoded_mean_squash])

# entire model
vae = Model(inputs, outputs)
vae.compile(optimizer='adadelta', loss=None)
vae.summary()

I wanted to ask whether it is possible for the network to nearly perfectly reconstruct the test timeseries when passed through the entire VAE network, but still output junk when using a random Normal input. For further details, here is one of the inputs and outputs when passing a test signal through the network.

Here is a reconstruction generated purely from a random sample.

How can this be? Even if there was a posterior collapse, the VAE should still be able to generate a good output sample with a random input. To further test this I decided to split the network into two parts (encoder and decoder), and then pass the test image through it. The encoder and decoder networks were made by simply splitting the trained VAE network as follows:
idx = 9 
input_shape = vae.layers[idx].get_input_shape_at(0)

layer_input = Input(shape=(input_shape[1],)) 

x = layer_input
for layer in vae.layers[idx:-1]:
    x = layer(x)

decoder = Model(layer_input, x)
decoder.summary()

idx = 0
input_shape = vae.layers[idx].get_input_shape_at(0)

layer_input = Input(shape=input_shape)

x = layer_input
for layer in vae.layers[idx + 1:7]:
     x = layer(x)

encoder = Model(layer_input, x)
encoder.summary()

Interestingly, I also got junk output here. I'm not sure how it is possible. If the model itself is getting a near perfect reconstruction, surely just passing an image through the encoder, extracting the latent mean, and then passing that latent mean through the decoder should also create a near perfect image?
Is there something I am missing here?
","['convolutional-neural-networks', 'keras', 'latent-variable']",
How to shape the weights or nodes during gradient training of neural network? Training with constraints?,"
Gradient training changes indiscriminately all the weights and nodes of the neural network. But one can imagine the situations when the training should be shaped, e.g.:

One can put constraints on some of the weights. E.g. human brain contains regions whose inner connections are more dense that external connections with different regions. One can try to mimic this region-shaped structure in neural networks as well and hence one can require that inter-regional weights (in opposit to intra-regional weights) are close to zero (except, possibly, for some channels among regions);
One can put constraints on some of the weights in such manner that some layer of neurons have specific structure. E.g. consider the popular encoder-decoder architecture of neural machine translation e.g. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html We can see that that the whole output of the encoder is expressed as a single layer of neurons which is forwarded to the input of the decoder. So - one can require that the set of all the possible outputs of the encoder (e.g. the possible values of this single layer of the neurons) forms some kind of structure, e.g. some grammar of some interlingua. This example is for illustration only, I have in mind more complex neural network which has one layer of neurons which indeed should output the encoded words of some interlingua. So, one is required to guid all the weights of the encoder in such manner that this single layer has only allowable values.

So - my question is - are there methods that guide the gradient descent training with additional information about the weights or about the nodes (i.e. about the whole subsets of weights that have some impact on specific layer of nodes)? E.g. about methods that impress the region structure on the neural network or that constrains the values of some nodes to be in specific range only?
Of course, it is quite easy to include such constraints in evolutionary neural networks - one can simply reject the neural networks with weights that violates the constraints. But is it possible to do this in gradient-like training?
","['neural-networks', 'training', 'gradient-descent', 'evolutionary-algorithms']",
"Will LMS always be convex function? If yes, then why do we change it for neural networks?","
In LMS(least mean square) since, we use a quadratic error function, and quadratic functions are generally parabola in (some convex like shape). I wonder whether that is the reason why we use least square error metric? If that is not the case(its not ALWAYS convex or reason WHY we use LMS), what is the reason then? why this metric changes for deep learning/neural networks but works for regression problems?
[EDIT]: Will this always be a convex function or is there any possibility that it will not be convex?
","['neural-networks', 'deep-learning', 'gradient-descent', 'linear-regression']","Square loss is fine for regression, since minimizing it is the same as maximizing the likelihood of the model parameters (under assumption that the error is Gaussian). However, if the model directly produces probabilities, then it is natural to use these probabilities directly within the loss. Hence, in all classification models we prefer to minimize negative log-likelihood of the correct class.Note that choosing a natural loss leads to several practical advantages. In particular, applying a quadratic loss after a sigmoid activation leads to very poor gradients when the sigmoid is saturated in the wrong direction. The negative log-likelihood loss has no such problems.This issue is not specific for neural networks. Logistic regression has used the negative log likelihood loss ever since 1958."
"Each training run for DDQN agent takes 2 days, and still ends up with -13 avg score, but OpenAi baseline DQN needs only an hour to converge to +18?","
Status:
For a few weeks now, I have been working on a Double DQN agent for the PongDeterministic-v4 environment, which you can find here.
A single training run lasts for about 7-8 million timesteps (about 7000 episodes) and takes me about 2 days, on Google Collab (K80 Tesla GPU and 13 GB RAM). At first, I thought this was normal because I saw a lot of posts talking about how DQNs take a long time to train for Atari games.
Revelation:
But then after cloning the OpenAI baselines repo, I tried running python -m baselines.run --alg=deepq --env=PongNoFrameskip-v4  and this took about 500 episodes and an hour or 2 to converge to a nice score of +18, without breaking a sweat. Now I'm convinced that I'm doing something terribly wrong but I don't know what exactly.
Investigation:
After going through the DQN baseline code by OpenAI, I was able to note a few differences:

I use the PongDeterministic-v4 environment but they use the PongNoFrameskip-v4 environment 
I thought a larger replay buffer size was important, so I struggled (with the memory optimization) to ensure it was set to 70000 but they set it to a mere 10000, and still got amazing results.
I am using a normal Double DQN, but they seem to be using a Dueling Double DQN.

Results/Conclusion
I have my doubts about such a huge increase in performance with just these few changes. So I know there is probably something wrong with my existing implementation. Can someone point me in the right direction?
Any sort of help will be appreciated. Thanks!
","['deep-learning', 'reinforcement-learning', 'dqn', 'open-ai', 'deepmind']","Although what @Jaden said may be true by itself, it does not really serve to answer my question as I have seen after conducting numerous experiments, and finally reaching close to Dueling Network performance using a normal Double DQN (DDQN).I made the following changes to my code after closely examining the OpenAI baselines code:Made sure that the make_atari() wrapper function is also called on the env:These wrappers may be implemented from scratch or can be obtained from the OpenAI baseline Atari wrappers. I personally used the latter since there is no point in reinventing the wheel.Conclusion:The biggest step that I overlooked, or rather didn't pay much attention to was the input preprocessing. These few changes improved my DDQN from an average score saturation at -13 in almost 5000 episodes to +18 in about 700-800 episodes. That is indeed a huge difference. You can check out my implementation here."
Rollout algorithm like Monte Carlo search suggest model based reinforcement learning?,"
From what I understand, Monte Carlo Tree Search Algorithm is a solution algorithm for model free reinforcement learning (RL).
Model free RL means agent doesnt know the transition and reward model. Thus for it to know which next state it will observe and next reward it will get is for the agent to actually perform an action.
my question is:
if that is the case, then how come the agent knows which state it will observe during the rollout, since rollout is just a simulation, and the agent never actually perform that action ? (it never really interact with the environment: e.g it never really move the piece in a Go game during rollout or look ahead, thus cannot observed anything). 
It can only assume observing anything when not actually interacting with environment (during simulation) if it knows the transition model as I understand it. The same arguments goes for the rewards during rollout/ simulation.
in this case, doesnt rollout in Monte Carlo Tree Search algorithm suggests that the agent knows the transition model and reward model and thus a solution algorithm for model based reinforcement learning and not model free reinforcement learning ?
** it makes sense in Alphago, since the agent is trained to estimate what it would observed. but MCTS (without policy and value netwrok) method assumes that agent knows what it would observed even though no additional training is included.
","['reinforcement-learning', 'models', 'monte-carlo-tree-search']",
Choosing the right neural network settings [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I'm trying to train a neural network on evaluating chess positions if rather white (0.0) or black would win (1.0)
Currently the input consists of 4 bits per chess field (piece id 0 - 12, equals 64*4). Factors like castling are being ignored for now. Also, all training sets are random positions from popular games where it's white's turn and the desired output is the outcome of the game (0.0, 0.5, 1.0).
Are my input values the right choice?
How many hidden layers / neurons for each layer should be used and what's the best learning rate?
What type of NN's and which activation function would you recommend for this project?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks']","Easy ones first:Now the hard bit - encoding your input:Finally, those labels of yours: do think about what you'll do with drawn games - perhaps you have 3 possible outcomes, not 2?We would all be fascinated to hear how you get on - I hope you'll write your work up in some form (and that you'll come back and complain/praise our advice, as appropriate!)."
Neural network with logical hidden layer - how to train it? Is it policy gradient problem? Chaining NNs?,"
I am doing neural machine translation task from language S to language T via interlingua L. So - there is the structure:
S ->
encoding of S (crisp) ->
S-L encoder -> S-L decoder ->
encoding of L (non-crisp, coming from decoder) ->
L ->
encoding of L (crisp) ->
L-T encoder -> L-T decoder ->
encoding of T (non-crisp, coming from decoder) ->
T

All of this can be implemented in Pytorch more or less adapting the usual encoder-decoder NMT. So, the layer of interlingua L acts as a somehow symbolic/discrete layer inside the whole S-L-T neural network. My question is - how such system can be trained in end-to-end (S-T) manner? The gradient propagates from the T to the L and at the L one should do some kind of symbolic gradient? I. e. one should be able do compute the difference L1-L2?
I am somehow confused by such setting. My question is - is there similar networks which contain the symbolic representation as the intermediate layer and how one can train such system. I have heard about policy gradients but are they relevant to my setting?
Essentially - if I denote some neural network by symbols x(Wi)y, then the training of this network means, that I change Wi and x stays intact. I.e. the last member of backpropagation equation has the form d.../dw1. But if I combine (chain!) 2 neural networks x(Wi)y-y(Wj)z, then the the last backpropagation term for the y(Wj)z has the form (d.../dw1+d.../dy) and hence both the w1 and y should be changed/updated by the gradient descent too. So, doesn't some ambiguity arise here? Is such chaining of neural networks possible? Is is possible to train end-to-end chains of neural networks?
I am also thinking about use of evolutionary training.
","['neural-networks', 'training', 'gradient-descent', 'policy-gradients', 'machine-translation']",
Are neural networks statistical models?,"
By reading the abstract of Neural Networks and Statistical Models paper it would seem that ANNs are statistical models.
In contrast Machine Learning is not just glorified Statistics.
I am looking for a more concise/summarized answer with focus on ANNs.
","['neural-networks', 'machine-learning', 'deep-learning', 'terminology', 'statistical-ai']","According to Anthony C. Davison (in the book Statistical Models), a statistical model is a probability distribution constructed to enable inferences to be drawn or decisions made from data. The probability distribution represents the variability of the data.Do neural networks construct or represent a probability distribution that enables inferences to be drawn or decisions made from data?For example, a multi-layer perceptron (MLP) trained to solve a binary classification task can be thought of as model of the probability distribution $\mathbb{P}(y \mid x; \theta)$. In fact, there are many examples of MLPs with a softmax or sigmoid function as the activation function of the output layer in order to produce a probability or a probability vector. However, it's important to note that, although many neural networks produce a probability or a probability vector, a probability distribution is not the same thing. A probability alone does not describe a full probability distribution and different distributions are defined by different parameters (e.g. a Bernoulli is defined by $p$, while a Gaussian by $\mu$ and $\sigma$). However, for example, if you make your neural network produce a probability, i.e. model $\mathbb{P}(y = 1 \mid x; \theta)$, at least in the case of binary classification, you could obviously derive the probability of the other label as follows: $\mathbb{P}(y = 0 \mid x; \theta) = 1 - \mathbb{P}(y = 1 \mid x; \theta)$. In any case, in this example, you only need the parameter $p = \mathbb{P}(y = 1 \mid x; \theta)$ to define the associated Bernoulli distribution.So, these neural networks (for binary classification) that model and learn some probability distribution given the data in order to make inferences or predictions could be considered statistical models. However, note that, once the weights of the neural network are fixed, given the same input, they always produce the same output.Variational auto-encoders (VAEs) construct a probability distribution (e.g. a Gaussian or $\mathbb{P}(x)$ that represents the probability distribution over images, if you want to generate images), so VAEs can be considered statistical models.There are also Bayesian neural networks, which are neural networks that maintain a probability distribution (usually, a Gaussian) for each unit (or neuron) of the neural network, rather than only a point estimate. BNNs can thus also be considered statistical models.The perceptron may be considered a ""statistical model"", in the sense that it learns from data, but it doesn't produce any probability vector or distribution, i.e. it is not a probabilistic model/classifier.So, whether or not a neural network is a statistical model depends on your definition of a statistical model and which machine learning models you would consider neural networks. If you are interested in more formal definitions of a statistical model, take a look at this paper.Statistical models are often also divided into parametric and non-parametric models. Neural networks are often classified as non-parametric because they make fewer assumptions than e.g. linear regression models (which are parametric) and are typically more generally applicable, but I will not dwell on this topic."
How to design an ANN to give an answer that includes two different components?,"
As an experiment, I want to teach an ANN to play the game of Nim.

The normal game is between two players and played with three heaps of any number of objects. The two players alternate taking any number of objects from any single one of the heaps. The goal is to be the last to take an object.

The game is easily solvable and I already wrote a small bot that can play Nim perfectly to provide data sets for supervised learning.
Now I am struggling with the design question, how should I output the solution to a specific board state. The answer always consists of two components:

How many stones to take (a more or less arbitrary integer value)
Which heap to take the stones from (the index of the heap)

What are available design choices in this regard and is there a state-of-the-art design for this type of problem?
",['ai-design'],
What is the difference between validation percentage and batch size?,"
I'm doing transfer learning using Inception on Tensorflow. The code that I used for training is https://raw.githubusercontent.com/tensorflow/hub/master/examples/image_retraining/retrain.py
If you take a look at the Argument Parser section at the bottom of the code, you will find these parameters :

testing_percentage
validation_percentage
test_batch_size
validation_batch_size

So far, I understand that testing and validation percentage is the number of images that we want to train at 1 time. But I don't really understand the use of test batch size and validation batch size. What is the difference between percentage and batch size?
","['comparison', 'tensorflow', 'cross-validation', 'batch-size', 'validation-datasets']","The percentages refer to the number of samples to use (out of full dataset) as the validation and test datasets. So if you pass a dataset that consists of 100,000 samples to the model and set the validation and testing percentages to be 10% each, your model will train on 80,000 samples, validate them on 10,000 and save additional 10,000 samples for the final test.The batch sizes refer to the number of samples in each batch during the test and validation evaluations.
Your model probably can't process 10,000 samples in a single run (due to memory limitations) so during evaluation it breaks the dataset into batches, which are processed sequentially and the result is the mean of all batches.When you are training, the batch size is an important hyper-parameter which has an affect on the properties and final results of the training process. During test/validation it has no affect and only needs to be small enough for your model to be able to run it (evaluation with different batch sizes will produce the same results). "
What does learning mean? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



Can someone explain what is the process of learning? What does it mean to learn something?
","['machine-learning', 'terminology', 'philosophy', 'definitions']",
Is there a relation between the size of the neural networks and speed of convergence in deep reinforcement learning?,"
Is there a connection between the approximator network sizes in a RL task and the speed of convergence to an (near) optimal policy or value function?
When thinking about this, I came across the following thoughts:

If the network would be too small, the problem won't get enough representation and would never be solved, and the network would converge to its final state quickly.
If the network would be infinitely big (assuming no vanishing gradients and the likes), the network would converge to some (desirable) over-fitting, and the network would converge to its final state very slowly, if at all.
This probably means there is some golden middle ground.

Which leads me to the interesting question:
4. Assuming training time is insignificant relative to running the environment (like in real life environments), then if a network of size M converges to an optimal policy in average after N episodes, would changing M make a predictable change on N?
Is there any research, or known answer to this?
How to know that there is no more need to increase the network size?
How to know if the current network is too large?
Note: please regard question 4 as the main question here.
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'deep-rl']","Speed and size, no. That's because speed is dependent on processor clock periods and the effective parallel nature of the particular deep RL design in the environment, which is also dependent on cores and host clustering. Size is not really quantifiable in a way that is meaningful in this relationship because there is a broader and more complex structure of a network that might be trained to produce a value to use in the RL algorithm chosen.One can say that number of clock cycles times number of effective parallel processors is correlated to all the above, the overhead of the RL algorithm used, and the size of each data tensor.There are a few inequalities developed in the PAC (probably approximately correct) framework, so it would not be surprising if there were some bounding rules for the relationship between clock cycles, effective parallelism, data widths, activation functions, and RL algorithms for deep RL.Study of the algorithm, perhaps in conjunction with experimentation, may reveal the primary factors, essentially the processing bottlenecks. Further study of the factors involved in controlling loop iteration counts, which cause the bottleneck, could permit the quantification of computing effort required to maintain a particular maximum allowable response time, but that would be specific to a particular design.Such might produce a metric that is effectively a count of clock cycles across the system's potentially parallel architecture for a worst case or mean RL action selection. That could then be used to determine the response time for a given system with all of the factors mentioned above fixed, including the priority and scheduling of the processes in each operating system involved.Here's a guess. Feel free to critique, since such a formulation is a project far beyond the effort that should be put into answering a question online.$$ t_r \propto e^{k_v} \, n_v \, \eta_v \, t_v \, \mu_v \, \sum c_v + e^{k_d} \, n_d \, \eta_d \, t_d \, \mu_d \, \sum c_d \; \text{,} $$where $t$ is time, $k$ is tensor complexity, $n$ is number of cores, $\eta$ is the effective efficiency of the parallel processing, $\mu$ is the effective overhead cost of the glue code, $c$ is the cycles require of the significant (and probably repetitive) elements in the algorithms, $t_r$ is the total time to response, and the subscripts $v$ and $d$ designate the variable subscripted as either RL value determination metrics or deep network metrics respectively."
Is my Neural Network program fully connected?,"
I have the following program for my neural network:  
n_steps = 9
n_inputs = 36
n_neurons = 50
n_outputs = 1
n_layers = 2
learning_rate = 0.0001
batch_size =100
n_epochs = 1000#200 
train_set_size = 1000
test_set_size = 1000
tf.reset_default_graph()
X = tf.placeholder(tf.float32, [None, n_steps, n_inputs],name=""input"")
y = tf.placeholder(tf.float32, [None, n_outputs],name=""output"")
layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons,activation=tf.nn.relu6, use_peepholes = True,name=""layer""+str(layer))
         for layer in range(n_layers)]    layers.append(tf.contrib.rnn.LSTMCell(num_units=n_neurons,activation=tf.nn.relu6, use_peepholes = True,name=""layer""+str(layer)))
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = outputs[:,n_steps-1,:]

I want to know whether my network is fully connected or not?
When I try to see the variables, I see:  
multi_layer_cell.weights

The output is:  
[<tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/kernel:0' shape=(86, 200) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/bias:0' shape=(200,) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/w_f_diag:0' shape=(50,) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/w_i_diag:0' shape=(50,) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/w_o_diag:0' shape=(50,) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/kernel:0' shape=(100, 200) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/bias:0' shape=(200,) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/w_f_diag:0' shape=(50,) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/w_i_diag:0' shape=(50,) dtype=float32_ref>,
 <tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/w_o_diag:0' shape=(50,) dtype=float32_ref>]

I didn't understood whether each layer is getting the complete inputs or not.
I want to know whether the following figure is correct for the above code:
 
If this is not then what is the figure for the network? Please let me know.
","['neural-networks', 'machine-learning', 'ai-design', 'long-short-term-memory']","Is figure = code?No.
Your figure shows a fully connected feed forward network (MLP). But in your code you are using a two layer LSTM with peepholes. For the visualization of LSTMs, blocks are usually used for each layer.Here is a figure of the LSTM with peepholes which is the base of the tensorflow implementation (Source: Paper, fig. 1).Why size 86?The input is concatenated with the hidden state:
n_inputs + n_neurons = 36 + 50 = 86.Why size 100 in the second layer?The second LSTM layer gets input of size 50 by the first layer (n_neurons) which is concatenated with the hidden state of the second layer (of size n_neurons = 50).
Therefore you get 50 + 50 = 100.Why size 200?There are four weight matrices of size $86 \times 50$ (fig.: colored circles and the g circle), which seem to be combined to one matrix ($4 \cdot 50$) of size $86 \times 200$ (layer0/kernel).Why size 50?The three variables w_f_diag, w_i_diag and w_o_diag are for the peephole connections (fig: dashed lines) and they have the size of n_neurons = 50."
Is it possible to recognise a person based on what they have written?,"
I needed to make a system for recognizing people based on hundreds of texts by finding similarities in their written text grammatically or similarities between words they choose for writing. I don't want it so accurate, but I wanted to know if it is possible.
For example, finding one person with two accounts or more on a forum or something in that case (texts already gathered). I'm just wondering if it's possible and what field should I research for.
","['natural-language-processing', 'reference-request', 'computational-linguistics']","The term you are looking for is stylometry, which is related to a technique in forensic linguistics called writeprint analysis. There are many different techniques to perform stylometric analysis, from the very basic 5-feature analysis classifying features such as the lexicon and idiosyncrasies unique to a person to more complex analysis utilizing neural networks and machine learning. Searching online for research papers focusing on stylometry should assist you in finding the best technique for the job."
How does a neural network output text box location data?,"
I'm interested in creating a convolutional neural network or LSTM to locate text in an image. I don't want to OCR the text yet, just find the text regions. Yes, I know Tesseract and other systems can do this, but I want to learn how it works by building my own. All of the tutorials and articles I've seen so far have the CNN output to a classification - ""image contains a cat"", ""image contains a dog"". Okay that's nice, but it doesn't say anything about where it was found. 
Can anyone point me to some information that describes the output layer of a NN that can give location information? Like, x-y co-ordinates of text boxes?
","['convolutional-neural-networks', 'long-short-term-memory', 'optical-character-recognition']",
Are innovation weights shared in the NEAT algorithm?,"
I am trying to implement NEAT algorithm in Python from scratch. However, I am stuck. When a new innovation number is created it has two nodes which represents the connection. Also this innovation number has a weight. 
However, I know that innovation numbers are global variables, in other words when a innovation number is created, 
ex. Innovation ID:1 - Node:1 to Node:4 - weight: 0.5

it will have a ID which will be used by other connections to represent the connection between Node:1 to Node:4. 
When this innovation is used by another neural network, will it also use the weight of the innovation 1, which is 0.5 in this example?
","['neural-networks', 'neat']","Quoting Evolving neural networks through augmenting topologies, p. 10 (emphasis mine):When crossing over, the genes in both genomes with the same innovation numbers are lined up. These genes are called matching genes. Genes that do not match are either disjoint or excess, depending on whether they occur within or outside the range of the other parent’s innovation numbers. They represent structure that is not present in the other genome. In composing the offspring, genes are randomly chosen from either parent at matching genes, whereas all excess or disjoint genes are always included from the more fit parent.Innovation numbers are used to line up genes in different genomes so you can perform crossover on networks with different topologies. Each network can optimise the weights in matching genes in a different way, so the weights are not shared. If they were, crossover would have nothing to contribute towards diversifying the population."
How to (theorically) build a neural network with input of size 0?,"
Say I want to train a NN that generates outputs of some sort (say, even numbers).
Note that the network does not classify outputs, but, rather GENERATES the output.
I want let it run forward and generate some number, then either give it a positive reward of 1 for an even number, and a reward of -1 for an odd number, to make i output only even numbers over time.
What would be an architecture for such a NN?
I am getting caught in the part where here is actually no input, and I can't really start with a hidden layer, can I?
I am quite confused and would appreciate guidance
","['neural-networks', 'machine-learning']","Whether a neural network has learned anything or not, it is a function that maps some input to an output. Training is the process of tweaking the weights so that the output is something that we want. Thus there is always in input of some sorts.The problem you have presented, of generating even numbers, is much like a Generative Adversarial Network (GAN). In a GAN there are 2 networks: a Generator that tries to generate a sample from a target distribution and a Discriminator that tries to tell real samples from fake samples. The classic analogy being a criminal making counterfeit money and a copy trying to tell what is real money or not.The generator input is usually a random number (or a matrix of random numbers). The generator then learns to transform a particular random input to a particular point in the target space.So to answer your question, no there can't be a neural network with 0 inputs as there must always be an input of some kind. Even if the network was to generate a sequence instead of one instance, it would still need something to start with.For your example, there would have to be some input for the  network to start with. A really simple NN that could solve your problem might look like:"
Nonbinary and binary values in input tensor,"
In my input tensor, I would like to use both integer values as well as booleans. For example, if there is a spelling difference between 2 texts, I want to set the value to true, and otherwise false. In the same tensor, I would like to assign a value to, for example, the maximum number of consecutive messages, which will be an integer. 
Am I allowed to use 0's and 1's for the booleans together with integers, or will it have any negative impact on the working of the network? The ANN wont see any difference between the binary and nonbinary values, but is it a problem?
",['neural-networks'],"If the two inputs for the binary values are held to the domain set $\{0, 1\}$ during training, testing, and use, it will not break the network functionality, although the question is a good one. Why use so many bits to hold one?Theory of Holors: A Generalization of Tensors, by Parry Moon and Domina Spencer (ISBNs 978-0521019002 and 0521019001), propose holors precisely because of this kind of limitation on the natural heterogeneity of numerical structure. Gibbs, Ricci, Einstein, and others used holors in their mathematical expressions but without the name holor. Holors are are conceptually related to objects in ontology and classes in object oriented design, but they are numbers, so they fit into mathematical expressions as scalars, vectors, matrices, vector fields, and other tensors do.This is the basis for Coplien's operator overload in C++ and one of the reasons the language is too mutable to be mastered by average applications programmers. Support for what Moon and Spenser called holors is also the basis of decoupled type safety in some of the early LISP object oriented frameworks.The current programming paradigm we seen in Java, Python, and other popular languages was set in FORTRAN, which was more attainable for average programmers, so we are stuck with homogeneous structure, even in AI libraries and GPUs. Therefore, once one number requires an IEEE 64 float, bytes and bits will use all 64.In artificial networks, this only matters in the very first multiplication with a parameter."
Does leaky relu help learning if the final output needs negative values?,"
In an MLP with ReLU activation functions after each hidden layer (except the final),
Let's say the final layer should output positive and negative values.
With ReLU intermediary activations, this is still possible because the final layer, despite taking in positive inputs only, can combine them to be negative.
However, would using leaky ReLU allow faster convergence? Because you can pass in negative values as input to the final layer instead of waiting till the final layer to make things negative
",['neural-networks'],
Help with Novelty Recognition and Binary Classification for Emotion Recognition,"
I’m looking for advice regarding my ML project.
Using a special wristband, I am able to collect a bunch of physiological data from human subjects. I want to develop an application to recognize when these physiological signals change in a meaningful way and only then ask the user how he/she is feeling. This data will later be used for machine learning testing. The problem is, that I am struggling to find appropriate ways to classify current data input as meaningful and ask for information only when relevant user input is to be gathered, not more and not less.
For me, this seems to be a novelty detection problem, combined with a binary classification problem. I have to recognize what values coming from the data stream are to be considered normal, and therefore not bother the user with unnecessary input requests. I would also use novelty detection to recognize the data coming out of the normal zone and ask the user about it. This new data is then not considered novelty anymore, and binary classification will tell if the user is to be asked about his emotions when getting the same data in the future.
So, these are my questions:
-  What do you think about my reasoning of the problem? Do you have other perspectives on how to handle these problems? I have been told this could also be considered an anomaly detection problem, for example.
-  What algorithms would you use to separate normal from more meaningful physiological data? Support Vector Machines perhaps? Maybe some decision theory?
-  Do you know any books or papers on similar matters? Even if I have found some after hours and hours of research, you may be able to point me to something different than those I have.
It is worth noting that data collection is supposed to be done when no other factors are messing with signal readings, such as sport.
Any help would be much appreciated.
Best regards,
Augusto
","['machine-learning', 'data-science', 'decision-theory']","The ProjectIt appears from the question that emotional detection and response is the longer term goal of the project and that recognizing potential emotional manifestations in easily detectable physiological metrics is an initial R&D objective.Mobile device applications are already available to do this, but biometric monitoring via a wristband, excellence in AI design, and marketing excellence could overtake these apps in the marketplace and provide emotional regulation to improve productivity and reduce health and wellness risks.The (at least initial) goal seems to be basic binary classification, which simplifies the output, but not the detection, which seems to have the following two criteria.Acquire biological information related to emotionAvoid drawing the user's attention to perform unnecessary tasksThe Biometric Device ChallengeThere is definitely a challenge to classifying biometric trends as meaningful in this context.What do you think about my reasoning of the problem? Do you have other perspectives on how to handle these problems? I have been told this could also be considered an anomaly detection problem, for example.Novelty detection is not the first milestone in this challenge. Detection of useful features is the prerequisite. Novelty comes into play once changes in the organism can be characterized with some reliability and accuracy (few false positives and false negatives) and the particular user has provided subjective reporting to correspond with some change in the organism.Biological systems have forms of stasis that can be indirectly sensed, including these.These affect the dermis at the wrists through perturbations in externally detectable metrics, the inclusion of which depends on the capabilities of the wrist device.With blood and urine lab tests, conditions can be controlled, however sampling of those fluids are usually infrequent, leading to the use of ranges to detect anomalies that may be indicative of disease, conditions, or other health risks. There are major challenges to using ranges of metrics at the wrist.One inroad to detection of the internal metrics of the organisms is the fact that biological stasis is not the same thing as perfect regulation. Biological systems are chaotic. Normal signals from time series sampling of biological metrics are neither constant nor periodic. When changes in the attractors and spectra characterizing the chaotic fluctuation of direct metrics (like electrical resistance) or indirect metrics (like systolic blood pressure) occur, a request for subjective information may be asked from the wearer.These are some chaotic principles to consider.Once a correlation is established, then the novelty detection makes sense, except for an occasional re-acquisition of the correlation, since the wearer may become more internally connected to their physical and emotional states as they use the app, and the answers to inquiries may mature correspondingly.Additional Questions Within the QuestionThese additional questions were also asked.What algorithms would you use to separate normal from more meaningful physiological data? Support Vector Machines perhaps? Maybe some decision theory?A recurrent neural network such as B-LSTM or GRU are commonly excellent at characterizing time series, but there may be a need for more than one network.This second training must be done during in situ field use of the first already trained network. Since training is not generally viable on a mobile device, there will need to be a client server arrangement where the training data and its resulting trained network can be communicated through secure RestFUL services so the training processes can be asynchronous and leverage GPUs or other hardware acceleration.Do you know any books or papers on similar matters? Even if I have found some after hours and hours of research, you may be able to point me to something different than those I have.The terms above in a scholarly search will bring up all kinds of study materials. Specifically theseOne good book for all but the middle item is Chaos Theory Tamed, 1997
by Garnett P. Williams.It is worth noting that data collection is supposed to be done when no other factors are messing with signal readings, such as sport.This is where higher level detection involving chaotic analysis is much more reliable than ranges. The chaotic features of athletic activity will be distinctly different from those of fear states or sexual arousal."
Why does potential-based reward shaping seem to alter the optimal policy in this case?,"
It is known that every potential function won't alter the optimal policy [1]. I lack of understanding why is that.
The definition:
$$R' = R + F,$$ with $$F = \gamma\Phi(s') - \Phi(s),$$
where, let's suppose, $\gamma = 0.9$.
If I have the following setup:

on the left is my $R$.
on the right my potential function $\Phi(s)$
the top left is the start state, the top right is the goal state


The reward for the red route is: $(0 + (0.9 * 100 - 0)) + (1 + (0.9 * 0 - 100)) = -9$.
And the reward for the blue route is: $(-1 + 0) + (1 + 0) = 0$.
So, for me, it seems like the blue route is better than the optimal red route and thus the optimal policy changed. Do I have erroneous thoughts here?
","['reinforcement-learning', 'reward-functions', 'reward-shaping', 'potential-reward-shaping']","The same $\gamma = 0.9$ that you use in the definition $F \doteq \gamma \Phi(s') - \Phi(s)$ should also be used as the discount factor in computing returns for multi-step trajectories. So, rather than simply adding up all the rewards for your different time-steps for the different trajectories, you should discount them by $\gamma$ for every time step that expires.Therefore, the returns of the blue route are:$$0 + (0.9 \times -1) + (0.9^2 \times 0) + (0.9^3 \times 1) = -0.9 + 0.729 = -0.171,$$and the returns of the red route are:$$(0 + 0.9 \times 100 - 0) + 0.9 \times (1 + 0.9 \times 0 - 100) = 90 - 89.1 = 0.9.$$"
How to identify too small network in reinforcement learning?,"
I am using Open AI's code to do a RL task on an environment that I built myself.
I tried some network architectures, and they all converge, faster or slower on CartPole.
On my environment, the reward seems not to converge, and keeps flickering forever.
I suspect the neural network is too small, but I want to confirm my belief before going the route of researching the architecture.
How can I confirm that the architecture is the problem and not anything else in a neural network reinforcement learning task?
","['neural-networks', 'machine-learning', 'deep-learning', 'reinforcement-learning']",
Approach to classify a photo and extract text from it,"
I am trying to make a personal ML project where my objective is using a photo from an invoice, for instance, a Walmart invoice, classify it as being a Walmart invoice and extract the total amount spent. I would then save this information in a relational database and infer some statistics about my spendings. The goal would be to classify invoices not only from Walmart but from the most frequent shops where I spend money and then extract the total amount spent. I already do this process manually, I insert my spendings in a relational database. I have a bunch of photos from different invoices that I have recorded over the past year for this purpose (training a model).
What algorithms would you guys recommend? From my point of view, I think that I need some natural language processing to extract the total amount spent and maybe a convolutional neural network to classify the invoice as being from a specific store?
Thanks!
","['machine-learning', 'deep-learning', 'computer-vision']",
DQN stuck at suboptimal policy in Atari Pong task,"
I am in the process of implementing the DQN model from scratch in PyTorch with the target environment of Atari Pong. After a while of tweaking hyper-parameters, I cannot seem to get the model to achieve the performance that is reported in most publications (~ +21 reward; meaning that the agent wins almost every volley). 
My most recent results are shown in the following figure. Note that the x axis is episodes (full games to 21), but the total training iterations is ~6.7 million.

The specifics of my setup are as follows:
Model
class DQN(nn.Module):
    def __init__(self, in_channels, outputs):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(in_features=64*7*7 , out_features=512)
        self.fc2 = nn.Linear(in_features=512, out_features=outputs)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(-1, 64 * 7 * 7)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x    # return Q values of each action

Hyperparameters

batch size: 32
replay memory size: 100000
initial epsilon: 1.0
epsilon anneals linearly to 0.02 over 100000 steps
random warmstart episodes: ~50000
update target model every: 1000 steps
optimizer = optim.RMSprop(policy_net.parameters(),  lr=0.0025, alpha=0.9, eps=1e-02, momentum=0.0)

Additional info

OpenAI gym Pong-v0 environment
Feeding model stacks of 4 last observed frames, scaled and cropped to 84x84 such that only the ""playing area"" is visible.
Treat losing a volley (end-of-life) as a terminal state in the replay buffer.
Using smooth_l1_loss, which acts as Huber loss
Clipping gradients between -1 and 1 before optimizing
I offset the beginning of each episode with 4-30 no-op steps as the papers suggest

Has anyone had a similar experience of getting stuck around 6 - 9 average reward per episode like this? 
Any suggestions for changes to hyperparameters or algorithmic nuances would be greatly appreciated!
","['reinforcement-learning', 'deep-rl', 'dqn', 'implementation', 'atari-games']",
What is the reason behind using a test batch size?,"
If one examines the SSD: Single Shot MultiBox Detector code from this GitHub repository, it can be seen that, for a testing phase (evaluating network on test data set), there is a parameter test batch size. It is not mentioned in the paper.
I am not familiar with using batches during network evaluation. Can someone explain what is the reason behind using it and what are advantages and disadvantages?
","['deep-learning', 'testing', 'batch-size', 'test-datasets', 'single-shot-multibox-detector']","I am not familiar with using batches during network evaluation. Can someone explain what is the reason behind using it and what are advantages and disadvantages?It is usually just for memory use limitation vs speed of assessment. Larger batches evaluate faster on parallelised systems such as GPUs, but use more memory to process. Test results should be identical, with same size of dataset and same model, regardless of batch size.Typically you would set batch size at least high enough to take advantage of available hardware, and after that as high as you dare without taking the risk of getting memory errors. Generally there is less to gain than with training optimisation though, so it is not worth spending a huge amount of time optimising the batch size to each model you want to test. In most code I have seen, users pick a moderate ""safe"" value that will speed up testing but doesn't risk failing if you wanted to add a few layers to the model and check what that does."
What does the Critic network evaluate in Actor Critic?,"
Following Pytorch's actor critic, I understand that the critic is a function mapping from the state space to the reward space, meaning, the critic approximates the state-value funcion.
However, according to This paper (you don't need to read it, just a glance at the nice picture at page 2 is enough), the critic is a function mapping from the action space to the reward, meaning it approximates the action value funcion
I am confused.
When people say ""actor critic"" - what do they mean by ""critic""?
Is the term ""Critic"" ambiguous in RL?
","['machine-learning', 'reinforcement-learning', 'open-ai']",
How do I classify measurements into only two classes?,"
I am a member of a robotics team that is measuring the amount of reflected IR light to determine the lightness/darkness of a given material. We eventually hope to be able to use this to follow a line using a pre-set algorithm, but the first step is determining whether the material is one of the binary options: light or dark.  
Given a large population of values between 0 and 1023, probably in two distinct groupings, how can I best go about classifying a given point as light or dark?
","['neural-networks', 'machine-learning', 'classification']",
Which explainable artificial intelligence techniques are there?,"
Explainable artificial intelligence (XAI) is concerned with the development of techniques that can enhance the interpretability, accountability, and transparency of artificial intelligence and, in particular, machine learning algorithms and models, especially black-box ones, such as artificial neural networks, so that these can also be adopted in areas, like healthcare, where the interpretability and understanding of the results (e.g. classifications) are required.
Which XAI techniques are there?
If there are many, to avoid making this question too broad, you can just provide a few examples (the most famous or effective ones), and, for people interested in more techniques and details, you can also provide one or more references/surveys/books that go into the details of XAI. The idea of this question is that people could easily find one technique that they could study to understand what XAI really is or how it can be approached.
","['reference-request', 'ethics', 'explainable-ai']","Explainable AI and model interpretability are hyper-active and hyper-hot areas of current research (think of holy grail, or something), which have been brought forward lately not least due to the (often tremendous) success of deep learning models in  various tasks, plus the necessity of algorithmic fairness & accountability.Here are some state of the art algorithms and approaches, together with implementations and frameworks.Model-agnostic approachesLIME: Local Interpretable Model-agnostic Explanations (paper, code, blog post, R port)SHAP: A Unified Approach to Interpreting Model Predictions (paper, Python package, R package). GPU implementation for tree models by NVIDIA using RAPIDS - GPUTreeShap (paper, code, blog post)Anchors: High-Precision Model-Agnostic Explanations (paper, authors' Python code, Java implementation)Diverse Counterfactual Explanations (DiCE) by Microsoft (paper, code, blog post)Black Box Auditing and Certifying and Removing Disparate Impact (authors' Python code)FairML: Auditing Black-Box Predictive Models, by Cloudera Fast Forward Labs (blog post, paper, code)SHAP seems to enjoy high popularity among practitioners; the method has firm theoretical foundations on co-operational game theory (Shapley values), and it has in a great degree integrated the LIME approach under a common framework. Although model-agnostic, specific & efficient implementations are available for neural networks (DeepExplainer) and tree ensembles (TreeExplainer, paper).Neural network approaches (mostly, but not exclusively, for computer vision models)The Layer-wise Relevance Propagation (LRP) toolbox for neural networks (2015 paper @ PLoS ONE, 2016 paper @ JMLR, project page, code, TF Slim wrapper)Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization (paper, authors' Torch code, Tensorflow code, PyTorch code, yet another Pytorch implementation, Keras example notebook, Coursera Guided Project)Axiom-based Grad-CAM (XGrad-CAM): Towards Accurate Visualization and Explanation of CNNs, a refinement of the existing Grad-CAM method (paper, code)SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (paper, code, Google blog post)TCAV: Testing with Concept Activation Vectors (ICML 2018 paper, Tensorflow code)Integrated Gradients (paper, code, Tensorflow tutorial, independent implementations)Network Dissection: Quantifying Interpretability of Deep Visual Representations, by MIT CSAIL (project page, Caffe code, PyTorch port)GAN Dissection: Visualizing and Understanding Generative Adversarial Networks, by MIT CSAIL (project page, with links to paper & code)Explain to Fix: A Framework to Interpret and Correct DNN Object Detector Predictions (paper, code)Transparecy-by-Design (TbD) networks (paper, code, demo)Distilling a Neural Network Into a Soft Decision Tree, a 2017 paper by Geoff Hinton, with various independent PyTorch implementationsUnderstanding Deep Networks via Extremal Perturbations and Smooth Masks (paper), implemented in TorchRay (see below)Understanding the Role of Individual Units in a Deep Neural Network (preprint, 2020 paper @ PNAS, code, project page)GNNExplainer: Generating Explanations for Graph Neural Networks (paper, code)Benchmarking Deep Learning Interpretability in Time Series Predictions (paper @ NeurIPS 2020, code utilizing Captum)Concept Whitening for Interpretable Image Recognition (paper, preprint, code)Libraries & frameworksAs interpretability moves toward the mainstream, there are already frameworks and toolboxes that incorporate more than one of the algorithms and techniques mentioned and linked above; here is a partial list:The ELI5 Python library (code, documentation)DALEX - moDel Agnostic Language for Exploration and eXplanation (homepage, code, JMLR paper), part of the DrWhy.AI projectThe What-If tool by Google, a feature of the open-source TensorBoard web application, which let users analyze an ML model without writing code (project page, code, blog post)The Language Interpretability Tool (LIT) by Google, a visual, interactive model-understanding tool for NLP models (project page, code, blog post)Lucid, a collection of infrastructure and tools for research in neural network interpretability by Google (code; papers: Feature Visualization, The Building Blocks of Interpretability)TorchRay by Facebook, a PyTorch package implementing several visualization methods for deep CNNsiNNvestigate Neural Networks (code, JMLR paper)tf-explain - interpretability methods as Tensorflow 2.0 callbacks (code, docs, blog post)InterpretML by Microsoft (homepage, code still in alpha, paper)Captum by Facebook AI - model interpetability for Pytorch (homepage, code, intro blog post)Skater, by Oracle (code, docs)Alibi, by SeldonIO (code, docs)AI Explainability 360, commenced by IBM and moved to the Linux Foundation (homepage, code, docs, IBM Bluemix, blog post)Ecco: explaining transformer-based NLP models using interactive visualizations (homepage, code, article).Recipes for Machine Learning Interpretability in H2O Driverless AI (repo)Reviews & general papersA Survey of Methods for Explaining Black Box Models (2018, ACM Computing Surveys)Definitions, methods, and applications in interpretable machine learning (2019, PNAS)Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead (2019, Nature Machine Intelligence, preprint)Machine Learning Interpretability: A Survey on Methods and Metrics (2019, Electronics)Principles and Practice of Explainable Machine Learning (2020, preprint)Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges (keynote at 2020 ECML XKDD workshop by Christoph Molnar, video & slides)Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI (2020, Information Fusion)Counterfactual Explanations for Machine Learning: A Review (2020, preprint, critique by Judea Pearl)Interpretability 2020, an applied research report by Cloudera Fast Forward, updated regularlyInterpreting Predictions of NLP Models (EMNLP 2020 tutorial)Explainable NLP Datasets (site, preprint, highlights)Interpretable Machine Learning: Fundamental Principles and 10 Grand ChallengeseBooks (available online)Interpretable Machine Learning, by Christoph Molnar, with R code availableExplanatory Model Analysis, by DALEX creators Przemyslaw Biecek and Tomasz Burzykowski, with both R & Python code snippetsAn Introduction to Machine Learning Interpretability (2nd ed. 2019), by H2OOnline courses & tutorialsMachine Learning Explainability, Kaggle tutorialExplainable AI: Scene Classification and GradCam Visualization, Coursera guided projectExplainable Machine Learning with LIME and H2O in R, Coursera guided projectInterpretability and Explainability in Machine Learning, Harvard COMPSCI 282BROther resourcesexplained.ai blogA Twitter thread, linking to several interpretation tools available for RA whole bunch of resources in the Awesome Machine Learning Interpetability repoThe online comic book (!) The Hitchhiker's Guide to Responsible Machine Learning, by the team behind the textbook Explanatory Model Analysis and the DALEX package mentioned above (blog post and backstage)"
Could a neural network be capable to diferentiate between two boards of a game?,"
Let's propose, that I can define the state of a board in a board game, with 234 neurons. In theory, could I be able to train a neural network, with 468 inputs (two game boards), and 1 output, to tell me which board state is 'better'? The output should give me ~-1 if the second board is better than the first, ~0 if they are equal, and ~1 if the first board is better than the second.
If yes, what could be the number of ideal neurons on the hidden layers? What could be the ideal number of hidden layers?
","['neural-networks', 'game-ai']","For optimal performance, the network complexity should fit the complexity of the game. Since we do not know the latter, your question is not answerable. "
What is the difference between search and planning?,"
I'm reading the book Artificial Intelligence: A Modern Approach (by Stuart Russell and Peter Norvig).
However, I don't understand the difference between search and planning. I was more confused when I saw that some search problems can be determined in a planning way. My professor explained to me in a confusing way that the real difference is that search uses a heuristic function, but my book says that planning uses a heuristic too (in chapter 10.2.3).
I read this Stack Overflow post that says in a certain way what I'm saying.
So, what is the difference between search and planning?
","['comparison', 'definitions', 'search', 'planning', 'state-space-search']",
What's the role of bounding boxes in object detection?,"
I'm quite new to the field of computer vision and was wondering what are the purposes of having the boundary boxes in object detection.
Obviously, it shows where the detected object is, and using a classifier can only classify one object per image, but my question is that

If I don't need to know 'where' an object is (or objects are) and just interested in the existence of them and how many there are, is it possible to just get rid of the boundary boxes?

If not, how does bounding boxes help detect objects? From what I have figured is that a network (if using neural network architectures) predicts the coordinates of the bounding boxes if there is something in the feature map. Doesn't this mean that the detector already knows where the object is (at least briefly)? So, continuing from question 1, if I'm not interested in the exact location, would training for bounding boxes be irrelevant?

Finally, in architectures like YOLO, it seems that they predict the probability of each class on each grid (e.g. 7 x 7 for YOLO v1). What would be the purpose of bounding boxes in this architecture other than that it shows exactly where the object is? Obviously, the class has already been predicted, so I'm guessing that it doesn't help classify better.


","['machine-learning', 'convolutional-neural-networks', 'object-detection', 'yolo', 'bounding-box']","A bounding box is a rectangle superimposed over an image within which all important features of a particular object is expected to reside. It's purpose is to reduce the range of search for those object features and thereby conserve computing resources: Allocation of memory, processors, cores, processing time, some other resource, or a combination of them. For instance, when a convolution kernel is used, the bounding box can significantly limit the range of the travel for the kernel in relation to the input frame.When an object is in the forefront of a scene and a surface of that object is faced front with respect to the camera, edge detection leads directly to that surface's outlines, which lead to object extent in the optical focal plane. When edges of object surfaces are partly obscured, the potential visual recognition value of modelling the object, depth of field, stereoscopy, or extrapolation of spin and trajectory increases to make up for the obscurity.A classifier can only classify one object per imageA collection of objects is an object, and the objects in the collection or statistics about them can be characterized mathematically as attributes of the collection object. A classifier dealing with such a case can produce a multidimensional classification of that collection object, the dimensions of which can correspond to the objects in the collection. Because of that case, the statement is false.1) If I don't need to know 'where' an object is (or objects are) and just interested in the existence of them and how many there are, is it possible to just get rid of the boundary boxes?If you have sufficient resources or patience to process portions of the frame that don't contain the objects, yes.Questions (2) and (3) are already addressed above, but let's look at them in that context.2.a) If not, how does bounding boxes help detect objects?It helps by fulfilling its purpose, to reduce the range of the search. If by thrifty method a bounding shape of any type can be created, then the narrowing of focus can be used to reduce the computing burden on the less thrifty method by eliminating pixels that are not necessary to peruse with more resource-consuming-per-pixel methods. These less thrifty methods may be necessary to recognize surfaces, motion, and obscured edges and reflections so that the detection of object trajectory can be obtained with reliability.That these thrifty mechanisms to find the region of focus and these less thrifty mechanisms to use that information and then determine activity at higher levels of abstraction are artificial networks of this type or that or use algorithms of this type or that is not relevant yet. First understand the need to reduce computing cost in AI, which is a pervasive concept for anything more complex than tic-tac-toe, and then consider how bounding boxes help the AI engineer and the stakeholders of the engineering project to procure technology that is viable in the market.2.b) From what I have figured is that a network (if using neural network architectures) predicts the coordinates of the bounding boxes if there is something in the feature map. Doesn't this mean that the detector already knows where the object is (at least briefly)?2.c) So continuing from question 1, if I'm not interested in the exact location, would training for bounding boxes be irrelevant?Cognition is something AI seeks to simulate, and many hope to have robots like in the movies that can help out and be invaluable friends, like TARS in the Nolan brothers 2014 film Interstellar. We're not there. The network knows nothing. It can train a complex connection between an input signal through a series of attenuation matrices and activation functions to produce an output signal statistically consistent with its loss function, value function, or some other criteria.The inner layers of an artificial net may, if not directed to do so, produce something equivalent to a bounding region only if velocity of convergence is present as a factor in its loss or value function. Otherwise there is nothing in the Jacobian leading convergence to reduce its own time to completion. Therefore, the process may complete, but not as well as if cognition steps in and decides that the bounding region will be found first and then used to reduce the total burden of mechanical (arithmetic) operations to find the desired output signal as a function of input signal.3) Finally, in architectures like YOLO, it seems that they predict the probability of each class on each grid (e.g. 7 x 7 for YOLO v1). What would be the purpose of bounding boxes in this architecture other than that it shows exactly where the object is? Obviously, the class has already been predicted so I'm guessing that it doesn't help classify better.Reading the section in A Real-Time Chinese Traffic Sign Detection Algorithm Based on Modified YOLOv2, J Zhang, M Huang, X Jin, X Li, 2017, may help further comprehension of these principles and their almost universal role in AI, especially the text around their statement, ""The Network Architecture of YOLO v2 YOLO employs a single neural network to predict bounding boxes and class probabilities directly from full images in one inference. It divides the input image into S × S grids."" This way you can see the use of these principles in the achievement of specific research goals.Other such applications can be found by simply reading the article full text available on the right side of an academic search for yolo algorithm and using ctrl-f to find the word bound."
"Deep Learning on how to find out the body measurement (e.g. shoulder length, waist, hips, legs length etc) from mobile camera captured images?","
I do understand that there are plenty of mobile apps available for body measurement (e.g. MTailor) or creating 3D model (3dlook).
What I would like to find out is how we can use deep learning to achieve the accurate body measurement/3D model with just smart phone camera?
For example, MTailor can predict one's body measurement quite accurately given the cameara angle/camera distance from the human and human height. Can we do the same using deep learning with some labeled images to achieve the same accurate body measurement prediction?
Thanks
Regards,
Han
","['deep-learning', 'image-recognition']",
"Using a ""is_padding"" attribute in your padding instead of simply zero vectors","
Typical Feed Forward Neural Networks require a fixed sized input and output. So when you have variable sized input, it seems to be common practice to pad the input with zero vectors.
Why does it not seem to be common practice to have a ""is_padding"" attribute?  That way the network can easily distinguish between padding and actual data?  Especially considering input is commonly centered around 0 by subtracting the mean and using unit variance.
","['neural-networks', 'feedforward-neural-networks']",
Why does is make sense to normalize rewards per episode in reinforcement learning?,"
In Open AI's actor-critic and in Open AI's REINFORCE, the rewards are being normalized like so
rewards = (rewards - rewards.mean()) / (rewards.std() + eps)

on every episode individually.
This is probably the baseline reduction, but I'm not entirely sure why they divide by the standard deviation of the rewards?
Assuming this is the baseline reduction, why is this done per episode?
What if one episode yields rewards in the (absolute, not normalized) range of $[0, 1]$, and the next episode yields rewards in the range of $[100, 200]$?
This method seems to ignore the absolute difference between the episodes' rewards.
","['reinforcement-learning', 'policy-gradients', 'variance-reduction', 'reward-normalization']","The ""trick"" of subtracting a (state-dependent) baseline from the $Q(s, a)$ term in policy gradients to reduce variants (which is what is described in your ""baseline reduction"" link) is a different trick from the modifications to the rewards that you are asking about. The baseline subtraction trick for variance reduction does not appear to be present in the code you linked to.The thing that your question is about appears to be standardization of rewards, as described in Brale_'s answer, to put all the observed rewards in a similar range of values. Such a standardization procedure inherently requires division by the standard deviation, so... that answers that part of your question.As for why they are doing this on a per-episode-basis... I think you're right, in the general case this seems like a bad idea. If there are rare events with extremely high rewards that only occur in some episodes, and the majority of episodes only experience common events with lower-scale rewards... yes, this trick will likely mess up training. In the specific case of the CartPole environment (which is the only environment used in these two examples), this is not a concern. In this implementation of the CartPole environment, the agent simply receives a reward with a value of exactly $1$ for every single time step in which it manages to ""survive"". The rewards list in the example code is in my opinion poorly-named, because it actually contains discounted returns for the different time-steps, which look like: $G_T = \sum_{t=0}^{T} \gamma^t R_t$, where all the individual $R_t$ values are equal to $1$ in this particular environment. These kinds of values tend to be in a fairly consistent range (especially if the policy used to generate them also only moves slowly), so the standardization that they do may be relatively safe, and may improve learning stability and/or speed (by making sure there are always roughly as many actions for which the probability gets increased as there are actions for which the probability gets decreased, and possibly by making hyperparameters easier to tune).It does not seem to me like this trick would generalize well to many other environments, and personally I think it shouldn't be included in such a tutorial / example.Note: I'm quite sure that a per-episode subtraction of the mean returns would be a valid, albeit possibly unusual, baseline for variance reduction. It's the subsequent division by standard deviation that seems particularly problematic to me in terms of generalizing to many different environments. "
What kind of decision rule algorithm is usable in this situation?,"
I am trying to write an AI to a game, where there is no real adversary. This means, that only the AI player has choices in which move to perform, his opponent may or may not react to the move the AI player made, but when he reacts, he will always do the one and only single move that he is able to do. The goal of this AI would be, to find a solution to the situation, which results in the least amount of monster activations.
To explain this a bit further, I will describe the game in a few words: there is a 3x3 board, on which there are some monsters. These monsters has a prewritten AI, and activate based on prewritten rules, ie, they do not have to make any decision at all. This is done, by an enrage mechanic, meaning, that when a monster hits it's enrage limit, it activates, and performs his single move action.
The AI should control the other side of this board, the hero players. Each hero player has a different number of possible moves, each move dealing an amount of damage to the monsters, and increasing it's enrage value, thus getting him closer to his enrage limit.
What I want to achieve, is to write an AI, that will perform this fight in the least amount of monster activations as possible.
For now, I've written a minimax algorithm for this, without the min player. I've done this, by calculating the negative effect of the monsters move, in the maximizing and only players move.
The AI works in the following way: he draws the game tree for a set amount of depth of moves, calculates the bottom move with a heuristic function, selects the highest value from the given depth, and returns the value of this function up one level, then repeat. When he reaches the top of the tree, he performs the move, with the highest quantification value.
This works, somewhat, but I have a big problem: As there is no randomness in the game, I was expecting that the greater the depth that he can search forward, the better moves he will find, but this is not always the case, sometimes a greater depth, returns a worse solution then a smaller depth
My questions are as follows:

what could cause the above error? My quantification function? The weights that I use in the function? Or something else?
is minimax the correct algorithm to use, for a game where there is no real adversarry, or is there any algorithm that will perform better for a game like this?

","['minimax', 'heuristics', 'decision-trees']",
Can a machine learning model predict the pattern of given sequence?,"
I am curious if it is possible to do so.
For example, if I supply

$[0, 1, 2, 3, 4, 5]$, the model should return ""natural number sequence"",

$[1,3,5,7,9,11]$, it should return ""natural number with step of $2$"",

$[1,1,2,3,5]$, it should return ""Fibonacci numbers"",

$[1,4,9,16,25]$, it should return ""square natural number""


and so on.
","['machine-learning', 'prediction', 'supervised-learning']",
How can a neural network learn when the derivative of the activation function is 0?,"
Imagine that I have an artificial neural network with a single hidden layer and that I am using ReLU as my activating function.
If by change I initialize my bias and my weights in such a form that:
$$
X * W + B < 0
$$
for every input x in X then the partial derivate of the Loss function with respect to W will always be 0! 
In a setup like the above where the derivate is 0 is it true that an NN won´t learn anything? 
If true (the NN won´t learn anything) can I also assume that once the gradient reaches the value 0 for a given weight, that weight won´t ever be updated?
","['neural-networks', 'deep-learning', 'gradient-descent']","In a setup like the above where the derivat[iv]e is 0 is it true that an NN won´t learn anything?There are a couple of adjustments to gradients that might apply if you do this in a standard framework:Momentum may cause weights to continue changing if any recent ones were non-zero. This is typically implemented as a rolling mean of recent gradients.Weight decay (aka L2 weight regularisation) is often implemented as an additional gradient term and may adjust weights down even in the absence of signals from prediction errors.If either of these extensions to basic gradient descent are active, or anything similar, then it is possible for the neural network to move out of the stationary zone that you have created after a few steps and then continue learning.Otherwise, then yes it is correct that the neural networks weights would not change at all through gradient descent, and the NN would remain unchanged for any of your input values. Your careful initialisation of biases and weights will have created a system that is unable to learn from the given data. This is a known problem with ReLU activation, and can happen to some percentage of artificial neurons during training with normal start conditions. Other activation functions such as sigmoid have similar problems - although the gradient is never zero in many of these, it can be arbitrarily low, so it is possible for parameters to get into a state where learning is so slow that the NN, whilst technically learning something on each iteration, is effectively stuck. It is not always easy to tell the difference between these unwanted states of a NN and the goal of finding a useful minimum error."
Can analog quantum computer implement real-valued neural networks and hence do hypercomputation?,"
It is said, that the essence of https://www.springer.com/us/book/9780817639495 ""Neural Networks and Analog Computation. Beyond the Turing Limit"" is that the continuous/physical/real-valued weights for neural networks can induce super-Turing capabilities. Current digital processors can not implement real-valued neural networks, they can only approximate them. There are very little efforts to build analog classical computers. But it is quite possible that quantum computers will be analogue. So - is there research trend that investigates true real-valued neural networks on analog quantum computers?
Google is of no use for my efforts, because it does not understand the true meaning of ""true real-valued neural network"", it gives just real-value vs complex valued neural networks articles, which are not relevant to my question.
","['neural-networks', 'quantum-computing', 'hypercomputation']","Digital and AnalogThe question about analog computing is important.Digital circuitry gained popularity as a replacement for analog circuitry during the four decades between 1975 to 2015 due to three compelling qualities.This quickly led to digital signaling standards, architecture of a general purpose computing, and central processing units on a chip. The later, combined with an array of registers to perform elementary operations is the meaning of the word microprocessor.Quanta and ComputingRegarding quantum computing, there have been some interesting proposals to pack digital gates into much smaller volumes, but the notion that a computer can be made of transistors the size of electrons might be a bit fantastic. That's what the term quantum computing implies. That degree of miniaturization would have to defy principles of particle physics that are very strongly supported by amassed empirical evidence. Among them is Heisenberg's uncertainty principle.All computing involves quanta, but statistically. For a transistor in a digital circuit to be statistically stable, there must be a sufficient number of Si atoms with at least 0.1% molar concentration of the atoms used to dope the Si to create a PN junction. Otherwise the transistor will not switch reliably.The lithographic limit of most mass produced VLSI chips is around 7 nm as of this writing. Crystalline Si, nucleus to nucleus, is about .2 nm, so the miniaturization of a stable transistor is near its quantum limit already. Exceeding that limit by a considerable amount destabilizes the digital circuitry. That's a quantum physics limitation, not a lithographic limitation.Projections, Models, and Techniques to Push LimitsMoore's law was simply an approximate model for the chip industry during the period between the invention of the integrated circuit to the physical limitation of the atomic composition of transistors, which we are now approaching.Field effect transistors (FETs) can take the miniaturization only slightly further than the mechanics of PN junctions. 3-D circuitry has theoretical promise, but no repeatable nanotechnology mechanisms have yet been developed to form complex circuitry in the third dimension.Returning to the Primary QuestionPlacing aside the magical idea that quantum computing will somehow completely revolutionize circuitry, we have a question that is both feasible and predictable.Can an analog computer implement real-valued neural networks and hence do artificial network computation better?If we define better in this context as cheaper and faster, while maintaining reliability and accuracy, the answer is straightforward.It definitely takes fewer transistors to create the feed forward part of an artificial network using an analog approximation of the closed forms resulting from the calculus of artificial networks than a digital one. Both are approximations. Analog circuitry has noise, and drift and digital circuitry has rounding error. Beyond rounding, digital multiplication is much more complex in terms of circuitry than analog, and multiplication is used quite a bit in artificial network implementations.Limitation Interplay of Gödel and TuringThe idea from the tail end of the title of the book this question referenced, ""Beyond the Turing Limit,"" is also a little fantastic. The thought experiment of Alan Turing leading to the Turing machine and the associated computability theory (including Turing completeness) was not developed to be a limit. Quite the opposite. It was an answer to Gödel's incompleteness theory. People in Turing's time saw the work Gödel's genius as the annoying but indismissable limit threatening the centuries-old vision of using machines to automatically expand human knowledge. To summarize this work, we can state with assurance this.The theory limiting what computers can do is not related to how the numbers are represented in electronic circuit implementations. It is a limitation of information mechanics.These principles are important but have little to do with the limitation.The above has to do with the feasibility of a project for which some person or corporation must pay and the intellectual capacities required to completing it, not the hard limit on what is possible.Defining what a super-Turing capability might be would be a dismissal or a dismantling of what mathematicians consider to be well constructed theory. Dismantling or shifting the contextual frame of some computability theory is plausible. Dismissing the work that has been done would be naive and counterproductive.Real Numbers are Now Less Real Than IntegersThe compelling idea contained in the question is the reference to continuity, physicality, and the real valued nature of parameters that acquire a learned state during the training of artificial networks.To multiply a vector of digital signals by a digital parameter matrix requires many transistors and can require a significant number of clock cycles, even when dedicated hardware is used. Only a few transistors per signal path are required to perform the analog equivalent, and the throughput potential is very high.To say that real values cannot be represented in digital circuits is inaccurate. The IEEE standards for floating point numbers, when processed in a time series, represent real valued signals well. Analog circuits suffer from noise and drift as stated above. Both analog and digital signals only appear to be comprised of real number values. Real numbers are not real except in the world of mathematical models. What we call quantities in the laboratory are essentially measurements of means of distributions. Solidifying and integrating the probabilistic nature of reality into science and technology may be the primary triumph of the twentieth century, For instance, when dealing in milli-Amps (mA), electric current seems to be a continuous phenomenon, but when dealing with nano-Amps (nA), the quantum nature of electric current begins to appear. This is much like what happens with the miniaturization of the transistor. Real numbers can only be represented in analog circuits through the flow of discrete electrons. The key to the advantage of an analog forward feed in artificial networks is solely that the density of network cells can be considerably higher, reducing the cost of the network in its VLSI space.In summary, real numbers received the name for their type prior to the emergence of quantum physics. The idea that quantities formerly considered real and continuous were actually statistical averages of discrete activities at a quantum level revolutionized the field of thermodynamics and microelectronics. This is something that disturbed Einstein in his later years. In essence, mathematics using real numbers is effective in engineering because it simplifies what physicist now believe are distributions of a large numbers of quantum phenomena occurring in concert.Summarizing the Probable Future of Analog ComputingThis phrase from the question is not precisely scientific, even though it points to a strong likelihood.It is quite possible that quantum computers will be analogue. This modified version is more consistent with scientific fact in the way it is phrased, and is also factual.It is possible that computers dealing with signals at a near quantum level of miniaturization will have a higher proportion of analog circuitry.This question and its answers have many links to papers and research work regarding analog computing: If digital values are mere estimates, why not return to analog for AI?."
How to choose method for solving planning problems? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



There are many methods and algorithms dealing with planning problems.
If I understand correctly, according to Wikipedia, there are classical planning problems, with:

a unique known initial state,
duration-less actions,
deterministic actions,
which can be taken only one at a time,
and a single agent.

Classical planning problems can be solved using classical planning algorithms. The STRIPS framework for problem description and solution, using backward chaining) of the GraphPlan algorithm can be mentioned here.
If actions are non-deterministic, according to Wikipedia, we have a Markow Decision Process (MDP), with:

duration-less actions,
nondeterministic actions with probabilities,
full observability, or partial observability for POMDP
maximization of a reward function,
and a single agent.

MDPs are mostly solved by Reinforcement Learning.
Obviously, classical planning problems can also be formulated as MDPs (with state transition probabilities of 1, i.e. deterministic actions), and there are many examples (e.g. some OpenAI Gyms), where these are successfully solved by RL methods.
Two questions: 

Are there some characteristics of a classical planning problem, which makes MDP formulation and Reinforcement Learning a better suiting solution method? Better suiting in the sense that it finds a solution faster or it finds the (near)optimal solution faster.
How do graph search methods like A* perform with classical planning problems? Does STRIPS with backward chaining or GraphPlan always outperform A*? Outperform in the sense of finding the optimal solution faster.

","['reinforcement-learning', 'graph-theory', 'planning']",
"What is a ""logit probability""?","
DeepMind's paper ""Mastering the game of Go without human knowledge"" states in its ""Methods"" section on its ""Neural network architecture"" that the output layer of AlphaGo Zero's policy head is ""A fully connected linear layer that outputs a vector of size 19^2+1=362, corresponding to the logit probabilities for all intersections and the pass move"" (emphasis mine).  I am self-trained regarding neural networks, and I have never heard of a ""logit probability"" before this paper.  I have not been able by searching and reading to figure out what it means.  In fact, the Wikipedia page on logit seems to make the term a contradiction.  A logit can be converted into a probability using the equation $p=\frac{e^l}{e^l+1}$, and a probability can be converted into a logit using the equation $l=\ln{\frac{p}{1-p}}$, so the two cannot be the same.  The neural network configuration for Leela Zero, which is supposed to have a nearly identical architecture to that described in the paper, seems to indicate that the fully connected layer described in the above quote needs to be followed with a softmax layer to generate probabilities (though I am absolutely new to Caffe and might not be interpreting the definitions of ""p_ip1"" and ""loss_move"" correctly).  The AlphaGo Zero cheat sheet, which is otherwise very helpful, simply echoes the phrase ""logit probability"" as though this is a well-known concept.  I have seen several websites that refer to ""logits"" on their own (such as this one), but this is not enough to satisfy me that ""logit probability"" must mean ""a probability generated by passing a logit vector through the softmax function"".
What is a logit probability?  What sources can I read to help me understand this concept better?
","['neural-networks', 'terminology', 'activation-functions', 'alphago-zero']","Indeed I haven't seen the term ""logit probability"" used in many places other than that specific paper. So, I cannot really comment on why they're using that term / where it comes from / if anyone else uses it, but I can confirm that what they mean by ""logit probability"" is basically the same thing that is more commonly referred to simply as ""logits"": they are the raw, unbounded scores of which we generally push a vector through a softmax function to generate a discrete probability distribution that nicely adds up to $1$.This definition fits the one you linked from wikipedia (although that link only covers the binary case, and AlphaGo Zero would have multinomial logits since it has more than two outputs for the policy head).In the AlphaGo Zero paper, the described architecture has a ""linear output layer"" (i.e. no activation function for the outputs, or the identity function as activation function for the outputs, or however you like to describe it) for the policy head. This means that these outputs are essentially unbounded, they could be any real number. We know for sure that these outputs cannot directly be interpreted as probabilities, even if this isn't stated quite explicitly in the paper.By calling them logits (or logit probabilities for reasons unknown to me), they are essentially implying that these outputs will still be post-processed by a softmax to convert them into a vector that can be interpreted as a discrete probability distribution over the actions, even if they do not explicitly describe a softmax layer as being a part of the network.It is indeed possible that in Leela Zero they decided to make the softmax operation explicitly a part of the Neural Network architecture. Mathematically they end up doing the same thing... the AlphaGo Zero paper implies (by using the word ""logit"") that a softmax is used as a ""post-processing"" step, and in Leela Zero they explicitly make it a part of the Neural Network.Here are a couple more sources for the reasoning that usage of the word ""logit"" basically implies usage of a softmax, though indeed they do not cover the term ""logit probability"":"
Is it possible to use AI to denoise noisy documents?,"
I have some documents containing some text (machine writing text) that I intend to apply OCR on them in order to extract the text.
The problem is that these documents contain a lot of noise but in different ways (some documents have noise in the middle, others in the top, etc.), which means that I can't apply simple thresholding in order to remove the noise (i.e applying simple threshold does not only remove the noise, but it removes some parts of the text).
For these reasons, I thought about using AI to de-noise the documents.
Does anyone know if it is possible to do that with AI or any alternative way?
","['machine-learning', 'reference-request', 'optical-character-recognition']",
How does one characterize a neural network with threshold-based activation functions?,"
In an attempt at designing a neural network more closely modeled by the human brain, I wrote code before doing the reading. The neuron I have modeled operates on the following method.

Parameters: potential, threshold, activation.
[activation] = 0.0
Receive inputs, added to [potential].
If ([potential] >= [threshold])


[activation] = [potential]
[potential] = 0.0

Else


[potential] *= 0.5


In short, the neuron receives inputs, and decides if it ""fires"" if the threshold is met. If not, the input sum, or potential, decreases. Inputs are applied by adding their values to the input potentials of the input neurons, and connections multiply neuron activation values by weights before applying them to their destination potentials. The only difference between this an a spiking network is the activation model.
I am, however, beginning to learn that Spiking Neural Networks (SNNs), the actual biologically-inspired model, operate quite differently. Forgive me if my understanding is terribly flawed. I seem to have the understanding that signals in these networks are sharp sinusoidal wave-forms with between 100 and 300 ""spikes"" in a subdivision of ""time,"" given for 1 ""second."" These signals are sampled for the ""1 second"" by the neuron, and processed by a differential equation that determines the activation state of the neuron. Synapses seem to function in the same manner -> multiplying the signal by a weight, but increasing or decreasing the period of the graph.
However, I wish to know what form of neuron activation model I created. I have been unable to find papers that describe a method like this.
EDIT. The ""learnable"" parameters of this model are [threshold] of the neuron and [weight] of the connections/synapses.
","['neural-networks', 'human-inspired', 'spiking-neural-networks']","The model you describe is a kind of a leaky integrate-and-fire (LIF) neuron (see p. 7). It is leaky because the membrane potential decreases steadily in the absence of input. In contrast, in the simple integrate-and-fire (IF) model the membrane potential is retained indefinitely until the neuron spikes, at which point it is reset to 0. However, LIF neurons are usually modelled with exponential decay of the membrane potential, where you have a time constant $\tau$ and you compute the potential $P_{t}$ at time $t$ based on the potential $P_{t_{last}}$ at time when the last input arrived as $P_{t} = P_{t_{last}} exp(- \frac{t - t_{last}}{\tau})$This is the same formula as radioactive decay (see here for more details). The idea is that this model is inherently 'aware' of time, whereas the IF model (and your design above) do not factor in the timing of the spikes, so they act like a classical neural network activation. In any case, whether or not a neuron would fire does depend on the firing threshold, so I think that treating the threshold as a learnable parameter is justified - you just have to decide what rules to use for updating it.Based on what you describe as your understanding of spiking neural networks, it seems that you have been reading about the Hodgkin-Huxley (HH) model (also in that paper I linked to). (Please correct me if I'm wrong.) You are correct in thinking that spikes in the brain are not infinitely narrow like a delta function but more like a very sharp sinusoidal signal, and the HH model faithfully reproduces that. However, the reason why the HH model is not actually used for simulations is that it is computationally very taxing. In practice, in most cases we do not actually care about the state of the neuron between inputs, as long as your model accurately describes the neuron state and what happens to it when an input arrives. There are other models that approximate the HH model very closely but are much faster to simulate (like the Izhikevich model). However, the LIF model is very fast and sufficient in most cases.Hope this helps!"
What is the difference between meta-learning and zero-shot learning?,"
What is the difference between meta-learning and zero-shot learning? Are they synonymous?
I have seen articles where they seem to imply that they are at least very similar concepts.
","['deep-learning', 'comparison', 'meta-learning']","First see the definition of meta-learning:Meta-learning is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017 the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.and zero-shot learning:Zero-shot learning is being able to solve a task despite not having received any training examples of that task. For a concrete example, imagine recognizing a category of object in photos without ever having seen a photo of that kind of object before. If you've read a very detailed description of a cat, you might be able to tell what a cat is in a photograph the first time you see it.As you can see, these are different but meta-learning can be used in zero-shot learning to work better. For example see this article, as an instance."
Eligibility trace In Model-based Reinforcement Learning,"
In model-based reinforcement learning algorithms, the model of the environment is constructed to efficiently use samples, models such as Dyna, and Prioritize Sweeping. Moreover, eligibility trace helps the model learns (action) value functions faster. 
Can I know if it is possible to combine learning, planning, and eligibility traces in a model to increase its convergence rate? If yes, how it is possible to use eligibility traces in the planning part, like Prioritize Sweeping?
","['reinforcement-learning', 'model-based-methods', 'prioritized-sweeping', 'eligibility-traces', 'dyna']",
How do I choose the size of the hidden state of a GRU?,"
I'm trying to understand how the size of the hidden state affects the GRU. 
For example, suppose I want to make a GRU count. I'm gonna feed it with three numbers, and I expect it to predict the fourth. 
How should I choose the size of the hidden state of a GRU?
","['machine-learning', 'recurrent-neural-networks', 'hyperparameter-optimization', 'hidden-layers', 'gated-recurrent-unit']","Yes, your understanding of the hidden state is correct. But the size of the hidden state is a hyperparameter that needs to found by trial-and-error. There is no closed-form formula or solution which links the size of the hidden state and the problem at hand. But, there are some rules of thumb like to start out with the size of the hidden state to be a power of 2, etc. Keep tuning the hyperparameter until you get very good predictions."
How to include exploration in Gaussian policy,"
When dealing with continuous action spaces, a common choice when designing a policy in policy gradient methods is to learn mean and variance of actions for a specific state and then simply sample from the normal distribution defined by the learned mean and variance to get an action.
My first question is, is explicit exploration strategy even needed is such cases, because the dose of randomness in actions would come from the sampling itself, on the other hand there could probably be cases where we would be stuck in a local optimum just by sampling.
My second question is, in case that explicit exploration is needed, how would one approach this problem of exploration for this specific setup.
","['reinforcement-learning', 'policy-gradients']",
What are the segment embeddings and position embeddings in BERT?,"

They only reference in the paper that the position embeddings are learned, which is different from what was done in ELMo.
ELMo paper - https://arxiv.org/pdf/1802.05365.pdf
BERT paper - https://arxiv.org/pdf/1810.04805.pdf
","['machine-learning', 'deep-learning', 'natural-language-processing', 'bert']","These embeddings are nothing more than token embeddings.You just randomly initialize them, then use gradient descent to train them, just like what you do with token embeddings."
"Given these two reward functions, what can we say about the optimal Q-values, in self-play tic-tac-toe?","
This corresponds to Exercise 1.1 of Sutton & Barto's book (2nd edition), and a discussion followed from this answer.
Consider the following two reward functions

Win = +1, Draw = 0, Loss = -1
Win = +1, Draw or Loss = 0

Can we say something about the optimal Q-values?
","['reinforcement-learning', 'q-learning', 'sutton-barto', 'self-play', 'tic-tac-toe']","Chapter 1 of Sutton & Barto, doesn't introduce the full version Q learning, and you are probably not expected to explain the full distribution of values at that stage.Probably what you are expected to notice is that the maximum Q values out of possible next states - after training/convergence - should represent the agent's best choice of move. What the actual optimal values are depends on how the opponent plays. In self-play it is possible to find optimal play for both players in the game, and thus the Q values represent true optimal play. However what ""optimal play"" means is dependent on the goals you have set the agent implied by the reward values.Any move which leads to a guarantee that a player can force a win regardless of what the opponent does, would have a Q value of +1. If the agent or opponent can force a draw at best, then it will have the Q value of the draw, and if the opponent can force a win (i.e. the current agent would lose), then the move will have the Q value of a loss. This happens because the learning process copies best case values backwards from the end game states to earlier game states that lead to them.In a game with two perfect players, and +1, 0, -1 reward system, then each player will on its turn only see the 0 and -1 moves available. That is because there is no way to force a win in tic-tac-toe, and the perfect opponent will always act to block winning moves. The best choice out of 0 or -1 is 0: each player, when acting under its value estimates, will force a draw. There will be states defined that have a value of +1, but they will never appear as a choice to either player.What happens if you don't make a difference in rewards between drawing and losing? In the extreme case of having win +1, lose or draw 0 against a perfect opponent, then all of the agent's available Q values will always be 0. The agent would then be faced with no way to choose between defensive plays that force a draw and mistake plays that allow the opponent to win. In turn that means some chance that the opponent will win, even when the agent had learned optimal play.When two agents learn through self-play using the +1, 0, 0 reward scheme it gets more complicated. That is because the opponent's behaviour is part of the Q value system. Some positions will have more opportunities for the opponent to make mistakes, and score more highly. A mistake that allows an opponent to force a win will actually score worse, because the opponent will not make mistakes once it has a sure route to a +1 score. So even though the agent apparently cannot tell the difference between a loss and a draw, it should still at least partially learn to avoid losses. In fact, without running the experiment, I am not sure whether this would be enough to still learn optimal play. Intuitively, I think it would be possible for the +1, 0, 0 agent to still learn optimal play, although maybe more slowly than the +1, 0, -1 system, because any situation that gave an opponent a chance of winning would allow it to pick the move with best score, reducing the first agents score for a move that arrived there - and this difference will be backed up to earlier positions. However, the learning would become unstable as described above, as against a perfect opponent the difference disappears as all the best options are draws or losses, and the agent will start to make mistakes again."
"What's the commercial usage of ""image captioning""?","
If ""image captioning"" is utilized to make a commercial product, what application fields will need this technique? And what is the level of required performance for this technique to be usable?
","['deep-learning', 'natural-language-processing', 'image-recognition']","If ""image captioning"" is utilized to make a commercial product, what application fields will need this technique?There are several important use case categories for image captioning, but most are components in larger systems, web traffic control strategies, SaaS, IaaS, IoT, and virtual reality systems, not as much for inclusion in downloadable applications or software sold as a product. These are a few examples.The first two are usually monetized in ways such as these.Improvements in online purchase daily volumeAcquisition of contact lists with consumer or business interests as fields in the list for marketing purposesAdditional draw of web traffic to enhance ad impression revenueWhat is the level of required performance can this technique be usable?Performance is in terms of these system qualities.The quantification of cost, accuracy, and reliability is business dependent. Some may have a nasty negative effect on the business if the caption is wrong or missing. Others use cases may not.In some cases the average revenue generated by the caption's presence is already known and known to be small, which requires that the CNN run time and computing resource requirements must be kept below that. In other cases, a Fortune 500 company CTO said, ""Do it and send me the budget needed."" In such cases the budget may be, for all practical purposes, unconstrained, not that a system that wastes time and resources is ever desirable, even if only for energy conservation reasons."
How to weight important features,"
Let's say there's a ball with features position, velocity, acceleration.
These three are all concatenated as inputs to my neural network.
However, I have prior knowledge that position is way more predictive than the other features.
How do I weight the position feature much more strongly than the others? Would just applying a large scalar coefficient to it as preprocessing work? Seems unprincipled...
",['neural-networks'],
Natural language recommendation system: to pre-classify inputs or not?,"
Does it help to ""pre-classify"" natural language inputs using labeled input fields? E.g., ""Who,"" ""What,"" ""Where,"" ""When,"" ""Why,"" ""How,"" and ""How much?"" Or is a single, monolithic, free-form, long-text input field equally effective and efficient for model training purposes?
Scenario 1: Without input labels

We are three research fellows, Alice, Bob and Charlie at the University of Copenhagen. We want to understand the development of the human visual system. This knowledge will help in the prevention and treatment of certain vision problems in children. Further, the rules that guide development in the visual system can be applied to other systems within the brain. Our work, therefore, has wide application to other developmental disorders affecting the nervous system. We will conduct this research in 2019 under a budget of $15,000.

Scenario 2: With input lables

Who: We are three research fellows, Alice, Bob and Charlie.
What: We want to understand the development of the human visual system.
Where: At the University of Copenhagen.
When: During the calendar year of 2019.
Why: This knowledge will help in the prevention and treatment of certain vision problems in children.
How: Further, the rules that guide development in the visual system can be applied to other systems within the brain.
How Much: The research will cost $15,000.

Use Case:
I am building an AI/ML recommendation system. Users subscribe to the system to get recommendations of research projects they might like to participate in or fund. There will be many projects from all over the globe. Far too many for a human to sort through and filter. So AI will sort and filter automatically.
Will pre-classifying input fields using labels help the training algorithm be more efficient or effective?
","['classification', 'natural-language-processing']",
How to design an AI that discovers more complex concepts on its own?,"
How would I go about designing a (relatively) simple AI that discovers and invents random more complex concepts on its own?
For example, say I had a robot car. It doesn't know it's a car. It has several inputs and outputs, such as a light sensor and the drive motors. If it stays in the dark, it's score drops (bad), and if it moves into the light, it's score rises (good). It'd have to discover that it's motor outputs cause the light input to change (because it's moving closer or farther away from a light source), and that brighter light means higher score.
Of course, it'd be easier to design an AI that does specifically that, but I want its behaviour discovery system to be more generic, if that makes any sense. Like later on, it could discover a way to fight or cooperate with other robots to increase its score (maybe other robots destroy light sources when they drive over them, and they can be disabled by driving into them), but it'd have to discover this without initially knowing that another robot could possibly exist, how to identify one, what they do, and how to interact with one.
Also, I want it to be creative instead of following a 'do whatever is best to increase your score' rule. Like maybe one day it could decide that cooperating with other robots is another way to increase its score (it finds out what love is), but if it's unable to do that, it becomes depressed and stops trying to increase it's score and just sits there and dies. Or it could invent any other completely random and possibly useless behavior.
How hard would it be to make something like this, that essentially builds itself up from a very basic system, provided I give it lots of different kinds of inputs and outputs that it can discover how to use and apply to its own evolving behavior?
","['ai-design', 'evolutionary-algorithms']",
Suitable reward function for trading buy and sell orders,"
I am working to build a deep reinforcement learning agent which can place orders (i.e. limit buy and limit sell orders). The actions are {""Buy"": 0 , ""Do Nothing"": 1, ""Sell"": 2}.
Suppose that all the features are well suited for this task. I wanted to use just the standard ""Profit & Loss"" as a reward, but I hardly thought to get something similar to the above image. The standard P&L will simply place the pair (limit buy order, limit sell order) on every up movement. I don't want that because very often it won't cover the commission and it is not a good indicator to trade manually. I would be interested that the agent can maximize the profit and give me a minimum profit of $100 on every pair (limit buy order, limit sell order).
I would be interested in something similar to the picture below.

Is there a reward function that could allow me to get such a result? If so, what is it?
UPDATE
Is the following utility function can work with the purpose of that question?
$$
U(x) = \max(\\\$100, x)
$$
That seems correct, but I don't know how the agent will be penalized if it covers a wrong transaction, i.e. the pair (limit buy order, limit sell order) creates a loss of money.
","['reinforcement-learning', 'deep-rl', 'reward-functions', 'reward-design', 'algorithmic-trading']",
How do I combine two electromagnetic readings to predict the position of a sensor?,"
I have an electromagnetic sensor and electromagnetic field emitter.
The sensor will read power from the emitter. I want to predict the position of the sensor using the reading.
Let me simplify the problem, suppose the sensor and the emitter are in 1 dimension world where there are only position X (not X,Y,Z) and the emitter emits power as a function of distance squared.
From the painted image below, you will see that the emitter is drawn as a circle and the sensor is drawn as a cross.

E.g. if the sensor is 5 meter away from the emitter, the reading you get on the sensor will be 5^2 = 25. So the correct position will be either 0 or 10, because the emitter is at position 5.
So, with one emitter, I cannot know the exact position of the sensor. I only know that there are 50% chance it's at 0, and 50% chance it's at 10.
So if I have two emitters like the following image:

I will get two readings. And I can know exactly where the sensor is. If the reading is 25 and 16, I know the sensor is at 10. 
So from this fact, I want to use 2 emitters to locate the sensor.
Now that I've explained you the situation, my problems are like this:

The emitter has a more complicated function of the distance. It's
not just distance squared. And it also have noise. so I'm trying to
model it using machine learning.
Some of the areas, the emitter don't work so well. E.g. if you are
between 3 to 4 meters away, the emitter will always give you a fixed
reading of 9 instead of going from 9 to 16.
When I train the machine learning model with 2 inputs, the
prediction is very accurate. E.g. if the input is 25,36 and the
output will be position 0. But it means that after training, I
cannot move the emitters at all. If I move one of the emitters to be
further apart, the prediction will be broken immediately because the
reading will be something like 25,49 when the right emitter moves to
the right 1 meter. And the prediction can be anything because the
model has not seen this input pair before. And I cannot afford to
train the model on all possible distance of the 2 emitters.
The emitters can be slightly not identical. The difference will
be on the scale. E.g. one of the emitters can be giving 10% bigger
reading. But you can ignore this problem for now.

My question is How do I make the model work when the emitters are allowed to move? Give me some ideas.
Some of my ideas:

I think that I have to figure out the position of both
emitters relative to each other dynamically. But after knowing the
position of both emitters, how do I tell that to the model?
I have tried training each emitter separately instead of pairing
them as input. But that means there are many positions that cause
conflict like when you get reading=25, the model will predict the
average of 0 and 10 because both are valid position of reading=25.
You might suggest training to predict distance instead of position,
that's possible if there is no problem number 2. But because
there is problem number 2, the prediction between 3 to 4 meters away
will be wrong. The model will get input as 9, and the output will be
the average distance 3.5 meters or somewhere between 3 to 4 meters.
Use the model to predict position
probability density function instead of predicting the position.
E.g. when the reading is 9, the model should predict a uniform
density function from 3 to 4 meters. And then you can combine the 2
density functions from the 2 readings somehow. But I think it's not
going to be that accurate compared to modeling 2 emitters together
because the density function can be quite complicated. We cannot
assume normal distribution or even uniform distribution.
Use some kind of optimizer to predict the position separately for each 
emitter based on the assumption that both predictions must be the same. If 
the predictions are not the same, the optimizer must try to move the 
predictions so that they are exactly at the same point. Maybe reinforcement 
learning where the actions are ""move left"", ""move right"", etc.

I told you my ideas so that it might evoke some ideas in you. Because this is already my best but it's not solving the issue elegantly yet.
So ideally, I would want the end-to-end model that are fed 2 readings, and give me position even when the emitters are moved. How would I go about that?
PS. The emitters are only allowed to move before usage. During usage or prediction, the model can assume that the emitter will not be moved anymore. This allows you to have time to run emitters position calibration algorithm before usage. Maybe this will be a helpful thing for you to know.
","['machine-learning', 'deep-learning', 'probability']",
How can I systematically learn about the theory of neural networks?,"
I have seen a few articles about neural nets. Mostly they went along these lines: we tried these architectures, these meta parameters, we trained it for $x$ hours on $y$ CPUs, and it gave us these results that are 0.1% better than state of the art.
What I am interested in is whether there exists (at least as a work in progress) a framework that gives some explanation why is some architecture better than other, what makes one activation function more suitable for image recognition than another, etc.
Do you have some tips about where to start? I would prefer something more systematic than a Google search (a book, a list of key articles is ideal).
","['neural-networks', 'reference-request', 'hyperparameter-optimization']","Good question, there is a lot of work in that field. The first part before saying which machine learning algorithm is better and why is defining the problem. Is the problem an optimisation, classification, anomaly detection problem because then you need to use the appropriate machine learning algorithms. Let's assume its an optimisation problem.Is this problem, dynamic, or static. Is this time series data? So we need to understand the problem.Each optimisation problem has a landscape or a fitness landscape. Computer science has some nice toy landscapes. There is a lot of work in determining the nature of the problem landscape. Have a look at K Malan's work.Once you can identify the problem space and understand it's characteristics then you can start to identify what machine learning functions work well on what kind of landscapes. This is a totally different field of research. For example some researchers are working on how different evolutionary algorithms handle different landscapes, or neural networks handle different landscapes.Start by exploring the types of machine learning problems.
Understand the complexity of the problem, followed by classification of machine learning algorithms for specific problem spaces. "
"For some reasons, a reward becomes a penalty if","
I am working to build a reinforcement agent with DQN. The agent would be able to place buy and sell orders for a day trading purpose. I am facing a little problem with that project. The question is ""how to tell the agent to maximize the profit and avoid the transaction where the profit is less than 100$"".
I want to maximize the profit inside a trading day and avoid to place the pair (limit buy order, limit sell order) if the profit on that transaction is less than 100$. The idea here is to avoid the little noisy movements. Instead, I prefer long beautiful profitable movements. Be aware that I thought using the ""Profit & Loss"" as the reward.
""I want the minimal profit per transaction to be 100$"" ==> It seems this is not something that is enforceable. I can train the agent to maximize profit per transaction, but how that profit is cannot be ensured.
At the beginning, I wanted to tell the agent, if the profit of a transaction is 50 dollars, I will remove 100 dollars, then It becomes a penalty of 50 dollars for the agent. I thought it was a great way to tell the agent to not place a limit buy order if you are not sure it will give us a minimal profit of 100$. It seems that all I would be doing there is simply shifting the value of the reward. The agent only cares about maximizing the sum of rewards and not taking care of individual transactions. 
How to tell the agent to maximize the profit and avoid the transaction where the profit is less than 100$? With that strategy, what guarantee that the agent will never make a buy/sell decision that results in less than 100 dollars profit? Does the sum of reward - # transaction * 100 can be a solution?
","['reinforcement-learning', 'dqn', 'rewards']",
Training the generator in a GAN pair with back propagation,"
For the purposes of this question I am asking about training the generator, assume that training the discriminator is another topic.
My understanding of generative adversarial networks is that you feed random input data to the generator and it generates images. Out of those images, the ones which the discriminator thinks are real are used to train the generator.
For example, I have the random inputs $i_1$, $i_2$, $i_3$, $i_4$... from which the generator produces $o_1$, $o_2$, $o_3$, $o_4$. Say for example, the discriminator thinks that $o_1$ and $o_2$ are real but $o_3$ and $o_4$ are fake, I then throw away input output pairs 3 and 4, but keep 1 and 2, and run back propagation on the generator to tell it that $i_1$ should produce $o_1$, and $i_2$ should produce $o_2$ since these were ""correct"" according to the discriminator.
The contradiction seems to come from the fact that the generator already generates those outputs from those inputs, so nothing will be gained by running backprop on those input output pairs.
Where is the flaw in my logic here? I seem to have something wrong in my reasoning, or a misunderstanding of how the generator is trained.
","['backpropagation', 'generative-adversarial-networks']",
"Why would we use attention in convolutional neural networks, and how would we apply it?","
Attention has been used widely in recurrent networks to weight feature representations learned by the model. This is not a trivial task since recurrent networks have a hidden state that captures sequence information. The hidden state can be fed into a small MLP that produces a context vector summarizing the salient features of the hidden state.
In the context of NLP, convolutional networks are not as straightforward. They have the notion of channels that are different feature representations of the input, but are channels the equivalent to hidden states? Particularly, this raises two questions for me:

Why use attention in convolutional networks at all? Convolutions have shown to be adept feature detectors––for example, it is known that higher layers learn small features such as edges while lower layers learn more abstract representations. Would attention be used to sort through and weigh these features?

In practice, how would attention be applied to convolutional networks? The output of these networks is usually (batch, channels, input_size) (at least in PyTorch), so how would the attention operations in recurrent networks be applied to the output of convolutional networks?



References
Convolutional Sequence to Sequence Learning, Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin, 2017
","['deep-learning', 'convolutional-neural-networks', 'recurrent-neural-networks', 'attention']",
How do I compute the variance of the return of an evaluation policy using two behaviour policies?,"
Suppose there is an evaluation policy called $\pi_{e}$ and there are two behavior policies $\pi_{b1}$ and $\pi_{b2}$. I know that it is possible to estimate the return of policy $\pi_{e}$ through behavior policies via importance sampling, which is unbiased. But I do not know about the variance of return estimated through two behavior policies $\pi_{b1}$ and $\pi_{b2}$. Does anybody know about the variance or any bound on the variance of estimated return?
Let $G_{0}^{b1}=\sum_{t=1}^{T}\gamma^{t-1}r_{t}^{b1}$ represent the total return for an episode through behavior policy $\pi_{b1}$ and $G_{0}^{b2}=\sum_{t=1}^{T}\gamma^{t-1}r_{t}^{b2}$ represent the total return for an episode through behavior policy $\pi_{b2}$.
It is possible to estimate the return of policy $\pi_{e}$ as follows:
$$G_{0}^{(e,b1)}=\prod_{t=1}^{T}\frac{\pi_{e}(a_{t}|s_{t})}{\pi_{b1}(a_{t}|s_{t})}*G_{0}^{b1}$$
$$G_{0}^{(e,b2)}=\prod_{t=1}^{T}\frac{\pi_{e}(a_{t}|s_{t})}{\pi_{b2}(a_{t}|s_{t})}*G_{0}^{b2}$$
I want to compare the variance of $G_{0}^{(e,b1)}$ and $G_{0}^{(e,b2)}$. Is there any formulation to compute the variance $G_{0}^{(e,b1)}$ and $G_{0}^{(e,b2)}$?
","['reinforcement-learning', 'policies', 'off-policy-methods']",
Why are lambda returns so rarely used in policy gradients?,"
I've seen the Monte Carlo return $G_{t}$ being used in REINFORCE and the TD($0$) target $r_t + \gamma Q(s', a')$ in vanilla actor-critic. However, I've never seen someone use the lambda return $G^{\lambda}_{t}$ in these situations, nor in any other algorithms.
Is there a specific reason for this? Could there be performance improvements if we used $G^{\lambda}_{t}$?
","['reinforcement-learning', 'policy-gradients', 'reinforce', 'return', 'td-lambda']","That can be done. For example, Chapter 13 of the 2nd edition of Sutton and Barto's Reinforcement Learning book (page 332) has pseudocode for ""Actor Critic with Eligibility Traces"". It's using $G_t^{\lambda}$ returns for the critic (value function estimator), but also for the actor's policy gradients. Note that you do not explicitly see the $G_t^{\lambda}$ returns mentioned in the pseudocode. They are being used implicitly through eligibility traces, which allow for an efficient online implementation (the ""backward view"").I do indeed have the impression that such uses are fairly rare in recent research though. I haven't personally played around with policy gradient methods to tell from personal experience why that would be. My guess would be that it is because policy gradient methods are almost always combined with Deep Neural Networks, and variance is already a big enough problem in training these things without starting to involve long-trajectory returns. If you use large $\lambda$ with $\lambda$-returns, you get low bias, but high variance. For $\lambda = 1$, you basically get REINFORCE again, which isn't really used much in practice, and has very high variance. For $\lambda = 0$, you just get one-step returns again. Higher values for $\lambda$ (such as $\lambda = 0.8$) tend to work very well in my experience with tabular methods or linear function approximation, but I suspect the variance may simply be too much when using DNNs.Note that it is quite popular to use $n$-step returns with a fixed, generally fairly small, $n$ in Deep RL approaches. For instance, I believe the original A3C paper used $5$-step returns, and Rainbow uses $3$-step returns. These often work better in practice than $1$-step returns, but still have reasonably low variance due to using small $n$."
Can NEAT produce neural networks where inputs are directly connected to outputs?,"
Can NEAT produce neural networks where inputs are directly (without intermediate hidden neurons) connected to outputs?
","['neural-networks', 'evolutionary-algorithms', 'neat', 'neuroevolution']","Yes, it is possible (depending on the nature of your problem), using the four types of standard NEAT mutation, but it is improbable.When the NEAT algorithm begins, it operates on a blank canvas. After each generation, the algorithm will either:Construct a new axonConstruct a new node on an existing axonUpdate existing weights/biasRemove a node or axon from the network(However, in general, NEAT does not produce neural networks where two input (or output) nodes are connected.)"
Method to check goodness of combinatorial optimization algorithm implementation,"
How do I check which algorithm solves my problem best?
Given a optimaization problem, I apply different well known optimization algorithms (genetic algorithm, simulated annealing, ant colony etc.) to solve my problem. However, how do I know if my implementation ( e.g. cost function) is working for every case? How can I compare the algorithms or their goodness in the context of my problem?
",['optimization'],
Where does the expectation term in the derivative of the soft-max policy come from?,"
At slide 17 of the David Silver's series, the soft-max policy is defined as follows
$$
\pi_\theta(s, a) \propto e^{\phi(s, a)^T \theta}
$$
that is, the probability of an action $a$ (in state $s$) is proportional to the ""exponentiated weight"".
The score function is then defined as follows
$$
\nabla_\theta \log \pi_\theta (s, a) = \phi(s, a) - 
\mathbb{E}_{\pi_{\theta}}[\phi(s, \cdot)]
$$
Where does the expectation term $\mathbb{E}_{\pi_{\theta}}[\phi(s, \cdot)]$ come from?
","['reinforcement-learning', 'math', 'policies', 'expectation']",
Pixel-Level Detection of Each Object of the Same Class In an Image,"
I have source data that can be represented as a 2D image of many similar curves. They may oftentimes cross over one another, so regions of interest will overlap.
My goal is to implement a neural network solution to identify each instance or the curves and the pixels that are associated with each instance.
(Each image is simple in its representation of the data. A pixel in the image is either a point on one of these curves or it is empty. So the image is represented by one or zero at each pixel. For training purposes, I have labels for every pixel, and I have about 150,000 images. The information in the images can be noisy in that there may be omissions of points and point locations are quantized due to measurement limitations and preprocessing for the image preparation.)
I started looking into what semantic segmentation can do for me, but since all of the instances are of the same class, distinguished mainly by their location in the image, I don't think semantic segmentation is the type of processing I would want to perform. (Am I wrong?)
I am very interested in seeing how a neural network will work on this problem to separate each instance. 
My question is this: what is the terminology that describes the process I'm looking for? (How can I effectively research for this problem?) Is this an extension of semantic segmentation or is it referred to some other way?
","['neural-networks', 'convolutional-neural-networks', 'image-recognition']","What you want to do is instance segmentation on a pixel level.
I can point you two different way:MaskRCNN, which in a nutshell, uses Region Proposal Networks then segment in the proposed regions.Some other works which don't use Region Proposals rather, regress the pixels of the image in an embeding space to be able to apply some clustering algorithm. Here is a link to a good blog post about it: https://towardsdatascience.com/instance-embedding-instance-segmentation-without-proposals-31946a7c53e1"
What to study for this simple poker game?,"
I'm a programmer with a background in mathematics, but I have no experience whatsoever with artificial intelligence/neural networks. I'd like to study it as a hobby, and my goal for now is to solve the following simple poker game, by letting the program play against itself:
We have two players, each with a certain number of chips. At the start of the game, they are obligated to put a certain amount of chips in the pot. Then they each get a random real number between 0 and 10. They know their own number, but not the one of their opponent. Then we have one round of betting. The first player puts additional chips in the pot (some number between 0 and their stack size). The second player can either fold (put no additonal chips in the pot, 1st player gets the entire pot), call (put the same number of chips in the pot, player with highest number gets the pot) or raise (put even more chips in the pot, action back on 1st player). There is no limit to the amount of times  a player can raise, as long as he still has chips behind to raise.
I have several questions:
- Is this indeed a problem that can be solved with neural networks?
- What do you recommend me to study in order to solve this problem?
- Is it feasible to solve this game when allowing for continuous bet/raise sizes? Or should I limit it to a few options as a percentage of the pot?
- Do you expect it to be possible to get close to an equilibrium with one nightly run on an 'average' laptop?
","['neural-networks', 'game-ai', 'game-theory', 'imperfect-information']",
Why isn't my Q-Learning agent able to play tic-tac-toe?,"
I tried to build a Q-learning agent which you can play tic tac toe against after training.
Unfortunately, the agent performs pretty poorly. He tries to win but does not try to make me 'not winning' which ends up in me beating up the agent no matter how many loops I gave him for training. I added a reward of 1 for winning the episode and it gets a reward of -0.1 when he tries to put his label on an non-empty square (after the attempt we have s = s'). I also start with an epsilon=1 which decreases in every loop to add some more randomness at the beginning because I witnessed that some (important in my opinion) states did not get updated. Since I spend some hours of debugging without noticeable progress I'd like to know what you think.
PS: Don't care about some print statements and count variables. Those where for debugging.
Code here or on Github
import numpy as np
import collections
import time

Gamma = 0.9
Alpha = 0.2


class Environment:
    def __init__(self):
        self.board = np.zeros((3, 3))
        self.x = -1  # player with an x
        self.o = 1  # player with an o
        self.winner = None
        self.ended = False
        self.actions = {0: (0, 0), 1: (0, 1), 2: (0, 2), 3: (1, 0), 4: (1, 1),
                        5: (1, 2), 6: (2, 0), 7: (2, 1), 8: (2, 2)}

    def reset_env(self):
        self.board = np.zeros((3, 3))
        self.winner = None
        self.ended = False

    def reward(self, sym):
        if not self.game_over():
            return 0
        if self.winner == sym:
            return 10
        else:
            return 0

    def get_state(self,):
        k = 0
        h = 0
        for i in range(3):
            for j in range(3):
                if self.board[i, j] == 0:
                    v = 0
                elif self.board[i, j] == self.x:
                    v = 1
                elif self.board[i, j] == self.o:
                    v = 2
                h += (3**k) * v
                k += 1
        return h

        def random_action(self):
            return np.random.choice(self.actions.keys())

    def make_move(self, player, action):
        i, j = self.actions[action]
        if self.board[i, j] == 0:
            self.board[i, j] = player

    def game_over(self, force_recalculate=False):
        # returns true if game over (a player has won or it's a draw)
        # otherwise returns false
        # also sets 'winner' instance variable and 'ended' instance variable
        if not force_recalculate and self.ended:
            return self.ended

        # check rows
        for i in range(3):
            for player in (self.x, self.o):
                if self.board[i].sum() == player*3:
                    self.winner = player
                    self.ended = True
                    return True

        # check columns
        for j in range(3):
            for player in (self.x, self.o):
                if self.board[:, j].sum() == player*3:
                    self.winner = player
                    self.ended = True
                    return True

        # check diagonals
        for player in (self.x, self.o):
            # top-left -> bottom-right diagonal
            if self.board.trace() == player*3:
                self.winner = player
                self.ended = True
                return True
            # top-right -> bottom-left diagonal
            if np.fliplr(self.board).trace() == player*3:
                self.winner = player
                self.ended = True
                return True

        # check if draw
        if np.all((self.board == 0) == False):
            # winner stays None
            self.winner = None
            self.ended = True
            return True

        # game is not over
        self.winner = None
        return False

    def draw_board(self):
        for i in range(3):
            print(""-------------"")
            for j in range(3):
                print(""  "", end="""")
                if self.board[i, j] == self.x:
                    print(""x "", end="""")
                elif self.board[i, j] == self.o:
                    print(""o "", end="""")
                else:
                    print(""  "", end="""")
            print("""")
        print(""-------------"")




class Agent:
    def __init__(self, Environment, sym):
        self.q_table = collections.defaultdict(float)
        self.env = Environment
        self.epsylon = 1.0
        self.sym = sym
        self.ai = True

    def best_value_and_action(self, state):
        best_val, best_act = None, None
        for action in self.env.actions.keys():
            action_value = self.q_table[(state, action)]
            if best_val is None or best_val < action_value:
                best_val = action_value
                best_act = action
        return best_val, best_act

    def value_update(self, s, a, r, next_s):
        best_v, _ = self.best_value_and_action(next_s)
        new_val = r + Gamma * best_v
        old_val = self.q_table[(s, a)]
        self.q_table[(s, a)] = old_val * (1-Alpha) + new_val * Alpha

    def play_step(self, state, random=True):
        if random == False:
            epsylon = 0
        cap = np.random.rand()
        if cap > self.epsylon:
            _, action = self.best_value_and_action(state)
        else:
            action = np.random.choice(list(self.env.actions.keys()))
            self.epsylon *= 0.99998
        self.env.make_move(self.sym, action)
        new_state = self.env.get_state()
        if new_state == state and not self.env.ended:
            reward = -5
        else:
            reward = self.env.reward(self.sym)
        self.value_update(state, action, reward, new_state)


class Human:
    def __init__(self, env, sym):
        self.sym = sym
        self.env = env
        self.ai = False

    def play_step(self):
        while True:
            move = int(input('enter position like: \n0|1|2\n------\n3|4|5\n------\n6|7|8'))
            if move in list(self.env.actions.keys()):
                break
        self.env.make_move(self.sym, move)



def main():
    env = Environment()
    p1 = Agent(env, env.x)
    p2 = Agent(env, env.o)
    draw = 1
    for t in range(1000005):

        current_player = None
        episode_length = 0
        while not env.game_over():
            # alternate between players
            # p1 always starts first
            if current_player == p1:
                current_player = p2
            else:
                current_player = p1

            # current player makes a move
            current_player.play_step(env.get_state())

        env.reset_env()

        if t % 1000 == 0:
            print(t)
            print(p1.q_table[(0, 0)])
            print(p1.q_table[(0, 1)])
            print(p1.q_table[(0, 2)])
            print(p1.q_table[(0, 3)])
            print(p1.q_table[(0, 4)])
            print(p1.q_table[(0, 5)])
            print(p1.q_table[(0, 6)])
            print(p1.q_table[(0, 7)])
            print(p1.q_table[(0, 8)])
            print(p1.epsylon)

    env.reset_env()
    # p1.sym = env.x

    while True:
        while True:
            first_move = input(""Do you want to make the first move? y/n :"")
            if first_move.lower() == 'y':
                first_player = Human(env, env.x)
                second_player = p2
                break
            else:
                first_player = p1
                second_player = Human(env, env.o)
                break
        current_player = None

        while not env.game_over():
            # alternate between players
            # p1 always starts first
            if current_player == first_player:
                current_player = second_player
            else:
                current_player = first_player
            # draw the board before the user who wants to see it makes a move

            if current_player.ai == True:
                current_player.play_step(env.get_state(), random=False)
            if current_player.ai == False:
                current_player.play_step()
            env.draw_board()
        env.draw_board()
        play_again = input('Play again? y/n: ')
        env.reset_env()
        # if play_again.lower != 'y':
        #     break


if __name__ == ""__main__"":
    main()

","['reinforcement-learning', 'q-learning', 'game-ai', 'combinatorial-games', 'tic-tac-toe']","The $Q$-learning rule that you have implemented updates $Q(S_t, A_t)$ estimates as follows, after executing an action $A_t$ in a state $S_t$, observing a reward $R_t$, and reaching a state $S_{t+1}$ as a result:$$Q(S_t, A_t) \gets (1 - \alpha) Q(S_t, A_t) + \alpha (R_t + \gamma \max_a Q(S_{t+1}, a))$$The implementation seems to be correct for the traditional setting for which $Q$-learning is normally described; single-agent MDPs. The problem is that you have a multi-agent setting, in which $Q$-learning is not always directly applicable.Now, as far as I can see from a very quick glance at your code, it seems like you actually already have taken some important steps towards allowing it to work, and I think it should be quite close to almost working (at least for a simple game like Tic-Tac-Toe) already. Important things that you appear to already be doing correctly:I think the major issue that remains to be solved is in how you define the subsequent state $S_{t+1}$ after making a move in a state $S_t$.The update target that the $Q$-learning update rule moves its $Q$-value estimates towards consists of two components:The problem is that, in your implementation, $S_{t+1}$ is a state in which the opponent is allowed to make the next move $a$, rather than the RL agent. This means that $\max_a Q(S_{t+1}, a)$ is an incredibly optimistic, naive, unrealistic estimate of future returns. In fact, $\min_a Q(S_{t+1}, a)$ would be a much more realistic estimate (against an optimally-playing opponent), because the opponent gets to pick the next action $a$.I think switching in $\min_a Q(S_{t+1}, a)$ rather than the $\max$ may have a good chance of working in this scenario, but I'm not 100% sure. It wouldn't be a ""pretty"" solution though, since you'd no longer be doing $Q$-learning, but something else altogether.The proper $Q$-learning update may work well if you only present states to agents in which they're actually allowed to make the next move in the update rule. Essentially, you'd be plugging $\max_a Q(S_{t + 2}, a)$ into the update rule, replacing $S_{t+1}$ with $S_{t+2}$. Well... that's what you'd be doing in most cases. The only exception to be aware of would be terminal states. If an agent makes a move that leads to a terminal state, you should make sure to also run an additional update for that agent with the terminal game state $S_{t+1}$ (where $Q(S_{t+1}, a)$ will always be $0$ for any action $a$ if $S_{t+1}$ is terminal).For a very closely related question, where I essentially provided an answer in the same spirit, see: How to see terminal reward in self-play reinforcement learning?"
What is the difference between human brains and neural networks? [duplicate],"







This question already has answers here:
                                
                            




How are Artificial Neural Networks and the Biological Neural Networks similar and different?

                                (3 answers)
                            

Closed 2 years ago.



There are many people trying to show how neural networks are still very different from humans, but I fail to see in what way human brains are different from neural models in anything but complexity.
The way we learn is similar, the way we process information is similar, the ways we predict outcomes and generate outputs are similar. Give a model enough processing power, enough training samples, and enough time and you can train a human.
So, what is the difference between human (brains) and neural networks?
","['neural-networks', 'comparison', 'neuroscience']","One incredibly important difference between humans and NNs is that the human brain is the result of billions of years of evolution whereas NNs were partially inspired by looking at the result and thinking ""... we could do that"" (utmost respect for Hubel and Wiesel).Human brains (and in fact anything biological really) have an embedded structure to them within the DNA of the animal. DNA has about 4 MB of data and incredibly contains the information of where arms go, where to put sensors and in what density, how to initialize neural structures, the chemical balances that drive neural activation, memory architecture, and learning mechanisms among many many other things. This is phenomenal. Note, the placement of neurons and their connections isn't encoded in dna, rather the rules dictating how these connections form is. This is fundamentally different from simply saying ""there are 3 conv layers then 2 fully connected layers..."". 
There has been some progress at neural evolution that I highly recommend checking out which is promising though.Another important difference is that during ""runtime"" (lol), human brains (and other biological neural nets) have a multitude of functions beyond the neurons. Things like Glial cells. There are about 3.7 Glial cells for every neuron in your body. They are a supportive cell in the central nervous system that surround neurons and provide support for and insulation between them and trim dead neurons. This maintenance is continuous update for neural structures and allows resources to be utilized most effectively. With fMRIs, neurologists are only beginning to understand the how these small changes affect brains. This isn't to say that its impossible to have an artificial NN that can have the same high level capabilities as a human. Its just that there is a lot that is missing from our current models. Its like we are trying to replicate the sun with a campfire but heck, they are both warm."
Comparing and studying Loss Functions,"
I have a Deep Feedforward Neural Network $F: W \times \mathbb{R}^d \rightarrow \mathbb{R}^k$ (where $W$ is the space of the weights) with $L$ hidden layers, $m$ neurones per layer and ReLu activation. The output layer has a softmax activation function.
I can consider two different loss functions: 
$L_1 = \frac{1}{2} \sum_i || F(W,x_i) - y||^2$ $
 \ \ \ $ and   $\ \ \ L_2  = -\sum_i log(F(w,x_i)_{y_i})$
where the first one is the classic quadratic loss and the second one is cross entropy loss. 
I'd like to study the norm of the derivative of the loss function and see how the two are related, which means: 
1) Let's assume I know that $|| \frac{\partial L_2(W, x_i)}{\partial W}|| > r$, where $r$ is a small constant. What can I assume about $|| \frac{\partial L_1(W, x_i)}{\partial W}||$ ?
2) Are there any result which tell you that, under some hypothesis (even strict ones) such as a specific random initialisation,  $|| \frac{\partial L_1(W, x_i)}{\partial W}||$ doesn't go to zero   during training?
Thank you
","['neural-networks', 'feedforward-neural-networks', 'objective-functions']","Let's first express a network of arbitrary topology and heterogeneous or homogeneous cell type arrangements as$$ N(T, H, s) := \, \big[\, \mathcal{Y} = F(P_s, \, \mathcal{X}) \,\big] \\
   s \in \mathbb{C} \; \land \; s \le S \; \text{,} $$where $S$ is the number of learning states or rounds, $N$ is the network of $T$ topology and $H$ hyper-parameter structure and values that at stage $s$ produces a $P$ parameterized function $f$ of $\mathcal{X}$ resulting in $\mathcal{Y}$. In supervised learning, the goal is that $F(P_s)$ approaches a conceptually ideal function $F_i$ as $s \rightarrow S$.The popular loss aggregation norms are not quite as the question defines them. The below more canonically expresses the level 1 and 2 norms, which systematically aggregate multidimensional disparity between an intermediate result at some stage (epoch and example index) of training and the conceptual ideal toward which the network in training is intended to converge.$$ {||F-\mathcal{Y}||}_1 = \sum{|F_i - y_i|} \\
   {||F-\mathcal{Y}||}_2 = \sqrt{\sum{(F_i - y_i)}^2} $$These equations have been mutated by various authors to make various points, but those mutations have obscured the obviousness of their original relationship. The first is where distance can be aggregated through only orthogonal vector displacements. The second is where aggregation uses the minimum Cartesian distance by extending the Pythagorean theorem.Note that quadratic loss is a term with some ambiguity. These are all broadly describable as quadratic expressions of loss.Cross entropy is an extension of Claude Shannon's information theory concepts based on the work of Bohr, Boltzmann, Gibbs, Maxwell, von Neumann, Frisch, Fermi, and others who were interested in quanta and the thermodynamic concept of entropy as a universal principle running through mater, energy, and knowledge.$$ S = k_B \log{\Omega} \\
   H(X) = - \sum_i p(x_i) \, \log_2{\, p(x_i)} \\
   H(p, \, q) = -\sum_{x \in \mathcal{X}} \, p(x) \, \log_2{\, q(x)} $$In this progression of theory, we begin with a fundamental postulate in quantum physics, where $k_B$ is Boltzmann's constant and $\Omega$ are the number of microstates for the quanta. The next relation is Shannon's adaptation for information, where $H$ is the entropy in bits, thus the $\log_2$ instead of a natural logarithm. The third relation above expresses cross-entropy in bits for features $\mathcal{X}$ is based on the Kullback-Leibler divergence.  the p-attenuated sum of bits of q-information in .Notice that $p$ and $q$ are probabilities, not $F$ or $\mathcal{Y}$ values, so one cannot substitute labels and outputs of a network into them and retain the meaning of cross entropy. Therefore level 1 and 2 norms are closely related, but cross-entropy is not a norm; it is the dispersion of one thing Cartesian distance aggregation scheme like them. Cross-entropy is remotely related but is statistically more sophisticated. To produce a cross-entropy loss function of form$$ {||F-\mathcal{Y}||}_H = \mathcal{P}(F, y) \; \text{,} $$one must derive the probabilistic function $\mathcal{P}$ that represents the cross entropy for two distributions in some way that is theoretically sound on the basis of both information theory and convergence resource conservation. It is not clear that the interpretation of cross entropy in the context of gradient descent and back propagation has caught up with the concepts of entropy in quantum theory. That's an area needing further research and deeper theoretical consideration.In the question, the cross-entropy expression is not properly characterized, most evident in the fact that the expression is independent of the labels $\mathcal{Y}$, which would be fine if for unsupervised learning except that no other basis for evaluation is represented in the expression. For the term cross-entropy to be valid, the basis for evaluation must include two distributions, a target one and one that represents the current state of learning.The derivatives of the three norms (assuming the cross entropy is properly characterized) can be studied for the case of $\ell$ ReLU layers by generalizing the chain rule (from differential calculus) as applied to ReLU and the loss function developed by applying each of the three norms to aggregate measures of disparity from optimal.Regarding the inference in sub-question (1) nothing of particular value can be assumed about the Jacobians of level 2 norms from level 1 norms, both with respect to parameters $P$ or vice versa, except the retention of sign. This is because we cannot determine much about the correlation between the output channels of the network.There is no doubt, regarding sub-question (2), that some constraint, set of constraints, stochastic distribution applied to initialization, hyper-parameter settings, or data set features, labels, or number of examples have implications for the reliability and accuracy of convergence. The PAC (probably approximately correct) learning framework is one system of theory that approaches this question with mathematical rigor. One of its practical uses, among others, is to derive inequalities that predict feasibility in some cases and produce more lucid approaches to learning system projects."
How to analyze data before going for machine learning training?,"
For example, I have the following csv: training.csv
I want to know how I can determine which column will be the best feature for getting the output prediction before I go for machine training.
Please do share your responses
","['neural-networks', 'machine-learning', 'problem-solving', 'feature-selection']",
"Could it be possible to detect text, symbols, and components directly in a scanned PDF file with a program like Tensorflow or another program?","
I have this problem where I need to get information out of PDF document sent from a scanner. The program needs to be learnable in some way to recognize what different figures mean. Most of this should happen without human interference so it could just give a result after scanning the file.
Do anyone know if it's possible to do with a machine learning program or any alternative way?
","['problem-solving', 'computer-programming']",
What is a bad local minimum in machine learning?,"
What is ""bad local minima""? 
The following papers all mention this expression.

Eliminating all bad Local Minima from Loss Landscapes without even adding an Extra Unit
limination of All Bad Local Minima in Deep Learning
Adding One Neuron Can Eliminate All Bad Local Minima

","['machine-learning', 'deep-learning', 'terminology', 'papers', 'calculus']","The adjective bad isn't mathematically descriptive. A better term is sub-optimal, which implies the state of learning might appear optimal based on current information but the optimal solution from among all possibilities is not yet located.Consider a graph representing a loss function, one of the names to measure disparity between the current learning state and the optimal. Some papers use the term error. In all learning and adaptive control cases, it is the quantification of disparity between current and optimal states. This is a 3D plot, so it only visualizes the loss as a function of two real number parameters. There can be thousands, but the two will suffice to visualize the meaning of local versus global minima. Some may recall this phenomenon from pre-calculus or analytic geometry.If pure gradient descent is used with an initial value to the far left, the local minimum in the loss surface would be located first. Climbing the slope to test the loss value at the global minimum further to the right would not generally occur. The gradient will in most cases cause the next iteration in learning algorithms to travel down hill, thus the term gradient descent.This is a simple case beyond just the visualization of two parameters, since there can be thousands of local minima in the loss surface.There are many strategic approaches to improve the speed and reliability in the search for the global minimum loss. These are just a few.Interestingly, the only way to prove that the optimal state is found is to try every possibility by checking each one, which is not nearly feasible in most cases, or by relying on a model from which the global optimal may be determined. Most theoretical frameworks target a particular accuracy, reliability, speed, and minimal input information quantity as part of an AI project, so that no such exhaustive search or model perfection is required.In practical terms, for example, an automated vehicle control system is adequate when the unit testing, alpha functional testing, and eventually beta functional testing all indicate a lower incidence of both injury and loss of property than when humans drive. It is a statistical quality assurance, as in the case of most service and manufacturing businesses.The graph above was developed for another answer, which has additional information for those interested."
How to use RNN With Attention Mechanism on Non Textual Data?,"
Recurrent Neural Networks (RNN) With Attention Mechanism is generally used for Machine Translation and Natural Language Processing. In Python, implementation of RNN With Attention Mechanism is abundant in Machine Translation (For Eg. https://talbaumel.github.io/blog/attention/, however what I would like to do is to use RNN With Attention Mechanism on a temporal data file (not any textual/sentence based data). I have a CSV file with of dimensions 21000 x 1936, which I have converted to a Dataframe using Pandas. The first column is of Datetime Format and last column consists of target classes like ""Class1"", ""Class2"", ""Class3"" etc. which I would like to identify. So in total, there are 21000 rows (instances of data in 10 minutes time-steps) and 1935 features. The last (1936th column) is the label column.
It is predominant from existing literature that an Attention Mechanism works quite well when coupled into the RNN. I am unable to locate any such implementation of RNN with Attention Mechanism, which can also provide a visualisation as well. Any help in this regard would be highly appreciated. Cheers! 
","['neural-networks', 'python', 'recurrent-neural-networks', 'attention']","Project DefinitionUse of Recurrent Artificial Network LearningIt is correct that recurrent networks are designed for temporally related data. The later variants of the original RNN design are most apt to produce favorable results. One of the most effective of these variants is the GRU network cell, which is well represented in all the main machine learning libraries, and visualization hooks in those libraries are well documented.Various Meanings of Attention MechanismsThe belief that an attention mechanism beyond those built into the RNN design are needed to emphasize important features may be over-complicating the problem.
The parameters of the GRU and the other RNN variants already focus attention on particular features during learning. Even a basic feed forward network does that, but the MLP (multilayer perceptron) does not recognize feature trends temporally, so the use of RNN variants is smart.There are other kinds of attention mechanisms that are not inside each cell of a network layer. Research into advanced attention based designs that involve oversight, various forms of feedback from the environment, recursion, or generative designs is ongoing. As the question indicates, those are targeted for natural language work. There is also attention based design for motion and facial recognition and automated walking, driving, and piloting systems. They are designed, tested, optimized, and evolving for the purpose of natural language processing or robotics, not 1,936 feature rows. It is unlikely that those systems can be morphed into something any more effective than a GRU network for this project without considerable further R&D.Output Layer and Class EncodingThe 14 labels should be coded as 14 of the 16 permutations of a 4 bit output prior to training. And the loss function should dissuade the two illegal permutations.Response to Comments[Of the] 1936 features, one of them [is] date-time timestamps and [the] rest [are] numeric. ... Can you please suggest the format of the input? Should I convert each column of feature to a list and create a list of lists or some other way around?Regardless of what types the library you use expect as inputs, the theory is clear. Features with a finite set of fixed discrete values are ordinals. The magnitude of their information content is given in bits $b$ as follows, where $p$ is the total number of possible discrete values for the feature.$$ b = \log_2 p $$This is also true of the timestamp, which has a specific possible range and time resolution, where $t_{\emptyset}$ is the initial timestamp where the project or its data began and $t_{res}$ is the time of one resolution step.$$ b_{timestamp} = \log_2 \frac {t_{max} - t_\emptyset} {t_{res}} $$The label also has a range. If the range is a fixed set of permutations, then assign an integer to each, starting with zero, to encode them. If the range of the text is unknown, use a library or utility that converts words or phrases to numbers. One popular one is word2vec.Integrating the features to reduce the number of input bits actually wastes a layer, so don't do that. The total information is given as this.$$ b_{total} = \sum_{i = 1}^{1,936} b_i $$The features, if they are real numbers, can remain so. The input layer of an artificial network expects a number entering the data flow for each cell. One can change the data type of the numbers to reduce computational complexity if no overflow or other mapping issue will occur. This is where the above information content can be useful in understanding how far one can go in collapsing the information into a smaller computational footprint."
Can't grasp MiniMax diagram (no alpha beta pruning),"

The image is one of many similar exam questions can anyone pelase help me understand it fully?
'Internal node': This is simply every node except A?
Move choices: His only options are B, C and D for this move?
Focusing on B: E=8 F=4 and G are all opponent responses, therefore they will pick the minimum value.
Now my confusion, are M N and P your known responses in the case the opponent picks G, so you should pick M=0 (the highest value), so then G gets passed 0 which the opponent should choose so B has a h-value of 0? 
Are the correct value then B=0, C=1 and D=2 so pick D as next move?
",['minimax'],
NN: Predicting choices when number of alternatives changes,"
Let's say I want to model purchase data (i.e. purchase records of many households across time). For simplicity, let's assume each household only picks one alternative at the time. A simple starting point is a multinomial logit model. Then, more flexible network architectures could be used. People have applied NN to this, but kept the number of alternatives (K) constant. In reality, the number of available options changes over time. Also, it would be interesting to predict how choices change when the number of alternatives is changed.
in bullet points

there are N households 
t_n purchases for each household
There are K_t alternatives at time t
Dependent variable Y=k indicates that alternative k was bought
Each alternative is characterized by features, so x_kt is a vector of those features (including brand name, price, ...). The number of features is constant across time. 

Any guidance or ideas?
",['neural-networks'],
How do I determine which relevant features have been learned during training in a CNN?,"
Is there any way to control the extraction of features? How do I determine which features are been learned during training, i.e relevant information is been learned or not?
","['deep-learning', 'convolutional-neural-networks', 'feature-extraction']",
What is the impact of scaling the features on the performance of the model?,"
I am trying to generate a model that uses several physicochemical properties of a molecule (including number of atoms, number of rings, volume, etc.) to predict a numeric value $Y$. I would like to use PLS Regression, and I understand that standardization is very important here. I am programming in Python, using scikit-learn.
The type and range for the features varies. Some are int64 while others are floating point numbers. Some features generally have small (positive or negative) values, while others have a very large value. I have tried using various scalers (e.g. standard scaler, normalize, min-max scaler, etc.). Yet, the R2/Q2 are still low. 
I have a few questions:

Is it possible that by scaling, some of the very important features lose their significance, and thus contribute less to explaining the variance of the response variable?
If yes, if I identify some important features (by expert knowledge), is it OK to scale other features but those? Or scale the important features only?
Some of the features, although not always correlated, have values that are in a similar range (e.g. 100-400), compared to others (e.g. -1 to 10). Is it possible to scale only a specific group of features that are within the same range?

","['machine-learning', 'python', 'data-science', 'data-preprocessing']",
Mismatch between the definition of the GAN loss function in two papers [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I was trying to understand the loss function of GANs, but I found a little mismatch between different papers.
This is taken from the original GAN paper:

The adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons. To learn the generator's distribution $p_{g}$ over data $\boldsymbol{x}$, we define a prior on input noise variables $p_{\boldsymbol{z}}(\boldsymbol{z})$, then represent a mapping to data space as $G\left(\boldsymbol{z} ; \theta_{g}\right)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\theta_{g} .$ We also define a second multilayer perceptron $D\left(\boldsymbol{x} ; \theta_{d}\right)$ that outputs a single scalar. $D(\boldsymbol{x})$ represents the probability that $\boldsymbol{x}$ came from the data rather than $p_{g}$. We train $D$ to maximize the probability of assigning the correct label to both training examples and samples from $G$. We simultaneously train $G$ to minimize $\log (1-D(G(\boldsymbol{z})))$ :
In other words, $D$ and $G$ play the following two-player minimax game with value function $V(G, D)$ :

$$
\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]
$$
Equation (1) in this version of the pix2pix paper

The objective of a conditional GAN can be expressed as
$$
\begin{aligned}
\mathcal{L}_{c G A N}(G, D)=& \mathbb{E}_{x, y}[\log D(x, y)]+\\
& \mathbb{E}_{x, z}[\log (1-D(x, G(x, z))],
\end{aligned}
$$
where $G$ tries to minimize this objective against an adversarial $D$ that tries to maximize it, i.e. $G^{*}=$ $\arg \min _{G} \max _{D} \mathcal{L}_{c G A N}(G, D)$.
To test the importance of conditioning the discriminator, we also compare to an unconditional variant in which the discriminator does not observe $x$ :
$$
\begin{aligned}
\mathcal{L}_{G A N}(G, D)=& \mathbb{E}_{y}[\log D(y)]+\\
& \mathbb{E}_{x, z}[\log (1-D(G(x, z))] .
\end{aligned}
$$

Putting aside the fact that pix2pix is using conditional GAN, which introduces a second term $y$, the 2 formulas are quite resemble, except that in the pix2pix paper, they try to get minimax of ${\cal{L}}_{cGAN}(G, D)$, which is defined to be $E_{x,y}[...] + E_{x,z}[...]$, whereas in the original paper, they define $\min\max V(G, D) = E[...] + E[...]$.
I am not coming from a good math background, so I am quite confused. I'm not sure where the mistake is, but assuming that $E$ is expectation (correct me if I'm wrong), the version in pix2pix makes more sense to me, although I think it's quite less likely that Goodfellow could make this mistake in his amazing paper. Maybe there's no mistake at all and it's me who do not understand them correctly.
","['neural-networks', 'deep-learning', 'papers', 'generative-model']",
Training an AI to recognize my voice (or any voice),"
I want to start a project for my artificial intelligence class about speaker recognition. Basically, I want to train my AI to detect if it's me who's speaking or somebody else. I would like some suggestions or libraries to work with.
","['neural-networks', 'machine-learning', 'deep-learning', 'training']","The human voice is based on the neural muscular control of vocal apparatus made up of many parts.These coordinated muscular manipulations produce envelopes (controlling) of audio that can be characterized by periodic and transient wave forms. Voices are unique to the learning state of neural activity and anatomic attributes, which is a way of saying that vocal habits and the physical attributes of the voice supports the distinguishing of vocal identity.The detection of distinguishing features of voices by the ear is equally complex. In a room full of people talking, the brain can learn to track a single voice.It is important to note that performing voice recognition to determine the identity of the human source is significantly different than performing voice recognition to produce text. To produce text accurately, the NLP must determine language elements and construct a semantic network that represents the vocal content or a text from that representation to be accurate in the case of like sounding words. Fortunately, the identification of the speaker is easier in some ways than the accurate voice to text. Unfortunately, the identification of the speaker has general limitations discussed below.The first stage of hearing in the ear is mechanical, involving the length of hairs along the cochlear surface, which is like a radio tuner that discriminates all frequencies within a range simultaneously. The software equivalent is a spectrum derived by applying a root mean square to the result of an FFT (fast Fourier transform) to provide magnitudes.$$ m_f := \sqrt{t_f^2 + {(it_f)}^2} $$The phase component of the FFT results ($\, \arctan(t, it) \,)$ can be discarded, since it is not correlated with neural control of voice.The application of the FFT to speech (as with any changing audio) requires windowing over the audio samples using one of the windowing tapers, such as the Hann window or Blackman window. The input is the audio stream or file contents as a sequence of pressure samples, the audio. The output is a sequence of spectra, each containing the volume of each frequency in the vocal range, from about 30 Hertz to 15 K Hertz.This series of spectra can be fed into the initial layer of one of the more advanced RNNs (recurrent neural networks), such as the LSTM (long short term memory) networks, its bidirectional version, the B-LSTM, or a GRU (gated recurrent network), which is touted as training equally well with less time or computing resource consumption.The identity of the speaker is the label. The series of spectra are the features.Using the PAC (probably approximately correct) learning framework, it may be possible to estimate, in advance of experimentation, the minimum number of words the speaker must speak to produce a particular accuracy and reliability in use of the learned parameters from the network training.It will take some study to set up the hyper-parameters and design the layers of the network in terms of depth (number of layers) and width sequence (number of cells per layer, which may vary from layer to layer).The use case limitation of this system is that each speaker must read some text that provides adequate training example sequences of adequate length, so that there are sufficient number of overlapping windows for the FFT to transform into spectra so that the training converges reasonably.There is no way around the individual user training as there is with recognition of linguistic content, which can be trained across a large set of speakers to recognize content somewhat independent of the speaker. The system can be adjusted and improved to minimize the amount of speech required, but information theory constraints keep that quantity from ever approaching zero.No network, whether artificial or biological, can learn something from nothing. Claude Shannon and John von Neumann realized decades ago that there is a kind of conservation of information, just as there is a conservation of matter and energy in space below nuclear reaction thresholds. This led to the definition of a bit and the formulation of information as a quantity of bits corresponding to a narrowing of probability that the information provides.$$ b_i = - \log_2 {\frac {P(x|i)} {P(x)}} $$"
Is it possible to use AI to reverse engineer software?,"
I was thinking of something of the sort:

Build a program (call this one fake user) that generates lots and lots and lots of data based on the usage of another program (call this one target) using stimuli and response. For example, if the target is a minesweeper, the fake user would play the game a carl sagan number of times, as well as try to click all buttons on all sorts of different situations, etc...
run a machine learning program (call this one the copier) designed to evolve a code that works as similar as possible to the target. 
kablam, you have a ""sufficiently nice"" open source copy of the target.

Is this possible?
Is something else possible to achieve the same result, namely, to obtain a ""sufficiently nice"" open source copy of the original target program?
","['machine-learning', 'deep-learning', 'applications']",
How to recognise metaphors in texts using NLP/NLU?,"
What are the current NLP/NLU techniques that can extract metaphors from texts?
For example

His words cut deeper than a knife.

Or a simpler form like:

Life is a journey that must be travelled no matter how bad the roads and accommodations.

","['machine-learning', 'natural-language-processing', 'natural-language-understanding', 'computational-linguistics']","This is still a research topic in linguistics. A quick google search brings up a couple of papers that might be useful:However, you probably won't get an off-the-shelf tool that recognises metaphors for you.To add more details, the problem with metaphors is that you cannot detect them by surface structure alone. Any sentence could (in theory) be a metaphor. This is different from a simile, which can usually be spotted easily through the word like, as in she runs like the wind. Obviously, like on its own is not sufficient, but it's a good starting point to identify possible candidates.However, his words cut deeper than a knife is -- on the surface -- a normal sentence. Only the semantic incongruence between words as the subject and cut as the main verb creates a clash. In order to detect this automatically, you need to identify possible semantic features of the verbal roles and look for violations of the expected pattern.The verb cut would generally expect an animate object, preferably human, or an instrument with a blade (the knife cuts through the butter) as its actor or subject. But it also can include (water)ways: the canal cuts through the landscape, the road cuts through the field. The more closely you look, the more exceptions/extensions you will find for your initial assumption.And every extension/exception will water down the accuracy of your metaphor detection algorithm.The second example is similar: Life is a journey. You could perhaps use a thesaurus and see what the hyperonyms of life are. Then you could do the same with journey, and see if they are compatible. A car is a vehicle is not a metaphor, because vehicle is a hyperonym of car. But journey is not a hyperonym of life, so could be a metaphor. But I would think that this is still very tricky to get right. In this case, the absence of a determiner might be a hint, as it's not a life is a journey -- you might restrict yourself to bare nouns for this type of metaphor. But this is also not a firm rule.In short, it is a hard problem, as you need to look at the meaning, rather than just the structure or word choice. And meaning is not easy to deal with in NLP, despite decades of work on it."
What is a continuous-attractor neural network?,"
I am reading about CANN. However, I do not seem to grasp what it is. Maybe someone who has worked with it can explain it? I found out about it while reading about RatSLAM. I understand that it helps to keep long/short term memory.
","['neural-networks', 'terminology', 'definitions']",
How do you distinguish between a complex and a simple model in machine learning?,"
How do you distinguish between a complex and a simple model in machine learning? Which parameters control the complexity or simplicity of a model? Is it the number of inputs, or maybe the number of layers?
Moreover, when should a simple model be used instead of a complex one, and vice-versa?
","['neural-networks', 'machine-learning', 'deep-learning', 'models', 'computational-learning-theory']","Consider a continuum of complexity in models.Trivial: $y = x + a$Simple: $y = x \, \log \, (a x + b) + c$Moderately complex: A wind turbine under constant wind velocityVery complex: Ray tracing of lit 3-D motion scenes to pixelsAstronomically complex: The weatherNow consider a continuum regarding the generality or specificity of models.Very specific: The robot for the Mars mission has an exact mechanical topology, materials call-out, and set of mechanical coordinates contained in the CAD files used to machine the robot's parts.Somewhat specific: The formulas guiding the design of an internal combustion engine, which are well known.Somewhat general: The phenomenon is deterministic and the variables and their domains are known.Very general: There's probably some model because it works in nature but we know little more.There are twenty permutations at the above level of granularity. Every one has purpose in mathematical analysis, applied research, engineering, and monetization.Here are some general correlations between input, output, and layer counts.Higher complexity often corresponds to larger layer count.Higher i/o dimensionality corresponds to higher width to the corresponding i/o layers.Mapping generality to or from specificity generally requires complexity.Now, to make this answer even less appealing to those who want formula answer they can memorize, ...Each artificial network is a model of an arbitrary function before training and a model of a specific function afterward.Loss functions are models of disparity.An algorithm is a model of a process created by spreading a recursive definition out in time to map into a model of centralized computation called a CPU.The recursive definition is a model too.There is almost nothing in science that is a not a model except ideas or data that are not yet modeled."
Can machine learning algorithms be used to differentiate between small differences in details between images?,"
I was wondering if machine learning algorithms (CNNs?) can be used/trained to differentiate between small differences in details between images (such as slight differences in shades of red or other colours, or the presence of small objects between otherwise very similar images?)? And then classify images based on these differences? If this is a difficult endeavour with our current machine learning algorithms, how can it be solved? Would using more data (more images) help?
I would also appreciate it if people could please provide references to research that has focused on this, if possible. 
I've only just begun learning machine learning, and this is something that I've been wondering from my research.
","['machine-learning', 'convolutional-neural-networks', 'image-recognition', 'classification']","Attentive Recurrent Comparators (2017) by Pranav Shyam et al. is an interesting paper that helps to answer the question you're wondering, along with a blog post that helps to describe it in easier terms.The way it's implemented is actually rather intuitive. If you have ever played a ""what is different"" game with two images usually what you'd do is look back and forth between the images to see what the difference is.  The network that the researchers created does just that!  It looks at one image and then remembers important features about that images and looks at the other image and goes back and forth."
How can I model regularity?,"
I have data that are a result of rules that are exceptionless. I want to my program to 'look' at my data and figure out those rules. However, the data might contain what might look like an exception (rule within a rule) but that is too, true for all occasions e.g.
All men of the dataset with x common characteristics go out for a beer on Thursday after work. That is true for all men with those characteristics. However, they will cancel their plans if their wife is sick. That last condition might initially look as an exception to the rule (go out for beer on Thursdays), but it is not as long as it is true for all men with those x characteristics. 
So the question is: Which approach/method would be suitable for this?
",['regularization'],
Extracting algebraic constraints from the input data,"
I would appreciate your help with this (naive) question of mine.
Given the set of points located on a circle, $x_{i}, y_{i}$ as the input data, Can a deep/machine learning algorithm infer that radius of the circle is constant ? In other words, given the data $x_{i}, y_{i}$ is there way that algorithm discovers the constraint: $x_{i}^2 + y_{i}^2 = \text{constant}$ ? 
I would also appreciate any related reference on the subject.    
","['machine-learning', 'deep-learning', 'training']",
Using Artificial Intelligence for Robot movement instead of regular Inverse Kinematics,"
I am currently working with classical roboticists who insist on inverse kinematics, and what I (perhaps mistakenly) call the old way of thinking about robots accomplishing tasks. 
Much of the relatively recent research focuses on Robots using Brain models such as   Multiple timescales (Artificial Intelligence models) that segment sequences and reproduce them, having learned them. The problem I face is this bunch of roboticists insist that a robot already knows the sequence, and training it to be reproduced is redundant, since a Robot can already reproduce the sequence anyway.
How accurate would you rate this assessment of using AI in robotics?
Are there any advantages of using AI to learn sequences for robot control?
","['deep-learning', 'robotics']",
Simulated Annealing: Why is e-function used as propability function to decide to accept a worse solution,"
Why is the e-function used to decide whether to accept a worse solution or not? 
To be more specific: Why was $e$ chosen as basis?
The propability to accept a worse solution is described with:
$p=e^{-\frac{E(y)-E(x)}{kT}}$
$E(y)$ is the energy from the old solution
$E(x)$ is the energy from new solution $T$ is a constant temprature decreasing with a constant factor k in every iteration.
",['algorithm'],"You can find the explanation by asking some question about the function. Suppose, the value of $\frac{E(y)-E(x)}{kT} >> 0$ is much more greater than zero. What does it mean? It means the value of $E(y)$ is much greater than $E(x)$ related to the $kT$ that is as a measure of temperature decreasing. Now, you want in this situation a probability which is near to zero. Hence, $e^{-\frac{E(y)-E(x)}{kT}}$ could be a good value for the probability of selection of worse solutions! Why $e$ instead of $2$ or other values greater than $1$? Because it could be a good function in optimization problems as its derivative is more simple than others!"
Is discrete reading in neural turing machines differentiable?,"
For a neural turing machine, there is an attention distribution over the memory cells. A read operation consists of multiplying the memory cell's value by its respective probability, and adding these results for all memory cells.
Suppose we only did the above operation for memory cells with a probability greater than 0.5, or suppose we concatenated the results instead of adding them. Can this be implemented/ trained with stochastic gradient descent? Or would it not be differentiable?
Thanks!
","['neural-networks', 'deep-learning']",
What makes a machine learning algorithm a low variance one or a high variance one?,"
Some examples of low-variance machine learning algorithms include linear regression, linear discriminant analysis, and logistic regression.
Examples of high-variance machine learning algorithms include decision trees, k-nearest neighbors, and support vector machines.
Source:

What makes a machine learning algorithm a low variance one or a high variance one? For example, why do decision trees, k-NNs and SVMs have high variance?
","['machine-learning', 'linear-regression', 'statistical-ai', 'decision-trees', 'bias-variance-tradeoff']","What this is talking about is how much a machine learning algorithm is good at ""memorizing"" the data. Decision trees, for their nature, tend to overfit very easily, this is because they can separate the space along very non-linear curves, especially if you get a very deep tree. Simpler algorithms, on the other hand, tend to separate the space along linear hyper surfaces, and therefore tend to under-fit the data and may not give very good prediction, but may behave better on new unseen data which is very different from the training data."
How does DARTS compare to ENAS?,"
How does DARTS compare to ENAS? Which one is better or what advantages does they each have?
Links:

DARTS: Differentiable Architecture Search
Efficient Neural Architecture Search via Parameter Sharing

","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'search']",
What limitations does the Markov property place on real time learning?,"
The Markov property is the dependence of a system's future state probability distribution solely on the present state, excluding any dependence on past system history. 
The presence of the Markov property saves computing resource requirements in terms of memory and processing in AI implementations, since no indexing, retrieval, or calculations involving past states is required.
However, the Markov property is often an unrealistic and too strong assumption.
Precisely, what limitations does the Markov property place on real-time learning?
","['reinforcement-learning', 'markov-decision-process', 'markov-property']",
Will a neural network always predict the correct label if it sees the exact same input during training and testing?,"
If I'm performing a text classification task using a model built in Keras, and, for example, I am attempting to predict the appropriate tag for a given Stack Overflow question:

How do I subtract 1 from an integer?

And the ground-truth tag for this question is:

objective-c

But my model is predicting:

c#

If I were to retrain my model, but this time add the above question and tag in both the training and testing data, would the model be guaranteed to predict the correct tag for this question in the test data?
I suppose the tl;dr is: Are neural networks deterministic if they encounter identical data during training and testing?
I'm aware it's not a good idea to use the same data in both training and testing, but I'm interested from a hypothetical perspective, and for gaining more insight into how neural networks actually learn. My intuition for this question is ""no"", but I'd really be interested in being pointed to some relevant literature that expands/explains that intuition.
","['neural-networks', 'deep-learning', 'classification', 'keras', 'computational-learning-theory']",
Are Neural Network layers resistent to noise?,"
Let's consider a classic feedforward neural network $F$ with input dimension $d$, output dimension $k$, $L$ layers $l_i$ with $m$ neurons each. ReLu activation.
This means that, given a point $x \in R^d$ its image $F(x) \in R^k$. Let's now assume i add some gaussian noise $\eta_i$ in EVERY hidden layer $l_i(x)$ at the same time, where the norm of this noise is 5% the norm of its layer computed on the point $x$. Let's call this new neural network $F_*$
I know that, empirically, neural networks are resistant to this kind of noise, especially on the first layers. How can i show this theoretically?
The question i'm trying to answer is the following:
After having injected this noise $\eta_i$ in every layer $l_i(x)$, how far the output $F_{*}(x)$ will be from the output of the original neural network $F(x)$?
","['neural-networks', 'deep-learning']",
How data augmentation like rotation affects the quality of detection?,"
I'm using an object detection neural network and I employ data augmentation to increase a little my small dataset. More specifically I do rotation, translation, mirroring and rescaling.
I notice that rotating an image (and thus it's bounding box) changes its shape. This implies an erroneous box for elongated boxes, for instance on the augmented image (right image below) the box is not tightly packed around the left player as it was on the original image. 
The problem is that this kind of data augmentation seems (in theory) to hamper the network to gain precision on bounding boxes location as it loosens the frame.
Are there some studies dealing with the effect of data augmentation on the precision of detection networks? Are there systems that prevent this kind of thing?
Thank you in advance!
(Obviously, it seems advisable to use small rotation angles)

","['convolutional-neural-networks', 'object-recognition']","The problem is that this kind of data augmentation seems (in theory) to hamper the network to gain precision on bounding boxes location as it loosens the frame.Yes, it is clear from your examples that the bounding boxes become wider. Generally, including large amounts of data like this in your training data will mean that your network will also have a tendency to learn slightly larger bounding boxes. Of course, if the majority of your training data still has tight boxes, it should stell tend towards learning those... but likely slightly wider ones than if the training data did not include these kinds of rotations.Are there some studies dealing with the effect of data augmentation on the precision of detection networks? Are there systems that prevent this kind of thing?(Obviously, it seems advisable to use small rotation angles)I do not personally work directly in the area of computer vision really, so I'm not sufficiently familiar with the literature to point you to any references on this particular issue. Based on my own intuition, I can recommend:"
Is it difficult to learn the rotated bounding box for a (rotated) object?,"
I have checked out many methods and papers, like YOLO, SSD, etc., with good results in detecting a rectangular box around an object, However, I could not find any paper that shows a method that learns a rotated bounding box.
Is it difficult to learn the rotated bounding box for a (rotated) object?
Here's a diagram that illustrates the problem.

For example, for this object (see this), its bounding box should be of the same shape (the rotated rectangle is shown in the 2nd right image), but the prediction result for the YOLO will be Ist right.
Is there any research paper that tackles this problem?
","['convolutional-neural-networks', 'computer-vision', 'object-detection', 'yolo']","Cartesian Bias and Pipeline EfficiencyYou are experiencing a techno-cultural artifact of Cartesian-centric imaging running all the way back to the dawn of coordinate systems. It is the momentum of practice as a consequence of applying Cartesian 2D coordinates to rasterize images appearing at the focal planes of lenses from the dawn of television and the earliest standards of raster based capture and display.Although some work was done toward adding tilt to bounding rectangles in the late 1990s and since, from a time and computing resource conservation perspective, it is computationally and programmatically less costly to include the four useless triangles of pixels and keep the bounding box orthogonal with the pixel grid.Adding a tilt angle to the bounding boxes is marginally competitive when detecting ships from a satellite only because two conditions offset the inefficiencies in that narrow domain. The ship appears as an oblong rectangle with rounded corners from a satellite positioned in geosynchronous orbit. In the general case, adding a tilt angle can slow recognition significantly.Biology Less BiasedAn interesting side note is that the neural networks of animal and human vision systems do not have that Cartesian-centricity, but that doesn't help this question's solution, since non-orthogonal hardware and software is virtually nonexistent.Early Non-Cartesian Research and Today's RasterizationGerber Scientific Techonology research and development in the 1980s (South Windsor, Connecticut, U.S.) investigated vector capture, storage, and display, but the R&D was not financially sustainable for a mid-side technology corporation for the reasons above.What remains, because it is economically viable and necessary from an animation point of view, is rasterization on the end of the system that converts vector models into frames of pixels. We see this in on the rendering  SVG, VRML, and the original intent of CUDA cores and other hardware rendering acceleration strategies and architectures.On the object and action recognition side, the support of vector models directly from imaging is much less developed. This has not been a major stumbling block for computer vision because the wasted pixels at one tilt angle may be of central importance at another tilt angle, so there are no actual wasted input pixels if the centering of key scene elements is widely distributed in translation and tilt, which is often the case in real life (although not so much in hygienically pre-processed datasets).Conventions Around Object Minus Camera Tilt and Skew from ParallaxOnce edge detection, interior-versus-exterior, and 3D solid recognition come into play, the design of CNN pipelines and the way kernels can do radial transformation without actually requiring $\; \sin, \, \cos, \, \text{and} \, \arctan \;$ functions evaporate the computational burden of the Cartesian nature of pixel tensors. The end result is that the bounding box being orthogonal to the image frame is not as problematic as it initially appears. Efforts to conserve the four triangles of pixels and pre-process orientation is often a wasted effort by a gross margin.SummaryThe bottom line is that efforts to produce vector recognition from roster inputs have been significantly inferior in terms of resource and wait time burden, with the exception of insignificant gains in the narrow domain of naval reconnaissance satellite images. Trigonometry is expensive, but convolution kernels, especially now that they are moving from software into hardware accelerated computing paths in VLSI, is computable at lower costs.Past and Current WorkBelow is some work that deals with tilting with regard to objects and the effects of parallax in relation to the Cartesian coordinate system of the raster representation. Most of the work has to do with recognizing 3D objects in a 3D coordinate system to project trajectories and pilot or drive vehicles rationally on the basis of Newtonian mechanics.Efficient Collision Detection Using Bounding Volume Hierarchies of k-DOPs, James T. Klosowski, Martin Held, Joseph S.B. Mitchell, Henry Sowizral, and Karel Zikan, 1998Sliding Shapes for 3D Object Detection in Depth Images, Shuran Song and Jianxiong Xiao, 2014Amodal Completion and Size Constancy in Natural Scenes, Abhishek Kar, Shubham Tulsiani, Joao Carreira and Jitendra Malik, 2015HMD Vision-based Teleoperating UGV and UAV for Hostile
Environment using Deep Learning, Abhishek Sawarkar1, Vishal Chaudhari, Rahul Chavan, Varun Zope, Akshay Budale and Faruk Kazi, 2016Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds, Z Liu, H Wang, L Weng, Y Yang, 2016Amodal Detection of 3D Objects:
Inferring 3D Bounding Boxes from 2D Ones in RGB-Depth Images, Zhuo Deng, 20173D Pose Regression using Convolutional Neural Networks, Siddharth Mahendran, 2017Aerial Target Tracking Algorithm Based on Faster R-CNN Combined with Frame Differencing, Yurong Yang, Huajun Gong, Xinhua Wang and Peng Sun, 2017A Semi-Automatic 2D solution for Vehicle Speed Estimation from Monocular Videos, Amit Kumar, Pirazh Khorramshahi, Wei-An Lin, Prithviraj Dhar, Jun-Cheng Chen, Rama Chellappa, 2018"
Why is the last layer of a DBN or DBM used for classification task?,"
I understand why deep generative models like  DBN ( deep belief nets ) or DBM ( deep boltzmann machines ) are able to capture underlying structures in data and use it for various tasks ( classification, regression, multimodal representations etc ...).
But for the classification tasks like in Learning deep generative models, I was wondering why the network is fine-tuned on labeled-data like a feed-forward network and why only the last hidden layer is used for classification?
During the fine-tuning and since we are updating the weights for a classification task ( not the same goal as the generative task ), could the network lose some of its ability to regenerate proper data? ( and thus to be used for different classification tasks ? )
Instead of using only the last layer, could it be possible to use a partition of the hidden units of different layers to perform the classifications task and without modifying the weights? For example, by taking a subset of hidden units of the last two layers ( sub-set of abstract representations ) and using a simple classifier like an SVM?
Thank you in advance!
","['machine-learning', 'deep-learning', 'classification', 'generative-model']",
Can neural networks learn to ignore an input datum?,"
Disclaimer: I'm not a student in computer science and most of my knowledge about ML/NN comes from YouTube, so please bear with me!

Let's say we have a classification neural network, that takes some input data $w, x, y, z$, and has some number of output neurons. I like to think about a classifier that decides how expensive a house would be, so its output neurons are bins of the approximate price of the house.
Determining house prices is something humans have done for a while, so let's say we know a priori that data $x, y, z$ are important to the price of the house (square footage, number of bedrooms, number of bathrooms, for example), and datum $w$ has no strong effect on the price of the house (color of the front door, for example). As an experimentalist, I might determine this by finding sets of houses with the same $x, y, z$ and varying $w$, and show that the house prices do not differ significantly.
Now, let's say our neural network has been trained for a little while on some random houses. Later on in the data set. it will encounter sets of houses whose $x, y, z$ and price are all the same, but whose $w$ are different. I would naively expect that at the end of the training session, the weights from $w$ to the first layer of neurons would go to zero, effectively decoupling the input datum $w$ from the output neuron. I have two questions:

Is it certain, or even likely, that $w$ will become decoupled from the layer of output neurons?
Where, mathematically, would this happen? What in the backpropagation step would govern this effect happening, and how quickly would it happen?

For a classical neural network, the network has no ""memory,"" so it might be very difficult for the network to realize that $w$ is a worthless input parameter.
Any information is much appreciated, and if there are any papers that might give me insight into this topic, I'd be happy to read them.
","['neural-networks', 'classification']",
"If expert systems are a bunch of if-then-else statements, then how are they termed as AI?","
An artificial intelligence (AI) is often defined as something that can learn over time and can imitate human behaviors.
If an Expert system (e.g. MYCIN) that only involves if-then-else statements qualifies to be an AI, then every program we write in our daily lives that involves some condition-based question answering should be an AI. Right? If not, then what should be an exact and universal definition for AI. How can a software qualify to be called AI?
","['definitions', 'expert-systems', 'ai-field']",
Is input normalization built-in into mammals sensory neurons?,"
The spectrum of human sensory inputs seems to fall within certain ranges suggesting normalization is built-in into biological NNs?
It also adapts to circumstantial conditions, e.g. people living in a city with certain factory smell eventually don't perceive the smell anymore, at least consciously (within working memory) / it adapts to a new baseline?
","['machine-learning', 'artificial-neuron', 'neurons', 'biology']","Yes, for many sensory inputs there is indeed something similar to normalization. But its not rally the same as in classical data analytics compared to what eg min/max normalization does or other technics.Lets look on some examples and considerations:mammals don't perceive heat or loudness in a linear way. This is because already many sensory receptors have chemical / physical limits. Double decibels will not perceived with double intensity. Inside your ear, the small hammer and abil will brace to protect you. --> its like normalization with logarithmic effects applied. heat perception is more like a difference integration than a absolute temperature measurement. Its measured via H+ ions flow in mitochondria in the cell (if i recall correctly)On the neuronal side gradual signals in the dendrites (analog signal) sum up gradually to later form an spike at the axon hill. where in turn a fire frequency is then encoded - the maximum frequency of this serves as a a natural maximum limit. I remember that grasshoppers increase axon fire frequency when objects started covering more ommatidial area on their ""eye"". The more of their ""eyes"" are covered by the shadow the more input on the neuron --> higher fire rate.a lot of sensory input is post processed in higher cerebral areas. Eg. compared to what is expect able and heuristics are applied to compare a signal with former events. when doing computational data analysis we may want go for accuracy and maximum comparability. Mostly on all data that could be available. --> eg. with respect to properties of a standard normal distribution. Hence we put some effort to be accurate and know the true parameters, remove outliers and so on --> big data comes into play here. 
Nature in contrast strives often for efficiency with the means of reaching the minimal required with minimal resources.Summary:
Compared to normalization in an analytical sense (eg. mean, min-max or other feature normalization techniques), nature is often only interested in the current difference between stimuli. And this only within some relevant range. Other information is not integrated. And normalization with the goal of making measurement points comparable only happens within this range of the mapping function provided by the sensor/neuron/receptor whatever! So this should also answer your question about, why you are not smelling something in the city after a while any more. However, this for sure happens at higher cerebral regions (it might also be that your smell receptors saturate) its the same principle. Your consciousness just saves energy by not concentrating on something that is anyway not changing.If you want to read more have a look here: https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law"
Using the opponent's mixed strategy in estimating the state value in minimax Q learning,"
In the paper Markov games as a framework for multi-agent reinforcement learning  (which introduces the minimax Q Learning algorithm), at the bottom left of page 3, my understanding is that the author suggests, for a simultaneous 1v1 zero-sum game, to do Bellman iterations with $$V(s)=\min_{o}\sum_{a}\pi_{a}Q(s,a,o)$$ with $\pi_{a}$ the probability of playing action $a$ for the maximizing player in his best mixed strategy to play in state $s$. 
If my understanding is correct, why does the opponent in this equation play a pure strategy ($\min_{o}$) rather than his best mixed strategy in state $s$. This would instead give $$V(s)=\sum_{o}\sum_{a}\pi_{a}\pi_{o}Q(s,a,o)$$ with $\pi_{o}$ the opponent's best mixed strategy in state $s$. Which of these two formulations is correct and why? Are they somehow equivalent?
The context of this question is that I am trying to use minimax Q learning with a Neural Network outputting the matrix $Q(s,a,o)$ for a simultaneous zero-sum game. I have tried both methods and so far have seen seemingly equally bad results, quite possibly due to bugs or other errors in my method.
","['reinforcement-learning', 'q-learning', 'minimax']","My understanding is now that the author's formula is deliberate. It seeks to learn a worst-case maximizing policy. The formula I instead suggest would, I believe, instead be Nash Q learning where the agent seeks to learn to play a Nash equilibrium.After debugging, I have gotten good results with the second formula but cannot speak for the original Minimax Q learning one."
Can gradient descent training be used for non-smooth loss functions?,"
I have non-smooth loss function $f(x) = \min(x, 0.5)$.
Can gradient descent be used for training neural networks with such functions? Can gradient descent be used for fairly general, mathematically not-nice functions?
PyTorch or TensorFlow can calculate numerically gradients from almost any function, but it is acceptable practice to use general, not-nice loss functions?
","['neural-networks', 'training', 'objective-functions', 'gradient-descent']","Gradient descent and stochastic gradient descent can be applied to any differentiable loss function irrespective of whether it is convex or non-convex. The ""differentiable"" requirement ensures that trainable parameters receive gradients that point in a direction that decreases the loss over time.In the absence of a differentiable loss function, the true gradient must be approximated through other methods. For example, in classification problems, the 0-1 loss function is considered the ""true"" loss, but it is non-convex and difficult to optimize. Instead, surrogate loss functions act as tractable proxies for true loss functions. They are not necessarily worse; negative log-likelihood loss gives a softmax distribution over $k$ classes rather than just the classification boundary.For your problem specifically, $f(x,a)=min(x,a)$ is not a differentiable loss function. It is not differentiable at $x=0.5$, but the gradient could be estimated through the subgradient. In practice, this works because neural networks often don't achieve the local/global minima of a loss function but instead asymptotically decreasing values that achieve good generalization error. Tensorflow and PyTorch use subgradients when fed non-differentiable loss functions. You could also use a smooth approximation of the $min$ function (see this thread) to get better gradients."
Why can the learning rate make the loss increase in stochastic gradient descent?,"
In Deep Learning by Goodfellow et al., I came across the following line on the chapter on Stochastic Gradient Descent (pg. 287):

The main question is how to set $\epsilon_0$. If it is too large, the
learning curve will show violent oscillations, with the cost function
often increasing significantly.

I'm slightly confused why the loss function would increase at all. My understanding of gradient descent is that given parameters $\theta$ and a loss function $\ell (\vec{\theta})$, the gradient update is performed as follows:
$$\vec{\theta}_{t+1} = \vec{\theta}_{t} - \epsilon \nabla_{\vec{\theta}}\ell (\vec{\theta})$$
The loss function is guaranteed to monotonically decrease because the parameters are updated in the negative direction of the gradient. I would assume the same holds for SGD, but clearly it doesn't. With a high learning rate $\epsilon$, how would the loss function increase in its value? Is my interpretation incorrect, or does SGD have different theoretical guarantees than vanilla gradient descent?
","['neural-networks', 'machine-learning', 'deep-learning', 'gradient-descent', 'learning-rate']",
How can the importance sampling ratio be different than zero when the target policy is deterministic?,"
In the book Reinforcement Learning: An Introduction (2nd edition) Sutton and Barto define at page 104 (p. 126 of the pdf), equation (5.3), the importance sampling ratio, $\rho _{t:T-1}$, as follows:
$$\rho _{t:T-1}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}$$
for a target policy $\pi$ and a behavior policy $b$.
However, on page 103, they state:

The target policy $\pi$ [...] may be deterministic [...].

When $\pi$ is deterministic and greedy it gives $1$ for the greedy action and 0 for all other possible actions.
So, how can the above formula give something else than zero, except for the case where policy $b$ takes a path that $\pi$ would have taken as well? If any selected action of $b$ is different from $\pi$'s choice, then the whole numerator is zero and thus the whole result.
","['reinforcement-learning', 'off-policy-methods', 'sutton-barto', 'importance-sampling']","You're correct, when the target policy $\pi$ is deterministic, the importance sampling ratio will be $\geq 1$ along the trajectory where the behaviour policy $b$ happened to have taken the same actions that $\pi$ would have taken, and turns to $0$ as soon as $b$ makes one ""mistake"" (selects an action that $\pi$ would not have selected).Before importance sampling is introduced in the book, I believe the only off-policy method you will have seen is one-step $Q$-learning, which can only propagate observations back along exactly one step. With the importance sampling ratio, you can often do a bit better. You're right, there is a risk that it turns to $0$ rather quickly (especially when $\pi$ and $b$ are very different from each other), at which point it essentially ""truncates"" your trajectory and ignores all subsequent experience... but that still can be better than one-step, there is a chance that the ratio will remain $1$ for at least a few steps. It will occasionally still only permit $1$-step returns, but also sometimes $2$-step returns, sometimes $3$-step returns, etc., which is often better than only having $1$-step returns.Whenever the importance sampling ratio is not $0$, it can also give more emphasis to the observations resulting from trajectories that would be common under $\pi$, but are uncommon under $b$. Such trajectories will have a ratio $> 1$. Emphasizing such trajectories more can be beneficial, because they don't get experienced often under $b$, so without the extra emphasis it can be difficult to properly learn what would have happened under $\pi$.Of course, it is also worth noting that your quote says (emphasis mine):The target policy $\pi$ [...] may be deterministic [...]It says that $\pi$ may be deterministic (and in practice it very often is, because we very often take $\pi$ to be the greedy policy)... but sometimes it won't be. The entire approach using the importance sampling ratio is well-defined also for cases where we choose $\pi$ not to be deterministic. In such situations, we'll often be able to propagate observations over significantly longer trajectories (although there is also a risk of excessive variance and/or numeric instability when $b$ selects actions that are highly unlikely according to $b$, but highly likely according to $\pi$)."
Is Monte Carlo Tree Search appropriate for problems with large state and action spaces?,"
I'm doing a research on a finite-horizon Markov decision process with $t=1, \dots, 40$ periods. In every time step $t$, the (only) agent has to chose an action $a(t) \in A(t)$, while the agent is in state $s(t) \in S(t)$. The chosen action $a(t)$ in state $s(t)$ affects the transition to the following state $s(t+1)$.
In my case, the following holds true: $A(t)=A$ and $S(t)=S$, while the size of $A$ is $6 000 000$ (6 million) and the size of $S$ is $10^8$. Furthermore, the transition function is stochastic.
Would Monte Carlo Tree Search (MCTS) an appropriate method for my problem (in particular due to the large size of $A$ and $S$ and the stochastic transition function?)
I have already read a lot of papers about MCTS (e.g. progressive widening and double progressive widening, which sound quite promising), but maybe someone can tell me about his experiences applying MCTS to similar problems or about appropriate methods for this problem (with large state/action space and a stochastic transition function).
","['reinforcement-learning', 'monte-carlo-tree-search', 'markov-decision-process', 'finite-markov-decision-process']",
How do biological neurons weights get initialized?,"
When trying to map artificial neuronal models to biological facts it was not possible to find an answer regarding the biological justification of randomly initializing the weights.
Perhaps this is not yet known from our current understanding of biological neurons?
","['machine-learning', 'training', 'artificial-neuron', 'neurons', 'biology']","In shortI mentioned in another post, how the Artificial Neural Network (ANN) weights are a relatively crude abstraction of connections between neurons in the brain. Similarly, the random weight initialization step in ANNs is a simple procedure that abstracts the complexity of central nervous system development and synaptogenesis.A bit more detail (with the most relevant parts italicized below)The neocortex (one of its columns, more specifically) is a region of the brain that somewhat resembles an ANN. It has a laminar structure with layers that receive and send axons from other brain regions. Those layers can be viewed as ""input"" and ""output"" layers of an ANN (axons ""send"" signals, dendrites ""receive""). Other layers are intermediate-processing layers and can be viewed as the ANN ""hidden"" layers.When building an ANN, the programmer can set the number of layers and the number of units in each layer. In the neocortex, the number of layers and layer cell counts are determined mostly by genes (however, see: Human echolocation for an example of post-birth brain plasticity). Chemical cues guide the positions of the cell bodies and create the laminar structure. They also seem to guide long term axonal connections between distant brain regions. The cells then sprout dendrites in certain characteristic ""tree-like"" patterns (see: NeuroMorpho.org for examples). The dendrites will then form synapses with axons or other cell bodies they encounter along the way, generally based on the encountered cell type. This last phase is probably the most analogous to the idea of random weight initialization in ANNs. Based on where the cell is positioned and its type, the encountered other neurons will be somewhat random and so will the connections to them. These connections are probably not going to be very strong initially but will have room to get stronger during learning (probably analogous to initial random weights between 0 and ~0.1, with 1 being the strongest possible connection). Furthermore, most cells are either inhibitory or excitatory (analogous to negative and positive weights). Keep in mind this randomization process has a heavy spatial component in real brains. The neurons are small and so they will make these connections to nearby neurons that are 10-200 microns away. The long-distance connections between brain regions are mostly ""programmed-in"" via genes. In most ANNs, there is generally no distance-based aspect to the initialization of connection weights (although convolutional ANNs implicitly perform something like distance-based wiring by using the sliding window).There is also the synaptic pruning phenomenon, which might be analogous to creating many low weight connections in an ANN initially (birth), training it for some number of epochs (adolescence), and then removing most low-weight connections (consolidation in adulthood)."
Is learning possible without random thoughts and actions?,"
In my view intelligence begins once the thoughts/actions are logical rather than purely randomn based. The learning environments can be random but the logic seems to obey some elusive rules. There is also the aspect of a parenting that guides through some really bad decisions by using the collective knowledge. All of this seems to hint that intelligence needs intelligence to coexist and a sharing communication network for validation/rejection.
Personally I believe that we must keep the human intelligence in a parental role for long enough time until at least the AI had fully assimilated our values. The actual danger is to leave the artificial intelligence parenting another AI and loose control of it. This step is not necessary from our perspective but can we resist the temptation and try it eventually, only time will tell.
Above all we must remember the purpose of AI. I think the purpose should always be to help humans achieve mastery of the environment while ensuring our collective preservation.
AI should not be left unsupervised as we would not give guns to kids, do we?
To resume it all AI needs an environment and supervision where to learn and grow. The environment can vary but the supervision must stay in place.
Are initiated thoughts/actions by the means of guidance and supervision considered random?
Lastly I believe that the sensible think to do is to only develop artificial intelligence that is limited by our own beliefs and values rather than searching for something greater than us.
It seems not possible to create greater than our intelligence without letting it go exploring!
Exploring has greater access to random actions and can go against the intended purpose.
","['ai-design', 'control-problem', 'random-variable']","Learning is possible without random thoughts and actions. Knowledge can be encapsulated in predetermined forms and passed through predetermined knowledge transfer mechanisms. Much of civilization is based on these predeterminations. Without them, humanity would be thrown back possibly 120,000 years.However, initial discovery requires trials and review of their outcome. Purely deterministic identifications of trials is necessarily systematic and the system used may interplay with the phenomena under study in such a way as to miss important cases. Furthermore, when the complexity of the phenomenon is high, the number of trials is often too numerous to check entirely. In this second scenario, random selection of trials is wise for a similar reason illustrated by this simple example.The phenomenon has one behavior for even numbers and another for odd. The system for determining trials is to check every factor of 100 to cover the range from 1 to 10,000 in 100 trials. The odd behavior will be inadvertently overlooked.Intelligence begins once the thoughts/actions are logical rather than purely random based.For the above reasons, intelligence begins with logic only in determining the domain of trials but, when one of the above two cases apply, is often quickly followed by lack of logic in the selection from that domain. Once models have formed as a result of these initial discovery activities, logical inference is useful again, to combine them in various ways. Through this process, engineering, business, resource planning, and other intelligence related disciplines have improved the living conditions for the species, albeit inconsistently.So there are two limitations of logic.It is naive to assume that there were logical faults that produces the inconsistency in this second limitation. There is no logical proof that logic necessarily improves conditions logically across the population that uses it. There may be qualities of the goal sets of many individuals that thwart the vision of logic producing peace and prosperity for all. That belief is not new and no person or political group has been able to make it work.Intelligence needs intelligence to coexist and a sharing communication network for validation/rejection. ... I believe that we must keep the human intelligence in a parental role for long enough time until at least the AI had fully assimilated our values.That assumes that those values are best. Some might want to agree on those values before letting the AI assimilate them, which, given the insanity evident in human history, could lead to a war. If we give the AI the below four objectives, we could probably live without the remainder of human values.This statement dismisses the human record.The actual danger is to leave the artificial intelligence parenting another AI and loose control of it. ... the purpose should always be to help humans achieve mastery of the environment while ensuring our collective preservation. ... AI should not be left unsupervised as we would not give guns to kids, do we? ... To resume it all AI needs an environment and supervision where to learn and grow. The environment can vary but the supervision must stay in place.It is impossible to supervise all adults and it will later become impossible to supervise all AI in data centers and robots. There are already more processes running in computers than there are people in the world. This is why risk management must be applied proactively, during the research and development of the AI. Some of the questions on this site address that very challenge. If it is not done proactively, it will become untenable and control will already have been passed for a random future regarding our species.Only develop artificial intelligence that is limited by our own beliefs and values rather than searching for something greater than us.There is no universal set of beliefs among humans. There will inescapably be a wide range in AI developments with a wide range of intended purposes. The key is to encourage researchers to think beyond their obsession with getting programs to do amusing and impressive things. AI research must continue to keep one foot in technology but another foot firmly placed in the practical elements of ethics, social science, environmental science, economics, and risk management."
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
,,,
Method to compute the sum when the activation is a continuous function?,"
Background
My understanding is the input neurons seem to seem to compute a weighted sum moving from one layer to another. 

$$ \sum_i a_i w_i = a'_{k} $$
But to compute this weighted sum the sum must be discrete. Is there any known method to compute the sum when the activation is a continuous function? Is the below formula of any consequence problems in artificial intelligence? Can anyone give a specific problem where it might be useful?
My Method
Let $b_r = \sum_{d \mid r} a_d\mu(\frac{m}{d})$. We prove that if the $b_r$'s are small enough, the result is true (where $\mu$ is the mobius function). 

Claim: If $\lim_{n \to \infty} \frac{\log^2(n)}{n}\sum_{r=1}^n |b_r| = 0$ and $f$ is smooth, then $$\lim_{k \to \infty} \lim_{n \to \infty} \sum_{r=1}^n a_rf\left(\frac{kr}{n}\right)\frac{k}{n} = \left(\lim_{s \to 1} \frac{1}{\zeta(s)}\sum_{r=1}^\infty \frac{a_r}{r^s}\right)\int_0^\infty f(x)dx.$$

I will not go into the proof of this over but for those who are interested: https://math.stackexchange.com/questions/2888976/a-rough-proof-for-infinitesimals I will merely state what the formula means:
Consider we have a curve $f(x)$ now if one wishes to perform a weighted sum in the limiting case of this function. 

Consider the curve $f(x)$. Then splitting it to $k/n= h$ intervals then adding the first strip ($d_1$ times): $  f(h) \cdot d_1$. Then the second strip ($d_2$ times) $ f(2h) \cdot d_2$ times ... And so on . Hence. $d_r$ can be thought of as the weight at $f(rh)$.
","['artificial-neuron', 'activation-functions']",
How do I update the Q values of a Deep Q Network when exploring?,"
I am trying to implement a Deep Q Network to play Asteroids. Unfortunately, I am not sure how to calculate the Q value exactly, if I am exploring. For example, the agent is exploring for 1 second (otherwise makes no sense; I cannot let it just explore one step). Unfortunately, it makes a mistake at 0.99s, and the reward collapses. 
At the moment, I am using the following formula to evaluate or update the Q value:
$$Q_{new,t} = reward + \gamma Q_{max,t+1}$$
But how do I know the max Q value of the next step? I could consider the best Q value the network says, but this is not necessarily true.
You can see the current implementation at the following URL:
https://github.com/SuchtyTV/RLearningBird/blob/master/src/main/java/rlgame/Brain.java.
","['reinforcement-learning', 'game-ai', 'q-learning', 'deep-rl']",
Handling emotion in informal text (Hi vs HIIIIII!!!!)?,"
This is a question related to Neural network to detect ""spam""?. 
I'm wondering how it would be possible to handle the emotion conveyed in text. In informal writing, especially among a juvenile audience, it's usual to find emotion expressed as repetition of characters. For example, ""Hi"" doesn't mean the same as ""Hiiiiiiiiiiiiiii"" but ""hiiiiii"", ""hiiiiiiiii"", and ""hiiiiiiiiii"" do. 
A naive solution would be to preprocess the input and remove the repeating characters after a certain threshold, say, 4. This would probably reduce most long ""hiiiii"" to 4 ""hiiii"", giving a separate meaning (weight in a context?) to ""hi"" vs ""long hi"".
The naivete of this solution appears when there are combinations. For example,
haha vs hahahahaha or lol vs lololololol. Again, we could write a regex to reduce lolol[ol]+ to lolol. But then we run into the issue of hahahaahhaaha where a typo broke the sequence.
There is also the whole issue of Emoji. Emoji may seem daunting at first since they are special characters. But once understood, emoji may actually become helpful in this situation. For example, 😂 may mean a very different thing than 😂😂😂😂😂, but 😂😂😂😂😂 may mean the same as 😂😂😂😂 and 😂😂😂😂😂😂. 
The trick with emojis, to me, is that they might actually be easier to parse. Simply add spaces between 😂 to convert 😂😂😂😂 to 😂 😂 😂 😂 in the text analysis. I would guess that repetition would play a role in training, but unlike ""hi"", and ""hiiii"", Word2Vec won't try to categorize 😂 and 😂😂 as different words (as I've now forced to be separate words, relying in frequency to detect the emotion of the phrase). 
Even more, this would help the detection of ""playful"" language such as 😠😂😂😂, where the 😠 emoji might imply there is anger, but alongside 😂 and especially when repeating 😂 multiple times, it would be easier for a neural network to understand that the person isn't really angry.
Does any of this make sense or I'm going in the wrong direction?
","['neural-networks', 'machine-learning', 'natural-language-processing']",
What is a high dimensional state in reinforcement learning?,"
In the DQN paper, it is written that the state-space is high dimensional. I am a little bit confused about this terminology.
Suppose my state is a high dimensional vector of length $N$, where $N$ is a huge number. Let's say I solve this task using $Q$-learning and I fix the state space to $10$ vectors, each of $N$ dimensions. $Q$-learning can easily work with these settings as we need only a table of dimensions $10$ x number of actions.
Let's say my state space can have an infinite number of vectors each of $N$ dimensions. In these settings, Q-learning would fail as we cannot store Q-values in a table for each of these infinite vectors. On the other hand, DQN would easily work, as neural networks can generalize for other vectors in the state-space.
Let's also say I have a state space of infinite vectors, but each vector is now of length $2$, i.e., small dimensional vectors. Would it make sense to use DQN in these settings? Should this state-space be called high dimensional or low dimensional?
","['reinforcement-learning', 'q-learning', 'dqn', 'terminology', 'state-spaces']","Usually when people write about having a high-dimensional state space, they are referring to the state space actually used by the algorithm.Suppose my state is a high dimensional vector of $N$ length where $N$ is a huge number. Let's say I solve this task using $Q$-learning and I fix my state space to $10$ vectors each of $N$ dimensions. $Q$-learning can easily work with these settings as we need only a table of dimensions $10$ x number of actions. In this case, I'd argue that the ""feature vectors"" of length $N$ are quite useless. If there are effectively only $10$ unique states (which may each have a very long feature vector of length $N$)... well, it seems like a bad idea to make use of those long feature vectors, just using the states as identity (i.e. a tabular RL algorithm) is much more efficient. If you end up using a tabular approach, I wouldn't call that a high-dimensional space. If you end up using function approximation with the feature vectors instead, that would be a high-dimensional space (for large $N$).Let's also say I have a state space of infinite vectors but each vector is now of length $2$ i.e. very small dimensional vectors. Would it make sense to use DQN in these settings ? Should this state-space be called high dimensional or low dimensional ? This would typically be referred to as having a low-dimensional state space. Note that I'm saying low-dimensional. The dimensionality of your state space / input space is low, because it's $2$ and that's typically considered to be a low value when talking about dimensionality of input spaces. The state space may still have a large size (that's a different word from dimensionality).As for whether DQN would make sense in such a setting.. maybe. With such low dimensionality, I'd guess that a linear function approximator would often work just as well (and be much less of a pain to train). But yes, you can use DQN with just 2 input nodes."
Can someone help me to understand the alpha-beta pruning algorithm?,"
I understand the minimax algorithm, but I am unable to understand deeply the minimax algorithm with alpha-beta pruning, even after having looked up several sources (on the web) and having tried to read the algorithm and understand how it works.
Do you have a good source that explains alpha-beta pruning clearly, or can you help me to understand the alpha-beta pruning (with a simple explanation)?
","['search', 'minimax', 'alpha-beta-pruning']",
Chatbots triggering emotions [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



I’m a researcher and I’m currently conducting a research project. I will conduct a study where I would like to trigger different emotions using chatbots on a smartphone (e.g. on Facebook Messenger).
Are there any existing chatbots which are able to trigger different emotions intentionally (also negative ones)?
","['intelligent-agent', 'chat-bots', 'emotional-intelligence']",Emotions can of course be triggered by lots of different things. I think the most rich source could well be socialbots like Mitsuku.com and Zo.ai -- Steve Worswick is the owner of Mitsuku and may be interested in helping you by doing (appropriately filtered) chat log queries. You can get him on Twitter at @Mitsuku. 
Does the correlation between inputs affect the model performance?,"
I'm currently working on a regression problem and I have 10 inputs/attributes. 
What should I do if there are correlations between different features of the input data? Does the correlation between inputs affect the performance (e.g. accuracy) of the model?
","['machine-learning', 'linear-regression', 'statistical-ai', 'features']","Non-correlation does not imply independence, that is, if two features are not correlated (i.e. zero correlation), it does not mean that they are independent. But (non-zero) correlation implies dependence (see https://stats.stackexchange.com/q/113417/82135 for more details). So, if you have non-zero correlation between two features, it means they are dependent. If they are dependent, then one feature gives you information about the other and vice-versa: in a certain way, one of the two is, at least partially, redundant. Unnecessary features might not affect the performance (e.g. the accuracy) of a model. However, if you reduce the number of features, the learning process might actually be faster.You may want to try some dimensionality reduction technique, in order to reduce the number of features."
is it possible to train several Neural Networks on different types of data and combine them?,"
I want to create a NHL game predictor and have already trained one neural network on game data. 
What I would like to do is train another model on player seasonal/game data and combine the two models to archive better accuracy. 
Is this approach feasible? If it is, how do I go about doing it?
EDIT:
I have currently trained a neural network to classify the probability of the home team winning a game on a dataset that looks like this:
h_Won/Lost  h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4
 1            10       10         10        10         10        10      1       0        0      0         0      1        0      0
 1            10       10         10        10         10        10      1       0        0      0         0      1        0      0
 1            10       10         10        10         10        10      1       0        0      0         0      1        0      0

and so on.
I am preparing a dataset of player-data for each game that will have the shape of this:
Player     PlayerID    Won/Lost     team      opponent     metric1     metric2   
 Henke         1           1          NY          CAP         10          10

Hopefully, this new dataset will have some accuracy on if team is going to have some predictive features that are good and recognised. 
Now, say I have these two trained Nural Networks and they both have an accuracy of 70% by them self. But I want to combine them both in the hopes to achieve better predictability. How is this archived? How will the test-dataset be structured?
","['neural-networks', 'deep-learning', 'data-science']",
Smaller interest area for images than the size of the image in classification neural networks,"
I have the following binary classification problem, my labeled dataset contains images 96x96 px. Now in every image the interest area is of size 32x32 px in the center of the image, and the images are labeled based on that 32x32 px area. If whatever i am trying to detect is in the outer region of the 32x32 px area the label of that image is not affected.
The problem here is that if i use the whole image when traing, my model will not learn that the interest area is only in the center of the image but on the other hand if i crop the images to be of size 32x32 i am loosing a lot of information which can help the model to train on. I found out that i am getting the best results if i crop the images to be of size 64x64 (kind of a trade of).
Now going to the test set, for the test set it doesn't make sense to use this trade of because the model is not learning anything anyways so i would rather crop the test images to 32x32 but then the test set and train set sizes are not the same.
Has anyone came across this problem before? Can i just pad the test images to the size of the train images? is this a good way to go?
","['convolutional-neural-networks', 'image-recognition']","""The problem here is that if i use the whole image when training, my model will not learn that the interest area is only in the center of the image"" Yes, it will, if you have enough samples and they are all classified based on this area only.""but on the other hand if i crop the images to be of size 32x32 i am loosing a lot of information which can help the model to train on"" . I don't understand what information you are loosing if you only care about the center 32x32 area.In the end, you need to ask yourself how will the test input look like and work based on that. If you only care about an area of 32x32 which is always is (and will be) at the center of all images, ignore everything and train and evaluate based just on that area (why add extra unnecessary calculations to the model?). If the size and location of the area might change, don't crop the image.Bottom line, the test set and the train set need to be of the nature (shape) for the model to work properly.  "
What is an adversarial attack?,"
I'm reading this really interesting article CycleGAN, a Master of Steganography. I understand everything up until this paragraph:

we may view the CycleGAN training procedure as continually mounting an adversarial attack on $G$, by optimizing a generator $F$ to generate adversarial maps that force $G$ to produce a desired image. Since we have demonstrated that it is possible to generate these adversarial maps using gradient descent, it is nearly certain that the training procedure is also causing $F$ to generate these adversarial maps. As $G$ is also being optimized, however, $G$ may actually be seen as cooperating in this attack by learning to become increasingly susceptible to attacks. We observe that the magnitude of the difference $y^{*}-y_{0}$ necessary to generate a convincing adversarial example by Equation 3 decreases as the CycleGAN model trains, indicating cooperation of $G$ to support adversarial maps.

How is the CycleGAN training procedure an adversarial attack?
I don't really understand the quoted explanation.
","['terminology', 'papers', 'adversarial-ml', 'adversarial-attacks', 'cycle-gan']",
"If deep learning is a black box, then why are companies still investing in it?","
If deep learning is a black box, then why are companies still investing in it?
","['neural-networks', 'deep-learning', 'applications']",
LSTM language model not working,"
I am trying to use a Keras LSTM neural network for character level language modelling. As the input, I give it the last 50 characters and it has to output the next one. It has 3 layers of 400 neurons each. For the training data, I am using 'War of The Worlds' by H.G. Wells which adds up to 269639 training samples and 67410 validation samples.
After 7 epochs the validation accuracy has reached 35.1% and the validation loss has reached 2.31. However, after being fed the first sentence of war of the worlds to start it outputs:

the the the the the the the the the the the the the the the the...

I'm not sure where I'm going wrong; I don't want it to overfit and output passages straight from the training data but I also don't want it to just output 'the' repeatedly. I'm really at a loss as to what I should do to improve it.
Any help would be greatly appreciated. Thanks!
","['neural-networks', 'natural-language-processing', 'recurrent-neural-networks', 'keras', 'long-short-term-memory']","You can follow the below steps :As a working example, you can refer this 
project which generates poem lines using LSTM in tensorflow."
What is the concept of channels in CNNs?,"
I am trying to understand what channels mean in convolutional neural networks. When working with grayscale and colored images, I understand that the number of channels is set to 1 and 3 (in the first conv layer), respectively, where 3 corresponds to red, green, and blue.
Say you have a colored image that is $200 \times 200$ pixels. The standard is such that the input matrix is a $200 \times 200$ matrix with 3 channels. The first convolutional layer would have a filter that is size $N \times M \times 3$, where $N,M < 200$ (I think they're usually set to 3 or 5).
Would it be possible to structure the input data differently, such that the number of channels now becomes the width or height of the image? i.e., the number of channels would be 200, the input matrix would then be $200 \times 3$ or $3 \times 200$. What would be the advantage/disadvantage of this formulation versus the standard (# of channels = 3)? Obviously, this would limit your filter's spatial size, but dramatically increase it in the depth direction.
I am really posing this question because I don't quite understand the concept of channels in CNNs.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'channel']","Say you have a colored image that is 200x200 pixels. The standard is such that the input matrix is a 200x200 matrix with 3 channels. The first convolutional layer would have a filter that is size $N×M×3$, where $N,M<200$ (I think they're usually set to 3 or 5). Would it be possible to structure the input data differently, such that the number of channels now becomes the width or height of the image? i.e., the number of channels would be 200, the input matrix would then be 200x3 or 3x200. What would be the advantage/disadvantage of this formulation versus the standard (# of channels = 3)? Obviously, this would limit your filter's spatial size, but dramatically increase it in the depth direction. The different dimensions (width, height, and number of channels) do have different meanings, the intuition behind them is different, and this is important. You could rearrange your data, but if you then plug it into an implementation of CNNs that expects data in the original format, it would likely perform poorly.The important observation to make is that the intuition behind CNNs is that they encode the ""prior"" assumption or knowledge, the ""heuristic"", the ""rule of thumb"", of location invariance. The intuition is that, when looking at images, we often want our Neural Network to be able to consistently (in the same way) detect features (maybe low-level features such as edges, corners, or maybe high-level features such as complete faces) regardless of where they are. It should not matter whether a face is located in the top-left corner or the bottom-right corner of an image, detecting that it is there should still be performed in the same way (i.e. likely requires exactly the same combination of learned weights in our network). That is what we mean with location invariance.That intution of location invariance is implemented by using ""filters"" or ""feature detectors"" that we ""slide"" along the entire image. These are the things you mentioned having dimensionality $N \times M \times 3$. The intuition of location invariance is implemented by taking the exact same filter, and re-applying it in different locations of the image.If you change the order in which you present your data, you will break this property of location invariance. Instead, you will replace it with a rather strange property of... for example, ""width-colour"" invariance. You might get a filter that can detect the same type of feature regardless its $x$-coordinate in an image, and regarldess of the colour in which it was drawn, but the $y$-coordinate will suddenly become relevant; your filter may be able to detect edges of any colour in the bottom of an image, but fail to recognize the same edges in the top-side of an image. This is not an intuition that I would expect to work successfully in most image recognition tasks.Note that there may also be advantages in terms of computation time in having the data ordered in a certain way, depending on what calculations you're going to perform using that data afterwards (typically lots of matrix multiplications). It is best to have the data stored in RAM in such a way that the inner-most loops of algorithms using the data (matrix multiplication) access the data sequentially, in the same order that it is stored in. This is the most efficient way in which to access data from RAM, and will result in the fastest computations. You can generally safely expect that implementations in large frameworks like Tensorflow and PyTorch will already require you to supply data in whatever format is the most efficient by default."
"What kind of AI technique can I use to play the ""Lines"" game?","
I am trying to find a good approach to create a computer player for the game ""Lines"" from gamious on Android. The concept of the game is pretty straightforward :

Lines is an abstract ‘zen’ game experience where form is just as important as function. Place or remove Dots to initiate a colourful race that fills a drawing. The colour that dominates the race wins.

The game starts with a drawing (that can be described as a set of ""blank"" lines, with connection to other lines). Dots of different colour are placed somewhat randomly on the lines. The player get a colour assigned. When the game start, paint start flowing from the dots and filling the (at first blank) lines of the drawing. You win if your colour dominates.
The game gives you different tools to win (the game starts when all of them have been used) :

[0 to 2] scissor to cut lines
[0 to 5] additional dot of your own color to place on the drawing
[0 to 4] enemy dots eraser
[0 to 3] additional straight lines to connect different part of the drawing

A quick example: the first image is the initial state of a round. ""My"" colour is the yellow (1 enemy = brown) and I have 4 tools (2 eraser and 2 lines). The second image shows the game running after I used the tools to put my colour in a winning position (yes, we can do better)


If I try to approach this as a classical optimization problem, things get messy pretty fast :

highly non-linear
high number of dimensions

AI seems to be the right way to go, but I would like your help to get in the right direction: what would be your approach to create an AI to play this game?
To limit the scope of this question, you can consider that I already have a data structure to represent the game initial state, the use of different tools and the game ""physics"". What I really want to do is finding how to create an AI which can learn how to efficiently use the tools.
Regarding my experience, I took 2 semesters of AI classes during the last year getting my engineering degree and have used non-linear optimization tools for a while: you can go technical, but I am not fully understand it.
","['game-ai', 'reference-request']",
Stereo matching using genetic algorithm,"
I have been reading a few papers (paper1, paper2) on stereo matching using genetic algorithms. I understand how genetic algorithms work in general and how stereo matching works, but I do not understand how genetic algorithms are used in stereo matching. 
The first paper by Han et al says that ""1) individual is a disparity set, 2) a chromosome has a 2D structure for handling image signals efficiently, and 3) a fitness function is composed of certain constraints which are commonly used in stereo matching"".
Does it mean that an individual is a disparity map with random numbers?
Then a chromosome is a block within the individual's disparity map. 
The constraint used for fitness function could be the famous epipolar line. 
I dont seem to understand how this works and even WHY you should use genetic algorithm on an algorithm that at its simplest form uses 5 for loops, for example, like in here.
",['genetic-algorithms'],
"Why do neural nets and machine learning tend to work well with MCTS, but not with regular Minimax game-playing AI?","
I've often heard MCTS grouped together with neural nets and machine learning. From what I gather, MCTS uses a refined intuition (from maching learning) to evaluate positions. This allows it to better guess which moves are worth playing out more.
But I've almost never heard of using machine learning for Minimax+alpha-beta engines. Couldn't machine learning be used for the Engine to better guess which move is best, and then look at that move's subtree first? A major optimization of the minimax algorithm is move-ordering, and this seems like a good way to accomplish that.
","['neural-networks', 'machine-learning', 'game-ai', 'monte-carlo-tree-search', 'minimax']","It just does.
Take a look at this post explaining how MCTS works.In both Alpha Go Lee and Alpha Zero the tree traversal follows the nodes that maximize the following UCT variant:\begin{equation}
UCT(v_i,v) = \frac{Q(v_i)}{N(v_i)} + cP(v_i, v)\sqrt{\frac{N(v)}{1+N(v_i)}}
\end{equation}where P(vi,v)
  is prior probability of the move (transition from v to vi ), its value comes from the output of deep neural network called Policy Network . Policy Network is a function that consumes game state and produce probability distribution over possible moves.As you can see the Policy Network(it is actually just one neural network for both the value and the policy) is used to guide the search tree. Not all possible moves are explored. Also during the learning phase the policy network uses the ""visit count"" of the MCTS Nodes to learn. Moves that were explored more are the better ones. The state-of-the-art chess engine Stockfish evaluates about 1000 times more positions per second than Alpha Zero. It relies on exploring ""most of"" the possible positions. Calculating the score using heuristics is much faster than using Alpha Zero's 19 layer residual network. If Google were to use minimax then they wouldn't be able to look very far ahead. Alpha Zero explores about 70K moves per second which would be only a couple ply deep. MCTS allows it to explore only the more promising moves and simulate those positions."
Does software remain even when hardware is demolished?,"
For example, if I constructed a neural network and the computer running it where to be demolished, is the information/program of the neural network still an existent entity within or outside the remnants of the hardware?  
",['hardware'],"Neural networks are just software. Software is just one form of data in the Von Neumann Architecture.Most of the data is stored on the disk. There are two common types of disks:If the part where the data is stored is destroyed, the data is destroyed.Of course, the data can be loaded into memory, but the principle is the same: If the part where the data is stored is destroyed, the data is destroyed.Data can also be partially be destroyed / corrupted. If you have systems like RAID or use error correcting codes then you can defend youself against data loss."
"Neural network to detect ""spam""? [closed]","







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I've inherited a neural network project at the company I work for. The person who developed gave me some very basic training to get up and running. I've maintained it for a while. The current neural network is able to classify messages for telcos: it can send them to support people in different areas, like ""activation"", ""no signal"", ""internet"", etc. The network has been working flawlessly. The structure of this neural network is as follows:
 model = Sequential()
 model.add(Dense(500, input_shape=(len(train_x[0]),)))
 model.add(Activation('relu'))
 model.add(Dropout(0.6))
 model.add(Dense(250, input_shape=(500,)))
 model.add(Activation('relu'))
 model.add(Dropout(0.5))
 model.add(Dense(len(train_y[0])))
 model.add(Activation('softmax'))
 model.compile(loss='categorical_crossentropy',
                  optimizer='Adamax',
                  metrics=['accuracy'])

This uses a Word2Vec embedding, and has been trained with a ""clean"" file: all special characters and numbers are removed from both the training file and the input data.
Now I've been assigned to make a neural network to detect if a message will be catalog as ""moderated"" (meaning it's an insult, spam, or just people commenting on a facebook post), or ""operative"", meaning the message is actually a question for the company. 
What I did was start from the current model and reduce the number of categories to two. It didn't go very well: the word embedding was in spanish from Argentina, and the training data was spanish from Peru. I made a new embedding and accuracy increased by a fair margin (we are looking for insults and other curse words. In spanish a curse word from a country can be a normal word for another: in Spain ""coger"" means ""to take"", and in Argentina it means ""to f__k"". ""concha"" means shell in most countries, but in Argentina it means ""c__t"". You get the idea).
I trained the network with 300.000 messages. Roughly 40% of these were classified as ""moderated"". I tried all sorts of combinations of cycles and epochs. The accuracy slowly increased to nearly 0.9, and loss stays around 0.5000. 
But when testing the neural network, ""operative"" messages generally seem to be correctly classified, with accuracy around 0.9, but ""moderated"" messages aren't. They are classified around 0.6 or less. At some point I tried multiple insults in a message (even pasting sample data as input data), but it didn't seem to improve.
Word2Vec works fantastically. The words are correctly ""lumped"" together (learned a few insults in Peruvian spanish thanks to it). 
I put the neural network in production for a week, to gather statistics. Basically 90% of the messages went unclassified, and 5% were correctly classified and 5% wrong. Since the network has two categories, this seems to mean the neural network is just giving random guesses.
So, the questions are:

Is it possible to accomplish this task with a neural network?
Is the structure of this neural network correct for this task?
Are 300k messages enough to train the neural network?
Do I need to clean up the data from uppercase, special characters, numbers etc?

","['neural-networks', 'feedforward-neural-networks']",
How to implement a Continuous Control of a quadruped robot with Deep Reinforcement Learning in Pybullet and OpenAI Gym?,"
Description
I have designed this robot in URDF format and its environment in pybullet. Each leg has a minimum and maximum value of movement. 
What reinforcement algorithm will be best to create a walking policy in a simple environment in which a positive reward will be given if it walks in the positive X-axis direction?
I am working in the following but I don´t know if it is the best way:
The expected output from the policy is an array in the range of (-1, 1) for each joint. The input of the policy is the position of each joint from the past X frames in the environment(replay memory like DeepQ Net), the center of mass of the body, the difference in height between the floor and the body to see if it has fallen and the movement in the x-axis.
Limitations
left_front_joint      => lower=""-0.4"" upper=""2.5"" id=0
left_front_leg_joint  => lower=""-0.6"" upper=""0.7"" id=2
right_front_joint     => lower=""-2.5"" upper=""0.4"" id=3
right_front_leg_joint => lower=""-0.6"" upper=""0.7"" id=5
left_back_joint       => lower=""-2.5"" upper=""0.4"" id=6
left_back_leg_joint   => lower=""-0.6"" upper=""0.7"" id=8
right_back_joint      => lower=""-0.4"" upper=""2.5"" id=9
right_back_leg_joint  => lower=""-0.6"" upper=""0.7"" id=11
The code below is just a test of the environment with a set of movements hardcoded in the robot just to test how it could walk later. The environment is set to real time, but I assume it needs to be in a frame by frame lapse during the policy training. (p.setRealTimeSimulation(1) #disable and p.stepSimulation() #enable)
A video of it can be seen in:
https://youtu.be/j9sysG-EIkQ
The complete code can be seen here:
https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS
CODE
import pybullet as p
import time
import pybullet_data

def moveLeg( robot=None, id=0, position=0, force=1.5  ):
    if(robot is None):
        return;
    p.setJointMotorControl2(
        robot,
        id,
        p.POSITION_CONTROL,
        targetPosition=position,
        force=force,
        #maxVelocity=5
    )

pixelWidth = 1000
pixelHeight = 1000
camTargetPos = [0,0,0]
camDistance = 0.5
pitch = -10.0
roll=0
upAxisIndex = 2
yaw = 0

physicsClient = p.connect(p.GUI)#or p.DIRECT for non-graphical version
p.setAdditionalSearchPath(pybullet_data.getDataPath()) #optionally
p.setGravity(0,0,-10)
viewMatrix = p.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)
planeId = p.loadURDF(""plane.urdf"")
cubeStartPos = [0,0,0.05]
cubeStartOrientation = p.getQuaternionFromEuler([0,0,0])
#boxId = p.loadURDF(""r2d2.urdf"",cubeStartPos, cubeStartOrientation)
boxId = p.loadURDF(""src/spider.xml"",cubeStartPos, cubeStartOrientation)
# boxId = p.loadURDF(""spider_simple.urdf"",cubeStartPos, cubeStartOrientation)



toggle = 1



p.setRealTimeSimulation(1)

for i in range (10000):
    #p.stepSimulation()


    moveLeg( robot=boxId, id=0,  position= toggle * -2 ) #LEFT_FRONT
    moveLeg( robot=boxId, id=2,  position= toggle * -2 ) #LEFT_FRONT

    moveLeg( robot=boxId, id=3,  position= toggle * -2 ) #RIGHT_FRONT
    moveLeg( robot=boxId, id=5,  position= toggle *  2 ) #RIGHT_FRONT

    moveLeg( robot=boxId, id=6,  position= toggle *  2 ) #LEFT_BACK
    moveLeg( robot=boxId, id=8,  position= toggle * -2 ) #LEFT_BACK

    moveLeg( robot=boxId, id=9,  position= toggle *  2 ) #RIGHT_BACK
    moveLeg( robot=boxId, id=11, position= toggle *  2 ) #RIGHT_BACK
    #time.sleep(1./140.)g
    #time.sleep(0.01)
    time.sleep(1)

    toggle = toggle * -1

    #viewMatrix        = p.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)
    #projectionMatrix  = [1.0825318098068237, 0.0, 0.0, 0.0, 0.0, 1.732050895690918, 0.0, 0.0, 0.0, 0.0, -1.0002000331878662, -1.0, 0.0, 0.0, -0.020002000033855438, 0.0]
    #img_arr = p.getCameraImage(pixelWidth, pixelHeight, viewMatrix=viewMatrix, projectionMatrix=projectionMatrix, shadow=1,lightDirection=[1,1,1])

cubePos, cubeOrn = p.getBasePositionAndOrientation(boxId)
print(cubePos,cubeOrn)
p.disconnect()



","['deep-learning', 'reinforcement-learning', 'tensorflow', 'unsupervised-learning', 'open-ai']",
How should we choose the dimensions of the encoding layer in auto-encoders?,"
How should we choose the dimensions of the encoding layer in auto-encoders?
","['neural-networks', 'autoencoders', 'hyperparameter-optimization', 'variational-autoencoder', 'hyper-parameters']",
Is it possible to combine two neural networks trained on different tasks into one that knows both tasks?,"
I'm relatively new to artificial intelligence and neural networks.
Let's say I have two different fully trained neural networks. The first one is trained for mathematical addition and the second one on mathematical multiplication. Now, I want to combine these two neural networks into one that knows about both operations.
Is this possible? Is there a representative name for this kind of technique?
I had read something about bilinear CNN models that sounds similar to what I'm looking for, right?
","['neural-networks', 'machine-learning', 'deep-learning']","Combining two different fully trained neural networks is not only feasible, it is commonly done. Let's look at the example given as two concepts involving integers, $C_a$ and $C_m$.$$ C_a: \mathcal{Y} = f_a (\mathcal{X}) = x_0 + x_1 $$$$ C_m: \mathcal{Y} = f_m (\mathcal{X}) = x_0 \, x_1 $$Now let's define a palette of operations, including these two binary operations, that can be used to construct, a concept $C_e$, an expression comprised of an arbitrary hierarchy of addition, multiplication, constants, and substitution.$$ C_e: \mathcal{Y} = f_e (\mathcal{X}) \; \text{, where} $$$$ f_e \in \{f_a, f_b\} \; \land \; i \in \{0, 1\} \; \land \; ( \, x_i \in \mathbb{I} \; \lor \; x_i \in \mathcal{Y} \, ) \; \text{.} $$Now, one artificial network can be trained to approximate $f_a$ within a concept class $\mathbb{C}$ of which $C_a$ and $C_m$ are members, using labeled examples of correct integer additions and another artificial network can be trained to approximate $f_b$ within that same concept class, using labeled examples of correct integer multiplications.An expression involving both can be trained to approximate arbitrary product of sums or sum of products under specific conditions. It's unclear though if this is what you are looking for.Normally, one wouldn't train a network to perform operations that are already known. Training is normally used to model operations that are not known.(Bilinear Convolutional Neural Networks (B-CNNs), introduced in Bilinear CNNs for Fine-grained Visual Recognition, 2017, Tsung-Yu Lin, Aruni RoyChowdhury, Subhransu Maji, is an approach to using two CNNs in conjunction to provide two fine and course visual recognition in the same way the human visual system can have a dual awareness of detail and panorama. B-CNNs probably don't apply to the scenario given in the question.)"
What is batch / batch size in neural networks?,"
I have some problems with understanding of the batch concept and batch size. I messed something up. First I start it consider based on convolutional neural network I heard two versions:

When the batch size is set to 50, the first network is fed with 50 images and then learned / recalculated (it doesn't make sense to me, because in this case the network learns one of 50 images).
When the batch size is set to 50, one of 50 neurons is recalculated in the learning process on a single image.

Both of these explanations seem to be wrong to me, so I assume, that I completely don't understand this. What is batch / batch size in RNN? Could you show any example?
I can tell you how I would teach a recurrent neural network. Let's say, that I would like to teach a neural network to predict the weather the next day:

I would take weather data from an expected area from the last 30,000 days.
I would assume that my prediction would be based on measurements from the last 365 days.
I would take data from day 1 to 365 - feed RNN with it and learn.
Then I would take data from day 2 to 366 => feed + learn
Then day 3 to 367 => feed + learn
And so on.

Is this 365 measurement concept a batch size?
","['python', 'machine-learning', 'neural-networks']","You are confusing two concepts, batch size and window size.  Batch SizeThe concept of a batch is related to your data and not the number of neurons.  A batch is just a subset of your training data.  The size of the batch affects computational speed, quality, and accuracy.  Large batch sizes will train faster than smaller ones but the model's accuracy can suffer.There is a rule of thumb that a batch size should be a power of two (e.g. 32, 64, 128, etc.).Generally speaking larger batch sizes do not generalize as well as smaller batch sizes.  You will need to experiment with the batch size to achieve optimal performance.  Batch size is considered a hyperparameter.  The following video, Batch Size in a Neural Network explained should help clarify things.Window SizeWindow size applies to time series and sequences when using recurrent style networks.  For your case, 365 is the window size.  You did not mentioned how far in the future you are trying to predict.  If we assume that you are predicting one day in the future your output would be the 366th measurement.  Your ApplicationA batch for you would be a number of training sets.  For example, if we used 32 as a batch size, you would have 32 sets of input windows and outputs.  The first two batches might look like the following:"
How to crossover chromosomes composed of genes that are tuples such that the elements of the tuples do not appear twice in the chromosome?,"
Each chromosome contains an array of genes, each gene contains a letter and a number, both letter and number can only exist once in each chromosome.
Parent A = {a,1}{c,2}{e,3}{g,4}
Parent B = {a,2}{b,1}{c,4}{d,3}

What would be the best crossover operator to create a child that doesn't break the rule described above?
","['genetic-algorithms', 'crossover-operators']",
Why isn't the Credit Card Fraud Detection dataset from Kaggle already balanced?,"
I am working on a credit card fraud detection problem using autoencoders. I have a question regarding the dataset I'll be using.
I've downloaded the dataset for the above problem from Kaggle, which is highly imbalanced: it contains only 492 frauds out of 284,807 transactions. Why isn't the dataset already balanced? Should I balance it before applying the auto-encoder?
","['neural-networks', 'datasets', 'unsupervised-learning', 'autoencoders']",
I need to predict ball position from set of Images [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I am a novice developer in AI. Any help appropriated. 
I have a set of images and from that I want to predict position(x,y co-ordinates) of the Ball.
Thanks in advance.
","['machine-learning', 'computer-vision', 'tensorflow']",
Beginner - Object classification data in a neural network,"
Imagine I wish to classify images of digits from 0-9.
Let's say I have trained the network to recognise '1'.
If I were to train the same network to recognise '2', wouldn't the backpropagation process mess up the weights and biases for '1'?
Or do programs like Tensorflow allocate a new layer of neural network for different object classification? Thanks.
","['neural-networks', 'classification']","The answer is yes, retraining a neural network on a new dataset will alter its internal state such that it would no longer give the same output as before (in your case 'messing it up'). There are techniques to allow you to reuse sections of trained networks for other problems (this is called 'Transfer Learning'). Transfer Learning involves freezing the weights/biases for parts of the already trained network while adding extra new layers to the end of it and training those (with Back-propagation) for your new problem. For your digit recognition problem you would normally just train the network on all the digits at the same time.Tensorflow is lower level than the idea of neural networks and layers. It is a library that allows you to write mathematical pipelines that implement the tensor calculations required to build a neural network. To do the sort of thing that you mentioned you would use a higher level library such as Keras (https://www.tensorflow.org/guide/keras) which now comes conveniently bundled with Tensorflow."
Should I apply ReLU to non negative output?,"
Suppose I want to predict the position of a sensor based on its reading.
I can first predict the unit vector and predict the distance to be multiplied to this vector.
And I know that distance will never be negative because all the negative parts are inside unit vector already.
Should I apply ReLU to the distance before multiplying it to the unit vector?
I'm thinking that this can be helpful to eliminate the network from needing too much training data by restricting the output ranges the network could give. But I also think that it could make the learning slower when the ReLU unit dies (value=0) so the gradient doesn't flow properly somehow.
","['neural-networks', 'deep-learning', 'tensorflow', 'keras']","Applying a ReLU activation function to a non-negative number is the same as applying the identity activation function.What you may wish to do is apply $\ln(|\vec{x}|)$, $\arctan(x[0], x[1])$, ..., $\arctan(x[0], x[N-1])$, where $N$ is the number of dimensions to $\vec{x}$, and pass this directly into a parameter matrix multiplication (which involves deliberately using an identity activation in the input layer in some frameworks). This typically improves speed, accuracy, and reliability of convergence when the magnitudes exhibit a distribution close to an exponential of Gaussian and the directional components are near Gaussian when expressed in radians."
How can I use a 2-dimensional feature matrix as the input to a neural network?,"
How can I use a 2-dimensional feature matrix, rather than a feature vector, as the input to a neural network?
For a WWII naval wargame, I have sorted out the features of interest to approximate the game state $S$ at a given time $t$.

they are a maximum of 50 naval task forces and airbases on the map

each one has a lot of features (hex, weather, number of ships, type, distance to other task forces, distance to objective, cargo, intelligence, combat value, speed, damage, etc.)


The output would be the probability of winning and the level of victory.

","['neural-networks', 'game-ai']",
How do I get a meaningful output value for a simple neural network that can map to a set of data?,"
I am trying to use a artifical neural network to produce a single output, which in my mind should be an index into a list of data (or close to it). All of the results I get are 0.9999+ and very close to each other. I don't know if my whole way of thinking here is off, or if I am just missing an approach, or if perhaps my network code is just broken.
I am trying to make use of the simple neural network from Microsoft here:
https://social.technet.microsoft.com/wiki/contents/articles/36428.basis-of-neural-networks-in-c.aspx
I have tried this with a significantly more complex data set, but I've also tried using a very simple data set. 
Here is the simple training data I'm trying to use:
eat poo bad
eat dirt bad
eat cookies okay
eat fruit good
study poo okay
study dirt okay
study cookies okay
study fruit okay
dispose poo good
dispose dirt okay
dispose cookies bad
dispose fruit bad

The basic idea is that the network has two input neurons and a single output neuron. I assigned a unique number to each distinct word such that I can train the network with two inputs (verb and object), and expect a single output (good, bad, or okay).
Example training:
input: 1 (for eat) 10 (for dirt) output: 15 (for bad)
input: 1 (for eat) 11 (for cookies) output: 16 (for good)

I would expect that after training, I would see the output numbers close to 15, 16, etc, but all I get are numbers like 0.999997333313168, etc.
Example run:
input: 1 (for eat) 10 (for dirt), output is 0.999997333313168 (instead of ~15 expected)

What do these outputs mean, or what am I missing in how I should be thinking about making a basic classification system (given inputs, get a meaningful output)?
The C# code I am using, if it is helpful:
using NeuralNet.NeuralNet;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;

namespace NeuralNet
{
    internal class TestSmallSampleNetwork
    {
        internal static void Run()
        {
            var data = @""eat poo bad
                        eat dirt bad
                        eat cookies okay
                        eat fruit good
                        sell poo bad
                        sell dirt okay
                        sell cookies okay
                        sell fruit okay
                        study poo okay
                        study dirt okay
                        study cookies okay
                        study fruit okay
                        dispose poo good
                        dispose dirt okay
                        dispose cookies bad
                        dispose fruit bad"";
            var simples = data.Split(new[] { ""\r\n"" }, StringSplitOptions.None ).Select(_ => new Simple(_)).ToList();

            var verbs = simples.Select(_ => _.Verb).Distinct().Select(_ => new NetworkValue(_)).ToList();
            var objects = simples.Select(_ => _.Object).Distinct().Select(_ => new NetworkValue(_)).ToList();
            var judgments = simples.Select(_ => _.Good).Distinct().Select(_ => new NetworkValue(_)).ToList();
            var values = verbs.Concat(objects).Concat(judgments).ToDictionary(_ => _.Term, _ => _);

            // Create a network with 2 inputs, 2 neurons on a single hidden layer, and 1 neuron output.
            var net = new NeuralNetwork(0.9, new int[] { 2, 2, 1 });

            for (int iTrain = 0; iTrain < 1000; iTrain++)
            {
                for (int iSimple = 0; iSimple < simples.Count; iSimple++)
                {
                    net.Train(MakeInputs(simples[iSimple], values), MakeOutputs(simples[iSimple], values));
                }
            }

            foreach (var value in values.Values)
            {
                Console.WriteLine(value);
            }

            Console.WriteLine();

            // Run samples and get results back from the network
            Run(""study"", ""poo"", values, net);
            Run(""eat"", ""poo"", values, net);
            Run(""dispose"", ""fruit"", values, net);
            Run(""sell"", ""dirt"", values, net);
        }

        private static void Run(string verb, string obj, Dictionary<string, NetworkValue> values, NeuralNetwork net)
        {
            var result = net.Run(new List<double> {
                values[verb].Value,
                values[obj].Value,
            }).Single();

            var good = ""xxx"";

            Console.WriteLine($""{verb} {obj} {good} ({result})"");
        }

        private static List<double> MakeInputs(Simple simple, Dictionary<string, NetworkValue> values)
        {
            return new List<double>() {
                values[simple.Verb].Value,
                values[simple.Object].Value
            };
        }

        private static List<double> MakeOutputs(Simple simple, Dictionary<string, NetworkValue> values)
        {
            return new List<double> { values[simple.Good].Value };
        }

        public class Simple
        {
            public string Verb { get; set; }
            public string Object { get; set; }
            public string Good { get; set; }

            public Simple(string line)
            {
                var words = line.Trim().Split("" "".ToCharArray(), 3, StringSplitOptions.None);
                Verb = words[0];
                Object = words[1];
                Good = words[2];
            }

            public override string ToString()
            {
                return $""{Verb} {Object} {Good}"";
            }
        }

        public class NetworkValue
        {
            private static int Next = 1;

            public string Term { get; set; }
            public double Value { get; set; }

            public NetworkValue(string term)
            {
                Term = term;
                Value = Next++;
            }

            public override string ToString()
            {
                return $""{Value}. {Term}"";
            }
        }
    }
}

",['neural-networks'],
neuralnetworksanddeeplearning.com chapter 5 problems,"
For http://neuralnetworksanddeeplearning.com/chap5.html , could anyone suggest:
1) how to approach the derivation of expression (123) ?
2) what constitutes value ~ 0.45 ?
3) why the need of taylor series when we can observe the identity property without any maths proof (input == output) ?

","['neural-networks', 'gradient-descent', 'artificial-neuron']",
"Using NEAT, will the child of two parent genomes always have the same structure as the more fit parent?","
I'm trying to implement the NEAT Algorithm using c#, based off of Kenneth O. Stanley's paper. On page 109 (12 in the pdf) it states ""Matching genes are inherited randomly, whereas disjoint genes (those that do not match in the middle) and excess genes (those that do not match in the end) are inherited from the more fit parent.""
Does this mean that the child will always have the exact structure that the more fit parent has? It seems like the only way the structure could differ from crossover was if the two parents were equally fit.
","['neural-networks', 'genetic-algorithms', 'neat']","Yes, if you follow the original implementation the children will inherit the topology from the most fit parent.Keep in mind that the goal is to obtain a good population, maintaining the genetic diversity high but at the same time selecting the best individuals from the population; so, in theory you are allowed to give the topology you prefer to the children.Here there is an example of an alternative topology inheritance in which a child gets the genes that lead to an excess node while another child gets only the genes that create a new connection. "
How can I design a reinforcement learning model for a game with multiple complex actions taken at a time?,"
I have a steady hex-map and turn-based wargame featuring WWII carrier battles.
On a given turn, a player may choose to perform a large number of actions. Actions can be of many different types, and some actions may be performed independently of each other while others have dependencies. For example, a player may decide to move one or two naval units, then assign a mission to an air unit or not, then adjust some battle parameters or not, and then reorganize a naval task force or not.
 Usually, boardgames allow players to perform only one action each turn (e.g. go or chess) or a few very similar actions (backgammon).
Here the player may select

Several actions
The actions are of different nature
Each action may have parameters that the player must set (e.g. strength, payload, destination)

How could I approach this problem with reinforcement learning? How would I specify a model or train it effectively to play such a game?
Here is a screenshot of the game.

Here's another.

","['reinforcement-learning', 'game-ai', 'monte-carlo-tree-search', 'action-spaces', 'hierarchical-rl']",
What AI designs are suited for producing title replacements?,"
Problem: ""For a given news article, generate another title for the article if the article is to be published under a different Publication.""
Which algorithm will be well suited for this? Should I use naive Bayesian or any NLP algorithm? 
","['machine-learning', 'ai-design', 'natural-language-processing']",
Feed forward neural network using numpy for IRIS dataset [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I tried to build a neural network for working on IRIS dataset using only numpy after reading an article (link: https://iamtrask.github.io/2015/07/12/basic-python-network/).
I tried to search the internet but everyone was using ml libraries and found no solution using just numpy. I tried to add different hidden layers to my feed forward neural network still it wasn't converging. I tried to use backpropagation. I used sigmoid and also relu neither of which was successful. 
Can someone please give me the code which will work on IRIS dataset and built only using feed forward neural networks and numpy as the only library or if it is not possible to built such a thing with these constraints then please let me know what goes wrong with these constraints.
Also tell me will it be possible to create a neural network to predict values of a matrix multiplication i.e. if we have A * B = C with matrix A as input and C as output, can we acheive substantial amount of accuracy with feed forward neural networks here?.  
","['neural-networks', 'deep-learning', 'backpropagation', 'gradient-descent', 'feedforward-neural-networks']","You can not use the ready code directly without any manipulation. Because every piece of code is written for specific datasets. In the article that you mentioned, the writer created a small dataset and then he creates an ANN architecture for it. If you want to build an ANN based on Iris dataset, you should think and create an architecture on paper maybe before coding. You should understand the Iris dataset first, I mean you should understand what is going to be your input shapes, and also the shape of weights are related to inputs' shape. After understanding these you should decide the number of layer of your Neural Network. And that tutorial that you followed is explaining all of these clearly. Do not skip and understand everything steps by step.If you want a build ANN architecture by yourself, you must understand every mathematics behind that. (forward and backward propagation, loss and cost, gradient descent) If you do not want to deal with all of these maths, you can use a library such as Keras, then you can create an ANN easily.See this as a source, https://ml-cheatsheet.readthedocs.io/en/latest/
And you see my Kaggle kernel, I create this network with details and only using numpy. Also I used Keras on another section. You can check; https://www.kaggle.com/erdemuysal/gender-recognition-with-lr-and-ann"
Choice of fuzzification function,"
I'm a relative newbie to fuzzie logic systems but I have some knowledge in mathematics. I have the following problem:
I want to fuzzify certain values. Some are in the range [-$\inf$,$\inf$] and some are in the range [$0$,$\inf$]. For the first range I have chosen the sigmoid function:
$f(x) = \frac{1}{1+e^{-x}}$
The question is, which fuzzification process I should use for the second range. Since the function $f(x) = \ln(x)$ transforms [$0$,$\inf$] to [-$\inf$,$\inf$] a natural choice could be:
$f(x) = \frac{1}{1+e^{-\ln(x)}} = \frac{x}{x+1}$
A different function could also be:
$f(x) = 1 - 2^{-x}$
Which one would be more suitable? Particularly when considering that I may want to compare values from both ranges. 
","['math', 'logic', 'fuzzy-logic']",
Appropriateness of 3D Convolutional Neural Network for segmentation of medical image data,"
I have a couple different segmentation tasks that I would like to perform on medical imaging data using CNN's. I'm currently trying to wrap my head around how well a 3D network might work, using a U-Net architecture, but I have some hesitation. 
My specific questions are as follows:
Question 1 - Say we have medical imaging taken at different slices (different heights/depths) of the patient's body. In order for a 3D CNN to work properly on such data, do the images need to be taken at a heights that are close together, i.e. does the 3rd dimension need to be fairly continuous? For example, if you had a 3D stack/volume of 5 images that you wanted to feed to a 3D CNN like so,

img_1_depth_0cm.png
img_2_depth_5cm.png
img_3_depth_10cm.png
img_4_depth_15cm.png
img_5_depth_20cm.png

and the images were taken 5 cm apart from one and other, I would imagine that a 3D convolution might not perform very well because of the 5 cm of depth in between images? Is this an incorrect assumption?
(As a reference point, this nice repository on GitHub was designed for training on images of the brain, but the images do appear to be fairly continuous/close together, almost like a video: https://github.com/ellisdg/3DUnetCNN)
Question 2 - For a 3D network to properly segment non-labelled volumes after training is finished, I know that the input data dimensions would have to match those of the training data. 
But I would also think that the new images must have been taken in a similar fashion (taken at similar depths and in general a similar orientation to the training data) in order for the NN to be able to perform its task. So unless the medical imaging processes are always performed similarly across machines and hospitals, I'm guessing that the NN performance might vary pretty wildly when it tries to segment new data. Is this correct?
","['neural-networks', 'convolutional-neural-networks', 'computer-vision']",
Can Alpha–Beta be used on symmetric zero sum games?,"
This question was asked in an AI exam. How would you answer such question?
","['game-ai', 'game-theory', 'alpha-beta-pruning']",
Can I filter barking sounds on the television? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



My dog goes bonkers every time the sound of a barking dog is heard on a television program.  I never noticed this before but literally every movie or show with an outdoors setting eventually includes the sound of a barking dog.
Is it possible to develop a real-time filter that blocks or masks these sounds?
",['audio-processing'],
"When training an object detection network for one class, should I include empty images in the dataset?","
I fine tuned MobileNetSSD for object detection using a dataset with just one class (~4000 images). All the training images include at least one bounding box related to that class (no empty images). By following the example with the VOC dataset, the labelmap includes two classes, the background and my custom class. However, as I mentioned, there are no annotations related to the background and I  am not sure if there should be any.
Now my fine tuned network performs very well when objects belonging to my class are present, however there are some false detections with very high confidence when the class is not present. Can this be related to the fact that I don't have empty images in my training set?
","['datasets', 'object-recognition']",
"What is the definition of ""soft label"" and ""hard label""?","
In semi-supervised learning, there are hard labels and soft labels. Could someone tell me the meaning and definition of the two things?
","['comparison', 'terminology', 'definitions', 'semi-supervised-learning', 'labels']",
Can we implement GAN (Generative adversarial neural networks) for classication problem like Fraud detecion? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



Problem: Fraud detection
Task : classifying transactions as either fraud/ Non-fraud using GAN 
","['neural-networks', 'generative-adversarial-networks']","Generative networks was the first popular class of topologies that had compound feedback, corrective signaling at more than one layer.  In popular network designs such as MLPs (multilayer perceptrons), CNNs (convolutional neural networks), and LSTM (long short term memory) networks, the backpropagation is a single layer mechanism.  The mechanism distributes corrective a signal creating a closed loop that ideally converges to an optimal network behavior.The speed, accuracy, and reliability of convergence of networks in an AI system depend on a number of engineering choices at a lower level of design and development.Generative networks are designed to achieve a higher level balance between two networks, each of which have their own backpropagation, and thus have a compound feedback topology.Considering the original GAN approach in the context of fraud detection, a discriminative network $D$, in a way, detects fraudulence originating from the generative network $G$, and, in a way, $G$ learns authenticity from the corrective signal provided by $D$, even if the authenticity learned is superficial.  However, in the case of credit card fraud detection from a numerical set of data, the fraudulence is not internal to the system to be designed as in the case of GAN.  Without both the $G$ and $D$ networks, the design is not a generative network.Since the comment stated, ""I have to explore GAN for this problem,"" the key question is this.If this system is to benefit from a GAN, what would the GAN generate that could contribute to the solution?If the answer is nothing, then a GAN is not the correct approach.  A discriminative network without a generative network is not a GAN.  If this question is from an assignment to train a network using a static and already existing data set, a MLP may suffice, depending on the expectations of the teaching staff.  If that is the case, the goal will be to analyze the data set using statistical tools to determine if it is already normalized well enough to lead to excellent convergence accuracy, speed, and reliability.  If normalization is indicated, find ways to present the data to the input layer of the MLP in such a way that each input is relevant and has a well distributed range of values across the data type.Ensure there is no data loss in this process, and remove redundancy where it can easily be removed.  By redundancy removal is meant, if one of the dimensions of data have only four possible values of between 10 and 1,000 bytes, encode the four possible values in two bits because to do so removes unnecessary burden from the training.Similarly, the output layer must be chosen to produce an encoded set of possible output values and the decoding must be written to produce usable output.  In this case, it appears that the labels are, Fraudulent and Authentic.  Which means that the last layer would be a single binary threshold cell.For this simple MLP approach and for many of the approaches mentioned next, there are examples available in Python, Java, C++ and other languages from which implementation can be initiated.  If the design must stand up to a more realistic fraud detection scenario, either for credit cards, junk mail, banking, log-in, or communications those experienced with fraud detection know that the above approach is entirely insufficient except as a starting point for learning.  In such a case, these considerations further complicate the design of the solution.In AI systems that work in real production environments, it may be worth investigating using an LSTM network in combination with a reinforcement learner.  Such may be over a beginner's level of design and implementation experience, but using either an LSTM network or a basic Q-learning approach instead of both of them together may be a reasonable starting point for dedicated students.  If the answer to the question about what a GAN could generate that would be of use must be yes, the following ideas may be helpful.The reason a GAN was suggested would need to be clarified before this answer could provide a more exact direction for AI system topology and project approach."
Image Segmentation Prediction with cropping 256x256 grids is very slow,"
I have only a limited dataset (<25) with large-sized images (>1500x2000) and their pixelwise labels. The aim is to find unusual patterns in this industry dataset and highlight them.
To generate training images I crop 256x256 grids out of every image and do some data augmentation and use these images to train my U-Net.
For my prediction I split my image with numpy again into 256x256px grids and predict every grid separately and put them together to an image. But this will take some time, like >10 Minutes. But it has a quite good accuracy. How can I optimize my prediction to be faster?
Is it faster to create this with a Tensorflow Pipeline? When I want to predict the full image with giving shape(None,None,3), I get ""ConcatOp : Dimensions of inputs should match"" after some time. 
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'computer-vision']",
"Why does the ""reward to go"" trick in policy gradient methods work?","
In the policy gradient method, there's a trick to reduce the variance of policy gradient. We use causality, and remove part of the sum over rewards so that only actions happened after the reward are taken into account (See here http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf, slide 18).
Why does it work? I understand the intuitive explanation, but what's the rigorous proof of it? Can you point me to some papers?
","['reinforcement-learning', 'math', 'policy-gradients', 'rewards', 'reward-to-go']","An important thing we're going to need is what is called the ""Expected Grad-Log-Prob Lemma here"" (proof included on that page), which says that (for any $t$):$$\mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) \right] = 0.$$Taking the analytical expression of the gradient (from, for example, slide 9) as a starting point:$$\begin{aligned}
\nabla_{\theta} J(\theta) &= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \left( \sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \right) \left( \sum_{t=1}^T r(s_t, a_t) \right) \right] \\
%
&= \sum_{t=1}^T \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t'=1}^T r(s_{t'}, a_{t'}) \right] \\
%
&= \sum_{t=1}^T \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t'=1}^{t-1} r(s_{t'}, a_{t'}) + \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right] \\
%
&= \sum_{t=1}^T \left( \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t'=1}^{t-1} r(s_{t'}, a_{t'}) \right] \\
+ \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right] \right) \\
\end{aligned}$$At the $t^{th}$ ""iteration"" of the outer sum, the random variables
$ \sum_{t'=1}^{t-1} r(s_{t'}, a_{t'}) $
and
$ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) $
are independent (we assume, by definition, the action only depends on the most recent state), which means we are allowed to split the expectation:$$\nabla_{\theta} J(\theta) = \sum_{t=1}^T \left( \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \sum_{t'=1}^{t-1} r(s_{t'}, a_{t'}) \right] \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \right] \\
+ \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right] \right)$$The first expectation can now be replaced by $0$ due to the lemma mentioned at the top of the post:$$
\begin{aligned}
\nabla_{\theta} J(\theta)
%
&= \sum_{t=1}^T \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right]  \\
%
&= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta} (a_t \mid s_t) \left( \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right). \\
\end{aligned}
$$The expression on slide 18 of the linked slides is an unbiased, sample-based estimator of this gradient:$$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta} (a_{i, t} \mid s_{i, t}) \left( \sum_{t'=t}^T r(s_{i, t'}, a_{i, t'}) \right)$$For a more formal treatment of the claim that we can pull $\sum_{t'=1}^{t-1} r(s_{t'}, a_{t'})$ out of an expectation due to the Markov property, see this page: https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html"
Python Network for Simple Image Classification,"
I'm wondering if there exists a network for simple image classification. What I mean by this is if I have two image datasets, one of horses and one of zebras, I want to train off the horses and classify an image as either a horse or not a horse, so if I test it on an image of a horse, it says it is a horse, but if I use a zebra, it says it is not a horse. Does any library/project for this exist?
",['classification'],"Using neural networksIf it is as simple as that, probably a neural network consisting of just a simple convolutional network with a few filters, a fully connected layer with 2 neurons, and a softmax layer, might do it.To develop that, you can use Keras wrapper over tensorflow or theano, but I think scikit-learn also provides some basic neural network functionalities.Depending on the simplicity of the images on your dataset, you might do it even with no convolutional layer if there are not many non-linearities, or you might need more complexity (eg. adding convolutional and max pooling layers).I made personally for teaching a workshop a very simple example of image classification, with 3 groups (trees, people, vehicles), after building a dataset collecting and selectioning images from the CIFAR-100 dataset.  You can see this ultra simple example here. However, it is in spanish. Let me translate the most important piece of code from it:So the input for this model is an array of shape [num_rows, 32, 32]. 32x32 is the image size, and I assume 1 color channel. As I say, this is ultra simple. The output, some kind of probability for each of the 3 classes.Using other aproachesHowever, there are many other supervised learning models that you can use for that, most of them supported by scikit-learn, such as kNN, random forests decision trees, RBF SVM, or even just a kernelized multiclass logistic regression. See here."
How can a specific connectivity pattern be stored in an optimally compact representation?,"
I am interested in optimizing the memory capacity of an AGI. Given a specific complex input an AI can create a simplified model. This is a problem that can be solved using sparse coding [1]. However, this solves only the problem of encoding and not the maintenance of online representations—in cognitive terms: the state of mind.
A default cognitive model of short-term memory can be separated in three different stages:
Encoding → Maintenance → Retrieval

One solution is to use specialized hardware [2][3], but I am interested in software approaches to this problem and I would thus like to emphasize that it is the digital representation, which I am most interested in.
With the exception of qubits, the smallest possible representation are binary digits. However, additional architecture is required to represent phase spaces (i.e. floating point precision memory) and higher-order representations maybe include arrays and dictionaries. (Optimizing these are trivial... or simply to be postponed until needed, according to Knuth).

How can a specific connectivity pattern be stored in an optimally
compact representation?
Is there an implementation with concrete example code?
What is the state-of-the-art?*

* I will not specify ""real-time"" here, but the context is humanoid AGI. 

 
[1] Papyan, V., Romano, Y., & Elad, M. (2017). Convolutional Neural Networks Analyzed via Convolutional Sparse Coding. Journal on Machine Learning Research, 18(83): 1–52. arXiv:1607.08194
http://jmlr.org/papers/volume18/16-505/16-505.pdf

[2] LeGallo et al. (2018). Mixed-precision in-memory computing. https://www.nature.com/articles/s41928-018-0054-8


[3] IBM. (2018). IBM Scientists Demonstrate Mixed-Precision In-Memory Computing for the First Time; Hybrid Design for AI Hardware. https://www.ibm.com/blogs/research/2018/04/ibm-scientists-demonstrate-mixed-precision-in-memory-computing-for-the-first-time-hybrid-design-for-ai-hardware/
","['optimization', 'agi', 'memory']",
How do we know the classification boundaries of the data?,"
Consider an image classification problem. Conceptually, we then have some high dimensional space where all the images can be represented as points, and having large enough labeled data set we can build a classifier. But how do we know that our data in this space has some structure? Like this one in two-dimensional case:

If we have a data set with images of, say, cats and dogs, why these two classes are not just uniformly mixed with each other but have some distribution or shape in appropriate space? Why it cannot be like this:

","['classification', 'image-recognition', 'pattern-recognition', 'clustering']","This is the classic question of what structure is or can be. It relates directly to the concepts of generalization, pattern recognition, over-fitting in surface fitting strategies, and learning tabula rasa, Latin for blank slate. The underlying questions are these:How can it be determined whether the organization of data in a set, which appears to correlate well with model $\mathbb{A}$, isn't just a random data set that merely appears to have $\mathbb{A}$ organization.How can it be determined whether the organization of data in a set, which appears to correlate well with model $\mathbb{A}$, isn't generated by a phenomenon that generally exhibits organization that correlates well with $\mathbb{B}$?How can we determine the interrelationship between $\mathbb{A}$ and $\mathbb{B}$, especially since inclusion and overlap relates to the above question?In the second image in the question, if $\mathbb{A}$ is defined as clustering of a type the smallest boundaries of which are not breached by instances of another type with at least five members per cluster, then several false positives would occur if the locations of the points were generated with quantum or thermal noise and the model was intended to illuminate clustering. In such a case there may be the appearance of clusters, but proximity would not be indicative of organization.In the first image in the question, if $\mathbb{A}$ is the compound angular projection of the orbital paths of the planets in this solar system, then the image would appear as noise.If a third image existed of the moon's sky, the same model would likely produce a false negative if the tolerance of inaccuracy in input was high.It is difficult to reliability detect organization in the measured properties of a complex system. The goal of feature extraction, using RBMs or some other approach is to detect patterns. Whether those patterns are organized or merely apparent is a hypothesis that may be strengthened or weakened as the amount of data analyzed increases.If an AI system is trained to detect rats but knows nothing of cats and dogs, one cannot assume it will differentiate cats from dogs on the basis of the closer relationship between rodent and cat features than between rodent and dog features unless the AI system was meta-trained or programmed statically with a specific conception of what a feature is."
"Relationship between input range and channel means, standard deviations for CNNs","
So, I'm using a pretrained PNASNet-5-Large model to do some image classification.
In the file, it says that the input range is in [0,1] (I'm assuming pixel values of input images). The images I have are already in this range.
The channel means and standard deviation for RGB channels are stated as [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] respectively.
Now when I use the torchvision.transforms.Normalize to normalize the images using the stated means and standard deviations, the pixel values get to the range [-1,1].
The code I wrote for normalization:
transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])

I believe I'm missing something fundamental. Should I normalize the images or should I not? Thanks!
","['convolutional-neural-networks', 'classification']",
a question about Zeiler's paper “Deconvolutional Networks”,"
In ""4.1 Learning multi-layer deconvolutional filters"" section, the last paragraph says that ""Since our model is generative, we can sample from it. In Fig. 3 we show samples from the two different models from each level projected down into pixel space. The samples were drawn using the relative firing frequencies of each feature from the training set.""

I don't know how the pictures in Fig.3 are generative. Since that the filters has been learned and feature maps in every layer can be infered, for example, in terms of fruit samples, in ""Layer 1"", is the the first layer's feature map ? i feel that not true...seems like the sample are low-level...paper says ""... from each level projected into pixel space"", these words are short and confuse me.
somebody could explain that for me? thank you very much!
","['deep-learning', 'unsupervised-learning']",
What is the difference between imitation learning and classification done by experts?,"
In short, imitation learning means learning from the experts. Suppose I have a dataset with labels based on the actions of experts. I use a simple binary classifier algorithm to assess whether it is good expert action or bad expert action.
How is this binary classification different from imitation learning?
Imitation learning is associated with reinforcement learning, but, in this case, it looks more like a basic classification problem to me.
What is the difference between imitation learning and classification done by experts?
I am getting confused because imitation learning relates to reinforcement learning while classification relates to supervised learning.
","['reinforcement-learning', 'classification', 'comparison', 'supervised-learning', 'imitation-learning']","Imitation learning is supervised learning applied to the RL setting.In any general RL algorithm (such as Q-learning), the learning is done on the basis of the reward function. However, consider a scenario where you have available the optimal policy in the form of a table, mapping each state to each action. In this scenario you will not care about the rewards received - rather, you'd simply do a table lookup to decide the optimal action. This scenario is impractical in most settings because the table for the optimal policy will be too big. However, if you have enough entries from the table, you can use a general function approximator such as a neural network to find the optimal action. Again, you do not need to look at the rewards, but only at the state $\rightarrow$ action mappings. I do not know imitation learning in detail beyond this, but I suspect in the case of discrete actions (such as in Chess, Go), it would be trained with a cross-entropy objective as is typical of classification tasks.I suspect that the reason this has a different name in the RL setting is because this is different than how the conventional RL algorithms work. Also, much of RL thinking is inspired from everyday interaction / biology. Think of how we learn how to drive, or play sports such as soccer. Typically there is a coach who tells you what actions to take under different conditions, and you imitate those actions. "
Why can't we use Google Translate for every translation task?,"
Once a book is published in a language, why can't the publishers use Google Translate AI or some similar software to immediately render the book in other languages? Likewise for Wikipedia: I'm not sure I understand why we need editors for each language. Can't the English Wikipedia be automatically translated into other languages?
","['neural-networks', 'machine-translation', 'google-translate']","Google has achieved significant progress in AI translation, but it's still no-where near a qualified human translator. Natural language translation is already very challenging, adding domain knowledge to the equation is too much even for Google.I don't think we have the technology to translate an arbitrary book from one language to another reliably."
SARSA won't work for linear function approximator for MountainCar-v0 in OpenAI environment. What are the possible causes?,"
I am learning Reinforcement Learning from the lectures from David Silver. I finished lecture 6 and went on to try SARSA with linear function approximator for MountainCar-v0 environment from OpenAI.
A brief explanation of the MountainCar-v0 environment. The state is denoted by two features, position, and velocity. There are three actions for each state, accelerate forwards, don't accelerate, accelerate backward. The goal of the agent is to learn how to climb a mountain. The engine of the car is not strong enough to power directly to the top. So speed has to be built up by oscillating in the cliff.
I have used a linear function approximator, written by myself. I am attaching my code here for reference :-
    class LinearFunctionApproximator:
      ''' A function approximator must have the following methods:-
          constructor with num_states and num_actions
          get_q_value
          get_action
          fit '''

      def __init__(self, num_states, num_actions):
        self.weights = np.zeros((num_states, num_actions))
        self.num_states = num_states
        self.num_actions = num_actions

      def get_q_value(self, state, action):
        return np.dot( np.transpose(self.weights), np.asarray(state) )[action]

      def get_action(self, state, eps):
        return randint(0, self.num_actions-1) if uniform(0, 1) < eps else np.argmax( np.dot(np.transpose(self.weights), np.asarray(state)) )

      def fit(self, transitions, eps, gamma, learning_rate):
        ''' Every transition in transitions should be of type (state, action, reward, next_state) '''
        gradient = np.zeros_like(self.weights)
        for (state, action, reward, next_state) in transitions:
          next_action = self.get_action(next_state, eps)
          g_target = reward + gamma * self.get_q_value(next_state, next_action)
          g_predicted = self.get_q_value(state, action)
          gradient[:, action] += learning_rate * (g_target - g_predicted) * np.asarray(state)

        gradient /= len(transitions)
        self.weights += gradient

I have tested the gradient descent, and it works as expected. After every epoch, the mean squared error between current estimate of Q and TD-target reduces as expected.
Here is my code for SARSA :-
def SARSA(env, function_approximator, num_episodes=1000, eps=0.1, gamma=0.95, learning_rate=0.1, logging=False):

  for episode in range(num_episodes):
    transitions = []

    state = env.reset()
    done = False

    while not done:
      action = function_approximator.get_action(state, eps)
      next_state, reward, done, info = env.step(action)
      transitions.append( (state, action, reward, next_state) )
      state = next_state

    for i in range(10):
      function_approximator.fit(transitions[::-1], eps, gamma, learning_rate)

    if logging:
      print('Episode', episode, ':', end=' ')
      run_episode(env, function_approximator, eps, render=False, logging=True)

Basically, for every episode, I fit the linear function approximator to the current TD-target. I have also tried running fit just once per episode, but that also does not yield any winning episode. Fitting 10 times ensures that I am actually making some progress towards the TD-target, and also not overfitting.
However, after running over 5000 episodes, I do not get a single episode where the reward is greater than -200. Eventually, the algorithm choses one action, and somehow the Q-value of other actions is always lesser than this action.
# Now, let's see how the trained model does
env = gym.make('MountainCar-v0')
num_states = 2
num_actions = env.action_space.n

function_approximator = LinearFunctionApproximator(num_states, num_actions)

num_episodes = 2000
eps = 0
SARSA(env, function_approximator, num_episodes=num_episodes, eps=eps, logging=True)

I want to be more clear about this. Say action 2 is the one which is the action which gets selected always after say 1000 episodes. Action 0 and action 1 have somehow, for all states, have their Q-values reduced to a level which is never reached by action 2. So for a particular state, action 0 and action 1 may have Q-values of -69 and -69.2. The Q-value of action 2 will never drop below -65, even after running the 5000 episodes.
","['reinforcement-learning', 'gradient-descent']","On doing some research on why this problem might be occurring, I delved into some statistics of the environment. Interestingly, after a small number of episodes (~20), the agent always chooses to take only one action (this has been mentioned in the question too). Also, the Q values of the state-action pairs do not change a lot after just about 20 episodes. Same is the case for policy of environment, as may be expected.The problem is that, although all the individual updates are being done correctly, the following scenario occurs.The update equation used is the following :-Now, any update to function approximator means that the Q value changes not just for the updated (state, action) pair, but for all (state, action) pairs. How much it changes for any specific (state, action) pairs is another issue. Now, since our function approximator is altered, next time we use the previous equation for updating any other state, we use the following equation :-But since Q has itself been changed, the target value of the record 3 changes. This is not desirable. Over time, all the changes cancel each other and Q value remains roughly the same. By using something known as the target function approximator(target network), we can maintain an older version of function approximator, which is used to get the Q-value of next state-action pair while the time of update. This helps avoid the problem and can be used to solve the environment."
Translating a single word Neural Networks,"
I've been given an assignment to create a neural network that will suggest a Croatian word for a word given in any other European language (out of those found here). The words are limited to drinks you can find on a bar menu.
I've looked at many NN examples, both simple and complex, but I'm having trouble with understanding how to normalize the input.
For example, words ""beer"", ""birra"" and ""cervexa"" should all translate to ""pivo"". If I include those 3 in the training set, and after the network has finished training I input the word ""bier"", the output should be ""pivo"" again.
I'm not looking for a working solution to this problem, I just need a nudge in the right direction regarding normalization.
",['neural-networks'],
How should I define the reward function in the case of Connect Four?,"
I'm using RL to train a Network on the game Connect4. It learns quickly that 4 connected pieces is good. It gets a reward of 1 for this. A zero is rewarded for all other moves.
It takes quite a time until the AI tries to stop the opponent from winning.
Is there a way this could be further reinforced?
I thought about giving a negative reward for the move played before the winning move. Thinking about this, I came to the conclusion that this is a bad idea. There'll be always a looser (except for ties), therefore there always be a last move from the losing player. This one hasn't to be a bad one. Mistakes could have been made much earlier.
Is there a way to improve this awareness of opponents? Or does it just have to train more?
I'm not perfectly sure if the rewards will propagate back in a way that encourages this behavior with my setup.
","['reinforcement-learning', 'game-ai', 'rewards', 'reward-design', 'connect-four']","The classic working reward scheme for two player zero sum games (i.e. if I win, you lose and vice versa) is simply:+1 for a win0 for a draw-1 for a lossThese rewards should be associated with the last move made by the player before the game is resolved.I thought about giving a negativ reward for the move played before the winning move. That is what is normally done.Thinking about this I came to the conclusion that this is a bad idea. There'll be always a looser (except for ties), therefor there always be a last move from the loosing player. This one hasn't to be a bad one. Mistakes could have been made much earlier.It's OK, what the agent is learning is the value of a combination of state and action. It will, eventually, correctly associate the negative reward with a chain of board states that decrease in value and that it will then try to avoid.This scenario of delayed rewards, not necessarily related to the choice of the current action, is handled correctly by reinforcement learning algorithms. It is still a tough problem though - a more delayed and difficult to predict reward makes for a tougher RL problem. However, even basic learners, such as Q-learning, can eventually solve this.To get better performance, you might want to look into adding planning algorithms to the RL part."
How should I predict which characters are going to die in a certain movie?,"
How would one go about predicting which characters (actors) are going to die e.g. in the next Avengers movie?
To elaborate a bit, given all leaked scripts (fake or not), interviews of different actors and directors, contracts with different actors and directors. How would/should one go about predicting if an actor is going to die in the upcoming movie or not. 
NOTE: I am not sure, but I think a similar effort has already been made for Game of Thrones. 
","['machine-learning', 'ai-design', 'prediction']",
What is the derivative function used in backpropagration?,"
I'm learning AI, but this confuses me. The derivative function used in backpropagation is the derivative of activation function or the derivative of loss function?
These terms are confusing: derivative of act. function, partial derivative wrt. loss function??
I'm still not getting it correct.
","['backpropagation', 'activation-functions', 'objective-functions']","OverviewThe derivatives of functions are used to determine what changes to input parameters correspond to what desired change in output for any given point in the forward propagation and cost, loss, or error evaluation &mdash whatever it is conceptually the learning process is attempting to minimize. This is the conceptual and algebraic inverse of maximizing valuation, yield, or accuracy.Back-propagation estimates the next best step toward the objective quantified in the cost function in a search. The result of the search is a set of parameter matrices, each element of which represents what is sometimes called a connection weight. The improvement of the values of the elements in the pursuit of minimal cost is artificial networking's basic approach to learning.Each step is an estimation because the cost function is a finite difference, where as the partial derivatives express the slope of a hyper-plane normal to surfaces that represent functions that comprise forward propagation. The goal is to set up circumstances so that successive approximations approach the ideal represented by minimization of the cost function.Back-propagation TheoryBack-propagation is a scheme for distribution of a correction signal arising from cost evaluation after each sample or mini-batch of them. With a form of Einsteinian notation, the current convention for distributive, incremental parameter improvement can be expressed concisely.$$ \Delta P = \dfrac {c(\vec{o}, \vec{\ell}) \; \alpha} {\big[ \prod^+ \! P \big] \; \big[ \prod^+ \!a'(\vec{s} \, P + \vec{z}) \big] \; \big[ c'(\vec{o}, \vec{\ell}) \big]} $$The plus sign in $\prod^+\!$ designates that the factors multiplied must be downstream in the forward signal flow from the parameter matrix being updated.In sentence form, $\Delta P$ at any layer shall be the quotient of cost function $c$ (given label vector $\vec{\ell}$ and network output signal $\vec{o}$), attenuated by learning rate $\alpha$, over the product of all the derivatives leading up to the cost evaluation. The multiplication of these derivatives arise through the recursive application of the chain rule.It is because the chain rule is a core method for feedback signal evaluation that partial derivatives must be used. All variables must be bound except for one dependent and one independent variable for the chain rule to apply.The derivatives include three types.Answer to the QuestionNote that, as a consequence of the above, the derivatives of both the cost (or loss or error) function and any activation functions are necessary.Redundant Operation Removal for an Efficient Algorithm DesignActual back propagation algorithms save computing resources and time using three techniques. In addition to these practical principles of algorithm design, other algorithm features arise from extensions of basic back-propagation. Mini-batch SGD (stochastic gradient descent) applies averaging to improve convergence reliability and accuracy in most cases, provided hyper-parameters and initial parameter states are well chosen. Gradual reduction of learning rates, momentum, and various other techniques are often used to further improve outcomes in deeper artificial networks."
Creating videos of AI generated photographs,"
I came across this article today: These faces show how far AI image generation has advanced in just four years. I would never in a million years have guessed that the people on the right (in the first image in the article) were fakes! 
Will it be possible to create videos of such AI generated images? What, then, will become of actors and actresses?
","['neural-networks', 'machine-learning', 'image-generation']","I wouldn't really consider what they are doing as AI - they are using a script that intelligently overlaps various images of existing people in order to create a new face.Animating those images isn't impossible - essentially extrapolation + additional ""real"" images will be used to know what the face would like from all angles and in all states (happy, sad, intrigued, etc...)."
"In the context of importance sampling ratio, how is the equation $\mathbb{E}\left[\rho_{t: T-1} G_{t} | S_{t}=s\right]=v_{\pi}(s)$ derived?","
When reading the book by Sutton and Barto, I came across the importance sampling ratio.
The first equation, I believe, describes the probability a particular sequence is obtained given the current state, and the policy.
\begin{align}
&\operatorname{Pr}\left\{A_{t}, S_{t+1}, A_{t+1}, \ldots, S_{T} | S_{t}, A_{t: T-1} \sim \pi\right\} \\
&=\pi\left(A_{t} | S_{t}\right) p\left(S_{t+1} | S_{t}, A_{t}\right) \pi\left(A_{t+1} | S_{t+1}\right) \cdots p\left(S_{T} | S_{T-1}, A_{T-1}\right) \\
&=\prod_{k=t}^{T-1} \pi\left(A_{k} | S_{k}\right) p\left(S_{k+1} | S_{k}, A_{k}\right)
\end{align}
The next part takes the ratio between the probabilities of the two trajectories:
$$\rho_{t: T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi\left(A_{k} | S_{k}\right) p\left(S_{k+1} | S_{k}, A_{k}\right)}{\prod_{k=t}^{T-1} b\left(A_{k} | S_{k}\right) p\left(S_{k+1} | S_{k}, A_{k}\right)}=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} | S_{k}\right)}{b\left(A_{k} | S_{k}\right)}$$
I don't understand how this ratio could lead to this:
$$\mathbb{E}\left[\rho_{t: T-1} G_{t} | S_{t}=s\right]=v_{\pi}(s)$$
The $G_t$ rewards are obtained through the $b$ policy, not the $\pi$ policy.
I think there is something to do with Bayes rule, but I could not derive it. Could someone guide me through the derivation?
","['reinforcement-learning', 'math', 'markov-decision-process', 'importance-sampling']","It has nothing to do with Bayes rule, like you said, $G_t$ is the return we get by following the policy $b$ , so the value of the state we get that return in is equal to the expected value of that return. On page 59 of the book, equation 3.14 derives the value of the state as expected value of the return but in this case we are following the behaviour policy $b$ so you would replace $\pi (a \mid s)$ with $b (a \mid s)$ and $v_\pi (s')$ with $v_b (s')$ and you would get the value of the state following the policy $b$, that is $v_b (s)$.
We are actually interested in the value of the state when we follow target policy $\pi$ , but we got the value by following behaviour policy, so we will multiply that expression with importance sampling ratio. If we do that $b (a \mid s)$ terms will cancel out and you are only left with term $\pi (a \mid s)$ from importance sampling ratio so we will get the value by following the target policy $\pi$ that is $v_\pi (s)$"
Is pooling a kind of dropout?,"
If I got well the idea of dropout, it allows improving the sparsity of the information that comes from one layer to another by setting some weights to zero.
On the other hand, pooling, let's say max-pooling, takes the maximum value in a neighborhood, reducing as well to zero, the influence of values apart from this maximum.
Without considering the shape transformation due to the pooling layer, can we say that pooling is a kind of a dropout step?
Would the addition of a dropout (or DropConnect) layer, after a pooling layer, make sense in a CNN? And does it help the training process and generalization property?
","['convolutional-neural-networks', 'training', 'dropout', 'generalization', 'pooling']","Dropout and Max-pooling are performed for different reasons.Dropout is a regularization technique, which affects only the training process (during evaluation, it is not active). The goal of dropout is reduce unnecessary feature dependencies in the network, allowing it to be simpler and improves its generalization abilities (reduces overfitting). In simple terms, it helps the model to learn that some features are an ""OR"" and not an ""AND"" requirements.Max-pooling is not a regularization technique and it is part of the model's architecture, so it is also used during evaluation. The goal of max-pooling is to down-sample an input representation. As a result the model becomes less sensitive to some translations (improving translation invariance).As for your last question, yes. dropout can be used after a pooling layer."
Why do we have to dot product in the Low-rank Bilinear Pooling?,"
I was reading this paper Hadamard Product for Low-rank Bilinear Pooling. I understand what they are trying to say, but I don't know why we have to convert the element-wise multiplication into a scalar (using the dot product)
$$
\mathbb{1}^{T}\left(\mathbf{U}_{i}^{T} \mathbf{x} \circ \mathbf{V}_{i}^{T} \mathbf{y}\right)+b_{i} \tag{2}\label{2}
$$
Why do we have to multiply the resulting vector by the one vector? We would still use the multiplicative interaction between elements if we did not consider multiplying by that one vector.
","['deep-learning', 'papers', 'pooling']",
Is there bidirection sequence-to-sequence neural machine translation?,"
I have heard about bidirectional RNN LSTM units (endcoders-decoders), but my question is - is there bidirectional neural machine translation, that uses A->B weights for the translation in the opposite direction B->A? If not, then what are the obstacles to such system?
","['neural-networks', 'machine-learning', 'natural-language-processing', 'long-short-term-memory']",
How does the NEAT speciation algorithm work?,"
I've been reading up on how NEAT (Neuro Evolution of Augmenting Topologies) works and I've got the main idea of it, but one thing that's been bothering me is how you split the different networks into species. I've gone through the algorithm but it doesn't make a lot of sense to me and the paper I read doesn't explain it very well either so if someone could give an explanation of what each component is and what it's doing then that would be great thanks.
The two equations are:
$\delta = \frac{c_{1}E}{N} + \frac{c_{2}D}{N} + c_{3} .\overline{W}$
$f_{i}^{'} = \frac{f_i}{\sum_{j=1}^{n}sh(\delta(i,j))}$
By the way, I can understand the greek symbols so you don't need to explain those to me
The original paper
","['neural-networks', 'machine-learning', 'neat']","The first equation deals with distance. Delta, or distance, is the measure of how compatible two genomes are with each other. c1, c2 and c3 are parameters you set to dictate the importance of E,  D and W. Note that if you change cc1, c2 or c3, you will most likely also have to change dt, which is the distance threshold, or the maximum distance apart 2 genomes can be before they are separated into different species. E represents the total number of excess genes. D represents the total number of disjoint genes in both genomes, W represents the total weight difference between genes that match, and finally, N represents the number of connections / genes the genome with the larger number of connections has. For example, take the following 2 genomes:Where the 0 index represents innovation number and the 1 index represents weight value. E  would be 1, since there is 1 excess gene, gene 6. D would be 4, since connections 2 and 4 are not in genome 2, and connections 3 and 5 are not in genome 1. W would be .10, since only connection 1 is shared between the two genomes.The second formula is a bit more complicated. From my understanding, correct me if I'm wrong, this is a formula for adjusting fitness. f′i is the adjusted fitness, which will replace the original fitness, fi. For every genome j in the entire population, yes, entire population and not just every genome in its specie, it will calculate the distance between j and i, i being the genome of fitness fi. Then it will sum up all the distance    values, and divide the original fitness fi by the total distance sum, and set f'i to that. Next,Every species is
  assigned a potentially different number of offspring in proportion to the sum of adjusted fitnesses f`i of its member organisms.This assigning of number of species offspring is used so that one specie can't take over the entire population, which is the whole point of speciation in the first place, so in conclusion, these two formulas are vital to the function and efficiency of the NEAT algorithm."
How to approach this handwritten digit recognition?,"
I have multiple pictures that look exactly like the one below this text. I'm trying to train CNN to read the digits for me. Problem is isolating the digits. They could be written in any shape, way, and position that person who is writing them wanted to. I thought of maybe training another CNN to recognize the position/location of the digits, but I'm not sure how to approach the problem. But, I need to get rid of that string and underline. Any clue would be a great one. Btw. I would love to get the 28x28 format just like the one in MNIST.
Thanks up front.

","['convolutional-neural-networks', 'image-recognition', 'computer-vision', 'handwritten-characters']",
How TensorFlow will know if the prediction is true or false?,"
I'm completely new at ML, but really interested. To be honest, read many articles about it, but still don't understand the workings of it.
I just started to understand this example: https://storage.googleapis.com/tfjs-examples/mnist/dist/index.html
My thinking about it is that TF has some resources, some examples of how numbers look like, and try to match them with the ones in the test. I saw that sometimes the test changes a right prediction to a wrong, but makes better and better predictions. But how? I think that the program doesn't know the right predictions (and this way it won't know the wrong ones). In the training how it makes better predictions? Test by test, from what exceptions it will change it's predictions? What happens in a new test?
","['neural-networks', 'machine-learning', 'tensorflow']",
Why is the reward signal normalized in openAI's REINFORCE? [duplicate],"







This question already has answers here:
                                
                            




Why does is make sense to normalize rewards per episode in reinforcement learning?

                                (3 answers)
                            

Closed 2 years ago.



Pytorch's example for the REINFORCE algorithm for reinforcement learning has the following code:
import argparse
import gym
import numpy as np
from itertools import count

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical


parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
                    help='discount factor (default: 0.99)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
                    help='random seed (default: 543)')
parser.add_argument('--render', action='store_true',
                    help='render the environment')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='interval between training status logs (default: 10)')
args = parser.parse_args()


env = gym.make('CartPole-v0')
env.seed(args.seed)
torch.manual_seed(args.seed)


class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.affine1 = nn.Linear(4, 128)
        self.affine2 = nn.Linear(128, 2)

        self.saved_log_probs = []
        self.rewards = []

    def forward(self, x):
        x = F.relu(self.affine1(x))
        action_scores = self.affine2(x)
        return F.softmax(action_scores, dim=1)


policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
eps = np.finfo(np.float32).eps.item()


def select_action(state):
    state = torch.from_numpy(state).float().unsqueeze(0)
    probs = policy(state)
    m = Categorical(probs)
    action = m.sample()
    policy.saved_log_probs.append(m.log_prob(action))
    return action.item()


def finish_episode():
    R = 0
    policy_loss = []
    rewards = []
    for r in policy.rewards[::-1]:
        R = r + args.gamma * R
        rewards.insert(0, R)
    rewards = torch.tensor(rewards)
    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)
    for log_prob, reward in zip(policy.saved_log_probs, rewards):
        policy_loss.append(-log_prob * reward)
    optimizer.zero_grad()
    policy_loss = torch.cat(policy_loss).sum()
    policy_loss.backward()
    optimizer.step()
    del policy.rewards[:]
    del policy.saved_log_probs[:]


def main():
    running_reward = 10
    for i_episode in count(1):
        state = env.reset()
        for t in range(10000):  # Don't infinite loop while learning
            action = select_action(state)
            state, reward, done, _ = env.step(action)
            if args.render:
                env.render()
            policy.rewards.append(reward)
            if done:
                break

        running_reward = running_reward * 0.99 + t * 0.01
        finish_episode()
        if i_episode % args.log_interval == 0:
            print('Episode {}\tLast length: {:5d}\tAverage length: {:.2f}'.format(
                i_episode, t, running_reward))
        if running_reward > env.spec.reward_threshold:
            print(""Solved! Running reward is now {} and ""
                  ""the last episode runs to {} time steps!"".format(running_reward, t))
            break


if __name__ == '__main__':
main()

I am interested in the function finish_episode():
the line
 rewards = (rewards - rewards.mean()) / (rewards.std() + eps)

makes no sense to me.
I thought this might be baseline reduction, but I can't see why divide by the standard deviation.
If it isn't baseline reduction, then why normalize the rewards, and where should the baseline reduction go?
Please explain that line
","['machine-learning', 'reinforcement-learning', 'reward-normalization']",
"If we use a perceptron with a non-monotonic activation function, can it solve the XOR problem?","
I found several papers about how to build a perceptron able to solve the XOR problem. The papers describe a solution where the heaviside step function is replaced by a non-monotonic activation function. Here are the papers:

Single Layer Neural Network Solution for XOR Problem
Solving the XOR and parity N problems using a single universal binary neuron

I also found this related post on Stackoverflow.
Can we really solve the XOR problem with a simple perceptron?
The use of a non-monotonic activation function is not really common, so I don't really know. Papers about this idea are scarce. Generally, the main solution is to build a multilayer perceptron.
","['neural-networks', 'activation-functions', 'perceptron', 'xor-problem']",
Recognition of small objects,"
I'm currently implementing an Android app for street sign recognition. My solution works quite well for the GTSRB dataset, since it provides a labeled test set of centered images. However, it doesn't scale up to more realistic scenarios like for images in the GTSDB, where the signs only take up some pixels. Is it still recommended to downsample the image to 224x224?
","['neural-networks', 'convolutional-neural-networks', 'image-recognition', 'object-recognition']",
How does reinforcement learning handle measured disturbances?,"
I recently encountered an interesting problem and was wondering how RL would solve it.  The objective of the problem is to maximize the coffee quality, given by box X.  The coffee quality objective function is defined by the company.  
To maximize the quality of the coffee, we can perform 2 actions:

change stirring speed of the coffee machine
change the temperature of the coffee machine

Now to the tricky part.  The coffee bean characteristic coming into the coffee machine is random.  We can measure its characteristics before sending them to the coffee machine, but we cannot change them.
I formulated this problem into a control problem, such that $X$ is a function of my previous states, $X(t - k)$, the control input , $U$, and the measured disturbance, $D$. Given a constant $D$, the problem is trival to solve because the disturbance is a constant part of the environment.  However, during times when $D$ changes rapidly, the policy is no longer optimal.
How do I inject the information of the measured disturbance into my RL agent?

","['reinforcement-learning', 'control-problem']",
Why is the Chinese Room argument such a big deal?,"
I've been re-reading the Wikipedia article on the Chinese Room argument and I'm... actually quite unimpressed by it. It seems to me to be largely a semantic issue involving the conflation of various meanings of the word ""understand"". Of course, since people have been arguing about it for 25 years, I doubt very much that I'm right. However... the argument can be thought of as consisting of several layers, each of which I can explain away (to myself, at least).

There is the assumption that being able to understand (interpret a sentence in) a language is a prerequisite to speaking in it.
Let's say that I don't speak a word of Chinese, but I have access to a big dictionary and a grammar table. I could work out what each sentence means, answer it, and then translate that answer back into Chinese, all without speaking Chinese myself. Therefore, being able to interpret (parse) a language is not a prerequisite to speaking it.
(Of course, by the theory of extended cognition I can interpret the language, but we can all agree that the books and lookup tables are simply a source of information and not an algorithm; I'm still the one using them.)
Nevertheless, this task can be removed by a dumb natural language parser and a dictionary, converting Chinese to the set of concepts and relationships encoded in it and vice versa. There is no understanding involved at this stage.
There is the assumption that being able to understand (identify and maintain a train of thought about concepts in) a language is a prerequisite to speaking in it.
We've already optimised away the language, to a set of concepts and relationships between concepts. Now all we need is another lookup table: a sort of verbose dictionary that maps concepts to other concepts and relationships between them. For example, one entry for ""computer"" might be ""performs calculations"" and another might be ""allows people to play games"". An entry for ""person"" might be ""has opinions"", and another might be ""has possessions"". Then an algorithm (yes, I'm introducing one now!) would complete a simple optimisation problem to find a relevant set of concepts and relationships between them, and turn ""I like playing computer games"" into ""What is your favourite game to play on a computer?"" or, if it had some entries on ""computer games"", ""Which console do you own?"".
The only ""understanding"" here, apart from the dumb optimisation algorithm, is the knowledge bank. This could conceivably be parsed from Wikipedia, but for a good result it would probably be at least somewhat hand-crafted. Following this would fall down, because this process wouldn't be able to talk about itself.
There is the assumption that being able to understand (""know"" how information in affects one's self) a language is a prerequisite to speaking in it.
A set of ""opinions"" and such associated with the concept ""self"" could be implemented into the knowledge bank. All meta-cognition could be emulated by ensuring that the knowledge bank had information about cognition in it. However, this program would still just be a mapping from arbitrary inputs to outputs; even if the knowledge bank was mutable (so that it could retain the current topics from sentence to sentence and learn new information) it would still, for example, not react when a sentence is repeated to it verbatim 49 times.
There is the assumption that being able to have effective meta-cognition is a prerequisite to speaking in a language.
Except... there's not. The program described would probably pass the Turing Test. It certainly fulfils the criteria of speaking Chinese. And yet it clearly doesn't think; it's a glorified search engine. It'd probably be able to solve maths problems, would be ignorant of algebra unless somebody taught it to it (in which case, with sufficient teaching, it'd be able to carry out algebraic formulae; Haskell's type system can do this without ever touching a numerical primitive!), and would probably be Turing-complete, and yet wouldn't think. And that's OK.

So why is the Chinese Room argument such a big deal? What have I misinterpreted? This program understands the Chinese language as much as a Python interpreter understands Python, but there is no conscious being to ""understand"". I don't see the philosophical problem with that.
","['philosophy', 'chinese-room-argument']","If you check the Wikipedia article on the argument that you linked, in the History section, you'll note the following statement:Most of the discussion consists of attempts to refute it.I think this directly answers the question,Why is the Chinese Room argument such a big deal?The Chinese Room Argument was a relatively early attempt in the intermingling of Philosophy and Computer Science to make as concrete an argument on the definition of a ""mind"", ""understanding"", and whether or not these can be created with a program. That is, it conjectures that a ""mind"" cannot.It's famous in part because the concreteness of this argument means that a counter-argument can be made in kind. That is, the argument spawned a large quantity of refutations, attacking the argument from a large number of vectors, outlined later in the argument.Put another way,The Chinese Room Argument is such a big deal because of how many people in the field have found value in refuting it.Incidentally, your post provides more evidence to this point."
How can I oppose two AI agents with keras / tensoflow?,"
I am trying to use tensorflow / keras to play a text based game. The game opposes two players that play by answering questions by choosing an answer among the proposed ones.
Game resembles this:

Questions asked from player 1, choose value {0, 1, 2}
Player 1 chooses answer 1
Questions asked from player 2, choose value {0, 1}
Player 2 chooses answer 0
( and so on )

The issue is that I do not have any data to use for training the agents and it not possible to evaluate each actions of the agent individually.
My idea is to get 2 agents to play against each other and evaluate them depending on who won / lost ( the games are very short with about 20 to 30 decisions made for each player ).
The issue I have is that I do not know where to start. 
I normalized my input, but I do not know how to get the 2 agents to compete, as I do not have any training data as shown in the tutorials and the agents have to complete a full game in order to evaluate their performance.
","['tensorflow', 'keras', 'self-play']","Keras/Tensorflow are mostly used of developing/training/deploying neural networks. For descision making problems, if you want to use machine learning, reinforcement learning is in most cases applied. Some reinforcement learning methods use neural networks (and therfore tensorflow) internally. You can check baseline implementations of different methods here. When using reinfocement learnig, for most of the reinforcement learning methods, you do not need and preexisting data, but you need and environment (esentially a simulations of your game). If you interface your game to one of the baseline agents you can start the agents playing ant training ocurs during they play. You can find a tutorial to start an agent here. If you only have a feedback (a reward) after a full game of 20 decisions, it will be really hard to train the agents, as you are facing a good example for what is called the problem of spare reward. There are some ways to deal with sparse reward, like Hindsight Experience Replay and maybe Curiosity, but improving the density of the reward would be very beneficial."
Why do layered neural nets struggle with continous data?,"
In this article here, the writer claims that a new type of neural net is required to deal with data that is both continuous, and also sparsely sampled. 
It was my understanding that this was the entire purpose of techniques that use neural nets, to make assumptions about a system with a non-continuous data set.
So why do we need to switch to a non-layered design to deal with these data sets better?
",['neural-networks'],
Why is the n-step tree backup algorithm an off-policy algorithm?,"
In reinforcement learning book from Sutton & Barto (2018 edition), specifically in section 7.5 of the book, they present an n-step off-policy algorithm that doesn't require importance sampling called n-step tree backup algorithm. 
In other algorithms, the return in the update consisted of rewards along the way and the estimated value(s) of the node(s) at the bottom, but, in tree backup update, a return consists of things mentioned before plus the estimated values of the actions that weren't picked during these n steps, all weighted by the probability of taking the action from the previous step.  
I have a few questions about things in this algorithm that are unclear to me.

Why is this algorithm considered an off-policy algorithm? As far as I could notice, only a single target policy is mentioned and there is no talk about behaviour policy generating actions to take.
In control we want our target policy to be deterministic, greedy policy, so how do we exactly generate actions to take in this case since behaviour policy isn't used? If we generate actions from the greedy policy, we won't explore, so we won't learn the optimal policy. What am I missing here?
If I understood something wrong and we are actually using behaviour policy, I don't understand how would the update work in the case where our target policy is greedy. The return consists of estimates taken from actions that weren't picked, but, because our policy is greedy, the probabilities used in calculating those estimates would be 0, so the total estimate of those actions would be 0 as well. The only non 0 probability in our target policy is the one of the greedy action (which is the probability of 1), so the entire return would fall down to n-step SARSA return. So, basically, how are we allowed to do this update in this case, why is this return allowed to replace the one with importance sampling?

","['reinforcement-learning', 'off-policy-methods']","As for your first two questions: there is indeed a behaviour and a target policy, which can be different. In the example image of the $3$-step tree-backup update in the beginning of the section you mention, the actions $A_t$, $A_{t+1}$, and $A_{t+2}$ are assumed to be selected according to some behaviour policy, whereas a (different) target policy is used to determine weights for the different leaf nodes.As for your third question, in the case where our target policy is greedy, lots of terms will indeed have $0$ weights and therefore entirely drop out. However, this is not always going to fall down to the same return as $n$-step Sarsa returns; that would not be correct, because ($n$-step) Sarsa is an on-policy algorithm. In the case where the target policy is a greedy policy, the return will depend very much on how many actions happened to get selected by the behaviour policy which the greedy target policy also would have ""agreed"" with. If the behaviour policy already happened to have made a ""mistake"" (selected an action that the greedy policy wouldn't have selected) for $A_{t+1}$, you'll end up with a standard $1$-step $Q$-learning update. If the behaviour policy only made a ""mistake"" with the action $A_{t+2}$ (and agreed with the target policy on $A_{t+1}$), you'll get something that kind of looks like a ""$2$-step $Q$-learning update"" (informally here, because $n$-step $Q$-learning isn't really a thing). Etc.You can see that this is the case by closely inspecting Equation 7.16 from the book:$$G_{t:t+n} \doteq R_{t+1} + \gamma \color{blue}{\sum_{a \neq A_{t+1}} \pi (a \mid S_{t+1}) Q_{t + n - 1}(S_{t+1}, a)} + \gamma \color{red}{\pi (A_{t + 1} \mid S_{t + 1}) G_{t + 1 : t + n}}.$$What you essentially end up getting in these cases is returns similar to those of $n$-step Sarsa, but they automatically get truncated as soon as there is disagreement between the behaviour policy and a greedy target policy (all subsequent reward observations get replaced by a single bootstrapping value estimate instead).In the examples above, I assumed completely greedy target policies, since that is what you appeared to be most interested in in your question (and is probably also the most common use-case of off-policy learning). Note that a target policy does not have to be greedy. You can also have non-greedy target policies if you like, and then the returns will obviously change quite a bit from the discussion above (fewer $\pi(S, A)$ terms would evaluate to $0$, there'd be more non-zero terms)."
Should I remove the units of a neural network or increase dropout?,"
When adding dropout to a neural network, we are randomly removing a fraction of the connections (setting those weights to zero for that specific weight update iteration). If the dropout probability is $p$, then we are effectively training with a neural network of size $(1−p)N$, where $N$ is the total number of units in the neural network.
Using this logic, there is no limit how big I can make a network, as long as I proportionately increase dropout, I can always effectively train with the same sized network, and thereby just increasing the number of ""independent"" models working together, making a larger ensemble model. Thereby improving generalization of the model.
For example, if a network with 2 units already achieves good results in the training set (but not in unseen data -i.e validation or test sets-), also a network with 4 units + dropout 0.5 (ensemble of 2 models), and also a network with 8 units + dropout 0.75 (ensemble of 4 models)... and also a network with 1000 units with a dropout of 0.998 (ensemble of 500 models)!
In practice, it is recommended to keep dropout at $0.5$, which advises against the approach mentioned above. So there seem to be reasons for this.
What speaks against blowing up a model together with an adjusted dropout parameter?
","['neural-networks', 'machine-learning', 'deep-learning', 'dropout', 'regularization']",
Is there any research on the identification of a person's feelings using features such as facial expressions or body temperature?,"
People could be sad, happy, depressive, angry, nervous, calm, relaxed, bored, etc. I don't know how to express all of these feelings and emotions in English terms (I'm not an English native speaker), which would enable me to search for research papers about this topic (e.g. in IEEE Xplore, Scopus, or ScienceDirect).
So, is there any research on the identification/recognition of a person's feelings/emotions using facial expressions, heartbeat, body temperature, sweating, or nervous behavior (using one or all of them)?
","['reference-request', 'research', 'sentiment-analysis', 'affective-computing', 'emotion-recognition']",
Classical Internet routing vs. Swarm routing (such as Ant routing)?,"
Is it possible to mention the drawbacks/advantages of Swarm routing (such as Ant routing etc) in comparison with classical routing algorithms in communication networks in a general view?
In other words, what will we gain if we replace a classical routing algorithm with a swarm routing based algorithm? 
Can we compare these two type of routing algorithms in a general view to mention their to count the drawbacks/advantages?
The main purpose of this question is to define applications of each of those routing approaches. Which one is more decentralized? And which one has more efficient performance?
Here is my personal opinion (I am not sure about it) :
classical internet routing in more centralized than swarm routing (such as ACO based routing) which does not use any routing table and router to avoid moving towards centralization where those routing tables and routers can be manipulated (as a point of failure). Instead, classical internet routing may be faster than swarm based routing. Briefly, classical may be more centralized, but faster and on the other side, swarm based is more decentralized but may be slower. Am I wrong?
Please note that when I say ""decentralized"", I mean a network like ""ad hoc networks"" that do not rely on routers or access points (as a point of failure) to avoid moving towards centralization. In case of using routers or access point, some kind of centralization is inherent. With this definition of decentralization, it seems Swarm based routing such as Ant routing would be more decentralized. However, I am not sure about it and that's my main question.
","['optimization', 'swarm-intelligence']",
What is the difference between continuous domains and discrete combinatorial optimization?,"
According to this website: http://yarpiz.com/67/ypea104-acor (in the website it is mentioned that it is a project aiming to be a resource of academic and professional scientific source codes and tutorials.):

""Originally, the Ant Algorithms are used to solve discrete and
  combinatorial optimization problems. Various extensions of Ant Colony
  Optimization (ACO) are proposed to deal with optimization problems,
  defined in continuous domains. One of the most useful algorithms of
  this type, is ACOR, the Ant Colony Optimization for Continuous
  Domains, proposed by Socha and Dorigo, in 2008 (here).""

What is the difference between continuous domains and discrete combinatorial optimization? I appreciate if you could also mention some examples for each type.
","['comparison', 'swarm-intelligence', 'combinatorics', 'ant-colony-optimization']",
Is there a mathematical example for Conditional Random Fields?,"
I am learning about probabilistic graphical models and I was wondering if there is an example explaining the math behind conditional random fields. Looking solely on the formula, I have no idea what we actually do. I found a lot of examples for the hidden Markov model. There is a part speech-tag task where we have to find the tags for the sentence ""flies like a flower"". On these slides (slide 8) Ambiguity Resolution: Statistical Method-Prof. Ahmed Rafea, an HMM is used to find the correct tags. How would I transform this model into a CRF and how would I apply the math?
","['math', 'reference-request', 'generative-model', 'conditional-random-field']",
Any interesting ways to combine Monte Carlo tree search with the minimax algorithm?,"
I've been working on a game-playing engine for about half a year now, and it uses the well known algorithms. These include minimax with alpha-beta pruning, iterative deepening, transposition tables, etc.
I'm now looking for a way to include Monte Carlo tree search, which is something I've wanted to do for a long time. I was thinking of just making a new engine from scratch, but if possible I'd like to somehow import MC tree search into the engine I've already built.
Are there any interesting strategies to import MC tree search into a standard game-playing AI?
","['game-ai', 'monte-carlo-tree-search', 'minimax']","There has indeed been some research towards combining MCTS and minimax-like algorithms. For example, the following two publications:The basic intuition behind such combinations tends to be to use small minimax-like searches inside a larger MCTS search, and/or backing up proven wins/losses (proven in the sense that minimax-like search has established that a certain state is a certain win or loss when following perfect play) through the MCTS tree, in addition to the more standard value estimates.but if possible I'd like to somehow import MC search into the engine I've already built.I'm not sure how easy that would be. As I described above, I think it is more common that small minimax searches are used inside a larger MCTS search. I think that would mean that you'd first want to build an MCTS, and then try to import your already implemented minimax into it. That's kind of the opposite / other way around of what you write that you'd like to do. I don't think integrating small MCTS searches inside a ""larger"" minimax would work well. The value estimates of small MCTS searches would be too unreliable for subsequent use in an ""outer"" minimax-like algorithm."
Reinforcement Learning with Adaptive Action Magnitude,"
How to select an action in a state if the action does not necesarily cause the environment to change state? 
Given 10 states ($S_0$ to $S_9$) and in each state $i$ there are two actions defined $(1,-1)$. $1$ increases a parameter of the environment and $-1$ decreases it.
For example, if the parameter is speed and it is currently 1 rad/s, corresponding to ${S_i}$, an action can be either increase or decrease this and therefore transition to the next state, ideally $S_{i-1}$ or $S_{i+1}$.
It is unclear how to formulate a reinforcement learning problem in this case. The problem is the same magnitude of the parameter change through an action. 
The range of the parameter is (3, 25) and step size is 1. The problem is thath the responce of the environment is not the same in every satate. In some states a parameter change with magnitude 1 results in a state transition in some state the magintude proves to be to small to provoke a transition change. 
For example, if the envitonment is instate $S_1$ and action 1 is applied, how can the magintude of the paremter be adopted in a way which assures that the environment will transition to state $S_2$?  How can the step in the paremeter change be made adaptive? Actually my environment is uncertain that is why I don't know exactly whether this action will take me to next state or not.
For your information, I am using Q learning off policy algorithm. Suppose state 9 is the goal state and  my Q table is $10 \times 2$.
",['reinforcement-learning'],"There are more possible approaches to tackle this problem:Use a reinforcement learning method which can cope with a continous state space. This would eliminate the need for discretizing the state space which in turn, if I understood correctly, leads to problems in transitioning between states. You can also consider selecting a reinforcement learning method that can cope with a continous state space and a continous action space. Actions do not have to be immediate. If your action 1 is transitioning form $S_i$ to $S_{i+1}$ you can have a function wich continously increases the spead until the next state is reached and only then you consider the execution of the action complete. Furthermore, the magnitude of the actions can be dependent on the current state, if it assures state transition. You can add more actions. Use RL for your advantage and define not just $+1$ and $-1$ but $+0.1$, $0.3$, $+0.5$, $+0.7$, $+1$ (and same for deacreasing the speed) etc. Add a negative reward for all actions which do not cause a state transition or which cause higher jump then needed. However, care must be taken to make sure you have the same speed fro each sampling. Eg. if you are in $S_1$ and the action +0.1 does not cause a state transition to $S_2$ you have to reset the state (if possible) or fail the epoch, since the state you are in is not only dependent on the action and the previous state. In other words the speed of e.g. $1 rad/s$ (with a tolarance band) defined as $S_1$ will be slightly higher and ending up in $S_2$ after applying a slight increase in the speed takes less change in the speed than from $S_1$ without the ""unnoticed"" speed change."
Is it possible to clean up an audio recording of a lecture using some type of AI system?,"
Is it possible to clean up an audio recording of a lecture from a smartphone (i.e. remove the background noise) using some type of AI system?
","['deep-learning', 'reference-request', 'algorithm-request', 'audio-processing', 'signal-processing']",
Grouping products and naming groups,"
I'm working on a home tool that will help create a shopping list from a list of recipes chosen for a coming week.
This boils down to:

Extracting ingredients and their quantities from recipes.
Grouping similar ingredients together.
Summing up quantities for similar ingredients.
Naming groups of similar products in a shopping list.

The tasks seem non-trivial for a few reasons.

Similar ingredients are described differently, depending on the recipe book/portal, e.g.:


5 lemons
5 lemons (to be squeezed)
5 fresh lemons
5 big yellow lemons


Recipes lists alternatives for ingredients (e.g., ""3 lemons or 5 limes""), leaving decision up to a user. 
Recipes involve some information about product-preprocessing. For instance, one has to buy lemons instead of lemon juice when the recipe says:


100ml lemon juice
100ml freshly squeezed lemon juice


My language has a complex inflection. For instance, there can be multiple plural forms of a noun and the form of an adjective must be agreed with a form of noun. Adapting NLP algorithms designed for English language might be not straightforward and require some lemmatizing/stemming but not for single words, but whole phrases.
Naming products group is hard. Once fresh lemons and big yellow lemons are group together and their quantities summed up, one need to decide how to name this group in a shopping list, e.g.: ""10 lemons"" or ""10 fresh lemons"".

Is there any research paper that would cover those challenges?
Especially applied in the same domain? 
","['natural-language-processing', 'applications']","I worked at a start-up doing this exact task for two years. There were several developers involved in it, and it took several years; by the end (with a lot of manual input, eg categorising ingredients in an ontology, converting measurements, and mapping ingredients to shopping items from a supermarket inventory) we had something that worked reasonably well.You rightly recognise that it is not trivial... The first step would be to identify the list of ingredients from the recipe (assuming you have scraped it from a website), and turn it into a list; beware that sometimes there might be more than one ingredient in a line. Then you need to analyse the entry: we had a grammar that distinguished between quantities, attributes (fresh), processing instructions (peeled) and a number of other things. The main item would of course be the actual ingredient. Then we would look this up in a supermarket inventory and map it to the closest item (but I don't think this is part of your remit).We tried different ways of analysing the recipe lines, and any grammar/parser is fine, as long as it can deal with partial analyses, as you cannot always rely on lines being complete and well-formed, and there is always scope for additional comments you want to ignore (""I only use the freshest lemons directly imported from Tuscany"", ""get your fishmonger to take the bones out""). In the simplest case you could have a list of items (lemon) with inflections etc, that you try to match in your recipe lines. If you don't care whether it's big lemons or not, don't bother using a full-blown grammar. And even then, you could just look at a list of adjectives preceding an ingredient to identify any modifiers. We used a grammar based on recursive transition networks (RTNs, where the parser was complex but the grammars easy to handle) and one that was a simple phrase structure one (where the parser was simpler, but the grammar was much more complex). Both worked fine.As far as I am aware there is not really much written about this kind of work. The main advice I can give you is to look at as many recipes as possible to get an idea for what you will encounter, and cut corners wherever possible -- it's a big job to do it perfectly, and you can get very far by using simple methods.We also transferred it into different languages (Polish too, which I assume you're working with?). We didn't find that inflections were that big an issue. You just have a list of all inflected forms linking to the lemma. As you have a limited domain, that is a perfectly feasible approach. Just have a master list of ingredients, with all alternate forms (courgette and zucchini are the same thing, for example)."
Are there other approaches to deal with variable action spaces?,"
This question is about Reinforcement Learning and variable action spaces for every/some states.
Variable action space
Let's say you have an MDP, where the number of actions varies between states (for example like in Figure 1 or Figure 2). We can express a variable action space formally as $$\forall s \in S: \exists s' \in S: A(s) \neq A(s') \wedge s \neq s'$$
That is, for every state, there exists some other state which does not have the same action set.
In figures 1 and 2, there's a relatively small amount of actions per state. Instead imagine states $s \in S$ with $m_s$ number of actions, where $1 \leq m_s \leq n$ and $n$ is a really large integer.

Environment
To get a better grasp of the question, here's an environment example. Take Figure 1 and let it explode into a really large directed acyclic graph with a source node, huge action space and a target node. The goal is to traverse a path, starting at any start node, such that we'll maximize the reward which we'll only receive at the target node. At every state, we can call a function $M : s \rightarrow A'$ that takes a state as input and returns a valid number of actions.
Approches

A naive approach to this problem (discussed here and here) is to define the action set equally for every state, return a negative reward whenever the performed action $a \notin A(s)$ and move the agent into the same state, thus letting the agent ""learn"" what actions are valid in each state. This approach has two obvious drawbacks:

Learning $A$ takes time, especially when the Q-values are not updated until either termination or some statement is fulfilled (like in experience replay)

We know $A$, why learn it?



Another approach (first answer here, also very much alike proposals from papers such as Deep Reinforcement Learning in Large Discrete Action Spaces and Discrete Sequential Prediction of continuous action for Deep RL) is to instead predict some scalar in continuous space and, by some method, map it into a valid action. The papers are discussing how to deal with large discrete action spaces and the proposed models seem to be a somewhat solution for this problem as well.

Another approach that came across was to, assuming the number of different action set $n$ is quite small, have functions $f_{\theta_1}$, $f_{\theta_2}$, ..., $f_{\theta_n}$ that returns the action regarding that perticular state with $n$ valid actions. In other words, the performed action of a state $s$ with 3 number of actions will be predicted by $\underset{a}{\text{argmax}} \ f_{\theta_3}(s, a)$.


None of the approaches (1, 2 or 3) are found in papers, just pure speculations. I've searched a lot, but I cannot find papers directly regarding this matter.
Does anyone know any paper regarding this subject? Are there other approaches to deal with variable action spaces?
","['reinforcement-learning', 'reference-request', 'deep-rl', 'function-approximation', 'action-spaces']",
How can I prevent the CNN from classifying a new input into one of the existing labels (it was trained with) when the input has a new different label? [duplicate],"







This question already has answers here:
                                
                            




How to implement an ""unknown"" class in multi-class classification with neural networks?

                                (2 answers)
                            

Closed 2 years ago.



I'm trying to perform image classification with a CNN. In my case, the inputs are the covers of 9 books, so there are 9 labels. I am using TensorFlow's Keras.
If I pass a new input (that has a label different than one of the 9 labels the CNN was trained with), it will be classified as one of the 9 books, even though it's not a book (but it's e.g. a wall, sofa, house, etc.). I want to avoid this. I want the model to first classify whether there is a book in the image and then classify the book in 9 classes. How could I achieve this?
","['deep-learning', 'image-recognition', 'keras', 'object-detection', 'multiclass-classification']","You can introduce another class to your network - ""not a book"". After that, you will need to add new data to your dataset, random images that do not contain books to classify and train your network on that data. So when your network won't see a book it will output high probability for ""not a book"" class, if an image with a book will be shown to the network probability of the ""not a book"" class should be low."
When should I use simulated annealing as opposed to a genetic algorithm?,"
What kind of problems is simulated annealing better suited for compared to genetic algorithms?
From my experience, genetic algorithms seem to perform better than simulated annealing for most problems.
","['genetic-algorithms', 'search', 'optimization', 'simulated-annealing']",
What kind of search method is A*?,"
What kind of search method is A*? Explain to me with an example.
","['algorithm', 'search', 'definitions', 'a-star']",
,,,
How does Hearthstone AI deal with random events,"
I want to learn a lot about the AI of CCG, such as Hearthstone. And now I have known one of the main algorithms that used in this kind of games, MCTS. It analyses the most promising moves, and expands the search tree based on random sampling of the search space. But there are too many random events in this game that can cause different results to one battle. For example, a card can randomly deal X damage to a hero or other follower, and X is a random number from 0 to 30. The number of X is important for the next decision, but there will be a low accuracy by only using MCTS.
So what does the AI do to deal with these random events?
","['machine-learning', 'game-ai', 'monte-carlo-tree-search']","The most ""standard"" implementation of MCTS probably involves storing copies of game states inside nodes. This works fine for deterministic games, but not for non-deterministic games due to the reasons you mentioned.In non-deterministic games, one of the easiest ways to make MCTS work is to take the perspective that every node in your tree should map to / represent / encode a trajectory or sequence of actions, rather than a state. This means that every node collects statistics (num wins / num visits etc.) for the trajectory of actions it represents, rather than for the state it represents.This perspective can be implemented very easily by simply not storing any game states in nodes at all. Instead, whenever you traverse the tree (in the Selection phase of MCTS), you should simply re-simulate all the actions along that path through the tree, starting from the root state again. This way, different visits through a single node can result in different game states being ""observed"" in that node due to nondeterministic game state transitions. In the limit (after an infinite amount of time), every node will estimate the expected value of the trajectory of actions leading to that node, rather than the game-theoretic value of a single, deterministic resulting game state. This whole idea is sometimes referred to as ""Open-Loop MCTS"", for example in this paper.What I described above is a simple way to get better performance from MCTS in games involving nondeterministic state transitions as you described (such as random amounts of damage being dealt), but there are many much more complicated ideas out there too. I suspect more complex approaches will especially be necessary to deal with more ""drastic"" forms of nondeterminism (resulting from random card draws, or resulting from randomization-based approaches for dealing with imperfect information such as not knowing which cards your opponent has).There was a Hearthstone AI competition relatively recently, and some useful ideas may be found, for example, on this page of work related to that competition."
What are examples of daily life applications that use simulated annealing?,"
In AIMA, 3rd Edition on Page 125, Simulated Annealing is described as:

Hill-climbing algorithm that never makes “downhill” moves toward states with lower value (or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maximum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly at random from the set of successors—is complete but extremely inefficient. Therefore, it seems reasonable to try to combine hill climbing with a random walk in some way that yields both efficiency and completeness. Simulated annealing is such an algorithm. In metallurgy, annealing is the process used to temper or harden metals and glass by heating them to a high temperature and then gradually cooling them, thus allowing the material to reach a lowenergy crystalline state. To explain simulated annealing, we switch our point of view from hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a
  ping-pong ball into the deepest crevice in a bumpy surface. If we just let the ball roll, it will come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the local minimum. The trick is to shake just hard enough to bounce the ball out of local minima but not hard enough to dislodge it from the global minimum. The simulated-annealing solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the intensity of the shaking (i.e., lower the temperature)

I know its about its example, but I just want more examples where Stimulated Annealing used in daily life 
","['search', 'applications', 'simulated-annealing']",
Geometry shape identification and vertex/side label association,"
I want like to be able to draw a shape outline e.g. (pen and paper) triangle, square, circle.. then label the vertices and sides And have ML identify the shape and the symbol associates with each vertex / side.
For example a triangle and I label it with the adjacent, opposite and hypotenuse. Or draw parallel and perpendicular lines and label angles and such.
I would prefer it make one myself rather than use a pre built one. However if you know of a simple pre built one then I will ould love to pick it apart.
Can you please share some guidance on how to solve the above problem, namely shape identification / vertex and side labelling.
Any language
",['image-recognition'],
Is there a mechanism in the human brain that works analog to LSTMs?,"
Is there a mechanism in the human brain that works analog to LSTMs? Is there a biological/neuroscientific interpretation of LSTMs and recurrent neural networks? How do long-term and short-term memories work in the brain, on a neuron level?
I would also appreciate links to more in-depth explanations in the literature.
","['comparison', 'recurrent-neural-networks', 'long-short-term-memory', 'brain', 'neuroscience']",
"Is it possible to generate ""Karel the robot"" programs with genetic programming?","
Karel the robot is an education software comparable to turtle graphics to teach programming for beginners. It's a virtual stack-based interpreter to run a domain-specific language for moving a robot in a maze. In its vanilla version, the user authors the script manually. That means, he writes down a computer program like

move forward
if reached obstacle == true then stop
move left.

This program is then executed in the virtual machine.
In contrast, genetic programming has the aim to produce computer code without human intervention. So-called permutations are tested if they are fulfilling the constraints and, after a while, the source code is generated. In most publications, the concept is explained on a machine level. That means assembly instructions are generated with the aim to replace normal computer code.
In ""Karel the robot"" a high-level language for controlling a robot is presented, which has a stack, but has a higher abstraction. The advantage is, that the state space is smaller.
My question is: is it possible to generate ""Karel the robot"" programs with genetic programming?
","['game-ai', 'applications', 'genetic-programming', 'robots']","A genetic algorithm is used to find an optimal solution to a problem. The parameters of the problem are encoded in the population, where each individual contains a solution to the problem in itself. Each individual is evaluated using a fitness function, and then the highest scoring individuals are used to create a new set of individuals by mixing up their parameters, and this new set replaces the worst scoring individuals from the previous generation. Over time, the average fitness of the population will (hopefully) increase, and a good solution be found. Essentially, the search space is explored in parallel through the many individuals in the population, which means that local maxima can probably be avoided.The question now is: What is the problem you want to optimise?The Karel language is fairly simple, and has few tokens. A program should be easily encodable in the right format, though genetic algorithms usually require individuals to have the same size (though shorter programs could be padded with no-ops). Recombining the best programs could mean swapping tokens. Maybe syntactic constraints could be factored into this to avoid crass syntactic errors. That should not be an insurmountable problem.However, that still leaves the question of the fitness function. A basic criterion would be syntactic: if the program doesn't parse, it's not 'fit'. This could potentially wipe out a lot of offspring, and doesn't really help you much: it would produce more or less random programs which are syntactically well-formed.What you really want is to evaluate the outcome of the programs. Like, which program can best navigate a maze. So you need to execute them and see what they produce. Then you run straight away into the halting problem: your little program might contain an endless loop and will never terminate. So you need to add a kind of time-out, or a limit on the steps the program can execute. This should be easy to do in a virtual machine.So, to answer your question: Yes, it is possible to turn creating Karel programs into a problem that a genetic algorithm can be applied to. However, you need to be clear about what problem you want to solve. I don't quite understand what you mean by ""learning from demonstration"" or how you can achieve the aim of replicating human actions, but if you can somehow encode that in a fitness function, GAs are a tool you can use."
How to use AI to depth map video? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



To be honest, I had no idea where to put this question, but it's sure that it's related to AI. I want to build an application which uses camera, and by the movement it can calculate the
-camera's position compared to the objects
-the objects creator and edge points by the movement.
What it means that if the camera is in a static position, it's just a picture. A set of coloured pixels. If we move the camera, we calculate the time, the gyroscope's values, but most importantly, we can have a comparison of two images taken by the same objects. This way:
-we can detect the edges
-from the edges, we can detect which is closer than the others
Today's phone camera's are accurate enough to create ~60 crystal clear images per second, and it should be enough resource to accurately create high res models from just moving the camera according to some instructions (that's why I'm surprised why it isn't existing in just a phone app). Here comes the problem. I think the idea is worth for the try, but I'm just a JavaScript developer. The browser can have access to the camera, with TensorFlow I can use machine learning to detect edges, but if I want to be honest, I have no idea where to start, and how to continue step by step. Can you please provide me some guidelines how it would be ideal to create the idea?
","['machine-learning', 'deep-learning', 'mapping-space']",
What does it mean to do multi-dimensional processing with tensors in tensor cores?,"
In some tweets about NeurIPS 2018, this video from NVIDIA appeared. At around 0.37, she says:

If you think about the current computations in our deep learning systems, they are all based on Linear Algebra. Can we come up with better paradigms to do multi-dimensional processing? can we do truly tensor-algebraic techniques in our tensor cores?

I was wondering what she is talking about. I'm not an expert so I'd like to understand better this specific point.
","['machine-learning', 'deep-learning', 'ai-design', 'linear-algebra']",
Maximum number of neurons in a layer given number of neurons in previous layer,"
Consider an extremely complicated feed-forward neural network training example but with no need of computational efficiency or limiting of processing time. 
What is the maximum number of hidden neurons h that a hidden layer should possess to detect all unique features/ correlations between input data from the previous layer which has n nodes?
In other words if we wanted to create a neural network with a large number of neurons in a hidden layer, what is the maximum neuron count possible that helps the network train (give n neurons are in the previous layer)?
","['neural-networks', 'neurons', 'feedforward-neural-networks', 'hyper-parameters']",
What is the main difference between additive rewards and discounted rewards?,"
What is the difference between additive and discounted rewards?
","['reinforcement-learning', 'comparison', 'rewards', 'reward-clipping']","Discounted reward has its opposite undiscounted reward. The difference between these is that discounted has multiplier gamma != 1 and undiscounted is gamma = 1. Gamma tells the multiplier, how much previous values are valued in next iterations. [1]Additive refers to different thing, a note found in [2]:An additive reward function decomposes the reward into a number of contributions and can be represented as a (non-partiotioning) MPA.This short excerpt does not reveal lot, but was the only I could find that made some sense to me. What I could indeed find out was that although they seem similar concepts by name, they appear totally different by nature.Sources:[1] https://en.wikipedia.org/wiki/Reinforcement_learning[2] The Logic of Adaptive Behavior: Knowledge Representation and Algorithms for Adaptive Sequential Decision Making Under Uncertainty In First-Order and Relational Domains.- M. Van"
How to get a binary output from a Siamese Neural Network,"
I'm trying to train a Siamese network to check if two images are similar. My implementation is based on this. I find the Euclidian distance of the feature vectors(the final flattened layer of my CNN) of my two images and train the model using the contrastive loss function. 
My question is, how do I get a binary output from the Siamese network for testing (1 if it two images are similar, 0 otherwise). Is it just by thresholding the Euclidian distance to check how similar the images are? If so, how do I go about selecting the threshold? If I wanted to measure the training and validation accuracies, the threshold would have to be increased as the network learns better. Is there a way to learn this threshold for a given dataset? 
I would appreciate any leads, thank you.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks']",
Why can't the XOR linear inseparability problem be solved with one perceptron like this?,"
Consider a perceptron where $w_0=1$ and $w_1=1$:

Now, suppose that we use the following activation function
\begin{align}
f(x)=
\begin{cases}
1, \text{ if }x =1\\
0, \text{ otherwise}
\end{cases}
\end{align}
The output is then summarised as:
\begin{array}{|c|c|c|c|}
\hline
x_0 & x_1 & w_0x_0 + w_1x_1 & f( \cdot )\\ \hline
0 & 0 & 0 & 0 \\ \hline
0 & 1 & 1 & 1 \\ \hline
1 & 0 & 1 & 1 \\ \hline
1 & 1 & 2 & 0 \\ \hline
\end{array}
Is there something wrong with the way I've defined the activation function?
","['neural-networks', 'activation-functions', 'perceptron', 'xor-problem']","It can be done.The activation function of a neuron does not have to be monotonic. The activation that Rahul suggested can be implemented via a continuously differentiable function, for example $ f(s) = exp(-k(1-s)^2) $ which has a nice derivative $f'(s) = 2k~(1-s)f(s)$. Here, $s=w_0~x_0+w_1~x_1$. Therefore, standard gradient-based learning algorithms are applicable.The neuron's error is $ E = \frac{1}{2}(v-v_d)^2$,
where $v_d$ - desired output, $v$ - actual output. The weights $w_i, ~i=0,1$ are initialized randomly and then updated during training as follows
$$w_i \to w_i - \alpha\frac{\partial E}{\partial w_i}$$
where $\alpha$ is a learning rate. We have
$$\frac{\partial E}{\partial w_i} = (v-v_d)\frac{\partial v}{\partial w_i}=(f(s)-v_d)~\frac{\partial f}{\partial s}\frac{\partial s}{\partial w_i}=2k~(f(s)-v_d)(1-s)f(s)~x_i$$
Let's test it in Python.For training, I take a few points randomly scattered around $[0, 0]$, $[0, 1]$, $[1, 0]$, and $[1, 1]$. Activation function:Initialize the weights, and train the network:The weights have indeed converged to $w_0=1,~w_1=1$:The training error is decreasing:Let's test it. I create a testing set in the same way as the training set. My test data are different from my training data because I didn't fix the seed.I calculate the root mean squared error, and the coefficient of determination (R^2 score):Result:... or I am doing something wrong? Please tell me."
"Implementing AI/ML for the card game ""Cheat""","
Background info
In Python, I've implemented a rudimentary engine to play ""Cheat"", supporting both bots and a human or only bots. When only bots are playing, the game is simulated.
When placing cards, input is represented by an array of integers corresponding to the indices of the cards. When bots play, they are presented with all valid combinations of cards to place (currently, the choice made is random). For example, if the bot has two cards, their options are:
[[0], [1], [0, 1]]
After a player places cards, the other players get a chance to call cheat (true to accuse, false not to).
When a player depletes their cards, they are appended to the winners list. The goal of the game is to have the lowest index possible in the winners list.
Summary of game data and end goal
In summary, here is the data which I believe would be useful for the bots to play:

the current number of cards that have been placed
the current type (e.g. Ace) to play
the type of and suit of each card of the bot's hand
the number of cards that were just placed by a player
the possible inputs to play during a bot's turn
the options for calling cheat (true, false)

with the goal of ending up with the lowest index in the winners list.
Help wanted
I'm very new to machine learning, so I apologize for a such a high level question, but how might I go about using a Python module to implement a system for bots to learn to play intelligently as they play? Are there any modules which you think would be ideal for this situation?
Thank you!
","['machine-learning', 'game-ai', 'python']",
Can a LFSR be approximated by a Neural Network?,"
I was wondering whether a LFSR could be approximated by a NN (output or current state). We know that a LFSR is called linear in some sort of mathematical sense, but is that true? Considering it follows Galois field mathematics. So can a Neural Network approximate a LFSR?
Answers with mathematical proof or actual experience is preferred.
","['neural-networks', 'deep-learning']",
"In speech recognition, what kind of signal is used?","
Speech is a major primary mechanism of communication between humans. With respect to artificial intelligence, which signal is used to identify the sequence of words in speech?
",['natural-language-processing'],"The signal path in speech recognition as one travels further from the basic representations of sound depends increasingly on the role the recognition plays. Consider these roles.There are similar differences in other natural language facilities of reading, writing, typing, speaking, interpreting body language, and expressing with body language. With auditory language, the stages of sensory processing proceed in a particular order of signal types, each representing the dynamics of language at successively abstract levels, essentially reversing the process of speaking.The first six elements are common between all language oriented hearing. The seventh is where the above four roles diverge. In translation and comprehension, comprehension of linguistic nuance is required. Colloquialisms, euphemisms, culturally dependent references and analogies and other higher level linguistic and social constructs is required for full proficiency. For transcription, comprehension is required only to improve transcription reliability and to distinguish between phononyms like to, too, and two.Notice that the sixth level is not words or sentences. Words and sentences are later developments in human language and occur later in an individual's childhood. Our educational systems are largely word oriented, but people do not talk or listen in words. There is no signal for ""way"" in the brain when someone hears, ""No way!"" which, in the current semantic state of U.S. English is a single linguistic element that means, ""What you just said is very surprising to me."" Two written words represent this one linguistic element.Conversely, there are two distinct signal representations for, ""wanted"", specifically, ""want,"" and ""-ed."" The second of the two is reused for, ""planted."" The -ed ending is not relearned for every verb.Consider these lines out of a dialog.Jenna: Cats run.Chelsea: That's ridiculous.In transcription, the voices must be distinguished and the words must be written, including the contraction of, ""That is,"" whereas the plural of cats could be possessive, so the computer attempting this transcription must somehow chose the plural over the possessive in this case. Out of context, the pronoun, ""That,"" does not refer to the cat or a person named Cat but the idea of the running. Consider this conversation between people out for food, drink, and laughs.Catherine: He's so sexy.Jenna: Cat, you always pine over the really toxic guys.Catherine: No I don't.Jenna: What about that cute (and well dressed) guy that wanted to meet you for lunch last week?Chelsea: She gave him her number.Julia: Yea, with the last two digits transposed.Chelsea: All the Jennas I've ever met are first to run when a nice guy comes around.Catherine nods. (beat) Patrick comes to the table.Patrick: Hey, you're Catherine, from Central High, right? Didn't we have study hall together in junior year?Catherine puts some cash on the table for the drinks.Catherine: I didn't go to Central. I went to East Barnard. (beat) Girls, I'll see you later. Gotta go so I can get up for work tomorrow.Catherine exits. Patrick wanders off.Jenna: Cats run.Chelsea: That's ridiculous.Julia: What, you think she left because of work?Chelsea: Why not?Jenna: She works at a restaurant closed on Mondays.This dialog is a series of signals relying heavily on understandings that are not in the language. There is no formal grammar that could faithfully guide parsing by words. The signals after all seven levels of processing still fall far short of complete descriptions of the scenarios or the ideas of the individuals involved. Yet a series of themes and triggers to existing information shared in common between those at the table communicate volumes of information.Level six from the layers listed above is important here because people are talking and possibly loudly at other tables. Level seven of the listening is entirely different at this girl's night out and may not share any significant functionality with the listening process when Jenna, Chelsea, Catherine, and Julia are in class taking notes.For computers to partake in a conversation like this one and actually socialize, many more functions beyond the seven layers of abstraction above must be assembled and must learn all the nuance and information brought in as cultural reference. For example, there are several cultural ideas involved."
How do we prove the n-step return error reduction property?,"
In section 7.1 (about the n-step bootstrapping) of the book Reinforcement Learning: An Introduction (2nd edition), by Andrew Barto and Richard S. Sutton, the authors write about what they call the ""n-step return error reduction property"":

But they don't prove it. I was thinking it should not be too hard but how can we show this? I was thinking of using the definition of n-step return (eq. 7.1 on previous page):
$$G_{t:t+n} = R_{t+1} + \gamma*R_{t+2} + ... + \gamma^{n-1}*R_{t+n} + \gamma^{n}*V_{t+n-1}(S_{t+n})$$
Because then this has the $V_{t+n-1}$ in it already. But in the definition above of the n-step return it uses $V_{t+n-1}(S_{t+n})$ but on the right side of the inequality (7.3) that we want to prove it is just little s $V_{t+n-1}(s)$ ? So kind of confused here which state s it is using? And then I guess after this probably pull out a $\gamma^{n}$ term or something, how should we go from here?
This is the newest Sutton Barto book (book page 144, equation 7.3):
https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view
","['reinforcement-learning', 'q-learning', 'math', 'proofs', 'sutton-barto']",
How do we find the length (depth) of the game tic-tac-toe in adversarial search?,"
When we perform the tic-tac-toe game using adversarial search, I know how to make a tree. Is there a way to find the depth of the tree, and which level is the last level?
","['game-ai', 'search', 'minimax', 'tic-tac-toe', 'adversarial-search']","If you mean minimax search as adversarial, you can take a number of free spaces on the board as an upper bound of the tree depth. "
How to encode Azul game state as NN input,"
Question to NN practicioners. I'd like to encode Azul board game state as an input to NN, let's focus on 2-player variant for a while.

There are 5 round ""Factories"" on the table (7 on picture, ignore it). Each one can keep 4 tiles of 5 colors. There is also center of the board which can keep up to 15 tiles. What are the advantages and disadvantages of different encodings? Here are my ideas:

Every Factory has five integer counters, one for each tile color. Sample encoding of single Factory: [3,1,0,0,0]
Every Factory has 20 binary flags, four for each color. Single flag encodes presence of tile of given color. Sample encoding of single factory: [1,1,1,0, 1,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0]
Every Factory has 20 binary flags, four for each color, but only one flag can be set for given color and position of raised flag encodes number of tiles of given color. Sample encoding of single factory: [0,0,1,0, 1,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0]
Every Factory has 4 enum fields, with 6 possible values each (5 colors + empty). Sample encoding of single factory: [red, red, red, blue] or [Empty,Empty,Empty,Empty]

(note: encoding schema would also cover center of the board, up to 15 tiles, as said earlier. Of course player's board would also be encoded, but I don't want to ask too broad question)
I'd like to train NN to play Azul, which means it needs to properly process number of tiles taken in given round (up to 15 in theory, 2-4 in practice) because it would also need to indicate where to put all those tiles to player board positions.
Based on your experience, which encoding is most promising? Or maybe there is some better method I didn't think of? Or it is not possible to tell or it doesn't matter?
","['neural-networks', 'deep-learning', 'game-ai', 'combinatorial-games']",
Can Machine Learning be applied to decipher the script of lost ancient languages?,"
Can Machine Learning be applied to decipher the script of lost ancient languages (namely, languages that were being used many years ago, but currently are not used in human societies and have been forgotten, e.g. Avestan language)?
If yes, is there already any successful experiment to decipher the script of unknown ancient languages using Machine Learning?   
","['machine-learning', 'natural-language-processing', 'applications']",
Label arrangement for custom Keras image generator,"
I am trying to generate 90 and 270 degrees rotated versions of my sample images on the fly during training. I found an example and modifying it. But I am confused about what should be the order? For instance in one batch I have 32 images and my image generator should return total 64 images. Let's say upper case letters are 90 degree and lower case letters are 270 degree rotated images. Should the order be AaBbCc or ABCabc? I apply the same to the validation set. Here is the related code fragment:
Edit: Code fragment added.
    def _get_batches_of_transformed_samples(self, index_array):
    # create list to hold the images
    batch_x = []
    # create list to hold the labels      
    batch_y = []
    # rotation angles
    target_angles = [0, 90, 180, 270]
    angle_categories = list(range(0, len(target_angles)))
    self.classes = target_angles
    self.class_indices = angle_categories
    # generate rotated images and corresponding labels
    for i, j in enumerate(index_array):           
        is_color = int(self.color_mode == 'rgb')
        image = cv2.imread(self.filenames[j], is_color)
        if is_color:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                               

        for rotation_angle, cat_angle in zip(target_angles, angle_categories):
            rotated_im = rotate(image, rotation_angle, self.target_size[:2])
            if self.preprocess_func: rotated_im = self.preprocess_func(rotated_im)
            # add dimension to account for the channels if the image is greyscale
            if rotated_im.ndim == 2: rotated_im = np.expand_dims(rotated_im, axis=2)                                
            batch_x.append(rotated_im)
            batch_y.append(cat_angle)

    # convert lists to numpy arrays
    batch_x=np.asarray(batch_x)
    batch_y=np.asarray(batch_y)

    batch_y = to_categorical(batch_y, len(target_angles))            
    return batch_x, batch_y

I actually rotate them as 0, 90, 180 and 270 degrees. As seen in the code, for each batch I return all the rotated versions of all the images in the batch. But is this correct or should I return first 0 degree rotated versions, second 90 degree rotated versions so on?
Edit2: I checked my previous work which I use the built in Keras ImageDataGenerator. generator.classes returns [zeros(100,1); ones(100,1)]. In that study I only have two classes. I understand that Keras indexes the images as [class1, class2, ...]. I think I have to do the same.
","['python', 'keras']","I am not allowed to comment, so I am adding it here. Not sure I understand, are you asking if the training data must be ordered?If so, training data must be random, and not AaBbCc or ABCabc.
Here the input to the NN will be the source image, and a category 90 degrees or 270 degrees as too separate inputs. i.e. inputs [0...k][k+1][k+2], where 0 - k are normalised pixel data, k+1 is 1 for 90 degree rotation else 0, and k+2 is 1 for 270 degree rotation else 0. Alternatively for more rotation options lets make a = input [k+1] and b = input [k+2]. If a = 0 and b = 0 then 0 degrees, if a = 0 and b = 1 then 90 degrees, if a = 1 and b = 0 then 180 degrees if a = 1 and b = 1 then 270 degrees.From your image set, draw 3 samples. Sample 1 [training set] must be say 50% of the images, sample 2 [validation set] say 30% of the images and sample 3 [out of sample set] 20% of the images.Use the training images from sample 1 to update the weights. Read in images from sample 1 at random, the rotation must be random, use your image rotator to rotate the image, and compare it to the output of the neural network, calculate the sum of the mean error and perform gradient descent to update the weights. Repeat the process. After every x weight updates randomly select a few images from the validation set and calculate an error value. If the average error value of the validation images are above some threshold then stop, or if the error begins to get worse. Do not use the validation set to update the weightsAt the end use the out-of-sample set of images to verify the performance of the neural network. The process above does not include hyper-parameter optimisation, or any other optimisation techniques, but does describe the basic way of training a NN using supervised learning.Edit: I assumed that you wanted to NN to learn to rotate the image. After your updates, I assume you want rotated images to increase generalisation of your NN. In which case randomising or shuffling the input batch is preferred. i.e. send your images to Keras to rotate them, get the batch back any order is fine ABab or AaBb and then shuffle them. Pass the shuffled training and validation set to the NN and compare it to the known label for weight updates."
Using an 'operation ID' as a neural network input,"
Sorry if this is basic or covered elsewhere, I am just starting here and I wasn't able to find an answer, but I might have not been searching for the right thing. So:
I am training a neural network to predict current draw in a system. There are a number of obvious numerical inputs, like temperature, counting rate, voltage, etc. 
The most predictive thing, however, is what operation the system is doing. So like, if it's doing a 'calibration' then the current profile is much different than if it's in 'standby'. I know that I can just use a different network for each operation, but in this case I have a couple hundred different macros defined and I don't want to have 200+ neural networks retrain all the time. 
I also know that I can have a digital value as an input, but my understanding is that it has to be either 0/1. Also, the relationship to operation is not at all correlated - so operation 100 is not necessarily more current draw than 99 or less than 101. 
So, is there a way to have an operation ID or something factor in, but not have it be in the linear combination mathematically? So, basically, tell the system to do a different training based on ID or something? I'll be using python and scikit-learn.
Thanks!
","['neural-networks', 'python']","Great question.The operational ID is actually a category, or categorical input. Each category requires its own input signal into the neural network. Each signal is actually an activation which is multiplied by a weight and added to all the other weights and inputs. This means that if you use the same input signal for all your operational IDs then 101 is more important than 50. As you have mentioned these inputs are very different, and therefore must have a binary input for each operation id. If you add more IDs then you have to retrain the neural network.You also mentioned you will be using scikit-learn, which has built in functions for category data. Have a look at this link on the one hot encoder."
What does a hybrid Bayesian network contain?,"
The field of artificial intelligence is so vast. There are many methodologies for handling continuous data, and I have just read about the hybrid Bayesian network. I just want to know that what a hybrid Bayesian network contains?
","['bayesian-networks', 'probabilistic-graphical-models', 'probabilistic-machine-learning']",A bayesian network where the random variables represented can be continuous or discrete. Usually the continuous variables are handled with Normal distribution with an input of some linear combination of the variables it is dependent on.Much more detail can be found in Koller's Probabilistic Graphical Model book: http://pgm.stanford.edu/
How can I evaluate the performance of a system that generates text?,"
I am preparing to perform research comparing the performance of two different systems that probabilistically generate the next word of an input sentence.
For example, given the word 'the', a system might output 'car', or any other word. Given the input 'the round yellow', a system might output 'sun', or it might output something that doesn't make sense.
My question is, how can I quantitatively evaluate the performance of the two different systems performing this task? Of course if I tested each system manually I could qualitatively determine how often each system responded in a way that makes sense, and compare how often each system responds correctly, but I'd really like a meaningful quantitative method of evaluation that I could preferably automate.
Precision and recall don't seem like they would work here, seeing as for each given input there are many potentially acceptable outputs. Any suggestions?
","['natural-language-processing', 'software-evaluation', 'data-science']",
How do randomly initialized neural networks behave?,"
I am wondering how the output of randomly initialized MLPs and ConvNets behave with respect to their inputs.  Can anyone point to some analysis or explanation of this?
I am curious about this because in the Random Network Distillation work from OpenAI, they use the output of randomly initialized network to generate intrinsic reward for exploration.  It seems that this assumes that similar states will produce similar outputs of the random network.  Is this generally the case?  
Do small changes in input yield small changes in output, or is it more chaotic?  Do they have other interesting properties?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
DQN exploration strategy for large grid-world environment,"
My task involves a large grid-world type of environment (grid size may be $30\times30$, $50\times50$, $100\times100$, at the largest $200\times200$). Each element in this grid either contains a 0 or a 1, which are randomly initialized in each episode.  My goal is to train an agent, which starts in a random position on the grid, and navigate to every cell with the value 1, and set it to 0. (Note that in general, the grid is mostly 0s, with sparse 1s). 
I am trying to train a DQN model with 5 actions to accomplish this task:

Move up
Move right
Move down
Move left
Clear (sets current element to 0)

The ""state"" that I give the model is the current grid ($N\times N$ tensor). I provide the agent's current location through the concatenation of a flattened one-hot ($1\times(N^2)$) tensor to the output of my convolutional feature vector (before the FC layers).
However, I find that the epsilon-greedy exploration policy does not lead to sufficient exploration. Also, early in the training (when the model is essentially choosing random actions anyway), the pseudo-random action combinations end up ""canceling out"", and my agent does not move far enough away from the starting location to discover that there is a cell with value 1 in a different quadrant of the grid, for example. I am getting a converging policy on a $5\times5$ grid w/ a non-convolutional MLP model, so I think that my implementation is sound.

How I might encourage exploration that will not always ""cancel out"" to only explore a very local region to my starting location?
Is this approach a good way to accomplish this task (assuming I want to use RL)?
I would think that attempting to work with a ""continuous"" action space (model outputs 2 values: vertical and horizontal indices of grid cells that contain 1s) would be more difficult to achieve convergence. Is it wise to always try to use discrete action spaces?

","['deep-learning', 'reinforcement-learning', 'dqn']",
Change parameter in Karaboga's code of ABC algorithm,"
I'm working on a problem and need to use Karaboga's code of the ABC algorithm but I have some questions...
Does this formula for calculating a parameter have to be changed:
{/v_{ij}=x_{ij}+\phi_{ij}(x_{kj}-x_{ij}) */}

A standard one or this what Karaboga see is better for algorithm.
The second is the same question for this formula of calculating fitness:
{fFitness(ind)=1./(fObjV(ind)+1)}

Link to ABC algorithm coded using C programming language
","['neural-networks', 'algorithm', 'training']",
When will we have computer programs that can compose mathematical proofs?,"
When will it be possible to give a computer program a bunch of assumptions and ask it if a certain statement is true or false, giving a proof or a counterexample respectively?
","['logic', 'automated-theorem-proving', 'automated-reasoning', 'general-problem-solver']",
How can active learning be used in the case of complex models that require a lot of data?,"
We have a series of data and we want to label the parts of each series. As we do not have any training data, we could try to use active learning as a solution, but the problem is that our classifier is something like RNN which needs a lot of data to be trained. Hence, we have a problem in converging fast to just label proportional small parts of unlabeled data. 
Is there any article about this problem (active learning and some complex classifiers, like RNN)? 
Is there any good solution to this problem or not? (as data is a series of actions)
","['machine-learning', 'recurrent-neural-networks', 'active-learning']","As I found this case backs to the sequence labeling. Sequence labeling has some classic solution such as conditional random fields (CRFs) and hidden Markov model (HMM). Also, have some solution in Active learning (AL) which use from algorihtms such as struct SVM ($\text{SVM}^{\text{strcut}}$) like this paper.Also, some NLP solutions in active learning could help to solve these kind of problems such as this paper which is about active learning in named entity recognition (NER).Besides, the combination of active learning with Deep networks such as CNN happens. For example, this paper explains more about the idea."
What is the use of the $\epsilon$ term in this back-propagation equation?,"
I am currently looking at different documents to understand back-propagation, mainly at this document. Now, on page 3, there is the $\epsilon$ symbol involved:
$$
\Delta w_{k j}=\varepsilon \overbrace{\left(t_{k}-a_{k}\right) a_{k}\left(1-a_{k}\right)}^{\delta_{k}} a_{j}
$$
While I understand the main part of the equation, I don't understand the $\epsilon$ factor. Searching for the meaning of the $\epsilon$ in math, it means (for example) an error value to be minimized, but why should I multiply with the error (it is denoted as E anyways).
Shouldn't the $\epsilon$  be the learning rate in this equation? I think that would be what makes sense, because we want to calculate by how much we want to adjust the weight, and since we calculate the gradient, I think the only thing that's missing is the multiplication with the learning rate. The thing is, isn't the learning rate usually denoted with the $\alpha$?
","['machine-learning', 'backpropagation', 'notation', 'learning-rate']",
Why do we need both encoder and decoder in sequence to sequence prediction?,"
Why do we need both encoder and decoder in sequence to sequence prediction?
We could just have a single RNN that, given input $x$, outputs some value $y(t)$ and hidden state $h(t)$. Next, given $h(t)$ and $y(t)$, the next output $y(t+1)$ and hidden state $h(t+1)$ should be produced, and so on. The architecture shall consists of only one network instead of two separate ones. 
","['machine-learning', 'ai-design', 'sequence-modeling', 'encoder-decoder']",
Why studying machine learning is an opportunity in today's world? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I just wanted to gather some perspective on why this is a great opportunity to be able to study machine learning today? 
With all the online resources (online courses like Andrew Ng's, availability of datasets such as Kaggle, etc), learning machine learning has become possible. 
I understood that you can have high paid jobs; but you also need a lot of work dedication to be good at it, which makes your salary not so attractive! (in comparison to the number of hours you spend to keep up with this fast moving field) 
Why it is so desirable to take this opportunity and start learning machine learning today? (community, ability to start a business, etc.)
","['machine-learning', 'social', 'profession']","What sort of opportunity it is depends on how much you want to focus on it.  If you want to be a regular programmer, you might take the time to learn a high level interface for some machine learning tools, such as Tensorflow or Keras. There will be plenty of things you don't know how to do (even within those tools), but you may be able to apply predesigned model architectures to problems. The models won't be as good as one designed specifically for the problem, but it's one more tool in your toolbox, and it's possible you'd be able to get some useful results occasionally without devoting a huge amount of time to mastering the techniques.But if you want to really focus on machine learning, at the research level, you can potentially tackle problems that existing techniques haven't been able to solve. This is where most of the big projects that you've probably heard of will be happening: self-driving cars, AlphaGo, etc. What you can expect here is a lot of hard work. You will need to develop a fairly deep understanding of the mathematics involved so you can visualize (to some extent) what is happening in the potentially high dimensional spaces involved, identify potential failure modes, and identify models that won't fall into them. It involves a lot of trial and error, failed attempts and gradual improvement before you're able to develop a model that beats the stuff already out there. It's very rewarding work if you enjoy it. There are well-paying positions in the field, but that's just a bonus if you already enjoy the work, and it isn't enough of a bonus if you don't. In my opinion, going into this for just the money would be a mistake. There are almost definitely other jobs that pay just as well but don't take anywhere near the time investment to become (and stay) competent at them. But if you really want to work in this field for its own sake, and also want to make sure you don't starve while you're doing it, it's absolutely worth it."
Which neural network should I use to approximate a specific but unknown function?,"
We have convolutional neural networks and recurrent neural networks for analyzing, respectively, images and sequential data.
Now, suppose I want to approximate the unknown function $f(x,y) = \sin(2\pi x)\sin(2\pi y)$, with domain $\Omega = [0,1]\times [0,1]$, that is, $x$ and $y$ can be between $0$ and $1$ (inclusive).
How do I determine which neural network architecture is more appropriate to approximate this function? Which kind of activation functions would be better suited for this?
Note that, generally, I don't know a priori which function the neural network has to learn. I am just asking for this specific $f(x, y)$, as it could be a solution for a differential equation. And $\Omega$ is the domain, i.e., I don't care about the output of the neural network outside $\Omega$.
","['neural-networks', 'activation-functions', 'function-approximation', 'model-request', 'network-design']",
"Are Modular Neural Networks more effective than large, monolithic networks at any tasks?","
Modular/Multiple Neural networks (MNNs) revolve around training smaller, independent networks that can feed into each other or another higher network.
In principle, the hierarchical organization could allow us to make sense of more complex problem spaces and reach a higher functionality, but it seems difficult to find examples of concrete research done in the past regarding this. I've found a few sources:
https://en.wikipedia.org/wiki/Modular_neural_network
https://www.teco.edu/~albrecht/neuro/html/node32.html
https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&isAllowed=y 
A few concrete questions I have:

Has there been any recent research into the use of MNNs?
Are there any tasks where MNNs have shown better performance than large single nets?
Could MNNs be used for multimodal classification, i.e. train each net on a fundamentally different type of data, (text vs image) and feed forward to a higher level intermediary that operates on all the outputs?
From a software engineering perspective, aren't these more fault tolerant and easily isolatable on a distributed system?
Has there been any work into dynamically adapting the topologies of subnetworks using a process like Neural Architecture Search?
Generally, are MNNs practical in any way?

Apologies if these questions seem naive, I've just come into ML and more broadly CS from a biology/neuroscience background and am captivated by the potential interplay. 
I really appreciate you taking the time and lending your insight!
","['neural-networks', 'topology', 'architecture', 'neurons', 'biology']",
Can the same input for a plain neural network be used for a convolutional neural network?,"
Can the same input for a plain neural network be used for CNNs? Or does the input matrix need to be structured in a different way for CNNs compared to regular NNs? 
","['neural-networks', 'convolutional-neural-networks']","There is no restriction on how you input a data to a NN. You can input it in 1D arrays and do element-wise multiplication using 4-5 loops and imposing certain conditions(which will be slow and hence $nD$ matrix notations are used for a CNN).
Ultimately, the library you are using (TensorFlow, NumPy might convert it into its own convenient dimensions).
The main thing different of a CNN from a normal NN is:Different people have different ways of viewing how the convolutional layer work but the general consensus is that the weights of the convolutional layers of a CNN are like digital filters. It will be a $nD$ filter if input dimension is $nD$. The  output is obtained by superimposing the filter on a certain part of the input and doing element-wise multiplication of the values of filter and the values of the input upon which the filter is superimposed upon. How you implement this particular operation depends on you.So the answer to your question will be same network cannot be used, but it might be used with modifications (a normal NN is the limiting case of a CNN where $features=parameters$."
How do we actually sample an action from a policy in policy gradient methods?,"
Recently I started to look at policy gradient methods and policies are represented as functions with features for larger problems with many states. Many articles and pseudocodes of algorithms mention sampling an action from the policy, but it is unclear to me how.
Actions are something we do in the environment, like going left, right, etc... And functions take some feature values and parameters, make calculations, and 'spit out' some number. So how do we actually map that number to a certain action, and how do we know what action to take?
",['reinforcement-learning'],"In those policy gradient algorithms, we typically have:Those bolded parts in the last point there are critical for your question. We do not normally have a function that ""spits out some number"" as you described, we typically have one that ""spits out"" multiple numbers; one for every action. Often, these numbers can also be directly interpreted as probabilities, because they are forced to all lie in $[0, 1]$ and sum up to $1$ (this is often done using a softmax layer at the end of a neural network for example).Once you have such a vector of ""probabilities"", one for every action, it is relatively straightforward to sample from that. Just give every action a probability of being selected equal to its corresponding output.Note that in some cases, people describe their function approximator as producing ""logits"" rather than ""probabilities"" as outputs. These are real-valued outputs which do not necessarily sum up to one, typically produced by a linear layer at the end of a neural network rather than a softmax layer (see e.g. AlphaGo Zero). It is then still implicitly assumed that we convert them into numbers that can be interpreted as probabilities through the application of a softmax, before sampling from it."
Which problems in AI are not machine learning?,"
Which problems in AI are not machine learning? Which problems involve both AI and machine learning?
",['machine-learning'],
Neural Network Optimizers in Reinforcement Learning non-well behaved environments,"
https://stackoverflow.com/questions/36162180/gradient-descent-vs-adagrad-vs-momentum-in-tensorflow
Here, the nice gifs explain how different algorithms approach towards the root. Unfortunately, the environment in the gif is way too simple and real cases have much more complex environments. Also, in reinforcement learning, the solutions should change each moment in a difficult enough environment since things are dynamic.
My question is which optimizer is best for reinforcement learning in such dynamically changing environment? Adadelta should not move beyond local minima so do we have to use SGD or Adadelta with an exploration heuristic? Please let me know in detail your thoughts.
","['reinforcement-learning', 'optimization']","The most commonly-used optimizer in Deep Reinforcement Learning research the past few years is probably ADAM (or its AMSGrad variant, which in most frameworks like keras/tensorflow/pytorch etc. can be used by setting an amsgrad flag to True in the construction of an ADAM optimizer, and I believe is often also already set to True by default). This is a somewhat newer optimizer which isn't included in that visualization you linked to.RMSProper is also still quite popular, and there may be some arguments that it might be better suited for non-stationary learning problems (which we typically have in RL, our learning targets tend to be non-stationary due to the agent adapting its behaviour over time).All of these more ""fancy"" optimizers, typically including some form of momentum term, can in fact be especially useful for optimization in loss ""landscapes"" that are not smooth. Imagine that your loss landscape looks like a tall mountain, where we'd like our optimizer to ""glide"" all the way down. Now suppose that the overall trend of a side of the mountain is downwards, but that there are lots of little bumps along the way down, that it's not a smooth mountain. The more fancy optimizers with momentum terms are more likely to ""jump"" over those bumps than a regular SGD optimizer without momentum terms. This is because, when a little bump is encountered, the local gradient may point the wrong way, but a momentum term ""remembers"" the overall trend of gliding downwards along the mountain and still points in that same direction for a while."
What is the role of biology in AI?,"
Biology is used in AI terminology. What are the reasons? What does biology have to do with AI? For instance, why is the genetic algorithm used in AI? Does it fully belong to biology?
","['philosophy', 'terminology', 'biology']",
When should I use Genetic Algorithms as opposed to Particle Swarm Optimization?,"
For which problems are Genetic Algorithms more suitable than Particle Swarm Optimization, and vice-versa? Are there any guidelines?
","['comparison', 'reference-request', 'genetic-algorithms', 'applications', 'particle-swarm-optimization']",
Is it feasible to use minimax to solve a board game with a large number of moves?,"
I have to build a KI for a made-up game similar to chess. As I did research for a proper solution, I came upon the MinMax algorithm, but I'm not sure it will work with the given game dynamics.
The challenge is that we have far more permutations per turn than in chess because of these game rules.

Six pieces on the board, with different ranges.
On average, there are 8 possible moves for a piece per turn.
The player can choose as many pieces to move as he likes. For example none, all of them, or some number in between (whereas in chess you can only move one.)

Actual questions:

Is it feasible to implement MinMax for the described game?
Can alpha-beta-pruning and a refined evaluation function help (despite the large number of possible moves)?
If no, is there a proper alternative?

","['game-ai', 'minimax', 'alpha-beta-pruning']",
block of worlds with position aware goal stack planning,"
Can someone suggest an AI approach to moving blocks, one at a time, assuming control of an robotic arm, to get from the initial state on the left to the final state on right, preferably using goal stack planning.
actions

Pickup() — to pick up a block from table only
Putdown — to putdown a block on table only
Unstack — unstack a block from another block
Stack — stack a block on another clear block only

property functions

On(x,y)
Above(x,y)
Table(x)
Clear(x)


","['ai-design', 'path-planning']",
Can I detect unique people in a video?,"
I am having a video feed with multiple faces in it. I need to detect each face and the gender as well and assign the gender against each person. I am not sure how to uniquely identify a face as Face1 and Face2 etc. I do not need to know their names, just need to track a person in the entire video. I thought of tracking methods but people are constantly moving and changing so a box location can be occupied by another person after some frames. 
I am interested in a way where I can assign an id to a face but I am not sure how to do it. I can use Facial Recognition Based embedding on each face and track that. But that seems to be an overkill for the job. Is there any other method available or Facial Recognition/Embedding is the only method to uniquely identify people in a video?
","['deep-learning', 'computer-vision']",
How a game playing agent could identify potential objects and proximity?,"
Most implementations I'm seeing for playing games like Atari (usually similar to DeepMind's work using DQN) have 4 graphical frames of input fed into 3 convolutional layers which are then fed into a single fully connected layer. The explanation of no pooling layer is due to positioning of features/objects being very critical to most games.
My concern with this is that it may be weighing visual features based on position without regard for feature->feature proximity. By this, I mean to question if learning to avoid bullets in the bottom left of the screen is knowledge also used in the bottom right of the screen in a game like Space Invaders.
So, question 1: Is my concern with only using 3 conv layers into a fc layer legitimate regarding spatially localized learning?
Question 2: If my concern is legitimate, how might the network be modified to still treat feature position as significant, but to also take note of feature to feature proximity?
(I'm still quite the novice if that isn't extremely obvious, so if my questions aren't completely ridiculous on their own, please try to keep responses relatively high level if you would.)
","['neural-networks', 'convolutional-neural-networks', 'image-recognition', 'object-recognition']",
Why do we apply the mutation operation after generating the offspring?,"
Why do we apply the mutation operation after generating the offspring, in genetic algorithms?
","['genetic-algorithms', 'evolutionary-algorithms', 'mutation-operators']",
How to know whether the object is moving after it is being detected?,"
If my algorithm detects the type of object, how should I know if that object is moving or not? Suppose a person carrying an umbrella. How to know that the umbrella is moving?
I am working on a project where I want to know whether that particular object belongs to the person entering inside the store.
I was thinking about the bounding boxes(bb) approach where if the person's bb overlaps with the object's bb. But the problem arises when there are multiple objects with a person. 
I would appreciate your help. Thanks. 
","['python', 'object-recognition']",
"For forecasting and trading control, given limited data, what AI approaches are well matched?","
I'm working on stock price prediction and automatic or semi-automatic control of trading.  The price trends of these stocks exhibit recurring patterns that may be exploited.  My dataset is currently small, only in the thousands of points.  There are no images or very high dimensional inputs at all.  The system must select from among the usual trading actions.

Buy $n$ shares at the current bid price
Hold at the current position
Sell $n$ shares at what the current market will bear

I'm not sure if reinforcement learning is the best choice, deep learning is the best choice, something else, or some combination of AI components.
It doesn't seem to me to be a classification problem, with hard to discern features.  It seems to be an action-space problem, where the current state is a main input.  Because of the recurring patterns, the history that demonstrates the observable patterns is definitely pertinent.
I've tried some code examples, most of which employ some form of artificial nets, but I've been wondering if I even need deep learning for this, having seen the question, When is deep-learning overkill? on this site.
Since I have very limited training data, I'm not sure what AI design makes most sense to develop and test first.
","['deep-learning', 'reinforcement-learning', 'forecasting']",
Are these steps to get a final linear regression model correct?,"
I am new to machine learning. I know Logistic Regression (LR) is a supervised learning technique. Therefore, we need training data to train the model.
I tried to understand the basic steps to get the final RL model.
According to my understanding, here are the steps.

We define the LR model, that is, $y = \text{sigmoid}(W x + B)$. Set $W$ and $B$ to zero or another value.

Given the training data (the inputs are $x_1, x_2, \dots, x_m$, and the outputs are $y_1, y_2, \dots, y_m$), we find $W$ and $B$ values by minimizing a cost function using gradient descent.

Then we use the found $W$ and $B$ values. We then again apply a known sample from the training data $\hat{x}$ to get the predicting of $\hat{y}$, that is, $\hat{y} = \text{sigmoid}(W \hat{x} + B)$.

We test the final model on unknown data.


Are these steps correct?
Please, if you could also give me the basic idea behind the supervised technique, I would appreciate it.
","['machine-learning', 'training', 'supervised-learning', 'logistic-regression']",
"How is inequality 31 derived from equality 30 in lemma 2 of the ""Trust Region Policy Optimization"" paper?","
In the Trust Region Policy Optimization paper, in Lemma 2 of Appendix A (p. 11), I didn't quite understand how inequality (31) is derived from equality (30), which is:
$$\bar{A}(s) = P(a \neq \tilde{a} | s) \mathbb{E}_{(a, \tilde{a}) \sim (\pi, \tilde{\pi})|a \neq  \tilde{a}} \left[ A_{\pi}(s, \tilde{a}) - A_{\pi}(s,a) \right]$$
$$|\bar{A}(s)| \le \alpha. 2 \max_{s,a} |A_{\pi}(s,a)|$$
Would you mind let me know how the inequality is derived?
","['reinforcement-learning', 'deep-rl', 'papers', 'proofs', 'trust-region-policy-optimization']","We can start with equation (30):$$
\bar{A}(s) = P(a \neq \tilde{a}) \mathbb{E}_{(a,\tilde{a})\sim(\pi,\tilde{\pi}|a\neq\tilde{a})} [A_\pi(s, \tilde{a}) - A_\pi(s, a)]
$$Taking the absolute value of both sides, the equality remains true. We can pull the probability term out of the absolute value since it is guaranteed to be nonnegative.$$
|\bar{A}(s)| = P(a \neq \tilde{a}) |\mathbb{E}_{(a,\tilde{a})\sim(\pi,\tilde{\pi}|a\neq\tilde{a})} [A_\pi(s, \tilde{a}) - A_\pi(s, a)]|
$$By Definition 1, $P(a \neq \tilde{a}) \leq \alpha$. Substituting this definition in, we get:$$
|\bar{A}(s)| \leq \alpha \cdot |\mathbb{E}_{(a,\tilde{a})\sim(\pi,\tilde{\pi}|a\neq\tilde{a})} [A_\pi(s, \tilde{a}) - A_\pi(s, a)]|
$$By Jensen's Inequality, we can take the absolute value inside the expectation.$$
|\bar{A}(s)| \leq \alpha \cdot \mathbb{E}_{(a,\tilde{a})\sim(\pi,\tilde{\pi}|a\neq\tilde{a})} [|A_\pi(s, \tilde{a}) - A_\pi(s, a)|]
$$The expectation of a random variable is always upper bounded by the max value of that variable.$$
|\bar{A}(s)| \leq \alpha \cdot \max_{a,\tilde{a}|a\neq\tilde{a}} |A_\pi(s, \tilde{a}) - A_\pi(s, a)|
$$This part is a little strange, and I'm not sure if this is the logic that the authors followed, but it is still true. For any $a,b$, we have that $|a - b| \leq |a| + |b|$.$$
|\bar{A}(s)| \leq \alpha \cdot \max_{a,\tilde{a}|a\neq\tilde{a}} (|A_\pi(s, \tilde{a})| + |A_\pi(s, a)|)
$$For $a \neq \tilde{a}$, it must be that either $|A_\pi(s, \tilde{a})| \geq |A_\pi(s, a)|$ or vice versa. We can use this to replace the pair of advantage functions with 2 times the max of the two.$$
|\bar{A}(s)| \leq \alpha \cdot 2 \max_{a} |A_\pi(s, a)|
$$Now, if we take the max over $a$ and $s'$, our inequality still holds, since we are taking the maximum over a set that contains $s$. Making this substitution gives us (31).$$
|\bar{A}(s)| \leq \alpha \cdot 2 \max_{a,s'} |A_\pi(s', a)|
$$"
Why does the hill climbing algorithm only produce a local maximum?,"
Apparently, the hill climbing algorithm just produces a local maximum, and not necessarily a global optimum. It's stuck on a local maximum. Why does hill climbing algorithm only produce a local maximum?
","['optimization', 'search', 'hill-climbing', 'local-search']",
Does IBM Cloud Private for Data run on public clouds like AWS or Azure?,"
I just started using IBM Cloud Private for Data this week and I wasn't sure if I can use other public clouds to connect with my ICP for data account. So I spoke with an IBM representative and I wanted to share their responses...
",['structured-data'],
How to deal with episode termination in Advantage Actor-Critic algorithm?,"
Advantage Actor-Critic algorithm may use the following expression to get 1-step estimate of the advantage:
$ A(s_t,a_t) = r(s_t, a_t) + \gamma V(s_{t+1}) (1 - done_{t+1})  - V(s_t) $
where $done_{t+1}=1$ if $s_{t+1}$ is a terminal state (end of the episode) and $0$ otherwise.
Suppose our learning environment has a goal, collecting the goal gives reward $r=1$ and terminates the episode. Agent also receives $r=-0.1$ for every step, encouraging it to collect the goal faster. We're learning with $\gamma=0.99$ and we terminate the episode after $T$ timesteps if the goal wasn't collected.
For the state before collecting a goal we have the following advantage, which seems very reasonable: $A(s_t,a_t) = 1 - V(s_t)$.
For the timestep $T-1$, regardless of the state, we have: $ A(s_{T-1},a_{T-1}) = r(s_{T-1}, a_{T-1}) - V(s_{T-1}) \approx -0.1 -\frac{-0.1}{1-\gamma} = -0.1 + 10 = 9.9 $ 
(this is true under the assumption that we're not yet able to collect the goal reliably often, therefore the value function  converges to something close to $\frac{r_{avg}}{1-\gamma} \approx -10 $ ).
Usually, $T$ is not a part of the state, so the value function has no way to anticipate the sudden change in reward-to-go. So, all of a sudden, we got a (relatively) big advantage for the arbitrary action that we took at the timestep $T-1$.  Following the policy gradient rule, we will significantly increase the probability of an arbitrary action that we took at the end of the episode, even if we didn't achieve anything. This can quickly destroy the learning process.
How do people deal with this problem in practice? My ideas:

Differentiate between actual episode terminations and ones caused by the time limit, e.g. for them we will not replace next step value estimate with $0$.
Somehow add $t$ to the state such that the value function can learn to anticipate the termination of the episode.

As I noticed, the A2C implementation in OpenAI baselines does not seem to bother with any of that:

A2C implementation - calculation of discounted rewards
discount_with_dones function

",['reinforcement-learning'],"First a small note: I don't think your expression for $A(s_{T-1}, a_{T - 1})$ looks correct. If we assume that $V(s_T) = 0$ (i.e., assume that we cannot possibly reach the goal in one single step from $s_{T - 1}$), we have:\begin{align}
A(s_{T-1}, a_{T-1}) &= r(s_{T-1}, a_{T-1}) + \gamma V(s_{T}) (1 - done_{T})  - V(s_{T-1}) \\
&= r(s_{T-1}, a_{T-1}) - V(s_{T-1}).
\end{align}In this expression, we'd normally have that $r(s_{T-1}, a_{T-1}) = -0.1$, whereas you seem to have mistakenly taken $+0.1$ in your post.Those details aside; yes, there can be sudden bumps of positive advantage estimates as you described. This is not a problem though, this is exactly what we'd expect to happen given your description of the environment.You describe an environment in which the agent is likely going to be wandering around randomly (at least when it hasn't learned a good policy yet), and incurring negative rewards over and over again. This naturally leads to negative value estimates for all encountered states. Suddenly, it does something and the episode terminates; it receives a nice reward of $0$ rather than yet another negative reward (this actually ""feels"" like a bonus, a positive reward, something more than was expected). When your agent has not yet learned a good policy that can reach the better reward of $1$, this is indeed a good result, a good action, and it rightfully should get reinforced.Because this event of the episode terminating is mostly uncorrelated with the state (I say ""mostly"", because in theory it probably ends up being a slightly rarer event in states close to the goal than in states far away from the goal), it will eventually (after sufficient training time) end up occurring approximately equally often in all states. From the perspective of an agent that is oblivious to the current time step, this will be perceived as an event that can simply occur by pure chance in a nondeterministic environment.This is not necessarily a problem. It can slow down learning due to increased variance in your reward observations (which can be addressed by using low learning rates / large batch sizes), but Reinforcement Learning algorithms are almost always naturally built to handle nondeterministic environments, it can work this out, it can average out all the different outcomes observed for the same state+action pairs. This is not a problem that requires dealing with.My ideas:The first idea fundamentally changes the quantity that your algorithm is learning, it will essentially make your learning algorithm incorrect. There's always a chance that it might still appear to learn something useful (many Machine Learning/Reinforcement Learning algorithms can still appear to be okay even when there are bugs/technically incorrect parts), but it'll very likely perform worse.The second idea, while not necessary as explained in my answer above, may still be beneficial to learning speed provided it is done well. It may help because it can add to the power of your algorithm to ""explain"" its observations, and more importantly ""explain"" the variance in its observations.The main problem I see with adding $t$ to your input is that it is not naturally a binary variable. Very often you'll find that we're just using a bunch of binary inputs in (Deep) Reinforcement Learning algorithms. When all inputs are of the same magnitude like that, it tends to be easier to get the learning process to run well, tune hyperparameters like learning rate, etc. If you suddenly plug in an additional input which can take significantly larger values (like straight up adding $t$ as an input), this will be more difficult. Adding $\frac{t}{T}$ as an input may be better, since that will always still be bounded between $0$ and $1$."
Am I able to visualize the differentiation in backprop as follows?,"
I'm wondering if I can visualize the backprop process as follows (please excuse me if I have written something terrible wrong). If the loss function $L$ on a neural network represents the function has the form 
$$L = f(g(h(\dots u(v(\dots))))$$
then we can visualize the derivative of $L$ wrt the $i$th function $v$ as 
$$\frac{\partial L}{\partial v} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial h}\dots\frac{\partial u}{\partial v}.$$
Am I able to view all neural networks as having a loss function of the form of $L$ given above? That is, am I correct in saying that any neural network is just a function composition and that I can write the partial derivative wrt any parameter as written above (I know I took the partial with respect to the function $v$). 
Thanks
","['neural-networks', 'backpropagation']","Yes, a neural network plus loss function can be viewed as a function composition as you have written, and back propagation is just the chain function repeated. Your equations $L = f(g(h(\dots u(v(\dots))))$ and $\frac{\partial L}{\partial v} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial h}\dots\frac{\partial u}{\partial v}$ are useful for high level intuition.At some point you need to look at the actual forms of the functions, and that introduces a bit more complexity. For instance, the loss function for a mini-batch can be expressed in terms of the output. This is the equivalent of $L = f(g)$ for a batch or mini-batch with mean squared error loss, where I am using $J$ (for cost) in place of $L$ and the output of the neural network $\hat{y}$ in place of $g$:$$J = \frac{1}{2N}\sum_{i=1}^{N}(\hat{y}_i - y_i)^2$$The gradient of $J$ with respect to $\hat{y}$ is equivalent to your first part $\frac{\partial f}{\partial g}$:$$\nabla_{\hat{y}} J = \frac{1}{N}\sum_{i=1}^{N}(\hat{y}_i - y_i)$$Many of the functions in a neural network involve sums over terms. They can be expressed as vector and matrix operations, which can make them look simpler, but you still need to have code somewhere that works through all the elements.There is one thing that the function composition view does not show well. The gradient you want to calculate is $\nabla_{\theta} J$, where $\theta$ represents all the parameters of the neural network (weights and biases). The parameters in each layer are end points of back propagation - they are not functions of anything else, and the chain rule has to stop with them. That means you have a series of ""dead ends"" - or possibly another way of thinking about it would be that your $\frac{\partial L}{\partial v} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial h}\dots\frac{\partial u}{\partial v}$ is the ""trunk"" of the algorithm that links layers together, and every couple of steps there is a ""branch"" to calculate the gradient of the parameters that you want to change.More concretely, if you have weight parameters in each layer noted as $W^{(n)}$, and two functions for each layer (the sum over weights times inputs, and an activation function) then your example ends up looking like this progression:$$\frac{\partial L}{\partial W^{(n)}} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial h}\frac{\partial h}{\partial W^{(n)}}$$$$\frac{\partial L}{\partial W^{(n-1)}} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial h}\frac{\partial h}{\partial i}\frac{\partial i}{\partial j}  \frac{\partial j}{\partial W^{(n-1)}}$$$$\frac{\partial L}{\partial W^{(n-2)}} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial h}\frac{\partial h}{\partial i}\frac{\partial i}{\partial j}\frac{\partial j}{\partial k}\frac{\partial k}{\partial l}  \frac{\partial l}{\partial W^{(n-2)}}$$. . . this is the same idea but showing the goal of calculating $\nabla_{W} L$ which doesn't fit into a single chain of gradients. Notice the last term on each line does not appear in the next line. However, you can keep all the terms prior to that and re-use them in the next line - this matches the layer-by-layer calculations in many implementations of back propagation."
Why do we use a last-in-first-out queue in depth-first search?,"
Why do we use a last-in-first-out (LIFO) queue in the depth-first search algorithm? 
In the breadth-first search algorithm, we use a first-in-first-out (FIFO) queue, so I am confused.
","['search', 'implementation', 'breadth-first-search', 'depth-first-search']","We use the LIFO  queue, i.e. stack, for implementation of the depth-first search algorithm because depth-first search always expands the deepest node in the current frontier of the search tree.Norvig and Russell write in section 3.4.3The search proceeds immediately to the deepest level of the search tree, where the nodes have no successors. As those nodes are expanded, they are dropped from the frontier, so then the search ""backs up"" to the next deepest node that still has unexplored successors.A LIFO queue means that the most recently generated node is chosen for expansion. This must be the deepest unexpanded node because it is one deeper than its parent — which, in turn, was the deepest unexpanded node when it was selected."
Understanding the math behind using maximum likelihood for linear regression,"
I understand both terms, linear regression and maximum likelihood, but, when it comes to the math, I am totally lost. So I am reading this article The Principle of Maximum Likelihood (by Suriyadeepan Ramamoorthy). It is really well written, but, as mentioned in the previous sentence, I don't get the math. 
The joint probability distribution of $y,\theta, \sigma$ is given by (assuming $y$ is normally distributed): 

This equivalent to maximizing the log likelihood:

The maxima can be then equating through  the derivative of l(θ) to zero:

I get everything until this point, but don't understand how this function is equivalent to the previous one :

","['machine-learning', 'linear-regression', 'maximum-likelihood']","Note first that the first $=$ (equals) in $\frac{dl(\theta)}{d\theta} 
 = 0 = −\frac{1}{2\sigma^2}(0−2X^TY + X^TX \theta)$ should be interpreted as a ""is set to"", that is, we set $\frac{dl(\theta)}{d\theta} 
 = 0$. Given that (apparently) $\frac{dl(\theta)}{d\theta} = −\frac{1}{2\sigma^2}(0−2X^TY + X^TX \theta)$, $\frac{dl(\theta)}{d\theta} 
 = 0$ is equivalent to $0 = −\frac{1}{2\sigma^2}(0−2X^TY + X^TX \theta)$.Now, let's apply some basic linear algebra:\begin{align}
0 &= −\frac{1}{2\sigma^2}(0−2X^TY + X^TX \theta) \iff \\
0 &= −(0−2X^TY + X^TX \theta) \iff \\
0 &= −0 + 2X^TY - X^TX \theta) \iff \\
0 &= 2X^TY - X^TX \theta \iff \\
X^TX \theta &= 2X^TY \iff \\
(X^TX)^{-1}(X^TX) \theta &= (X^TX)^{-1}2X^TY \iff \\
\theta &= (X^TX)^{-1}2X^TY
\end{align}Now, you can ignore the $2$, because it is just a constant, and, when optimizing, this does not influence the result.Note that using $\hat{\theta}$ instead of $\theta$ is just to indicate that what we will get is an ""estimate"" of the real $\theta$, because of round off errors during the computations, etc."
How do I show that uniform-cost search is a special case of A*?,"
How do I show that uniform-cost search is a special case of A*? How do I prove this?
","['comparison', 'search', 'proofs', 'a-star', 'uniform-cost-search']","Yes, UCS is a special case of A*. UCS uses the evaluation function $f(n) = g(n)$, where $g(n)$ is the length of the path from the starting node to $n$, whereas A* uses the evaluation function $f(n) = g(n) + h(n)$, where $g(n)$ means the same thing as in UCS and $h(n)$, called the ""heuristic"" function, is an estimate of the distance from $n$ to the goal node. In the A* algorithm, $h(n)$ must be admissible. UCS is a special case of A* which corresponds to having $h(n) = 0, \forall n$. A heuristic function $h$ which has $h(n) = 0$, $\forall n$, is clearly admissible, because it always ""underestimates"" the distance to the goal, which cannot be smaller than $0$, unless you have negative edges (but I assume that all edges are non-negative). So, indeed, UCS is a special case of A*, and its heuristic function is even admissible!To see this with an example, just draw a simple graph, and apply the A* algorithm using $h(n) = 0$, for all $n$, and then apply UCS to the same graph. You will obtain the same results."
Can I do oversampling by copying the same image multiple times? Will it effect my neural network accuracy?,"
I am working on an image data-set. As you may have guessed it is imbalanced data. I have 'Class A, 19,000 images' and 'Class B, 2,876 images'.
So I did an undersampling by removing randomly from the majority class till it becomes equal to the minority class.
On doing this I am loosing lot of information from those 19000 images which I could get. 
So I do an oversampling of minority class, by simply copying the 2,876 images again and again.
Is this undersampling method correct, will it effect my accuracy? I trained an Inceptionv4 model using this oversampled data and it is not at all stable and I am getting poor accuracy.
What should be my strategy ?
","['convolutional-neural-networks', 'ai-design']",
Does an advanced Dialogue state tracking eliminate the need of intent classifier and slot filling models in dialogue systems/ chatbots?,"
I am learning to create a dialogue system. The various parts of such a system are Intent classifier, slot filling, Dialogue state tracking (DST), dialogue policy optimization and NLG.
While reading this paper on DST, I found out that a discriminative  sequence model of DST can identify goal constraints, fill slots and maintain state of the conversation. 
Does this mean that now I dont need to create an intent classifier and slot filling models separately as the tasks are already being done by this DST? Or I am misunderstanding both the things and they are separate?
","['neural-networks', 'machine-learning', 'natural-language-processing', 'recurrent-neural-networks', 'chat-bots']",
Are all ant routing algorithms the same?,"
Are all ant routing algorithms the same? If no, what is the common properties of all of them? In other words, how we can detect a routing algorithm is an ant routing algorithm?
","['algorithm', 'swarm-intelligence', 'ant-colony-optimization']",
Why isn't my DQN implementation working properly?,"
I'm trying to build a DQN to replicate the DeepMind results. I'm doing with a simple DQN for the moment, but it isn't learning properly: after +5000 episodes, it couldn't get more than 9-10 points. Each episode has a limit of 5000 steps but it couldn't reach more than 500-700. I think the problem is in the replay function, which is:
def replay(self, replay_batch_size, replay_batcher):
    j = 0
    k = 0
    replay_action = []
    replay_state = []
    replay_next_state = []
    replay_reward= []
    replay_superbatch = []

    if len(memory) < replay_batch_size:
        replay_batch = random.sample(memory, len(memory))
        replay_batch = np.asarray(replay_batch)
        replay_state_batch, replay_next_state_batch, reward_batch, replay_action_batch = replay_batcher(replay_batch)
    else:
        replay_batch = random.sample(memory, replay_batch_size)
        replay_batch = np.asarray(replay_batch)
        replay_state_batch, replay_next_state_batch, reward_batch, replay_action_batch = replay_batcher(replay_batch)
        
    for j in range ((len(replay_batch)-len(replay_batch)%4)):
        
        if k <= 4:
            k = k + 1              
            replay_state.append(replay_state_batch[j])
            replay_next_state.append(replay_next_state_batch[j])
            replay_reward.append(reward_batch[j])
            replay_action.append(replay_action_batch[j])
            
        if k >=4:                
            k = 0
            replay_state = np.asarray(replay_state)
            replay_state.shape = shape
            replay_next_state = np.asarray(replay_next_state)
            replay_next_state.shape = shape
            replay_superbatch.append((replay_state, replay_next_state,replay_reward,replay_action))

            replay_state = []
            replay_next_state = []
            replay_reward = []
            replay_action = []
                                       
    states, target_future, targets_future, fit_batch = [], [], [], []
    
    for state_replay, next_state_replay, reward_replay, action_replay in replay_superbatch:

        target = reward_replay
        if not done:
            target = (reward_replay + self.gamma * np.amax(self.model.predict(next_state_replay)[0]))

        target_future = self.model.predict(state_replay)

        target_future[0][action_replay] = target
        states.append(state_replay[0])
        targets_future.append(target_future[0])
        fit_batch.append((states, targets_future))

    history = self.model.fit(np.asarray(states), np.array(targets_future), epochs=1, verbose=0)

    loss = history.history['loss'][0]

    if self.exploration_rate > self.exploration_rate_min:

        self.exploration_rate -= (self.exploration_rate_decay/1000000)
    return loss

What I'm doing is to get 4 experiences (states), concatenate and introduce them in the CNN in shape (1, 210, 160, 4). Am I doing something wrong? If I implement the DDQN (Double Deep Q Net), should I obtain similar results as in the DeepMind Breakout video? Also, I'm using the Breakout-v0 enviroment from OpenAI gym.
Edit
Am I doing this properly? I implemented an identical CNN; then I update the target each 100 steps and copy the weights from model CNN  to target_model CNN. Should it improve the learning? Anyway I'm getting low loss.
for state_replay, next_state_replay, reward_replay, action_replay in replay_superbatch:

            target = reward_replay
            if not done:

                target = (reward_replay + self.gamma * np.amax(self.model.predict(next_state_replay)[0]))
            if steps % 100 == 0:

                target_future = self.target_model.predict(state_replay)

                target_future[0][action_replay] = target
                states.append(state_replay[0])
                targets_future.append(target_future[0])
                fit_batch.append((states, targets_future))
                agent.update_net()

        history = self.model.fit(np.asarray(states), np.array(targets_future), epochs=1, verbose=0)

        loss = history.history['loss'][0]

Edit 2
So as far I understand, this code should work am I right?
if not done:

            target = (reward_replay + self.gamma * np.amax(self.target_model.predict(next_state_replay)[0]))
            target.shape = (1,4)
            
            target[0][action_replay] = target
            target_future = target
            states.append(state_replay[0])
            targets_future.append(target_future[0])
            fit_batch.append((states, targets_future))

        if step_counter % 1000 == 0:

            target_future = self.target_model.predict(state_replay)

            target_future[0][action_replay] = target
            states.append(state_replay[0])
            targets_future.append(target_future[0])
            fit_batch.append((states, targets_future))
            agent.update_net()

    history = self.model.fit(np.asarray(states), np.array(targets_future), epochs=1, verbose=0)

","['reinforcement-learning', 'deep-rl', 'q-learning', 'dqn', 'implementation']","It looks like on each step, you're calling both self.model.predict and self.model.fit. If you do this, you're going to run into stability problems, since your learning target is moving as you train.The way the DQN paper gets around this problem is by using 2 Q-networks, $Q$ and $\hat{Q}$, where $\hat{Q}$ is called the target network. The target network's parameters are frozen, and its outputs are used to compute the learning targets for $Q$ (targets_future in your code). Every $C$ training steps (where $C$ is a hyperparameter), the target network $\hat{Q}$ is updated with the weights of $Q$. See Algorithm 1 on Page 7 of the DQN paper for the details of this swap."
Neural network architecture for comparison,"
When someone wants to compare 2 inputs, the most widespread idea is to use a Siamese architecture. Siamese architecture is a very high level idea, and can be customized based on the problem we are required to solve.
Is there any other architecture type to compare 2 inputs ? 

Background
I want to use a neural network for comparing 2 documents (semantic textual similarity). Siamese network is one approach, I was wondering if there is more.
","['neural-networks', 'natural-language-processing', 'models', 'architecture']","A RBM (restricted Boltzmann machine) can be trained to extract document features. The same resulting machine can extract features of two or more documents.  Because documents can be just as easily processed in series using the same machine parameters and CPU (saving the feature results) as documents could be processed in parallel using separate CPUs, the Siamese idea is less of a characteristic of system architecture and more of a characteristic of the process topology. The architectural decision of how to run the process topology on available hardware should usually remain somewhat decoupled.The comparison operation could be a MLP (multi-layer perceptron) trained to produce the comparison results based on a requisite number example document comparisons.  In such a case, the MLP should be trained with the previously trained RBMs as a front end.  You will need some human labor to produce or extract from available data sources the example document comparison results.  The example results will be need, along with a reference to the pair of RBMs feature extraction results corresponding to the comparison, to train the MLP.Even further upstream from the RBMs, it is often useful to producing reliable and accurate document processing to pre-process the documents.  Images and text can be delineated so that features can be extracted from the images in one way and from the text in another and then from the sequence of text and images in a third way.  In such process topologies, CNNs (convolution network) typically pre-process the images and LSTM networks typically pre-process the text.  They may then directly feed the RBM or indirectly through another process component."
Is AlphaZero an example of an AGI?,"
From DeepMind's research paper on arxiv.org:

In this paper, we apply a similar but fully generic algorithm, which
  we call AlphaZero, to the games of chess and shogi as well as Go,
  without any additional domain knowledge except the rules of the game,
  demonstrating that a general-purpose reinforcement learning algorithm
  can achieve, tabula rasa, superhuman performance across many
  challenging domains.

Does this mean AlphaZero is an example of AGI (Artificial General Intelligence)?
","['game-ai', 'definitions', 'agi', 'alphazero', 'alphago']","Good question!AlphaGo, though strong at the game of Go, is narrowly strong (""strong-narrow AI""), defined as strength in a single problem or type of problem (such as Go and other non-chance, perfect information games.) AGI is often associated with superintelligence, defined as intelligence that surpasses human levels.AGI does not necessarily imply super-intelligence, in the sense that we'd consider an android that can perform all human activities with the same capability as humans as an Artificial General Intelligence.  But technically, AlphaGo is a narrow superintelligence in that it exceeds all human performance in a single problem."
How to preprocess a modified dataset so that a fitted CNN makes correct predictions on an un-modified version of the dataset?,"
for a school project I have been given a dataset containing images of plants and weeds. The goal is to detect when there is a weed in the pictures. The training and validation sets have already been created by our teachers, however they probably didn't have enough images for both so they ""photoshopped"" some weeds in some of the training pictures.
Here are examples of images with the weed label in the training set:

In some cases, the ""photoshopped"" weed is hard to detect, and no shape resembling a weed is clearly visible like in this picture (weed at the very bottom, near the middle):

And here is an example of an image with the weed label in the validation set:

How would I go about preprocessing the training set so that a CNN trained on it would perform well on the validation set? I was thinking of applying a low-pass filter to the rough edges of the photoshopped images so that the network doesn't act as an edge detector, but it doesn't seem very robust. Should I manually select the best images from the training set? Thank you!
","['convolutional-neural-networks', 'image-recognition', 'datasets']",
How is the cost of the path to each node computed in A*?,"
How is the cost of the path to each node $n$ computed in the A* algorithm? Do we need to add the cost of the path to the parent node $p$ to the cost of the path of the child node $n$?
","['search', 'a-star']",
Is there an efficiency swarm Intelligence algorithm for off-chain channels routing in blockchain?,"
One of the solutions to scale blockchain is to use off-chain channels. You can find its definition here: https://en.bitcoin.it/wiki/Off-Chain_Transactions.
However, one of the problems of off-chain channels is finding a suitable decentralized routing mechanism.
Since in Bitcoin there is no routing table and transactions are only broadcast and also, in general, we need to avoid centralized approaches for routing, is it practical to use swarm intelligence algorithms, such as ant colony optimization ones, for off-chain channels?
I refer you to a proposed ant routing algorithm in the paper Ant routing algorithm for the Lightning Network for an instance of employing ACO algorithms for routing in _lightning network. However, the paper has not been evaluated to demonstrate its performance. 
","['applications', 'swarm-intelligence', 'ant-colony-optimization', 'blockchain']",
How Dempster-Shafer theory work in AI?,"
How does Dempster-Shafer theory work in representing ignorance in the AI field?
",['dempster-shafer-theory'],
Scrabble game using machine learning,"
I've been thinking if machine learning can be used to play the game Scrabble. My knowledge is limited in the ML field, thus I've seeking some pointers :) 
I want to know how could I possibly build a model that picks a move from all the given valid moves of the current game state, and then plays the move and wait for the delayed reward. The actions here aren't static actions, they are basically selecting move to maximize the final score.
Is there any way to encode the valid moves and then use a model to pick those moves?
I've also considered the genetic approach, but I think if I can represent my move with a set of features (score, consonantVowels ration, rack leave score, #blank tiles after the move, ...etc), training a neural network like this could take a long time.
Another training related question, is it feasible to run the training on a GPU given that I will be waiting for a response (the new game state) from the opponent (e.g. Quackle) after every action?
Thank you :)
","['machine-learning', 'reinforcement-learning', 'genetic-algorithms']",
How can the sum of squared errors have negative gradient if it's defined as the squared of the error?,"
The formula for the sum of squared errors (SSE) is:
$$
\frac{1}{2} \sum_{i=1}^n (t^i - o^i)^2
$$
I have a few related questions.

If $t^i - o^i$ is negative, doesn't the power of 2 eliminate any negative result?

How can then exist any negative gradient at the output?

What if the output is too high, and so the gradient should be negative, meaning that the weights (on average) have to be decreased? How can it be differentiated between 'output too high' and 'output too low'?


","['machine-learning', 'objective-functions', 'gradient', 'sse']","If $t^i - o^i$ is negative, doesn't the power of 2 eliminate any negative result?In the loss function, yes that is correct, and is what you want - a measurement that gets higher due to any difference between the predicted and correct results. Minimising the value of that measurement is a goal for the optimiser.How can then exist any negative gradient at the output?The gradient of the cost function is different to the cost function. Technically it is the derivative of the cost function with respect to the parameters you can change. Taking the derivative changes the powers in the equation.If you plot the graph from a single example, you can clearly see the gradient is negative when the prediction is too low, and positive when the prediction is too high:Note this is the opposite relationship to the one you suggest in the question. That is why it is called gradient descent when we apply corrections. The steps to correct the parameters are in the opposite direction to the gradient.In terms of algebra (and moving the example index into a subscript so that is doesn't get in the way visually), assuming $o_i$ is your output value, and $t_i$ is the target value, then you want the gradient of your loss function $L$ with respect to $o_i$ or $\frac{\partial L}{\partial o_i}$:$$\frac{\partial L}{\partial o_i} =  \frac{\partial}{\partial o_i} \frac{1}{2}(t_i - o_i)^2 = \frac{\partial}{\partial o_i} \frac{1}{2}(t_i^2 - 2t_i o_i + o_i^2) = \frac{1}{2}(-2t_i + 2o_i) = o_i - t_i$$This simple gradient value is negative if your output $o_i$ is lower than the target $t_i$, and positive if $o_i$is greater than $t_i$.As an aside, if you wondered why the multiplier of $\frac{1}{2}$ was included in the definition of SSE, it is intended by design to end up with this simple result. Without it, everything works, just you would end up with the gradient being $2(o_i - t_i)$"
Are 1D CNNs really the appropriate model for human activity recognition?,"
This article on human activity recognition states that 1D convolutional neural networks work the best on the classification of human activities using data from the accelerometer. But I think that human activities, like swinging the arm, are sequential actions and they require LSTMs.
So, which one should be more effective for this task: CNNs or LSTMs? In other words, is spatial learning required or sequence learning?
","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'human-activity-recognition']","Human activity recognition, for example as done with the Sports-1M Dataset, is a classification task. Given a video, say which activity / action was executed.So it is a special case about video classification.human activities like swinging the arm are sequential actions and they require LSTMsThis is wrong. As a simple example, think of handwriting recognition as done on write-math.com: I receive a sequence of points (x, y) -- the pen-tip --  and classify which symbol is written. So it is a sequence, but still there a plenty of ways to use a simple multi-layer perceptron (MLP) for it. Two big groups of approaches:You can do similar things for movies:You might be interested in Beyond Short Snippets: Deep Networks for Video ClassificationModel A is more effective than model B, if its optimizarion criterion is better.Model A is more efficient than model B, if it uses less resources (e.g. memory) to achieve the same optimization value (e.g. same accuracy / MSE)"
Can BERT be used for sentence generating tasks?,"
I am a new learner in NLP. I am interested in the sentence generating task. As far as I am concerned, one state-of-the-art method is the CharRNN, which uses RNN to generate a sequence of words.
However, BERT has come out several weeks ago and is very powerful. Therefore, I am wondering whether this task can also be done with the help of BERT? I am a new learner in this field, and thank you for any advice!
","['neural-networks', 'deep-learning', 'natural-language-processing', 'bert', 'text-generation']","For newbies, NO.Sentence generation requires sampling from a language model, which gives the probability distribution of the next word given previous contexts. But BERT can't do this due to its bidirectional nature.For advanced researchers, YES.You can start with a sentence of all [MASK] tokens, and generate words one by one in arbitrary order (instead of the common left-to-right chain decomposition). Though the text generation quality is hard to control.Here's the technical report BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model, its errata and the source code.In summary:"
What are some limitations of using Collaborative Deep learning for Recommender systems?,"
Recently I worked on a paper by Hao Wang, Collaborative Deep learning for Recommender Systems; which uses a two way tightly coupled method, Collaborative filtering for Item correlation and Stacked Denoising Autoencoders for the Optimization of the problem.
I want to know the limitations of using stacked Autoencoders and Hierarchical Bayesian methods to Recommender systems.
","['deep-learning', 'autoencoders', 'recommender-system']",
Can intelligent agents have personalities and emotions?,"
Can intelligent agents (and chatbots) have personalities and emotions, given a properly defined ontology?
","['chat-bots', 'intelligent-agent', 'human-like', 'emotional-intelligence', 'ontology']",
What does it essentially mean if the neural network has convex error surface?,"
Suppose if I am building a Linear Regression model with one fully connected layer and a sigmoid with minimizing mean squared error as objective. Why would the error surface be convex?
Does finding the optimal parameters for this network mean we cannot do better than this? Under what assumptions, this solution would be optimal? If we relax the linearity assumption and add some non-linearity to the network can we do better than this? Why so?
","['neural-networks', 'optimization']",
How to improve the efficiency of the backtracking search in CSPs?,"
Backtracking search is the basic uninformed search algorithm for constraint satisfaction problems (CSPs). How could we improve the efficiency of the backtracking search in CSPs?
","['search', 'constraint-satisfaction-problems', 'efficiency', 'depth-first-search', 'uninformed-search']",
What is a successor function (in CSPs)?,"
In Constraint Satisfaction Problems (CSPs), a state is any data structure that supports

a successor function,
a heuristic function, and
a goal test.

In this context, what is a successor function?
","['terminology', 'definitions', 'search', 'constraint-satisfaction-problems']","A successor function is a function that generates a next state from the current state, plus the choices that affect state changes. In e.g. the 8 queens problem, a state might be the location of 5 queens so far, the choice might be where to put the next queen, and the successor function would return the resulting state with 6 queens on the board.Typically a solver will store/cache a current state, make a choice, and use the successor function to discover what the next state is. Then it may call the heuristic function on the next state and make a decision whether to continue that search deeper into the search tree (or recursion) or try another choice at the current state."
What happens before the first 8 moves in Alpha Zero?,"
The Alpha zero paper says (in the caption of Table S1) that

The first set of features are repeated for each position in a $T = 8$-step history.

So, what happens before the first 8 moves? Do they just repeat the starting position?
","['papers', 'alphazero']","On page 13, right under Table S1 in the linked paper, this is explained (emphasis in bold at the end mine):Each set of planes represents the board position at a time-step $t - T + 1, \dots, t$, and is set to zero for time-steps less than $1$.I suspect the solution they write there would indeed work better than just repeating the starting position up to 8 times. Intuitively, you'll want the Neural Network to learn to primarily focus on the current game state. If the starting position is repeated a bunch of times in those planes, the Neural Network cannot distinguish between any of them in the learning process in the first few steps, and may start relying on them all equally. Only in later time steps will it ""figure out"" that they're sometimes not equal, and that the last one is probably the most informative one. If the ""useless planes"" during the first few steps are all-zero, they can be much more easily ignored in the start of the learning process.Note that I suspect the difference really won't matter a whole lot at all, I suspect there'd just be a tiny difference in learning speed based on the intuition described above."
How to generalize finite MDP to general MDP?,"
Suppose, for simplicity sake, to be in a discrete time domain with the action set being the same for all states $S \in \mathcal{S}$. Thus, in a finite Markov Decision Process, the sets $\mathcal{A}$, $\mathcal{S}$, and $\mathcal{R}$ have a finite number of elements. We could then say the following
$$p(s',r | s,a) = P\{S_t=s',R_t=r | S_{t-1}=s,A_t=a\} ~~~ \forall s',s \in \mathcal{S}, r \in \mathcal{R} \subset \mathbb{R}, a \in \mathcal{A}$$
where the function $p$ defines the dynamics of the finite MDP and $P$ defines the probability.

How could I extend this to a general MDP? That is, an MDP where the sets $\mathcal{A}$, $\mathcal{S}$, and $\mathcal{R}$ haven't a finite number of elements? To be more precise, in my case $\mathcal{A} \subset \mathbb{R}^n$, $\mathcal{S} \subset \mathbb{R}^m$, and $\mathcal{R} \subset \mathbb{R}$. My thought is that the equation above is still true, however, the probability is zero for each tuple $s',r,s,a$. 
Is it sufficient to say that for finite MDP we have
$$\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1 ~~~ \forall s\in\mathcal{S},a\in\mathcal{A}$$
while in non-finite MDP (supposing that the sets $\mathcal{s}$ and $\mathcal{A}$ are continuous) we have
$$\int_{s'\in\mathcal{S}}\int_{r\in\mathcal{R}}p(s',r|s,a)=1 ~~~ \forall s\in\mathcal{S},a\in\mathcal{A}$$
or is it more complex than this?
","['markov-decision-process', 'continuous-action-spaces', 'transition-model', 'finite-markov-decision-process', 'continuous-state-spaces']",
Why is it ok to calculate the reward based on a hidden state?,"
I'm looking at this source code, where the reward is calculated with
reward = cmp(score(self.player), score(self.dealer))

Why is it ok to calculate the reward based on a hidden state?
A player only sees the dealer's first card.
self.dealer[0]

","['reinforcement-learning', 'rewards']","The code you reference is not part of the learning agent. It is part of:If, as in this case, the environment is provided entirely by software simulation, it is absolutely necessary for it to include a full working model of all state transitions and rewards. That is independent of whether any hidden state makes the problem harder. Why is it ok to calculate the reward based on a hidden state?In the case of Blackjack, this can be treated not as a hidden state that would affect the outcome if only known, but as randomness in the environment over which the agent has no control. Critically, the dealer has no options to behave differently depending on the unknown card, and the dealer's eventual score is entirely unaffected by the player's earlier choices.It is a subtle difference. If you applied the same environment rules to Poker, where an opponent could behave differently depending on this hidden knowledge, then a simple MDP model is not enough theory to result in an optimal solution. In that case, you would need to look into Partially Observable MDPs (POMDPs). Note this would not affect reward calculation in the environment, just the choices of which agent type to use. If you are just learning RL, you probably don't know of any algorithms that could solve this yet.In practice, a lot of problems are somewhere between a classic MDP and a POMDP - they contain elements which, if the agent could know them, may allow it to achieve a higher expected reward. In many cases though, these elements can either be treated as random (as here in Blackjack) and thus the system is still theoretically an MDP, or they have a very small effect on the optimal policy, so can be ignored for practical purposes (e.g. think of all the physical details in a real cart pole balancing system - friction, temperature, flexing motions, etc)."
How to design a fitness function for the 8-queens problem?,"
In evolutionary computation and, in particular, in the context of genetic algorithms, there is the concept of a fitness function. The better a state, the greater the value of the fitness function for that state.
What would be a good fitness function for the 8-queens problem?
","['genetic-algorithms', 'fitness-functions', 'evolutionary-computation', 'fitness-design', '8-queens-problem']","Here you can find an example of how to apply genetic algorithms to solve the 8-queens problem.The proposed fitness function is based on the chessboard arrangement, and in particular, it is inversely proportional to the number of clashes amongst attacking positions of queens; thus, a high fitness value implies a low number of clashes."
How to create a good fitness function?,"
In genetic algorithms, a function called ""fitness"" (or ""evaluation"") function is used to determine the ""fitness"" of the chromosomes. Creating a good fitness function is one of the challenging tasks in genetic algorithms. How would you create a good fitness function?
","['genetic-algorithms', 'evolutionary-algorithms', 'fitness-functions', 'fitness-design']",
How is AlphaZero different from Stockfish or Rybka?,"
I don't know much about AI or chess engines, but what is the fundamental difference between AlphaZero and Stockfish or Rybka?
",['chess'],
Is an algorithm that is no longer actively learning an AI?,"
This question assumes a definition of AI based on machine learning, and was inspired by this fun Technology Review post:

SOURCE: Is this AI? We drew you a flowchart to work it out (Karen Hao, MIT Technology Review)
As the definition of artificial intelligence has been a continual subject of discussion on this stack, I wanted to bring it to the community for perspectives.
The formal question here is:

Is an algorithm that is no longer actively learning an AI?

Specifically, can applied algorithms that are not actively learning be said to reason?  
","['machine-learning', 'terminology', 'philosophy', 'definitions']",
"Training by one batch of examples, what does it mean","
Say I have a batch of examples, each examples represent a state:
[0.1, 0.2, 0.5] #1st example
[0.4, 0.0, 0.3] #2nd example 
..........
[0.1, 0.1, 0.1] #16th example

I feed through the NN, and then the NN predict the following class:
[move up]   #1st example
[move down] #2nd example
........
[move left] #16th example

And then I take the square loss (which calculated to be 0.1 after taking average over 16 examples), and do backward propagation. 
So, can I assume that each of these examples will assign (or contribute) to a 0.1 loss?
","['training', 'backpropagation', 'objective-functions']",
How can a genetic algorithm adapt and get better in a changing environment?,"
I've just started studying genetic algorithms and I'm not able to understand why a genetic algorithm can improve if, at each learning, the 'world' that the population encounters change. For example, in this demo (http://math.hws.edu/eck/js/genetic-algorithm/GA.html), it's pretty clear to me that the eating statistics will improve every year if bunches of grass grow exactly in the same place, but instead they always grow in different positions and I can't figure out how it  can be useful to evaluate (through the fitness function) the obtained eating stats given that the next environment will be different.
","['genetic-algorithms', 'evolutionary-algorithms', 'fitness-functions']","There are a couple of ways of dealing with this. Very often, the approach is just to design your representation and operators in a way to account for the fact that the world changes. The idea is to give the algorithm something that can be used to learn general behaviors or solutions rather than specific ones.Take an example of learning to steer a race car around a track. You want to represent the state of the world and have the GA learn to select an appropriate action. You might choose to represent the state of the world as a vector of $[x, y, v, a]$, where $(x, y)$ is your location on the track, $v$ is your current velocity vector, and $a$ is an acceleration to apply. The fitness function could return how ""good"" it was to apply that acceleration. If you do this, your algorithm can probably learn to navigate this track, but a different track will be hopeless, as the locations aren't corresponding to the same turn locations on the new track.However, what if you encode the world as $[s, v, a]$, where instead of an $(x, y)$ pair representing your current position, you have $s$ as a vector of sensor readings? Is there a wall coming up in front of you or is the track starting to bank? Now, your algorithm can learn to be more general. It doesn't need to be the exact track it learned on, because what it's learning is not to brake at a specific point, but to brake when it detects a wall coming up.I didn't do a lot of digging into the example you linked, but if you run it for a few years, you can see evidence of this. You see agents that appear to have learned to move in lines horizontally or vertically until they encounter a green square, and then they'll stick around and eat in the patches around that square. That behavior is general, because every environment it encounters has lots of blank space with clusters of green. It's not learning ""go to square $(20, 30)$"". It's learning to move in a pattern until it finds green and then move around that location.You can do this in a lot of cases where the specific environment can change, but the objective is the same. There are problems where the actual fitness function changes over time, however. For those problems, there are specific techniques to deal with dynamic fitness functions. Generally, this involves doing something to maintain diversity so that your whole population isn't getting stuck on whatever the current ""best"" looks like. That's a bit more advanced topic though, and I think your question was really more about the former type of problems."
What kind of distributions can be used to model discrete latent variables?,"
If we take the vanilla variational auto-encoder (VAE), we $p(z)$ is a Gaussian distribution with zero mean and unit variance and we approximate $p(z|x) \approx q(z|x)$ to be a Gaussian distribution as well, for each latent variable $z$.
But what if $z$ is a discrete variable? What kind of distributions can be used to model discrete latent variables? For example, what kind of distribution can be used to model $p(z)$ and $p(z|x) \approx q(z|x)$?
","['machine-learning', 'generative-model', 'random-variable', 'probability-distribution', 'latent-variable']",
DQN Breakout adding an extra negative reward to help training?,"
I'm trying to train a DQN, so I'm using OpenAI gym and Breakout (Breakout-v0).
I have altered the reward supplied by the environment: If the episode is not completed fully, the agent gets a -10 reward. Could be this counterproductive for learning?
","['ai-design', 'training', 'game-ai', 'dqn', 'open-ai']","In general, the approach of adding or altering the reward structure of a RL environment, when you are still trying to create an agent that solves the original problem, is called Reward Shaping.Reward Shaping can be tricky to get right. In your case I think it may be counter-productive. The main issue is that the agent does not know the current time step as part of the state. So from its perspective, a negative reward for ""timing out"" actually looks like a random chance of getting that same reward in the kind of states that happen later in the game. The effect may be small enough chance that it doesn't really matter (after all it still gets positive rewards for hitting bricks). However, it may appear as high variance on certain game positions, making the agent learn to avoid some of them for no good reason.If the aim here is to try and get the agent to speed up and finish an episode, a simpler trick might be to reduce the discount factor (e.g. from $0.999$ to $0.99$). This will cause the agent to focus on getting more short-term rewards, at the expense of long-term planning. In some environments this could be a problem of a different kind, but when the rewards are not sparse and there are not any special high-reward states that need extended setup, it should be OK.If the aim is to punish losing a ""game life"" more severely, then this is less likely to cause a problem (because the state will clearly show what to expect), although it may change what the optimal behaviour is, or how it is approached. In general the impact would be reduced risk-taking, and even though Breakout is a deterministic game, the agent is presented with a stochastic environment because each action lasts a random number of frames (2-4). With a strong penalty for losing a life, I think that the agent will be less likely to try to get high bounce angles by hitting with extreme edge of the bat. Note that you don't need to punish the agent for losing a life, it learns that because the episode ends and it cannot score any more points. Value functions are all about predicting future rewards, so the value of any life-losing state is always $0$ anyway.If you have more time, then there are also several extensions to basic DQN that require less sampled experience to learn an environment, or improve the resulting policies. A good source for those ideas is the paper Rainbow: Combining Improvements in Deep Reinforcement Learning."
"Which of these two numerical methods for z-score normalisation is preferable, in multivariate linear regression?","
In the exercise Exercise 3: Multivariate Linear Regression, by Andrew Ng, the author suggests to ""scale both types of inputs by their standard deviations and set their means to zero"".
$$x_{n e w}=\frac{x-\mu}{\sigma}$$
Method 1
The author provides the following Matlab (and Octave) code to scale the inputs.
x = [ones(m, 1), x];
sigma = std(x);
mu = mean(x);
x(:,2) = (x(:,2) - mu(2))./ sigma(2);
x(:,3) = (x(:,3) - mu(3))./ sigma(3);

Method 2
But why not simply scale the inputs between zero and one, or, divide by the maximum?
x_range=max(x)
x(:,2) = (x(:,2)/x_range(2));
x(:,3) = (x(:,3)/x_range(3));

I have done the exercise with method 2 and these are the results.

Question
Is there a computational advantage with the first method over the second method?
","['implementation', 'data-preprocessing']",
Why are all the actions converging to the same index?,"
I am using PPO with an LSTM agent. My agent is performing 10 actions for each episode, one action is corresponding to one LSTM timestep and the action space is discrete. I have only one reward per episode which I can compute after the last action of the episode.
For each timestep (~ action) my agent has 20 choices. The following plot shows the reward (y-axis) versus the current episode (x-axis).
The plot shows a decreasing reward because I want to mimize this reward so I use: minus of the true reward.
At the beginning of the process, the agent seems to learn very well and the reward is decreasing but then it's converging to a value which is not the best. When I look at the results of my experiment it appears that the index of all actions are same (for example the agent is always choosing the second value of my discrete action space).
Does anyone have an idea about what is happening here ?

","['reinforcement-learning', 'recurrent-neural-networks']",
How does a robot protect its own existence [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



What are the many ways that artificial intelligence robots protect their existence?
Isaac Asimov's ""Three Laws of Robotics""
A robot may not injure a human being or, through inaction, allow a human being to come to harm.
A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
","['mythology-of-ai', 'asimovs-laws', 'survival']","Right now, the scenarios where automata protect their own existence is limited.  In the case of autonomous vehicles, primary goals would certainly be collision avoidance, and other hazards (deep water, cliff faces, etc.)It's possible that arial drones could have certain automated defense mechanisms specific to airborne threats such a missiles.  Bots might be said to protect their existence by copying themselves onto new systems.   "
What are the available selection methods in genetic algorithms?,"
In a genetic algorithm, there are different steps. One of those steps is the selection of chromosomes for reproduction. What are the available selection strategies in genetic algorithms?
","['genetic-algorithms', 'evolutionary-algorithms', 'selection-operators']","There are many selection operators (or methods) for evolutionary algorithms. These selection methods differ in the way they evaluate the individuals given their fitness or how they compute their fitness in the first place. Some selection operators select individuals deterministically, while others select them stochastically, which can prevent premature convergence, given that, for instance, in fitness proportionate selection (aka roulette wheel selection), a well-known selection operator, all individuals can be selected, although the fitter individuals have a higher probability of being selected. Each selection operator has different advantages and disadvantages compared to other selection operators.All these details are covered from chapter 22 (p. 166 and 203 of the linked pdf) onwards of the well-written book Evolutionary Computation 1 Basic Algorithms and Operators by Thomas Bäck et al. These concepts are also described in Computational Intelligence: An Introduction (section 8.5, p. 135), in case you want to have an alternative reading.For completeness, I will enumerate and briefly describe some of the well-known selection operators.This selection method is also known as roulette wheel selection (RWS). In FPS, the probability of an individual being selected is proportional to its fitness. More precisely, let $f_i$ be the fitness of individual $i$, then the probability of that individual being selected is$$p_{i}={\frac {f_{i}}{\Sigma _{j=1}^{N}f_{j}}}$$FPS does not handle minimization problems directly and it requires the fitnesses to be non-negative (given that probabilities are in the range $[0, 1]$), but the minimization problem can be transformed into a maximization one. The solutions are thus selected probabilistically in this case. There are other problems/issues with this selection method, which are described in the cited book (chapter 23, p. 172).In TS, at each generation, a set/group $Q = \{ a_1, \dots, a_q \}$ of $|Q| = q$ individuals is sampled (with or without replacement) at random from the current population of all individuals $P$. Successively, this group $Q$ takes part in a so-called tournament, i.e. a loop of $\lambda$ iterations, where, for each iteration $j$, the individual from the group $Q$ with the highest fitness $a_1'$ is selected (deterministically or stochastically) and inserted in a new set $S$, which will contain the individuals that you will mutate and recombine. After this first iteration $j$, $S$ contains only one individual, $S = \{a_1' \}$. In the second iteration, we perform the same thing, and obtain $S = \{a_1', a_2' \}$, and so on. So, after $\lambda$ iterations, $S$ contains $\lambda$ individuals, i.e. $|S| = \lambda$. Note that $a_1'$ is not necessarily equal to $a_1 \in Q$. The individuals in $S$ are the ones that undergo mutation and cross-over in this generation. In the next generation, you repeat this tournament process for selecting the best $q$ individuals. Typically, you need to have at least $q = 2$ (i.e. binary tournament selection), so that you have the opportunity to choose 2 different individuals for the cross-over. If $q=1$, individuals are picked randomly from the population (given that you do not perform any tournament at all).In RS, only the rank ordering of the individuals within the current population determines the probability of being selected. So, in RS, an individual has a probability of being selected not proportional to its fitness (as in FPS), but it is based on its rank (order) with respect to the other individuals. So, if the individual $a$ has is ranked $1$st and individual $b$ is ranked $2$nd, even if $a$ had a fitness a lot higher than individual $b$, then its probability of being selected would not be a lot higher than the probability of selecting $b$, while, in FPS, this would be the case. There are different ways of computing the rank, such as linear or non-linear. The details of how the rank can be computed are described in chapter 25 of the cited book.There's also the related concept of elitism, where the best individuals of the current population are carried over to the next population/generation, and hall of fame, which is a data structure that stores the best individual for each generation."
"What is the difference between ""mutation"" and ""crossover""?","
In the context of evolutionary computation, in particular genetic algorithms, there are two stochastic operations ""mutation"" and ""crossover"". What are the differences between them?
","['comparison', 'evolutionary-algorithms', 'crossover-operators', 'mutation-operators', 'genetic-operators']","The mutation is an operation that is applied to a single individual in the population. It can e.g. introduce some noise in the chromosome. For example, if the chromosomes are binary, a mutation may simply be the flip of a bit (or gene).The crossover is an operation which takes as input two individuals (often called the ""parents"") and somehow combines their chromosomes, so as to produce usually two other chromosomes (the ""children""), which inherit, in a certain way, the genes of both parents.For more details about these operations, you can use the book Genetic Algorithms in Search, Optimization, and Machine Learning by David E. Goldberg (who is an expert in genetic algorithms and was just advised by John H. Holland). You can also take a look at the book Computational Intelligence: An Introduction (2nd edition, 2007) by Andries P. Engelbrecht."
Setting learning rate as negative number for wrong train cases,"
I was watching a video which tells a bit about reinforcement learning, and I learnt that If the robot makes wrong movement then they train the network with negative learning rate. From this method, something came to my mind.
My question is ""Can I use a wrong data to train a neural network?"".
To illustrate the method, I'll be using the eye tracker project that I'm working on right now. In my project There are photos and the points that corresponds the locations that I m looking to at that photo. Its like grid (9, 16). If I look to the middle of the screen, it means the output is (4, 7.5). if I look left up side of the screen it means (0,0). Normally for a photo that I'm looking into the middle, we use that photo as input and (4, 7.5) as output to train network using positive learning rate. Now let me rephrase the question. Can I train a model giving a photo that I'm looking into the middle as input and (0,0) as output(label) using negative learning rate?
Thank you, If I made a mistake against the rules of stackoverflow, I'm so sorry. I'll be waiting your valuable answers. 
Edit: this is a conversation between me and someone from stackoverflow, I'll let you read, hope you get a point.
-> Yes, you can. But, what would be the reason of passing a wrong ground truth to your training process? – Neb 14 hours ago 
-> If I have no various data to train, I can create more data via this method to increase the certainty when I use squared error loss. But I have doubts about this method. for example lets assume we have a photo named 'X' and its label is (5,5). at first epoch, Let the model gives (2,2) for photo 'X'. if I try to train network with a photo X and label -> (4,4) using negative learning rate, it might send away the point from (2,2) to (1,1) whereas we expect it to send the point (2,2) to (5,5). Did you get what I meant? – Faruk Nane 14 hours ago   
-> You are right. Using a negative learning rate and a wrong ground truth will not necessarly make the learning process converge to the optimal value for your net's parameters – Neb 13 hours ago 
-> So can I say that ""when I'm sure that the absolute error for each case is less than 2, I can use this method using points away 2 units."" So It'll make the outputs closer to the target point. I don't really know if we can easily say that. because we consider this method as if there are only 2 parameters which is the output point. However a model has many parameters so It might affect so differently. My brain is so confused. I think this might be an academic work, right? – Faruk Nane 13 hours ago   
-> Well, it is difficult to suggests you the path to follow without knowing the exact specifics of your problem. In any case, if you're trying to solve this problem for fun or self-improvement, I'd suggest you to experiment with the solutions you came up with and see if they works. – Neb 13 hours ago 
//EDIT: UP UP
",['deep-learning'],
What is the basic purpose of local search methods?,"
I read about the hill climbing algorithms, the simulating annealing algorithm, but I am confused. What is the basic purpose of local search methods?
","['optimization', 'search', 'hill-climbing', 'simulated-annealing', 'local-search']","Hill climbing, Simulated annealing, genetic algorithms are different variants of local search algorithm. Hill Climbing use to find good solution for NP-hard problems. Simulated annealing was first used extensively to solve VLSI layout problems. It has been applied widely to factory scheduling and other large-scale optimization tasks. Genetic algorithm have had a wide-spread impact on optimization problems, such as circuit layout and job-shop scheduling."
"Regarding L0 sparsification of DNNs proposed by Louizos, Kingma and Welling","
I am reading the paper on $\ell_0$ regularization of DNNs by Louizos, Welling and Kingma (2017) (Link to arxiv).
In Section 2.1 the authors define the cost function as follows: $$ \mathcal{R}\left( \tilde{\theta}, \pi \right) = \mathbb{E}_{q(z|\pi)}\left[ \frac{1}{N} \left(\sum_{i=1}^N \mathcal{L}\left(h\left( x_i, \tilde{\theta}\circ Z\right), y_i \right) \right)\right] + \lambda\sum_{i=1}^{|\tilde{\theta}|}\pi_i. $$
In the above display, $\tilde{\theta}$ are the weights, $Z$ is a random vector of the same dimension as $\tilde{\theta}$ consisting of independent Bernoulli components $q(Z_i|\pi) \sim Bernoulli (\pi_i)$, and $\circ$ is the element-wise product. 
The authors then state the following:

the first term is problematic for $\pi$ due to the discrete nature of $Z$, which does not allow for efficient gradient based optimization. 

I am not sure I understand this. Denoting the first term by $\mathcal{R}_1 = \sum_{i=1}^N \frac{1}{N}R_i$ ($R_i$ defined below), and using the notation $\pi_z = \prod \pi_i^{z_i} (1-\pi_i)^{1-z_i}$ and $\mathcal{Z}$ for the set of all possible values of $Z$, we should have 
$$
R_i := \mathbb{E}_{q(z|\pi)}\left[ \mathcal{L}\left( h\left(x_i, \tilde{\theta}\circ z\right), y_i \right) \right] = \sum_{z \in \mathcal{Z}}\pi_z\mathcal{L}\left( h\left(x_i, \tilde{\theta}\circ z\right), y_i \right)
$$
So, it seems to me that the gradient of $R_i$ with respect to $\pi_j$ can be obtained as 
$$
\frac{d R_i}{d\pi_j} = \sum_{z \in \mathcal{Z}}\mathcal{L}\left( h\left(x_i, \tilde{\theta}\circ z\right), y_i \right) \frac{d\pi_z}{d\pi_j}
$$
and 
$\frac{d\pi_z}{d\pi_j} = \frac{\pi_z}{\pi_j}$ if $z_j=1$ and $-\frac{\pi_z}{1-\pi_j}$ if $z_j=0$. So, it appears that we can obtain the derivative of the first term with respect to $\pi_j$ as well. 
My question is the following:

If my above calculation is correct, then the derivatives $\frac{d\mathcal{R}_1}{d\pi_j}$ can be computed, and we can perform SGD on the cost function $\mathcal{R}(\tilde{\theta}, \pi)$. But the authors claim that it cannot be obtained and hence they introduce the `hard concrete' distribution etc. to construct a differentiable cost function. 

","['deep-learning', 'training', 'regularization']",
How Swarm Intelligence can empower Blockchain?,"
Are there examples of applications in blockchain consensus using swarm intelligence, as opposed to classical consensus mechanisms like PoW or PBFT?
Please note that recent classical consensuses, including lottery-based such as PoW in which the winner of lottery creates the new block, or voting-based such as PBFT or Paxos in which the entities achieve a consensus through a voting process; both approaches have problems of efficiency, latency, scalability, performance etc. And emerging a new alternative approach seems necessary. In this way, can nature-inspired algorithms (such as evolutionary algorithms (EA), particle swarm optimization (PSO), ant colony optimization (ACO) etc) be employed as an alternative to classical consensus algorithms? 
","['terminology', 'evolutionary-algorithms', 'applications', 'swarm-intelligence']",
"Why does Q-learning converge to the optimal policy, even if the agent acts sub-optimally?","
In Q-learning, during training, it doesn't matter how the agent selects actions. The algorithm always converges to the optimal policy. Why does this happen? What's the intuition?
","['reinforcement-learning', 'q-learning', 'proofs', 'convergence']",
Image prediction model when data-set classes have visual similarity,"
Lets say we have a data-set of all cats and we have to identify the cat breed based on given test image. As, the two different cat breeds have visual similarity can we use existing networks (VGG, ImageNet, GoogleNet) to solve this problem?

Should faceNet be applied here? As, the problem is similar to face detection where face characteristics of two different people are same yet it can correctly recognize a person.
What if with visual similarity in data-set we have only few example of each class? Like for a problem (random) we have good amount of data but for each class we have only few examples.

Is there any model that can be applied here?
","['neural-networks', 'convolutional-neural-networks', 'image-recognition']",
Machine learning approach to facial recognition,"
First of all I'm very new to the field. Maybe my question is a bit too naive or even trivial...
I'm currently trying to understand how can I go about recognizing different faces.
Here is what I tried so far and the main issues with each approach:
1) Haar Cascade -> HOG -> SVM:
  The main issue is that the algorithm becomes very indecisive when more than four people are trained... The same occurs when we change Haar Cascade for a pre-trained CNN to detect faces...  
2) dlib facial landmarks -> distance between points -> SVM or Simple Neural Network Classification:
  This is the current approach and it behaves very well when when four people are trained... When more people are trained it becomes very messy, jumping from decision to decision and never resolves to a choice.
I've read online that Triplet loss is the way to go... But I very confused as to how I'd go about implementing it... Can I use the current distance vectors found using Dlib or should I scrap everything and train my own CNN?
If I can use the distance vectors, how would I pass the data to the algorithm? Is Triplet loss a trivial neural network only with its loss function altered? 
I've took the liberty to show exactly how the distance vectors are being calculated:

The green lines represent the distances being calculated.
A 33 float list is returned which is then fed to the classifier.
Here is the relevant code for the classifier (Keras):
def fit_classifier(self):
    x_train, y_train = self._get_data(self.train_data_path)
    x_test, y_test = self._get_data(self.test_data_path)
    encoding_train_y = np_utils.to_categorical(y_train)
    encoding_test_y = np_utils.to_categorical(y_test)
    model = Sequential()
    model.add(Dense(10, input_dim=33, activation='relu'))
    model.add(Dense(20, activation='relu'))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(40, activation='relu'))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(20, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(max(y_train)+1, activation='softmax'))
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    model.fit(x_train, encoding_train_y, epochs=100, batch_size=10)

I think this is a more theoretical question than anything else... If someone with good experience in the field could help me out I'd be very happy! 
","['neural-networks', 'convolutional-neural-networks', 'classification', 'facial-recognition']",
Is it possible to use a feed-forward neural network to predict the actions in reinforcement learning?,"
I have done a lot of research on the internet about Reinforcement Learning and I found encountered methods of Reinforcement Learning: Q-Learning and Deep Q-Learning. And I have developed a vague idea of how these two work.
Before I knew anything about Reinforcement Learning this is how I thought it would work:
Suppose I have 2 virtual players in a game who can shoot each other, one of them is a decent playing hard-coded/pre-coded AI, and the other one is the player I want to train (to shoot the other player and dodge his bullets), the aim of the game would be to get the greatest net score (shots you hit minus shots you took) within 1 minute (a session), and you only have 20 bullets.

You have 3 actions, move left-right (0 = maxleftspeed, 1 = maxrightspeed ), jump (0=don't jump, 1=jump), shoot (0 = don't shoot, 1 = shoot).
What I thought was, you could create a basic Feed-Forward Neural Network use the enemy's position and his bullet(s)'s position(s) and bullet direction(s) for the input layer and the action being taken will be given by the (3 nodes in the) output layer.
The untrained player starts off with a randomized algorithm, then (for back-propagation) at the end of each session it modifies one of the parameters by a bit in the neural network, and a new session is started with the slightly modified NN. If this session ends with more points than the previous session, it keeps the changes and makes more changes towards that direction, otherwise, the changes are redone, or possibly reversed. I would visualise this as gradient descent similar to that of Supervised learning.
So my questions are:

Is something like this already out there? What is it called?
If nothing like this is out there, could you give me any tips to optimize this method or point out any key points I should keep in minds while carrying this out?
Since I have written this game, I have control over the speed of the actions, but if I did not, I know this AI would take ages to learn, so is there any way to make the learning faster while still keeping the basic idea in mind?
How exactly is this different from deep Q-learning (if it is)?

Thanks in advance!
","['deep-learning', 'reinforcement-learning', 'game-ai', 'q-learning']","Is something like this already out there? What is it called?There are three algorithms that have similarities to yours, but are not identical:NEAT, which is an evolutionary method that mutates neural networks and compares their fitness across populations of variations.The Cross Entropy Method, which trains a single, initially randomly behaving, neural network based on the outcomes of its best performances.REINFORCE is a policy-gradient method that trains a neural network after completing games based on the results of every game. It is very similar to your idea, except that it includes the maths/logic to sample a full gradient of all NN parameters in a single go. It does not require making a test change to a single parameter, but can calculate the full error gradient at once. Usually, however, to reduce noise and effects of correlation within a game, multiple gradients are collected from different games and averaged.The main similarity in each case is that you are performing a policy search, and training based on selecting outcomes that are closer to your goal after trials. A policy-based approach means that your NN represents the choice of action directly. A policy is a function that converts knowledge of the state of an environment into an action. If nothing like this is out there, could you give me any tips to optimize this method or point out any key points I should keep in minds while carrying this out?Your suggested method is not exactly the same as any of the suggested algorithms, and may have a problem with performance due to this:The untrained player starts off with a randomized algorithm, then (for back-propagation) at the end of each session it modifies one of the parameters by a bit in the neural network, and a new session is started with the slightly modified NN. If this session ends with more points than the previous session, it keeps the changes and makes more changes towards that direction, otherwise, the changes are redone, or possibly reversed. I would visualise this as gradient descent similar to that of Supervised learning.This single parameter change search is likely to be slow, probably unusably so for any reasonably-sized neural network. Effectively you would be performing a single parameter step for a very noisy gradient descent algorithm, per full play of the game. All of NEAT, CEM and REINFORCE are likely to perform much better than this (and none of them are state-of-the-art).is there any way to make the learning faster while still keeping the basic idea in mind?Instead of sampling the total reward at the end of each game and generating a single parameter's gradient, use a deep reinforcement learning method that works with full gradients. You might find REINFORCE interesting, as it logically the idea closest to yours, and it leads to some state-of the art methods such as A3C, DDPG, PPO . . . the main problem with all of these being that the maths is hard to understand, even though the implementations can be simple you may be left scratching your head as to why they even work. However, if you are happy with just seeing the policy gradient theorem as some maths which completes your idea, then it is there for the taking.Otherwise, if your goal is to implement something you understand the internals of fully, I would suggest that you search for a tutorial on deep Q learning, maybe something that implements DQN, as the maths and intuition behind it are more accessible.How exactly is this different from deep Q-learning (if it is)?Both your suggested method and Q learning can probably be considered ""deep reinforcement learning"".The main differences are:Q learning is a value-based method, and yours is a policy-based methodQ learning is off-policy (it learns best overall actions even whilst exploring other possibilities) whilst your approach is on-policy (the actions it learns to take are the same as the actions it takes during exploration)Deep Q learning updates all the parameters of the NN on each time step, whilst you will be updating a single parameter per episode. That makes Q learning far more sample efficient than your suggested approach."
Why are Q values updated according to the greedy policy?,"
Apparently, in the Q-learning algorithm, the Q values are not updated according to the ""current policy"", but according to a ""greedy policy"". Why is that the case? I think this is related to the fact that Q-learning is off-policy, but I am also not familiar with this concept. 
","['reinforcement-learning', 'q-learning', 'off-policy-methods', 'greedy-policy']",
Can the mean squared error be negative?,"
I'm new to machine learning. I was watching a Prof. Andrew Ng's video about gradient descent from the machine learning online course. It said that we want our cost function (in this case, the mean squared error) to have the minimum value, but that minimum value shown in the graph was not 0. It was a negative number! 
How can our cost function, which is mean squared error, have a negative value, given that the square of a real number is always positive? Even if it is possible, don't we want our error to be 0?
","['machine-learning', 'gradient-descent', 'objective-functions']",
What is the time complexity of the value iteration algorithm?,"
Recently, I have come across the information (lecture 8 and 9 about MDPs of this UC Berkeley AI course) that the time complexity for each iteration of the value iteration algorithm is $\mathcal{O}(|S|^{2}|A|)$, where $|S|$ is the number of states and $|A|$ the number of actions.
Here is the equation for each iteration:
$$
V_{k+1}(s) \gets \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V_k(s')]
$$
I could't understand why the time complexity is $\mathcal{O}(|S|^{2}|A|)$. I searched the internet, but I didn't find any good explanation. 
","['reinforcement-learning', 'algorithm', 'time-complexity', 'value-iteration']","The update equation for value iteration that you show is time complexity $O(|\mathcal{S}\times\mathcal{A}|)$ for each update to a single $V(s)$ estimate, because it iterates over all actions to perform $\text{max}_a$ and over all next states for $\sum_{s'}$.The sources you have found are probably counting an entire sweep through the state space as an ""iteration"" i.e. $\forall \space s \in \mathcal{S}: V_{k+1}(s) \leftarrow \text{max}_a \sum_{s'} T...$ That adds another factor of $|\mathcal{S}|$ making the overall complexity $O(|\mathcal{S}\times\mathcal{S}\times\mathcal{A}|)$ or $O(|\mathcal{S}|^2|\mathcal{A}|)$This definition of iteration makes sense, as the basic value iteration algorithm is required to sweep through the whole state space in order to converge. This also matches the standard test for convergence, which is made after each full sweep, and checks what the largest absolute update was at the end of the sweep - if it is below some target value for accuracy then the process is declared complete. "
What is the difference between the breadth-first search and recursive best-first search?,"
What is the difference between the breadth-first search and recursive best-first search? How can I describe the key difference between them?
","['comparison', 'search', 'breadth-first-search']","According to this articleBreadth First Search (BFS) searches breadth-wise in the problem space. Breadth-First search is like traversing a tree where each node is a state which may a be a potential candidate for solution. It expands nodes from the root of the tree and then generates one level of the tree at a time until a solution is found. It is very easily implemented by maintaining a queue of nodes. Initially the queue contains just the root. In each iteration, node at the head of the queue is removed and then expanded. The generated child nodes are then added to the tail of the queueAccording to the book Algorithms and Theory of Computation Handbook by Mikhail J. Atallah.Recursive best-first search is a best-first search that runs in space that is linear with respect to the maximum search depth, regardless of the cost function used.It works by maintaining on the recursion stack the complete path to the current node being expanded as well as all immediate siblings of nodes on that path, along with the cost of the best node in the sub-tree explored below each sibling.Whenever the cost of the current node exceeds that of some other node in the previously expanded portion of the tree, the algorithm backs up to their deepest common ancestor, and continues the search down the new path.In effect, the algorithm maintains a separate threshold for each sub-tree diverging from the current search path."
Deep Q-Learning poor convergence on Stochastic Environment,"
I'm trying to implement a Deep Q-network in Keras/TF that learns to play Minesweeper (our stochastic environment). I have noticed that the agent learns to play the game pretty well with both small and large board sizes. However, it only converges/learns when the layout of the mines is the same for each game. That is, if I randomize the mine distribution from game to game, the agent learns nothing - or near to it. I tried using various network architectures and hyperparameters but to no avail.
I tried a lot of network architectures including:

The input to the network is the entire board matrix, with the individual cells having values of -1 if unrevealed, or 0 to 8 if revealed.
The output of the network is also the entire board representing the desirability of clicking each cell.
Tried fully connected hidden layers (both wide and deep).
Tried Convolutional hidden layers (tried stacked them, using different kernel sizes, padding, etc.).
Tried adding Dropout after hidden layers too.

Is DQN applicable for environments that change every episode or have I approached this from the wrong side?
It seems no matter the network architecture, the agent won't learn. Any input is greatly appreciated. Please let me know if you require any code or further explanations.
","['reinforcement-learning', 'deep-neural-networks', 'keras', 'q-learning', 'convergence']","The inputs that you describe seem like they should be sufficient for a DQN-based agent to learn a good strategy for playing Minesweeper, regardless of whether or not the starting layout changes. The inputs contain all information that is necessary.However, the problem certainly becomes much easier (probably too easy) if the initial problem is always the same. The DQN algorithm will be very likely to ""pick up"" on this trend and ""exploit"" it. The inputs may be sufficient to learn a more general Minesweeper strategy, but if it is consistently faced with the same ""level"" every single time, it will be much easier for the DQN algorithm to just memorize exactly where the mines are and play perfectly based on that memory, rather than any actual strategy. Due to the way learning is implemented (based on gradient descent), the algorithm will generally tend to converge to such an easily-reachable ""memorization"" strategy rather than something that is actually ""smart"".For that reason, I do think training will be much slower in the case where the layout is randomized. I'm not just thinking along the lines of e.g. twice as slow here, but would expect multiple orders of magnitude more experience required for successful learning. That's just my educated guess though, never tried training DQN for minesweeper specifically so can't tell for sure. It might also require more elaborate hyperparameter tuning, and maybe require a different network architecture (probably require a larger network)."
When should I use feature learning as opposed to feature engineering?,"
With the advancement of deep learning and a few others automated features learning techniques, manual feature engineering started becoming obsolete. 
Any suggestion on when to use manual feature engineering, feature learning or a combination of the two?
","['machine-learning', 'comparison', 'feature-selection', 'data-preprocessing', 'representation-learning']","manual feature engineering started becoming obsoleteThat is wrong.Any suggestion on when to use manual feature engineering, feature learning or a combination of the two?Deep learning is awesome for natural signals like images, audio or large amounts of unstructured text (e.g. arbitrary crawled websites)There are some basic steps that make almost always sense. For example, for image recognition, it is the normalization from pixel color features in the range $\{0, 1, \dots, 255 \}$ to features in $[0, 1]$. And then there are a lot of use cases where you have structured, CSV-like data. For those, every single case I had so far was improved by feature engineering. Most of the time you actually don't have a choice here as well: how would you feed in a date time object into a neural network?Now, where does feature engineering make sense in image recognition?Resource limitations: neural networks often take a lot of resources. For example, viola-jones face detection is very resource friendly in contrast. But there are developments like the mobile net to relax that issue.Prior knowledge: for more special image data like CT scans you might observe super simple operations that almost give you the target. This might make it simpler to interpret the results at the end and, again, it helps to keep resource usage low.One other field where I suspect that manual feature engineering could help is adversarial attacks. But for this I'm not 100% sure."
Using unsupervised learning for classification problems,"
Let's say there are two types of cancer(Type 1 and Type 2). Say we want to see if one of pour friends has cancer Type 1 or 2. We can treat this as a classification problem. But what if we use unsupervised learning (clustering) to separate the data into to 2 different groups and see each whether each item in group 1 belongs to a person with cancer Type 1 or 2. We will then  see whether our friend belongs to group 1 or 2. I know it is stupid to do this and we have to do extra work but can we even do this?
Let's say that the features are only the age and the height (I know it's really dumb but just bear with me). The data associated with people with cancer Type 1 is [10, 150], [12, 153], [9, 143], [13, 160] and for people with Type 2 cancer : [20, 175], [23, 180], [19, 174]. Let's say we plot the data on a graph (without labelling the data) and the unsupervised program (Clustering) just separates the two groups (Say group 1 for Type 1). We then can see that to whom each data in group 1 belongs. We see those people have cancer Type 1. So given new data, we see what group our friend  belongs to. If she/he belonged to group 1, he's got cancer Type 1 and if not, she/he has cancer Type 2.
","['classification', 'unsupervised-learning']",
What is the best way to integrate unchangeable ethics into a chatbot,"
I am building a generative model chatbot as a research and learning project. One of the most important parts of my project is to research ways in which I can make this chatbot work in a consistently ethical fashion. 
This chatbot is simply a single Seq2Seq network running on my local machine. It can't be interacted with over the internet (yet), although I may end up creating a way to do that. It has no feedback loops of any kind as of right now, though reinforcement learning with a loop might be helpful.
The idea is that there would be some sort of unchanging knowledgebase for the chatbot to use that has hard-coded ethical statements and values that the bot has no ability to change. Whenever a question is asked of the chatbot,before being inputted to the network, the knowledgebase is searched and relevant facts will be appended to the input (separated from regular inptu by  tokens. 
My question is, will this even be effective at allowing it to generate its own responses yet still be confined to the ethical standards given it?
My main concern is that it may begin to ignore these ""facts"" over time, and they may become irrelevant. 
Another (possibly much better) approach might be to use deep reinforcement learning. However I may find it difficult to implement with my existing Sequence-to-Sequence network. 
So which would likely be better? Or perhaps I should try a combination of the two?
","['reinforcement-learning', 'chat-bots', 'ethics']","Basic FairnessIf by ethics the question means that the system design is to foster basic ethics in the context of a conversation, then at least three attributes can be designed into the system.These attributes can be encoded in fuzzy rules, incorporated into the value function of an ongoing learning algorithm, or expressed in the loss function used during training, directing what is learned by a network.  All of these strategies require the tracking of metrics and the uses of them in adjusting behavior, a kind of social awareness.There are obvious ways to do this in the mathematical expressions of values, assets, advantages, affirmations, attentions, losses, errors, pains, or other learning feedback approaches are assembled.  In generative network topologies, a discriminative network would need to be able to recognize unfairness and selfishness using the above principles.  Such places specific requirements on the designs of metrics and how fairness and the balance of interests are quantified.A sum of squares across the dimensions (features) of potential inequity that can develop during a conversation is a possibility.  Using the fourth power instead of squaring would imbue permissiveness for smaller inequities but a drastic aversion to greater inequities.$$\ell = \sum {(b_i - b_j)}^4 + \dots$$The variable $\ell$ is the loss function result and $b$ is the benefit to conversation participants $i$ and $j$ where the two might be a human and the chatbot, a human and a stakeholder of the chatbot production server, or two humans that are in separate conversations with the chatbot, or two humans that are in the same conversation with the chatbot.  Note the ellipsis indicates more terms that affect loss as needed.Cognitive Ethical SophisticationIf by ethics the question means the kind of ethics passed from families that value honor and truth to their children or taught at Harvard Law, the system design may have to wait until cognitive abilities are further developed in computing machinery.  Otherwise the question author will have to further AI to achieve the ethical sophistication.Research into semantic acquisition, abstraction, automated assembly of causal models, application of these abstractions and models in planning and execution, and the integration of these with natural linguistics is ongoing.  Broad cognition and What some might call wisdom escapes the products of AI as of this writing.A sufficiently deep network may approximate dialog that demonstrates what appears to be ethical awareness if trained with data that represents stories and conversational actions deemed to be ethical in those stories.  However, compacting learning the layers of comprehension that humans acquire through books, movies, other media, family life, and community events over a few decades (if they acquire it at all) into a computing project may be an overoptimistic objective at this time.Although a collection of classic works might demonstrate much of a high ethical standard, it is not labeled data.  One would need to identify which character was transformed in a way that demonstrates the valuable ethical standards and then configure the learning process to learn from the character arcs of ethical characters in the stories.Inherent FeedbackIt has no feedback loops of any kind as of right now, though reinforcement learning with a loop might be helpful.There is no way to adapt without feedback.  If the system is adapting, some feedback mechanism is correcting conditions that are maladaptive, even if the mechanism is opaque to the observer or identified by some other name.  In the case of GANs, the feedback creates a balance through the discriminative and generative network components.$$\Bigg(\Big((G, E_1) \Rightarrow D\Big) \Rightarrow G, E_2\Bigg) \Rightarrow D \dots$$That does not exclude the possibility that additional feedback paths may provide additional design advantages.Hard Coding EthicsCan judicious behavior be hard coded?The idea is that there would be some sort of unchanging knowledge base for the chatbot to use that has hard-coded ethical statements and values that the bot has no ability to change.Will this even be effective at allowing it to generate its own responses yet still be confined to the ethical standards given it?It is not unchanging knowledge in the domain of ethics that leads to justice, but unchanging principles that guide the detection of what is not ethical that guides a continuous improvement of ethical standards.  That is why even constitutions have rules for amendment.  In probabilistic terms, without the balance of doubt and certainty, no ethical growth can occur.No one would read a book or watch a movie where the protagonist has no character arc because it is unremarkable.  When a story is remarkable it is because ethics are adaptive and the adaptation must be a new kind that is not already covered many times in previous books or movies.  People like to be surprised by these adaptations.  They must occur against all fears and opposing external forces that are trapping the protagonist in a prison of mediocrity.The story climax is when the protagonist, against all the building forces that keep them unremarkable, takes a path that discards all previous planning and forges a new radically different but fundamentally better one.Excellent Objective in the QuestionThe intention to develop a system that is ethical is honorable and ethical in itself, so the question author may imbue some of those qualities into the system being designed.  In my opinion, such would be a triumph of greater value to the field of AI than all other progress up until now.To make such an advancement, the framing of the problem will require thinking in new directions.  Consider the stated concern.It may begin to ignore these ""facts"" over time, and they may become irrelevant.Two principles may prove elucidating in regard to this concern.First Decision in ApproachThe decision to make is the level of abstraction to apply.  Should this chatbot be ethical or be ethically minded?The first would require the imbuing of ethical character into the bot.  The second would require the imbuing of the acquisition of ethical character into the bot.  As difficult as the more abstract second choice may seem, the first may be unachievable.  We don't have an example of it.  We do, however, have one proof of concept for the second choice: Good people.The other sub-question to address is regarding selection of AI components.So which would likely be better [generative or reinforcement]? Or perhaps I should try a combination of the two?If you are serious about this objective beyond the simple tracking of basic fairness metrics, then you may have to use elements of both and possibly new things that extend the list of AI components and principles to use.Take One Step BackOne thing to understand about generative topologies is that they are not really adversarial, even if that word is in the name of a seminal paper.  They are usually highly symbiotic and involve equilibria.  What would the discriminative network be without the generative one?  What would the generative one generate without the discriminative one?The two collaborate to create a feedback system exhibiting a balanced equilibrium.  This is like biological stasis, which is the genius of it.  It is also like chemistry.  Salt in saturated suspension in water involves both dissolving and crystallization in a constant expression of opposing reactions at a molecular level.  Similarly, all ethical decisions are decisions are made within the context of at least one equilibrium, and the details matter.Every court case where a judge or jury is to apply the public perception of ethics to the case has two sides presented.  They may appear to be adversaries, but the entire legal process is a sophisticated and symbiotic design to force collaboration in the interest of justice.Every educated journalist understands the importance of avoiding sensational reporting.  Dramatically presenting only one side of the story sells news and is a constant temptation.  Journalistic integrity demands striking an equilibrium.  The result of the ethical choice determines whether the news item educates the public or adds polarization to a culture that may already be partially crippled by extremism.Take Another Step BackReinforcement is an odd name to give what is essentially Agile planning via the probabilistic projection of expected advantages.  Should lean methods for strategic approaches to conversation be in this ethical chatbot?  If ethical behavior is defined to include telling stories that convey wisdom, then yes.  Roger Schank's work is of great interest in that case.  His work in story based reasoning may be a worthy direction of study.Adapting the foundations of ethics to the ever changing story inherent in every significant conversation may require planning.  It is possibly unavoidably inherent every conversation when the intention of the participants is to do more than exchange pleasantries."
What is an objective function?,"
Local search algorithms are useful for solving pure optimization problems, in which the aim is to find the best state according to an objective function.
My question is what is the objective function?

","['terminology', 'objective-functions', 'optimization', 'local-search', 'meta-heuristics']","The ""objective function"" is the function that you want to minimise or maximise in your problem.The expression ""objective function"" is used in several different contexts (e.g. machine learning or linear programming), but it always refers to the function to be maximised or minimised in the specific (optimisation) problem. Hence, this expression is used in the context of mathematical optimisation. For example, in machine learning, you define a model, $\mathcal{M}$. To train $\mathcal{M}$, you usually define a loss function $\mathcal{L}$ (e.g., a mean squared error), which you want to minimise. $\mathcal{L}$ is the ""objective function"" of your problem (which in this case is to be minimised).In the context of search algorithms, the objective function could represent e.g. the cost of the solution. For example, in the case of the travelling salesman problem (TSP), you define a function, call it $C$, which represents the ""cost"" of the tour or Hamiltonian cycle, that is, a function which sums up the weights of all edges in the tour. In this case, the ""objective"" of your problem is to minimise this function $C$, because, essentially, you want to find an inexpensive tour, which is associated with either a local (or global) minimum of $C$. This function $C$ is the ""objective function"".It should now be easy to memorise the expression ""objective function"", as it contains the term ""objective"", and the ""objective"" (or goal) in your (optimisation) problem is to minimise (or maximise) the corresponding function."
Do we need the transition probability function when calculating the importance sampling ratio?,"
I am reading the book titled ""Reinforcement Learning: An Introduction"" (by Sutton and Barto). I am at chapter 5, which is about Monte Carlo methods, but now I am quite confused.
There is one thing I don't particularly understand. Why do we need the state-transition probability function when calculating the importance sampling ratio for off-policy prediction?
I understood that one of the main benefits of MC over Dynamic Programming (DP) is that one does not need to have a model of the state-transition probability for a system. Or is this only the case for on-policy MC?
","['reinforcement-learning', 'on-policy-methods', 'monte-carlo-methods', 'importance-sampling', 'dynamic-programming']","There is one thing I don't particularly understand. Why do we need the state-transition probability function when calculating the importance sampling ratio for off-policy prediction?It is not needed for calculation. It must be included in the theory, to compare the correct probability of each trajectory (on-policy vs off-policy). However, the state transition probabilities are then also shown to cancel out exactly, so there is no requirement to know what the values are.State transition probabilities are irrelevant to probability ratios between identical trajectories where the policy varies but the environment does not. Which is the case for off-policy learning.This is all explained on the relevant pages of the book, but replicating here for completeness. If your trajectory is from steps $m$ to $n$ and called $\tau = (s_m, a_m, r_{m+1}, s_{m+1}, a_{m+1}, r_{m+2} . . . r_{n}, s_n)$, then the probability of seeing that trajectory under two different policies $\pi(a|s)$ and $b(a|s)$ is:$$p(\tau|\pi) = (\prod_{i=m}^{n-1} \pi(a_i|s_i))(\prod_{j=m+1}^{n} p(s_j, 
r_j|s_{j-1},a_{j-1}))$$$$p(\tau|b) = (\prod_{i=m}^{n-1} b(a_i|s_i))(\prod_{j=m+1}^{n} p(s_j, r_j|s_{j-1},a_{j-1}))$$You can clearly see that the second product in both cases is the same, and cancels out in the ratio:$$\frac{p(\tau|\pi)}{p(\tau|b)} =\frac{\prod_{i=m}^{n-1} \pi(a_i|s_i)}{\prod_{i=m}^{n-1} b(a_i|s_i)}$$So it doesn't matter what $p(s', r|s, a)$ actually is, just that it exists and is not allowed to change in theory between the on-policy and off-policy cases. Or in other words, that the environment is a consistent MDP."
How to know which kind of adverb in NLP Parts of Speech (POS) tagging? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






There are 4 kinds of adverbs :

Adverbs of Manner. For example, slowly, quietly
Adverbs of Place. For example, there, far
Adverbs of Frequency. For example, everyday, often
Adverbs of Time. For example, now, first, early

nltk, spacy and textblob only tag a token as an adverb without specifying which kind it is.
Are there any libraries which tag including the type of adverb?
",['natural-language-processing'],
"Neural machine translation, that outputs multiple alternative, ambiguous translations?","
Is there neural machine translation methods, that for one input sentence outputs multiple alternative output sentences in that target language. It is quite possible, that sentence in source language have multiple meanings and it is not desirable that neural network discards some of the meanings if there is no context for disambiguation provided. How multiple outputs can be acommodated into encode-decoder architecture, or different architecture is required?
I am aware of only one work https://arxiv.org/abs/1805.10844 (and one referen herein) but I am still digesting whether their network outputs multiple sentences or whether it just acommodates variations during training phase.
","['neural-networks', 'computational-linguistics']",
Reinforcement Learning to Grouped Scheduling Optimisation Problem,"
I am not sure the name of this kind of problem, but anyway, the situation is as below.
Assign teachers into Groups and consider on each of their workload, availability etc.
There are some other soft/hard constraint (equality/inequality) like

Each group should have at least 2 teachers
Everyone in the group have similar workload
Total workload in the group is below a certain value
All are in different expertise

and more...
I am trying to build a sub-optimal solution to solve this problem. Linear/non-linear programming seems not working for grouping problems. I am thinking of genetic algorithm or reinforcement learning.
Can this problem solve by using RL or DRL?
I am trying to define the groups as state, and actions include ""assignToGroup"" and ""removeFromGroup"".
And any kind of idea or suggestion of how to solve this problem?
Many thanks
","['reinforcement-learning', 'optimization']","It is often possible, by describing certain parts of a problem as state, others as actions, and some measurement you want to optimise as a reward, to frame a problem as a reinforcement learning task. However, this is not always helpful. I suspect your problem is in the ""RL is not that helpful"" category. You could give it a try, and maybe expect to find working solutions but take an unreasonable amount of time to find optimal solutions.What you appear to have here is a problem in combinatorial optimisation. There are many applicable algorithms, a good selection are described in the free book Clever Algorithms: Nature-Inspired Programming Recipes - which is best will be determined by the specific details of your constraints and the size of the problem.If many constraints are soft and can be assigned a cost, you may be able to use genetic algorithms (with genome being group assignments in order) or simulated annealing for example."
What are the limitations of the hill climbing algorithm and how to overcome them?,"
What are the limitations of the hill climbing algorithm? How can we overcome these limitations?
","['algorithm', 'search', 'optimization', 'problem-solving', 'hill-climbing']","As @nbro has already said that Hill Climbing is a family of local search algorithms. So, when you said Hill Climbing in the question I have assumed you are talking about the standard hill climbing. The standard version of hill climb has some limitations and often gets stuck in the following scenario:To resolve these issues many variants of hill climb algorithms have been developed. These are most commonly used:The success of hill climb algorithms depends on the architecture of the state-space landscape. Whenever there are few maxima and plateaux the variants of hill climb searching algorithms work very fine. But in real-world problems have a landscape that looks more like a widely scattered family of balding porcupines on a flat floor, with miniature porcupines living on the tip of each porcupine needle (as described in the 4th Chapter of the book Artificial Intelligence: A Modern Approach). NP-Hard problems typically have an exponential number of local maxima to get stuck on.
Given algorithms have been developed to overcome these kinds of issues:Reference Book - Artificial Intelligence: A Modern Approach"
How is simulated annealing better than hill climbing methods?,"
In hill climbing methods, at each step, the current solution is replaced with the best neighbour (that is, the neighbour with highest/smallest value). In simulated annealing, ""downhills"" moves are allowed.
What are the advantages of simulated annealing with respect to hill climbing approaches? How is simulated annealing better than hill climbing methods?
","['search', 'comparison', 'hill-climbing', 'simulated-annealing']","In the least technical, most intuitive way possible: Simulated Annealing can be considered as a modification of Hill Climbing (or Hill Descent). Hill Climbing/Descent attempts to reach an optimum value by checking if its current state has the best cost/score in its neighborhood, this makes it prone to getting stuck in local optima.Simulated Annealing attempts to overcome this problem by choosing a ""bad"" move every once in a while. The probability of choosing of a ""bad"" move decreases as time moves on, and eventually, Simulated Annealing becomes Hill Climbing/Descent.If configured correctly, and under certain conditions, Simulated Annealing can guarantee finding the global optimum, whereas such a guarantee is available to Hill Climbing/Descent iff the all local optima in the search space have equal scores/costs.For more, go through Wolfram Mathworld's entry here."
Basic Functions and Results,"
If the number of input neurons and output neurons doesn't change, what will change if I have one hidden layer, but first with 1 neuron, then with 4 neurons? 
Taking into consideration the fact that each perceptron is able to linearly separate points on an unknown/unwritten linear function, would this then be able to, theoretically, instead of simply linearly separate points, separate points into those that occur inside a square, and those that occur outside?
This is, of course, without a bias neuron present.
",['artificial-neuron'],
"Machine mathematical reasoning by clever substitutions, How to do with AI","
I have three equations that relates five variables {a, b, c, r, s} with a sum and two ratios.

Eq. 1: a = b + c;
Eq. 2: s = b / a;
Eq. 3: r = b / c.

Given two values for any of the five variables I get a solution. But, this is not the automation problem I want to solve.
I can have the solution of variable r by simply knowing s. This is solved by a ""human algorithm"" as follows.

Substitute a of Eq. 1 in Eq. 2.
Divide the second term of the new Eq. 2 by the variable c.
Replace b/c by the expression of Eq. 3.

That means s = r / (r+1).
The questions is -How can an AI algorithm solve this?, i.e. the machine should recognize that given the variable r she can obtain directly the variable s and do no require another variable.
","['math', 'reasoning']",
Nature-inspired artificial intelligent methods for Blockchain?,"
I am working on the blockchain technology and I am not very familiar with the AI concept. 
The proposal of this web page: (http://www.euraxess.lu/jobs/349354) opens a discussion about use of nature inspired artificial intelligent methods for traceability chain decision in blockchain technology as an alternative to current consensus mechanisms. It continues as follows:

""It has been demonstrated that using traceability chain is a more
  effective method. In traceability chain, since the mechanism has to
  trace related information among participant’s nodes across the entire
  chain, the extraction and recognition of the data features plays a
  crucial role in improving the efficiency of the process.""

However, it does not give any example to demonstrate an instance of this approach. So, I searched in google.scholar and any ordinary web pages to find only an instance similar to this approach since it has mentioned: ""It has been demonstrated that using traceability chain is a more effective method."" (Please read the web page)   
Is someone here familiar with this approach? And if yes, Is there any article/example to explain a more about this approach of consensus in blockchain? And what does exactly mean ""traceability chain""? And also ingeneral, can we call this approach as a consensus? The text is not really clear to me.
Please not that I have no idea about this proposal and just I'd like to know if it's practicable? or it's buzzwords?
Also, may this approach related to the approach that has been mentioned in this answer (Swarm Intelligence): https://ai.stackexchange.com/a/1315/19910 Or it is a different concept?
Thanks for your help
","['algorithm', 'swarm-intelligence']",
How does Hindsight Experience Replay learn from unsuccessful trajectories?,"
I am confused by how HER learns from unsuccessful trajectories. I understand that from failed trajectories it creates 'fake' goals that it can learn from.
Ignoring HER for now, if in the case where the robotic arm reaches the goal correctly, then the value functions ($V$) and action-value functions ($Q$) that correspond to the trajectories that get to the goal quicker will increase. These high $Q$ and $V$ values are ultimately important for getting the optimal policy.
However, if you create 'fake' goals from unsuccessful trajectories - that would increase the $Q$ and $V$s of the environment that lead to getting the 'fake' goal. Those new $Q$ and $V$s would be unhelpful and possibly detrimental for the robotic arm to reach the real goal.
What am I misunderstanding?
","['reinforcement-learning', 'deep-rl', 'hindsight-experience-replay']",
What is the actual learning algorithm: back-propagation or gradient descent?,"
What is the actual learning algorithm: back-propagation or gradient descent (or, in general, the optimization algorithm)?
I am reading through chapter 8 of Parallel Distributed Processing hand book and the title of the chapter is ""Learning internal representation by error propagation"" by PDP research Group. 
https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf
If there are no hidden units, no learning happens.
If there are hidden units, they learn internal representation by propagating error back.
Does this mean back propagation[delta rule] is the learning rule and gradient descent is an optimization algorithm used to optimize cost function?
","['machine-learning', 'backpropagation', 'terminology', 'optimization']",
Train a recurrent neural network by concatenating time series. Is it safe?,"
As the title says, I want to train a Jordan network (i.e. a particular kind of recurrent neural network) using a certain number of time series.
Let's say that $x_1, x_2, \ldots x_N$ are $N$ input time series (i.e. $x_i = [x_{i,1}, x_{i,2}, \ldots, x_{i,T}]$, where $T$ is the length of the time series) and $y_1, y_2, \ldots y_N$ (i.e. $x_i = [y_{i,1}, y_{i,2}, \ldots, y_{i,T}]$) are the corresponding target time series. 
More specifically, the target time series are just sequences of ""$0$s"", which may end with sequences of ""$1$""s. Here I show you some example:
$$y_i = [0 ~ 0 ~ 0 \ldots 0 ~ 0 ~ 1 ~ 1 ~ 1 \ldots 1 ~1 ], $$
$$y_i = [0 ~ 0 ~ 0 \ldots 0 ~ 0]. $$
This means that I want that my machine ""learn to raise"" under some situations related to the corresponding inputs $x_i$. Indeed, the objective of my network is to ""raise"" an alarm if ""something"" happens.
At the moment, my training strategy is the following. I create a new time series which corresponds the concatenation of all the available $x_i$ and $y_i$. Let's call the concatenated series $X$ and $Y$. Then I use $X$ and $Y$ to train a network.
Here is my problem. If I concatenate, then I also teach to my machine to ""drop"", since I can have situation like this:
$$Y = [ \ldots 1 ~ 1 ~ 0 ~ 0 \ldots].$$
Is this really a problem? Are there other ""training strategies"" to be employed so that I avoid this kind of unwanted behaviors?
","['neural-networks', 'machine-learning', 'training', 'recurrent-neural-networks']",
Possible to translate generic English-language document into higher-order logic?,"
(Un-original) idea:
Wouldn't it be cool if we could fact-check using an algorithm that could understand a whole bunch of documents (e.g. scientific papers) as higher-order logic?
Question:
What work has been done on this to date?
What I've got so far:
(1) I seem to recall there being prior work to create a subset of English (I think intended for use in scientific writing) that could be easily interpreted by an algorithm. This doesn't quite get us to the algorithm described above (as it's restricted to a subset of English) - but seems pertinent.
(2) Once parsed, I guess a resolution algorithm like that in Prolog could be used to check wether a fact (presumably also inputted as a logical statement) contradicts the logic of the documents?
","['natural-language-processing', 'logic']",
Why does the number of feature maps increases in the VGG model?,"
I found the below image of how a CNN works 

But I don't really understand it. I think I do understand CNNs, but I find this diagram very confusing. 
My simplified understanding: 

Features are selected
Convolution is carried out so that to see where these features fit (repeated with every feature, in every position)
Pooling is used to shrink large images down (select the best fit feature).
ReLU is used to remove your negatives
Fully-connected layers contribute weighted votes towards deciding what class the image should be in.
These are added together, and you have your % chance of what class the image is.

Confusing points of this image to me:

Why are we going from one image of $224 \times 224 \times 3$ to two images of $224 \times 224 \times 64$? Why does this halving continue? What is this meant to represent?
It continues on to $56 \times 56 \times 256$. Why does this number continue to halve, and the number, at the end, the $256$, continues to double?

","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'vgg']",
What are the differences between uniform-cost search and greedy best-first search?,"
What are the differences between the uniform-cost search (UCS) and greedy best-first search (GBFS) algorithms? How would you convert a UCS into a GBFS?
","['comparison', 'search', 'uniform-cost-search', 'best-first-search']",
Use of backpropagation for weight updates in a combination of 2 neural networks,"
Every neural network updates its weights through back-propagation.
How is back-propagation used for updating weights in a combination of 2 or more neural networks (e.g.:CNN-LSTM, GAN-CNN, etc.).
For instance a CNN-LSTM model is a CNN model stacked on top of an LSTM model. When CNN model is stacked on top of an LSTM model, do we consider hidden layer of both model or hidden layer of outer model(LSTM)?
","['neural-networks', 'deep-learning', 'backpropagation']",
Is there a better way of calculating the chance of winning than $\mu * (1 - (\sigma * f)) * 100$ for the card game schnapsen?,"
My AI (for the card game schnapsen) currently calculates every possible way the game could end and then evaluates the percentage of winning for every playable card / move. The calculation is done recursively using a tree. If a game could move on in three different ways the percentage of winning on this node would be
$$\mu * (1 - (\sigma * f)) * 100,$$
where $f$ is between 0 and 2, $\mu$ is the mean and $\sigma$ the standard deviation. When the game can't move on and the AI wins the percentage is 100, when lost 0. I'm including the standard deviation in this formula to prevent the AI from risking too much. In other words: I'm using an MCTS that uses percentages.
Is there a better formula or way of calculating the next move to maximize the chance of winning? Does including the standard deviation make sense?
","['game-ai', 'monte-carlo-tree-search']","It can make sense to incorporate the standard deviation of Monte-Carlo-based evaluations in some ways to reduce risk, but I don't think the way of using it that you described would work well.For MCTS evaluations, if you're taking a zero-sum-style approach (which you are if you are trying to estimate win/loss probabilities), it is very important that your estimates are ""symmetric"" with respect to the players. If you evaluate the probability of winning for player A to be $p$, it is important that your algorithm simultaneously evaluates the probability of the other player B to be equal to $1 - p$ (or, if you prefer winning chance, if the chance of winning for one player is $p\%$, it should be $100\% - p\%$ for the other player). This appears to be violated by your idea, which subtracts standard deviation regardless of player ""perspective"".A better place to take standard deviation (or, similarly, variance) into account would be, for example, the Selection phase of MCTS. The most common strategy for the Selection phase is using the ""UCB1 equation"". You can modify that to include the variance in your observations, for example using the ""UCB1-Tuned"" strategy as described in the beginning of Section 4 of Finite-time Analysis of the Multiarmed Bandit Problem.In my answer above, I assume that you were talking about evaluations ""inside"" the algorithm, while it is still running. If you were rather thinking of the final move selection for the ""real"" game after having run the algorithm for a while, the most common approach there is to simply play the move with the maximum number of visits (also referred to as robust child), rather than playing the move with the maximum score. It should not be necessary to include standard deviation at this stage anymore."
How are artificial neural networks different from normal computer programs?,"
How are artificial neural networks different from normal computer programs (or software)?
","['neural-networks', 'machine-learning', 'comparison']","ANNs learn and adapt to the patterns inside inputs, however, general PCs don't. PCs take the process as input and provide results based on user preferences, but never adapt i.e., always depends on users what to do with the data, meanwhile, ANNs are customized for specific tasks like forecasting, function approximations etc."
Are artificial intelligence learnings or trainings transferable from one agent to the other?,"
One disadvantage or weakness of Artificial Intelligence today the slow nature of learning or training success. For instance, an AI agent might require a 100,000 samples or more to reach an appreciable level of performance with a specific task. But this is unlike humans who are able to learn very quickly with a minimum number of samples. Humans are also able to teach one another, or in other words, transfer knowledge acquired.
My question is this: are Artificial Intelligence learnings or trainings transferable from one agent to the other? If yes, how? If no, why?
","['machine-learning', 'reinforcement-learning', 'training', 'intelligent-agent']",
Significance of depth of a deep neural network,"
How is a feed-forward neural network with few hidden layers and lots of nodes in those hidden layers different from a network with a lot of hidden layers but relatively lesser nodes in those hidden layers?
","['neural-networks', 'machine-learning', 'deep-learning', 'deep-neural-networks', 'feedforward-neural-networks']",
"Given an image and two points $A$ and $B$ on that image, how could we find a path from $A$ to $B$?","
If we have a search or path-finding problem, A* and Dijkstra's algorithm require that we formulate it as a search in a graph with nodes and connections between these nodes. If there are obstacles, we also need to encode this information in the graph, so that they are not traversed. Additionally, there may be costs/weights on the connections between points. If such weights/costs are high, the algorithms won't take that path.
I've been using A* and Dijkstra's algorithm this way so far. However, it's a bit cumbersome to always have to define the nodes/points and the relationships (or connections) between them. There's no learning here. I just define a graph and the algorithms search on this graph.
Let's say I have a white image, a green blob in the middle, and points $A$ and $B$ at either side of the blob, I need to get from $A$ to $B$. I don't have a search space represented as a graph here. I just have this image.
Could I use machine learning to solve this problem (and would it generalize to more complex maps)? If so, are there any research works on this topic? Or is that the wrong route to take (pardon the pun)?
","['machine-learning', 'reference-request', 'search', 'path-planning', 'path-finding']",
What are the differences between A* and greedy best-first search?,"
What are the differences between the A* algorithm and the greedy best-first search algorithm? Which one should I use? Which algorithm is the better one, and why?
","['algorithm', 'search', 'comparison', 'a-star']","Both algorithms fall into the category of ""best-first search"" algorithms, which are algorithms that can use both the knowledge acquired so far while exploring the search space, denoted by $g(n)$, and a heuristic function, denoted by $h(n)$, which estimates the distance to the goal node, for each node $n$ in the search space (often represented as a graph). Each of these search algorithms defines an ""evaluation function"", for each node $n$ in the graph (or search space), denoted by $f(n)$. This evaluation function is used to determine which node, while searching, is ""expanded"" first, that is, which node is first removed from the ""fringe"" (or ""frontier"", or ""border""), so as to ""visit"" its children. In general, the difference between the algorithms in the ""best-first"" category is in the definition of the evaluation function $f(n)$.In the case of the greedy BFS algorithm, the evaluation function is $f(n) = h(n)$, that is, the greedy BFS algorithm first expands the node whose estimated distance to the goal is the smallest. So, greedy BFS does not use the ""past knowledge"", i.e. $g(n)$. Hence its connotation ""greedy"". In general, the greedy BST algorithm is not complete, that is, there is always the risk to take a path that does not bring to the goal. In the greedy BFS algorithm, all nodes on the border (or fringe or frontier) are kept in memory, and nodes that have already been expanded do not need to be stored in memory and can therefore be discarded. In general, the greedy BFS is also not optimal, that is, the path found may not be the optimal one. In general, the time complexity is $\mathcal{O}(b^m)$, where $b$ is the (maximum) branching factor and $m$ is the maximum depth of the search tree. The space complexity is proportional to the number of nodes in the fringe and to the length of the found path.In the case of the A* algorithm, the evaluation function is $f(n) = g(n) + h(n)$, where $h$ is an admissible heuristic function. The ""star"", often denoted by an asterisk, *, refers to the fact that A* uses an admissible heuristic function, which essentially means that A* is optimal, that is, it always finds the optimal path between the starting node and the goal node. A* is also complete (unless there are infinitely many nodes to explore in the search space). The time complexity is $\mathcal{O}(b^m)$. However, A* needs to keep all nodes in memory while searching, not just the ones in the fringe, because A*, essentially, performs an ""exhaustive search"" (which is ""informed"", in the sense that it uses a heuristic function).In summary, greedy BFS is not complete, not optimal, has a time complexity of $\mathcal{O}(b^m)$ and a space complexity which can be polynomial. A* is complete, optimal, and it has a time and space complexity of $\mathcal{O}(b^m)$. So, in general, A* uses more memory than greedy BFS. A* becomes impractical when the search space is huge. However, A* also guarantees that the found path between the starting node and the goal node is the optimal one and that the algorithm eventually terminates. Greedy BFS, on the other hand, uses less memory, but does not provide the optimality and completeness guarantees of A*. So, which algorithm is the ""best"" depends on the context, but both are ""best""-first searches. Note: in practice, you may not use any of these algorithms: you may e.g. use, instead, IDA*."
Do GANs come under supervised learning or unsupervised learning?,"
Do GANs come under supervised learning or unsupervised learning?
My guess is that they come under supervised learning, as we have labeled dataset of images, but I am not sure as there might be other aspects in GANs which might come into play in the determination of the class of algorithms GAN falls under.
","['neural-networks', 'terminology', 'unsupervised-learning', 'generative-adversarial-networks', 'supervised-learning']",
"Why is the variational auto-encoder's output blurred, while GANs output is crisp and has sharp edges?","
I observed in several papers that the variational autoencoder's output is blurred, while GANs output is crisp and has sharp edges.
Can someone please give some intuition why that is the case? I did think a lot but couldn't find any logic.
","['comparison', 'generative-adversarial-networks', 'autoencoders', 'generative-model', 'variational-autoencoder']",
Why doesn't VAE suffer mode collapse?,"
Mode collapse is a common problem faced by GANs. I am curious why doesn't VAE suffer mode collapse? 
","['comparison', 'generative-adversarial-networks', 'autoencoders', 'variational-autoencoder']",
Why do we need artificial intelligence?,"
We seem to be experiencing an AI revolution, spurring advancement in many fields.
Please explain, at a high level, the uses of artificial intelligence and why we might need it.
",['applications'],"The question, ""Why do we need artificial intelligence?"" is quite to the point.  Technically, the answer is, ""No reason.""  If we needed artificial intelligence, then we would have become extinct over the last 50,000 or more years we've been a species with human intelligence, so we want it.We do want it, and there are benefits.  Some claim there are risks too, which is logical.  We should be open to all views.  Some of them may be spot on, and others may be without merit.One list of benefits already seen might include these, with most beneficial on top.These are further potential benefits that may also emerge.These are the risks from highest to lowest."
Overfitted model performs better in test set,"
There are two models for the same task:
model_1: 98% accuracy on training set, 54% accuracy on test set.
model_2: 48% accuracy on training set, 47% accuracy on test set.
From the statistics above we can say that model_1 overfits training set.
Q1: Can we say that model_2 underfits?
Q2: Why model_1 is bad choice if it performs better than model_2 on test set?
","['machine-learning', 'deep-learning', 'overfitting']",
How many trees should be generated in a random forest?,"
What are ways of determining the number of trees to be generated in a random forest algorithm?
","['machine-learning', 'decision-trees', 'hyper-parameters', 'random-forests']",The number of estimators in Random Forest is a hyper-parameter. If you are using SKLearn's Random Classifier you can use one of the following techniques to find a (near) optimal hyperparameter settings (Note:You can tweak other hyperparameters like min_leaf_size etc as well with this approach);GridSearchCV You can specify a grid of all the hyperparameters and a scoring criteria. This function will then evaluate all combinations of these parameters for you and return the setting which performed best on the validation set.RandomSearchCV You can specify a grid of all the hyperparameters and a scoring criteria. This function will then evaluate n randomly choosen combinations of these parameters for you and return the setting which performed best on the validation set.Bayesian Optimization You can treat the hyperparameter settings and corresponding score as a black box function and use exploitation-exploration paradigm using bayesian optimization.
Do genetic algorithms and neural networks really think? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I'm aware of those AI systems that can play games and neural networks that can identify pictures.
But are they really thinking? Do they think like humans? Do they have consciousness? Or are they just obeying a bunch of codes?
For example, when an AI learns to play Pacman, is it really learning that it should not touch the ghosts, or are they just following a mechanical path that will make them win the game?
","['neural-networks', 'philosophy', 'genetic-algorithms', 'artificial-consciousness', 'genetic-programming']","TL;DR Ignore the hype, current systems (in 2018) are very far removed from human-like ""thinking"", despite interesting and useful results. State-of-the-art for ""thinking and behaving like a creature in general"" has not reached the sophistication level of insects, even though we have example narrow AIs that can beat the world's best at intellectual games.There are some issues with the question as phrased, it is not a precise question, and includes some common wooly logic that many people have when discussing AI. However, I think these occur so often, that they are worth addressing here:But are they really thinking. Define ""thinking"". It is not easy.Do they think like humans? We don't fully know or understand how humans think.Do they have consciousness? We don't fully know or understand what consciousness is.Or are they just obeying a bunch of codes?Are you just obeying a ""bunch of codes""? There is no reason to suspect that humans have a magic ""something"" that powers thought. All scientific evidence points to humans being sophisticated machines that follow the laws of physics and chemistry. However, the level of complexity is such that how brains, thinking and consciousness work is a very hard problem to solve. It is also possible that our lack of knowledge obscures some unknown property of the brain or ""thinking"" that means it is more than applied physics and chemistry - but there is zero evidence for such a thing.For example, when an AI learns to play pacman, is it really learning that it should not touch the ghosts or are they just following a mechanical path which will make them win the game?This is an interesting question, and there is more to it than a simple yes/no answer:Learning is not the same as thinking. We expect an intelligent entity to be able to learn, when it receives new information. However, something that only learns - the only feature of it is that it gains some performance measure when fed experience - is only solving part of what it means to be intelligent or thinking.A human player starts with assumptions. A game like Pacman presents a simplified world that obeys many rules that we are already familiar with. In the game there is a space (the screen), in which rules of distance apply. There are objects (the pacman, walls, pills, power pills, ghosts) that have coherent shapes, and persistent properties that are recognisable. Object persistence is a thing. The game play follows familiar concepts of time. All these things are at least initially meaningless to a neural network.A neural network generalises. One of the reasons of the success of Deep Learning is that neural networks can, within limits, learn rules such as ""avoid the ghosts"" or in the case of a typical agent in DQN, that ghosts getting closer to the player is an indication that there is a low value in staying still or moving towards a ghost, and a high value in moving away towards an escape route. Not only that, but there is a good chance that a deep neural network will learn an internal representation that really does detect the ghosts and learn an association between them and the type of action, and this can be inspected by generating ""heat maps"" of neuron response to different areas of an imageNeural networks require far more experience than humans to learn to play well. This demonstrates that they are learning differently. Some of this may be due to human innate knowledge transferring to playing a game. However, a typical reinforcement-learning training scenario would need to demonstrate that ghosts are dangerous 100s of times before the neural network finally is able to generalise well. Whilst a human would learn that after only a few encounters, maybe only one. There is much research in this area, as making learning as efficient as possible is an important goal.Neural networks are too direct and simple to possess internal world models. Raw generalisation is not the same as having the kind of rich internal model that you may be considering as ""thinking"". After training, the neural network is a function which maps a visual field of pixels into values and/or actions. There is no internal ""narrative"", even though the function is enough to behave correctly, it does not encode concepts such as space, object persistence, object traits etc, and it most definitely does not experience them. In some ways, asking if an artificial neural network can think is like asking if a small slice of your retina, or a tiny cube (less than 1mm3) of brain tissue can think. Or perhaps if your walking reflex and innate sense of balance counts as thinking. Current state-of-the-art efforts are at that kind of scale both in terms of computer power and complexity."
Are Computer Vision and Digital Image Processing part of Artificial Intelligence?,"
There are some fields of Computer Vision that are similar to Artificial Intelligence. For example, pattern recognition and path tracking. Based on these similarities, can we say that the Computer Vision is a part of Artificial Intelligence?
","['machine-learning', 'image-recognition', 'computer-vision']",
What are the most instructive movies about artificial intelligence?,"
The field of AI has expanded profoundly in recent years, as has public awareness and interest. This includes the arts, where fiction about AI has been popular since at least Isaac Asimov.  Films on various subjects can be good teaching aids, especially for younger students, but it can be difficult for a non-expert to know which films have useful observations and insights, suitable for the classroom.
What are insightful films about AI?
Listed films must be suitable for academic analysis, providing insight into theory, methods, applications, or social ramifications.
","['social', 'mythology-of-ai']","Here are my suggestionsHer, the AI part (movie spoiler): The AI can define a user's profile just by hearing his short story, and ""act"" based on the user's profile. The AI makes the user comfort with it (her). It shows a very advance user profiling.Ex Machina the AI part (movie spoiler): This movie will show you how an AI learn to trick someone. The AI can express her feelings, and make you trust their feeling.Eagle Eye, the AI part (movie spoiler): A movie about a general story ""AI that want to kill"". This movie can show you how The AI can compile a lot of information for its purpose.  Big Hero 6, the AI part (movie spoiler): Baymax is a very good example of a very complex expert system, he has a ""knowledge chip"" and a very smart way to diagnose people"
"How Hopfield neural networks are connected with ""industrial"" neural networks used in machine learning?","
I am trying to read https://arxiv.org/abs/1701.01727 about generalisation of Hopfield neural networks and I like the clear ideas that physics and Hamiltoanian framework can be used for modeling such networks and for deducing lot of properties. My question is - how Hopfield networks are connected to the standard networks?
I see at least 3 differences:

Hopfield neurons has binary values on/off (+1, -1) but machine learning neurons have real (are at least approximately (within machine limits) real) values.
threshold function is the only activation function used in Hopfield neurons, machine learning has lot of  more complex, real-valued functions
Connectivity patterns in machine learning networks (LSTM, GRU cells) are far more richer.

So - maybe Hopfield neural networks can be generalized up to the level of machine learning networks and at the same time preserving the use of Hamiltonian framework. Is it possible, is there any work in this direction? 
",['neural-networks'],
Can an artificial intelligence eventually think like a human?,"
It seems to me that the way neural networks are trained is similar to the way we educate a child (or a person, in general).
Can an AI eventually think like a human?
","['philosophy', 'human-like']","You need to fully understand the human mind before you can say that a  machine can eventually think like a human. However, the human brain is nowhere near fully understood, nor is human cognition."
RNN and LSTM for discovering time lag,"
Is there a good reference / tutorial for using RNN/LSTM to determine lag interval for 2 time series? E.g. I have {x_n}, {y_n} and I want to figure out by how much does {x_n} typically lags behind {y_n}?
",['deep-learning'],
How is iterative deepening A* better than A*?,"
The iterative deepening A* search is an algorithm that can find the shortest path between a designated start node and any member of a set of goals. 
The A* algorithm evaluates nodes by combining the cost to reach the node and the cost to get from the node to the goal. How is iterative deepening A* better than the A* algorithm?
","['search', 'comparison', 'a-star', 'ida-star']","A* is a best-first search algorithm, which means that it is an algorithm that uses both ""past knowledge"", gathered while exploring the search space, denoted by $g(n)$, and an admissible heuristic function, denoted by $h(n)$, which estimates the distance to the goal node, for each node $n$. There are other best-first search algorithms, which differ only in the definition of their ""evaluation function"", denoted by $f(n)$. For example, the evaluation function of A* is $f(n) = g(n) + h(n)$, where $h$ is admissible.A* is optimal, that is, it finds the optimal path between the starting and goal nodes or states. The optimality of A* is due to the use of an admissible heuristic function, which is a heuristic function which always underestimates the distance to the goal. A* is also complete, that is, it eventually finds this optimal path. However, the problem with A*, in many real-world situations, is that it has an exponential space complexity; more specifically, its space complexity is $\mathcal{O}(b^m)$, where $b$ is the (maximum) branching factor and $m$ is the maximum depth of the search tree. It also has a time complexity of $\mathcal{O}(b^m)$, but, in practice, it tends to find the solutions quite ""fast"" (for relatively small search spaces).IDA* combines A* and IDDFS (or IDS). It does it ""well"", so it gets the advantages of both algorithms. It can be thought of as the IDDFS which uses, instead of the depth, the evaluation function $f(n) = g(n) + h(n)$ as the ""cost threshold"" or ""cutoff"". In the IDA* algorithm, a child $c$ of a node $n$ is added to the frontier (or fringe or border) if $f(c) < T$. This threshold $T$ is, at the beginning of the algorithm, the estimate of the distance to the goal from the starting node $s$, that is, $T = h(s)$. $T$ increases at each iteration of the algorithm (See the pseudocode of the IDA* algorithm for the details).IDA* is also complete and optimal, but, as opposed to A*, it has a polynomial space complexity, more specifically, $\mathcal{O}(bd)$, where $b$ is the (maximum) branching factor and $d$ is the maximum depth fo the tree. IDA* still has an exponential time complexity of A*.To conclude, IDA* has a better memory usage than A*. As in A* and unlike IDDFS, it concentrates on exploring the most promising nodes, and thus does not go to the same depth everywhere in the search tree (whereas ordinary IDDFS does). A* can be thought of as a dynamic programming algorithm. IDA* often ends up exploring the same nodes many times (as IDDFS), but, asymptotically, both A* and IDA* have the same exponential time complexity, that is, $\mathcal{O}(b^m)$.So, which one is the best one? A* becomes impractical when the search space is huge, because of the memory constraints. So, in those cases, IDA* is definitely more appropriate. In general, IDA* is one of the very best optimal state space
search techniques around. However, A* is conceptually simpler than IDA*. As a consequence, in practice, A* may be easier to implement than IDA*, but, in real-world scenarios, this is not really a problem, as they are both relatively easy to implement (anyway)."
Getting better results in improving the configuration,"
Currently, I found the right recipe for a time series regression problem to finally get acceptable to good results. 
Here is the config file 
{
    ""data"": {
        ""sequence_length"":45,
        ""train_test_split"": 0.85,
        ""normalise"": false,
        ""num_steps"": 10
    },
    ""training"": {
        ""epochs"":30,
        ""batch_size"": 32
    },
    ""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 161,
                ""input_timesteps"": 45,
                ""input_dim"": 161,
                ""return_seq"": true,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 161,
                ""activation"": ""relu"",
                ""return_seq"": false
            },
            {
                ""type"": ""dense"",
                ""neurons"": 128,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dense"",
                ""neurons"": 1,
                ""activation"": ""linear""
            }
        ]
    }
}

Here is the results I got

What can be good improvements I can bring to my model so that I can get better results? There are a lot of small spikes and other places where the curve is rather constant that I should get a rise or fall of the curve.
","['neural-networks', 'recurrent-neural-networks']",Try using an RMSProp optimizer instead of Adam optimizer. Also try decreasing the batch size and keep a small learning rate like 0.001.
How do I predict if it is rainy or not?,"
I'm building a weather station, where I'm sensing temperature, humidity, air pressure, brightness, $CO_2$, but I don't have a raindrop sensor. 
Is it possible to create an AI which can say if it's raining or not, with the help of the given data above and maybe analyzing the slope from the last hour or something? Which specific technology should I use and how can I train it?
","['training', 'prediction', 'models']",
How to add external training in chatterbot? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



I created a very simple bot to learn how to use chatterbot. This library already comes with a training, but I wanted extra training with an import of a corpus in Portuguese that I found in github.
from chatterbot import ChatBot

bot = Futaba(
""Terminal"",
storage_adapter=""chatterbot.storage.SQLStorageAdapter"",
logic_adapters=[
""chatterbot.logic.MathematicalEvaluation"",
""chatterbot.logic.TimeLogicAdapter"",
""chatterbot.logic.BestMatch""
],

input_adapter=""chatterbot.input.TerminalAdapter"",
output_adapter=""chatterbot.output.TerminalAdapter"",
database_uri=""../database.db""
)

print(""Type something to begin..."")

while True:
    try:
        bot_input = bot.get_response(None)
    except (KeyboardInterrupt, EOFError, SystemExit):
        break

That's all I have.
How can I import this corpus into my chatbot?
","['training', 'python', 'chat-bots']",
Is there any deep learning object detection algorithms that can work without bounding boxes annotated data?,"
For example Haar Cascade can be trained using only positive and negative examples, you don't need any bounding box annotations. But it not a deep learning approach.
Another example can be the most straight forward Image recognition model + sliding window. But it is very slow
","['deep-learning', 'computer-vision']",
Chess policy network [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I am interested in making a simple chess engine using neural networks. I already have a fairly good value network but I can't figure out how to train a policy network. I know that Leela chess zero outputs the probability of any of the about 1800 possible moves. But how do you train such a network? How do you calculate the loss when you  only have the 1 move that was played in the game 
to work with?
","['deep-learning', 'deep-neural-networks', 'chess', 'objective-functions']","It may come as a surprise, after learning all about dynamic programming, temporal difference learning, SARSA/Q-learning, to then discover that there is yet another whole dimension to reinforcement learning (on top of choices for on-policy/off-policy, model-based/model-free, bootstrap/monte carlo etc). That is value-based vs policy-based methods. Policy-based methods are often taught after value-based methods, because they are more complex.You can learn the parameters of a policy function by training it using a policy gradient method. The archetypical policy gradient method is REINFORCE, although that is not very efficient. You may have heard of policy gradient methods developed recently: A3C, A2C, DDPG, TRPO, PPO . . . there are a few.How do you calculate the loss when you only have the 1 move that was played in the game to work with?You can pre-train a policy network using supervised learning (perhaps using the moves of winning players in high quality games) - that would use multi-class cross-entropy loss that you may be familiar with from supervised classification problems. Policy Gradient methods work with a reward summation function defined as expected reward given the distribution of states. If your network parameters are $\theta$, then that may look like this:$$J(\theta) = \sum_{s \in \mathcal{S}} \rho_{\pi}(s) \sum_{a \in \mathcal{A}} \pi(a|s,\theta)q_{\pi}(s,a)$$where $\rho_{\pi}(s)$ is the expected proportion of time steps spent in state $s$. There is a way to take a sample gradient of this that can be used for gradient ascent - the derivation is called the Policy Gradient Theorem. It's a bit long to include in this answer, but the upshot is that you can use your sampled single step to generate an approximate gradient towards improving the policy. There are a few variations, but for instance advantage actor critic uses this:$$\nabla J(\theta) = \hat{A}(s,a)\nabla\text{log}(\pi(a|s,\theta))$$where $\hat{A}(s,a)$ is your current estimate of the advantage (or $Q(s,a) - V(s)$) for taking a specific action in state s.The related loss function is $$\mathcal{L}(\theta) = -A(s,a)\text{log}(\pi(a|s,\theta))$$The $\text{log}$ function looks like an odd addition, but is just a consequence of adjusting $\nabla J$ to take account of ratios in which actions are taken in the current policy. In fact $\nabla\text{log}(\pi(a|s,\theta)) = \frac{\nabla\pi(a|s,\theta)}{\pi(a|s,\theta)}$ and it may help your intuition to keep it in that form (the $\text{log}$ form is concise and used elsewhere in statistics as the ""score function"", but it is not necessary for anything specific in RL).The variations in policy gradients may use other functions than the advantage function, and it is not clear that there is any ""best"" one. The policy gradient theory basically gives us ways of estimating relative benefits of actions, and allows for any offset to estimated return from $(s,a)$ that does not further depend on the choice of action $a$. So you can use any method for getting an estimated return, and offset it with anything that you think might normalise the updates - common choices for the latter include subtracting average reward, or subtracting the state value function."
What are the differences in scope between statistical AI and classical AI?,"
What are the differences in scope between statistical AI and classical AI?
Real-world examples would be appreciated.
","['comparison', 'statistical-ai', 'symbolic-ai']","Statistical AI, arising from machine learning, tends to be more concerned with inductive thought: given a set of patterns, induce the trend.Classical AI is the branch of artificial intelligence research that concerns itself with attempting to explicitly represent human knowledge in a declarative form, i.e given a set of constraints, deduce a conclusion.Another difference is that C++, Python, and R tend to be a favorite language for statistical AI, while LISP and PROLOG dominate in classical AI.A system can't be more intelligent without displaying properties of both inductive and deductive thought. This leads many to believe that, in the end, there will be some kind of synthesis of statistical and classical AI."
"Understanding the proof of theorem 2.1 from the paper ""Efficient reductions for imitation learning""","
I am trying to understand the proof of theorem 2.1 from this paper:

Ross, Stéphane, and Drew Bagnell. ""Efficient reductions for imitation learning."" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.

The cost-to-go is given as 
$$J(\pi) = \sum_{t=1}^{T}\mathbb{E}_{s\,\sim\, d^t_{\pi}(s)}\left[C_\pi(s)\right].$$
In the paper they use $\hat{\pi}=\pi$ for the learned policy and $\pi^*$ for the expert policy.
In the derivation they write
$$J(\pi)\leq \sum_{t=1}^{T}\{ p_{t-1}\mathbb{E}_{s\, \sim \, d_t(s)}\left[C_\pi(s) \right]+(1-p_{t-1})\}$$
$$\leq \sum_{t=1}^{T}\{ p_{t-1}\mathbb{E}_{s\, \sim \, d_t(s)}\left[C_{\pi^*}(s) \right]+p_{t-1}{\ell_t(s,\pi)}+(1-p_{t-1})\},$$
in which $p_{t-1}$ is the probability of not not making an error with policy $\pi$ up to the time $t-1$. And $\ell$ is the surrogate 0-1 loss. 
The following steps are easy to follow, but how did they come up with these steps? 
","['reinforcement-learning', 'proofs']",
Deep Q-Learning: why don't we use mini-batches during experience reply?,"
In examples and tutorial about DQN, I've often noticed that during the experience replay (training) phase people tend to use stochastic gradient descent / online learning. (e.g. link1, link2) 
# Sample minibatch from the memory
minibatch = random.sample(self.memory, batch_size)
# Extract informations from each memory
for state, action, reward, next_state, done in minibatch:
    # if done, make our target reward
    target = reward
    if not done:
      # predict the future discounted reward
      target = reward + self.gamma * \
               np.amax(self.model.predict(next_state)[0])
    # make the agent to approximately map
    # the current state to future discounted reward
    # We'll call that target_f
    target_f = self.model.predict(state)
    target_f[0][action] = target

Why can't they use mini batches instead?
I'm new to RL, but in deep learning people tends to use mini-batches as they would result in a more stable gradient. Doesn't the same principle apply to RL problems? Is the randomness/noise introduced actually beneficial to the learning process? Am I missing something, or are these sources all wrong?

Note:
Not all the sources rely on stochastic gradient descent: e.g. keras-rl seems to rely on minibatches (https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py)
","['deep-learning', 'reinforcement-learning', 'gradient-descent']","DQN ""library"" implementations that I have seen do use mini-batches to train, and I would generally recommend this, as it usually strikes a reasonable balance between number of weight updates and accuracy of the gradients.In your first link, and the code excerpt, the sample list is literally called minibatch. However, the developer then goes on to make a series of individual SGD steps per item within that sample. Why can't they use mini batches instead? They can, and it works just fine. In my DQN explorations I tend to use a single mini-batch per timestep, and the Adam optimiser to accelerate learning. I also use a mini-batch to calculate TD target values.I'm new to RL, but in deep learning people tends to use mini-batches as they would result in a more stable gradient. Doesn't the same principle apply to RL problems? It does, but I don't know if this is the most sample efficient that it could be. I prefer to use mini-batches and more complex optimisers because this runs faster (for a given number of replay training samples per time step), and I can run more time steps of the environment in less time.It is possible for instance that a replay sample size of 32 per time step, with 4 mini-batch updates of 8 each would be the most sample efficient approach for a specific problem (because it makes more updates from the given samples), but it might not be the most CPU efficient.Is the randomness/noise introduced actually beneficial to the learning process? I don't think so, but there are plenty of other tuning issues to consider in RL.Am I missing something, or are these sources all wrong?They are not necessarily wrong, but may be CPU inefficient. More likely in my opinion, your sources are explaining algorithms in principle, and did not want to add too many layers of optimisation, or too many components that may need tuning to work. There is a lot going on in a typical DQN agent, and having many performance optimisations can hide the details that the person is trying to teach."
zonal or template ocr invoices reading,"
I'd like to explore the possibilities of applying artificial intelligence to ocr reading.
Basic ocr invoices processing let me convert 30% of them only. 
The main purpose is defining invoices areas by training an ai, then process those areas with ocr.
So I am looking into ai to define and recognize a document topology first, then apply ocr locally.
From a brief search, it is classified as zonal or template ocr.
Any chance of a premade open source library?
","['neural-networks', 'topology', 'optical-character-recognition']",
Price difference predictions curve almost vanished,"
With a team, we are studying how it is possible to predict the price movement with high-frequency. Instead of predicting the price directly, we have decided to try predicting price difference as well as the features. In other words, at time t+1, we predict the price difference and the features for time t+2. We use the predicted features from time t+1 to predict the price at time t+2. 
We got very excited, because we thought getting good results with the following graph

We got problems in production and we wasn't known the problem till we plot the price difference.

Here is the content of the config file
{
    ""data"": {
        ""sequence_length"":30,
        ""train_test_split"": 0.85,
        ""normalise"": false,
        ""num_steps"": 5
    },
    ""training"": {
        ""epochs"":200,
        ""batch_size"": 64
    },
    ""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""input_timesteps"": 30,
                ""input_dim"": 101,
                ""return_seq"": true,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""activation"": ""relu"",
                ""return_seq"": false
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""linear""
            }
        ]
    }
}

Prices don't change very fast. Therefore, the next price is almost always very close to the last price. In other words, P_{t+1} - P_{t} is very often close to zero or zero directly. If there is too many zeros then the network will only recognize the zeros. The model has picked up on that.
I guess the model learned almost nothing except the very simple relationship that the next price is close to the last price. There is not necessarily anything wrong with the model. Predicting stock prices should be a very hard problem. 
So a straightforward improvement should be of taking the features as a whole instead of their difference. 
I want to keep working with price difference instead of the price in itself because we are making the series potential more stationary.
What might be a good solution to deal with the repetitive zeros related to our ""price difference"" problem? Does applying the log-return is a better idea than applying price differences?
Does a zero inflated estimators is a good idea? First predict whether it's gonna be a zero. If not predict the value. https://gist.github.com/fonnesbeck/874808 ?
","['neural-networks', 'machine-learning', 'deep-learning', 'long-short-term-memory']",
Which algorithm is used in the robot Sophia to understand and answers the questions?,"
Which algorithm is used in the robot Sophia to understand and answer the questions?
","['robotics', 'robots', 'question-answering', 'sophia']","Sophia ,first , has all the questions and corresponding answers preprogrammed. It is a system which is a hybrid of Bayes Text classification and decision trees. It may consist of a speak recognizer which converts the question into a string.This string then travels into a algorithm which gets a suitable answer for it.The answer is then spoken by the speech synthesizer along with some face actions.It might also have a sentiment analyser which identifies sentiments in a answer to show face expressions accordingly.Along with this, it identifies the emotions on the face of the person sitting in front of it, so as to provide a greater expression.Hence, Sophia understands or thinks nothing like the human brain. It is just a system which gives predefined answers corresponding to the question asked."
How to overcome overfitting to single player styles in reinforcement learning?,"
I am implementing an actor-critic reinforcement learning algorithm for winning a two player tic-tac-toe like game. The agent is trained against a min-max player and after a number of episodes is able to learn a set of rules which lead it to winning a good majority of games.
However, as soon as I play against the trained agent by using even a slightly different playing style, it looses miserably. In other words, it is evident the agent overfitted with respect to the deterministic behaviour of the min-max player. It is clear to me what are the roots of the problem, but I would like to get an overview of the different methodologies which can be applied to overcome (or mitigate) this issue. 
The two solutions I would like to try are the following:
1. Training the agent with different opponents for fixed amounts of episodes (or time) each. So that for example I train the agent by using a depth 2 min-max player for the first 10000 episodes, then I use a random playing agent for the next 10000 episodes, then I use a depth 4 min-max player for other 10000 episodes and repeat the process.
2. Starting episodes from different initial configurations. In this way the agents will play a much wider set of sampled games and will be more difficult for the agent to overfit.  
Are these two reasonable approaches? Are there other tricks/good practices to try out?
","['reinforcement-learning', 'overfitting']","Both of the solutions you suggest seem to be built around the intuition that it's good to ensure that there is sufficient variety in the experiences that you provide to your RL algorithm.That intuition is good, but it should not come at (too much of) a cost in playing strength of the opponent. I'm afraid that your first solution may break down because of this; Tic Tac Toe is such a simple game, any agent that doesn't play optimally can really be viewed as a very poor agent... and I think minimax agents with very low limits on search depth may end up playing suboptimally. Your second solution seems better in this regard, that could help.For this particular case of Tic Tac Toe, I suspect you should be able to train just fine against only optimal minimax agents, as long as you make sure that those optimal minimax agents break ties randomly. In some situations, there may be multiple different actions that are all ""equally optimal"". Then, you'll want to make sure that your minimax agents break ties randomly, rather than always deterministically picking the same action. This can be done, for example, by making sure to always shuffle your lists of legal moves in minimax after they are generated."
How does the uniform-cost search algorithm work?,"
What is the uniform-cost search (UCS) algorithm? How does it work? I would appreciate seeing a graphical execution of the algorithm. How does the frontier evolve in the case of UCS?
","['definitions', 'search', 'uniform-cost-search']","Uniform Cost Search is also called the Cheapest First Search. For an example and entire explanation you can directly go to this link: Udacity - Uniform Cost Search.In this answer I have explained what a frontier is. To put it in simple words, you can describe the UCS algorithm as 'expanding the frontier only in the direction which will require the minimum cost to travel from initial point among all possible expansions, i.e. adding a point on the graph (which can be reached from the frontier without going through any other point) that has the shortest route from the initial point. We keep on doing this until a path has explored the goal frontier: this path is the cheapest path from the initial point.I strongly suggest you check out both the links for examples and a better understanding."
How to use Genetic Algorithm for varying lengths of solutions,"
Until now, I always thought that Genetic Algorithm can be used for problems of which the solution space can be encoded (modeled) as a chromosome of a specific length. However, some people claim that they used GA for this game and this game. They are basically games in which we control an agent on a 2-dimensional area. 
Obviously, the length of the genome sequence depends on how fast the game is finished. So, how is GA used for such games?
If you think GA is not the most suitable method for this kind of problems can you explain why and give better alternatives?
","['game-ai', 'genetic-algorithms']",
Can AutoKeras be used for neural networks of PyTorch,"
I use PyTorch, bauces AllenNLP is built on it and good libraries are for it. But can AutoKeras be used for PyTorch based ML pipelines, or am I required to switch to Keras? Google is quite silent when asked for tutorials for this combination. Maybe PyTorch is lacking automated ML framework?
","['neural-networks', 'machine-learning', 'automation']",
Was Pierce way off the mark? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 1 year ago.







                        Improve this question
                    




Funding artificial intelligence is real stupidity.
-- John R. Pierce

Was this computer pioneer way off the mark? – or was there an important sub-text there?
Pierce was an expert on machine translation in the 1960s. He coauthored the following paper Pierce, John R., and John B. Carroll. ""Language and machines: Computers in translation and linguistics."" (1966). That means, he worked on the domain of AI but explained his subject as useless. Perhaps Marvin Minsky, Rodney Brooks and Sebastian Thrun would agree to him?
",['history'],"John R. Pierce led the Bell Labs research team that created the first transistor and gave it its name.  He was later the Chief Engineer at the Jet Propulsion Laboratory at CalTech.His relationship to artificial intelligence research was mainly in regard to language translation.  He wrote the following.1The computer has opened up to linguists a host of challenges, partial insights, and potentialities. ... Certainly, language is second to no phenomenon in importance. ... The new linguistics presents an attractive as well as an extremely important challenge. ... The most reasonable government source of support for research in computational linguistics is the National Science Foundation. ... We estimate that work on a reasonably large scale ... would be justified at ... an annual expenditure of \$2.5 to \$3 million.No official record of Pierce stating unequivocally that all artificial intelligence funding is stupid can be found.  Nils J. Nilsson's The Quest for Artificial Intelligence quotes Donald Knuth stating this.John R. Pierce, whom I have already mentioned in connection with the ALPAC report on machine translation (in section 7.2) and his negative comments about speech understanding (p 222) wrote me a very short letter
  in which he stated,Concerning artificial intelligence, I believe I invented the slogan, ""Artificial intelligence is real stupidity.""...I resent artificial intelligence because I feel that it is unfair to computers. But then, artificial intelligence people did devise LISP, which is pretty good.The letter did not elaborate on the slogan or why AI is ""unfair to computers.""The paragraphs that may have drawn the most negative attention (and possibly fabricated quips) are these.2It would be too simple to say that work in speech recognition is carried out simply because one can get money for it.  That is a necessary but not a sufficient condition.  We are safe in asserting that speech recognition is attractive to money.  The attraction is perhaps similar to the attraction of schemes for turning water into gasoline, extracting gold from the sea, curing cancer, or going to the moon.  One doesn't attract thoughtlessly given dollars by means of schemes for cutting the cost of soap by 10%.  To sell suckers, one uses deceit and offers glamor.It is clear that glamor and any deceit in the field of speech recognition blind the takers of funds as much as they blind the givers of funds.  Thus, we may pity workers whom we cannot respect.  People who work in the field are full of innocent (in their own view) enthusiasm.In hindsight, we can see a few things Pierce called correctly.Whether Pierce meant that artificial intelligence is real stupidity literally or whether he was referring to the unrealistic expectations planted by researchers at MIT to obtain defense funding after the launch of Sputnik, we may never know for sure, but is easy to guess.The comment about LISP is an obvious reference to Marvin Minsky's lab, which allegedly did not produce for the U.S. government what was promised.  It is merely an allegation because research failures can be fabrication to assist in classifying publicly funded research.I say this as one who spent time with those in the closely linked AI lab at the United Technologies Research Center.  Everyone had national security clearances, and we all saw fabricated successes and fabricated failures.  There is good reason for mixing in misinformation when espionage is a known phenomenon.Based on the way Pierce commonly wrote, the unfairness to computers was likely a satirical comment about research funding pitches and the unrealistic expectation they placed on the speed of computer technology development.  He is certainly not a person that would anthropomorphize a computer witlessly.Pierce was given the leading role at the JPL because he gained credibility among fiscal stakeholders by telling the truth while at Bell Labs.  Some of the futurists in leading technology roles today might benefit from that history.References[1] Language and Machines, Pierce, 1966, ALPAC report to the U.S. National Science Foundation[2] Whither Speech Recognition?, Pierce, 1969, Journal of the Acoustical Society of America, Vol. 46, No. 4"
Price Movement Forecasting Issue,"
I am working on a project for price movement forecasting and I am stuck with poor quality predictions.
At every time-step I am using an LSTM to predict the next 10 time-steps. The input is the sequence of the last 45-60 observations. I tested several different ideas, but they all seems to give similar results. The model is trained to minimize MSE.
For each idea I tried a model predicting 1 step at a time where each prediction is fed back as an input for the next prediction, and a model directly predicting the next 10 steps(multiple outputs). For each idea I also tried using as input just the moving average of the previous prices, and extending the input to input the order book at those time-steps.
Each time-step corresponds to a second.
These are the results so far:
1- The first attempt was using as input the moving average of the last N steps, and predict the moving average of the next 10.
At time t, I use the ground truth value of the price and use the model to predict t+1....t+10
This is the result:
Predicting moving average:

On closer inspection we can see what's going wrong:
Prediction seems to be a flat line. Does not care much about the input data:


The second attempt was trying to predict differences, instead of simply the price movement. The input this time instead of simply being X[t] (where X is my input matrix) would be X[t]-X[t-1].
This did not really help.
The plot this time looks like this:

Predicting differences:

But on close inspection, when plotting the differences, the predictions are always basically 0.

At this point, I am stuck here and running our of ideas to try. I was hoping someone with more experience in this type of data could point me in the right direction.
Am I using the right objective to train the model? Are there any details when dealing with this type of data that I am missing?
Are there any ""tricks"" to prevent your model from always predicting similar values to what it last saw? (They do incur in low error, but they become meaningless at that point).
At least just a hint on where to dig for further info would be highly appreciated.
UPDATE
Here is my config
{
    ""data"": {
        ""sequence_length"":30,
        ""train_test_split"": 0.85,
        ""normalise"": false,
        ""num_steps"": 5
    },
    ""training"": {
        ""epochs"":200,
        ""batch_size"": 64
    },
    ""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""input_timesteps"": 30,
                ""input_dim"": 101,
                ""return_seq"": true,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""activation"": ""relu"",
                ""return_seq"": false
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""linear""
            }
        ]
    }
}

Notice the last layer with 101 neurons. It is not an error. We just want to predict the features as well as the price. In other words, we want to predict the price for time t+1 and use the features predicted to predict the price and new features at time t+2, ...
","['neural-networks', 'long-short-term-memory', 'dropout']",
"In transfer learning, should we apply standardization if the pre-trained model was (or not) trained with standardised data?","
Assume one is using transfer learning via a model which was trained on ImageNet.

Assume that the pre-processing, which was used to achieve the pre-trained model, contained z-score standardization using some mean and std, which was calculated on the training data.
Should one apply the same transformation to their new data? Should they apply z-score standardization using a mean and std of their own training data?

Assume that the pre-processing now did not contain any standardization.
Should one apply no standardization on their new data as well? Or should one apply the z-score standardization, using the mean and std of their new data, and expect better results?


For example, I've seen that the Inception V3 model, which was trained by Keras, did not use any standardization, and I'm wondering if using z-score standardization on my new data could yield better results.
","['neural-networks', 'training', 'data-preprocessing', 'transfer-learning', 'standardisation']",
Reason for issues with correlation in the dataset in DQN,"
From the paper Human level Control through DeepRL,  the correlation in the data causes instability in the network and may causes the network to diverge. I wanted to understand what does this instability and divergence mean ? And why correlated data causes this instability.
","['q-learning', 'dqn']","I wanted to understand what does this instability and divergence mean ?These are with reference to learning curves for the neural network. If a neural network is stable and converges, it means that the value of the error or cost function reduces consistently over time and reaches a stable point of minimum error. In practice this is often a noisy process and not smooth. There are degrees of noise that are expected and acceptable when solving real problems. However, an instable learning curve will oscillate wildly, and a divergent learning curve will get consistently worse error function during training.A typical stable, converging learning curve, cost function versus training data consumed, might look like this:Whilst an instable, diverging learning curve might look like this:These are not using the same vertical scale - the lowest point of the unstable curve will typically be higher than most or even all of the stable curve. And why correlated data causes this instability.This is because for gradient descent to work, it needs gradient samples that it uses on every weight update step to be unbiased estimates of a true gradient. In RL you have either an online learning process or non-stationary targets (and usually both), so you must use stochastic or mini-batch gradient descent, working with a few samples at a time. You need those samples to be independent, not related to each other other than by random chance, otherwise the gradient value will be biased and gradient descent will consistently make updates in the wrong overall direction.A good way to illustrate the difference is to use a really simple example, using gradient descent updates to estimate a mean value (this is roughly equivalent to training a neural network with one neuron, with weight fixed to zero and learning a bias value to represent the mean of the target- no actual input is required).Say we have an array of values from 0 to 200 inclusive (example code is in Python):If this array is kept sorted, then sequential values are highly correlated. If you plot consecutive pairs against each other, you will get a straight line.We can estimate the mean value by setting a ""bias"" value to some arbitrary number, and running an update rule based on MSE (between current bias and observed value):This prints (roughly) $191$ as the estimate for the mean, almost double.However, if we shuffle the array first, it removes the correlation. If you plot consecutive pairs against each other, you will get a scatter graph with no apparent pattern. Adding the one line np.random.shuffle(x) to do this changes the results radically:We get much better estimates (typically between $90$ and $110$), closer to the true value of $100$, and not biased to be higher or lower (run it enough times and you would find the expected result of this algorithm is very close to the true value).This is because in the first version of the code, the gradient of the error was not sampled fairly - it kept pointing ""up"" even for high estimates due to the correlation. In the shuffled version, gradients are likely to be in either direction depending only on the current estimate, and will appear roughly in the ratios necessary to find the correct value.As an exercise you could extend this simple example with mini-batches and repeated ""epochs"" to show that the effect persists with those changes, and the shuffling is the most important change here for better estimates. "
When should the iterative deepening search and the depth-limited search be used?,"
When should the iterative deepening search (IDS), also called iterative deepening depth-first search (IDDFS), and the depth-limited search be used?
","['applications', 'search', 'iddfs']",
What is the mapping between actions and numbers in OpenAI's gym? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






In a gym environment, the action space is often a discrete space, where each action is labeled by an integer. I cannot find a way to figure out the correspondence between action and number. For example, in frozen lake, the agent can move Up, Down, Left or Right. The code:
import gym
env = gym.make(""FrozenLake-v0"")
env.action_space

returns Discrete(4), showing that four actions are available. If I env.step(0), which direction is my agent moving?
","['reinforcement-learning', 'gym']","There is no way to tell via the Gym API, and to any RL-based learning agent this is entirely unimportant, discrete actions are just arbitrary labels, and their effects are learned by trial and error in terms of reward and changes to state.If you need to know, you can read the code for each environment. Often it is described in a comment, or as constants.E.g. in FrozenLake-v0, you can find this code:If I env.step(0), which direction is my agent moving?Left, according to those constant names at least."
Can current AI techniques distinguish a fake old paper from a real one?,"
It is an easy matter to make a paper look old, for example, using any of the techniques explained on this page of WikiHow: https://www.wikihow.com/Make-Paper-Look-Old.
Is current AI sufficient to distinguish a fake old paper from a real one?
","['machine-learning', 'classification', 'applications']",
Problem extracting features from convolutional layer where the dimensions are big for feature maps,"
I have trained a convolutional neural network on images to detect emotions. Now I need to use the same network to extract features from the images and use them to train an LSTM. The problem is: the dimensions of the top layers are: [None, 4, 4, 512] or [None, 4, 4, 1024]. Therefore, extracting features from this layer will result in a 4 x 4 x 512 = 8192 or 4 x 4 x 1024 = 16384 dimensional vector for each image. Clearly, this is not what I want.
Therefore, I would like to know what to do in this case and how to extract features that are of reasonable size. Should I apply global average pooling to the activation or what?
Any help is much appreciated!
","['convolutional-neural-networks', 'long-short-term-memory', 'feature-selection', 'dimensionality']",
What is a learning agent?,"
What is a learning agent, and how does it work? What are examples of learning agents (e.g., in the field of robotics)?
","['terminology', 'definitions', 'intelligent-agent', 'learning-agents']",
3D environment for RL research in Academia,"
I'm doing my thesis on Reinforcement Learning. My focus on Partially Observable Environments like 3D Games. I want to choose a 3D platform for testing and doing research. 
I know some of them. DeepMind Lab and OpenAi Universe. But my question is that which of these environments is good for me? Is there any environment for this purpose that is benchmark and reliable?
I want a platform that accepted in Academia and reliable. For example DeepMind is not a standard or Open Source friendly, Is it rational to use their platform for research in academia?
What i have to do?
","['machine-learning', 'reinforcement-learning', 'research', 'deepmind', 'open-ai']",
What is the difference between a receptive field and a feature map?,"
In a CNN, the receptive field is the portion of the image used to compute the filter's output. But one filter's output (which is also called a ""feature map"") is the next filter's input.
What's the difference between a receptive field and a feature map?
","['convolutional-neural-networks', 'comparison', 'terminology', 'feature-maps', 'receptive-field']",
Ensemble models - XGboost,"
I am building 2 models using XGboost, one with x number of parameters and the other with y number of parameters of the data set.
It is a classification problem.
A yes-yes, no-no case is easy, but what should I do when one model predicts a yes and the other model predicts a no ?
Model A with x parameters has an accuracy of 82% and model B with y parameters has accuracy of 79%.  
",['data-science'],
How much can the addition of new features improve the performance?,"
How much can the addition of new features improve the performance of the model during the optimization process?
Let's say I have a total of 10 features. Suppose I start the optimisation process using only 3 features.
Can the addition of the 7 remaining ones improve the performance of the model (which, you can assume, might already be quite high)?
","['machine-learning', 'deep-learning', 'optimization', 'feature-selection']",
How to perform PCA in the validation/test set?,"
I was using PCA on my whole dataset (and, after that, I would split it into training, validation, and test datasets). However, after a little bit of research, I found out that this is the wrong way to do it.
I have few questions:

Are there some articles/references that explain why is the wrong way?

How can I transform the validation/test set?


Steps to do PCA (from https://www.sciencedirect.com/science/article/pii/S0022460X0093390X):

zero mean

$$\mu = \frac{1}{M}\sum_{i=1}^{M} x_{i}$$
where x is my training set

centering (variance)

$$S^{2} = \frac{1}{M}\sum_{i=1}^{M} (x_{i}-\mu)^{T}(x_{i}-\mu)$$

use (1) and (2) to transform my original training dataset

$$x_{new} = \frac{1}{\sqrt{M}} \frac{(x_{i} - \mu)}{S}$$

calculate covariance matrix (actually correlation matrix)

$$C= x_{new}^T x_{new}$$

take the k-eigenvectors (/phi) from the covariance matrix and defined the new space for my new dimension training set (where k are the principal components that I choose according to my variance)

$$ x_{new dim} = x_{new}\phi$$
Ok, then I have my new dimensional training dataset after PCA (till here it's right, according to other papers that I have read).
The question is: *What I have to do now for my validation/testing set? Just the equation below?
$$y_{new dim} = y\phi $$
where y is my (for example) validation original dataset?
Can someone explain the right thing to do?
","['machine-learning', 'principal-component-analysis', 'test-datasets', 'validation-datasets']","for the first point I'm very sorry that I cannot give you any literature on this, but I might be able to explain you, why you don't take PCA on both datasets independently.Principal components analysis is simply a transformation of your data into another (less dimensional) coordinate system. The axis for your new coordinate system are defined by the principal components (i.e. eigen-vectors) of your covariance matrix.Since you will train your machine learning algorithm in the domain that is generated by the PCA, your test data must be exactly in the same domain. So as you said, you use exactly the same transformation for the test data as for the training data, i.e.
$y_{newdim} = y \phi $.
Of course if you applied standardization to your training data, you have to apply the same standardization to your test data. So you need to store the mean $\mu_x$ and the standard deviation $S_x$ and also standardize your test data y to 
\begin{equation}
y_{standardized} = \dfrac{y_i - \mu_x}{S_x}
\end{equation}Note here that you have an error in your standardization formula (you do not need to divide by $M$).The point is that the principal components of your test data $\phi_{y}$ would not match the principal components of your training data $\phi_{x}$. Thus the transformations from original space into PCA-space $\Phi_x(u)$ and $\Phi_y(u)$ would diverge and similar data points in the original space might be far away in the PCA-representation and vice versa. This is why you generate the mapping $x_{newdim} = \Phi_x(x) = x\phi$ and apply it also on the test data.I hope I could make it clear to you.Best"
"Is my understanding of the differences between MDP, Semi MDP and POMDP correct?","
I just wanted to confirm that my understanding of the different Markov Decision Processes are correct, because they are the fundamentals of reinforcement learning. Also, I read a few literature sources, and some are not consistent with each other. What makes the most sense to me is listed below.
Markov Decision Process
All the states of the environment are known, so the agent has all the information required to make the optimal decision in every state.  We can basically assume that the current state has all information about the current state and all the previous states (i.e., the Markov property)
Semi Markov Decision Process
The agent has enough information to make decisions based on the current state.  However, the actions of the agent may take a long time to complete, and may not be completed in the next time step. Therefore, the feedback and learning portion should wait until the action is completed before being evaluated.  Because the action takes many time steps, ""mini rewards"" obtained from those time steps should also be summed up.
Example: Boiling water

State 1: water is at 23 °C
Action 1: agent sets the stovetop at 200 °C
Reward (30 seconds after, when water started to boil):

+1 for the fact that water boiled in the end, but -0.1 reward for each second it took for the water to start boiling.
So, the total reward was -1.9 (-2.9 because the water did not boil for 29 seconds, then +1 for water boiling on 30th second)



Partially Observable Markov Decision Process
The agent does not have all information regarding the current state, and has only an observation, which is a subset of all the information in a given state.  Therefore, it is impossible for the agent to truly behave optimally because of a lack of information.  One way to solve this is to use belief states, or to use RNNs to try to remember previous states to make better judgements on future actions (i.e., we may need states from the previous 10 time steps to know exactly what's going on currently).
Example
We are in a room that is pitch black.  At any time instant, we do not know exactly where we are, and if we only take our current state, we would have no idea.  However, if we remember that we took 3 steps forward already, we have a much better idea of where we are.
Are my above explanations correct?  And if so, isn't it possible to also have Partially Observable Semi Markov Decision Processes?
","['reinforcement-learning', 'comparison', 'markov-decision-process', 'pomdp', 'semi-mdp']",
Are there any pretrained models for human recognition from all angles?,"
I need to be able to detect and track humans from all angles, especially above.
There are, obviously, quite a few well-studied models for human detection and tracking, usually as part of general-purpose object detection, but I haven't been able to find any information that explicitly works for tracking humans from above.
","['object-recognition', 'models']",
How to define a reward function in POMDPs?,"
How do I define a reward function for my POMDP model?
In the literature, it is common to use one simple number as a reward, but I am not sure if this is really how you define a function. Because this way you have to do define a reward for every possible action-state combination. I think that the examples in the literature might not be practical in reality, but only for the purpose of explanation.
","['reinforcement-learning', 'reward-design', 'reward-functions', 'pomdp']","There is no major difference here between a POMDP and MDP. When setting reward values, you are generally trying to give the minimal information to the agent that when the sum of rewards is maximised, it solves the problem that you are posing. In literature it is common to use one simple number as a reward, but I am not sure if this is really how you define a function. Because this way you have do define a reward for every possible Action-State combination.Some defined value of reward has to be returned after all state, action pairs taken in the environment. The value could be $0$ of course. Reward can depend on, or be a function of, current state, action, next state and a random factor. In a POMDP it may also be from any unobserved factor in the environment (you might know this in a simulation because you have created the environment and are choosing not to share the data with the agent).In practice, the reward often does not have to depend on all the possible factors. In addition, the relationship between the factors and the possible reward can be very simple or sparse. Classic examples you may find in the literature include:Reward in a game can be simply $+1$ for winning, or $-1$ for losing, granted at the end. All other rewards are $0$If your goal is to reach a certain state, such as exit from a maze, in minimum time, then a fixed reward of $-1$ per time step is enough to express the need to minimize the total number of steps.For a goal of maintaining stability and avoiding a failure in e.g. CartPole, then a reward of +1 per time step without failure suffices.All of these examples express the reward function as a simple condition plus one or two numbers. The key thing is that they allow you to calculate a suitable reward on each and every time step. They are all strictly reward functions - the general case can cover very complex functions if you wish, that will depend entirely on the goals you want to set the agent to learn/solve."
Using features extracted from a CNN as convolutional filter,"
I'm a bit confused about this. Assume I have a CNN network with two branches:

Top
Bottom

The top branch outputs a feature vector of shape 1x1x1x10 (batch, h, w, c)
The bottom branch outputs a feature vector of shape (1, 10, 10, 10).
I want to use the top feature vector as a convolutional filter, and convolve it with the bottom feature vector. I can do this in pytorch with the ""functional.Conv2D"" function, the problem is, I don't know how back-prop works in this case (will it be unstable?) since the output feature is a now a parameter as well, do I need to stop gradients or do something else in this case to backprop correctly?
","['convolutional-neural-networks', 'backpropagation']",
How to design the reward for an action which is the only legal action at some state,"
I am working on a RL project,but got stuck at one point: The task is continuous (Non-episodic). Following some suggestion from Sutton's RL book, I am using a value function approximation method with average reward (differential return instead of discount return). For some state (represented by some features), only one action is legal. I am not sure how to design reward for such action. Is it ok to just assign the reward in the previous step? Could anyone tell me the best way to decide the reward for the only legal action. Thank you! 
UPDATE:
To give more details, I added one simplified example:
Let me explain this by a simplified example: the state space consists of a job queue with fix size and a single server. The queue state is represented by the duration of jobs and the server state is represented by the time left to finish the current running job.  When the queue is not full and the server is idle, the agent can SCHEDULE a job to server for execution and see a state transition(taking next job into queue) or the agent can TAKE NEXT JOB into queue. But when the job queue is full and server is still running a job, the agent can do nothing except take a BLOCKING action and witness a state transit (time left to finish running job gets decreased by one unit time). The BLOCKING action is the only action that the agent can take in that state.
",['reinforcement-learning'],
Reinforcement learning objective as conditional expectations,"
In one of his lectures Levine describes the objective of reinforcement learning as: $$J(\tau) = E_{\tau\sim p_\theta(\tau)}[r(\tau)]$$
where $\tau$ refers to a single trajectory and $p_\theta(\tau)$ is the probability of having taken that trajectory so that $p_\theta(\tau) = p(s_1)\prod_{t = I}^T \pi_{\theta}(a_t, s_t)p(s_{t+1}|s_t, a_t))$.
Starting from this definition, he writes the objective as $J(\tau) =\sum_{t=1}^T E_{(s_t, a_t)\sim p_\theta(\tau)}[r(s_t, a_t)]$ and argues that this sum can be decomposed by using conditional expectations, so that it becomes:
$$J(\tau) = E_{s_1 \sim p(s_1)}[E_{a_1 \sim \pi(a_1|s_1)}[r(s_1, a_1) + E_{s_2 \sim p(s_2|s_1, a_1)}[E_{a_2 \sim \pi(a_2|s_2)}[r(s_2, a_2)] + ...|s_2]|s_1,a_1]|s_1]]$$
Can anyone explain this last step? I guess the law of total expectation is involved, but I can not figure out how exactly.
","['machine-learning', 'reinforcement-learning', 'statistical-ai']","I tried to sketch a mathematical justification of the equality. So we have:$$J(\tau) = E_{\tau\sim p_\theta(\tau)}[r(\tau)]$$ where $p_\theta(\tau) = p(s_1)\prod_{t = 1}^T \pi_{\theta}(a_t, s_t)p(s_{t+1}|s_t, a_t))$.  Now, $p_\theta(\tau)$ can be re-written in terms of the Markov Chain transition probabilities, namely: 
$p_\theta(\tau)= \prod_{t = 1}^T \pi_{\theta}(a_t, s_t)p(s_{t}|s_{t-1}, a_{t-1})) = \prod_{t = 1}^T p(s_{t}, a_{t})$.Here we focus on T = 2 (I guess it is not difficult to prove with a general T, maybe by induction over T). The following holds:
$$J(\tau) = E_{\tau\sim p_\theta(\tau)}[r(\tau)] = E_{(s_1, a_1, s_2, a_2)\sim p(s_1,a_1)p(s_2,a_2)}[r(s_1,a_1) + r(s_2,a_2)] \\ 
= \int_{(s_1,a_1)} \int_{(s_2,a_2)} (r(s_1,a_1) + r(s_2,a_2))p(s_1,a_1)p(s_2,a_2) d(s_1,a_1)d(s_2, a_2) \\
= \int_{(s_1,a_1)} \left( \int_{(s_2,a_2)} (r(s_1,a_1)  + r(s_2,a_2))p(s_2,a_2) d(s_2,a_2)\right)p(s_1,a_1)d(s_1, a_1) \\ 
= E_{(s_1, a_1)\sim p(s_1,a_1)}\left[E_{(s_2, a_2)\sim p(s_2,a_2)}[r(s_1,a_1) + r(s_2,a_2)]\right] \\
= E_{(s_1, a_1)\sim p(s_1,a_1)} \left[E_{(s_2, a_2)\sim p(s_2,a_2)}[r(s_1,a_1)] + E_{(s_2, a_2)\sim p(s_2,a_2)}[r(s_2,a_2)] \right] \\
= E_{(s_1, a_1)\sim p(s_1,a_1)} \left[ r(s_1,a_1) + E_{(s_2, a_2)\sim p(s_2,a_2)}[r(s_2,a_2)] \right] $$where the second line is due to the definition of the expectation, in the third line we just changed the order of the terms, in the fourth we used the definition of expectation, in the fifth we used the linearity property of the expectation and the last line is because $r(s_1, a_1)$ is constant with respect to the integration over $(s_2, a_2)$. I preferred to move to the integral form of the expectation in line two and three because it was not clear to me how exactly the expectation could factorise.To conclude the justification, note that we defined $p(s_t, a_t) = \pi_{\theta}(a_t, s_t)p(s_{t}|s_{t-1}, a_{t-1})$, so we have another product here which can be factorised in the same way we did in line two and three of the set of equations above. This brings us the to conditional expectation form of the objective. In general, every term we have in the original trajectory distribution $p_\theta(\tau)$ corresponds to an expectation and every reward can be moved up to the first outer expectation which it depends on."
What is the difference between goal-based and utility-based agents?,"
What is the difference between goal-based and utility-based agents? Please, provide a real-world example.
","['comparison', 'intelligent-agent', 'goal-based-agents', 'utility-based-agents']","Utility is a fundamental to Artificial Intelligence because it is the means by which we evaluate an agent's performance in relation to a problem. To distinguish between the concept of economic utility and utility-based computing functions, the term ""performance measure"" is utilized.  The simplest way to distinguish between a goal-based agent and a utility-based agent is that a goal is specifically defined, where maximization of utility is general.  (Maximizing utility is itself a form of goal, but generalized as opposed to specific.)A goal-based navigation agent is tasked with getting from point A to point B.  If the agent succeeds, the goal has been satisfied. A utility-based navigation agent could seek to get from point A to point B in the shortest amount of time, with the minimum expenditure of fuel, or both.In the above example, the utility agent is also goal based, but where the performance measure for the goal agent is a binary [succeed/fail], the utility agent can use real numbers and measure performance by degree. The utility agent allows more granularity in evaluation.For an example of a non-goal based utility agent consider a form of a partisan sudoku in which players compete to control regions on the gameboard by placement of weighted integers.In a game with 9 regions, the goal based agent seeks to control a specific number of regions at the end of play.  If the agent is conservative, the goal might be 5 regions. If the agent is hyper-aggressive, the goal might be 9 regions.  When evaluating the environment (gameboard), if the agent dominates the desired number of regions, it could choose to consolidate (reinforce); if the agent does not dominate the desired number of regions, it could choose to expand (attack).The above strategy can be effective, but is limited by the specificity of the goal.  A hyper-aggressive goal would work well against a weak opponent, but against a strong opponent it might prove disastrous.  If the agent is sophisticated, where performance has been poor, it might alter it's goal by switching to a ""turtling strategy"" and seek to control fewer regions, but, because the new goal is still specific, the agent may miss opportunities to improve it's final status beyond the adjusted goal.The utility-based agent can approach the game with no specific goal beyond improving it's status. Rather than seeking to control a set number of regions, the utility-agent evaluates whether a given choice improves or worsens it's status. (""Do I dominate more or less regions if I take this position?"") The utility agent can distinguish between sets of beneficial choices (""which choice maximizes my expected benefit?"") and, where no benefit can be obtained, distinguish among the set of choices with the least downside (""among the set of bad choices, which is the least bad choice?"") In this example, the utility-agent doesn't even need to understand the victory condition (controlling the more regions than the opponent at the end of play.)  Instead, the utility-agent merely seeks to maximize the number of controlled regions over the course of play, which will result in victory if the agent makes more optimal choices than the opponent. "
Is there a limit of minimum error for a particular training dataset in artificial Neural Network?,"
In error-based learning using gradient descent, if I give you a training dataset, then can you find the minimum error after training? And the minimum error should be true for all architectures of a neural network? Consider you will use MSE for calculating the error. You can choose anything you want other than my specified condition. It's like no matter how you change your network you can never cross the limit.
","['neural-networks', 'math', 'proofs']",
How can I use Generative Adversarial Networks to solve the imbalanced class problem?,"
Problem setting
We have to do a binary classification of data given a training dataset $D$, where most items belong to class $A$ and some items belong to class $B$, so the classes are heavily imbalanced.
Approach
We wanted to use a GAN to produce more samples of class $B$, so that our final classification model has a nearly balanced set to train.
Problem
Let's say that the data from both classes $A$ and $B$ are very similar. Given that we want to produce synthetic data with class $B$ with the GAN, we feed real $B$ samples that we have into the discriminator alongside with generated samples. However, $A$ and $B$ are similar. It could happen that the generator produces an item $x$, that would naturally belong to class $A$. But since the discriminator has never seen class-$A$ items before and both classes are very close, the discriminator could say that this item $x$ is part of the original data that was fed into the discriminator. So, the generator successfully fooled the discriminator in believing that an item $x$ is part of the original data of class $B$, while $x$ is actually part of class $A$.
If the GAN keeps producing items like this, the produced data is useless, since it would add heavy noise to the original data, if combined.
At the same time, let's say before we start training the generator, we show the discriminator our classes $A$ and $B$ samples while giving information, that the class-$A$ items are not part of class $B$ (through backprop). The discriminator would learn to reject class-$A$ items that are fed to it. But wouldn't this mean that the discriminator has just become the classification model we wanted to build in the first place to distinguish between class $A$ and class $B$?
Do you know any solution to the above-stated problem or can you refer to some paper/other posts on this?
","['neural-networks', 'generative-adversarial-networks', 'binary-classification', 'imbalanced-datasets']","In my experience, GANs work really well for the scenario of semi-supervised learning, where you don't necessarily have labels for all your class $B$ data, but you do have a balanced dataset. In my (limited) experience, you do have to have a balanced (in numbers) set of $A$ and $B$ objects, even though you are not sure of the labels.And yes, GANs can overfit to outliers as well, especially in the absence of a lot of examples, so be cautious.Currently, the version that works best for me (in terms of GANs) is WGAN-GP or WGAN-LP in combination with Optimistic Mirror Descent Adam (here, Ncritic/Nactor = 1). Take a look at the paper by Adiwardana et al., especially Fig. 7 (astonishing!), for semi-supervised learning with a limited number of class labels."
What is the difference between learning and non-learning agents?,"
What is the difference between learning agents and other types of agents?
In what ways learning agents can be applied? Do learning agents differ from deep learning?
","['deep-learning', 'comparison', 'intelligent-agent', 'learning-agents']","The key difference between a learning agent and non-learning agents is that the learning agent can improve it's performance on it's own, allowing it to get ""smarter"".  Russel & Norvig cover the different types of intelligent agents in detail in their textbook Artificial Intelligence: A Modern Approach, and the wikipedia entry for intelligent agent mirrors their definitions. [Free pdf of the 3rd edition here.]They break down agents into the following categories:I'm not going to go into depth because understanding of technical terminology is required, but the links in the above list provide simple flow-chart explanations.In the first 4, the more sophisticated agents contain the primitive agents.  The model-based agent is actually a ""model-based reflex agent"", and the goal and utility agents are also model-based.You will notice is that the structure changes fundamentally with the learning agent.The key components of the learning agent are categorized as the ""Critic"", the ""Learning Element"", the ""Performance Element"", and ""Problem Generator"".  This structure allows the agent to evaluate past performance and modify subsequent actions to improve performance (learning).  The problem generator allows the agent to seek novel strategies (experimentation).Where the simple reflex agent operates under a ""condition-action rule"" (if a condition is met, take this action), the learning agent can improve it's decision-making beyond it's initial programming.Deep learning is a form of machine learning that uses representations. In the most general sense, the algorithms use a trial and error and statistical analysis to optimize performance.  Various forms of neural networks are classified as deep learning systems. [See also Deep Reinforcement Learning vs. Reinforcement Learning.] My sense is that learning agents can be applied to any type of problem in which parameters can be sufficiently defined.  I don't want to get too specific because there are many types of learning algorithms, but we've been seeing application of machine learning (in various forms) expanding significantly since ~2016.  This includes replacing classical control systems, medical diagnostics, natural language processing, image recognition, and consumer recommendations. The applications are initially ""narrow"", defined as capability at a single task, but can be aggregated for tasks such as self-navigating vehicles which involve sets of discrete problems.  "
How to combine input from different types of data sources?,"
I've to train a neural network using microphone data (wav files), accelerometer sensor data and light sensor data. 
Right now the approach I thought was to convert all data into images and combine them into a single image and train my neural network.
Another approach was to convert wav files into arrays and combine them along with sensor data and train my neural net.
Are my approaches correct or is there a better way to do this?
Any suggestions/ideas are welcome.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'ai-design', 'audio-processing']","Handling Multiple Input TypesMultiple input types are common as learning technology moves from academic labs and open source examples into the real world.  The design process includes the below four steps, which related directly to this question.  There are other steps that don't centrally related not mentioned here.First Step — List Inputs in Most Usable FormIn the case of this question, there are three in the list.Fixed sample rate is listed with each type.  Variable sample rates pose additional burden on learning.  If any of the three are acquired with a variable sample rate, they can be converted to fixed sample rate arrays provided there is a time stamp or a time-since-last-sample channel available with each sample vector.Variable to fixed sample rate conversion can be accomplished easily via ffmpeg, programmatically via its library, or through any equivalent software designed for this purpose.  The acceleration and visible light signals can also be adapted for ffmpeg's input.When a specific dynamic characteristic must be preserved, finer control over the interpolation can be obtained using cubic spline algorithms.  Such algorithms were originally written in FORTRAN or C and have been ported to Java and Python.Second Step — When Spectral Data is Practically RequiredWith time series data, the phenomena being learned is often largely a function of vibration frequencies and amplitudes.  This is almost always the case with audio data.  The exception is when transient detection is the primary teller of the conditions being measured rather than more continuous sounds.A note is spectral in description.  A knock is a transient and both spectra and actual rise characteristics are indicative of source.Presenting digital samples from the transducer circuit (in this case the microphone diaphragm's position in response to pressure waves) to the learning system directly is not a best practice.  Unconverted vibration information forces the learning system to learn transformations for which optimized algorithms already exist.  This slows learning and generally pushes complexity beyond the learning system's capacity.When the spectral features best characterize the phenomena being learned, as in hums, screeches, vowel sounds, music, horns, or beeps, the best practice is to convert the samples in the time domain to frequency domain data.This is usually done using one of the forms of the FFT (fast Fourier transform) algorithm in combination with a RMS (root mean square) calculation of amplitude and windowing.  Windowing attenuation forms, such as the Hamming window or the cosine window, are available in most packages that have FFT capabilities.  The sample rate is also important.  Nyquist's sample rate criteria must be met by the A-to-D converter (in the sound card or instrumentation circuitry) to ensure that the spectral range includes the best indicators of the phenomena to be learned.  In machine learning, such indicators are called features.In most cases, audio or vibration information is presented to a learning system or sub-system as spectral frequencies and amplitudes.  Vibration recognition systems and speech to text systems generally execute FFTs (with RMS and windowing) on the raw input.  In human ears, the conversion to spectral information is mechanical, using the tapering geometry of the ear's cochlea.There also exists algorithms that can directly develop spectral information from variable sample rate data to amplitudes and frequencies, combining two transformations into one for efficiency of computing resources and additional input accuracy.In summary of this section, whether light, IR, acceleration, or other data is treated as audio and converted to spectral information is based on whetherThird Step — When Topology of TypesThe topological orientation of the input source types to one another is key to feeding the information into any learning system.  These are the two pure extremes.In the case of (1), a normalized sample from one type of input channel is, from a learning point of view, indistinguishable from a normalized sample from another.  This would be the case if the accelerometer is measuring the same vibration that the microphone is measuring.  This is the less usual case, but is more common when the data is for study of vibration to project mechanical or materials failures in advance.With Background Clarified, the Q&A Becomes ClearerThe question asked this.How [does one best] combine input from different types of data sources?Whether the source data is case (1) or case (2) or some midpoint between them is key to answering this question.  Samples from different source types that measure single phenomena can be phase adjusted, spectrally adjusted, and otherwise normalized to present a single data type at the input.  Merging into a single serial data stream would be the optimal pre-processing for training in case (1).More often, the topological orientation of the different input source types conforms to case (2) above.  In that case, different input types should be presented in parallel to the learning system inputs.  The merging would be counterproductive in this case, for a reason that will be clarified in the next section below.To avoid sparsity in input data, which creates additional training burden, the sample rates should be equalized for all channels of all data source types.  This may require conversion of the sample rate, which can be done using the same techniques (above) for converting from variable to fixed sample rates.Images of DataThe question stated this.Right now, the approach I [was considering is] to convert all data into images and combine them into a single image and train my neural network.The attempt to normalize and combine into one input stream is the correct approach if (1) above is the case.  However, presenting data as images requires the learning system restore the data to a less redundant, more feature encoded form, so conversion to images is not recommended.If (2) above is the case, the provision of unique and property-independent data through the same input tensors requires the artificial network sort the input samples by type, which can be done with recurrent or attention based artificial networks, but it is unnecessary.  Here's the rule.If the signals from different sources are measuring independent properties to which learning will be applied, do not merge the input streams.  Instead, present them in parallel to the learning system so that the learning system does not need to also learn how to discriminate sample types.The reason topology is mentioned in the subtitle of this section is that there is a third case, (3), which is a hybrid of (1) and (2) as hinted above.Many excellent detection and control systems process the input channels in relation to one another prior to the actual learning system components.  This pre-processing takes advantage of known relationships in the data and the topology of signals in phase space.The phase space of two or three signals can be visualized in 2D or 3D where each channel is assigned to a Cartesian coordinate system's axis and the time domain is not represented by any axis.  Mechanical fault detection can be optimized using phase space analysis.An example of this more complex case (3) is in the human recognition of both consonant and vowel sounds and various other non-speech audio phenomena.  After the cochlear conversion to spectra, the transient information (as from a 'k' sound or a click) and tone information (as from an 'e' or a whistle) is presented to speech recognition nets in the brain in an integrated form.This decreases the learning time newborns take to learn voice and household sounds and probably improved evolutionary viability in times of global stress or local predatory activity.In automated vehicles and smart robotics, the same is true.  Some sounds, accelerations, and visible light changes are transient and some are vibrations.  Studying the topology of the surface created by the input signal types and their spectra in phase space provides much information.  Armed with the results of such studies allows excellence in the preparation of input to a learning system.Fourth Step — Normalizing and StandardizingOnce the above is complete, understanding how to present the data to the input layer of an artificial network becomes clearer.  The goal of this design step is to have relatively equal ranges for the components of each input tensor, whether presented serially or in parallel depending on the above criteria.Doing so reduces the burden on the learning system.  All signals are presented at full strength but within the clipping range of the signal.  This is a general principle and is important for both data center (batch) learning and real time learning and adaptive systems."
Is there a way of representing the minimax algorithm mathematically?,"
I have successfully figured out how the minimax algorithm works for a game like chess, where a game tree is used, and you assign a value to the terminal nodes and propagate that value up the tree.
Is there a way to represent this algorithm mathematically? If so, how would I go about showing it? 
","['math', 'minimax', 'chess']",
What were the criticisms of the BACON algorithm?,"
The BACON algorithm, introduced by Pat Langley and Herbert Simon etc. were meant to automate scientific discovery -- producing causal explanation to variations in given data.
It was found, in particular, to have been able to ""discover"" the laws of planetary motion, but there were criticism concerning whether or not its achievement could be consider true scientific discovery.
The one I remember was that--the data given to the algorithm are pre-processed by human operators so as to include mostly relevant factors of the world. For example, the distances of planets from the sun was given, showing that the human operators implicitly understood the importance of this variable for the laws of planetary motion.
I was wondering if there were other significant criticism of the algorithm--either in general as a automaton of discovery or concerning its particular feat with planetary motion.
","['cognitive-science', 'automation']",
How do I apply reinforcement learning to a game with infinitely many actions?,"
I am trying to figure out how to use a reinforcement learning algorithm, if possible, as a ""black box"" to play a game. In this game, a player has to avoid flying birds. If he wants to move, he has to move the mouse on the display, which controls the player position by applying a force. The player can choose any position on the display for the mouse. A human can play this game. To get a sense what needs to be done, have a look at this Youtube video.
I thought about using an ANN, which takes as input the information of the game (e.g. positions, speeds, radius, etc.) and outputs a move. However, I am unlikely to ever record enough training data to train the network properly.
Therefore, I was thinking that a RL algorithm, like Q-learning, would be more suited for this task. However, I have no clue how to apply Q-learning to this task. For example, how could the coder possibly know what future reward another move will bring?
I have a few questions:

In this case, the player has infinitely many actions. How would I apply RL to this case?
Is RL a good approach to solving this task?
Is there a handy Java library which would allow me to use a RL algorithm (as a block-box) to solve this problem?
Are there alternatives?   

","['reinforcement-learning', 'game-ai', 'applications', 'java']",
Has the spontaneous emergence of replicators been modeled in Artificial Life?,"
One of the cornerstones of The Selfish Gene (Dawkins) is the spontaneous emergence of replicators, i.e. molecules capable of replicating themselves.
Has this been modeled in silico in open-ended evolutionary/artificial life simulations?
Systems like Avida or Tierra explicitly specify the replication mechanisms; other genetic algorithm/genetic programming systems explicitly search for the replication mechanisms (e.g. to simplify the von Neumann universal constructor)
Links to simulations where replicators emerge from a primordial digital soup are welcome.
","['reference-request', 'evolutionary-algorithms', 'artificial-life', 'self-replicating-machines']",
Influence of location on a Neural Network trained for parking detection occupancy,"
I loaded a neural network model trained with Caffe by other people in OpenCV. 
The model should detect the presence of a car in a single parking spot outputting the probability of it being free/occupied.
The model was trained with images all belonging to the same parking area, taken at different hours of day and with different light conditions. Images were taken by different cameras but the cameras are all of the same model (raspberry cameras).
I tried to run the model with a few images some of them taken from their dataset and other downloaded from google.
The images taken from their dataset are correctly classified while the ones taken from google are not correctly classified.
My question is: is it possible to deploy a NN model trained with images all coming from a single parking area in another parking area? Is not such a model for parking detection occupancy supposed to generalize independently from the location where training images have been taken?
If you know about an already existing trained model that works good please let me know.
","['neural-networks', 'image-recognition', 'classification', 'training']",
How to measure the reasoning capabilities of neural networks [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



Which possibilities exist to evaluate the visual reasoning capabilities of neural networks in the field of image recognition?
Are there methods to measure the ability of machine reasoning?
Or something more specific: Is it possible to measure if a network understood the concept of a car / a cat / a human without using the classification accuracy.
","['convolutional-neural-networks', 'image-recognition', 'reasoning']",
How can we use data augmentation for creating data set for face recognition and will the inverted faces on augmented images detected?,"
I saw when browsing we can use data augmentation for creating a dataset for face recognition. The augmented images may include inverted, tilted or distorted faces. Do the model detect the face from the inverted image. When I tried my model cant able to detect any inverted or tilted faces.
","['image-recognition', 'facial-recognition']",
Why did fuzzy logic fall out of fashion?,"
Fuzzy logic seemed like an active area of research in machine learning and data mining back when I was in grad school (early 2000s). Fuzzy inference systems, fuzzy c-means, fuzzy versions of the various neural network and support vector machine architectures were all being taught in grad courses and discussed in conferences. 
Since I've started paying attention to ML again (~2013), Fuzzy Logic seems to have dropped off the map completely and its absence from the current ML landscape is conspicuous given all the AI hype. 
Was this a case of a topic simply falling out of fashion, or was there a specific limitation of fuzzy logic and fuzzy inference that led to the topic being abandoned by researchers? 
","['machine-learning', 'fuzzy-logic']",
Data / model preparation for spatio-temporal deep-learning analysis for traffic congestion events detection,"
I am preparing the Bus movement dataset for deep learning (ANN/CNN/RNN) analysis for congestion events detection. This is an extension to my original question, which can be located at 'Deep learning model training and processing requirement for Traffic data' for the general approach on this topic, and this question is for preparing the dataset and need your kind advice on it. In simple words, I would like to know the state of congestion for a bus route at a specific point in time (year). 
Here are my entities:

Routes
Bus_Scheduled_Routes
Bus_Route_Stops
Bus_Trips (operational_date, Vehicle_id, Trip_id, Vehicle_Position_Update, Trip_stop_id, passenger_loaded, velocity, direction, scheduled_arrival_time, actual_arrival_time)
Events (human and non-human induced)
Points of Interests (POIs)

If I have these entities based data and I create a view that gives me a time reference based view comprising of week(52), day(7), Vehicle_id, Trip_id,  Stop/Position_update_interval, speed, acceleration, velocity, scheduled_arrival_time, actual_arrival_time. Will this view be recommended to start training the model? 
Secondly, how can I integrate the human / non-human induced events and Points of Interests (POIs) data into this view so my model can predict better results? To generalize the model data will be 'time segment / trips time (seasons), location component (Bus Routes and Stops), Arrival time / trip completion time'. I am thinking to add an attribute for human/non-human induced events as type tying with the 'time segment' and adding the POIs as type and vicinity to the stop points. What are your recommendation about it? Thanks in advance for your help.
","['deep-learning', 'time']",
Can we make Object Detection as human eyes+brain do?,"
I am so much curious about how do we see(with eyes ofc) and detect things and their location so quick. Is the reason that we have huge gigantic network in our brain and we are trained since birth to till now and still training. 
basically I am saying , are we trained on more data and huge network? is that the reason?
or what if there's a pattern for about how do we see and detect object.
please help me out, maybe my thinking is in wrong direction.
what I wanna achieve is an AI to detect object in picture in human ways.thanks.
","['human-like', 'human-inspired']",
Why did the L1/L2 regularization technique not improve my accuracy?,"
I am training a multilayer neural network with 146 samples (97 for the training set, 20 for the validation set, and 29 for the testing set). I am using:

automatic differentiation,
SGD method,
fixed learning rate + momentum term,
logistic function,
quadratic cost function,
L1 and L2 regularization technique,
adding some artificial noise 3%.

When I used the L1 or L2 regularization technique, my problem (overfitting problem) got worst.
I tried different values for lambdas (the penalty parameter 0.0001, 0.001, 0.01, 0.1, 1.0 and 5.0). After 0.1, I just killed my ANN. The best result that I took was using 0.001 (but it is worst comparing the one that I didn't use the regularization technique).
The graph represents the error functions for different penalty parameters and also a case without using L1.

and the accuracy

What can be?
","['neural-networks', 'deep-learning', 'training', 'overfitting', 'regularization']",
"How can I ensure convergence of DDQN, if the true Q-values for different actions in the same state are very close?","
I am applying a Double DQN algorithm to a highly stochastic environment where some of the actions in the agent's action space have very similar ""true"" Q-values (i.e. the expected future reward from either of these actions in the current state is very close). The ""true"" Q-values I know from an analytical solution to the problem.
I have full control over the MDP, including the reward function, which in my case is sparse (0 until the terminal episode). The rewards are the same for identical transitions. However, the rewards vary for any given state and action taken therein. Moreover, the environment is only stochastic for a part of the actions in the action space, i.e. the action chosen by the agent influences the stochasticity of the rewards.
How can I still ensure that the algorithm gets these values (and their relative ranking) right?
Currently, what happens is that the loss function on the Q-estimator decreases rapidly in the beginning, but then starts evening out. The Q-values also first converge quickly, but then start fluctuating around.
I've tried increasing the batch size, which I feel has helped a bit. What did not really help, however, was decreasing the learning rate parameter in the loss function optimizer.
Which other steps might be helpful in this situation?
So, the algorithm usually does find only a slightly suboptimal solution to the MDP.
","['reinforcement-learning', 'value-functions', 'convergence', 'reward-functions', 'double-dqn']","Let $Q^*(s, a)$ denote the ""true"" $Q$-value for a state-action pair $(s, a)$, i.e. the values that we're hoping to learn to approximate using a neural network that outputs $Q(s, a)$ values.The problem you describe is basically that you have situations where $Q^*(s, a_1) = Q^*(s, a_2) + \epsilon$ for some very small value $\epsilon$, where $a_1 \neq a_2$.An important thing to note is that the ""true"" $Q^*$ values are directly determined by the reward function, which essentially encodes your objective. If your reward function is constructed in such a way that it results in the situation described above, that means the reward function is essentially saying: ""it doesn't really matter much whether we pick action $a_1$ or $a_2$, there is some difference in returns but it's a tiny difference, we hardly care."" The objective implied by the reward function in some sense contradicts what you describe in the question that you want. You say that you care (quite a lot) about a tiny difference in $Q^*$-values, but normally the ""extent to which we care"" is proportional to the difference in $Q^*$-values. So, if there is only a tiny difference in $Q^*$-values, we should really only care a tiny bit.Now, with tabular RL algorithms (like standard $Q$-learning), we do expect to be able to eventually converge to the truly correct solutions, we expect to eventually be capable of correctly ranking different actions even when the differences in $Q^*$-values are small. When you enter function approximation (especially neural networks), this story changes. It's pretty much in the name already; function approximation, we cannot guarantee being able to do better than just approximating. So, 100% ensuring that you'll be able to learn the correct rankings is not going to be feasible. There may be some things that can help though:"
neural network deconvolution filters,"
I understand the concept of convolution.
Let's say that my input dimension is 3 x 10 x 10
And if I say that I will have 20 activation maps and a filter size of 5, I will end up with 20 different filters for my layer, each with the dimension of (3 x 5 x 5)
My output will therefor be (20 x ? x ?). I Put a ""?"" there, because it obviously depends on the filter stride etc.

Now I wanted to implement deconvolution but I am stuck at the following point:
For the following questions, let's assume that the input size for the deconvolution is (5 x 8 x 8), 

If we think about a filter in 3 dimensions. Can I choose any depth for the filter?
How would the effect of the amount of filters (amount of activation maps) work with deconvolution? Do I only have one filter?
How does the input depth (5) come into play. Would the output depth be equal to         (filter depth) * (input depth) ?

I am trying to find the symmetry to forward convolution but I do not understand how to use the amount of activation maps in deconvolution.
I am very thankful for any help.
",['convolutional-neural-networks'],
Fitness function in genetic algorithm based on an interval,"
I am writing an app, where when a ball is shot from a canon it is supposed to land in a hole that is on a given distance. The ball is supposed to land between the distance of the beginning of the hole and the end of the hole. The size of the hole is 4m and the size of the ball is 0.4m. My problem is that I am not sure how to write the fitness function for this. The place where the ball falls should be close to this interval of [D, D+3.6], where D is the distance of the hole. If anyone could give me a hint on how to approach this problem, I would be grateful.
","['genetic-algorithms', 'java', 'fitness-functions']",
Unbalanced dataset in regression rather than classification,"
Assume that we have a labeled dataset with inputs and outputs, where the output range is $\left[0, 2\right]$, but the majority of outputs is in $\left[0, 1\right]$. Should one adopt some kind of over- or undersampling approach after compartmentalising the output space to make the dataset more balanced? That would usually be done in classification, but does it apply to regression problems, too? 
Thanks in advance!
",['machine-learning'],
Can artificial intelligence applications be hacked? [duplicate],"







This question already has answers here:
                                
                            




Is artificial intelligence vulnerable to hacking? [closed]

                                (7 answers)
                            

Closed 3 years ago.



Can artificial intelligence (or machine learning) applications or agents be hacked, given that they are software applications, or are all AI applications secure? 
","['ai-security', 'ai-safety', 'adversarial-ml']",
Can artificial intelligence also make mistakes?,"
The intelligence of the human brain is said to be a strong factor leading to human survival. The human brain functions as an overseer for many functions the organism requires. Robots can employ artificial intelligence software, just as humans employ brains.
When it comes to the human brain, we are prone to make mistakes. However, artificial intelligence is sometimes presented to the public as perfect. Is artificial intelligence really perfect? Can AI also make mistakes?
",['philosophy'],
What is a good descriptor for similar objects?,"
I am developing an image search engine. The engine is meant to retrieve wrist watches based on the input of the user. I am using SIFT descriptors to index the elements in the database and applying Euclidean distance to get the most similar watches. I feel like this type of descriptor is not the best since watches have a similar structure and shape. Right now, the average difference between the best and worst matches is not big enough (15%)
I've been thinking of adding colour to the descriptor, but I'd like to hear other suggestions.
","['image-recognition', 'computer-vision', 'feature-selection']",
Best way to create an image dataset for CNN,"
I am creating a dataset made of many images which are created by preprocessing a long time series. Each image is an array of (128,128) and the there are four classes. I would like to build a dataset similar to the MNIST in scikit-learn.database  but I have no idea how to do it. 
My aim is to have something that I can call like this:
(x_train, y_train), (x_test, y_test) = my_data()

Should I save them as figures? or as csv? 
Which is the best way to implement this? 
","['convolutional-neural-networks', 'datasets']","I found this solution and will be happy about any improvements or suggestions:First, I create a random dataset of images, which are 28x28 pixels, and corresponding random labels (just for sake of clarification, I have another image dataset, this is just for explaining). Then I use a sklearn module for splitting the data:"
How to constraint the output value of a neural network?,"
I am training a deep neural network. There is a constraint on the output value of the neural network (e.g. the output has to be between 0 and 180). I think some possible solutions are using sigmoid, tanh activation at the end of the layer.
Are there better ways to put constraints on the output value of a neural network?
","['neural-networks', 'deep-learning', 'keras', 'activation-functions', 'network-design']",
How much extra information can we conclude from a neural network output values?,"
Consider I have a 3 layers neural network.  

Input Layer containing 784 neurons.
Hidden layer containing 100 neurons.
Output layer containing 10 neurons.

My objective is to make an OCR and I used MNIST data to train my network.
Suppose I gave the network an input taken from an image, and the values from the output neurons are the next:  

$0: 0.0001$ 
$1: 0.0001$ 
$2: 0.0001$
$3: 0.1015$
$4: 0.0001$
$5: 0.0002$
$6: 0.0001$
$7: 0.0009$
$8: 0.001$
$9: 0.051$

When the network returns this output, my program will tell me that he identified the image as number 3.  
Now by looking at the values, even though the network recognized the image as 3, the output value of number 3 was actually very low: $0.1015$. I am saying very low, because usually the highest value of the classified index is as close as 1.0, so we get the value as 0.99xxx.  
May I assume that the network failed to classify the image, or may I say that the network classified the image as 3, but due to the low value, the network is not certain?
Am I right thinking like this, or did I misunderstand how does the output actually works?
","['neural-networks', 'optical-character-recognition']","From the values you have provided I can easily guess your output layer has the sigmoid (do clarify!) activation function. For sigmoid activation function this can be a quite normal occurrence. Also maybe the number of training epochs is not high enough.The case you have mentioned of 0.99 is generally in the case of the output being subjected to a softmax probability function. Although 0.99 is still achievable using sigmoid activation, it will depend on your network hyper-parameters and in general the data. If data is very easily separable the sigmoid will generally give very high contrasting difference among classes. Also if the hidden network has ReLu it becomes easy to provide contrasting class scores due to its huge scale difference compared to a sigmoid activation in the hidden layer.The point here to note is contrasting difference among classes, because your sigmoid might give 0.99 for the correct class while 0.9 for other classes, which is undesirable."
Which model should I use to determine the similarity between predefined sentences and new sentences?,"
The Levenshtein algorithm and some ratio and proportion may handle this use case.

Based on the pre-defined sequence of statements, such as ""I have a dog"", ""I own a car"" and many more, I must determine if an another input statement such as ""I have a cat"" is the same or how much percentage does the input statement is most likely equal to the pre-defined statements.

For Example:

Predefined statements: ""I have a dog"", ""I own a car"", ""You think you are smart""

Input statements and results:

I have a dog - 100% (because it has exact match), I have a cat - ~75% (because it was almost the same except for the animal, think - ~10% (because it was just a small part of the third statement), bottle - 0% (because it has no match at all)

The requirement is that TensorFlow be used rather than Java, which is the language I know, so any help with what to look at to get started would be helpful.
My plan was to use the predefined statements as the train_data, and to output only the accuracy during the prediction, but I don't know what model to use. Please, guide me with the architecture and I will try to implement it.
","['machine-learning', 'natural-language-processing', 'tensorflow']",
Why are neural networks considered to be artificial intelligence?,"
Why are we now considering neural networks to be artificial intelligence?
","['neural-networks', 'terminology', 'definitions', 'social']","Why are we now considering neural networks to be artificial intelligence?""We"" aren't. It is generally due to reporting by media sources that simplify science and technology news.The definition of AI is somewhat fluid, and also contentious at times, but in research and scientific circles it has not changed to the degree that AI=NN.What has happened is that research into neural networks has produced some real advances in the last decade. These advances have taken research-only issues such as very basic computer vision, and made them good enough to turn into technology products that can be used in the real world on commodity hardware.These are game-changing technology advances, and they use neural networks internally. Research and development using neural networks is still turning out new and improved ideas, so has become a very popular area to learn.A lot of research using neural networks is also research into AI. Aspects such as computer vision, natural language processing, control of autonomous agents are generally considered parts of AI. This has been simplified in reporting, and used by hyped-up marketing campaigns to label pretty much any product with a NN in it as ""Artificial Intelligence"". When often it is more correctly statistics or Data Science. Data Science is another term which has been somewhat abused by media and technology companies - the main difference between use of AI and Data Science is that Data Science was a new term, so did not clash with pre-existing uses of it.The rest of AI as a subject and area of study has not gone away. Some of it may well use neural networks as part of a toolkit to build or study things. But not all of it, and even with the use of NNs, the AI part is not necessarily the neural network."
Detect root cause across many event occurrences,"
Suppose there are sensors which supply numerical metrics. If a metric goes above or below a healthy threshold, an event (alert) is raised. Metrics depend on each other in one way or another (we can learn the dependencies via ML algorithms) so when the system is in alerting state only one or a few metrics will be a root cause and all others will be simply consequences.
We can assume there is enough historical metric data available, to learn dependencies but there are just a few historical malfunctions. Also, when malfunction happens there is no one to tell what was the root cause, the algorithm should learn how to detect root causes by itself.
Which algorithms can be used to detect the root cause events in the situation above? Are there any papers available on the subject?
",['machine-learning'],
How do I choose the optimal batch size?,"

Batch size is a term used in machine learning and refers to the number of training examples utilised in one iteration. The batch size
can be one of three options:

batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent
mini-batch mode: where the batch size is greater than one but less than the total dataset size. Usually, a number that can be divided into the total dataset size.
stochastic mode: where the batch size is equal to one. Therefore the gradient and the neural network parameters are updated after each sample.


How do I choose the optimal batch size, for a given task, neural network or optimization problem?
If you hypothetically didn't have to worry about computational issues, what would the optimal batch size be?
","['neural-networks', 'machine-learning', 'training', 'stochastic-gradient-descent', 'batch-size']",
"How do compute the table for $p(s',r|s,a)$ (exercise 3.5 in Sutton & Barto's book)?","
I am trying to study the book Reinforcement Learning: An Introduction (Sutton & Barto, 2018). In chapter 3.1 the authors state the following exercise

Exercise 3.5 Give a table analogous to that in Example 3.3, but for $p(s',r|s,a)$. It should have columns for $s$, $a$, $s'$, $r$, and $p(s',r|s,a)$, and a row for every 4-tupel for which $p(s',r|s,a)>0$.

The following table and graphical representation of the Markov Decision Process is given on the next page.

I tried to use $p(s'\cup r|s,a)=p(s'|s,a)+p(r|s,a)-p(s' \cap r|s,a)$ but without a significant progress because I think this formula does not make any sense as $s'$ and $r$ are not from the same set. How is this exercise supposed to be solved?
Edit
Maybe this exercise intends to be solved by using
$$p(s'|s,a)=\sum_{r\in \mathcal{R}}p(s',r|s,a)$$
and
$$r(s,a,s')=\sum_{r\in \mathcal{R}}r\dfrac{p(s',r|s,a)}{p(s|s,a)}$$
and
$$\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1$$
the resulting system is a linear system of 30 equation with 48 unknowns. I think I am missing some equations...
","['reinforcement-learning', 'sutton-barto', 'probability-theory', 'transition-model']",
How exactly is equivariance achieved in capsule neural networks?,"
I have read quite a lot about capsule networks, but I cannot understand how the squashed vector would also rotate in response to rotation or translation of the image. A simple example would be helpful. I understand how routing by agreement works.
","['machine-learning', 'deep-learning', 'computer-vision', 'capsule-neural-network']",
Variable sized input-Multi Label Classification with Neural Network,"
I have a data input vector ( No Image classification) which size varys from 2 to 7 entrys. Every one of them belongs to a class Out of 7. So I have a variable Input size and a variable Output size. How can I deal with the variable Input sizes? I know Zero padding is a option but maybe there are better ways?
Seconds: Is multi Label classification possible in one Network? What I mean: The first entry has to bei classified in one of the seven classes, the second entry... and so on.
I am also open to other classification techniques, If there is a better one that suits the problem.
Best regards,
Gesetzt
","['neural-networks', 'classification']",
What are the current trends/open questions in logics for knowledge representation?,"
What are the future prospects in near future from a theoretical investigation of description logics, and modal logics in the context of artificial intelligence research?
","['logic', 'knowledge-representation']",
What is the best programming language to learn to implement genetic algorithms? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            



Closed 1 year ago.











Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






What is the best and easiest programming language to learn to implement genetic algorithms? C++ or Python, or any other?
","['comparison', 'genetic-algorithms', 'programming-languages', 'genetic-programming']","Matlab may be a good option to get started with the implementation of genetic algorithms, given that there a lot of pre-defined functions.See e.g. https://www.mathworks.com/discovery/genetic-algorithm.html and https://www.mathworks.com/help/gads/examples/coding-and-minimizing-a-fitness-function-using-the-genetic-algorithm.html."
Does overfitting imply an upper bound on model size/complexity?,"
Suppose that I have a model M that overfits a large dataset S such that the test error is 30%. Does that mean that there will always exist a model that is smaller and less complex than M that will have a test error less than 30% on S (and does not overfit S). 
","['machine-learning', 'overfitting']",
Which Rosenblatt's paper describes Rosenblatt's perceptron training algorithm?,"
I struggle to find Rosenblatt's perceptron training algorithm in any of his publications from 1957 - 1961, namely:

Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms
The perceptron: A probabilistic model for information storage and organization in the brain
The Perceptron — A Perceiving and Recognizing Automaton

Does anyone know where to find the original learning formula?
","['neural-networks', 'machine-learning', 'reference-request', 'perceptron']","The paper (or report) that formally introduced the perceptron is The Perceptron — A Perceiving and Recognizing Automaton (1957) by Frank Rosenblatt. If you read the first page of this paper, you can immediately understand that's the case. In particular, at some point (page 2, which corresponds to page 5 of the pdf), he writesRecent theoretical studies by this writer indicate that it should be feasible to construct an electronic or electromechanical system which will learn to recognize similarities or identities between patterns of optical, electrical, or tonal information, in a manner which may be closely analogous to the perceptual processes of a biological brain. The proposed system depends on probabilistic rather than deterministic principles for its operation, and gains its reliability from the properties of statistical measurements obtained from large populations of elements. A system which operates according to these principles will be called a perceptron.See also Appendix I (page 19, which corresponds to page 22 of the pdf).The paper The perceptron: A probabilistic model for information storage and organization in the brain (1958) by F. Rosenblatt is apparently an updated and nicer version of the original report.A more accessible (although not the most intuitive) description of the perceptron model and its learning algorithms can be found in the famous book Perceptrons: An Introduction to Computational Geometry (expanded edition, third printing, 1988) by Minsky and Papert (from page 161 onwards)."
To what level of abstraction must fully automated vehicles build their driving model before safety can be maximized?,"
There are several levels of abstraction involved in piloting and driving.

Signals representing the state of the vehicle and its environment originating from multiple transducers1
Latched sample vectors/matrices
Boundary events (locations, spectral features, movement, appearance and disappearance of edges, lines, and sounds)
Objects
Object movements
Object types (runways, roads, aircraft, birds, cars, people, pets, screeches, horns, bells, blinking lights, gates, signals, clouds, bridges, trains, buses, towers, antennas, buildings, curbs)
Trajectory probabilities based on object movements and types
Behaviors based on all the above hints
Intentions based on behavior sequences and specific object recognition
Collision risk detection

Moving from interpretation to control execution ...

Preemptive collision avoidance reaction
Horn sounding
Plan adjustment
Alignment of plan to state
Trajectory control
Skid avoidance
Skid avoidance reaction
Steering, breaking, and signalling
Notifications to passengers

What, if any, levels of higher abstraction can be sacrificed?  Humans, if they are excellent pilots or drivers, can use all of these levels to improve pedestrian and passenger safety and minimize expense in time and money.
Footnotes
[1] Optical detectors, microphones, strain gauge bridges, temperature and pressure gauges, triangulation reply signals, voltmeters, position encoders, key depression switches, flow detectors, altimeters, radar transducers, tachometers, accelerometers 
","['path-planning', 'autonomous-vehicles', 'ai-safety', 'collision-avoidance']",
"Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time?","
I am reviewing a statement on the website for ES regarding structured exploration. 
https://blog.openai.com/evolution-strategies/

Structured exploration. Some RL algorithms (especially policy
  gradients) initialize with random policies, which often manifests as
  random jitter on spot for a long time. This effect is mitigated in
  Q-Learning due to epsilon-greedy policies, where the max operation can
  cause the agents to perform some consistent action for a while (e.g.
  holding down a left arrow). This is more likely to do something in a
  game than if the agent jitters on spot, as is the case with policy
  gradients. Similar to Q-learning, ES does not suffer from these
  problems because we can use deterministic policies and achieve
  consistent exploration.

Where can I find sources showing that policy gradients initialize with random policies, whereas Q-Learning uses epsilon-greedy policies? 
Also, what does ""max operation"" have to do with epsilon-greedy policies?
","['reinforcement-learning', 'evolutionary-algorithms', 'q-learning']","Where can I find sources showing that policy gradients initialize with random policies, whereas Q-Learning uses epsilon-greedy policies?You can find example algorithms for Q learning and policy gradients in Sutton & Barto's Reinforcement Learning: An Introduction - Q learning is in chapter 6, and policy gradients explained in chapter 13.Neither of these things are strictly true in all cases. However, both are very common situations for the two kinds of learning agent:Policy gradient solvers learn a policy function $\pi(a|s)$ that gives the probability of taking action $a$ given observed state $s$. Typically this is implemented as a neural network. Neural networks are initialised randomly. For discrete action selection using softmax output layer, the initial function will roughly be choosing evenly from all possible actions. So if some actions oppose and undo each other, e.g. move left and move right are options - then the situation as described in your quote can easily happen. Nowadays there are many other solutions using policy gradients that don't suffer as much with this effect. For instance, deterministic policy gradient methods such as A2C or DDPG.For Q learning, and many variants of it, $\epsilon$-greedy is a very commonly used action selection mechanism. It is convenient and simple, and allows a simple parameter to control balance between exploration and exploitation whilst learning. However, Q learning can work with any action selection mechanism that has some possibility of acting optimally at least occasionally. The best approach to action selection during the learning process in both policy-based methods and value-based methods is an active area of research. So if you read the RL literature you may find a lot of variation. The blog you quoted from has identified two quite common choices.Also, what does ""max operation"" have to do with epsilon-greedy policies?The ""max operation"" is finding the maximum value of some function produced when varying a parameter. The related ""argmax operation"" is finding the value of the parameter that produces the maximum value. Q learning can use both types of operation, but specifically uses $\text{argmax}$ for $\epsilon$-greedy action selection.An $\epsilon$-greedy policy requires acting optimally (according to current estimates of value function) with probability $p=1-\epsilon$, and randomly with equal probability of each action with probability $p=\epsilon$In order to do this, the algorithm usually (with probability $p=1-\epsilon$) needs to know what the current best guess at an optimal solution is. That is the greedy action with respect to the current Q values i.e. $\text{argmax}_a Q(s,a)$  - "
How do I use truth tables to prove Entailment?,"
For example, consider an agent concerned with predicting the weather, with variable R indicating whether or not it is likely to rain, variable C indicating whether or not it is cloudy, and variable L indicating low pressure. Given knowledge base K:
L (Pressure is low)
C (It is cloudy)
C ∧ L ⇒ R, (Clouds and low pressure imply rain)
the agent may conclude R; thus, the agent’s knowledge implies that R is true, because K |= R.
Similarly, given knowledge base L:
¬L (Pressure is high)
C (It is cloudy)
C ∧ L ⇒ R, (Clouds and low pressure imply rain)
the agent cannot conclude that R is true; L 6|= R
Deriving a truth table:
L   C   r   ((L ∧ C) → r)
F   F   F   T
F   F   T   T
F   T   F   T
F   T   T   T
T   F   F   T
T   F   T   T
T   T   F   F
T   T   T   T
but this does not make sense.
",['logic'],
AI that maximizes the storage of rectangular parallelepipeds in a bigger parallelepiped,"
As you can see in the title, I'm trying to program an AI in Java that would help someone optimize his storage.
The user has to enter the size of his storage space (a box, a room, a warehouse, etc...) and then enter the size of the items he has to store in this space. (note that everything must be a rectangular parallelepiped) And the AI should find the best position for each item such that space is optimized.
Here is a list of what I started to do :

I asked the user to enter the size of the storage space (units are trivial here except for the computing cost of the AI, later on, I'm guessing), telling him that the values will be rounded down to the unit
I started by creating a 3-dimensional array of integers representing the storage space's volume, using the 3 values taken earlier. Filling it with 0s, where 0s would later represent free space and 1s occupied space.
Then, store in another multidimensional array the sizes of the items he has to store And that's where the AI part should be starting. The first thing the AI should do is check whether the addition of all the items' volumes doesn't surpass the storage space's volume. But then there are so many things to do and so many possibilities that I get lost in my thoughts and don't know where to start...

In conclusion, can anyone give me the proper terms of this problem in AI literature, as well as a link to an existing work of this kind? Thanks
","['optimization', 'storage']","A simple approach that gives a good baseline for such problems is simulated annealing. The idea is that you do something random. If it improves things, then it is good. If it makes things worse, you still take it with some probability $p$, where $p$ shrinks over time.The more bad solutions you can rule out beforehand / the smarter you can encode your problem, the better solutions simulated annealing will give."
How do we design a neural network such that the $L_1$ norm of the outputs is less than or equal to 1?,"
What are some ways to design a neural network with the restriction that the $L_1$ norm of the output values must be less than or equal to 1? In particular, how would I go about performing back-propagation for this net?
I was thinking there must be some ""penalty"" method just like how in the mathematical optimization problem, you can introduce a log barrier function as the ""penalty function""
","['neural-networks', 'machine-learning', 'backpropagation', 'objective-functions', 'constrained-optimization']",
"Why is the equation $r(s', a, s') =\sum_{r \in \mathcal{R}} r \frac{p\left(s^{\prime}, r \mid s, a\right)}{p\left(s^{\prime} \mid s, a\right)}$true?","
I am referring to eq. 3.6 (page 49) based on Sutton's online book  and can be found in an image below.

I could not make sense of the final derivation of the equation $r(s, a, s')$. My question is actually how do we come to that final derivation?
Surprisingly, the denominator of $p(s'|s, a)$ can literally be replaced by $p(s', r|s, a)$ as eq. 3.4 suggests, then it will end up with ""$r$"" term only due to cancellation of numerator $p(s', r|s, a)$ and denominator $p(s'|s, a)$.
Any explanation on that would be appreciated.
","['reinforcement-learning', 'math', 'notation', 'reward-functions', 'probability-theory']","No, the substitution you suggest based on Equation (3.4) is not correct because you forgot about the $\sum_{r \in \mathcal{R}}$ in the right-hand side Equation (3.4).Equation (3.4) says (leaving out the middle part):$$p(s' \vert s, a) \doteq \sum_{r \in \mathcal{R}} p(s', r \vert s, a).$$If you plug this into Equation (3.6) to substitute the denominator, you can't forget about that sum, you have to include the complete sum in the denominator. Because we have two different sums summing over all rewards in $\mathcal{R}$, I'll change the symbol used in the second sum to $r'$ rather than $r$. This yields:$$r(s, a, s') \doteq \sum_{r \in \mathcal{R}} r \frac{p(s', r \vert s, a)}{\sum_{r' \in \mathcal{R}} p(s', r' \vert s, a)}.$$The numerator and denominator are different and will not cancel out.Intuitively, the numerator is just a single probability; the probability of observing a specific next state $s'$ and a reward $r$ given a specific current state $s$ and action $a$. The denominator is a sum of many such probabilities, for all possible rewards $r'$ rather than just one specific reward $r$. Because this sum ""covers"" all possible events for the rewards, it essentially represents simply the probability of observing state $s'$  given state $s$ and action $a$, paired with any arbitrary reward. That can more simply be denoted as $p(s' \vert s, a)$.... which makes sense because that's the very thing we started with :DEquation (3.6) is as follows (again leaving the middle part out):$$r(s, a, s') \doteq \sum_{r \in \mathcal{R}} r \frac{p(s', r \vert s, a)}{p(s' \vert s, a)}.$$In normal English, the left-hand side says ""what reward do we expect to get (on average) if we transition from state $s$ to state $s'$ by executing action $a$?""Such an $\color{red}{\text{expectation of a quantity }r}$ can always be computed by multiplying $\color{blue}{\text{the possible values that }r\text{ can take}}$ by $\color{orange}{\text{the probability of each particular value occurring}}$, and summing up those multiplications. In math, this looks like:$$\color{red}{r(s, a, s')} = \sum_{\color{blue}{r \in \mathcal{R}}} \color{blue}{r} \times \color{orange}{p(r \vert s, a, s')}.$$Now, there is a rule in probability that says (see Rule of Multiplication on https://stattrek.com/probability/probability-rules.aspx):$$p(A, B) = p(A) \times p(B \vert A),$$so, if we take $A = s'$, $B = r$, and $s, a$ as additional givens for all probabilities, we get:$$p(s', r \vert s, a) = p(s' \vert s, a) \times p(r \vert s, a, s').$$This can be rewritten (dividing both sides of the equation by $p(s' \vert s, a)$ and swapping the left-hand and right-hand sides) to:$$p(r \vert s, a, s') = \frac{p(s', r \vert s, a)}{p(s' \vert s, a)}.$$Plugging this in for the orange term in the coloured equation above (which itself is hopefully fairly easy to understand intuitively) yields Equation (3.6)."
Can the addition of dropout in a non-overfitting neural network increase accuracy?,"
According to Wikipedia

Dropout is a regularization technique for reducing overfitting in neural networks

My neural network is simple enough and does not overfit.
Can the addition of dropout, in a non-overfitting neural network, increase accuracy? Even if I increase the complexity of the neural network?
","['neural-networks', 'machine-learning', 'deep-learning', 'overfitting', 'dropout']","Can the addition of dropout, in a non-overfitting neural network, increase accuracy?Yes, maybe.Even if I increase the complexity of the neural network?Yes, maybe.As always when making changes to ML algorithms, you need to test carefully to see if your changes have made an improvement. There are very few theories in non-linear machine learning models that make solid guarantees of results. One general difference you should note is that training a network with dropout will take longer (more epochs) than training a similar network without dropout, to reach the same levels of accuracy.However, as well as the regularisation effect of dropout, it shares some behaviour with ensemble techniques such as bagging. Dropout effectively trains many sub-networks (that share weights) on different samples of the training data. This pseudo-ensemble effect can boost accuracy, and other success metrics. This is not a guaranteed effect, but it does happen in practice."
input annotations quality check for large scale image data,"
while dealing with image data at very large scale, there are different sources where data is coming from. Often, we do not have any control over quality of labels/ annotations. I already do use sampling quality checks method to manually check the quality of annotations but as the volume of data has increased,even sampling QC become an inefficient job. Are there other methods to automate / simplify this task for data at large scale?
","['deep-learning', 'computer-vision', 'datasets']",
Which neural network to use for optical mark recognition?,"
I've created a neural net using the ConvNetSharp library which has 3 fully connected hidden layers. The first having 35 neurons and the other two having 25 neurons each, each layer with a ReLU layer as the activation function layer.
I'm using this network for image classification - kinda. Basically it takes inputs as raw grayscale pixel values of the input image and guesses an output. I used stochastic gradient descent for the training of the model and a learning rate of 0.01. The input image is a row or column of OMR ""bubbles"" and the network has to guess which of the ""bubble"" is marked i.e filled and show the index of that bubble.
I think it is because it's very hard for the network to recognize the single filled bubble among many.
Here is an example image of OMR sections:

Using image-preprocessing, the network is given a single row or column of the above image to evaluate the marked one.
Here is an example of a preprocessed image which the network sees:

Here is an example of a marked input:

I've tried to use Convolutional networks but I'm not able to get them working with this.
What type of neural network and network architecture should I use for this kind of task? An example of such a network with code would be greatly appreciated.
I have tried many preprocessing techniques, such as background subtraction using the AbsDiff function in EmguCv and also using the MOG2 Algorithm, and I've also tried threshold binary function, but there still remains enough noise in the images which makes it difficult for the neural net to learn.
I think this problem is not specific to using neural nets for OMR but for others too. It would be great if there could be a solution out there that could store a background/template using a camera and then when the camera sees that image again, it perspective transforms it to match exactly to the template
I'm able to achieve this much - and then find their difference or do some kind of preprocessing so that a neural net could learn from it. If this is not quite possible, then is there a type of neural network out there which could detect very small features from an image and learn from it.  I have tried Convolutional Neural Network but that also isn't working very well or I'm not applying them efficiently.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'model-request']","From what I understand, don't bother with a CNN, you have essentially perfectly structured images.You can hand code detectors to measure how much filled in a circle is.Basically do template alignment and then search over the circles.Ex a simple detector would measure the average blackness of the circle which you could then threshold."
Any problems/games/puzzles in which exhaustive search cannot show that a solution does not exist?,"
Introduction
Exhaustive search is a method in AI planning to find a solution for so-called Constraint Satisfaction Problems. (CSP). Those are problems that have some conditions to fulfill and the solver is trying out all the alternatives. An example CSP problem is the 8-queens problem which has geometrical constraints. The standard method in finding a solution for the 8-queens problem is a backtracking solver. That is an algorithm that generates a tree for the state space to search inside the graph.
Apart from practical applications of backtracking search, there are some logic-oriented discussions available which are asking on a formal level which kind of problems have a solution and which not. For example to find a solution for the 8-queen problem many millions of iterations of the algorithm are needed. The question is now: which problems are too complex to find a solution. The second problem is, that sometimes the problem itself has no solution, even the complete state space was searched fully.
Let us take an example. At first, we construct a problem in which the constraints are so strict that even a backtracking search won't find a solution. One example would be to prove that “1+1=3” another example would be to find a chess sequence if the game is lost or it is also funny to think about how to arrange nine! queen on a chess table so that they don't hurt.
Is there any literature available which is describing Constraint Satisfaction Problems on a theoretical basis in which the constraints of the problem are too strict?
Original posting
Just wondering - like with an 8-queens problem. If we change it to a 9-queens problem and do an exhaustive search, we will see that there is no solution. Is there a problem in which the search fails to show that a solution does not exist?
",['search'],
When is the Markov decision process not adequate for goal-directed learning tasks?,"
In the book Reinforcement Learning: An Introduction (Sutton and Barto, 2018). The authors ask

Exercise 3.2:  Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear
  exceptions?

I thought maybe a card game would be an example if the state does not contain any pieces of information on previously played cards. But that would mean that the chosen state leads to a system that is not fully observable. Hence, if I track all cards and append it to the state (state vector with changing dimension) the problem should have the Markov Property (no information on the past states is needed). This would not be possible if the state is postulated as invariant in MDP.
If the previous procedure is allowed, then it seems to me that there are no examples where the MDP is not appropriate.
I would be glad if someone could say if my reasoning is right or wrong. What would be an appropriate answer to this question?
","['reinforcement-learning', 'applications', 'markov-decision-process']","BackgroundThe Markov Decision Process is an extension of Andrey Markov's action sequence that visualize action-result sequence possibilities as a directed acyclic graph.  One path through the acyclic graph, if it satisfies the Markov Property is called a Markov Chain.The Markov Property requires that the probability distribution of future states at any point within the acyclic graph be evaluated solely on the basis of the present state.Markov Chains are thus a stochastic model theoretically representing one of the set of possible paths.  And the action-result sequence is a list of state transitions corresponding to actions chosen solely by each action's previous state and the expectations that the expected subsequent state will most probably lead to the desired outcome.Andrey Markov based his work on Gustav Kirchhoff's work on spanning trees, which is based on Euler's initial directed graph work.The ExerciseExercise 3.2 was given with two parts.Is the MDP framework adequate to usefully represent all goal-directed learning tasks?Can you think of any clear exceptions?The first question is subjective in that it inquires about usefulness but does not define what it means.  If ""useful"" means the MDP will improve the chances of achieving a goal over a random selection of action at each state, then except in no win scenarios or the most contrived case where all actions have equal distribution of probable results, then the MDP is useful.If ""useful"" means optimal, then there are other approaches, with additional complexity and requiring additional computing resources that will improve odds of goal achievement.  These other approaches overcome one or more of the limitations of pure MDP.Advancements and AlternativesAdvancements made to MDP and alternatives to MDP, which number in the hundreds, include these.Card GamesGame play for a typical card game could make use of MDP, so MDP would be strictly useful, however not optimal.  Some of the above decisioning features would be more optimal, particularly those that deals with unknowns and employ rules, since the card game has them.Random or DecoupledTwo obvious cases are (a) a truly randomized action-result world where goal achievement has equal probability no matter the sequence of moves or (b) a scenario where goal achievement is entirely decoupled from actions the actor can take.  In those cases, nothing will be useful with regard to the particular objective chosen.ChallengeThe way to best learn from the exercise, though, is to find a scenario where MDP would be useless and one of the above listed Advancements and Alternatives would be required rather than simply preferred.  If you look at the list, there will be some cases that will eventually come to mind.  I suggest you think it through, since the goal is to learn from the book."
"Where are the parentheses in the definition of $r(s,a)$?","
I am new to RL and I am trying to work through the book Reinforcement Learning: An Introduction I (Sutton & Barto, 2018). In chapter 3 on Finite Markov Decision Processes, the authors write the expected reward as
$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in \mathcal{R}}r\sum_{s'\in \mathcal{S}}p(s',r|s,a)$$
I am not sure if the authors mean
$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in \mathcal{R}}\left[r\sum_{s'\in \mathcal{S}}p(s',r|s,a)\right]$$
or 
$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\left[\sum_{r\in \mathcal{R}}r\right]\cdot\left[\sum_{s'\in \mathcal{S}}p(s',r|s,a)\right].$$
If the authors mean the first, is there any reason why it is not written like the following?
$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in \mathcal{R}}\sum_{s'\in \mathcal{S}}\left[r\,p(s',r|s,a)\right]$$
","['reinforcement-learning', 'definitions', 'notation', 'reward-functions', 'sutton-barto']","Your first option is correct:$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in \mathcal{R}}\left[r\sum_{s'\in \mathcal{S}}p(s',r|s,a)\right]$$It's partly a matter of taste, but I prefer not moving the $r$ into the double sum, because its value does not change in the ""inner loop"". There is a small amount of intuition to be had that way around, especially when it comes to implementation (it is one multiplication after the sum, as opposed to many within the sum).There are a lot of sums containing sums in Sutton & Barto, and they mainly follow the convention of not using any parentheses or brackets to show the one sum containing the other explicitly.In this case, the formulae help link to other treatments of RL, which work with the expected reward functions $r(s,a)$ or $r(s,a,s')$, or reward matrices $R_s^a$, $R_{ss'}^a$ such as the first edition of Sutton & Barto's book. The second edition of the book uses $p(s', r|s, a)$ almost everywhere though, and you won't see $r(s,a)$ mentioned much again. So it's not worth getting too concerned about how it is presented and what the author might be saying with the presentation.Generally you don't need to know the distribution of reward, just its expectation (and how that depends on $s, a, s'$), in order to derive and explain most of the results in RL. So using $r(s,a)$ and similar functions is fine, in places like the Bellman equations. However, the use of $p(s', r|s, a)$ is general without needing to bring in more functions describing the MDP."
Does it make sense to apply softmax on top of relu?,"
While working through some example from Github I've found this network (it's for FashionMNIST but it doesn't really matter). 
Pytorch forward method (my query in upper case comments with regards to applying Softmax on top of Relu?):
def forward(self, x):
    # two conv/relu + pool layers
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))

    # prep for linear layer
    # flatten the inputs into a vector
    x = x.view(x.size(0), -1)

    # DOES IT MAKE SENSE TO APPLY RELU HERE
    **x = F.relu(self.fc1(x))

    # AND THEN Softmax on top of it ?
    x = F.log_softmax(x, dim=1)**

    # final output
    return x

","['convolutional-neural-networks', 'python']",
"What is meant by ""model discriminability for local patches within the receptive field""?","
In the abstract of the paper Network In Network, the authors write

We propose a novel deep network structure called ""Network In Network""(NIN) to enhance model discriminability for local patches within the receptive field

What does the part in bold mean?
","['convolutional-neural-networks', 'papers', 'models']",
How to understand marginal loglikelihood objective function as loss function (explanation of an article)?,"
I am reading article https://allenai.org/paper-appendix/emnlp2017-wt/ http://ai2-website.s3.amazonaws.com/publications/wikitables.pdf about training neural network and the loss function is mentioned on page 6 chapter 3.4 - this loss function O(theta) is expressed as marginal loglikelihood objective function. I simply does not understand this. The neural network generates logical expression (query) from some question in natural language. The network is trained using question-answer pairs. One could expect that simple sum of correct-1/incorrect=0 result could be good loss function. But there is strange expression that involves P(l|qi, Ti; theta) that is not mentioned in the article. What is meant by this P function? As I understand, then many logical forms l are generated externally for some question qi. But further I can not understand this. The mentioned article largely builds on other article http://www.aclweb.org/anthology/P16-1003 from which it borrows some terms and ideas.
It is said that l is treated as latent variable and P seems to be some kind of probability. Of course, we should assign the greated probability to the right logical form l, but where can I find this assignment. Does training/supervision data should contain this probability function for training/supervision data?
","['neural-networks', 'natural-language-processing', 'long-short-term-memory', 'objective-functions']",
How can my Q-learning agent trained to solve a specific maze generalize to other mazes?,"
I implemented Q-learning to solve a specific maze. However, it doesn't solve other mazes. How could my Q-learning agent be able to generalize to other mazes?
","['reinforcement-learning', 'q-learning', 'function-approximation', 'generalization', 'state-representations']","I'm going to assume here that you're using the standard, basic, simple variant of $Q$-learning that can be described as tabular $Q$-learning, where all of your state-action pairs for which you're learning $Q(s, a)$ values are represented in a tabular fashion. For example, if you have 4 actions, your $Q(s, a)$ values are likely represented by 4 matrices (corresponding to the 4 actions), where every matrix has the same dimensionality as your maze (I'm assuming that your maze is a grid of discrete cells here).With such an approach, you are learning $Q$ values separately for every single individual state (+ action). Such learned values will always only be valid for one particular maze (the one you have been training in), as you seem to have already noticed. This is a direct consequence of the fact that you're learning individual values for specific state-action pairs. The things you are learning ($Q$ values) can therefore not directly be transferred to a different maze; those particular states from the first maze do not even exist in the second maze!Better results may be achievable with different state representations. For example, instead of representing states by their coordinates in a grid (as you would likely do with a tabular approach), you'd want to describe states in a more general way. For example, a state could be described by features such as:An alternative that also may actually be able to better generalize to some extent could be pixel-based inputs if you have images (a top-down image or even a first-person-view).When states are represented by such features, you can no longer use the tabular RL algorithms that you are likely familiar with though. You'd have to use function approximation instead. With good state-representations, those techniques may have a chance of generalizing. You'd probably want to make sure to actually also use a variety of different mazes during the training process though, otherwise they'd likely still overfit to only a single maze used in training."
How could I use machine learning to detect text and non-text regions in scanned documents?,"
I have a collection of scanned documents (which come from newspapers, books, and magazines) with complex alignments for the text, i.e. the text could be at any angle w.r.t. the page. I can do a lot of processing for different features extraction. However, I want to know some robust methods that do not need many features.
Can machine learning be helpful for this purpose? How could I use machine learning to detect text and non-text regions in these scanned documents?
","['machine-learning', 'natural-language-processing', 'pattern-recognition', 'optical-character-recognition']",
What does the agent in reinforcement learning exactly do?,"
What is an agent in reinforcement learning (RL)? I think it is not the neural network behind. What does the agent in RL exactly do?
","['reinforcement-learning', 'intelligent-agent']","The agent in RL is the component that makes the decision of what action to take.In order to make that decision, the agent is allowed to use any observation from the environment, and any internal rules that it has. Those internal rules can be anything, but typically in RL, it expects the current state to be provided by the environment, for that state to have the Markov property, and then it processes that state using a policy function $\pi(a|s)$ that decides what action to take. In addition, in RL we usually care about handling a reward signal (received from the environment) and optimising the agent towards maximising the expected reward in future. To do this, the agent will maintain some data which is influenced by the rewards it received in the past, and use that to construct a better policy.One interesting thing about the definition of an agent, is that the agent/environment boundary is usually considered to be very close to the abstract decision making unit. For instance, for a robot, the agent is typically not the whole robot, but the specific program running on the robot's CPU that makes the decision on the action. All the relays/motors and other parts of the physical body of the robot are parts of the environment in RL terms. Although often loose language is used here, as the distinction might not matter in most descriptions - we would say that ""the robot moves its arm to achieve the goal"" when in stricter RL terms we should say that ""the agent running on the robot CPU instructs the arm motors to move to achieve the goal"".I think it is not the Neural Net behind?That is correct, the agent is more than the neural network. One or more neural networks might be part of an agent, and take the role of estimating the value of a state, or state/action pair, or even directly driving the policy function. "
Is time/space estimation of possible actions required for creating an AGI?,"
Given infinite resources/time, one could create AGIs by writing code to simulate infinite worlds. By doing that, in some of the worlds, AGIs would be created. Detecting them would be another issue.
Since we don't have infinite resources, the most probable way to create an AGI is to write some bootstrapping code that would reduce the resources/time to reasonable values.
In that AGI code (that would make it reasonable to create with finite resources/time) is it required to have a part that deals with time/space estimation of possible actions taken? Or should that be outside of the code and be something the AGI discovers by itself after it starts running?
Any example of projects targeting AGI that are using time/space estimation might be useful for reaching a conclusion.
Clarification, by time/space I mean time/space complexity analysis for algorithms, see: Measures of resource usage and Analysis of algorithms
I think the way I formulated the question might lead people to think that the time/space estimation can only apply to some class of actions called algorithms. To clarify my mistake, I mean the estimation to apply to any action plan.
Imagine you are an AGI and you have to make a choice between different set of actions to pursue your goals. If you had 2 goals and one of them used less space and less time then you would always pick it over the other algorithm. So time/space estimation is very useful since intelligence is about efficiency. There is at least 1 exception though, imagine in the example before that the goal of the AGI is to pick the set of actions that leads to the most expensive time/space set of actions (or any non-minimal time/space cost) then obviously because of the goal constraint you would pick the most time/space expensive set of actions. In most other cases though, you would just pick the most time/space efficient algorithm.
","['agi', 'computational-complexity']",
How do I know how changes in the weights are changing the reward in Reinforcement Learning,"
I already know the basics of the basic of Machine Learning. E.g.: Backpropagation, Convolution, etc.
First of let me explain Reinforcement learning to make sure I grasped the concept correctly.
In Reinforcement learning a random-initialized network will first ""play""/""do"" a sequence of moves in an environment. (In this case a Game). After that, it will receive a reward $r$. Furthermore a q-Value gets defined by the engineer/hooby coder. This reward times the q-Value $q$ to the power of the  position $n$ of the action will be feeded back using BP. 
So how do I know how slight chances in $\vec{w}$ are changing $rq^n$?
","['reinforcement-learning', 'game-ai', 'backpropagation']",
Learning an arbitrary function using a feedforward net,"
I would like to get a simple example running in matlab that will use a neural net to learn an arbitrary function from input output data (basically model identification) and then be able to approximate that function from just the input data. As means of training this net I have implemented a simple back propagation algorithm in matlab but I was not able to get anywhere close to satisfactory results. I would like to know what I may be doing wrong and also what approach I may use instead.
The goal is to have the network represent an identified function f(x) which takes a series x as input and outputs the learned mapping from x -> y. 
Here is the GNU octave code I have so far:
pkg load control signal

function r = sigmoid(z)
    r = 1 ./ (1 + exp(-z));
end

function r = linear(z)
    r = z;
end 

function r = grad_sigmoid(z)
    r = sigmoid(z) .* (1 - sigmoid(z));
end 

function r = grad_linear(z)
    r = 1;
end 

function r = grad_tanh(z)
    r = 1 - tanh(z) .^ 2;
end

function nn = nn_init(n_input, n_hidden1, n_hidden2, n_output)
    nn.W2 = (rand(n_input, n_hidden1) * 2 - 1)'
    nn.W3 = (rand(n_hidden1, n_hidden2) * 2 - 1)'
    nn.W4 = (rand(n_hidden2, n_output) * 2 - 1)'
    nn.lambda = 0.005;
end

function nn = nn_train(nn_in, state, action)
    nn = nn_in;

    [out, nn] = nn_eval(nn, state);

    d4 = (nn.a4 - action) .* grad_linear(nn.W4 * nn.a3); 
    d3 = (nn.W4' * d4) .* grad_tanh(nn.W3 * nn.a2);
    d2 = (nn.W3' * d3) .* grad_tanh(nn.W2 * nn.a1);

    nn.W4 -= nn.lambda * (d4 * nn.a3');
    nn.W3 -= nn.lambda * (d3 * nn.a2');
    nn.W2 -= nn.lambda * (d2 * nn.a1');
end

function [out,nn] = nn_eval(nn_in, state)
    nn = nn_in;

    nn.z1 = state;
    nn.a1 = nn.z1;

    nn.a2 = tanh(nn.W2 * nn.a1);
    nn.a3 = tanh(nn.W3 * nn.a2);
    nn.a4 = linear(nn.W4 * nn.a3);

    out = nn.a4;
end

nn = nn_init(1, 100, 100, 1);
t = 1:0.1:3.14*10;
input = t;
output = sin(input);
learned = zeros(1, length(output));

for j = 1:500
    for i = 1:length(input)
        nn = nn_train(nn, [input(i)], [output(i)]); 
    end
    j
end

for i = 1:length(input)
    learned(i) = nn_eval(nn, [input(i)]);    
end

plot(t, output, 'g', t, learned, 'b');

pause

Here is the result:

The result is not even close to where I want it to be. Has it got something to do with my implementation of back propagation? 
What changes do I need to do to the code to get a better approximation going? 
",['feedforward-neural-networks'],"You need to scale the input. Neural networks work best with a limited input domain, and train badly when it is exceeded.For statistical data, you would typically scale your input to have mean 0, standard deviation 1.Here, you will be better off fitting the input to roughly -1 to 1. Up to you where you scale the values, but usually this is done outside of the NN code. So I would do something like:And then use nn_input in the training and evaluation loops. As you are putting these directly into a sorted array for plotting, you won't need to do any further re-mapping back or maintain a conversion function. However, in the more common case of arbitrary inputs, you would need to store the conversion factors somewhere (in this case just hardcoded as a function perhaps) in order to make use of the trained NN.Another thing that may help is shuffling your input/output data pairs during training to remove correlation between sequence of input pairs."
How to recognize non-circular radial symmetry in images?,"
This is a question about pattern recognition and feature extraction.
I am familiar with Hough transforms, the Fast Radial Transform and variants (e.g., GFRS), but these highlight circles, spheres, etc.
I need an image filter that will highlight the centroid of a series of spokes radiating from it, such as the center of a asterix or the spokes of a bicycle wheel (even if the round wheel is obscured.  Does such a filter exist?
","['pattern-recognition', 'feature-selection']","The Hough Transform extended to orthogonal ellipses uses this model, accumulating on $\theta$ for all $\{x, y\}$ with parameter matrix\begin{Bmatrix} 
c_x & c_y \\ 
r_x & r_y
\end{Bmatrix}where$$1 = \dfrac {(x - c_x) \, \cos \theta} {r_x} + \dfrac {(y - c_y) \, \sin \theta} {r_y}$$The question is looking to detect the normal lines, so any of the several algorithms for the above model can be modified to accumulate on $r$ for all $\{x, y\}$ with parameter matrix\begin{Bmatrix} 
c_x & c_y  \\ 
r_x & r_y
\end{Bmatrix}where$$0 = \dfrac {x - c_x} {r_x} + \dfrac {y - c_y} {r_y}$$Lines that intersect $(c_x, c_y)$ don't rely on $r_x$ or $r_y$.  However, it may be useful to recognize that, if radially equally spaced, viewing the lines from a position other than one that projects into the plane of the lines at $(c_x, c_y)$ will present a line density that is a function of $\arctan (r_x, r_y)$."
"What are the ways to calculate the error rate of a deep Convolutional Neural Network, when the network produces different results using the same data?","
I am new to the object recognition community. Here I am asking about the broadly accepted ways to calculate the error rate of a deep CNN when the network produces different results using the same data.
1. Problem introduction
Recently I was trying to replicate some classic deep CNNs for the object recognition tasks. Inputs are some 2D image data including objects and the output are the identification/classification results of the object. The implementation involves the use of Python and Keras.
The problem I was facing is that, I may get different validation results among multiple runs of the training even using the same training/validation data sets. To me, that made it hard to report the error rate of the model since every time the validation result may be different.
I think this difference is because of the randomness involved in different aspects of deep CNN, such as random initialization, the random ‘dropout’ used in the regulation, the ‘shuffle’ process used in the choosing of epochs, etc. But I do not know yet the “right” ways to deal with this difference when I want to calculate the error rate in object recognition field. 
2. My exploration – online search
I have found some answers online here. The author proposed two ways, and he/she recommended the first one shown below:

The traditional and practical way to address this problem is to run your network many times (30+) and use statistics to summarize the performance of your model, and compare your model to other models.

The second way he/she introduced is to go to every relevant aspect of the deep CNN, to ""freeze"" their randomness nature on purpose. This kind of approach has also been introduced from Keras Q&A here. They call this issue the “making reproductive results”.
3. My exploration – in academia community (no result yet, need your help!)
Since I was not sure whether the two ways mentioned above are the “right” ones broadly accepted, I was going further exploring in the object recognition academia community.
Now I just begin to read from imageNet website. But I have not found the answer yet. Maybe you could help me knowing the answer easier. Thanks!
Daqi
","['deep-learning', 'convolutional-neural-networks', 'python', 'keras', 'object-recognition']",
Checkers AI game engines,"
I have coded an AI checkers game but would like to see how good it is. Some people have informed me to use the Chinook AI opensource code. But I am having trouble trying to integrate that software into my AI code. How do I integrate another game engine in checkers with the AI I have coded?
","['game-ai', 'checkers']",
"How to use Machine Learning to create a ""Draw-A-Person Test""","
The process revolves around a child's drawing. Each part of each drawing corresponds to a score as in the Draw a Person Test conceived by Dr. Florence Goodenough in 1926. The goal of the machine is to measure a child's mental age through a figure drawing task.
","['machine-learning', 'ai-design', 'image-recognition', 'intelligence-testing']",
What are ontologies in AI?,"
What exactly are ontologies in AI? How should I write them and why are they important?
","['ai-design', 'terminology', 'definitions', 'ontology']",
Can we combine multiple different neural networks in one?,"
I want to make a kind of robotic brain, i.e. a big neural network, which includes an NLP model (for understanding human voice), real-time object recognition system (so that it can identify particular objects), a face recognition model (for identifying faces), etc.
Is possible to build a huge neural network in which we can combine all these separate models together, so we can use all 3 model's capabilities at same time in parallel?
For example, if I ask the robot, using the microphone, ""Can you see that table or that boy?"", the robot would start recognizing the objects and faces, then answer me back by speaking if it could identify them or not.
If this is possible, can you kindly share your idea how can I implement this? Or is there any better to make such AI (e.g. in TensorFlow)?
","['neural-networks', 'deep-learning', 'tensorflow', 'reference-request', 'robots']",
Why are there transition layers in DenseNet?,"
The DenseNet architecture can be summarizde with this figure:

Why there are transition layers between each block?
In the papers, they justify the use of transition layers as follow :

The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes. However,  an essential part of convolutional networks is pooling layers that change the size of feature-maps. To facilitate pooling in our architecture we divide the network into multiple densely connected dense blocks

So, if I understood correctly, the problem is that the feature map size can change, thus we can't concatenate. But how does the addition of transition layers solve this problem?
And how can several dense blocks connected like this be more efficient that one single bigger dense block?
Furthermore, why are all standard DenseNets made of 4 dense blocks? I guess I will have the answer to this question if I understood better the previous questions.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'papers', 'residual-networks']","The point of DenseNet was to go as deep as ResNets, if not deeper, and keep multiple skip connections to preserve the gradient flow back better as well as to keep the earlier layers context (which prevents overfitting). With layers as deep as 120, having a single block being fully concatenated to all the previous ones would mean having a way large feature map, which, I guess, would be computationally very expensive and not feasible.About transition layers (convolution + pooling), I think it's just a way of downsampling the representations calculated by DenseBlocks slowly upto the end as after transition layers the representations go from $56 \times 56$ to $28 \times 28$ to $14 \times 14$, and so on. The authors state it this wayTo further improve model compactness, we can reduce the number of feature-maps at transition layers"
How to improve testing accuracy when training accuracy is high?,"
Following-up my question about my over-fitting network 
My deep neural network is over-fitting : 

I have tried several things : 

Simplify the architecture
Apply more (and more !) Dropout
Data augmentation

But I always reach similar results : training accuracy is eventually going up, while validation accuracy never exceed ~70%.
I think I simplified enough the architecture / applied enough dropout, because my network is even too dumb to learn anything and return random results (3-classes classifier => 33% is random accuracy), even on training dataset :
 
My question is : This accuracy of 70% is the best my model can reach ?
If yes :

Why the training accuracy reach such high scores, and why so fast, knowing this architecture seems to be not compatible ?
My only option to improve the accuracy is then to change my model, right ?

If no :

What are my options to improve this accuracy ?

I'v tried a bunch of hyperparameters, and a lot of time, depending of these parameters, the accuracy does not change a lot, always reaching ~70%. However I can't exceed this limit, even though it seems easy to my network to reach it (short convergence time)
Edit
Here is the Confusion matrix :

I don't think the data or the balance of the class is the problem here, because I used a well-known / explored dataset : SNLI Dataset 
And here is the learning curve :

Note : I used accuracy instead of error rate as pointed by the resource of Martin Thoma
It's really ugly one. I guess there is some problem here.
Maybe the problem is that I used the result after 25 epoch for every values. So with little data, training accuracy don't really have time to converge to 100% accuracy. And for bigger training data, as pointed in earlier graphs, the model overfit so the accuracy is not the best one.
","['neural-networks', 'machine-learning', 'deep-learning', 'overfitting']","I identified the origin of this overfitting..I tried a lot of models, putting more and more dropout, simplifying as much as I could.No matter what I did, after a few epoch of good learning, invariably my loss function was going up. I tried simpler and simpler models, always the same overfitting behavior.
What bugged me at that moment is that no matter what kind of model I used, how deep or how complex, always the accuracy was fine, stabilized at some nice level.So I tried the simplest model I could imagine : Input => Dense with 3 hidden units =>  Output. Finally I got random results, with a 33% accuracy !
From here, I guilt again my network, layer by layer, to see which one was causing the overfitting.And it was the Embedding layer.Even with a simple network like Input => Embeddings => Dense with 3 hidden units =>  Output, the model was overfitting.In Keras, simply instantiate the Embeddings layer with trainable=False. After doing this, no more overfit.In my opinion, this is quite counter-intuitive : I want my embeddings to evolve with the data I show to the network. But look like I can't..."
What should I do when I have a variable-length sequence when instantiating an LSTM in Keras?,"
In keras, when we use an LSTM/RNN model, we need to specify the node [i.e., LSTM(128)]. I have a doubt regarding how it actually works. From the LSTM/RNN unfolding image or description, I found that each RNN cell take one time step at a time. What if my sequence is larger than 128? How to interpret this? Can anyone please explain me? Thank in advance.
","['machine-learning', 'recurrent-neural-networks', 'long-short-term-memory']","In Keras, what you specify is the hidden layer size. So :gives you a Keras layer representing a LSTM with a hidden layer size of 128.As you said :From the LSTM/RNN unfolding image or description, I found that each RNN cell take one time step at a timeSo if you picture your RNN for one time step, it will look like this : And if you unfold it in time, it look like this : You are not limited in your sequence size, this is one of the feature of RNN : since you input your sequence element by element, the size of the sequence can be variable.  That number, 128, represent just the size of the hidden layer of your LSTM. You can see the hidden layer of the LSTM as the memory of the RNN. Of course the goal is not for the LSTM to remember everything of the sequence, just link between elements. That's why the size of the hidden layer can be smaller than the size of your sequence.Sources :From this blog :The larger the network, the more powerful, but it’s also easier to overfit. Don’t want to try to learn a million parameters from 10,000 examples – parameters > examples = trouble.So the consequence of reducing the size of hidden state of LSTM is that it will be simpler. Might not be able to get the links between the element of the sequence. But if you put a too big size, your network will overfit ! And you absolutely don't want that.Another really good blog on LSTM : this link"
Can AI 'fix' heavily compessed videos/photos?,"
So let's say you had a really nice day in a flight simulator and you are getting videos of this type of quality:

This is Full HD (1080p), but heavily compressed. You can literally see the pixels. Now I tried to use something like RAISR, and this python implementation, but it only scales the image up and does not 'fix the thicc pixels'. 
So is there a type of AI that does fix this kind of video/photo into a reasonable quality video? I just want to get rid of those pixels and image artefacts that was generated during the compression.
","['machine-learning', 'deep-learning']",
Transposition table is only used for roughly 17% of the nodes - is this expected?,"
I'm making a Connect Four game using the typical minimax + alpha-beta pruning algorithms. I just implemented a Transposition Table, but my tests tell me the TT only helps 17% of the time. By this I mean that 17% of the positions my engine comes across in its calculations can be automatically given a value (due to the position being calculated previously via a different move order).
For most games, is this figure expected? To me it seems very low, and I was optimistically hoping for the TT to speed up my engine by around 50%. It should be noted though that on each turn in the game, I reset my TT (since the evaluation previously assigned to each position is inaccurate due to lower depth back then).
I know that the effectiveness of TT's are largely dependent on the game they're being used for, but any ballparks of how much they speed up common games (chess, go, etc) would be helpful.
EDIT - After running some more tests and adjusting my code, I found that the TT sped up my engine to about 133% (so it took 75% as much time to calculate). This means those 17% nodes were probably fairly high up in the tree, since not having to calculate the evaluation of these 17% sped up things by 33%. This is definitely better, but my question still remains on whether this is roughly expected performance of a typical TT.
","['game-ai', 'search', 'gaming', 'minimax', 'efficiency']","I don't think that's necessarily a strange number. It's impossible for anyone to really tell you whether that 17% is ""correct"" or not without reproducing it, which would require much more info (basically would have to know every single tiny detail of your implementation to be able to reproduce).Some things to consider:The size of your transposition table / the number of bits you use for indexing into the TT. If you have a relatively small TT, meaning you use relatively few bits for indexing, you'll have bigger probabilities of collisions. That means you will have to replace existing entries more often, which means they might no longer be in the table anymore by the time you encounter transpositions during the search.Where in the search tree are the nodes located that are recognized as transpositions already in the table? If you detect transpositions very high up in the search tree, you save a lot more search time than if you detect a transposition somewhere deep down in the search tree; once you detect a transposition that has already been searched sufficiently deep for the value stored in the table to be valid, you can cut off the complete subtree below that node from the search. This becomes more valuable as it happens closer to the root. So, just the number ""17% of nodes"" doesn't really tell us much.Are you using iterative deepening? Since you mentioned only minimax + alpha-beta pruning in the question, I suspect you're not using iterative deepening. TTs become significantly more valuable once you do use iterative deepening, because then almost every state encountered becomes a ""transposition"". You'll already have seen all those states in a previous iteration with a lower search depth limit. Now, it is important to note with this combo of ID + TTs, that you can no longer completely cut off searches for all recognized transpositions. If an entry in the table holds a value that was computed with a search depth of $d$, that value will no longer be valid when performing a subsequent iteration of ID with a max search depth of $d + 1$ for example. However, that ""outdated"" value stored in the TT can still be used for move ordering, which can lead to significantly more prunings from alpha-beta pruning.How efficient is the remainder of your engine? A TT is not 100% free, it takes a bit of additional time too (for example to compute the hash values for your game states). If the rest of your engine is relatively slow (i.e. inefficient implementation for playing moves, copying game states, etc.), the computational overhead of the TT won't matter much and even a low number of recognized transpositions will still be valuable. If the rest of your engine is very fast, it'll be more important to have a high number of transpositions for the TT to be really valuable.As an ""educated guess"", I'd say the number of 17% you describe is not necessarily strange. Especially given your edit to the question, where you indeed mention that it is likely that transpositions are found high up in the tree (close to the root). When this happens, you immediately remove the probability of recognizing all those states deeper down in the tree of getting recognized as transpositions yourself. So, the pool of states that could ""potentially"" be found in the TT is much less than 100% of the states stored in the TT.It's really just that though, just an educated guess. It's going to be very difficult for anyone to give a conclusive ""yes"" or ""no""."
Choosing Instance Semantic Detection,"
A fixed video camera records people moving through its field of view.
The goal is to detect and track the head, in real-time as it moves through the video.  The norm is there are many heads, which often are sometimes partially obscured.  This example video boxes heads and provides a head count.
There seems to be many different models.  Examples include:   

Adaboost-haar Head detection 
MASK R-CNN
LBP Cascade

Given the context of the video, what is the thought process that you would use to choose a model?    
","['neural-networks', 'semantics']",
Maxpooling in inception? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



Maxpooling is performed as one of the steps in inception which yields same output dimension as that of the input.
Can anyone explain how this max pooling is performed? 
","['neural-networks', 'convolutional-neural-networks']",
Recognising Noise in Simple Classification,"
I have created a classifier for some simple gestures using an input layer, a hidden layer with tanh activation and an output softmax layer. I'm also using the Adam optimiser. The network classifies perfectly with validation data. However, I'd like it to be able to take in random noise that looks nothing like the shapes and not be able to classify it confidently. For example:
One gesture input looks like this and is correctly classified as gesture 'A':

However, when I pass this 'noise', which is clearly differentiable to the human eye, as input it still classifies it with 100% confidence that it is the same gesture 'A'.

I assume it's because the inputs are still very close to 0? My instinct is to scale up the inputs perhaps to increase the differentiation between the noise and the input. However, in real operation the noise will all be on a similar scale to the inputs and I won't know what is noise and what isn't so I will still have to apply the same scaling to that noise. Will I run into the same problem? 
On a more general note is there a teaching approach to prevent misclassifications, particularly if we know what they might look like? For example, in this case I thought I could perhaps generate some noise and use it at training time to create an extra noise class, or is it just best to come up with such a well-trained network that you can use some sort of confidence threshold? For example, if the network only produces 50% classifcation confidence for an input then I can discard it as noise. Any suggestions much appreciated!  
","['convolutional-neural-networks', 'classification']","The network classifies perfectly with validation data, however I'd like it to be able to take in random noise that looks nothing like the shapes and not be able to classify it confidently. You need to train the network on noise well if you want it to be able to recognize it.or is it just best to come up with such a well-trained network that you can use some sort of confidence threshold?You can use spot to get a confidence score for clarification. But training the network on a noise class will work way betterSee:Gal: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
Using reinforcement learning to find a preconditioner for linear systems of the form Ax = b,"
Sparse linear systems are normally solved by using solvers like MINRES, Conjugate gradient, GMRES. 
Efficient preconditioning, i.e., finding a matrix P such that PAx = Pb is easier to solve than the original problem, can drastically reduce the computational effort to solve for x. However, preconditioning is normally problem-specific and there is not ONE preconditioner that works well for every problem.
I thought this would be an interesting problem to apply RL since there are certain norms (e.g. condition number of matrix PA) to measure if P is a good preconditioner, but I could not find any research in this field.
Is there a specific problem why RL could not be applied?
","['reinforcement-learning', 'linear-algebra']",
Difficulty understanding Keras LSTM fitting data,"
I'm try to train a RNN with a chunk of audio data, where X and Y are two audio channels loaded into numpy arrays.  The objective is to experiment with different NN designs to train them to transform single channel (mono) audio into a two channel (stereo) audio.
My questions are:

Do I need a stateful network type, like LSTM? (I think yes.)
How should I organize the data, considering that there are millions of samples and I can't load into memory a matrix of each window of data in a reasonable time-span?

For example if I have an array with: [0, 0.5, 0.75, 1, -0.5, 0.22, -0.30 ...] and I want to take a window of 3 samples, for example.  I guess I need to create a matrix with every sample shift like this, right?
[[0.00, 0.50, 0.75]
 [0.50, 0.75, 1.00]
 [0.75, 1.00,-0.50]
 [1.00,-0.50, 0.22]]

Where is my batch_size? Should I make the matrix like this per each sample shift? Per each window? This may be very memory consuming if I intend to load a 4 min song. 
Is this example matrix a single batch? A single sample?
","['keras', 'long-short-term-memory', 'audio-processing']",
How to find the category of a technical text on a surface-semantic-level,"
There are some predefined categories( Overview, Data Architecture, Technical Details, Applications, etc). The requirement is to classify the input text of paragraphs into their resp. category. I can't use any pre-trained word embeddings (Word2Vec, Glove) because the data entered is not in general English ( talking about dogs, environment, etc) but pure technical (How does a particular program orks, steps to download anaconda, etc). Don't have any data available on the internet to train as well. Anything that understands semantic-surface-level of a sentence will work
","['machine-learning', 'deep-learning', 'natural-language-processing', 'python', 'semantics']",
Deep learning model training and processing requirement for Traffic data [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I am a newbie in deep learning and am looking for advice on predicting traffic congestion events. I have a table for vehicle travel times data, another table with the road length segmented based on stop locations. I am thinking to derive the time-wise route-specific speed details based on stop locations. After initial data cleansing and messaging, my input parameters are the time and stop location with actual speed details. I train my model with the training dataset and validate it as per the deep learning recommended approach. 
So my questions are:

Is this approach correct or how can I improve it? I am not sure if
the number of inputs can be increased for better results.   
Which activation method will be best to utilize to get a range of conditions/event types rather than binary 1 or 0?     
This will require dealing with a bigger dataset of at least over a few GBs. This will evolve into around 200GBs in the final product. Can I use my professional grad laptop to process this data or if I should consider going to Big Data Processing power?

Please advise. Thanks in advance for your help.
",['deep-learning'],"AdjustmentsThere are a few considerations that may help with problem analysis:We can assume that the system design predicts congestion for some purpose, which was not stated in the question.  It is reasonable to assume, because of the economics of transportation, that the objective is to reduce the area under the probability distribution curve for undesirable events resulting from congestion.The mechanism of control through which such objectives could be achieved can be any of these.These controls can be to human drivers or pilots or to automated driving or piloting systems.  The context of the system is key to designing it properly, since the use cases are entirely driven by this context.The QuestionsRegarding computing resources, it is inadvisable to begin with some of the most congested places.Cities Definitely Too CongestedIt is likely that these cities would require a super-computing platform of significant size and a multinational corporation sized budget.Commuter locations such as Melbourne, Australia; Palm Beach, Florida; or New Haven, Connecticut are examples of good choices to start."
People Counting Video Analytics: data acquisition parameters,"
I am interested in understanding how to choose data-acquisition parameters for the subject matter:

Frame Resolution
Frame rates (FPS)

The goal is to have 'enough' (preferably the minimal) resolution and frames to enable AI to identify people. 
QUESTIONS

Are there any published rules of thumb or processes to select video parameters?
Is there a term or label for the selection of video parameters for AI projects?

",['datasets'],
"Would this work to prevent forgetting: train a neural net with N nodes. Then, add more nodes and stop training the original nodes","
Would this work at all?
Idea is to start training a neural net with some number of nodes. Then, add some new nodes and more layers and start training only the new nodes (or only modifying the old nodes very slightly). Ideally, we would connect all old nodes to the new layer added since we might have learned many useful things in the hidden layers. Then repeat this many times.
Intuition is that if the old nodes give bad information the new layer of nodes will weight the activations of old nodes close to zero and learn new/better concepts in the new nodes. The benefit is that we will keep old knowledge forever.
Caveat is that the network can still temporarily ""forget"" concepts if a new layer weights old information close to zero, but it can potentially remember it again too.
If this completely fails, I'm curious if there's some known way to prevent a neural network from forgetting concepts it learned.
",['neural-networks'],"Per Neil's line of questioning:If you're doing offline, supervised or reinforcement learning, where you have the dataset and decide what to show to the network next, there's no reason to do this. It's better to just adjust your training to show rare examples to the network more often.If you're doing online reinforcement learning, then the agent typically controls which examples it sees next through its choice of actions. Increasing the agent's exploration rate will cause it to see a more diverse set of examples, but these examples are also less likely to be useful for solving the task the agent is working on.If you're doing supervised, online, learning, and you don't control the order in which examples appear, it might be useful to freeze some of the weights, but it might be better, and would certainly be simpler, to increase the learning rate for rare classes instead (in effect, showing the network a given rare example repeatedly when it does show up, so that it takes longer to forget it)."
Should the reward or the Q value be clipped for reinforcement learning,"
When extending reinforcement learning to the continuous states, continuous action case, we must use function approximators (linear or non-linear) to approximate the Q-value.  It is well known that non-linear function approximators, such as neural networks, diverge aggressively.  One way to help stabilize training is using reward clipping.  Because the temporal difference Q-update is a bootstrapping method (i.e., uses a previously calculated value to compute the current prediction), a very large previously calculated Q-value can make the current reward relatively minuscule, thus making the current reward not impact the Q-update, eventually leading the agent to diverge.
To avoid this, we can try to avoid the large Q-value in the first place by clipping the reward between [1, -1].
But I have seen some other people say that instead of clipping the reward itself, we can instead clip the Q-value between an interval.
I was wondering which method is better for convergence, and under what assumptions / circumstances.  I was also wondering if there are any theoretical proofs/explanations about reward/Q-value clipping and which one being better. 
","['machine-learning', 'reinforcement-learning', 'value-iteration', 'reward-clipping']","I'll start with the last question in your post:I was also wondering if there are any theoretical proofs/explanations about reward/Q-value clipping and which one being better. I highly doubt there will be any such theoretical work. The problem is that these variants of clipping (clipping rewards and clipping $Q$ values) fundamentally modify the task / the original objective. Once you clip either of those things, you fundamentally change what your agent is trying to optimize for from what the original goal was. I don't think it's ever going to be possible to get any rigorous, theoretical proofs about which one would be better in general. You'd likely have to start out with some very strong assumptions on the reward structure in the original task to have any hope of proving anything here, but such strong assumptions make you lose generality.Intuitively... I think reward clipping feels ""safer"" to me more often than clipping $Q$-values. Clipping $Q$-values seems more aggressive, it could be viewed as some combination of clipping rewards (if you clip $Q$-values to $[-1, 1]$, you're still at the very least also clipping all rewards to that range), but additionally also putting a constraint on how far in the future you're looking (in some sense). This whole argument is very handwavy thoughI suppose, slightly less handwavy, you could say that reward clipping is definitely ""better"" (in the sense that you don't deviate as much from the original objective) in environments where rewards of similar magnitudes can be collected frequently. I struggle to really think of a situation where clipping $Q$-values would be a clear favorite based on intuition. I wouldn't be surprised if clipping $Q$-values may turn out to be better after empirical evaluation in some cases, but it's difficult to say where that would be. It will also very much depend on what range is chosen. Clipping rewards to a range of $[-1, 1]$ is very different from clipping $Q$-values to the same range."
"Dyna-Q algorithm, having trouble when adding the simulated experiences","
I'm trying to create a simple Dyna-Q agent to solve small mazes, in python. For the Q function, Q(s, a), I'm just using a matrix, where each row is for a state value, and each column is for one of the 4 actions (up, down, left, right).
I've implemented the ""real experience"" part, which is basically just straightforward SARSA. It solves a moderately hard (i.e., have to go around a few obstacles) mazes in 2000-8000 steps (in the first episode, it will no doubt decrease with more). So I know that part is working reliably.
Now, adding the part that simulates experience based on what it knows of the model to update the Q values more, I'm having trouble. The way I'm doing it is to keep an experiences list (a lot like experience replay), where each time I take real action, I add its (S, A, R, S') to that list. 
Then, when I want to simulate an experience, I take a random (S, A, R, S') tuple from that list (David Silver mentions in his lecture (#8) on this that you can either update your transition probability matrix P and reward matrix R by changing their values or just sample from the experience list, which should be equivalent). In my case, with a given S and A, since it's deterministic, R and S' are also going to be the same as the ones I sampled from the tuple. Then I calculate Q(S, A) and max_A'(Q(S', A')), to get the TD error (same as above), and do stochastic gradient descent with it to change Q(S, A) in the right direction.
But it's not working. When I add simulated experiences, it never finds the goal. I've tried poking around to figure out why, and all I can see that's weird is that the Q values continually increase as time goes on (while, without experiences, they settle to correct values).
Does anyone have any advice about things I could try? I've looked at the sampled experiences, the Q values in the experience loop, the gradient, etc... and nothing really sticks out, aside from the Q values growing.
edit: here's the code. The first part (one step TD learning) is working great. Adding the planning loop part screws it up.
def dynaQ(self, N_steps=100, N_plan_steps=5):

    self.initEpisode()
    for i in range(N_steps):
        #Get current state, next action, reward, next state
        s = self.getStateVec()
        a = self.epsGreedyAction(s)
        r, s_next = self.iterate(a)
        #Get Q values, Q_next is detached so it doesn't get changed by the gradient
        Q_cur = self.Q[s, a]
        Q_next = torch.max(self.Q[s_next]).detach().item()
        TD0_error = (r + self.params['gamma']*Q_next - Q_cur).pow(2).sum()
        #SGD
        self.optimizer.zero_grad()
        TD0_error.backward()
        self.optimizer.step()
        #Add to experience buffer
        e = Experience(s, a, r, s_next)
        self.updateModel(e)

        for j in range(N_plan_steps):

            xp = self.experiences[randint(0,len(self.experiences)-1)]
            Q_cur0 = self.Q[xp.s, xp.a]
            Q_next0 = torch.max(self.Q[xp.s_next]).detach().item()
            TD0_error0 = (xp.r + self.params['gamma']*Q_next0 - Q_cur0).pow(2).sum()

            self.optimizer.zero_grad()
            TD0_error0.backward()
            self.optimizer.step()

",['reinforcement-learning'],
How to define a loss function for a classifier where the confusion between some classes is more important than the confusion between others?,"
I have a dataset of images belonging to $N$ classes, $A_1, A_2...A_n,B_1,B_2...B_m$ and I want to train a CNN to classify them. The classes can be considered as subclasses of two broader classes $A$ and $B$, therefore the confusion between $A_i$ and $A_j$ is much less problematic than the confusion between $A_i$ and $B_j$. Therefore I want the CNN to be trained in such a way that the difference between $A_i$ and $B_j$ is considered as more relevant. 
1) Are there any loss functions that take this requirement into account? Could a weighted cross entropy work in this case?
2) How this loss would change if the classes were unbalanced?
","['convolutional-neural-networks', 'classification', 'objective-functions']",
Interpretation of a good overfitting score,"
As shown below, my deep neural network is overfitting :


where the blue lines is the metrics obtained with training set and red lines with validation set
Is there anything I can infer from the fact that the accuracy on the training sets is really high (almost 1) ?
From what I understand, it means that the complexity of my model is enough / too big. But does it means my model could theoretically reach such a score on validation set with same dataset and appropriate hyperparameters ? With same hyperparameters but bigger dataset ? 
My question is not how to avoid overfitting.
","['neural-networks', 'machine-learning', 'deep-learning', 'overfitting']","It doesn't tell you very much, to be honest. It does mean that (assuming your training and validation distributions are similar) your model could get the same results on your validation set should you train on that, but that would still be overfitting.Really, the only useful thing overfitting tells you is that you don't have enough regularisation."
Pre priming a network for white space,"
When a human looks at a page. He notices the sets of letters are grouped together separated by white space. If the white space was replaced by another character say z, it would be harder to distinguish words.
For a neural network, spaces are ""just another character"". How can we set up an RNN so it gives special importance to the difference between certain characters like white spaces and letters so that it will train faster? Assume the input is just a sequence of ASCII characters.
","['neural-networks', 'machine-learning', 'natural-language-processing']",
NEAT - Managing species across generations,"
I (mis?)understood the NEAT algorithm has the following steps:

Create a genome pool with N random genomes
Calculate each genome fitness
Assign each genome to a species
Calculate the adjusted fitness and the number of offspring of each species
Breed each species through mutation/crossover from the stronger genomes
go to step 2.

Step 3 is tricky: speciation is made placing each genome G in the first species in which it is compatible with the representative genome of that species, or in a new species if G is not compatible with any existing species. Compatible is meant as having compabilitity distance below a certain threshold. Regarding representative genome NEAT paper says:

Each existing species is represented by a random genome inside the
  species from the previous generation

Somewhere I've found that keeping the number of species stable is good, and this is achieved automatically with dynamic thresholding. However, dynamic thresholding makes hard to evaluate species behaviour across generations.
Let me give one example:
Assume that in Generation 20, Species 1 has Genome A as representative and Species 2 has Genome B as representative. Assume elitism is implemented.
As the representative genome is taken from previous generation, assume that in Generation 21, Genome A and B are still representatives for Species 1 and 2, however assume compatibility threshold has changed (i.e. bigger) in order to reach the target species number. With this change, A and B have now a compatibility distance lower than threshold and should be placed in the same Species, however they are representatives of different species.
How to solve this issue?
More in general, with dynamic thresholding, how to make sure species management across generations is consistent? E.g. NEAT paper also says:

If the maximum fitness of a species did not improve in 15 generations,
  the networks in the stagnant species were not allowed to reproduce.

How to make sure that across all 15 generations, we are still considering that same single species and this has not drastically changed (so that they are actually different 'objects'?). E.g. in the example above, if A and B are both placed in Species 1 in Generation 21, Species 2 no longer represents what it represented in Generation 20.
","['genetic-algorithms', 'neat']",
Reinforcement learning for segmenting the robot path to reflect the true distances,"
I have a grid of rectangles acting as blocks. The robot traverses through the inter-spaces between these consecutive blocks. Now I have sensor data streaming in representing Right and left wheel speeds. Based on the differences in the speeds of the left and right wheels, I infer the robot's position and the path it has threaded. I get the associated individual segments of the total distance when it travels straight, left, or right.
These distances are a function of the actual speed of the robot and the time interval elapsed before the end of that activity. These computed distances for the segments though don't map and fit-in well when projected on the grid layout of the environment. The segments are rather not adhering to the boundary limitations.
I wanted to know if I can use RL to force the calculated distances to fit in with the layout given certain knowledge (or conditions, if you will): the start and end position of the robot and the inter-space distances.
If not RL, do you know how can I solve this problem? I suspect my function computing the distances is off and wondering if RL can help me figure out the right mapping of sensor data to the path traveled adhering to the grid layout dimensions.

If you consider the illustration above you will notice S, D, and D' signifying the starting position, the true destination, and the destination location computed by adding together the calculated distances for each of the segments representing right(r), left(l) and straight(s) along the path towards the destination. Inter-space length is given 7m and dimensions of the blocks are (27m x 15m). If you look at the data presented on the left side you will notice 18m left and consecutive 24m right represents the activity, in the grid, as the passage through the blocks. Granted -- perhaps the car negotiates the edges and corners through this passage in a protracted left(l) and right(r) movements, without necessarily going straight(s) straddling and linking the turns as one would expect. 
The question arises, however, when taken into account these individual segment lengths and stitch them together you end up in a destination, not in the ballpark range of the expected value. How can we design this problem so as to employ RL methods to, sort of, impose these grid dimensional constraints on this distance calculation methodology to yield better results? Or, probably best to re-imagine the whole problem so it is amenable to the application of RL. 
Any advice/ insights would be appreciated.
","['machine-learning', 'reinforcement-learning', 'robotics']",
How should I label the classes in RNA?,"
I have a project, which is the keyboard biometrics of users.
suppose I have 3 users, 
I do not know how to label in two types of class, (+ 1, -1). 
If I want to verify the identity to user1, my idea of ​​class designation would be:
       TIMES                LABEL
user 1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1

user 2
0.1  3.2  1.0  1.2  1.7      -1
3.4  1.2  3.0  1.1  2.8      -1
2.4  2.2  3.0  1.6  2.9      -1
1.4  3.2  2.0  2.6  3.6      -1
3.4  0.2   3.0  2.7  3.5     -1

user N
0.2  1.4  4.5  3.7  2.9      -1
9.2  1.5  7.6  2.6  2.6      -1
9.3  1.6  7.5  2.9  3.4      -1
9.8  3.8  6.6  2.8  2.5      -1
9.8  2.8  1.7  3.8  1.6      -1

but as my system has more and more users classes -1 will be too many compared to classes +1,
How should I label the classes?
",['neural-networks'],
"In the inception neural network, how is an image of shape $224 \times 224 \times 3$ converted into one of shape $112 \times 112 \times 64$?","
According to the original paper on page 4, $224 \times 224 \times 3$ image is reduced to $112 \times 112 \times 64$ using a filter $7 \times 7$ and stride $2$ after convolution.

$n \times n = 224 \times 224$
$f \times f = 7 \times 7$
stride: $s = 2$
padding: $p = 0$

The output of the convolution is $(((n+2p-f)/s)+1)$ (according to this), so we have $(n+2p-f)=(224+0-7)=217$, then we divide by the stride, i.e. $217/2=108.5$ (taking the lower value), then we add 1, i.e. $118+1=119$.
How do we get an output image of $112$ now?
","['neural-networks', 'convolutional-neural-networks', 'pooling', 'convolution-arithmetic', 'inception']",
Is 1mb an acceptable memory size for images being trained in a CNN?,"
I am using Tensorflow CNN to build an image classification/prediction model. Currently all the images in the dataset are each about 1mb in size.
Most examples out there use very small images.
The image size seems large, but I not too sure. 
Any thoughts on the feasibility of 1mb images? If not what can I do to compress programmatically?
","['convolutional-neural-networks', 'image-recognition', 'training', 'tensorflow', 'datasets']","1 MB by images is too much. It means that you have a lot of pixels to compute in the inputs, and your images have more features that are not very useful for the classification (we human don't need high resolution images for recognize objects, it's the same for model)It means that maybe you need also a more deeper network, and then, more computation.In your particular case, you have a dataset of trees, so images are somewhat similar and then, maybe the classification is harder. So, images need enough information in pixels to have a good prediction.You should resize your whole datsets to more common size because 1mb are too much (I don't know the resolution, but I think it could be around thousand for both dimension). 
Imagenet have images of size 224x224, and there are 1000 classes, which some of them are close. For examples, there are many dogs classes (boxer, american terrier, ...), cats classes (tiger, egyptian, ....), thus I think size of a few hundred would be enough. 
In your comment you said you will try with 336x252, I think it can be a good start.
Of course you will need some experimentation in this regard. Maybe you can try to train some models in a few epoches with different image sizes, to see the best one, and keep it to train the model further !"
How to handle rectangular images in convolutional neural networks?,"
Almost all the convolutional neural network architecture I have come across have a square input size of an image, like $32 \times 32$, $64 \times 64$ or $128 \times 128$. Ideally, we might not have a square image for all kinds of scenarios. For example, we could have an image of size $384 \times 256$
My question is how to we handle such images during

training,
development, and
testing

of a neural network?
Do we force the image to resize to the input of the neural network or just crop the image to the required input size?
","['neural-networks', 'convolutional-neural-networks', 'fully-convolutional-networks']",
Can the recurrent neural network's input come from a short-time Fourier transform?,"
Can the recurrent neural network input come from a short-time Fourier transform? I mean the input is not from the time-series domain.
","['recurrent-neural-networks', 'sequence-modeling', 'fourier-transform']",
NEAT + Keras : reproducibility problem (World Models implementation),"
I'm trying to apply the World Models architecture to the Sonic game (using the gym-retro library).
My problem concerns the evolutionnary algorithm part that I use as the controller (worldmodels = auto encoder + RNN + controller).
I'm using a genetic algorithm called NEAT (I use the neat-python library). I am searching for someone who can help me with the neat-python implementation.
Here is the method that runs a generation :
python
    best_genome = pop.run(popEvaluator.evaluate_genomes, 1)

Currently, all the individuals of the population are evaluated on the first level of Sonic The HedgeHog.
The ""run"" method should return the best genome of the population based on their performance on this level.
Then, I use this best genome to re-create the associated neural network in order to run it in the same level.
I was expected to see the exact same run as the best individual, but this is not the case.
Sometimes it does, sometimes not. 
There are not a lot of examples with NEAT and I based my code on this one from the official documentation.
Here is my own implementation, if you want to check.
If anybody has already used NEAT, help would be welcome !
","['neural-networks', 'genetic-algorithms', 'keras', 'evolutionary-algorithms', 'neat']",
How does a single hidden layer affect output? [duplicate],"







This question already has answers here:
                                
                            




What is the purpose of the hidden layers?

                                (4 answers)
                            

Closed 3 years ago.



I'm learning about multilayer perceptrons, and I have a quick theory question in regards to hidden layer neurons. 
I know we can use two hidden layers to solve a non-linearly separable problem by allowing for a representation with two linear separators. However, we can solve a non-linearly separable problem using only one hidden layer. 
This seems fine, but what kind of representation does one hidden layer add? And how is the output of the network affected?
I've drawn a diagram of a multilayer perceptron with one hidden layer neuron. I used this same layout to solve a non-linearly separable problem. The single hidden layer node is inside the red square. Forgive my poor MS-Paint skills.

","['neural-networks', 'machine-learning', 'hidden-layers', 'multilayer-perceptrons']","When we add a single layer with a non-linear activation function, right after the application of the activation function, a new basis function is found (for that neuron), which is some combination of the weights and biases, which acts as a new way to view or analyze the feature sets. With an increasingly deeper network, we keep finding representations, which are new basis vectors of the combination of the previous layer's weights and biases, that is, higher-level representations. If they're error-free, they'll give better performances, but if small errors creep in earlier basis vectors, the error increases through depth.A nice analogy is the Taylor series, where $1$, $x$, $x^2$, and so on, are the basis vectors for estimating the function in 1D."
How to detect a Neural Network will work with the whole dataset?,"
I want to implement a neural network on a big dataset. But training time is long (~1h30 per epoch). I'm still in the development process, so I don't want to wait such long time just to have poor results at the end.
This and this suggest that overfitting the network on a very small dataset (1 ~ 20 samples) and reach a loss near 0 is a good start.
I did it and it works great. However, I am looking for the next step of validating my architecture. I tried to overfit my network over 100 samples, but I can't reach a loss near 0 in reasonable time.
How can I ensure the results given by my NN will be good (or not), without having to train it on the whole dataset ?
","['neural-networks', 'machine-learning', 'deep-learning']","You can try to train it on 1% data, then on 2%, 3%, etc... Then plot it and see if increasing data increases the performance and how it is changing. Not sure if that's correct answer, but at least you can iterate this method pretty fast."
Is it meaningful to give more weight to the result of monte carlo search with less turn win?,"
I'm programming on Connect6 with MCTS.
Monte Carlo Tree Search is based on random moves. It counts up the number of wins in certain moves. (Whether it wins in 3 turns or 30 turns)
Is the move with less turns more powerful than the move with more turns?(as mcts just sees if it's win or not -- not considering the number of turns it took to win) And if so, is it meaningful to give bigger weight to the one with less turn win?
","['game-ai', 'monte-carlo-tree-search']",
Why do you not see dropout layers on reinforcement learning examples?,"
I've been looking at reinforcement learning, and specifically playing around with creating my own environments to use with the OpenAI Gym AI. I am using agents from the stable_baselines project to test with it.
One thing I've noticed in virtually all RL examples is that there never seems to be any dropout layers in any of the networks. Why is this?
I have created an environment that simulates currency prices and a simple agent, using DQN, that attempts to learn when to buy and sell. Training it over almost a million timesteps taken from a specific set of data consisting of one month's worth of 5-minute price data it seems to overfit a lot. If I then evaluate the agents and model against a different month's worth of data is performs abysmally. So sounds like classic overfitting.
But is there a reason why you don't see dropout layers in RL networks? Is there other mechanisms to try and deal with overfitting? Or in many RL examples does it not matter? e.g. there may only be one true way to the ultimate high score in the 'breakout' game, so you might as well learn that exactly, and no need to generalise?
Or is it deemed that the chaotic nature of the environment itself should provide enough different combinations of outcomes that you don't need to have dropout layers?
","['machine-learning', 'reinforcement-learning', 'overfitting', 'dropout']","Dropout essentially introduces a bit more variance. In supervised learning settings, this indeed often helps to reduce overfitting (although I believe there dropout is also already becoming less.. fashionable in recent years than in the few years before that; I'm not 100% sure though, it's not my primary area of expertise).In Reinforcement Learning, additional variance is not really what we're looking for. There already tends to be a large amount of variance in the learning signals that we get, and this variance already tends to be a major issue for learning stability and/or learning speed. For example:Many important parts of Deep RL algorithms (without which our training processes empirically turn out to destabilize and break down) are very much tailored towards reducing that variance. For example, Target Networks in DQN were introduced specifically to reduce the moving target problem. From this point of view, it's not surprising that if we were to add more artificial variance through other means again (such as dropout), that this would hurt performance / destabilize learning.Is there other mechanisms to try and deal with overfitting? Or in many RL examples does it not matter? e.g. there may only be one true way to the ultimate high score in the 'breakout' game, so you might as well learn that exactly, and no need to generalise?In the majority of current (Deep) Reinforcement Learning research, overfitting is indeed not viewed as a problem. The vast majority of RL research consists of training in one environment (for example Cartpole, or Breakout, or one particular level in Pacman, or navigating in one specific maze, etc.), and either constantly evaluating performance during that learning process, or evaluating performance after such a learning process in the same environment.If we were to compare that evaluation methodology to what happens in supervised learning... we are basically evaluating performance on the training set*. In supervised learning, this would be absolutely unacceptable, but in RL it is very much treated as acceptable and more rule than exception. Some say this is simply a problem in current RL research, something that needs to change. It could also be argued that it's not necessarily a problem; if we really are able to train the agent in precisely the same environment that we wish to deploy it in later... well, then what's the problem with it overfitting to that environment? So, when we're using the evaluation methodology described above, indeed we are overfitting to one specific environment, but overfitting is good rather than bad according to our evaluation criteria. It is clear that this methodology does not lead to agents that can generalize well though; if you consistently train an agent to navigate in one particular maze, it will likely be unable to navigate a different maze after training.*Note: the truth, in my opinion, is slightly more nuanced than that we are really ""evaluating on the training set"" in RL. See, for example, this nice thread of tweets: https://twitter.com/nanjiang_cs/status/1049682399980908544I have created an environment that simulates currency prices and a simple agent, using DQN, that attempts to learn when to buy and sell. Training it over almost a million timesteps taken from a specific set of data consisting of one month's worth of 5-minute price data it seems to overfit a lot. If I then evaluate the agents and model against a different month's worth of data is performs abysmally. So sounds like classic overfitting.Note that your evaluation methodology described here indeed no longer fits the more ""common"" evaluation methodology. You have a problem with concept drift, with nonstationarity in the environment. This means overfitting may be a problem for you. Still, I'm not sure if dropout would help (it's still additional variance which may hurt). First and foremost, you'd want to make sure that there's some way to keep track of the time / month in your inputs, such that you'll at least have a chance of learning a policy that adapts itself over time. If you have a clear, solid boundary between ""training phase"" and ""evaluation phase"", and you know that concept drift occurs across that boundary (you know that your environment behaves differently in the training phase from the evaluation phase)... you really don't have much hope of learning a policy only from experience in the training phase that still performs well in the evaluation phase. I suspect you'll have to get rid of that clear, solid boundary. You'll want to keep learning throughout the evaluation phase as well. This enables your learning algorithm to actually collect experience in the changed environment, and adapt to it."
Huge variations in epoch count for highest generalized accuracy in CNN,"
I have written my own basic convolutional neural network in Java as a learning exercise. I am using it to analyze the MIT CBCL face database image set. They are a set of 19x19 pixel greyscale images.
Network specifications are:
Single Convolution Layer with 1 filter:
Filter Size: 4x4.
Stride Size: 1
Single Pooling Layer
2x2 Max Pooling
3 layer MLP(input, 1 hidden and output)
input = 64 neurons
hidden = 15 neurons
output = 2 neurons
learning rate = 0.1
Now I am getting reasonable accuracy(92.85%), but my issue is that it is being achieved at very different points in the epoch count across network runs:
Epochs  Training Accuracy   Test Accuracy   Validation Accuracy

Run 1   415 93.13   92.44   93.35
Run 2   515 92.44   93.18   92.84
Run 3   327 93.83   92.05   92.38
I am using the Java random class with the same seed for every run to initialize the kernel, the MLP weights and break the input data into 3 sets.(training is being done using the 33-33-33 method)
I am a loss as to what is causing this variation in epoch count to achieve the highest point in validation accuracy. Can anybody explain this?
","['convolutional-neural-networks', 'java']","Fixed. Was an issue with the random generator. In my class for the Neuron layer where I initialize the weights I get new doubles from the generator for each of the initial weight values, but I found a bug where I was re-initializing the random generator, which was of course causing different values."
Keywords to describe people counting from a camera?,"
The subject matter is to count the number of people in a large room, wherein a camera is placed in a very high ceiling: an example would be Grand Central Station.   Faces are not visible: the scalp (top of the head) is visible to the camera as shown in the link's video.  
The goal: I would like to perform a Google literature search to assess the work that has been performed on overhead head recognition,  however, I am not sure what the best  keyword pairs: (scalp? head? people?) to describe the object that is to be recognized from a camera positioned in the ceiling (overhead? bird's eye? satellite?).  I'd like the search to return leading-edge (AI) techniques that benchmark results
",['object-recognition'],
Is it suitable to find inverse of last layer's activation function and apply it on the target output?,"
I have a neural network with the following structure:

I am expecting specific outputs from the neural network which are the target values for my training. Let's say the target values are 0.8 for the upper output node and -0.3 for the lower output node.
The activations function used for the first 2 layers are ReLu or LeakyReLu while the last layer uses atan as an activation function.
For back propogation, instead of adjusting values to make the network's output approach 0.8,-0.3. is it suitable if I use the inverse function for atan -which is tan itself- to get ""the ideal input to the output layer multiplied by weights and adjusted by biases"". 
The tan of 0.8 and -0.3 is 0.01396 and -0.00524 approximately. 
My algorithm would then adjust weights and biases of the network so that the ""pre-activated output"" of the output layer -which is basically (sum(output_layer_weight*output_layer's inputs)+output_layer_biases)- approaches 0.01396 and -0.00524.
Is this suitable 
","['neural-networks', 'activation-functions']","For the above stated artificial network, these two training scenarios are similar.Distinctions between them include these."
What's the difference between poker with public cards and without them?,"
Example: Texas Holdem poker vs Texas Holdem poker with the same rounds, just with no public cards dealt.
Would algorithms, like CFR, approximate the Nash equilibrium more easily? Could AI that does not look at public cards achieve similar performance in normal Texas Holdem as AI that looks at public state tree?
","['game-ai', 'poker']","It depends a little on what you mean by ""the same rounds, just with no public cards dealt.""If you mean that each player will just be dealt 2 cards, and no public cards exist, then really we're playing a sort of ""high card"" game. The best hand is just a pair of aces, CFR will solve this quickly, because the number of possible game states is extremely small compared to a full poker game (especially if we exploit the symmetry of suits, since flushes aren't possible).If you mean that each player will be dealt 5 cards, with several rounds of betting as before, CFR will probably do less well. The state space will be larger, since there are more cards in play (10 instead of 9). Betting may become more complex, and more complex betting expands the state space enormously. If you mean that the cards are dealt as before, but the program simply will not look at the cards, then you've kept the state space of the game the same size, but radically reduced the number of information sets. Playing against an opponent who can look at the cards on the table, your program would be at an enormous disadvantage. For instance, imagine the cards on the table are ""3 3 8 8 5"", and that you have a pair of 2's in your hand. You would want to play very differently from if the cards on the table were ""2 2 10 4 7"", but an AI without access to the table cards would have to act the same in both situations. "
How do I calculate the gradient of the hinge loss function?,"
With reference to the research paper entitled Sentiment Embeddings with Applications to Sentiment Analysis, I am trying to implement its sentiment ranking model in Python, for which I am required to optimize the following hinge loss function: 
$$\operatorname{loss}_{\text {sRank}}=\sum_{t}^{T} \max \left(0,1-\delta_{s}(t) f_{0}^{\text {rank}}(t)+\delta_{s}(t) f_{1}^{\text {rank}}(t)\right)$$
Unlike the usual mean square error, I cannot find its gradient to perform backpropagation.
How do I calculate the gradient of this loss function?
","['neural-networks', 'objective-functions', 'backpropagation', 'math', 'gradient-descent']",
"What are the meanings of these (P(x;y), P(x;y,z),P(x,y;z))?","
I was reading a machine learning book that uses probabilities like these:
$P(x;y), P(x;y,z), P(x,y;z)$
I couldn't find what they are and how can I read and understand them?
Apart from the context, I saw one of these probabilities on it here:

","['machine-learning', 'probability']",
Which evaluation methods can I use for image segmentation?,"
I implemented an image segmentation pipeline and I trained it on the DICOM dataset. I compared the results of the model with manual segmentation to find the accuracy. Is there other methods for evaluation? 
","['computer-vision', 'datasets', 'image-segmentation']",
What are the advantages and disadvantages of using LISP for constraint satisfaction in 3D space,"
We are currently working on developing a 3D modeling software that allows designers to set spatial constraints to models. The computer then should generate a 3D mesh conforming to these constraints.
Why should or shouldn't we use Lisp for the constraint satisfaction part? Will Prolog environment be any better? Or should we stick to C/C++ libraries?
One requirement we have is that we want to use the Unity Game Engine as it has a lot of 3D tools built in
","['optimization', 'programming-languages', 'lisp', 'prolog']",
How to create Partially Connected NNs with prespecified connections using Tensorflow?,"
I'd like to implement a partially connected neural network with ~3 to 4 hidden layers (a sparse deep neural network?) where I can specify which node connects to which node from the previous/next layer. So I want the architecture to be highly specified/customized from the get-go and I want the neural network to optimize the weights of the specified connections, while keeping everything else 0 during the forward pass AND the backpropagation (connection does not ever exist).
I am a complete beginner in neural networks. I have been recently working with tensorflow & keras to construct fully connected deep networks. Is there anything in tensorflow (or something else) that I should look into that might allow me to do this? I think with tf, I should be able to specify the computational graph such that only certain connections exist but I really have no idea yet where to start from to do this...
I came across papers/posts on network pruning, but it doesn't seem really relevant to me. I don't want to go back and prune my network to make it less over-parameterized or eliminate insignificant connections. 
I want the connections to be specified and the network to be relatively sparse from the initialisation and stay that way during the back-propagation.
","['neural-networks', 'machine-learning', 'deep-learning', 'deep-neural-networks', 'feedforward-neural-networks']",
"How are ""lags"" and ""exogenous factors"" accounted for in reinforcement learning?","
In reinforcement learning, the system sets some controllable variables, and then determines the quality of the result of the dependent variable(s); using that ""quality"" to update the algorithm.
In simple games, this works fine because for each setting there is a single result.
However, for the real world (e.g. an airflow system), the result takes some time to develop and there is no single precise ""pair"" result to the conditions set.  The flow change takes time and even oscillates a bit as flow stabilizes to a steady-state.
In practical systems, how is this ""lag"" accounted for?  How are the un-settled (false) results ignored?  How is this noise distinguished from exogenous factors (un-controlled system inputs e.g. an open window exposed to wind)?
","['machine-learning', 'reinforcement-learning', 'applications', 'credit-assignment-problem']",
Is known math really enough for AI,"
As an Electronics & Communication Engineering student I've heard some stories and theories about ""The math we have is not enough to complete a thinker-learner AI.""
What is the truth? Is humankind waiting for another Newton to make new calculus or another Einstein-Hawking to complete the quantum mechanics?
If so, what exactly do we need? What will we call it? 
","['machine-learning', 'agi', 'math']","Maybe.AI has a long history of encountering mathematical impossibilities and then working around them already. While the individuals who solved these problems don't get as much press as Newton, Einstein, or Hawking, a case could be made that their contributions to human knowledge are on a similar scale. Unfortunately, their results don't relate to physical systems, so they can be harder to explain to the layperson. Something to keep in mind is that your question assumes the correctness of the ""Great Man Theory"" of science history, which holds that science advances by the efforts of exceptional people, and that we need to wait around for more such people to appear for science to advance. This view of scientific history is overly simplistic, and probably wrong. For example, most (all?) of Newton's discoveries were likely to have been made by someone else, around the same time, if he didn't make them (see, e.g. Leibnitz, who actually did discover calculus at the same time), so a better view might be of a large community of researchers who gradually develop more advanced models based on each others' work.To answer your question, I've listed out some past examples of problems that were overcome, and some outstanding problems that might require the development of new mathematical tools to solve properly. Keep in mind though, that we can't know for sure whether new tools are needed: maybe existing tools are sufficient, but no one has applied them in the right way yet!Some past problems that plagued AI and required the development of better mathematical approaches:The combinatorial explosion problem appeared to preclude having an AI system reason about probabilities and causation in a logically correct way. This problem was solved by the development of Bayesian networks and causal networks, with the work being led by Judea Pearl and his students. Pearl won a Turing award for this, but has received very little coverage in the popular press.Many optimization problems that AI systems need to solve are NP-hard. This means that no general purpose algorithms exist that can give exact solutions to these problems in a polynomial number of steps. Because of this, the problems were initially viewed as intractable. The development of PTAS algorithms, and a deep understanding of phase-changes in NP-Hard problems (pioneered by Cheeseman et al.) led to an ability to identify exactly what makes these problems hard, to identify which subparts cause the hardness, and to practical algorithms that solve all sorts of problems in these domains. Much effort in AI was spent designing new classification methods, and arguing about why one might or might not be better than another. The No Free Lunch Theorems, along with other work in computational learning theory provided a clear mathematical framework for understanding how systems can learn, and where their limits are. As a more recent example, games involving chance, like Poker, have state spaces so vast that they cannot be searched through effectively, even with heuristics. Phrasing these games in the language of Game Theory, and proving the convergence of the counter-factual minimax regret algorithm provided solutions. Bowling et al. were at the center of this work. It got some press coverage, but most of the coverage focused on ""computers can play poker"", and not the exiting, more generalizable, lessons AI researchers had learned in the process.Some problems that are still outstanding, and might require new mathematical techniques:Are there efficient algorithms for solving problems in the complexity classes PPAD, NP, or #P? Many AI problems fall into these categories. We do not know the answer definitively, and although most researchers suspect that no such algorithms exist, understanding why not seems like it could also provide major research advantages. Existing proof techniques do not seem likely to crack this problem.We have no good mathematical models of subjective experience. While some researchers (like Churchland) think this is not likely to play a major role in the development of intelligent systems, a concrete model could either support or refute that view, and might provide solid frameworks for current problems in AI like the study of motivation. The original project of the social sciences was to provide mathematical models of individual humans, and then of human societies. This has mostly been abandoned (Psych is still at it, but Economics prefers to study rational agents instead of humans, and most of the others gave up mathematical modeling entirely to pursue the methodologies of the humanities instead of the sciences). Nonetheless, the lack of sound mathematical descriptions of human behaviours is becoming a major topic within AI, with work on norms, trust, emotion, and other topics. If a mathematical framework were developed to describe the actions of human societies or of individual humans, then AI would be advanced significantly (along with many other fields!)."
Is the bias supposed to be updated in the perceptron learning algorithm?,"
I am using the following perceptron formula $\text{step}\left(\sum(w_ix_i)-\theta \right)$.
Is $\theta$ supposed to be updated in a perceptron, like the weights $w_i$? If so, what is the formula for this?
I'm trying to make the perceptron learn AND and OR, but without updating $\theta$, I don't feel like it's possible to learn the case where both inputs are $0$. They will, of course, be independent of the weights, and therefore the output will be $\text{step}(-\theta)$, meaning $\theta$ (which has a random value) alone will determine the output.
","['neural-networks', 'machine-learning', 'perceptron']","I found the answer to my question. Treat $\theta$ as a normal weight, associated with an input that always equals $-1$."
Traffic signs dataset [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I'm looking for annotated dataset of traffic signs. I was able to find Belgium, German and many more traffic signs datasets. The only problem is these datasets contain only cropped images, like this:

While i need (for YOLO -- You Only Look Once network architecture) not-cropped images.

I've been looking for hours but didn't find dataset like this. Does anybody know about this kind of annotated dataset ?
EDIT:
I prefer European datasets.
","['neural-networks', 'convolutional-neural-networks', 'datasets']","Direct AnswerThe Belgium TS Dataset may be helpful, as well as The German Traffic Sign Detection Benchmark.Additional Notes Based on Question Author's IdeaThe idea in the question author's addendum of placing signs onto street sides and corners is a good one, but to do it repeatably and in a way that doesn't bias the training is its own research project.  However, it is a good research direction.  What would be of benefit to AV researchers worldwide is a multi-network topology and equilibrium strategy with the objective to create the following data generation features.This is obviously not a basic data hygiene problem. It is its own AI project, but the return on this research project in terms of furthering the AV technology is immense and may have significant data set statistical advantages over collecting data from the vendors that supply images to Google maps and other Big Data aggregators."
How do I segment each part of a DICOM image?,"
As I'm beginner in image processing, I am having difficulty in segmenting all the parts in DICOM image. 
Currently, I'm applying watershed algorithm, but it segments only that part that has tumour. 
I have to segment all parts in the image. Which algorithm will be helpful to perform this task?
The image below contains the tumour.

This image is the actual DICOM image

","['computer-vision', 'matlab', 'image-segmentation']",
Using a DQN with a variable amount of Valid Moves per turn for a Board Game [duplicate],"







This question already has answers here:
                                
                            




How should I handle invalid actions (when using REINFORCE)?

                                (5 answers)
                            

Closed last year.



I have created a game on an 8x8 grid and there are 4 pieces which can move essentially like checkers pieces (Forward left or Forward right only). I have implemented a DQN in order to pull this off. 
Here is how I have mapped my moves:
self.actions = {""1fl"": 0, ""1fr"": 1,""2fl"": 2, 
  ""2fr"": 3,""3fl"": 4, ""3fr"": 5,""4fl"": 6, ""4fr"": 7}

essentially I assigned each move to an integer value from 0-7 (8 total moves).

My question is: During any given turn, not all 8 moves are valid, how do I make sure when model.predict(state) the resulting prediction will be a move that is valid? Here is how I am currently handling it.

def act(self, state, env):
    #get the allowed list of actions
    actions_allowed = env.allowed_actions_for_agent()

    #Do a random move if random # greater than epsilon
    if np.random.rand(0,1) <= self.epsilon: 
        return actions_allowed[random.randint(0, len(actions_allowed)-1)]

    #get the prediction from the model by passing the current game board
    act_values = self.model.predict(state)

    #Check to see if prediction is in list of valid moves, if so return it
    if np.argmax(act_values[0]) in actions_allowed:
        return np.argmax(act_values[0])

    #If prediction is not valid do a random move instead....
    else:
        if len(actions_allowed) > 0:
            return actions_allowed[random.randint(0,len(actions_allowed)-1)]

I feel like if the agent predicts a move, and if that move is not in the actions_allowed set I should punish the agent.
But because it doesn't pick a valid move I make it do a random one instead, but I think this a problem. Because its bad prediction may ultimately end up still winning the game since the random move may have a positive outcome. I am at a total loss. The agent trains....but it doesn't seem to learn.... I have been training it for over 100k games now, and it only seems to win 10% of it games.... ugh. 
Other helpful information:
- I am utilizing experience replay for the DQN which I have based on the code from here:
Here is where I build my model as well:
self.action_size = 8
LENGTH = 8
def build_model(self):
    #builds the NN for Deep-Q Model
    model = Sequential() #establishes a feed forward NN
    model.add(Dense(64,input_shape = (LENGTH,), activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(self.action_size, activation = 'linear'))
    model.compile(loss='mse', optimizer='Adam')

","['reinforcement-learning', 'game-ai', 'python', 'keras', 'dqn']",
Why did MYCIN fail?,"
I've been reading about expert systems and started reading about MYCIN.
I was astonished to find that MYCIN diagnosed patients better than the infectious diseases physicians.
http://www.aaaipress.org/Classic/Buchanan/Buchanan33.pdf
Since, it had such a good success rate, why did it fail?
",['expert-systems'],
What is the name of an AI system that learns by trial and error?,"
Imagine a system that controls dampers in a complex vent system that has an objective to perfectly equalize the output from each vent. The system has sensors for damper position, flow at various locations and at each vent. The system is initially implemented using a rather small data set or even a formulaic algorithm to control the dampers. What if that algorithm were programmed to ""try"" different configurations of dampers to optimize the air flows, guided broadly by either the initial (weak) training or the formula? The system would try different configurations and learn what improved results, and what worsened results, in an effort to reduce error (differential outflow).
What is that kind of AI system called? What is that system of learning called? Are there systems that do that currently?
","['machine-learning', 'reinforcement-learning', 'terminology', 'incremental-learning', 'ai-field']","I believe ""Reinforcement Learning"" is the term you are looking for (as mentioned by others as well) but keep in mind that the scope of your problem falls under the section of AI that is called Search.Search algorithms are based upon experimenting with different actions (decisions) and selecting the one that minimizes an arbitrary cost function (reward), given the current and past problem states."
"What is the AI discipline where an algorithm learns from an initial training set, but then refines its learning as it uses that training?","
Imagine a system that is trained to manipulate dampers to manage air flow. The training data includes damper state and flow characteristics through a complex system of ducts. The system is then given an objective (e.g. maintain even flow to all outputs) and set loose to manage the dampers. As it performs those functions there are anomalies in the results which the system is able to detect. The algorithm CONTINUES to learn from its own empirical data, the result of implemented damper configurations, and refines its algorithm to improve performance seeking the optimum goal of perfectly even flow at all outputs.
What is that kind of learning or AI system called?
","['machine-learning', 'reinforcement-learning', 'training', 'terminology']","I believe this can best be done with reinforcement learning via Deep Q Learning. That's where I would start. Steps are:Initialize a Q table.Choose an action.Perform the action.Measure the reward.Update the Q.A neural net will approximate the Q function. See: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0Also consider policy gradients, actor critic, and inverse reinforcement learning."
Can you help me understand how weight normalization works?,"
I am trying to dissect the paper Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.
Unfortunately, because my math is a little bit rusty, I got a little bit stuck with the proof. Could you provide me with some clarification about proof of the topic?
What I understand is that we introduce, instead of the weight vector $w$ a scalar $g$ (magnitude of original $w$?) and $\frac{v}{\|v\|}$ (direction of original $w$?) of the vector.
$$\nabla_{g} L=\frac{\nabla_{\mathbf{w}} L \cdot \mathbf{v}}{\|\mathbf{v}\|}$$
and
$$\nabla_{\mathbf{v}} L=\frac{g}{\|\mathbf{v}\|} \nabla_{\mathbf{w}} L-\frac{g \nabla_{g} L}{\|\mathbf{v}\|^{2}} \mathbf{v}$$
What I am not really sure about is:
If the gradients are noisy (does this mean that in some dimension we have small and in some high curvature or that error noise differs for very similar values of $w$?) the value will quickly increase here and effectively limit the speed of descent by decreasing value of ($\frac{g}{\|v\|}$). This means that we can choose larger learning rates and it will somehow adjust the effect of the learning rate during the training.
And what I completely miss is:
$$\nabla_{\mathbf{v}} L=\frac{g}{\|\mathbf{v}\|} M_{\mathbf{w}} \nabla_{\mathbf{w}} L$$
with
$$\mathrm{M}_{\mathrm{w}}=\mathrm{I}-\frac{\mathrm{w} \mathrm{w}^{\prime}}{\|\mathrm{w}\|^{2}}$$
It should somehow explain the reasoning behind the idea of the final effects. Unfortunately, I don't really understand this part of the paper, I probably lack some knowledge about Linear Algebra.
Can you verify that my understanding of the paper is correct?
Can you recommend some sources (books/videos) to help me to understand the second part of proof (related to the second set of formulas?)
","['neural-networks', 'deep-learning', 'math', 'papers', 'weight-normalization']","If the gradients are noisy (does this mean that in some dimension we have small and in some high curvature or that error noise differs for very similar values of w?)Gradients being noisy means that they are ""inconsistent"" across different epochs / training steps. With that I mean that they'll sometimes point in one direction, later in a different (maybe the opposite) direction, etc., that the gradients at different time steps give inconsistent/conflicting information, that we don't consistently keep following the same direction through gradient descent but keep jumping all over the place. Note that this is across epochs, it's not across different dimensions within the same epoch.So, for example, we might have a vector of gradients that looks like $[1, 1, 1, \dots, 1]$ at one time step (i.e., positive gradients in all dimensions), and the next training step get a gradient more like $[-1, -1, -1, \dots, -1]$ (this example is pretty much the ""most extreme"" kind of noise you can have, completely conflicting directions, in reality it'd generally be less extreme).Such ""noisy gradients"" can exist because, in practice, we almost always use estimates of the gradient of our loss function, rather than the true gradients (which we'll generally not even be able to compute). For example, if we have a very large training dataset in a supervised learning setting, we'd ideally use the gradient of the loss function computed across the complete dataset. That tends to be computationally expensive (requires forwards and backwards passes for every single instance in the dataset), so in practice we'll often only use a small minibatch to estimate the gradient; this can result in widely different estimates of the gradient for different minibatches. (note: there can also be different reasons for using minibatches rather than full dataset, such as avoiding overfitting, but that's not too important to consider for this specific question)A different example of a setting where we can have noisy gradients is in Deep Reinforcement Learning. There, computing the loss function itself is often rather noisy (in the sense that our own predictions, which tend to still be incorrect during the training phase, are a component of the targets that we're updating towards), so our estimates of the gradients will also be noisy.Derivation for your second question, about the gradient of the loss with respect to $\mathbf{v}$:We start with the following, from Equation (3) in the paper (this gradient you understand, right?):$$\nabla_{\mathbf{v}} L = \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g \nabla_g L}{\vert \vert \mathbf{v} \vert \vert^2} \mathbf{v}$$Note that Equation (3) also gives:$$\nabla_{g} L = \frac{\nabla_{\mathbf{w}} L \cdot \mathbf{v}}{\vert \vert \mathbf{v} \vert \vert}$$If we plug that into the previous Equation we get:\begin{aligned}
\nabla_{\mathbf{v}} L &= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g \frac{\nabla_{\mathbf{w}} L \cdot \mathbf{v}}{\vert \vert \mathbf{v} \vert \vert}}{\vert \vert \mathbf{v} \vert \vert^2} \mathbf{v} \\
&= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g \nabla_{\mathbf{w}} L \cdot \mathbf{v}}{\vert \vert \mathbf{v} \vert \vert^3} \mathbf{v} \\
\end{aligned}The $\frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L$ term that shows up before the minus can also be isolated in the term after the minus:\begin{aligned}
\nabla_{\mathbf{v}} L &= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L \cdot \frac{\mathbf{v}}{\vert \vert \mathbf{v} \vert \vert^2} \mathbf{v} \\
&= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L \cdot \frac{\mathbf{v}}{\vert \vert \mathbf{v} \vert \vert} \cdot \frac{\mathbf{v}}{\vert \vert \mathbf{v} \vert \vert} \\
\end{aligned}Equation (2) tells us that $\mathbf{w} = \frac{g}{\vert \vert \mathbf{v} \vert \vert} \mathbf{v}$, which means that $\frac{\mathbf{w}}{g} = \frac{\mathbf{v}}{\vert \vert \mathbf{v} \vert \vert}$. Plugging this into the above leads to:\begin{aligned}
\nabla_{\mathbf{v}} L &= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L \cdot \frac{\mathbf{w}}{g} \cdot \frac{\mathbf{w}}{g} \\
\end{aligned}Below Equation (2), they also explain that $\vert \vert \mathbf{w} \vert \vert = g$, so we can rewrite the above to:\begin{aligned}
\nabla_{\mathbf{v}} L &= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L \cdot \frac{\mathbf{w}}{\vert \vert \mathbf{w} \vert \vert} \cdot \frac{\mathbf{w}}{\vert \vert \mathbf{w} \vert \vert} \\
&= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L - \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L \cdot \frac{\mathbf{w} \mathbf{w}'}{\vert \vert \mathbf{w} \vert \vert^2} \\
\end{aligned}In that last step, the $'$ symbol in $\mathbf{w}'$ denotes that we transpose that vector.Finally, because we have that common $\frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L$ before and after the minus symbol, we can pull it out, multiplying it once by the identity matrix $\mathbf{I}$, and once by the remaining negated term. This is very similar to how you would simplify something like $a - ax$ to $a (1 - x)$ in ""standard"" algebra; the identity matrix $\mathbf{I}$ plays a very similar role here to the number $1$ in ""standard"" algebra:\begin{aligned}
\nabla_{\mathbf{v}} L &= \frac{g}{\vert \vert \mathbf{v} \vert \vert} \nabla_{\mathbf{w}} L \cdot \left( \mathbf{I} - \frac{\mathbf{w} \mathbf{w}'}{\vert \vert \mathbf{w} \vert \vert^2} \right) \\
\end{aligned}Note: in all of the above, I kind of neglected to pay attention to order of matrix/vector multiplications and their dimensions. Probably the $\nabla_{\mathbf{w}} L$ term should by now already have been moved to after those large brackets, instead of being before those brackets. In this case, it should work out fine regardless because $\mathbf{I}$ as well as $\mathbf{w} \mathbf{w}'$ are square and symmetric matrices. The only resulting difference is in whether you get a row or a column vector out at the end.This can now finally be rewritten as the Equation you mentioned that you didn't understand yet, Equation (4) in the paper."
Why is the short-time Fourier transform used for preprocessing audio samples?,"
I've been told this is how I should be preprocessing audio samples, but what information does this method actually give me? What are the alternatives, and why shouldn't I use them?
","['data-preprocessing', 'voice-recognition', 'fourier-transform']","Fourier transform is used to transform audio data to get more information (features).For example, raw audio data usually represented by a one-dimensional array, x[n], which has a length n (number of samples). x[i] is an amplitude value of the i-th sample point.Using the Fourier transform, your audio data will be represented as a two-dimensional array. Now, x[i] is a not a single value of amplitude, but a list of frequencies which compose original value at the i-th frame (a frame consists of a few samples).See the image below (from wikipedia), the red graph is an original value of n samples before transformed, and the blue graph is a transformed value of one frame."
Creating a self learning Mario Kart game AI?,"
I will be undertaking a project over the next year to create a self learning AI to play a racing game, currently the game will be Mario Kart 64.
I have a few questions which will hopefully help me get started: 

What aspects of AI would be most applicable to creating a self learning game AI for a racing game (Q-Learning, NEAT etc)
Could a ANN or NEAT that has learned to play Mario Kart 64 be used to learn to play another racing game? 
What books/material should i read up on to undertake this project? 
What other considerations should i take throughout this project? 

Thank you for your help! 
","['neural-networks', 'game-ai', 'python', 'autonomous-vehicles', 'neat']","What aspects of AI would be most applicable to creating a self learning game AI for a racing game (Q-Learning, NEAT etc)In general, you are looking at a problem that involves sequential decision making, in a machine learning context. If you are wanting to build an agent that can learn by receiving screen images, then NEAT cannot scale to that complexity directly. Although there might be clever combinations of deep learning and evolutionary algorithms you could apply, the most heavily explored and likely successful solution will be found in Deep Reinforcement Learning. Algorithms like DQN, A3C, A2C, PPO . . . there are dozens to consider, but all are based around agents using samples of experience to update functions that measure either the ""value"" of acting in a certain way (a policy) or estimate the best policy directly.Could a ANN or NEAT that has learned to play Mario Kart 64 be used to learn to play another racing game?Within limits, yes. You will have built a system that takes pixel inputs, and outputs controller messages. If you re-start training from scratch on any compatible N64 game (with same screen resolution and same controller outputs) there is a chance it could learn to play that new game well. As other driving games on the N64 are a subset of all games, and more similar to each other than, say, a scrolling shooter or adventure game, then an agent that can successfully learn one will likely learn another too.It is unlikely that a Mario Kart agent will immediately be good at another game without re-training. The visual and control differences will probably be too much. You could try though. An interesting experiment would be to take your trained agent, or some part of it, and see if starting with that improves learning time on a new game. This is called Transfer Learning.What books/material should i read up on to undertake this project?You will need at least the first parts of an introduction to Reinforcement Learning. If your goal is to aim to just make the agent, then you can skip theory-heavy parts, but within limits the more theory you understand, the easier it will be to change code features towards getting something working. I can suggest the following:The book Sutton & Barto, Reinforcement Learning: An Introduction - this covers theory behind basic RL, and the online draft is freeA course of lectures on Reinforcement Learning from UCL, by David Silver. The video lectures cover same topics as the Sutton & Barto book.The book Deep Reinforcement Learning Hands-On, which you will need to buy. As the title suggests, it covers more practical aspects of building RL systems. It uses Python and PyTorch. As RL has become more popular in recent years, there are plenty of other similar books - I have no reason to recommend this one over them, but I am reading it at the moment, and the quality is adequate.What other considerations should i take throughout this project?Before you get started, you should know this could be an ambitious project, requiring a lot of compute time using currently-available toolkits. You need to think ahead a little, as you will be faced with these decisions:In addition, I can think of the following:Building your Mario Kart player is a long-term goal. You will need to start with simpler agents in order to understand the techniques that you hope to apply to Mario Kart.You may need more compute power to solve this game than you have available.You will need to solve the issue of automating control of a N64 emulator. There is at least an existing emulator Mupen64Plus - I do not know whether it will be adequate for you, but at least one person has attempted to wrap this for automated learning, in the gym-mupen64plus project."
Can a brain be intelligent without a body? [duplicate],"







This question already has an answer here:
                                
                            




What kind of body (if any) does intelligence require?

                                (1 answer)
                            

Closed 3 years ago.



The dialog context
Turing proposed at the end of the description of his famous test, ""Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, 'Can machines think?'""1
Turing effectively challenged the 1641 statement of René Descartes in his Discourse on Method and Meditations on First Philosophy:

""It never happens that [an automaton] arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.""

Descartes and Turing, when discussing automatons achieving human abilities, shared a single context through which they perceived intelligence.  Those that have been either the actor or the administrator in actual Turing Tests understand the context: Dialog.
Other contexts2
The context of the dialog is distinct from other contexts such as writing a textbook, running a business, or raising children. If you apply the principle of comparing machine and human intelligence to automated vehicles (e.g. self-driving cars), an entirely different context becomes immediately apparent.
Question
Can a brain be intelligent without a body? More generally, does intelligence require a context?

References
[1] Chapter 1 (""Imitation Game"") of Computing Machinery and Intelligence, 1951.
[2] Multiple Intelligences Theory
","['philosophy', 'cognitive-science', 'turing-test', 'intelligence', 'embodied-cognition']","Can a brain be intelligent without a body?No. Don't forget that the main function of the brain is to provide homeostasis between the body and the environment. Without the body, the utility of the brain is no longer relevant.Alternatively, why consider intelligence only in the brain? How far does our body extend? Embodied cognitive science asks us to consider our entire body and its 
 surrounding extensions utility on the intelligent faculty; The fact that our thumbs stick out in a direction different to our other four fingers allows us to grab things in a effortless way is itself, intelligent.From this understanding then, the segregation between the intelligent faculty and the ""non-intelligent"" seems to be murky at best. We'd might as well not consider it unless there is better motivation.Does intelligence require a context?Pragmatically speaking, yes. Intelligent behaviour is typically understood as being goal-directed and intentional. Intentionality implies some sort of agency. It being an agent, implies some sort of agent/environment relationship, which implies an environment to act as context. On the other hand, Karl Friston notes in his review that a general principle that the brain (and thus one would imagine, intelligent behaviour) entails is reduction of thermodynamic free energy while maintaining homeostasis with its environment.This hints at a promise to describe intelligent behaviour purely in terms of thermodynamic processes, but interpretively what this means in a general language is still unclear.Also, keep in mind that there is no guarantee that we'd recognize a agent as intelligent if its construction is radically different from ours. "
Structure of a multilayered LSTM neural network?,"
I implemented a LSTM neural network in Pytorch. It worked but I want to know if it worked the way I guessed how it worked.
Say there's a 2-layer LSTM network with 10 units in each layer.
The inputs are some sequence data Xt1, Xt2, Xt3, Xt4, Xt5.
So when the inputs are entered into the network, Xt1 will be thrown into the network first and be connected to every unit in the first layer. And it will generate 10 hidden states/10 memory cell values/10 outputs. Then the 10 hidden states, 10 memory cell values and Xt2 will be connected to the 10 units again, and generate another 10 hidden states/10 memory cell values/10 outputs and so on.
After all 5 Xt's are entered into the network, the 10 outputs from Xt5 from the first layer are then used as the inputs for the second layer. The other outputs from Xt1 to Xt4 are not used. And the the 10 outputs will be entered into the second layer one by one again. So the first from the 10 will be connected to every unit in the second layer and generate 10 hidden states/10 memory cell values/10 outputs. The 10 memory cell values/10 hidden states and the second value from the 10 will be connected and so forth?
After all these are done, only the final 10 outputs from the layer 2 will be used. Is this how the LSTM network works? Thanks.
",['long-short-term-memory'],
Add training data to YOLO post-training,"
(Cross-posting here from the data science stack exchange, as my question didn't get any replies. I hope it's okay!)
I've been playing around with YOLOv3 and obtaining some good results on the ~20 custom classes I trained. However, one or two classes look like they can use some additional training data (not a lot, say about 10% more data), which I can provide.
What is the most efficient way to train my model now? Do I need to start training from scratch? Can I just throw in my additional data (with the appropriate changes to the config files etc.) and run the training based on the weight matrix I already acquired, but for a small number of iterations? (1000?) Or is this more like a transfer learning problem now?
Thanks for all tips!
","['deep-learning', 'training', 'object-recognition']",
How do I calculate the policy in the Proximal Policy Optimization algorithm?,"
I recently watched the video on Proximal Policy Optimization (PPO). Now, I want to upgrade my actor-critic algorithm written in PyTorch with PPO, but I'am not sure how the new parameters / thetas are calculated.
In the paper Proximal Policy Optimization Algorithms (at page 5), the pseudocode of the PPO algorithm is shown:
 
It says to run $\pi_{\theta_{\text{old}}}$, compute advantage estimates and optimize the objective. But how can we calculate $\pi_\theta$ for the objective ratio, since we have not updated the $\pi_{\theta_{\text{old}}}$ yet?
","['reinforcement-learning', 'actor-critic-methods', 'proximal-policy-optimization']",
"If two perfect chess AI's played each other, would it always be a stalemate or would white win for an inherent first-move advantage?","
In the circumstances of two perfect AI's playing each other, will white have an inherent advantage? Or can black always play for a stalemate by countering every white strategy?
","['game-theory', 'chess']",
Is it possible to state an outliers detection problem as a reinforcement learning problem?,"
To me it seems to be ill defined. Partially because of absence of knowledge which points are to be considered outliers in the first place.
The problem which I have in mind is ""bad market data"" detection. For example if a financial data provider is good only most of the time, but about 7-10% of data do not make any sense.
The action space is binary: either take an observation or reject it.
I am not sure about the reward, because the observations would be fed into an algorithm as inputs and the outputs of the algo would be outliers themselves. So the outliers detection should prevent outputs of the algorithm going rouge.
It is necessary to add that if we are talking about the market data (stocks, indices, fx), there's no guarantee that the distributions are stationary and there might be trends and jumps. If a supervised classifier is trainer based on historical data, how and how often should it be adjusted to be able to cope with different modes of the data.
",['reinforcement-learning'],
Where can I find the original paper that introduced RNNs?,"
I was able to find the original paper on LSTM, but I was not able to find the paper that introduced ""vanilla"" RNNs. Where can I find it?
","['recurrent-neural-networks', 'long-short-term-memory', 'reference-request', 'papers']",
How fast is TensorFlow compared to self written neural nets? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I made my first neural net in C++ without any libraries. It was a net to recognize numbers from the MNIST dataset. In a 784 - 784 - 10 net with sigmoid function and 5 epochs with every 60000 samples, it took about 2 hours to train. It was probably slow anyways, because I trained it on a laptop and I used classes for Neurons and Layers.
To be honest, I've never used TensorFlow, so I wanted to know how the performance of my net would be compared to the same in TensorFlow. Not too specific but just a rough approximation.
","['neural-networks', 'tensorflow', 'implementation']","I wanted to know how the performance of my net would be compared to the same in Tensor Flow. Not to specific but just a rough aproximation.This is very hard to answer in specific terms because benchmarking is very hard and is often wrong. The main point of TensorFlow as I see it is to make it easier for you to use a GPU and further allows you to use a large supply of programs written in Python/JavaScript that still give C++ level performance.How fast is TensorFlow compared to self written neural nets?This is answering the general question of using TensorFlow/PyTorch vs a custom solution, rather than your specific question of how much of a speed up you'd get.There was a relatively recent MIT paper, Differentiable Programming for Image Processing and Deep Learning in Halide trying to discuss the performance vs flexibility vs time spent coding a solution of 3 different languages.Specifically they compared a solution in their language Halide vs PyTorch vs CUDA.Consider the following example. A recent neural network-based
  image processing approximation algorithm was built around a new
  “bilateral slicing” layer based on the bilateral grid [Chen et al. 2007; Gharbi et al. 2017]. At the time it was published, neither PyTorch
  nor TensorFlow was even capable of practically expressing this
  computation. As a result, the authors had to define an entirely
  new operator, written by hand in about 100 lines of CUDA for the
  forward pass and 200 lines more for its manually-derived gradient
  (Fig. 2, right). This was a sizeable programming task which took
  significant time and expertise. While new operations—added in just
  the last six months before the submission of this paper—now make
  it possible to implement this operation in 42 lines of PyTorch, this
  yields less than 1/3rd the performance on small inputs and runs
  out of memory on realistically-sized images (Fig. 2, middle). The
  challenge of efficiently deriving and computing gradients for custom
  nodes remains a serious obstacle to deep learning.So in general you'll probably get faster performance with TensorFlow/PyTorch than a custom C++ implementation, but for specific cases if you have CUDA knowledge on top of C++ then you will be able to write more performant programs."
How does an svm work? How does it perform comparisons between malignant and benign tumor,"
How do Support Vector Machines (SVMs) differentiate between a glass and a bottle or between a malignant and a benign tumor when it dealing with it for the first time?
What will be the analysis mechanism involved in this?
","['image-recognition', 'classification', 'support-vector-machine']",
Is the cube root function suitable as a n activation function?,"
I am trying to design a neural network on Python.
Instead of the sigmoid function which has a limited range, I am thinking of using the cube root function which has the following graph:

Is this suitable?
","['neural-networks', 'activation-functions']","There are a few traits that you want the activation function to have, and cube roots rate as OK-ish:Nonlinear – check.Continuously differentiable – no. There is a problem at $x=0$. Unlike other discontinuous functions like ReLU, although the gradient can be calculated near zero, it can be arbitrarily high as you approach $x=0$, because $\frac{d}{dx}x^{\frac{1}{3}} = \frac{1}{3x^{\frac{2}{3}}}$ Range considerations – limited range functions are more stable, large/infinite range functions are more efficient. You may need to reduce learning rates compared to e.g. tanh.Monotonic – check.Monotonic derivative - no.Approximates identity near the origin – no, the approximation is bad near the origin.If you look through the list of current, successful activation functions, you will see a few that also fail to provide one or more desirable traits, yet are still used routinely.I would worry about the high gradients near $x=0$, but other than that I think the function could work ok. It may sometimes be unstable during learning, as small changes near zero will result in large changes to output. You might be able to work around the high gradients simply in practice, by clipping them. If the raw calculation returns value greater than $1$ (or less than $-1$) then treat the gradient as if it were $1$ (or $-1$) for the rest of backpropagation. The only way of finding out if the function is competitive with other more standard activation functions is to try it on some standard data sets and make comparisons."
How do I optimize a specific function using a genetic algorithm?,"
I recently learned about genetic algorithms and I solved the 8 queens problem using a genetic algorithm, but I don't know how to optimize any functions using a genetic algorithm.
$$
\begin{array}{r}
\text { maximize } f(x)=\frac{-x^{2}}{10}+3 x \\
0 \leq x \leq 32
\end{array}
$$
I want a guide on how to find chromosomes and fitness functions for such a function? And I don't want code.
","['genetic-algorithms', 'optimization', 'evolutionary-algorithms']",
What is the correct way to use the implication in first-order logic?,"
I know the implication symbol, $\rightarrow$, is used for conditions like

If $A$ is true, then $B$ will be true.

which can be written as
$$
A \rightarrow B
$$
However, sometimes the implication symbol is also used in other contexts. For example, if we want to say that

All $A$'s are $B$.

We could write
$$\forall X (A(X) \rightarrow B(X))$$
I don't understand why the implication is used here? And if the implication is necessary to use here, then why isn't the implication used in the example written below?

Some $A$'s are $B$'s.

$$\exists X (A(X) \land B(X))$$
",['logic'],
Using 3D Points as Inputs to a Neural Net,"
I am currently looking to use a neural network to classify gestures. I have a series of Dx,Dy,Dz readings that represent the differences across the three axes made during the gesture. About 10 movements for each example of the gesture. Basically a 10x3 matrix and then classify the training data into about 15 classes. I plan to use a CNN classifier to do this because, while the time domain is relevant this problem the difference in the movements can be differentiated when presented with as a discrete matrix.
I'm used to using images with a neural net so I instinctively want to just convert the matrices into a 2D tensor and feed them into a CNN, but I was wondering if there was a better way to do this? For example, I have seen 1D tensors passed to a fully connected neural network for classification which seems like it could be more appropriate for this data input type? 
Any tips on general architecture would be really appreciated as well!
Thanks!
","['convolutional-neural-networks', 'classification']","A few thoughts :10x3 matrix for each example is really a small amount of data. FCNN could do a good job on that.As a result, I'm not sure CNN is appropriated. The smallest dimension is 3, so it'll force you to have really small kernels.Have you thought about LSTM? since that your data is sequential, LSTM may be useful. However, I'm not sure they could be really effective on this little amount of data. but it could be nice to try."
Is a linear activation function (in the output layer) equivalent to an identity function?,"
I have a simple question about the choice of activation function for the output layer in feed-forward neural networks.
I have seen several codes where the choice of the activation function for the output layer is linear.
Now, it might well be that I am wrong about this, but isn't that simply equivalent to a rescaling of the weights connecting the last hidden layer to the output layer? And following this point, aren't you just as well off with just using the identity function as your output activation function?
","['neural-networks', 'terminology', 'activation-functions']",
Questions on the identifiability issue and equations 8 and 9 in the D3QN paper,"
I have difficulty understanding the following paragraph in the below excerpts from page 4 to page 5 from the paper Dueling Network Architectures for Deep Reinforcement Learning.
The author said ""we can force the advantage function estimator to have zero advantage at the chosen action.""
For the equation $(8)$ below, is it correct that $A - \max A$ is at most zero?

... lack of identifiability is mirrored by poor practical performance when this equation is used directly.


To address this issue of identifiability, we can force the advantage
function estimator to have zero advantage at the chosen action. That is, we let the last module of the network implement the forward mapping


$$Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \left( A(s, a; \theta, \alpha) - \max_{a' \in | \mathcal{A} |} A(s, a'; \theta, \alpha) \right). \tag{8}$$


Now, for $a^∗ = \text{arg max}_{a' \in \mathcal{A}} Q(s, a'; \theta, \alpha, \beta) = \text{arg max}_{a' \in \mathcal{A}} A(s, a'; \theta, \alpha)$, we obtain $Q(s, a^∗; \theta, \alpha, \beta) = V (s; \theta, \beta)$.  Hence, the stream $V(s; \theta, \beta)$ provides an estimate of the value function, while the other stream produces an estimate of the advantage function.

I would like to request further explanation on Equation 9, when the author wrote what is bracketed between the red parentheses below.

An alternative module replaces the max operator with an average:


$$Q(s, a; \theta, \alpha, \beta) = V (s; \theta, \beta) + \left( A(s, a; \theta, \alpha) − \frac {1} {|A|} \sum_{a' \in \mathcal{A}} A(s, a'; \theta, \alpha) \right). \tag{9}$$


On the one hand this loses the original semantics of $V$ and $A$ because they are now off-target by a constant,
but on the other hand it increases the stability of the optimization: with (9) the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action’s advantage in (8).

In the paper, to address the identifiability issue, there are two equations used. My understanding is both equations are trying to fix the advantage part - the last module.
For equation $(8)$, are we trying to make $V(s) = Q^*(s)$, as the last module is zero?
For equation $(9)$, the resulting $V(s)$ = true $V(s)$ + mean$(A)$? As the author said ""On the one hand this loses the original semantics of $V$ and $A$ because they are now off-target by a constant"". And the constant refers to mean$(A)$? Is my understanding correct?
","['reinforcement-learning', 'q-learning', 'papers', 'd3qn']",
"What is the name of the NLP technique that determines ""who did what to whom"" given a sentence?","
Within a piece of text, I'm trying to detect who did what to whom.
For instance, in the following sentences:

CV hit IV. CV was hit by IV.

I'd like to know who hit whom.
I can't remember what this technique is called.
","['natural-language-processing', 'reference-request', 'computational-linguistics']","You might be referring to Semantic role labeling. SRL is the task of assigning labels to words or phrases in a sentence that shows their semantic role in that sentence. In your example CV was hit by IV, the task is to identify the verb ""hit"" carried out by the actor ""CV"" affected ""IV"" the recipient.Note: If you're only interested in the syntactic relationship among words or phrases in a sentence, not the semantic relationship between them, simple dependency parsing would do the job."
Should we focus more on societal or technical issues with AI risk,"
I have trouble finding material (blog, papers) about this issue, so I'm posting here.
Taking a recent well known example: Musk has tweeted and warned about the potential dangers of AI, saying it is ""potentially more dangerous than nukes"", referring the issue of creating a superintelligence whose goals are not aligned with ours. This is often illustrated with the paperclip maximiser though experiment. Let's call this first concern ""AI alignment"".
By contrast, in a recent podcast, his concerns seemed more related to getting politicians and decision makers to acknowledge and cooperate on the issue, to avoid potentially dangerous scenarios like an AI arms race. In a paper co-authored by Nick Bostrom: 
Racing to the Precipice: a Model of Artificial Intelligence Development, the authors argue that developing AGI in a competitive situation incentivises us to skim on safety precautions, so it is dangerous. Let's call this second concern ""AI governance"".
My question is about the relative importance between these two issues: AI alignment and AI governance.
It seems that most institutions trying to prevent such risks (MIRI, FHI, FLI, OpenAI, DeepMind and others) just state their mission without trying to argue about why one approach should be more pressing than the other.
How to assess the relative importance of those two issues? And can you point me any literature about this?
","['agi', 'social', 'neo-luddism', 'value-alignment', 'risk-management']",
"Can a basic CNN (Conv2D, MaxPooling2D, UpSampling2D) find a good approximation of a product of its input channels?","
Let's assume I want to teach a CNN some physics. Starting with a U-Net, I input images A and B as separate channels. I know that my target (produced by a very slow Monte-Carlo code) represents a signal such as f(g(A) * h(B)), where f, g and h are fairly ""convolutional"" operations -- meaning, involving mostly blurring and rescaling operations.
I feel safe to state that this problem would not be too difficult for the case of f(g(A) + h(B)) -- but what about f(g(A) * h(B))? Can I expect a basic CNN such as the U-Net to be able to represent the * (multiplication) operation?
Or should I expect to be forced to include a Multiply layer in my network, somewhere where I expect that the part before can learn the g and h parts, and the part after can learn the f part?
",['convolutional-neural-networks'],"I think U-Net is already a quite complex network that probably should (as of my experience) be able to approximate this multiplication. However this would still be an approximation that might not be accurate and maybe only solves a function that looks like a multiplication for the range of the shown input samples defined by your dataset (therefore potentially overfitting on your training dataset).So in general if you know that your target function does have this multiplication then you should definetly excplicitly enforce it in your network. If you know so much about the wanted function it's always better to build a good fitting architecture than using a general neural network architecture. That will ease the optimization and should generalize much better.However, it's hard or impossible to tell you for sure how much depth or complexity you need to solve this task within your wanted accuracy. Eventually you should just try it out."
What is the difference between the study of evolutionary algorithms and optimization?,"
I have a course named ""Evolutionary Algorithms"", but our teacher is always mentioning the word ""optimization"" in his lectures.
I am confused. Is he actually teaching optimization? If yes, why is the name of the course not ""Optimization""?
What is the difference between the study of evolutionary algorithms and optimization?
","['comparison', 'optimization', 'evolutionary-algorithms']","Optimization problems are formally defined by two things:Optimization theory as a field deals with variations of such problems. The algorithms that are part of optimization include gradient descent and variants, the simplex algorithm, simulated annealing and many more. I would also include evolutionary algorithms as one of the algorithms in this field.I am confused. Is he actually teaching optimization? If yes, why is the name of the course not ""Optimization""?Evolutionary algorithms are a subfield of optimization. The name of the course is evolutionary algorithms as you likely don't deal with the other aspects of optimization theory (e.g. the algorithms mentioned above)."
Which marketing-related classification challenges is a feed forward neural network suited to solve?,"
I am trying to think of some marketing-related classification challenges that a feed-forward neural network would be suited for. 
Any ideas?
","['neural-networks', 'classification', 'feedforward-neural-networks']",
"What is the purpose of the ""gamma"" parameter in SVMs?","
I want to understand what the gamma parameter does in an SVM. According to this page.

Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.

I don't understand this part ""of a single training example reaches"", does it refer to the training dataset?
","['machine-learning', 'support-vector-machine', 'hyper-parameters']",
Why does the value of state change depending on the policy used to get to that state?,"
From what I understand, the value function estimates how 'good' it is for an agent to be in a state, and a policy is a mapping of actions to state.
If I have understood these concepts correctly, why does the value of a state change with the policy with which an agent gets there?
I guess I'm having difficulty grasping the concept that the goodness of a state changes depending on how an agent got there (different policies may have different ways, and hence different values, for getting to a particular state).
If there can be a concrete example (perhaps on a grid world or on a chessboard), that might make it clear why that might be the case.
","['reinforcement-learning', 'policies', 'value-functions']","I guess I'm having difficulty grasping the concept that the goodness of a state changes depending on how an agent got thereIt doesn't.The value of a state changes depending on what the agent will do next. That is where the dependency on the policy comes in, not in past behaviour, but expectations of future behaviour. The future behaviour depends on the state transitions and rewards presented by the environment, plus it depends on the distribution of actions chosen by the policy. More formally, the value function of a state is not just a relative and arbitrary scoring system, but equals the expected (discounted) sum of rewards, assuming the MDP follows the given dynamics, including action selection:$$v_{\pi}(s) = \mathbb{E}_{A \sim \pi}[\sum_{k=0}^\infty \gamma^k R_{t+k+1} | S_t = s]$$Without identifying a policy, it is not possible to assess a value function. In value-based control methods, the policy to evaluate can be implied, somewhat self-referentially, as the policy that acts greedily (or maybe $\epsilon$-greedily) according to the current estimates of the value function.If there can be a concrete example (perhaps on a GridWorld or on a chess board), that might make it clear why that might be the caseA very simple deterministic MDP with a start state and two terminal states illustrates this:Start in state B. Taking the left action is followed by a transition (with $p=1$) to terminal state A, and a reward of $0$. Taking the right action is followed by a transition (with $p=1$) to terminal state C, and a reward of $3$.What is the value of state B? It depends on what the policy chooses. A deterministic left policy $\pi_1$ has $v_{\pi_1}(B) = 0$, a random policy $\pi_2$ choosing left and right with $p=0.5$ has $v_{\pi_2}(B) = 1.5$. The optimal policy chooses action right always and has $v_{\pi_3}(B) = v^*(B) = 3.0$"
Connect 4 minimax does not make the best move,"
I'm trying to implement an algorithm that would choose the optimal next move for the game of Connect 4. As I just want to make sure that the basic minimax works correctly, I am actually testing it like a Connect 3 on a 4x4 field. This way I don't need the alpha-beta pruning, and it's more obvious when the algorithm makes a stupid move.
The problem is that the algorithm always starts the game with the leftmost move, and also during the game it's just very stupid. It doesn't see the best moves.
I have thoroughly tested methods makeMove(), undoMove(),  getAvailableColumns(), isWinningMove() and isLastSpot() so I am absolutely sure that the problem is not there.
Here is my algorithm.
NextMove.java
private static class NextMove {
    final int evaluation;
    final int moveIndex;

    public NextMove(int eval, int moveIndex) {
        this.evaluation = eval;
        this.moveIndex = moveIndex;
    }

    int getEvaluation() {
        return evaluation;
    }

    public int getMoveIndex() {
        return moveIndex;
    }
}

The Algorithm
private static NextMove max(C4Field field, int movePlayed) {
    // moveIndex previously validated
    
    // 1) check if moveIndex is a final move to make on a given field
    field.undoMove(movePlayed);
    
    // check
    if (field.isWinningMove(movePlayed, C4Symbol.BLUE)) {
        field.playMove(movePlayed, C4Symbol.RED);
        return new NextMove(BLUE_WIN, movePlayed);
    }
    if (field.isWinningMove(movePlayed, C4Symbol.RED)) {
        field.playMove(movePlayed, C4Symbol.RED);
        return new NextMove(RED_WIN, movePlayed);
    }
    if (field.isLastSpot()) {
        field.playMove(movePlayed, C4Symbol.RED);
        return new NextMove(DRAW, movePlayed);
    }
    
    field.playMove(movePlayed, C4Symbol.RED);
    
    // 2) moveIndex is not a final move
    // --> try all possible next moves
    final List<Integer> possibleMoves = field.getAvailableColumns();
    int bestEval = Integer.MIN_VALUE;
    int bestMove = 0;
    for (int moveIndex : possibleMoves) {           
        field.playMove(moveIndex, C4Symbol.BLUE);
        
        final int currentEval = min(field, moveIndex).getEvaluation();
        if (currentEval > bestEval) {
            bestEval = currentEval;
            bestMove = moveIndex;
        }

        field.undoMove(moveIndex);
    }
    
    return new NextMove(bestEval, bestMove);
}

private static NextMove min(C4Field field, int movePlayed) {
    // moveIndex previously validated
    
    // 1) check if moveIndex is a final move to make on a given field
    field.undoMove(movePlayed);
    
    // check
    if (field.isWinningMove(movePlayed, C4Symbol.BLUE)) {
        field.playMove(movePlayed, C4Symbol.BLUE);
        return new NextMove(BLUE_WIN, movePlayed);
    }
    if (field.isWinningMove(movePlayed, C4Symbol.RED)) {
        field.playMove(movePlayed, C4Symbol.BLUE);
        return new NextMove(RED_WIN, movePlayed);
    }
    if (field.isLastSpot()) {
        field.playMove(movePlayed, C4Symbol.BLUE);
        return new NextMove(DRAW, movePlayed);
    }
    
    field.playMove(movePlayed, C4Symbol.BLUE);
    
    // 2) moveIndex is not a final move
    // --> try all other moves
    final List<Integer> possibleMoves = field.getAvailableColumns();
    int bestEval = Integer.MAX_VALUE;
    int bestMove = 0;
    for (int moveIndex : possibleMoves) {
        field.playMove(moveIndex, C4Symbol.RED);
        
        final int currentEval = max(field, moveIndex).getEvaluation();
        if (currentEval < bestEval) {
            bestEval = currentEval;
            bestMove = moveIndex;
        }
        
        field.undoMove(moveIndex);
    }
    
    return new NextMove(bestEval, bestMove);
}

The idea is that the algorithm takes in the arguments of a currentField and the lastPlayedMove. Then it checks if the last move somehow finished the game. If it did, I just return that move, and otherwise I go in-depth with the subsequent moves.
Blue player is MAX, red player is MIN.
In each step I first undo the last move, because it's easier to check if the ""next"" move will finish the game, than check if the current field is finished (this would require to analyze for all possible winning options in the field). After I check, I just redo the move.
From some reason this doesn't work. I am stuck with that for days! I have no idea what's wrong... Any help greatly appreciated!
EDIT
I'm adding the code how I'm invoking the algorithm.
@Override
public int nextMove(C4Game game) {
    C4Field field = game.getCurrentField();
    C4Field tmp = C4Field.copyField(field);

    int moveIndex = tmp.getAvailableColumns().get(0);
    final C4Symbol symbol = game.getPlayerToMove().getSymbol().equals(C4Symbol.BLUE) ? C4Symbol.RED : C4Symbol.BLUE;
    tmp.dropToColumn(moveIndex, symbol);

    NextMove mv = symbol
            .equals(C4Symbol.BLUE) ? 
                    max(tmp, moveIndex) : 
                        min(tmp, moveIndex);

                    int move = mv.getMoveIndex();
                    return move;
}

","['game-ai', 'minimax', 'java']","I suspect that you'll have to remove this code:from the max() method, and remove this code:from the min() method.In the first case, you're checking whether the move that RED just made was a winning move. You don't want to check there if it was a winning move from BLUE, because it wasn't BLUE who just made that move; it was red. The same counts the other way around in the second case.Additionally, the initial call into the algorithm seems overly complicated. I am not sure what the intended use of the tmp variable there is, or that dropToColumn() call. I would rewrite it to be more like:This will require an adaptation of the max() and min() methods such that they skip the whole checking-for-wins thing if the previous movePlayed equals -1.With the code you currently have there, you do not perform a minimax search for the optimal move in the current game state; instead you first arbitrarily modify the current game state using that tmp.dropToColumn() call, and perform the minimax search in that arbitrarily modified game state. The optimal move to play in such an arbitrarily-modified game state will tend not to be the optimal move in the game state that you really are in."
How does L2 regularization make weights smaller?,"
I'm learning logistic regression and $L_2$ regularization.
The cost function looks like below.
$$J(w) = -\displaystyle\sum_{i=1}^{n} (y^{(i)}\log(\phi(z^{(i)})+(1-y^{(i)})\log(1-\phi(z^{(i)})))$$
And the regularization term is added. ($\lambda$ is a regularization strength)
$$J(w) = -\displaystyle\sum_{i=1}^{n} (y^{(i)}\log(\phi(z^{(i)})+(1-y^{(i)})\log(1-\phi(z^{(i)}))) + \frac{\lambda}{2}\| w \|$$
Intuitively, I know that if $\lambda$ becomes bigger, extreme weights are penalized and weights become closer to zero. However, I'm having a hard time to prove this mathematically.
$$\Delta{w} = -\eta\nabla{J(w)}$$
$$\frac{\partial}{\partial{w_j}}J(w) = (-y+\phi(z))x_j + \lambda{w_j}$$
$$\Delta{w} = \eta(\displaystyle\sum_{i=1}^{n}(y^{(i)}-\phi(z^{(i)}))x^{(i)} - \lambda{w_j})$$
This doesn't show the reason why incrementing $\lambda$ makes weight become closer to zero. It is not intuitive.
","['machine-learning', 'proofs', 'hyper-parameters', 'regularization', 'l2-regularization']","Here is my take.The larger the $\lambda$, the more the corresponding regularization term for a coefficient will be big, so when minimizing the cost function, the coefficient will be reduced by a bigger factor, you can see this effect in the derivation of the update rule for gradient descent for example:
\begin{align*} \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \end{align*}\begin{align*} \theta_j := \theta_j (1- \alpha \frac{\lambda}{m})\left( \frac{\alpha}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline & \end{align*}From this derivation, it is clear that at every update the coefficients get reduced by a factor that is usually a little less than 1 and directly proportional to $\lambda$, so, as $\lambda$ gets bigger, the weights get reduced more and more; eventually, for very big values of $\lambda$, we risk to totally underfit the data, since only the regularization term will remain in the cost function and all the weights will go to zero.This is for linear regression, but it is essentially the same logic also for logistic regression.This is taken from Andrew Ng's course on Coursera. A more mathematical precise (and complex) traction of the problem can be found in the Bloomberg machine learning course material.PS: in the derivation of the update rule for gradient descent, the $\lambda$ should be divided by the number of training examples, this is important in choosing the right $\lambda$ because we want this relationship to be inversely proportional, otherwise the coefficient won't be decreased."
Why would giving my AI more data make it perform worse?,"
So I trained an AI to generate shakespeare, which it did somewhat well. I used this 10,000 character sample.
Next I tried to get it to generate limericks using these 100,000 limericks. It generated garbage output.
When I limited it to 10,000 characters, it then started giving reasonable limerick output.
How could this happen? I thought more data was always better.
The AI was a neural network with some LSTM layers, implemented in keras.
","['python', 'recurrent-neural-networks', 'datasets', 'keras', 'long-short-term-memory']",
"Why is the ""square error function"" sometimes defined with the constant 1/2 and sometimes with the constant 1/m?","
Depending on the source, I find people using different variations of the ""squared error function"". How come that be?
Here, it is defined as
$$
E_{\text {total }}=\sum \frac{1}{2}(\text {target}-\text {output})^{2}
$$
OTOH, here, it's defined as
$$
\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$$
Notice that it is being divided by 1 over $m$ as opposed to variation 1, where we multiply by $1/2$.
The stuff inside the $()^2$ is simply notation, I get that, but dividing by $1/m$ and $1/2$ will clearly get a different result. Which version is the ""correct"" one, or is there no such thing as a correct or ""official"" squared error function?
","['neural-networks', 'objective-functions', 'definitions', 'mean-squared-error']","The first variation is named ""$E_{total}$"". It contains a sum which is not very well-specified (has no index, no limits). Rewriting it using the notation of the second variation would lead to:$$E_{total} = \sum_{i = 1}^m \frac{1}{2} \left( y^{(i)} - h_{\theta}(x^{(i)}) \right)^2,$$where:Because the term inside the large brackets is squared, the sign doesn't matter, so we can rewrite it (switch around the subtracted terms) to:$$E_{total} = \sum_{i = 1}^m \frac{1}{2} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2.$$Now it already looks quite a lot like your second variation. The second variation does still have a $\frac{1}{m}$ terms outside the sum. That is because your second variation computes the mean squared error over all the training examples, rather than the total error computed by the first variation. Either error can be used for training. I'd personally lean towards using the mean error rather than the total error, mainly because the scale of the mean error is independent of the batch size $m$, whereas the scale of the total error is proportional to the batch size used for training. Either option is valid, but they'll likely require different hyperparameter values (especially for the learning rate), due to the difference in scale.With that $\frac{1}{m}$ term explained, the only remaining difference is the $\frac{1}{2}$ term inside the sum (can also be pulled out of the sum), which is present in the first variation but not in the second. The reason for including that term is given in the page you linked to for the first variation:The $\frac{1}{2}$ is included so that exponent is cancelled when we differentiate later on. The result is eventually multiplied by a learning rate anyway so it doesn’t matter that we introduce a constant here."
Is there a way that helps me to architect my CNN fundamentally before training?,"
While we train a CNN model we often experiment with the number of filters, the number of convolutional layers, FC layers, filter size, sometimes stride, activation function, etc. More often than not after training the model once, it is just a trial & error process.  

Is there a way that helps me to architect my model fundamentally before training?
Once I train model, how do I know which among these variables (number of filters, size, number of convolutional layers, FC layers) should be
changed - increased or decreased?

P.S. This question assumes that data is sufficient in volume and annotated properly and still accuracy is not up to the mark. So, I've ruled out the possibility of non-architectural flaws for the question.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'ai-design']","See Analysis and Optimization of Convolutional Neural Network Architectures (my master's thesis).Yes, but the architecture learning approaches are computationally intensive. When I wrote my masters thesis it was ~250,000 USD to run the Google experiment. Meanwhile, there seem to be more efficient methods, e.g. https://autokeras.com/See Chapter 3.See Chapter 2.5 for some approaches. But there is no silver bullet / no clear answer to this question."
Do good approximations produce good gradients?,"
Let’s say I have a neural net doing classification and I’m doing stochastic gradient descent to train it. If I know that my current approximation is a decent approximation, can I conclude that my gradient is a decent approximation of the gradient of the true classifier everywhere?
Specifically, suppose that I have a true loss function, $f$, and an estimation of it, $f_k$. Is it the case that there exists a $c$ (dependent on $f_k$) such that for all $x$ and $\epsilon > 0$ if $|f(x)-f_k(x)|<\epsilon$ then $|\nabla f(x) - \nabla f_k(x)|<c\epsilon$? This isn’t true for general functions, but it may be true for neural nets. If this exact statement isn’t true, is there something along these lines that is? What if we place some restrictions on the NN?
The goal I have in mind is that I’m trying to figure out how to calculate how long I can use a particular sample to estimate the gradient without the error getting too bad. If I am in a context where resampling is costly, it may be worth reusing the same sample many times as long as I’m not making my error too large. My long-term goal is to come up with a bound on how much error I have if I use the same sample $k$ times, which doesn’t seem to be something in the literature as far as I’ve found.
","['neural-networks', 'gradient-descent']","In general $|f(x) - f_k(x)| \leq \epsilon$ doesn't ensure $|\nabla f(x) - \nabla f_k(x)| \leq c\epsilon$. And for neural networks there is no reason to believe it will happen either. You can also look at Sobolev Training Paper (https://arxiv.org/abs/1706.04859). In particular, note that Sobolev training was better than critic training, which indirectly may indicate approximating function may not be the same as approximating gradient and function. In Sobolev training, the network is trained to match gradient and function whereas in critic training network is trained to match function. They produce quite different results which might give us some hints about the above problem. In general, if two functions are arbitrary close, they might not be close in gradients.Edit: 
(Trying to come up with a negative example)
Consider $f(x) = g(x) + \epsilon \sin (\frac {kx} {\epsilon}) $. $g(x)$ is some neural network. Now, we train some another neural network $h(x)$ to fit $f(x)$ and after training we get $h(x) = g(x)$ ($h(x)$ and $g(x)$ have same weights precisely). However, $\nabla f_x =\nabla g_x + k\cos (\frac {kx} {\epsilon})$ is not arbitarariy close $\nabla g_x$.  I hope this example is enough to prove that a neural network that nicely approximates the function may not nicely approximate the gradients and no such result can be proved mathematically rigorously. However, considering the paper in the discussion, you might think for practical purposes it works. However, if you have both function and grad information available that is expected to work better. "
Does inflation should occur in output layer when I do Artificial Neural Network to increase smartness of the model?,"
The idea that come to my mind is called Value Based Model for ANN. We use simple DCF formula to calculate kind of Q value: Rewards/Discount rate. Discount rate is a risk of getting the reward on the information that agent know about. Of course if you have many factors you just sum that. So, we calculate FV for every cell that agent know information about and this is a predicted data. We put predicted - actual and teach model how to run using loss function. Rephrased, does increase in output layer actually train the model to be better? The human logic is that if I took course I have a bigger value that helps me to live. What about NN? Does it actually more precise if we with time increase output?
",['reinforcement-learning'],
Is a calculus or ML approach to varying learning rate as a function of loss and epoch been investigated?,"
Many have examined the idea of modifying learning rate at discrete times during the training of an artificial network using conventional back propagation.  The goals of such work have been a balance of the goals of artificial network training in general.

Minimal convergence time given a specific set of computing resources
Maximal accuracy in convergence with regard to the training acceptance criteria
Maximal reliability in achieving acceptable test results after training is complete

The development of a surface involving these three measurements would require multiple training experiments, but may provide a relationship that itself could be approximated either by curve fitting or by a distinct deep artificial network using the experimental results as examples.

Epoch index
Learning rate hyper-parameter value
Observed rate of convergence

The goal of such work would be to develop, via manual application of analytic geometry experience or via deep network training the following function, where

$\alpha$ is the ideal learning rate for any given epoch indexed by $i$,
$\epsilon$ is the loss function result, and
$\Psi$ is a function the result of which approximates the ideal learning rate for as large an array of learning scenarios possible within a clearly defined domain.

$\alpha_i = \Psi (\epsilon, i)$
The development of arriving at $\Psi$ as a closed form (formula) would be of general academic and industrial value.
Has this been done?
","['deep-learning', 'optimization', 'topology', 'convergence', 'hyper-parameters']","Has this been done?Difficult to prove a negative, but I suspect although plenty of research has been done into finding ideal learning rate values (the need for learning rate at all is an annoyance), it has not been done to the level of suggesting a global function worth approximating.The problem is that learning rate tuning, like other hyperparameter tuning, is highly dependent on the problem at hand, plus the other hyperparamater values currently in use, such as size of layers, which optimiser is in use, what regularisation is being used, activation functions.Although you may be hoping for $\Psi(\epsilon, i | P)$ to exist where P is the problem domain, it likely does not except as a mean value over all $\Psi(\epsilon, i | D, H)$ for the problem domain, where D is the dataset and H all the other hyperparameters.It is likely that such a function exists, of ideal learning rate for best expected convergence per epoch. However, it would be incredibly expensive to sample it with enough detail to make approximating it useful. Coupled with limited applicability (not domain-specific, but linked to data and other hyperparameters), a search through all possible learning rate trajectories looks like it would give poor return on investment.Instead, the usual pragmatic approaches are:Include learning rate in hyperparameter searches, such as grid search, random search, genetic algorithms and other global optimisers.Decay learning rate using one of a few approaches that have been successfully guessed and experiments have shown working. These have typically been validated by plotting learning curves of loss functions or other metrics, and the same tracking is usually required in new experiments to check that the approach is still beneficial.Some optimisers use a dynamic learning rate parameter, which is similar to your idea but based on reacting to measurements during learning as opposed to changes based on an ideal function. They have a starting learning rate, then adjust it based on  heuristics derived from measuring learning progress. These heuristics can be based on per-epoch measurements, such as whether a validation set result is improving or not. One such approach is to increase learning rate whilst results per epoch are improving, and reduce learning rate if results are not improving, or have got worse. I have tried this last option, on a Kaggle competition, and it worked to some extent for me, but did not really improve results overall - I think it is one of many promising ideas in ML that can be made to work, but that has not stayed as a ""must have"", unlike say dropout, or using CNNs for images.Some optimisers store a multiplier per layer or even per weight - RMSProp and Adam for example track rate of change of each parameter, and adjust the rate for each weight during updates. These can work very well in large networks, where the issue is not so much needing a specific learning rate at any time, but that a single learning rate is too crude to cover the large range of gradients and differences in gradients across the index space of all the connections. With RMSProp and Adam, the need to pick specific learning rates or explore them is much reduced, and often a library's default is fine."
Loss jumps abruptly when I decay the learning rate with Adam optimizer in PyTorch,"
I'm training an auto-encoder network with Adam optimizer (with amsgrad=True) and MSE loss for Single channel Audio Source Separation task. Whenever I decay the learning rate by a factor, the network loss jumps abruptly and then decreases until the next decay in learning rate.
I'm using Pytorch for network implementation and training.
Following are my experimental setups:

 Setup-1: NO learning rate decay, and 
          Using the same Adam optimizer for all epochs

 Setup-2: NO learning rate decay, and 
          Creating a new Adam optimizer with same initial values every epoch

 Setup-3: 0.25 decay in learning rate every 25 epochs, and
          Creating a new Adam optimizer every epoch

 Setup-4: 0.25 decay in learning rate every 25 epochs, and
          NOT creating a new Adam optimizer every time rather
          using PyTorch's ""multiStepLR"" and ""ExponentialLR"" decay scheduler 
          every 25 epochs

I am getting very surprising results for setups #2, #3, #4 and am unable to reason any explanation for it. Following are my results:
Setup-1 Results:

Here I'm NOT decaying the learning rate and 
I'm using the same Adam optimizer. So my results are as expected.
My loss decreases with more epochs.
Below is the loss plot this setup.

Plot-1:

optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

for epoch in range(num_epochs):
    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)


Setup-2 Results:  

Here I'm NOT decaying the learning rate but every epoch I'm creating a new
Adam optimizer with the same initial parameters.
Here also results show similar behavior as Setup-1.

Because at every epoch a new Adam optimizer is created, so the calculated gradients
for each parameter should be lost, but it seems that this doesnot affect the 
network learning. Can anyone please help on this?

Plot-2:

for epoch in range(num_epochs):
    optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)


Setup-3 Results: 

As can be seen from the results in below plot, 
my loss jumps every time I decay the learning rate. This is a weird behavior.

If it was happening due to the fact that I'm creating a new Adam 
optimizer every epoch then, it should have happened in Setup #1, #2 as well.
And if it is happening due to the creation of a new Adam optimizer with a new 
learning rate (alpha) every 25 epochs, then the results of Setup #4 below also 
denies such correlation.

Plot-3:

decay_rate = 0.25
for epoch in range(num_epochs):
    optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

    if epoch % 25 == 0  and epoch != 0:
        lr *= decay_rate   # decay the learning rate

    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)


Setup-4 Results:  

In this setup, I'm using Pytorch's learning-rate-decay scheduler (multiStepLR)
which decays the learning rate every 25 epochs by 0.25.
Here also, the loss jumps everytime the learning rate is decayed.

As suggested by @Dennis in the comments below, I tried with both ReLU and 1e-02 leakyReLU nonlinearities. But, the results seem to behave similar and loss first decreases, then increases and then saturates at a higher value than what I would achieve without learning rate decay.
Plot-4 shows the results.
Plot-4:

scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[25,50,75], gamma=0.25)

scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.95)
scheduler = ......... # defined above
optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

for epoch in range(num_epochs):

    scheduler.step()

    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)


EDITS: 

As suggested in the comments and reply below, I've made changes to my code and trained the model. I've added the code and plots for the same. 
I tried with various lr_scheduler in PyTorch (multiStepLR, ExponentialLR) and plots for the same are listed in Setup-4 as suggested by @Dennis in comments below.
Trying with leakyReLU as suggested by @Dennis in comments.

Any help.
Thanks
","['machine-learning', 'deep-learning', 'optimization', 'autoencoders', 'objective-functions']","I see no reason why decaying learning rates should create the kinds of jumps in losses that you are observing. It should ""slow down"" how quickly you ""move"", which in the case of a loss that otherwise consistently shrinks really should, at worst, just lead to a plateau in your losses (rather than those jumps).The first thing I observe in your code is that you re-create the optimizer from scratch every epoch. I have not yet worked enough with PyTorch to tell for sure, but doesn't this just destroy the internal state / memory of the optimizer every time? I think you should just create the optimizer once, before the loop through the epochs. If this is indeed a bug in your code, it should also actually still be a bug in the case where you do not use learning rate decay... but maybe you simply get lucky there and don't experience the same negative effects of the bug.For learning rate decay, I'd recommend using the official API for that, rather than a manual solution. In your particular case, you'll want to instantiate a StepLR scheduler, with:You can then simply call scheduler.step() at the start of every epoch (or maybe at the end? the example in the API link calls it at the start of every epoch).If, after the changes above, you still experience the issue, it would also be useful to run each of your experiments multiple times and plot average results (or plot lines for all experiments). Your experiments should theoretically be identical during the first 25 epochs, but we still see huge differences between the two figures even during those first 25 epochs in which no learning rate decay occurs (e.g., one figure starts at a loss of ~28K, the other starts at a loss of ~40K). This may simply be due to different random initializations, so it'd be good to average that nondeterminisim out of your plots."
Compute Jacobian matrix of Deep learning model? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am trying to implement this paper. In this paper, the author uses the forward derivative to compute the Jacobian matrix dF/dx using chain rule where F is the probability got from the last layer and X is input image.
My model is given below. Kindly let me know how to go about doing that?
class LeNet5(nn.Module):

def __init__(self):

    self.derivative= None # store derivative

    super(LeNet5, self).__init__()
    self.conv1= nn.Conv2d(1,6,5)
    self.relu1= nn.ReLU()
    self.maxpool1= nn.MaxPool2d(2,2)

    self.conv2= nn.Conv2d(6,16,5)
    self.relu2= nn.ReLU()
    self.maxpool2= nn.MaxPool2d(2,2)

    self.conv3= nn.Conv2d(16,120,5)
    self.relu3= nn.ReLU()

    self.fc1= nn.Linear(120,84)
    self.relu4= nn.ReLU()

    self.fc2= nn.Linear(84,10)
    self.softmax= nn.Softmax(dim= -1)


def forward(self,img, forward_derivative= False):
    output= self.conv1(img)
    output= self.relu1(output)
    output= self.maxpool1(output)

    output= self.conv2(output)
    output= self.relu2(output)
    output= self.maxpool2(output)

    output= self.conv3(output)
    output= self.relu3(output)

    output= output.view(-1,120)
    output= self.fc1(output)
    output= self.relu4(output)

    output= self.fc2(output)
    F= self.softmax(output)

    # want to comput the jacobian dF/dimg 
    jacobian= computeJacobian(F,img)#how to write this function

    return F, jacobian

","['deep-learning', 'feedforward-neural-networks']",
Does a solution for Wumpus World with neural networks exist?,"
The Wumpus World proposed in book of Stuart Russel and Peter Norvig, is a game which happens on a 4x4 board and the objective is to grab the gold and avoiding the threats that can kill you. The rules of game are:

You move just one box for round
Start in position (1,1), bottom left
You have a vector of sensors for perceiving the world around you.
When you are next to another position (including the gold), the vector is 'activated'.
There is one wumpus (a monster), 2-3 pits (feel free to put more or less) and just one gold pot
You only have one arrow that flies in a straight line and can kill the wumpus
Entering the room with a pit, the wumpus or the gold finishes the game


Scoring is as follows: +1000 for grabbing the gold, -1000 for dying to the wumpus, -1 for each step, -10 for shooting an arrow. Fore more details about the rules, chapter 7 of the book explains them.
Well now that game has been explained, the question is: in the book, the solution is demonstrated by logic and searching, does there exist another form to solve that problem with neural networks? If yes, how to do that? What topology to use? What paradigm of learning and algorithms to use?
1*: My English is horrible, if you can send grammar corrections, I'm grateful.
2*: I think this is a bit confusing and a bit complex. if you can help me to clarify better, please do commentary or edit!
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'game-ai', 'learning-algorithms']","Yes! If you read ahead to the chapters in reinforcement learning in the same book, you'll see that the wompus world appears again there. Techniques like Q-learning can be used to solve it, and since Q-learning involves learning the shape of a function, a neural network can be employed as a function approximator.The basic idea is to treat this problem as an input/output mapping (states -> actions), and to learn which actions produce the greatest rewards. Note however, that these approaches rely on trial and error. The logic based approach reasons about the rules of the game, and can play reasonably well right away. The learning approach will need to try and fail many times before playing well."
Machine learning to predict 8*8 matrix values using three independent matrices,"
Problem Statement
I have 4 main input features.
This is a small snippet of the data for clearer understanding.
Gate name -> for example AND Gate
index_1 -> [0.001169, 0.005416, 0.01391, 0.03037, 0.06381, 0.1307, 0.2645, 0.532]
index_2 -> [7.906e-05, 0.001123, 0.00321, 0.007253, 0.01547, 0.03191, 0.06478, 0.1305]
values -> [[11.0081, 14.0303, 18.8622, 27.3426, 43.8661, 76.7538, 142.591, 274.499], 
           [11.3461, 14.3634, 19.1985, 27.6827, 44.2106, 77.0954, 142.926, 274.879], 
           [12.258, 15.2816, 20.1095, 28.5856, 45.1057, 77.9778, 143.8, 275.758], 
           [13.665, 16.7457, 21.5835, 30.0545, 46.5581, 79.4212, 145.252, 277.192], 
           [15.6636, 18.9526, 23.9051, 32.4281, 48.9011, 81.7052, 147.477, 279.371], 
           [17.8838, 21.5839, 26.8957, 35.7103, 52.3901, 85.2132, 150.89, 282.714], 
           [19.3338, 23.6933, 29.7184, 39.1212, 56.4053, 89.9721, 155.913, 287.637], 
           [18.7856, 23.9999, 31.1794, 41.7549, 60.0043, 95.0488, 162.951, 295.005]]
My task is to predict this values matrix, given that I have index_1 and index_2. Originally this values matrix is propagation delay, calculated using a simulator called SPICE. 
Where I am facing problem

There is no written relation between Index_1, index_2 or values since simulator calculates this value using it's own models. 
I have made a CSV file which contains the data in separate columns. 
Another approach that I thought. If I can give index_1, index_2 and any 5*5 sub-matrix to the model, and the model can predict the values of whole 8*8 Matrix. But the problem is again, which machine learning model do I use. 

Approaches Tried so Far

I have tried a CNN model for this but it is giving me very low accuracy.
Used one dense fully connected neural network but it is over-fitting the data and not giving me any values for matrix.

I am still stuck at how to predict the matrix values given this data.  What are other strategies can be used? 
","['machine-learning', 'deep-learning']",
Mapping Actions to the Output Layer in Keras Model for a Board Game,"
I have created a game based on this game here. I am attempting to use Deep Q Learning to do this, and this is my first foray into Neural networks (please be gentle!!)
I am trying to create a NN that can play this game. Here are some relevant facts about the game:

Player 1 (the fox) has 1 piece that he can move diagonally 1 step in any direction 
Player 2(The geese) has 4 pieces that they can move only forward diagonally (either diagonal left or diagonal right) 1 step.
The Fox wins if he reaches the other end of the board, the geese win if they trap the fox so it cannot move.

I am trying to work on the agent first for the geese as it seems to be the harder agent with more pieces and restrictions. Here is the important sections of code I have so far:

This is where I setup the game board, and set the total actions for the geese

def __init__(self):
    self.state_size = (LENGTH,LENGTH) ##LENGTH is 8 so (8,8)
    #...
    #other DQN variables that aren't important to question
    #...
    self.action_size = 8 ##4 geese, each can potentially make 2 moves
    self.model = self.build_model()


And here is where I create my model

def build_model(self):
    #builds the NN for Deep-Q Model
    model = Sequential() #establishes a feed forward NN
    model.add(Dense(64,input_shape = (LENGTH,), activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(self.action_size, activation = 'linear'))
    model.compile(loss='mse', optimizer='Adam')


This is where I perform an action

def act(self, state,env):
    #get the list of allowed actions for the geese
    actions_allowed = env.allowed_actions_geese_agent()

    if np.random.rand(0,1) <= self.epsilon: ##do a random move
        return actions_allowed[random.randint(0, len(actions_allowed)-1)]
    act_values = self.model.predict(state)
    print(act_values)
    return np.argmax(act_values)


My question: Since there are 4 geese and each can make 2 possible moves, am I correct in thinking that my action_size should be 8 (2 for each goose) or should it be maybe 2 (for diagonal left or right) or something else entirely? 

The reason why I am at a loss is because on any given turn, some of the geese may have an invalid move, does that matter?

My next Question: Even if I have the right output layer for the geese agent, when I call model.predict(state) where I pick my action...how do I interpret the output? And how would I map that action it selects to a valid action that can be made?

Here is a picture of the result of using model.predict(state), as you can see it returns a ton of data and then when I call return np.argmax(act_values) I get 59 back...not sure how to utilize that (or if it's even correct based on my output layer)... and finally I included a drawing of the board. F is the fox and 1,2,3,4 are the different geese.

I apologize for the massive post, but I am just trying to provide as much information that is helpful. 
","['machine-learning', 'game-ai', 'python', 'keras']",
How do we stack two U-Nets to yield one final prediction?,"
I am trying to reproduce the model described in the paper DocUNet: Document Image Unwarping via A Stacked U-Net, i.e. stacking two U-Nets to yield one final prediction. The paper mentions that: 

The deconvolution features of the first U-Net and the intermediate prediction y1 are concatenated together as the input of the second U-Net.

What does it mean by concatenating deconvolution features and the prediction (which is an array? cm)?
The next paragraph says that:

The second U-Net finally gives a refined prediction y2, which we use as the final output of our network. We apply the same loss function to both y1 and y2 during training.

It leads to the next question: Does it mean that I have to train U-Net twice?
","['neural-networks', 'convolutional-neural-networks', 'papers', 'u-net']",
SEIF motion update algorithm doubt,"
I want to implement Sparse Extended information slam. There is four step to implement it. The algorithm is available in Probabilistic Robotics Book at page 310, Table 12.3.

In this algorithm line no:13 is not very clear to me. I have 15 landmarks. So $\mu_t$ will be a vector of (48*1) dimension where (3*1) for pose. Now $H_t^i$ is a matrix whose columns are dynamic as per the algorithm it is (3j-3) and 3j. J is the values of landmarks 1 to 15. Now how could I multiply a dynamic quantity with a  static one. There must be a error that matrix dimension mismatch when implement in matlab.
Please help me to understand the algorithm better. 
","['research', 'intelligent-agent', 'autonomous-vehicles', 'probability', 'robotics']","You are right, that pseudocode is not correct. In particular, the definition of $H_t^i$ in line $11$ should be changed; all the way on the right-hand side, it should have $3N - 3j$ columns of $0$s, rather than $3j$ columns of $0$s.With that change, every matrix $H_t^i$ will have the same number of columns: $$6 + 3j - 3 + 3N - 3j = 3 + 3N,$$which evaluates to a total of $48$ in your case (because you have $N = 15$ landmarks). That's precisely the correct dimensionality required for matrix multiplication with your $\mu_t$ vector.The version of the book that you linked to appears to be a fairly old draft. This webpage contains errata for the third edition for the book, in which page 393 corresponds to what was page 310 in your version of the book. The errata for that third edition of the book can be downloaded at the following URL: http://probabilistic-robotics.informatik.uni-freiburg.de/corrections/pg393.pdfThere you'll find the fix that I described above, but also some other fixes (most of them are just notational, adding bars over the $\mu$ vectors, but it looks like a more serious issue was additionally fixed in line 13, where a minus was changed to a plus)."
What is chaotic behavior and how it is achieved in non-linear regression and artificial networks?,"
I'm finding it hard to understand the relationship between chaotic behavior, the human brain, and artificial networks.  There are a number of explanations on the web, but it would be very helpful if I get a very simple explanation or any references providing such simplifications.
","['deep-learning', 'philosophy', 'human-inspired', 'convergence']","Regression for models more complex than $y = a x + b$ is a convergence strategy. Surface fitting algorithms, such as Levenberg–Marquardt, are often successful at achieving regression using a damped version of least squares as an optimization criterion. The marriage of regression and the multilayer perceptron, an early model artificial network, led to the use of a back propagation strategy to distribute corrective signals that drive regression.Back propagation using gradient descent is now used in artificial networks with a variety of cell and connection designs, such as LSTM and CNN networks as a convergence strategy. Both surface fitting and artificial network convergence share the method of successive approximation. With each successive application of some test, the result is used to attempt to improve the next iteration. Proofs have developed around convergence for many algorithms. Actual successive approximation runs have five possible outcomes.The following illustration from Chaos Theory Tamed (Garnett P. Williams, 1997, p 164) modified slightly for easy viewing can explain how chaos arises when the learning rate or some other factor is set too aggressively.  The graphs are of the behavior of the logistic equation $x_{i+1} = k x_i (1 - x)$ which plots as an inverted parabola in phase space.  The one dimensional maps on the right of each of five cases show the relationship between adjacent values in the time series on the left of each of the five.  Although the logistic equation is quite simple compared to regression algorithms and artificial nets, the principles involved are the same.The right hand cases, with $k = 3.4$ and $k = 3.75$ correspond to the last two possible outcomes in the list above, oscillation and chaos respectively.Care in Drawing ParallelsCare should be taken in drawing parallels between distinct things.Regression and artificial networks can be compared meaningfully because the math for each is fully defined and easy for those with the mathematical skill to analyze them for the comparison.Comparing known mathematical systems with unknown biological ones is interesting, but to a large degree, grossly premature.  The perceptron, on which MLPs (multilayer perceptrons) and their deep learning derivatives are based, are simplified and flattened models of what was once thought to be how neurons in the brain work.  By flattened is meant that they are placed in the time domain where they are convenient for looping in software and do not take into consideration these complexities.In summary, multilayer perceptrons are not a model of neural networks in the human brain.  They are merely roughly inspired by obsolete knowledge of them.Chaos in the Human BrainWhether there is chaotic behavior in the brain is known.  It has been observed in real time.  How coupled it is with human intelligence is a matter of conjecture, but it is already clear that it may appear to contribute to function in some cases and contribute to dysfunction in others.  This is also true in artificial systems.In summary, chaos in a system is neither productive nor counterproductive in every case.  It depends on where it is in the system (in detail) and what the system design is expected to perform."
CNN Pooling layers unhelpful when location important?,"
I'm trying to use a CNN to analyse statistical images. These images are not 'natural' images (cats, dogs, etc) but images generated by visualising a dataset. The idea is that these datasets hopefully contain patterns in them that can be used as part of a classification problem.

Most CNN examples I've seen have one of more pooling layers, and the explaination I've seen for them is to reduce the number of training elements, but also to allow for some locational independance of an element (e.g. I know this is an eye, and can appear anywhere in the image).
In my case location is important and I want my CNN to be aware of that. ie. the presence of a pattern at a specific location in the image means something very specific compared to if that feature or pattern appears elsewhere.
At the moment my network looks like this (taken from an example somewhere):
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 196, 178, 32)      896       
_________________________________________________________________
activation_1 (Activation)    (None, 196, 178, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 98, 89, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 96, 87, 32)        9248      
_________________________________________________________________
activation_2 (Activation)    (None, 96, 87, 32)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 48, 43, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 46, 41, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 46, 41, 64)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 23, 20, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 29440)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                942112    
_________________________________________________________________
activation_4 (Activation)    (None, 32)                0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 99        
_________________________________________________________________
activation_5 (Activation)    (None, 3)                 0         
=================================================================
Total params: 970,851
Trainable params: 970,851
Non-trainable params: 0
_________________________________________________________________

The 'images' I'm training on are 180 x 180 x 3 pixels and each channel contains a different set of raw data.
What strategies are there to improve my CNN to deal with this? I have tried simply removing some of the pooling layers, but that greatly increased memory and training time and didn't seem to really help.
","['deep-learning', 'convolutional-neural-networks', 'keras']",
How can I use A.I/Image Processing to construct mathematical graphs from drawing?,"
In physics, there are a lot of graphs, such as 'velocity vs time' , 'time period vs length' and so on. 
Let's say I have a sample set of points for a 'velocity vs time' graph. I draw it by hand, rather haphazardly, on a canvas. This drawn graph on the canvas is then provided to the computer. By computer I mean AI. 
I want it to sort of beautify my drawn graph, such as straightening the lines, making the curves better, adding the digits on axes and so on. In other words, I want it to give me a better version of my drawn graph which I can readily use in, say, a word document for a report.
a) Is it possible/plausible to do this?
b) Are there any APIs available that can already do this? (Don't want to reinvent the wheel)
c) Any recommendations/suggestions to make the idea possible by altering it somehow?
",['image-recognition'],
What heuristic to use when doing A* search with multiple targets? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Usually, using the Manhattan distance as a heuristic function is enough when we do an A* search with one target. However, it seems like for multiple goals, this is not the most useful way. Which heuristic do we have to use when we have multiple targets?
","['search', 'heuristics', 'a-star']",
Can neural network take decision about its own weights (update of weights)?,"
Can neural network take decision about its own weights (update of weights) during training phase or during the phase of parallel training and inference? When one region of hierarchical NN takes decision about weights of other region is the special case of my question.
I am very keen to understand about self-awareness, self-learning, self-improvement capabilities of neural networks, because those exactly those self-* capabilities are the key path to the artificial general intelligence (e.g. Goedel machine). Neural networks are usually mentioned as examples of special, single-purpose intelligence but I can not see the reason for such limitation if NN essentially trys to mimic human brains, at least in purpose if not in mechanics.
Well - maybe this desired effect is already effectively achieved/emerges in the operation of recurrent ANNs as the effect of collective behavior?
","['neural-networks', 'artificial-consciousness', 'superintelligence']",
Neural network as (BDI) agent - running in continuous mode (that do inference in parallel with learning)?,"
Is there research work that uses neural network as the (BDI) agent (or even full-scale cognitive architecture like Soar, OpenCog) - that continuously receives information from the environment and act in an environment and modifies its base of belief in parallel? Usually NN are trained to do only one task and TensorFlow/PyTorch supports batch mode only out of the box. Also NN algorithms and theory are constructed assuming that training and inference phases are clearly separated and they have each own algorithms. So - completely new theory and software can be required for this - are there efforts in this direction? If no, then why not? It is so self-evident that such systems can be of benefit.
https://arxiv.org/abs/1802.07569 is good review about incremental learning and it contains chapters of implemented systems, but all of them still separates learning phase from inference phase. Symbolic systems and symbolic agents (like JSON AgentSpeak) can have updating belief/knowledge base and they can also act during  receiving new information or during forming new beliefs. I am specifically seeking research about NNs which do learning and inference in parallel. As far as I sought then this separation still persists in self-organizing incremental NNs that are gaining some popularity.
I can image the construction of chained NNs in Tensorflow - there is some controller network that receives input (possibly preprocessed by hierarchically lower networks) and that decides what to the: s.c. mental actions are the ouput of this controller, these actions determine whether some subordinated network is required to undergo additional learning or whether it can be temporary used for the processing of some information. Central network itself, of course, can decide to move into temporary learning phase from time to time to improve its reasoning capabilities. Such pipeline of master-slave networks is indeed possible in TensorFlow but still TensorFlow will have one central clock, not distributed, loosely connected processing. But I don't know whether existence of central clock is any restriction on the generality of capabilities of such system. Well, this hierarchy of networks maybe can be realized inside the one large network as well - maybe this large network can allow separate parts (subsets of neurons) to function in somehow independent and mutually controlling mode, maybe such regions of large neural network can emerge indeed. I am interested in this kind of research - maybe there are available some good papers for this?
","['neural-networks', 'intelligent-agent']",
Which artificial intelligence algorithms could use tensor specific hardware?,"
AI algorithms involving neural networks can use tensor specific hardware. Are there any other artificial intelligence algorithms that could benefit from many tensor calculations in parallel?  Are there any other computer science algorithms (not part of AI) that could benefit from many tensor calculations in parallel?
Have also a look at TensorApplications and Application Theory.
","['algorithm', 'applications', 'hardware']",
Does balancing the training data set distribution for a neural network affect its understanding of the original distribution of data?,"
I have a very imbalanced dataset of two classes: 2% for the first class and 98% for the second. Such imbalance does not make training easy and so balancing the data set by undersampling class 2 seemed like a good idea. 
However, as I think about it, should not the machine learning algorithm expect the same data distribution in nature as in its training set? I know, for sure, that the distribution of data in nature matches my imbalanced dataset. Does that mean that the balanced dataset will negatively affect the neural net performance with testing? when it assumed a different distribution of data caused by my balanced data set. 
","['neural-networks', 'machine-learning', 'deep-learning']","This is a very good question. Your problem is the classic classification problem of Neural Networks. In this problem the main objective of the Neural Network is to transform the data by some non-linear (in general) transformation so that the data becomes linearly separable for the final layer to perform classification.Point to Note: This is not a regression problem, so that you are trying to fit a curve. Whenever there is a regression problem logically you can use a PDF to write some kind of information about new data. You can express mathematically the probability of your data falling within a certain range of error, since this is a optimising continuous function problem (generally RMSE). This is not in the case of classifier. Classifier follow Bernoulli probability (though we represent the cost function as continuous). So current event is independent of past events. This makes a classifier harder to train for unbalanced class. So if we write:It pretty much has 98% accuracy, but you can understand we do not want this type of classifier.In general we want both classes to have good accuracy scores, sometimes this is measured by $F1_{score}$ but I like to think it in terms of proportionality. If we have $n$ examples with $m$ in one class, then and the classifier $a_1$ and $a_2$ correct predictions in both classes respectively, then I would check both the metric $ \frac {a_1}{m}$ and $ \frac {a_2}{n-m}$, which gives you the general idea.Also in practical the identification 2% is sometimes far more important than the rest 98% (air-plane defects, cancer detection). So we use special ML algorithm called Anomaly Detectors for such type of problems."
Calculation of GPU memory consumption on softmax layer doesn't match with the empirical result,"
I'm training a language model with 5000 vocabularies using a single M60 GPU (w/ actually usable memory about 7.5G). 
The number of tokens per batch is about 8000, and the hidden dimension to the softmax layer is 512. So, if I understand correctly, fully-connected (softmax) layer theoretically consumes 5000*8000*512*4=81.92GB for a forward pass (4 is for float32).
But the GPU performed the forward and backward passes without any problem, and it says the GPU memory usage is less than 7GB in total. 
I used PyTorch. What's causing this? 
EDIT: To be clearer, the input to the final fc layer (256x5000 matrix) is of size [256, 32, 256]. 
","['neural-networks', 'natural-language-processing']","GPU DRAM capacity - 7.5G
the below link explains how nVIDIA GPU cUDNN does memory optimization.
https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/
the below link has detailed steps to calculate memory required by parameters and data.
http://cs231n.github.io/convolutional-networks/#case
one data point missing in question is number of output classes in softmax layer.
The above two links will help you to calculate memory required and how software handles large matrix multiplication. "
Adding voices to voice synthesis corpuses,"
If one uses one of the open source implementations of the WaveNet generative speech synthesis design, such as https://r9y9.github.io/wavenet_vocoder/, and trains using something like the CMU's arctic corpus, now can one add a voice that sounds younger, older, less professional, or in some other way distinctive.  Must the entire training begin from scratch, or is there a more resource and time friendly way?
","['training', 'generative-model', 'speech-synthesis']",
Neural networks of arbitrary/general topology?,"
Usually neural networks consist from layers, but is there research effort that tries to investigate more general topologies for connections among neurals, e.g. arbitrary directed acyclic graphs (DAGs).
I guess there can be 3 answers to my question:

every imaginable DAG topology can be reduced to the layered DAGs already actively researched, so, there is no sense to seek for more general topologies;
general topologies exist, but there are fundamental restrictions why they are not used, e.g. maybe learning is not converging in them, maybe they generate chaotic osciallations, maybe they generate bifurcations and does not provide stability;
general topologies exist and are promising, but scientists are not ready to work with them, e.g. maybe they have no motivation, standard layered topologies are good enough.

But I have no idea, which answer is the correct one. Reading the answer on https://stackoverflow.com/questions/46569998/calculating-neural-network-with-arbitrary-topology I start to think that answer 1 is the correct one, but there is no reference provided.
If answer 3 is correct, then big revolution can be expected. E.g. layered topologies in many cases reduces learning to the matrix exponentiation and good tools for this are created - TensorFlow software and dedicated processors. But there seems to be no software or tools for general topologies is they have some sense indeed.
","['neural-networks', 'topology']","The simplistic neural networks that have been given away for free after they prove insufficient by themselves in field use consist solely of two orthogonal dimensions.However, in large corporations that have AI pipelines, this is not the case.  We are beginning to see more interesting topologies in open source.  We see this in generative systems for images, text, and speech.  We see this in robotic control of robots.  The truth is that these more sophisticated topologies have been in play for years, but were just not appearing in the open source community because they were company confidential.  Enough academic work, releasing of portions of corporate IP, and the accumulation of independent OSS work has occurred to start to see these topologies in GIT repos.Cyclic Not AcyclicArtificial network topologies are generally cyclic, not acyclic in terms of their causality or their signal pathways, depending on how you depict them theoretically.  These are three basic examples from among dozens in the literature and in the open source repositories.Back-propagation represents the introduction of a deliberate cycle in signal paths in a basic multilayer perceptron, making that topology a sequence of layers represented by vertices, connected sequentially by a set of directed edges representing forward propagation, and a set of directed edges in the reverse direction to distribute the corrective error determined at the network output according to the principle of gradient descent.  For efficiency, the corrective signal is distributed recursively backward through the layers to the $N - 1$ matrices of parameters attenuating the signals between $N$ layers.  Back propagation requires the formation of these $N - 1$ cycles for convergence to occur.In a generative adversarial network (GAN), we have the signal path of each of the two networks feeding the training criteria of the other.  Such a topological arrangement is like negative feedback in a stable control system in that an equilibrium is formed between the generative network and discriminative network. The two directed edges, (a) the one that causally affects G with Ds result, and (b) the one that causally affects D with Gs result, create a cycle on top of the cycles in each of G and D.In attention based networks being touted as theoretically advantageous over LSMT (which has been dominating over CNNs) has a much more complex topology and more cycles above those in supervisory layer than those in GANs.Analysis of Answer One of ThreeIt is true that every directed graph can be realized in an arbitrarily large RNN because they are Turing complete, but that doesn't mean they are a great topology for all finite algorithms.Turing was aware that his punched tape model was not the best general purpose, high speed computing architecture.  He was not intending to prove anything about computing speed but rather what could be computed.  His Turing machine had a trivial topology deliberately.  He wanted to illustrate his completeness theorem to others and resurrect the forward movement of rationalism after Gödel disturbed it with his two incompleteness theorems.Similarly, John von Neumann proposed his computing architecture, with a central processing unit (CPU) and unified data and instruction bus, to reduce the number of relays or vacuum tubes, not to maximize parallel algorithm execution.  That topology as a directed graph has the instruction controller and the arithmetic unit in the center and everything else branching out from the data and address bus leading from them.That a topology can accomplish a task is no longer a justification for persisting in the use of that topology, which is why Intel acquired Nirvana, which deviates from traditional von Neumann architecture, DSP architecture, and the current CUDA core architecture that NVidia GPUs use and offer for artificial network realization through C libraries that can be called via integrated Java and Python adapters.There is definitely sense to seek for more general topologies, if they are fit for the purpose, just as with Turing's or von Neuman's.Analysis of Answer Two of ThreeGeneral topologies exist, the most economically viable of which is the CUDA cores begun by NVidia, which can be configured for MLPs, CNNs, RNNs, and general 2D and 3D video processing.  They can be configured with or without cycles depending on the characteristics of the parallelism desired.The realization of topologies unlike the Cartesian arrangements of activation functions in artificial networks or the kernel cells in convolution engines do have barriers to use, but they are not fundamental restrictions. The primary barrier is not one of hardware or software.  It is one of linguistics.  We don't think topologically because we don't talk topologically.  That's what is great about this question's challenge.FORTRAN began to dominate over LISP during the time when general purpose programming began to emerge in many corporations.  That is not surprising because humans communicate in orthogonal ways.  It is cultural.  When a child scribbles, teachers are indoctrinated to say nice things but respond by drawing a shape.  If the child draws a square, the teacher smiles.  The child is given blocks.  The books are rectangular.  Text is justified into rectangles.We can see this in building architecture dating back to Stonehenge.  Ninety degree angles are clearly dominant in artificial things, whereas nature doesn't seem to have that bias.Although directed graphs were easy to implement and traverse in recursive structure and were commonplace in the LISP community.  FORTRAN with its realization of vectors and matrices in one and two dimensional arrays respectively were easier to grasp by those with less theoretical background in data structures.The result is that even if learning EMMASCRIPT (JavaScript) which has its seed from the LISP community and is not biased toward orthogonal data structures, people tend to proceed from HelloWorld.js to something with a basic loop in it, with an underlying array through which the loop iterates.There are three wonderfully inquisitive and insightful phrases in answer two of three.Analysis of Answer Three of ThreeGeneral topologies exist and are promising and researchers are ready to work with them.  It is enthusiasts that can have a dismissive attitude.  They don't yet understand the demos they've downloaded and painstakenly tweaked to run on their computer, they're about to launch their AI carrier amidst the growing demand from all the media hype, and now someone is introducing something interesting and not yet implemented in code.  The motivational direction is generally to either dismiss or resist the creative proposals.In this case, Google, CalTech, IBM, MIT, U Toronto, Intel, Tesla, Japan, and a thousand other governments, institutions, corporations, and open source contributors will solve that problem, provided people keep talking about topology and the restrictions inherent in purely Cartesian thinking.Misunderstanding Topology to Mean Dimensionality or TopographyThere has been some confusion in terms.  The SO reference in the question is an example of thinking that changing an array dimension is changing the topology.  If such were so, then there would be no change one could make to the geometry of an AI system that would not be topological.  Topology can only have meaning if there are features that are not topological.  When one draws a layer, they don't need to increase the height of the rectangle representing it if the number of activitations, the width of the layer, is changed from 100 to 120.I've also seen academic papers that called the texture or roughness of an error surface its topology.  That completely undermines the concept of topology.  They meant to use the term topography.  Unfortunately neither the publisher nor the editor noticed the error.Software or ToolsMost programming languages support directed graphs in recursive hashmaps.  LISP and its derivatives supported them at a more machine instruction efficient level, and that's still the case.  Object oriented databases and graph libraries exist and are in use.  Google uses them extensively in web indexing and lookup.  FaceBook's API is called the Graph API, because it is a query and insert API into the graph that is FaceBook's user data store.The explosion is here in global software giants.  There is open source for it.  The revolution that is missing is among those who are not yet educated as to the meaning of topology, the difference between a hierarchy and a network or the role of feedback in any learning system.Regarding Java and Python there are many barriers to the revolution in thinking, primarily these.Graphviz and other graphing software that auto-generates diagrams from unconstrained directed or bidirectional graph representations have done much to bust through the barriers because the generated images are visible across the web.  It may be through visual representations of graphs that linguistic representations, thought, hardware, and software begin to emerge representing the paradigm shift the question investigates.It is not that constraints are not useful.  Only some patterns and paradigms produce results, but since the results from the human brain demand attention, and the human brain isOne can all but conclude that those are not particularly well chosen constraints.  Neither is the acyclic criteria.  Nature is cyclic, and intelligence probably requires it in many ways and at many levels."
What is the approach to deduce formal rules based on data?,"
We have data in text format as sentences.
The goal is to detect rules which exist in this set of sentences.
I have a limited set of contextless sentences that fit a pattern and want to find the pattern.
I might not have sentences that don't fit the pattern.
What should be an approach to do that?
","['machine-learning', 'learning-algorithms', 'unsupervised-learning']",
Convolutional Layers on a hexagonal grid in Keras [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



Keras' convolutional and deconvolutional layers are designed for square grids. Is there was a way to adapt them for use in hexagonal grids?
For example, if we were using axial coordinates, the input of the kernel of radius 1 centered at (x,y) should be:
[(x-1,y), (x-1,y+1), (x,y-1), (x,y+1), (x+1,y-1), (x+1, y)]
One option is to fudge it with a 3 by 3 box, but then you are using cells at different distances.
Some ideas:

Modify Kera's convolutional layer code to use those inputs instead of the default inputs. The problem is that Kera calls its backend instead of implementing it itself, which means we need to modify the backend too.
Use a 3 by 3 box, but set the weights at (x-1,y-1) and (x+1,y+1) to zero. Unfortunately, I do not know how to permanently set weights to a given value in Kera.
Use cube coordinates instead of Axial coordinates. In this case, a 3 by 3 by 3 box will only contain the central hex's neighbors and inputs set to 0. The problem is that it makes the input array much bigger. Even more problematic, some coordinates that correspond to non-hexes (such as (1,0,0)) will be assigned non-zero outputs (since (0,0,0) falls within its 3 by 3 by 3 box).

Are there any better solutions?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'python', 'keras']",
"What is the dimensionality of the output map, given the dimensionality of the input map, number of filters, stride and padding?","
I am trying to understand the dimensionality of the outputs of convolution operations. Suppose a convolutional layer with the following characteristics:

Input map $\textbf{x} \in R^{H\times W\times D}$
A set of $F$ filters, each of dimension $\textbf{f} \in R^{H'\times W'\times D}$
A stride of $<s_x, s_y>$ for the corresponding $x$ and $y$ dimensions of the input map
Either valid or same padding (explain for both if possible)

What should be the expected dimensionality of the output map expressed in terms of $H, W, D, F, H', W', s_x, s_y$?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
Are artificial networks based on the perceptron design inherently limiting?,"
At the time when the basic building blocks of machine learning (the perceptron layer and the convolution kernel) were invented, the model of the neuron in the brain taught at the university level was simplistic.

Back when neurons were still just simple computers that electrically beeped untold bits to each other over cold axon wires, spikes were not seen as the hierarchical synthesis of every activity in the cell down to the molecular scale that we might say they are today. In other words, spikes were just a summary report of inputs to be integrated with the current state, and passed on. In comprehending the intimate relationships of mitochondria to spikes (and other molecular dignitaries like calcium) we might now more broadly interpret them as synced messages that a neuron sends to itself, and by implication its spatially extended inhabitants. Synapses weigh this information heavily but ultimately, but like the electoral college, fold in a heavy dose of local administration to their output. The sizes and positions within the cell to which mitochondria are deployed can not be idealized or anthropomorphized to be those metrics that the neuron decides are best for itself, but rather what is thermodynamically demanded.1

Notice the reference to summing in the first bolded phrase above.  This is the astronomically oversimplified model of biology upon which contemporary machine learning was built.  Of course ML has made progress and produced results.  This question does not dismiss or criticize that but rather widen the ideology of what ML can become via a wider field of thought.
Notice the second two bolded phrases, both of which denote statefulness in the neurons.  We see this in ML first as the parameters that attenuate the signals between arrays of artificial neurons in perceptrons and then, with back-propagation into deeper networks.  We see this again as the trend in ML pushes toward embedded statefulness by integrating with object oriented models, the success of LSTM designs, the interrelationships of GAN designs, and the newer experimental attention based network strategies.
But does the achievement of higher level thought in machines, such as is needed to ...

Fly a passenger jet safely under varying conditions,
Drive a car in the city,
Understand complex verbal instructions,
Study and learn a topic,
Provide thoughtful (not mechanical) responses, or
Write a program to a given specification

... requiring from us a much more radical is the transition in thinking about what an artificial neuron should do?
Scientific research into brain structure, its complex chemistry, and the organelles inside brain neurons have revealed significant complexity.  Performing a vector-matrix multiplication to apply learning parameters to the attenuation of signals between layers of activations is not nearly a simulation of a neuron.  Artificial neurons are not very neuron-like, and the distinction is extreme.
A little study on the current state of the science of brain neuron structure and function reveals the likelihood that it would require a massive cluster of GPUs training for a month just to learn what a single neuron does.

Are artificial networks based on the perceptron design inherently limiting?

References
[1] Fast spiking axons take mitochondria for a ride,
by John Hewitt, Medical Xpress, January 13, 2014, 
https://medicalxpress.com/news/2014-01-fast-spiking-axons-mitochondria.html
","['neural-networks', 'research', 'hardware', 'architecture', 'long-short-term-memory']","In my opinion, there are many functions in our brain. Surely much more than  the artificial neural network nowadays. I guess this is the field of brain science or cognitive psychology.Some brain structures may help for certain applications, but not all. Neural network though is a simplest form of our brain, but has the most general usages. On the other words, if you want to improve the neural networks, different fields or different functions may needs totally different structures. You can refer this as so many types of neural networks nowadays for different applications."
How to find a cost function for human data,"
Currently, I am interested in how NNs or any other AI models can be used for composing music. 
But there are many other interesting applications too, like language processing.
I am wondering that: NNs generally need a cost function for learning. But for example, for composing music, what would be an appropriate cost function? I mean,  algorithms can't (yet) really 'calculate' how good music is, right?
",['neural-networks'],"You've hit upon the central conundrum of supervised learning: if you want a machine to learn to do something, you need to know how to explain what that something is.In the case of music, there are several possible approaches:Make one set of ""bad"" songs, and one set of ""good"" songs. Develop a measure of how similar two songs are (maybe euclidian distance between their discrete Fourier transforms is a good starting place?). Your cost function is then based minimizing the average distance to ""good"" songs, and maximizing the average distance to ""bad"" songs. This may not work well though, because good and bad songs might differ only in the occasional misplaced notes.Move to a reinforcement learning paradigm. Listen to each song proposed by your network. Give is a score based on your subjective enjoyment. Your cost function is based on maximizing this score. This might work well, but again, it might not. Music is tricky.Use unsupervised approaches. Reward your network just for making something that resembles music (perhaps using the Fourier transform approach above), without labelling good and bad. The advantage is that you don't need to decide what is good or bad, and so you can use a lot more music in your dataset. The drawback is music as a whole might be too diverse to learn easily from examples.Treat your music as a sequence of notes, and train a generative model to predict future notes on the basis of past notes. You can then generate new music by starting the model with a set of notes and letting it generate new ones for a long time."
"Why do we use $D(x \mid y)$ and not $D(x,y)$ in conditional generative adversarial networks?","
In conditional generative adversarial networks (GAN), the objective function (of a two-player minimax game) would be
$$\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x} | \boldsymbol{y})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z} | \boldsymbol{y})))]$$
The discriminator and generator both take $y$, the auxiliary information.
I am confused as to what will be the difference by using $\log D(x,y)$ and $\log(1-D(G(z,y))$, as $y$ goes in input to $D$ and $G$ in addition to $x$ and $z$?
","['neural-networks', 'machine-learning', 'generative-adversarial-networks', 'papers', 'generative-model']","It looks like you're asking about the difference between using conditional and joint probabilities.The joint probability $$D(x,y)$$ is the probability of x and y both happening together.The conditional probability $$D(x | y)$$ is the probability that x happens, given that y has already happened. So, $$D(x,y) = D(y) * D(x | y)$$. Notice that, in a C-GAN, we have some extra information that is given, like a class label $y$. We actually don't care at all about how likely that information is to appear. We care only about how likely it is to appear with a given $x$ from the source distribution, versus how likely it is to appear with a given $z$ from the generated distribution.If you tried to minimize the joint probabilities, you would be attempting to change something that the networks have no ability to control (the chance of $y$ appearing)."
Can a crossover result in a node with no outgoing connections?,"
I'm currently implementing the original NEAT algorithm in Swift.
Looking at figure 4 in Stanley's original paper, it seems to me there is a chance that node 5 will have no (enabled) outgoing connection if parent 1 is assumed the fittest parent and the connection is randomly picked from parent 2.
Is my understanding of the crossover function correct and can it indeed result in a node with no outgoing connections?
",['neat'],"The disabled/enabled bit in a connection gene indicates whether or not it should be expressed in the calculation of the network.Here's an example:
This is a neural network and its corresponding connection genes which represent the layout of the network among other things.
The top connection gene going from 1 -> 3 that has a weight of 0.1 is expressed in the calculation of the network.
The bottom connection gene going from 2 -> 3 that has a weight of 0.4 is not expressed in the calculation of the network.Calculating the network:1633 * 0.1 = 163.3Given the weights in the example the output of this network is 163.3Hypothetically, if both connection genes had their enabled bit set to true (which can happen in the future, to quote the paper: ""The disabled genes may become enabled again in future generations"") then the output of the network would be:(1633 * 0.1) + (10 * 0.4) = 167.3To answer your question, the connection between two nodes still exists in the network regardless of whether the bit in the gene is enabled or disabled, but, if the bit in the gene is disabled it is not used when the output of the network is being calculated as I've shown above."
Why does the policy network in AlphaZero work?,"
In AlphaZero, the policy network (or head of the network) maps game states to a distribution of the likelihood of taking each action. This distribution covers all possible actions from that state.
How is such a network possible? The possible actions from each state are vastly different than subsequent states. So, how would each possible action from a given state be represented in the network's output, and what about the network design would stop the network from considering an illegal action?
","['neural-networks', 'reinforcement-learning', 'ai-design', 'alphazero', 'alphago-zero']","The output of the policy network is as described in the original paper:A move in chess may be described in two parts: selecting the piece to move, and then
  selecting among the legal moves for that piece. We represent the policy π(a|s) by a 8 × 8 × 73
  stack of planes encoding a probability distribution over 4,672 possible moves. Each of the 8×8
  positions identifies the square from which to “pick up” a piece. The first 56 planes encode
  possible ‘queen moves’ for any piece: a number of squares [1..7] in which the piece will be
  moved, along one of eight relative compass directions {N, NE, E, SE, S, SW, W, NW}. The
  next 8 planes encode possible knight moves for that piece. The final 9 planes encode possible
  underpromotions for pawn moves or captures in two possible diagonals, to knight, bishop or
  rook respectively. Other pawn moves or captures from the seventh rank are promoted to a
  queen.So each move selector scores the relative probability of selecting a piece in a given square and moving it in a specific way. For example, there is always one output dedicated to representing picking up the piece in A3 and moving it to A6. This representation includes selecting opponent pieces, selecting empty squares, making knight moves for rooks, making long diagonal moves for pawns. It also includes moves that take pieces off the board or through other blocking pieces. The typical branching factor in chess is around 35. The policy network described above always calculates discrete probabilities for 4672 moves.Clearly this can select many non-valid moves, if pieces are not available, or cannot move as suggested. In fact it does this all the time, even when fully trained, as nothing is ever learned about avoiding the non-valid moves during training - they do not receive positive or negative feedback, as there is never any experience gained relating to them. However, the benefit is that this structure is simple and fixed, both useful traits when building a neural network.The simple work-around is to filter out impossible moves logically, setting their effective probability to zero, and then re-normalise the probabilities for the remaining valid moves. That step involves asking the game engine for what the valid moves are - but that's fine, it's not ""cheating"".Whilst it might be possible to either have the agent learn to avoid non-valid moves, or some clever output structure that could only express valid moves, these would both distract from the core goal of learning how to play the game optimally."
Can I develop a chatbot to carry on a natural conversation with a human using NLP and neural networks?,"
I would like to develop a chatbot that is able to pass the Turing test, i.e. a chatbot that is able to carry on a natural conversation with a human.
Can natural language processing (NLP) be used to do that? What if I combine NLP with neural networks?
","['neural-networks', 'natural-language-processing', 'chat-bots']","I would not recommend using neural networks and NLP together to create a system sufficiently capable of conversation/dialogue that it would pass that current crop of Turing-like tests.Conversations follow certain rules and regularities (which we have only partially discovered so far), and training an ANN with dialogues in order to pick up those regularities is simply not feasible. In conversations you have a memory of what has been mentioned previously, you build up assumptions about the intentions of your dialogue partner, and keep track of the current topic and sub-topics. This is far too complex to be reduced to a machine learning approach.As a starting point I would suggest looking at ELIZA, developed by Weizenbaum in the mid-1960s. There are plenty of implementations in various programming languages available. Use that as a starting point to extend the capabilities according to topics you want to talk about, and store in memory what the user has said before, trying to refer back to it, etc. This is a lot easier to do with 'symbolic' AI rather than subsymbolic processing.A lot of current tech companies offer chatbot variants based on machine learning, but they rarely go beyond intent recognition or simple question-answer dialogues. For more sophisticated dialogues they are simply not suitable.(Disclaimer: I work for a company producing conversational software)"
How can I minimize the number of answers that are relevant to a machine learning model?,"
Problem:
We have a fairly big database that is built up by our own users. 
The way this data is entered is by asking the users 30ish questions that all have around 12 answers (x, a, A, B, C, ..., H). The letters stand for values that we can later interpret. 
I have already tried and implemented some very basic predictors, like random forest, a small NN, a simple decision tree etc.
But all these models use the full dataset to do one final prediction. (fairly well already).
What I want to create is a system that will eliminate 7 to 10 of the possible answers a user can give at any question. This will reduce the amount of data we need to collect, store, or use to re-train future models.
I have already found several methods to decide what are the most discriminative variables in the full dataset.  Except, when a user starts filling the questions I start to get lost on what to do. None of the models I have calculate the next question given some previous information. 
It feels like I should use a Naive Bayes Classifier, but I'm not sure. Other approaches include recalculating the Gini or entropy value at every step. But as far as my knowledge goes, we can't take into account the answers given before the recalculating.
","['machine-learning', 'feature-selection', 'decision-trees']",
Are there dynamic neural networks?,"
Are there neural networks that can decide to add/delete neurons (or change the neuron models/activation functions or change the assigned meaning for neurons), links or even complete layers during execution time? 
I guess that such neural networks overcome the usual separation of learning/inference phases and they continuously live their lives in which learning and self-improving occurs alongside performing inference and actual decision making for which these neural networks were built. Effectively, it could be a neural network that acts as a Gödel machine.
I have found the term dynamic neural network but it is connected to adding some delay functions and nothing more.
Of course, such self-improving networks completely redefine the learning strategy, possibly, single shot gradient methods can not be applicable to them. 
My question is connected to the neural-symbolic integration, e.g. Neural-Symbolic Cognitive Reasoning by Artur S. D'Avila Garcez, 2009. Usually this approach assigns individual neurons to the variables (or groups of neurons to the formula/rule) in the set of formulas in some knowledge base. Of course, if knowledge base expands (e.g. from sensor readings or from inner nonmonotonic inference) then new variables should be added and hence the neural network should be expanded (or contracted) as well.
","['neural-networks', 'incremental-learning', 'online-learning']",
How to design a classifier while the patterns of positive data are changing rapidly?,"
In some situation, like risk detection and spam detection. The pattern of Good User is stable, while the patterns of Attackers are changing rapidly. How can I make a model for that? Or which classifier/method should I use?
","['classification', 'ai-security']","The phenomenon where the prediction targets (in your case, behaviour) change over time is referred to as ""concept drift"".If you search for that term, you'll find that there have been many publications attempting to tackle that over multiple decades, way too many papers to all summarize here in a single answer. It's still a difficult problem though, by no means a ""solved"" problem.Two different, broad directions for ideas are:This github page contains a large list of papers on credit card fraud detection, where the problem you describe occurs because fraudsters change their behaviour in an attempt to evade detection. Most of those papers discuss variants of the first approach. Basically, many of those papers use an ensemble of multiple Random Forests. Every day, new labelled data becomes available. They often then remove the oldest of multiple Random Forests, and add a new Random Forest trained on the most recent data made available that day. There are also some variants where they don't always train new models at a fixed schedule (e.g., every day), but try to detect when the statistical properties of the data have changed using statistical tests, and only train new models when it is ""necessary"" (due to such changes).For the second idea, you'll often be thinking of approaches that use Stochastic Gradient Descent-like approaches for learning; with a non-decreasing learning rate / step size, such techniques will naturally, slowly ""forget"" what they have learned from old data, and focus more on the latter data. If you have some method to obtain accurate labels for certain instances relatively quickly, you could consider an approach like the one proposed in this paper (disclaimer: I'm an author on this paper). For example, in that paper the assumption is that human experts can relatively quickly investigate and obtain accurate labels for a small selection of transactions, and this can be exploited to quickly learn in an online manner."
How can a reinforcement learning agent generalize if it is trained against only one opponent?,"
I started teaching myself about reinforcement learning a week ago and I have this confusion about the learning experience. Let's say we have the game Go. And we have an agent that we want to be able to play the game and win against anyone. But let's say this agent learn from playing against one opponent, my questions then are: 

Wouldn't the agent (after learning) be able to play only with that opponent and win? It estimated the value function of this specific behaviour only.
Would it be able to play as good with weaker players? 
How do you develop an agent that can estimate a value function that generalizes against any behaviour and win? Self-play? If yes, how does that work? 

",['reinforcement-learning'],"Reinforcement Learning (RL) at its core does not have anything directly to say about adversarial environments, such as board games. That means in a purely RL set up, it is not really possible to talk about the ""strength"" of a player.Instead, RL is about solving consistent environments, and that consistency requirement extends to any opponents or adversarial components. Note that consistency is not the same as determinism - RL theory copes well with opponents that effectively make random decisions, provided the distribution of those decisions does not change based on something the RL agent cannot know.Provided an opponent plays consistently, RL can learn to optimise against that opponent. This does not directly relate to the ""strength"" of an opponent, although usually strong opponents present a more challenging environment to learn overall.If the RL has enough practice and time to optimise against the opponent, then yes the value function (and any policy based on it) would be specific to that opponent. Assuming, the opponent did not play flawlessly, then the RL would learn to play such it would win as often as possible against the opponent.When playing against other opponents, the success of the RL agent will depend on how similar the new opponent was to the original that it trained against.As stated above, there is not really a concept of ""stronger"" or ""weaker"" in RL. It depends on the game, and how general the knowledge is that strong players require in order to win. In theory you could construct a game, or deliberately play strongly, but with certain flaws, so that RL would play very much to counter one play style, and would fail against another player that did not have the same flaws. It is difficult to measure this effect, because human players learn from their mistakes too, and are unlikely to repeat the exact same game time after time, but with small variations at key stages. Humans do not make consistent enough opponents, and individual humans do not play enough games at each stage of their ability to study fine-grained statistics of their effective policies.In practice it seems likely that the effect of weakening against new players would be there in RL, due to sampling error if nothing else. However, it seems that the ""strength"" of players as we measure them in any game of skill such as chess or go, does correlate with a generalised ability. In part this is backed up by consistent results with human players and Elo ratings.Any game where you can form ""rings"" of winning players:Could cause issues of the type you are concerned about when applying RL to optimise an artificial agent.If is possible to play perfectly, then a value function which estimated for perfect play would work. No player could beat it. Think of Tic Tac Toe - it is relatively easy to construct perfect play value functions for it.This is not achievable in practice in more complex games. To address this, and improve the quality of its decisions, what AlphaGo does is common to many game-playing systems, using RL or not. It performs a look-ahead search of positions. The value function is used to guide this. The end result of the search is essentially a more accurate value function, but only for the current set of choices - the search focuses lots of computation on a tiny subset of all possible game states. One important detail here is that this focus applies at run time whilst playing against any new opponent. This does not 100% address your concerns about differing opponents (it could still miss a future move by a different enough opponent when searching). But it does help mitigate smaller statistical differences between different opponents. This search tree is such a powerful technique that for many successful game playing algorithms, it is possible to start with an inaccurate value function, or expert heuristics instead, which are fixed and general against all players equally. IBM's Deep Blue is an example of using heuristics.self-play?  if yes, how does that work?Self-play appears to help. Especially in games which have theoretical optimal play, value functions will progress towards assessing this optimal policy, forming better estimates of state value with enough training. This can give a better starting point than expert heuristics when searching."
Can genetic algorithms be used to learn to play multiple games of the same type?,"
Is it possible for a genetic algorithm + Neural Network that is used to learn to play one game such as a platform game able to be applied to another different game of the same genre.
So for example, could an AI that learns to play Mario also learn to play another similar platform game.
Also, if anyone could point me in the direction of material i should familiarise myself with in order to complete my project.
","['neural-networks', 'game-ai', 'python', 'genetic-algorithms']","Genetic algorithms and Neural Networks both are ""general"" methods, in the sense that they are not ""domain-specific"", they do not rely specifically on any domain knowledge of the game of Mario. So yes, if they can be used to successfully learn how to play Mario, it is likely that they can also be applied with similar success to other Platformers (or even completely different games). Of course, some games may be more complex than others. Learning Tic Tac Toe will likely be easier than Mario, and learning Mario will likely be easier than StarCraft. But in principle the techniques should be similarly applicable.If you only want to learn in one environment (e.g., Mario), and then immediately play a different game without separately training again, that's much more complicated. For research in that area you'll want to look for Transfer Learning and/or Multi-Task learning. There has definitely been research there, with the latest developments that I'm aware of having been published yesterday (this is Deep Reinforcement Learning though, no GAs I think).The most ""famous"" recent work on training Neural Networks to play games using Genetic Algorithms that I'm aware of is this work by Uber (blog post links to multiple papers). I'm not 100% sure if that really is the state of the art anymore, if it's the best work, etc... I didn't follow all the work on GAs in sufficient detail to tell for sure. It'll be relevant at least though.I know there's also been quite a lot of work on AI in general for Mario / other platformers (for instance in venues such as the IEEE Conference on Computational Intelligence and Games, and the TCIAIG journal)."
Should I use Monte Carlo or a classifier for this Decision Making problem?,"
I want to build a model to support decision making for loan insurance proposal.
There are three actors in the problem: a bank, a loaner applicant (someone who ask for a loan) and a counselor. The counselor studies the loaner application and if it has a good profile it will propose to him loan from banks that fits his profile. Then the application is sent to the bank but the bank could refuse the applicant (based on criteria we don't know).
The counselor has also to decide whether or not he will propose to the loaner applicant a loan insurance.
The risk is that some banks reject loan applicant who accepts a loan insurance and other banks accept more applicants with a loan insurance. But there aren't rules regarding banks since some banks accept or reject applicants with loan insurance according of the type of acquisition applicants want with their loan for example.
Thus, the profile of the applicant can matter in their rejection from banks but all criteria influencing the decision are quite uncertain.
I've researched online and found several scholarly articles on using Monte Carlo for decision making. Should I use Monte Carlo or a simple classifier for this Decision Making problem ?
I saw that Monte Carlo (possibly Monte Carlo Tree Search) can be used in Decision Making and it is good when there is uncertainty. But it seems that it would forecast by producing some strategy (after running a lot of simulations) but what I want is an outcome based on both the profile of the loaner applicant and the bank knowing that criteria from banks (to accept loaner applicant from could change every six months. And I would have too model banks which seems quite difficult.
A classifier seems to me to not really fit the problem. I am not really sure. Actually, I don't see how a classifier like a decision tree, for example, would work here. Because I have to predict decision of the counselor to propose or not based on the decision of banks (and I don't know their criteria) to refuse or accept applicants who were proposed loan insurance and accepted it.
The data I have is former applicants profile who were sent to banks and if they were accepted or not by the bank, if they wanted a loan insurance or not and the type of acquisition they wanted to make with their loan.
I am new to Decision Making. Thank you!
","['machine-learning', 'classification', 'monte-carlo-tree-search', 'decision-theory']","A classifier seems to me to not really fit the problem. I am not really sure. Actually, I don't see how a classifier like a decision tree, for example, would work here. Because I have to predict decision of the counselor to propose or not based on the decision of banks (and I don't know their criteria) to refuse or accept applicants who were proposed loan insurance and accepted it.The data I have is former applicants profile who were sent to banks and if they were accepted or not by the bank, if they wanted a loan insurance or not and the type of acquisition they wanted to make with their loan.Why does this seem to you like something where a classifier wouldn't fit? Unless I'm missing something, it sounds like a prototypical example of a classification problem to me.You have:Approaches like Monte-Carlo Tree Search can only be used if you have a forward model or simulator. In your setting, you could view the features (applicants' profile) as a ""game state"", and model the problem as a game with two actions (propose or not propose). However, you don't have a forward model (a function that, given a current state and action, generates a possible reward and subsequent state). In applications where MCTS is often used (such as games), you do have such a forward model: for a game like Go or chess, you can easily program the game's rules, program how you transition from one state into another when you select an action, etc. This does not appear to be the case for you."
What is the physics engine used by DeepMimic?,"
I found a video for the paper DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills
 on YouTube.
I looked in the related paper, but could not find details of how to the environment was created, such as the physics engine it used. I would like to use it, or something similar.
",['reinforcement-learning'],Bullet physics engineTheir paper says
How does the degree of neuronal realism affect computing in a deep learning scenario?,"
Neurons can be simulated using different models that vary in the degree of biophysical realism. When designing an artificial neuronal network, I am interested in the consequences of choosing a degree of neuronal realism.
In terms of computational performance, the FLOPS vary from integrate-and-fire to the Hodgkin–Huxley model (Izhikevich, 2004). However, properties, such as refraction, also vary with the choice of neuron.

When selecting a neuronal model, what are consequences for the ANN other than performance? For example, would there be trade-offs in
terms of stability/plasticity?
Izhikevich investigated the performance question in 2004. What are
the current benchmarks (other measures, new models)?
How does selecting a neuron have consequences for scalability in terms of hardware for a deep learning network?
When is the McCulloch-Pitts neuron inappropriate?


References
Izhikevich, E. M. (2004). Which model to use for cortical spiking neurons? IEEE Transactions on Neural Networks, 15(5). https://www.izhikevich.org/publications/whichmod.pdf 
","['deep-learning', 'artificial-neuron', 'neurons', 'biology']",
Doubt regarding research paper on Crowd Counting using Convolutional neural networks and Markov Random Field,"
I am currently reading the research paper Image Crowd Counting Using Convolutional Neural Network and Markov Random Field  by Kang Han, Wanggen Wan, Haiyan Yao, and Li Hou. 
I did not understand the following context properly: 

Formally, the
  Markov random field framework for the crowd counting
  can be defined as follows (we follow the notation in [18]).
  Let P be the set of patches in an image and C be a possi-
  ble set of counts. A counting c assigns a count c p ∈ C to
  each patch p ∈ P. The quality of a counting is given by an
  energy function:
E(c) =
  ∑ D p (c p ) + ∑
  p∈P
  V (c p − c q )
  . . . (2)
  (p,q)∈N
where N are the (undirected) edges in the four-connected
  image patch graph. D p (c p ) is the cost of assigning count
  c p to patch p, and is referred to as the data cost. V (c p −c q )
  measures the cost of assigning count c p and c q to two
  neighboring patch, and is normally referred to as the dis-
  continuity cost.
  For the problem of smoothing the adjacent patches
  count, D p (c p ) and V (c p − c q ) can take the form of the
  following functions:
  D p (c p ) = λ min((I(p) − c p ) 2 , DATA K) . . . (3)
  V (c p − c q ) = min((c p − c q ) 2 , DISC K)
  . . . (4)
  where λ is a weight of the energy items, I(p) is the ground
  truth count of the patch p, DATA K and DISC K are the
  truncating item of D p (c p ) and V (c p − c q ), respectively.


Can anyone explain the above part in detail and give me a detailed insight on how should I implement this part of the project? 
",['convolutional-neural-networks'],
"Can I reduce the ""number of weights"" in CNN to 1/3 by restricting the input as greyscale image?","
In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?
This question helps me a lot.
Let, I have RGB input image. (3 channels)
Then each filter has n×n weights for one channel.
It means, actually the filter has totally 3×n×n weights.
For channel R, it has own n×n filter.
For channel G, it has own n×n filter.
For channel B, it has own n×n filter.
After inner product, add them all to make one feature map.
Am I right?
And then, my question starts here.
For some purpose, I will only use greyscale images as input.
So the input images always have the same values for each RGB channel.
Then, can I reduce the number of weights in the filters?
Because in this case, using three different n×n filters and adding them is same with using one n×n filter that is the summation of three filters.
Does this logic hold on a trained network?
I have a trained network for RGB image input, but it is too heavy to run in real time.
But I only use the greyscale images as input, so it seems I can make the network less heavy (theoretically, almost 1/3 of original).
I'm quite new in this field, so detailed explanations will be really appreciated.
Thank you.
","['deep-learning', 'convolutional-neural-networks', 'signal-processing']",
How can AI be used to more reliably analyze and plan around the tie between climate and emissions?,"
Note to the Duplicate Police
This question is not a duplicate of the Q&A thread referenced in the close request.  The only text even remotely related in that other thread is the brief mention of climate change in the Q and two sentences in the sole answer: ""Identify deforestation and the rate at which it's happening using computer vision and help in fighting back based on how critical the rate is. The World Resources Institute had entered into a partnership with Orbital Insight on this.""
If you look at the four bullet items below, you will find that this question asks a very specific thing about the relationship between climate and emissions.  Neither that question nor that answer overlaps with the content of this question in any meaningful way.  For instance, it is well known that CO2 is NOT causing deforestation.  The additional carbon dioxide in the atmosphere causes faster regrowth.  This is because plants need CO2 to grow.  Hydroponic containers deliberately boost it to improve growth rates.  Plants manufacture their own oxygen from the CO2 via chlorophyll.
If you recall from fifth grade biology, that's why they are plants.

Now Back to the Question
Several climate models have been proposed and used to model the relationship between human carbon emissions, added to the natural carbon emissions of life forms on earth, and features of climate that could damage the biosphere.
Population growth and industrialization have many impacts on the biosphere, including loss of terrain and pollution. Negative oceanic effects, including unpredictable changes in plankton and cyanobacteria are under study.  Carbon emissions from combustion has received attention in recent decades just as sulfur emissions were central to concerns a century or more ago.
Predicting weather and climate is certainly difficult because it is complex and chaotic, as typical inaccuracies in forecasts clearly demonstrate, but that is looking forward.  Looking backward, analyses of data already collected have shown a high probability that ocean and surface temperature rises followed increases in industrial and transportation related combustion of fuels.
How might AI be used to produce some of the key models humans need to protect the biosphere from severe damage.

A more reliable analysis of what has already occurred, since there is some legitimacy to the differing views as to how gross the effect of carbon emissions has been on extinctions of species in the biosphere and on arctic and antarctic melting
A better understanding as to whether the climate of the biosphere behaves as a buffer of climate, always tending to re-balance after a volcanic eruption, meteor stroke, or other event, or whether the runaway scenario described by some climatologist, where there is a point of no return, is realistic
A better model to use in trying out scenarios so that solutions can be applied in the order that makes sense from both environmental and economic perspectives
Automation of climate planning so that the harmful effects of the irresponsibility of one geopolitical entity wishing to industrialize without constraint on other geopolitical entities can be mitigated

Can pattern recognition, feature extraction, the learned functionality of deep networks, or generative techniques be used to accomplish these things?  Can rules of climate be learned?  Are there discrete or graph based tools that should be used?
","['pattern-recognition', 'topology', 'generative-model', 'survival']","Yes. The work of Judea Pearl and others over the last 20 years began out of a desire to address uncertainty within AI. Eventually, this led Pearl to become fascinated by the need to quantifiably determine when one event has caused another, the problem at the root of ""correlation is not causation"". He substantively succeeded with the combination of causal modeling and the do-calculus. The do-calculus allows you to formulate queries of the form ""To what degree did X cause Y?"", and to automatically determine what measurements are needed to determine the answer in a statistically meaningful way. The algorithms used under the hood are closely related to those used in AI and robotics systems to reason about uncertainty (i.e. Bayesian Inference). Causal modeling is still relatively new, and is not yet as widely used as it could be. An open problem is how to specify what the model of the world looks like from data (rather than just reasoning over a model that is given). If this problem is solved, we could see major improvements in the ability to provide analyses like those you ask about.Probably not. Predicting future events without any observable precedents is not something that anyone can be sure about. The questions raised in those kinds of scenarios aren't about calculation, but about modeling. For example, if you believe that there is a large amount of methane trapped in the arctic permafrost, and you believe that temperatures above a certain range release this much faster than in the past, and you believe that methane warms the climate rapidly, then you would tend to believe that the runaway scenarios are plausible. AI can't tell us whether the methane is there (we have to go measure it, though maybe AI can make the measurement more accurate), and can't tell us how temperature will affect the release rate (again, we have models of this, but they rely on different assumptions, and we have to go measure to find out which are right).Can AI provide better simulations of the impacts of interventions on both climate and the economy, to inform decision making?Maybe. Agent-based modeling can help with this to some degree, and is arguably part of AI. In fact, it already has been to some degree, by Beeger & Troost in J. Agriculture Economics, in 2014 and again in 2017, though interest in this kind of modeling looks like a pretty new development in this area. Although ABM can give us reasonable models and help simulate the impact of interventions, ultimately they are just one modeling tool among many. Their potency may improve if more realistic agent models are used, but it is not clear that AI is going to provide advances in this area in the near future.Probably not. Although AI techniques have made some kinds of economic planning problems a lot easier, the main barrier to the effects you describe is a social/political one: countries are sovereign, and the world operates as a de facto anarchy (i.e. the UN is impotent). Your AI model can tell, say, India not to industrialize, but Indians want to enjoy the same kind of lifestyle improvements that Americans do, and would rather enjoy them sooner than later. India would collectively rather that Americans put an enormous tax on their carbon emissions, drive less, eat much, much less beef, and stop flying everywhere, than that Indians continue living on the equivalent of $7,000 each per year. In contrast, Americans would rather that Indians just wait a few decades while the developed world decarbonizes, and only industrialize once we have enough solar panels to replace all our current needs within current industrial economies.Basically this is a resource allocation problem within an anarchy: we can only burn X carbon within Y time, everyone wants to burn some, and the only way to enforce contracts is with the threat of massive violence (or massive economic sanctions, which in turn, are imposed through the threat of violence against other actors if they trade with the sanctioned country). AI can help us answer questions like ""How much should the USA pay India in exchange for India not industrializing this year?"", see, e.g. work in Auction Theory, but AI can't actually make nations do those things if they can't reach a diplomatic compromise."
What part of the game is the value network trained to predict a winner on?,"
The Alpha Zero (as well as AlphaGo Zero) papers say they trained the value head of the network by ""minimizing the error between the predicted winner and the game winner"" throughout its many self-play games. As far as I could tell, further information was not given.
To my understanding, this is basically a supervised learning problem, where, from the self-play, we have games associated with their winners, and the network is being trained to map game states to the likelihood of winning. My understanding leads me to the following question:
What part of the game is the network trained to predict a winner on? 
Obviously, after only five moves, the winner is not yet clear, and trying to predict a winner after five moves based on the game's eventual winner would learn a meaningless function. As a game progresses, it goes from tied in the initial position to won at the end. 
How is the network trained to understand that if all it is told is who eventually won?
","['machine-learning', 'reinforcement-learning', 'alphago', 'alphazero', 'alphago-zero']","To my understanding, this is basically a supervised learning problem, where from the self play we have games associated with their winners, and the network is being trained to map game states to likelihood of winning.Yes, although the data for this supervised learning problem was provided by self-play. As AlphaZero learned, the board evaluations of the same positions would need to change, so this is a non-stationary problem, requiring that the ML forgot the training for older examples over time.What part of the game is the network trained to predict a winner on?Potentially all of it, including the starting empty board. I am not sure if the empty board was evaluated in this way, but it is not only feasible, but can be done accurately in practice for simpler games (Tic Tac Toe and Connect 4 for example), given known player policies.Obviously after only five moves, the winner is not yet clear, and trying to predict a winner after five moves based on the game's eventual winner would learn a meaningless function.Not at all. This is purely a matter of complexity and difficultly. In practice at such an early stage, the value network will output something non-committal, such as $p=0.51$ win chance for player 1. And it will have learned to do this, because in its experience during self-play similar positions at the start of the game lead to almost equal numbers of player 1 and player 2 winning.The function is not meaningless either, it can be used to assess results of look-ahead searches without needing to play to the end of the game. It completely replaces position-evaluation heuristics as used in more traditional game tree searches. In practice, very early position data in something as complex as chess or go is not going to be as useful as later position evaluations, due to ambivalent predictions. However, for consistency it can still be learned and used in the game algorithms.How is the network trained to understand that, if all it is told is who eventually won?If a supervised learning technique is given the same input data $X$ that on different examples predicts the labels $A, B, B, B, A, A, B, B$, then it should learn $p(B|X) = 0.625$. That would minimise a cross-entropy loss function, and is what is going on here."
Snake game: snake converges to going in the same direction every time,"
This is a q-learning snake using a neural network as a q function aproximator and I'm losing my mind here the current model it's worst than the initial one.
The current model uses a 32x32x32 MLPRegressor from scikit-learn using relu as activation function and the adam solver.
The reward function is like following:

death reward = -100.0
alive reward = -10.0
apple reward = 100.0

The features extracted from each state are the following:

what is in front of the snake's head(apple, empty, snake)
what is in the left of the snake's head
what is in the right of the snake's head
euclidian distance between head and apple
the direction from head to the apple measured in radians
length of the snake

One episode consists of the snake playing until it dies, I'm also using in training a probability epsilon that represent the probability that the snake will take a random action if this isn't satisfied the snake will take the action for which the neural network gives the biggest score, this epsilon probability gradually decrements after each iteration.
The episode is learned by the regressor in reverse order one statet-action at a time.
However the neural network fails too aproximate the q function, no matter how many iterations the snake takes the same action for any state.
Things I tried:

changing the structure of the neural network
changing the reward function
changing the features extracted, I even tried passing the whole map to the network

Code (python): https://pastebin.com/57qLbjQZ
",['reinforcement-learning'],"There are two problems here.The code you posted doesn't incrimentally train your multilayer perceptron. Instead, it effectively re-randomizes the weights, and then re-fits the model each time you call .fit() at lines 35 & 54. Using SKLearn's _fit() function with Incremental=true might solve this, or you can package up the data into a larger batch, and train on that offline instead after several episodes.Your reward function makes it painful to be alive, and doesn't give enough benefits through the Apples to make up for this. There are 100 squares that could contain the apple. On average, the apple will spawn about 5 squares from the snake in each direction. Since the snake can't move diagonally, that's 10 moves (5 left/right, 5 up/down). That means that if the snake plays perfectly, then on average, it might be able to get zero reward total. In practice, the snake will not play perfectly. This means living gives negative expected reward.In contrast, if the snake can kill itself, it will stop getting negative rewards. The reward function you've used is maximized by getting big enough to run into your own tail as fast as possible. The snake should be able to do this after eating 3 apples I think. There is some incentive to hunt for food well, but not much compared with hitting your own tail as soon as possible.If you want the snake to learn to hunt for the food, reduce the penalty for being alive to -1, or even -0.1. The snake will be much more responsive to signals from the food."
Should Q values be changing within an epoch/episode or should they change after one episode/epoch?,"
I am trying to use Deep-Q learning environment to learn Super Mario Bros. The implementation is on Github.
I have a neural network that Q values update within an episode for a very small learning rate (0.00005). However, even if I increase the learning rate to 0.00025, the Q values do not change within an episode as they are predicting the same Q values regardless of what state it is in. For example, if Mario moves right, the Q value is the same. When I start a new episode, the Q values change though.
I think that the Q values should be changing within an episode as the game should be seeing different parts and taking different actions. Why don't I observe this?
","['neural-networks', 'machine-learning', 'deep-learning', 'reinforcement-learning', 'game-ai']",
Does AI rely on determinism?,"
I don’t believe in free will, but most people do. Although I’m not sure how an act of free will could even be described (let alone replicated), is libertarian freewill something that is considered for AI? Or is AI understood to be deterministic?
","['philosophy', 'human-like', 'artificial-consciousness']","I'm going to assume that by free will, you mean something like the philosophical concept of libertarian free will, which is defended by philosophers like Robert Kane. In Libertarian Free Will, individuals have some capability to make choices about their actions. The classic way to argue this is by assuming some kind of spirit-stuff (e.g. a soul) that exists outside the material world, and that this spirit-stuff constitutes the consciousness of a person. Kane tries some mental gymnastics to avoid this, but then concedes something like it in a footnote. I'm not aware of any serious work that doesn't make some kind of non-physical assumption to justify this view. If someone can point at one, I'll update the answer.By determinism, I'm going to assume you mean the usual notion of philosophical determinism: since people's decisions depend on what happened in the past, and where they are in the present, they don't really have a choice in any meaningful sense. Philosopher's like Dennett adopt a slightly softer view (compatibilism, essentially: you don't get to make big choices, but you do get to make small ones). Appeals to Quantum Mechanics are common to justify that view. In this context, free action means something more like ""did something we couldn't predict exactly"". An example might be: you are pre-destined to put a can of campbell's brand tomato soup in your shopping cart, but ""make a choice"" about exactly which of the dozens of cans you will put in. Since small choices can have large impacts (maybe that can will give you food poisoning, and the others wouldn't), this can make all sorts of things impossible to predict exactly. I think most AI researchers don't worry too much about these issues, but Turing actually addresses them in his paper right at the start of the field, Computing Machinary and Intelligence. The deterministic/compatibilist view point is introduced as Lady Lovelace's objection: Computers only know how to do what we tell them to, so they can't be called intelligence.Turing's counterargument uses two prongs. First, Turing notes that computers can probably be made to learn (he was right!). If they can learn, they can do things that we don't expect, just like children do. Second, Turing notes that computers already do all sorts of things we don't expect: they have bugs. Anytime a computer exhibits a bug, it did something that was unexpected. Since we cannot generally rule out bugs in programs, computers will always do surprising things. Therefore, computers satisfy the deterministic notion of free will.Turing also addresses the libertarian notion of free will, which is part of what he calls the ""Theological Objection"". The objection is that intelligence requires some kind of divine spark (like free will). Turing argues that we can't detect sparks like this right now (he actually thought we would be able to one day, and spent a lot of time looking at supernatural phenomena too). However, there's no reason to suppose that computers with the right programs won't be endowed with them. A divine creator could decide that anytime you build something brain-like, it gets a spark. If we build a program that's brain-like, maybe it gets a spark too. In the absence of some way to detect souls, it seems like we ought to just agree to treat things that seem intelligent as though they had these souls, since otherwise we don't have a really clear way to decide who is and isn't intelligent. The only remaining way is to say ""only things made of human meat have souls and are intelligent"". While a lot of people do actually say things like this (e.g. animals have no souls), this is a pretty arbitrary view, and I think there's no hope arguing against it. Turing suggests the ""Anthropic Principal"": we shouldn't assume that we're special, because the earth isn't in a special place in the galaxy, or in the universe, and we have pretty compelling evidence that we're an evolved version of other animals around us, but some groups (e.g. biblical literalists) find this unconvincing. "
Historical weakness of GOFAI in relation to partisan combinatorial games?,"
I was recently perusing the paper Some Studies in Machine Learning Using the Game of Checkers II--Recent Progress (A.L. Samuel, 1967), which is interesting historically.
I was looking at this figure, which involved Alpha-Beta pruning.
 
It occurred to me that the types of non-trivial, non-chance, perfect information, zero-sum, sequential, partisan games utilized (Chess, Checkers, Go) involve game states that cannot be precisely quantified. For instance, there is no way to ascribe an objective value to a piece in Chess, or any given board state.  In some sense, the assignment of values is arbitrary, consisting of estimates.
The combinatorial games I'm working on are forms of partisan Sudoku, which are bidding/scoring (economic) games involving territory control.  In these models, any given board state produces an array of ratios allowing precise quantification of player status. Token values and positions can be precisely quantified.  
This project involves a consumer product, and the approach we're taking currently is to utilize a series of agents of increasing sophistication to provide different levels challenge for human players.  These agents also reflect what is known as a ""strategy ladder"".   
Reflex Agents (beginner) 
Model-based Reflex Agents (intermediate) 
Model-based Utility Agents (advanced)
Goals may also be incorporated to these agents such as desired margin of victory (regional outcome ratios) which will likely have an effect on performance in that narrower margins of victory appear to entail less risk.
The ""respectably weak"" vs. human performance of the first generation of reflex agents suggests that strong GOFAI might be possible. (The branching factors are extreme in the early and mid-game due to the factorial nature of the models, but initial calculations suggest that even a naive minimax lookahead will be able to look farther more effectively than humans.) Alpha-Beta pruning in partisan Sudoku, even sans a learning algorithm, should provide greater utility than in previous combinatorial game models where the values are estimates. 

Is the historical weakness of GOFAI in relation to non-trivial combinatorial games partly a function of the structure of the games studied, where game states and token values cannot be precisely quantified?

Looking for any papers that might comment on this subject, research into combinatorial games where precise quantification is possible, and thoughts in general. 
I'm trying to determine if it might be worth attempting to develop a strong GOFAI for these models prior to moving up the ladder to learning algorithms, and, if such a result would have research value.  
There would definitely be commercial value in that strong GOFAI with no long-term memory would allow minimal local file size for the apps, which must run on lowest-common-denominator smartphones with no assumption of connectivity.  
PS- My previous work on this has involved defining the core heuristics that emerge from the structure of the models, and I'm slowly dipping my toes into the look ahead pool.  Please don't hesitate to let me know if I've made any incorrect assumptions.
","['game-ai', 'alpha-beta-pruning', 'symbolic-ai', 'combinatorial-games']",
Should I be decaying the learning rate and the exploration rate in the same manner?,"
Should I be decaying the learning rate and the exploration rate in the same manner? What's too slow and too fast of an exploration and learning rate decay? Or is it specific from model to model?
","['reinforcement-learning', 'deep-rl', 'hyper-parameters', 'learning-rate', 'exploration-strategies']","First of all, I'd say that there is a reason to give Learning Rate (LR) and Exploration Rate (ER) the same decay: they play at the same scale (the number of successive batches you'll train your model on). But if I refine the analysis, I would rather say that it's a reason to choose them in the same range, i.e. close to 1, but not specifically at the same number.For LR decay, people often choose it very close to one (which can mean really different things like 0.98 or 0.997), because it plays on a large scale, and you don't want the LR to disappear too brutally.However, the choice of ER decay can have more variation from model to model. It depends on the initial value of ER (you don't wanna decay fastly ER if your ER is initially low), and also to the ""learning speed"" of your model: if your model learns efficiently at the beginning, you could want to fastly decrease ER in order to reduce the noise on the action, supposing that you did enough exploration at the beginning (but I think this last opinion is more controversial). You can find an interesting paper here, where the author tries different ER decay and finds out that 0.99 is the best, for CartPole environment."
"Clarification regarding ""Image Crowd Counting Using Convolutional Neural Network and Markov Random Field""","
I am currently reading the research paper Image Crowd Counting Using Convolutional Neural Network and Markov Random Field  by Kang Han, Wanggen Wan, Haiyan Yao, and Li Hou. 
I did not understand the following context properly: 

We employ the residual network, which is trained on ImageNet dataset for image classication task, to extract the deep features to represent the density of the crowd. This pre-trained CNN network created a residual item for every three convolution layer to bring the layer of the network to 152. We resize the image patches to the size of 224 × 224 as the input of the model and extract the output of the fc1000 layer to get the 1000 dimensional features. The features are then used to train 5 layers fully connected neural network. The network's input is 1000dimensional, and the number of neurons in the network is given by 100-100-50-50-1. The network's output is the local crowd count

Can anyone explain the above part in detail? 
","['deep-learning', 'convolutional-neural-networks']","I will try to do it part by part :We employ the residual network, which is trained on ImageNet dataset for image classication task, to extract the deep features to represent the density of the crowd.If you look at the figure 2, you can see that they use the Neural network architecture ResNet. This is a deep network, here is the paper. It has good performance and do image classification.This pre-trained CNN network created a residual item for every three convolution layer to bring the layer of the network to 152If you are in the layer k, it means this layer has in input, the ouput of the k-3th layer. See the paper, figure 5 explains it well without much explications needed. Furthermore, Resnet has 3 different architectures wih different number of layers, and they take the deeper one, the 152 layers deep Resnet.We resize the image patches to the size of 224 × 224 as the input of the model and extract the output of the fc1000 layer to get the 1000 dimensional featuresThe input of Resnet is images of size 224x224, so they need to resize them to fit the input requirement of Resnet. The output of Resnet is 1000 because Imagenet is a dataset of 1000 classes.The features are then used to train 5 layers fully connected neural network. The network's input is 1000dimensional, and the number of neurons in the network is given by 100-100-50-50-1.Then they give the output of Resnet to their own Network, which is 5 layer deep. See figure 2 of their paper. Obviously, the input layer has 1000 inputs because of the output of Resnet. The network have layers of 100, 100, 50 and finally 1 neuron. See figure 2.The network's output is the local crowd countI don't think I need to explain it, they want only the number of people in the crowd, so they need only one output. This is obviously not a classifcation problem, but a regression problem.Has you don't really point out what you don't understand, I don't explain it in details. Feel free to ask more precise question if some part are still blurred to you !"
Would AlphaGo Zero become perfect with enough training time?,"
Would AlphaGo Zero become theoretically perfect with enough training time? If not, what would be the limiting factor?
(By perfect, I mean it always wins the game if possible, even against another perfect opponent.)
","['neural-networks', 'monte-carlo-tree-search', 'alphago', 'alphazero', 'alphago-zero']","We cannot tell with certainty whether AlphaGo Zero would become perfect with enough training time. This is because none of the parts (Neural Network) that would benefit from infinite training time (= a nice approximation of ""enough"" training time) are guaranteed to ever converge to a perfect solution. The main limiting factor is that we do not know whether the Neural Network used is big enough. Sure, it's pretty big, it has a lot of capacity... but is that enough? Imagine if they had used a tiny Neural Network (for example just a single hidden layer with a very low number of nodes, like 2 hidden nodes). Such a network certainly wouldn't have enough capacity to ever learn a truly, perfectly optimal policy. With a bigger network it becomes more plausible that it may have sufficient capacity, but we still cannot tell for sure.Note that AlphaGo Zero does not just involve a trained part; it also has a Monte-Carlo Tree Search component. After running through the Neural Network to generate an initial policy (which in practice turns out to already often be extremely good, but we cannot tell for certain if it's perfect), it does run some MCTS simulations during it's ""thinking time"" to refine that policy.MCTS doesn't benefit* from increased training time, but it does benefit from increased thinking time (i.e. processing time per turn during the actual game being played, rather than offline training time / self-play time before the evaluation game). In the most common implementation of MCTS (UCT, using the UCB1 equation in the Selection Phase), we can prove that it does in fact learn to play truly perfectly if it is given an infinite amount of thinking time. Now, AlphaGo Zero does use a slightly different implementation of the Selection Phase (which involves the policy generated by the trained Neural Network as prior probabilities), so without a formal analysis I can't tell for sure whether that proof still holds up here. Intuitively it looks like it still should hold up fine though.*Note: I wrote above that ""MCTS doesn't benefit from increased training time. In practice, of course it does tend to benefit from increased training time because that tends to result in a better Network, which tends to result in better decisions during the Selection Phase and better evaluations of later game states in the tree. What I mean is that MCTS is not theoretically guaranteed to always keep benefitting from increases in training time as we tend to infinity, precisely because that's also where we don't have theoretical guarantees that the Neural Network itself will forever keep improving."
How would I go about creating a neural network that outputs a non-binary number?,"
I would like to create a neural network, which, given the training data (e.g. 58, 2) outputs a non-binary number (e.g 100). Perhaps I am not searching for the correct thing, but all the examples I have found have shown classifiers using a sigmoid function (range of 1 to 0). I am looking for something that would output nonbinary numbers.
","['neural-networks', 'activation-functions', 'regression', 'network-design']","First of all, sigmoid does not output 0 or 1, it outputs any real number in the range between 0 and 1.Furthermore, neural networks don't usually output binary values, unless the output layer uses the step function as an activation function (which is rare).I'm not really sure if you want the neural network to be a classifier or regressor, but it sounds like you want a regressor.Regression is when you are interested in the value of the output neuron(s) itself. A simple example is if you want the network to predict the sum of two input neurons.If you want to change the network from a classifier to a regressor you should probably reduce the number of neurons in the output layer to 1, and change the activation function of that neuron from softmax to the identity function ($f(x)=x$; which is the same as no activation function at all)."
What AI technique should I use to assign a person to a task?,"
I'm trying to learn AI and thinking to apply it to our system. We have an application for the translation industry. What we are doing now is the coordinator $C$ assigns a file to a translator $T$. The coordinator usually considers these criteria (but not limited to):

the deadline of the file and availability of a translator
the language pair that the translator can translate
is the translator already reached his target? (maybe we can give the file to other translators to reach their target)
the difficulty level of the file for the translator (basic translation, medical field, IT field)
accuracy of translator
speed of translator

Given the following, is it possible to make a recommendation to the coordinator, to whom she can assign a particular file?
What are the methods/topics that I need to research?
(I'm considering javascript as the primary tool, and maybe python if javascript will be more of a hindrance in implementation.)
In addition to suggesting a translator, we are also looking into suggesting the ""deadline of the translator"". Basically, we have ""deadline of the customer"" and ""deadline of the translator""
The reason for this is that, if the translators are occupied throughout the day, it makes sense to suggest it to a busy translator but allow him to finish it until next day.
","['optimization', 'constraint-satisfaction-problems', 'planning', 'linear-programming']","What you have could be well described as a Task Allocation problem, which is studied as part of the planning subfield of AI. Chapters 10 & 11 of Russell & Norvig provide a good overview of this area, although I think they don't talk too much about Task Allocation in particular.There are two basic approaches to this problem: centralized approaches, and decentralized approaches.In centralized approaches, the properties of each task (or sub-task) and the skills of each processing entity are recorded in a central database. The task is phrased as an optimization problem. For example, given the skills of the processors and the tasks' types, find the schedule that minimizes average processing time (or cost, or usage of rare-resource types, or whatever you're interested in). Common approaches include phrasing the optimization task as a linear-programming problem; phrasing the problem as a graph and using something like the graphplan algorithm; or phrasing the problem as a constraint satisfaction problem and using some kind of heuristic-guided local search.There are all kinds of other more modern techniques too. I'm not aware of a survey paper for translation tasks in particular, but there are lots of examples in robotics and distributed computing.Although good AI techniques exist for scheduling the task, they are predicated on being able to quantify the tasks' properties and the abilities of the agents, and on the translators accepting the decisions of the system. If you want an interactive system, you may need to look at techniques from Natural Language Processing. The work on Mixed Initiative Scheduling Systems might also be relevant if you have to go that route."
How to connect AI neural network processor to laptop?,"
I have an average laptop. 
How can I connect specialized AI neural network processors (say, Intel Nvidia or Intel Nervana https://venturebeat.com/2018/05/23/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/) to thelaptop.
Should I buy some external motherboard or even server unit with NN processors inside or is there available more lightweight solution like external HDD?
",['neural-networks'],
How to build a commercial image captioning system?,"
Image Captioning is a hot research topic in the AI community. There are considerable image captioning models for research usage such as NIC, Neural Talk 2 etc. But can these research models be used for commercial purpose? Or we should build much more complex structured ones for commercial usage? Or if we can make some improvements based these models to meet the business applications situation? If so, what improvements should we take? Are there any existing commercial Image Captioning applications can be referenced?
","['deep-learning', 'image-recognition', 'natural-language-processing']","Thinking CommerciallyA commercial solution will need to be able to ascertain, continuously verify, and utilize the best of options for captioning learning models.Each fairly successful image captioning learning models could be placed in an adapter to provide a common training, optimizing, testing, evaluation, and usage interface.  The hot swappable container-addon mega pattern used in peripheral device installation, RAID, J2EE containers, browsers, and other containment sub-systems can be applied.System DescriptionThe system acceptance criteria being as follows.States can be idle or any of these.For example, for the two learning models usable in captioning systems suggested in this question, NIC and Neural Talk 2 we could have a system resource allocation like this:Samples may be pulled from a pool of samples that have been vetted.  That pool may be augmented by real images passing through the system, filtered in accordance with security criteria to avoid external attempts at control.In the assignment of resources, the sample pool selection criteria must be specified.  If the system is already at 100%, the model-state combinations from which the resources shall be drawn must also be specified.Handling Multiple Output OptionsSince there may be more than one model in use and each model may have zero, one, or multiple caption suggestions for each image, each with a reliability measure, the outputs must be analyzed to provide the best choice to associate with the image being analyzed.  Additional system criteria must cover this process scenario.  For any given image, final evaluation must follow the following general guidelines.Another artificial network may be placed at the output and appropriate encoding and normalization may be applied before training so that a properly trained network, converged using a quantification of the above additional criteria, can select the best caption from the options for each image.Phased Development ApproachPhase one of such a system would likely require manual handling of model-state allocation.  Phase two would be semi-automation.  The location of new models would still require expert attention.  Perhaps further in the future a hunt for new models could be automated too."
Is there research that employs realistic models of neurons?,"
Is there research that employs realistic models of neurons? Usually, the model of a neuron for a neural network is quite simple as opposed to the realistic neuron, which involves hundreds of proteins and millions of molecules (or even greater numbers). Is there research that draws implications from this reality and tries to design realistic models of neurons?
Particularly, recently, Rosehip neuron was discovered. Such neuron can be found only in human brain cells (and in no other species). Are there some implications for neural network design and operation that can be drawn by realistically modelling this Rosehip neuron?
","['neural-networks', 'artificial-neuron', 'neurons', 'brain', 'neuromorphic-engineering']","State of Rosehip ResearchThe Rosehip neuron is an important discovery, with vast implications to AI and its relationship to the dominant intelligence on earth for at least the last 50,000 years.  The paper that has spawned other articles is Transcriptomic and morphophysiological  evidence for a specialized human cortical GABAergic cell type, Buldog et. al., September 2018, Nature Neuroscience.The relationship between this neuron type and its DNA expression is beginning.  No data is available regarding the impact of the Rosehop distinctions on neural activity during learning or leveraging what has been learned.  Surely, research along those lines is indicated, but the discovery was just published.Benefit of the Interdisciplinary Approach to AIThat those who reference papers like this can see value in the unification or at least alignment of knowledge across disciplines is most likely beneficial to AI progress and progress in the other fields of cognitive science, bioinformatics, business automation, manufacturing and consumer robotics, psychology, and even law, ethics and philosophy.That such interest in aligning understanding along interdisciplinary lines is present in AI Stack Exchange is certainly beneficial to the community growth in both professional and social dimensions.Disparity Between What WorksIn the human brain, neurons work.  Whether Rosehip neurons are a prerequisite to language, the building of and leveraging of complex models, or transcendent emotions such as love in homo sapiens is unknown and will remain so in the near future.  However, we have a fifty millennia long proof of concept.We also know that artificial networks work.  We use them in business, finance, industry, consumer products, and a variety of web services today.  When a pop-up asks whether the answer given was helpful, our answer becomes a label in a set of real data from which samples are extracted for machine learning.Nonetheless, the cells that are working are offspring of the 1957 perceptron with the addition of the application of gradient descent using an efficient corrective signal distribution strategy we call back propagation.  The comprehension of neuron function in 1957 was grossly short of what we now know to be functional features of mammalian brain neurons.  The Rosehip discovery may widen that gap.Spiking NetworksThe spiking network research more realistically models neurons, and neuromorphic research and development has been placing improved models into VLSI chips.  The joint venture between IBM and MIT is another.Correlating Neural Function to Brain FunctionThe relationship intelligence and the number of proteins or molecules may not be the most telling.  These are more likely relationships between metrics and features and the intelligence of the system.None of these are yet modelled in such a way that simulation accuracy has been confirmed, but the need to research along these lines is clearly indicated as this question implies."
Why is baseline conditional on state at some timestep unbiased?,"
In the homework for the Berkeley RL class, problem 1, it asks you to show that the policy gradient is still unbiased if the baseline subtracted is a function of the state at time step $t$.
$$ \triangledown _\theta \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p(s_t,a_t)} [b(s_t)] = 0   $$
I am struggling through what the first step of such a proof might be. 
Can someone point me in the right direction? My initial thought was to somehow use the law of total expectation to make the expectation of $b(s_t)$ conditional on $T$, but I am not sure.
","['reinforcement-learning', 'policy-gradients', 'proofs']",
Is Experience Replay like dreaming?,"
Drawing parallels between Machine Learning techniques and a human brain is a dangerous operation. When it is done successfully, it can be a powerful tool for vulgarisation, but when it is done with no precaution, it can lead to major misunderstandings.
I was recently attending a conference where the speaker described Experience Replay in RL as a way of making the net ""dream"".
I'm wondering how true this assertion is. The speaker argued that a dream is a random addition of memories, just as experience replay. However, I doubt the brain remembers its dream or either learns from it. What is your analysis?
","['reinforcement-learning', 'dqn', 'deep-rl', 'experience-replay']","The speaker argued that a dream is a random addition of memories, just as experience replay.The speaker is taking some liberties due to a general lack of scientific understanding of what dreams are. We don't even have strong consensus on why sleep is a necessary feature of animals, let alone what part dreaming plays in it. However, there are some widely-accepted theories, with supporting evidence, that dreams are part of a learning and memorisation process. Studies that manipulate sleep or dreaming have shown changes in the speed that skills are learned for example.Experience replay in reinforcement learning is a far more precise and well-understood affair, whereby individual time steps that occurred in the past are visited and re-assessed in light of current knowledge about long-term value, at random. If dreams were really like experience replay as it is practiced in RL today, then they would consist of a random jumble of tiny seemingly inconsequential events strung together, and all taken very exactly from the events of the past day. Sometimes dreams do contain content like this, but typically the content is far more varied.Taken with a large dose of artistic license, then yes, the speaker is referring to real theories and conjectures about dreaming, that do have scientific support. Although it is equally good to draw parallels between dreams and a higher-level management of the memory or experience replay data - which items to replay, and which to keep, depending on what is salient about the information. For instance, there is good evidence that dreams help filter what is forgotten, and also evidence that events associated with strong emotional state are more likely to feature in dreams.It is important to separate the speaker's analogy, and any suggestion that a current reinforcement learning agent has a subjective experience. We are still a long way away from anything like that, and other similar use of a dreaming metaphor in machine learning - e.g. ""Deep Dream"" - is equally not an assertion that the devices are having an experience of any kind."
How to use a Generative Adversarial Network to generate images for developmental analysis?,"
I want to generate images of childrens' drawings consistent with the developmental state of children of a given age.  The training data set will include drawings made by real children in a school setting.  The generated images will be used for developmental analysis. 
I have heard that Generative Adversarial Networks are a good tool for this kind of problem. If this is true, how would I go about applying a GAN to this challenge?
","['machine-learning', 'generative-adversarial-networks']","A generative adversarial network is probably not the best approach for generating the images desired.  We can assume from the comments that the data is not collected.  That's a good thing, because a set of rasterized images, labeled with student age or grade is an inferior input form.It appears that access to a student population is planned or already negotiated, which is also good.Although the drawing, as it is being drawn, is seen through each student's eyes, the primary features correlated with drawing skill development is motor control, shape formation, and color choice.  If the sheet of paper is placed over a drawing tablet, the tablet's incoming USB stream events are captured to a file, and the color selection is somehow recorded or automatically determined by having students hold the pencil or crayon up to the computer's camera before using it, a much better in natura input stream can be developed.Pre-processing can lead to an expression of each drawing experience as a sequence of events arranged in temporal order with the following dimensions for each event.Determining color from camera input may be developed using LSTM approaches.The dimensions of the label for each of these sequences would be those demographics and rankings that would most closely correlated with developmental stages.The micro-analysis attached to each ELEMENT in the sequence includes these additional dimensions.This is a modification of the system Google uses to synthesize speech, based on the WaveNet design. In the diagram, the residual function is defined as follows.$z = \tanh \, (W_{f,k} x + V_{f,k} y) \, \odot \, \sigma \, (W_{g,k} x + V_{g,k} y)$The development required is that the $\vec{a}$ must now be accompanied with scalars $r, \theta, \kappa, and \lambda$, but the resulting drawings are likely to have many of the hand-eye developmental features of the examples.
"
VIsual/musical/multimedia discourse (analysis) - are there such notions?,"
Formal semantics of natural language perceives sentences as logical expressions. Full paragraphs and even stories of natural language texts are researched and formalized using discourse analysis (Discourse Representation Theory is one example). My question is - is there research trend that applied the notion ""discourse"" to images, sounds and even animation? Is there such a notion as ""visual discourse""?
Google gives very few and older research papers, so - maybe the field exists, but it uses different terms and Google can not relate those terms to my keyword ""visual discourse"".
Basically - there are visual grammars and other pattern matching methods that can discover objects in the picture and relate them. But one should be able to read whole store from the picture (musical piece, multimedia content) and I imagine that such reading can be researched by multimedia discourse analysis. But there is no work under such terms. How it is done and named in reality?
","['image-recognition', 'sentiment-analysis']",
Which algorithms can we use on games with high branching factors (e.g. Connect6)?,"
Connect6 is an example of a game with a very high branching factor. It is about 45 thousand, dwarfing even the impressive Go.
Which algorithms can we use on games with such high branching factors?
I tried MCTS (soft rollouts, counting a ply as placing one stone), but it does not even block the opponent, due to the high branching factor.
In the case of Connect6, there are stronger AIs out there, but they aren't described in any research papers that I know of.
","['game-ai', 'monte-carlo-tree-search', 'algorithm-request', 'combinatorial-games', 'branching-factors']",
How to parse conjunctions in natural language processing?,"
Is there an accepted way in NLP to parse conjunctions (and/or) in a sentence?
By following the example below, how would I parse

I drink orange juice if its the weekend or if it's late and I'm tired.

into

it's the weekend

and 

it's late

and 

I'm tired

?
Implying an action will be taken when one of the above elements at the 1st level of depth is true.
I know when I hear the sentence that it means ""its the weekend"" OR (""it's late"" AND ""I'm tired""), but how could this be determined computationally?
Can an existing python/other library do this?
","['natural-language-processing', 'python', 'computational-linguistics']","This seems not easy for NLP. I doubt that state-of-the-art NLP tools can reliably determine the correct hierarchical structure of independent clauses. Examples below.The Berkeley parser gets it basically right in the sense that it can put its late and I'm tired on parallel, and they together on parallel with the weekend. But still not perfect (the weekend should be in the same subtree with It's rather than if its late and I'm tired)The Stanford parser, which is available in Python (NLTK), incorrectly parsed I\'m tired to the same level of I drink orange juice."
What are applications of object/human tracking in autonomous cars?,"
Objects tracking is finding the trajectory of each object in consecutive frames. Human tracking is a subset of object tracking which just considers humans.
I've seen many papers that divide tracking methods into two parts: 

Online tracking: Tracker just uses current and previous frames.
Offline tracking: Tracker uses all frames.

All of them mention that online tracking is suitable for autonomous driving and robotics, but I don't understand this part. What are the applications of object/human tracking in autonomous driving? 
Do you know some related papers?
","['computer-vision', 'autonomous-vehicles']",There are two important parts:
Which nodes are expanded in the expansion phase of MCTS?,"
I'm confused regarding a specific detail of MCTS.
To illustrate my question, let's take the simple example of tic-tac-toe.
After the selection phase, when a leaf node is reached, the tree is expanded in the so-called expansion phase. Let's say a particular leaf node has 6 children. Would the expansion phase expand all the children and run the simulation on them? Or would the expansion phase only pick a single child at random and run simulation, and only expand the other children if the selection policy arrives at them at some later point?
Alternatively, if both of these are accepted variants, what are the pros/cons of each one?
","['game-ai', 'monte-carlo-tree-search']","By far the most common (and likely also the most simple / straightforward) implementation is to expand exactly one node in the Expansion Phase; specifically, the node corresponding to the very first state selected (semi-)randomly by the Play-Out Phase. This is also pretty much the bare minimum you have to do if you want any form of tree growing at all (which you do).Other variants are possible too, but are much less common. The variant you suggest in the question is to expand all the children of the final node encountered during the Selection Phase, and run a Play-Out for all of them. I am not familiar with any literature on such a strategy really, never tried that myself. Intuitively, I would expect it to perform very similarly, perhaps slightly worse. Essentially what this would do is that it moves the ""behaviour"" of the search algorithm slightly more towards Breadth-First Search behaviour, rather than Best-First search behaviour. You spend a bit less of your computation time in the Selection Phase, because every Selection Phase is followed up by for example 6 (or whatever branching factor you have) Play-Outs instead of just a single one. On average I'd expect this to be slightly worse, because the Selection Phase is the primary source of the ""Best-First Search"" behaviour of the algorithm. I certainly don't expect a change like this to cause a large difference in performance though, if any. It will also likely be domain-dependent; worse in some cases, better in other cases.A different variant that I did once use myself is to expand every node in the complete line of play followed by the Play-Out phase. You can visualize this as a very ""thin"", but ""deep"" expansion, whereas your suggestion discussed above would be visualized as a ""shallow"" but ""wide"" expansion (and the conventional expansion strategy of a single node would be ""thin"" and ""shallow"").For this strategy, it is much easier to clearly define the advantages and disadvantages that it has in comparison to the standard strategy. The main advantage is that you retain more information from your Play-Outs, you throw less information away. This is because the Backpropagation phase, after the Play-Out terminates, can only store information in nodes that exist. If you immediately expand the complete line of play followed in the Play-Out Phase, you can store the result (the evaluation in the terminal state) in all of those nodes. If you don't expand the complete Play-Out (e.g. only expand the very first node), you'll have to ""skip"" all of those nodes which you didn't expand yet (they don't exist), and you can't store results in there yet.The main disadvantage of this approach is that it requires more memory, the tree grows a lot more quickly.I would personally recommend this approach if you have very strict limitations on computation time, if you expect to be able to only run very few iterations of the MCTS algorithm. For example, I personally used this in my General Video Game AI agent; this is a real-time game where you have to make decisions every ~40 milliseconds. In such a low amount of time, you cannot run many MCTS simulations. This means that:For contrast, if you're developing an agent to play a board game, and it has in the order of multiple minutes of thinking time per turn, the standard approach of only expanding a single node per Expansion Phase becomes a lot more appealing. If you're capable of running tens of thousands of iterations or more, it really doesn't hurt if you ""forget"" about a little bit of information deep down in the tree. The risk of running out of memory also becomes a lot more serious if you're running many iterations, so you don't want to grow the tree too quickly."
Is the singularity something to be taken seriously?,"
The term Singularity is often used in mainstream media for describing visionary technology. It was introduced by Ray Kurzweil in a popular book The Singularity Is Near: When Humans Transcend Biology (2005).
In his book, Kurzweil gives an outlook to a potential future of mankind which includes nanotechnology, computers, genetic modification and artificial intelligence. He argues that Moore's law will allow computers an exponential growth which results in a superintelligence.
Is the technological singularity something that is taken seriously by A.I. developers or is this theory just a load of popular hype?
","['philosophy', 'social', 'superintelligence', 'singularity', 'mythology-of-ai']",
Does depth-first search always stop when it has found the leftmost solution?,"
I'm a fresh learner of AI. I was told that depth-first search is not an optimal searching algorithm since ""it finds the 'leftmost' solution, regardless of depth or cost"". Therefore, does it mean that in practice, when we implement DFS, we should always have a checker to stop the search when it finds the first solution (also the leftmost one)?
","['search', 'implementation', 'depth-first-search']","One of the more standard assumptions when first introducing new students to search algorithms (like Depth-First Search, Breadth-First Search which you've also likely heard about or will hear about soon, etc.) is indeed that our goal is to find some sort of solution, and only find one.If our intention is to find just a single solution, then yes, you will need to check at every node whether that is a solution node, and you can stop the search process once you've found one.In practice, there can be all kinds of variants of this idea. Maybe in a different case you are interested in finding all solutions, rather than a single one; in such a case you would naturally not stop the search process after finding the first one, but continue searching.So, to conclude, it really depends on exactly what you want, why are you using a search algorithm. If you only care about finding a solution, you can stop when you have one."
How to architect a network to find bounding boxes in simple images?,"
I have an application where I want to find the locations of objects on a simple, relatively constant background (fixed camera angle, etc). For investigative purposes, I've created a test dataset that displays many characteristics of the actual problem.
Here's a sample from my test dataset.

Our problem description is to find the bounding box of the single circle in the image. If there is more than one circle or no circles, we don't care about the bounding box (but we at least need to know that there is no valid single bounding box).
For my attempt to solve this, I built a CNN that would regress (min_x, min_y, max_y, max_y), as well as one more value that could indicate how many circles were in the image.
I played with different architecture variations, but, in general, the architecture a was very standard CNN (3-4 ReLU convolutional layers with max-pooling in between, followed by a dense layer and an output layer with linear activation for the bounding box outputs, set to minimise the mean squared error between the outputs and the ground truth bounding boxes).
Regardless of the architecture, hyperparameters, optimizers, etc, the result was always the same - the CNN could not even get close to building a model that was able to regress an accurate bounding box, even with over 50000 training examples to work with.
What gives? Do I need to look at using another type of network as CNNs are more suited to classification rather than localisation tasks?
Obviously, there are computer vision techniques that could solve this easily, but due to the fact that the actual application is more involved, I want to know strictly about NN/AI approaches to this problem.
","['convolutional-neural-networks', 'computer-vision', 'object-detection', 'bounding-box']",
Which layer in a CNN consumes more training time: convolution layers or fully connected layers?,"
In a convolutional neural network, which layer consumes more training time: convolution layers or fully connected layers? 
We can take AlexNet architecture to understand this. I want to see the time breakup of the training process. I want a relative time comparison so we can take any constant GPU configuration.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'training']","NOTE: I did these calculations speculatively, so some errors might have crept in. Please inform of any such errors so I can correct it.In general in any CNN the maximum time of training goes in the Back-Propagation of errors in the Fully Connected Layer (depends on the image size). Also the maximum memory is also occupied by them. Here is a slide from Stanford about VGG Net parameters:Clearly you can see the fully connected layers contribute to about 90% of the parameters. So the maximum memory is occupied by them. As far as training time goes, it somewhat depends on the size (pixels*pixels) of the image being used. In FC layers it is straightforward that the number of derivatives you have to calculate is equal to the number of parameters. As far as the convolutional layer goes, let's see an example,let us take the case of 2nd layer:
It has got 64 filters of $(3*3*3)$ dimensions to be updated. The error is propagated from the 3rd layer. Each channel in the 3rd layer propagates it error to its corresponding $(3*3*3)$ filter. Thus $224*224$ pixels will contribute to about $224*224*(3*3*3)$ weight updations. And since there are $64$ such $224*224$ channels we get the total number of calculations to be performed as $64*224*224*(3*3*3) \approx 87*10^6 $ calculations.Now let us take the last layer of $56*56*256$. It will pass it gradients to the previous layer. Each $56*56$ pixel will update a $(3*3*256)$ filter. And since there are 256 such $56*56$ the total calculations required would be $256 * 56 * 56 * (3*3*256) \approx 1850 *10^6$ calculations.So number of calculations in a convolutional layer really depends on the number of filters and the size of the picture. In general I  have used the following formulae to calculate the number of updations required for filters in a layer, also I have considered $stride = 1$ since it will be the worst case:$channels_{output} * (pixelOutput_{height} * pixelOutput_{width}) * (filter_{height} * filter_{width} * channels_{input})$  Thanks to fast GPU's we are easily able to handle these huge calculations. But in FC layers the entire matrix needs to be loaded which causes memory problems which is generally not the case of convolutional layers, so training of convolutional layers is still easy. Also all these have to be loaded in the GPU memory itself and not the RAM of the CPU.Also here is the parameter chart of AlexNet:And here is a performance comparison of various CNN architectures:I suggest you check out the CS231n Lecture 9 by Stanford University for better understanding of the nooks and crannies of CNN architectures."
"What does ""hard for AI"" look like?","
In theoretical computer science, there is a massive categorization of the difficulty of various computational problems in terms of their asymptotic worst-time computational complexity. There doesn't seem to be any analogous analysis of what problems are ""hard for AI"" or even ""impossible for AI."" This is in some sense quite reasonable, because most research is focused on what can be solved. I'm interested in the opposite. What I do need to prove about a problem to prove that it is ""not reasonably solvable"" by AI?
Many papers say something along the lines of

AI allows us to find real-world solutions to real-world instances of NP-complete problems.

Is there a theoretical, principled reason for saying this instead of ""... PSPACE-complete problems""? Is there some sense in which AI doesn't work on PSPACE-complete, or EXPTIME-complete, or Turing complete problems?
My idea answer would be a reference to a paper that shows AI cannot be used to solve a particular kind of problem based on theoretical or statistical reasoning. Any answer exhibiting and justifying a benchmark for ""too hard for AI"" would be fine though (bonus points if the benchmark has a connection to complexity and computability theory).
If this question doesn't have an answer in general, answers about specific techniques would also be interesting to me.
","['reference-request', 'computational-learning-theory', 'computational-complexity', 'ai-completeness', 'theory-of-computation']","Nice Question!This is a perennial topic of discussion among AI researchers. The short answer is ""we don't really know which topics are hard in general, but we do know which we haven't got good techniques for yet.""Let's start by explaining why AI is not concerned with notions of computational complexity like NP-Completeness. AI researchers figured out in the 90's that most problems that are computationally hard in theory aren't actually hard in practice at all! For example, Boolean Satisfiability, the canonical NP-Hard problem, is known to have hard instances, but in practice, these almost never show up. Even when they do, we can usually get good approximate solutions with minimal computation time. Since many AI problems are reducible to SAT, whole areas of the field just use these approximation techniques and solvers. There's a good, if a bit old, survey here. Since 2008, things have only gotten better. Basically, NP-Hard stuff just isn't that hard. Worst-case complexity is therefore probably the wrong tool to guess at which problems are hard for AI.At the other end of the scale, we have subjective complexity based on things like how ""large"" the problem is. This has proven to be pretty unreliable. One example is Go, which I was told would ""never be solved by AI"" as late as 2010. Clearly, we were completely wrong. We just didn't know which techniques to use yet. Another example is language. Rule-based AI'ers tried at it for decades with minimal success. Probabilistic methods have achieved essentially human-level performance in less time. If you asked a researcher in the 1970s if language was hard for AI, they'd have said yes, but they'd have been wrong. This has been closely related to the advances in computing hardware: techniques that seemed wasteful and slow 40 years ago are now entirely practical. Sometimes they turn out to solve problems really well.Part of this ties into the issue that AI'ers don't really know or agree on what intelligence is, or what it means to solve a problem. Some AI'ers maintain that language really is hard, and that the systems we use now aren't really solving language, they're just making blind guesses that happen to be right a lot. Under that view, language is still hard. Fodor was a strong proponent of this view in the past, and his writings are a good place to read about it, but I've still heard people espouse views like this at AI conferences as recently as 2015.Something this usually considered hard right now is making a good general intelligence, that integrates knowledge across many domains and could plausibly pass something like the Turing test. Yet, there has been some progress here (look at virtual assistants), and some AI'ers reckon this might not be as hard as we thought either. So, basically, we don't know what's hard because we don't know when someone will come up with a new exciting technique that solves problems previously thought to be hard, we know that worst-case complexity isn't a good measurement system for ""intelligence"" requirements, and we don't know what intelligence really is. "
How can we compare the intelligence of AI systems?,"
One way of ranking human intelligence is based on IQ tests. But how can we compare the intelligence of AI systems? 
For example, is there a test that tells me that a spam filter system is more intelligent than a self-driving car, or can I say that a chess program is more intelligent than AlphaGo?
","['agi', 'performance', 'intelligence-testing']",
Why is the derivative of this objective function 0 if the policy is deterministic?,"
In the Berkeley RL class CS294-112 Fa18 9/5/18, they mention the following gradient would be 0 if the policy is deterministic.
$$
\nabla_{\theta} J(\theta)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right]
$$
Why is that?
","['reinforcement-learning', 'policy-gradients', 'policies', 'gradient', 'calculus']","Here is the gradient that they are discussing in the video:$$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta} (\mathbf{a}_{i, t} \vert \mathbf{s}_{i, t}) \right) \left( \sum_{t = 1}^T r(\mathbf{s}_{i,t}, \mathbf{a}_{i, t}) \right)$$In this equation, $\pi_{\theta} (\mathbf{a}_{i, t} \vert \mathbf{s}_{i, t})$ denotes the probability of our policy $\pi_{\theta}$ selecting the actions $\mathbf{a}_{i, t}$ that it actually ended up selecting in practice, given the states $\mathbf{s}_{i, t}$ that it encountered during the episode that we're looking at.In the case of a deterministic policy $\pi_{\theta}$, we know for sure that the probability of it selecting the actions that it did select must be $1$ (and the probability of it selecting any other actions would be $0$, but such a term does not show up in the equation). So, we have $\pi_{\theta} (\mathbf{a}_{i, t} \vert \mathbf{s}_{i, t}) = 1$ for every instance of that term in the above equation. Because $\log 1 = 0$, this leads to:\begin{aligned}
\nabla_{\theta} J(\theta) &\approx \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta} (\mathbf{a}_{i, t} \vert \mathbf{s}_{i, t}) \right) \left( \sum_{t = 1}^T r(\mathbf{s}_{i,t}, \mathbf{a}_{i, t}) \right) \\
%
&= \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=1}^T \nabla_{\theta} \log 1 \right) \left( \sum_{t = 1}^T r(\mathbf{s}_{i,t}, \mathbf{a}_{i, t}) \right) \\
%
&= \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=1}^T \nabla_{\theta} 0 \right) \left( \sum_{t = 1}^T r(\mathbf{s}_{i,t}, \mathbf{a}_{i, t}) \right) \\
%
&= \frac{1}{N} \sum_{i=1}^N 0 \left( \sum_{t = 1}^T r(\mathbf{s}_{i,t}, \mathbf{a}_{i, t}) \right) \\
%
&= 0 \\
\end{aligned}(i.e. you end up with a sum of terms that are all multiplied by $0$)."
Is there a way of pre-determining whether a CNN model will perform better than another?,"
I developed a CNN for image analysis. I've around 100K labeled images.  I'm getting a accuracy around 85% and a validation accuracy around 82%, so it looks like the model generalize better than fitting. So, I'm playing with different hyper-parameters: number of filters, number of layers, number of neurons in the dense layers, etc. 
For every test, I'm using all the training data, and it is very slow and time consuming.  
Is there a way to have an early idea about if a model will perform better than another?
","['machine-learning', 'training', 'deep-neural-networks']",
What are some good approaches that I can use to count the number of people in a crowd?,"
What are some good approaches that I can use to count the number of people in a crowd?
Tracking each person individually is obviously not an option. Any good approaches or some references to research papers would be very helpful.
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'reference-request', 'algorithm-request']",
What is artificial intelligence?,"
What is the definition of artificial intelligence?
","['terminology', 'definitions', 'history', 'academia', 'ai-field']",
How many episodes does it take for a vanilla one-step actor-critic agent to master the OpenAI BipedalWalker-v2 problem?,"
I'm trying to solve the OpenAI BipedalWalker-v2 by using a one-step actor-critic agent. I'm implementing the solution using python and tensorflow.
I'm following this pseudo-code taken from the book Reinforcement Learning An Introduction by Richard S. Sutton and Andrew G. Barto.

in summary, my question can be reduced to the following:

Is it a good idea to implement a one-step actor-critic algorithm to solve the OpenAI BipedalWalker-v2 problem? If not what would be a good approach? If yes; how long would it take to converge?
I run the algorithm for 20000 episodes, each episode has an avg of 400 steps, for each step, I immediately update the weights. The results are not better than random. I have tried different standard deviations (for my normal distribution that represents pi), different NN sizes for the Critic and Actor, and different learning-steps for the optimizer algorithm. The results never improve. I don't know what I'm doing wrong.

My Agent Class
import tensorflow as tf
import numpy as np
import gym
import matplotlib.pyplot as plt

class agent_episodic_continuous_action():
    def __init__(self, lr,gamma,sample_variance, s_size,a_size,dist_type):
       ... #agent parameters

    def save_model(self,path,sess):    
    def load_model(self,path,sess):       
    def weights_init_actor(self,hidd_layer,mean,stddev): #to have control over the weights initialization      
    def weights_init_critic(self,hidd_layer,mean,stddev):  #to have control over the weights initialization            
    def create_actor_brain(self,hidd_layer,hidd_act_fn,output_act_fn,mean,stddev):  #actor is represented by a fully connected NN      
    def create_critic_brain(self,hidd_layer,hidd_act_fn,output_act_fn,mean,stddev): #critic is represented by a fully connected NN      
    def critic(self):            
    def get_delta(self,sess):                 
    def normal_dist_prob(self): #Actor pi distribution is a normal distribution whose mean comes from the NN 
    def create_actor_loss(self): 
    def create_critic_loss(self):
    def sample_action(self,sess,state): #Sample actions from the normal dist. Whose mean was aprox. By the NN
    def calculate_actor_loss_gradient(self):
    def calculate_critic_loss_gradient(self):   
    def update_actor_weights(self):
    def update_critic_weights(self):
    def update_I(self):  
    def reset_I(self):      
    def update_time_step_info(self,s,a,r,s1,d):  
    def create_graph_connections(self):
    def bound_actions(self,sess,state,lower_limit,uper_limit):  

Agent instantiation
tf.reset_default_graph()
agent= agent_episodic_continuous_action(learning-step=1e-3,gamma=0.99,pi_stddev=0.02,s_size=24,a_size=4,dist_type=""normal"")
agent.create_actor_brain(hidden_layers=[12,5],hidden_layers_fct=""relu"",output_layer=""linear"",mean=0.0,stddev=0.14)
agent.create_critic_brain(hidden_layers=[12,5],hidden_layers_fct=""relu"",output_layer=""linear"",mean=0.0,stddev=0.14)
agent.create_graph_connections()

path = ""/home/diego/Desktop/Study/RL/projects/models/biped/model.ckt""   
env = gym.make('BipedalWalker-v2')
uper_action_limit = env.action_space.high
lower_action_limit = env.action_space.low   
total_returns=[]

Training loops
with tf.Session() as sess:
    try:
        sess.run(agent.init)
        sess.graph.finalize()
        #agent.load_model(path,sess)        
        for i in range(1000): 
            agent.reset_I()
            s = env.reset()    
            d = False
            while (not d):
                a=agent.bound_actions(sess,s,lower_action_limit,uper_action_limit)  
                s1,r,d,_ = env.step(a)
                #env.render()
                agent.update_time_step_info([s],[a],[r],[s1],d)                 
                agent.get_delta(sess)
                sess.run([agent.update_critic_weights,agent.update_actor_weights],feed_dict={agent.state_in:agent.time_step_info['s']})
                agent.update_I()  
                s = s1
        agent.save_model(path,sess)    
    except Exception as e:
        print(e)

","['reinforcement-learning', 'tensorflow', 'python', 'open-ai']",
Why does the clipped surrogate objective work in Proximal Policy Optimization?,"
In Proximal Policy Optimization Algorithms (2017), Schulman et al. write

With this scheme, we only ignore the change in probability ratio when it would make the objective improve, and we include it when it makes the objective worse.

I don't understand why the clipped surrogate objective works. How can it work if it doesn't take into account the objective improvements?
","['reinforcement-learning', 'papers', 'objective-functions', 'proximal-policy-optimization']","Ok, so I think I have a better understanding of this now.Firstly, let's remind the main idea of the PPO : staying close to the previous policy. It's the same idea than in TRPO, but the L function is improved.So, you wanna make ""small but safe steps"". With clipped surrogate objective, you don't give too much importance to promising actions. You learn that bad actions are bad, so you decrease their probability according to ""how bad"" they are. But for good actions, you only learn that they are ""a little bit good"", and their probability will be just slightly increased.This mechanism allows you to perform small but relevant updates of your policy.hope this will help someone :)"
Is it necessary to know the details behind the AI algorithms and models?,"
I am interested in the field of artificial intelligence. I began by learning the various machine learning algorithms. The maths behind some were quite hard. For example, back-propagation in convolutional neural networks. 
Then when getting to the implementation part, I learnt about TensorFlow, Keras, PyTorch, etc. If these provide much faster and more robust results,  will there be a necessity to code a neural network (say) from scratch using the knowledge of the maths behind back-prop, activation functions, dimensions of layers, etc., or is the role of a data scientist only to tune the hyper-parameters?
Further, as of now the field of AI does not seem to have any way to solve for these hyperparameters, and they are arrived at through trial and error. Which begs the question, can a person with just basic intuition about what the algorithms do be able to make a model just as good as a person who knows the detailed mathematics of these algorithms?
","['machine-learning', 'data-science', 'academia', 'education']",
Integration of Sentiment analysis in CRM,"
What is the process for integrating sentiment analysis in a CRM? What I am searching for is a system which analyzes the customer comments or reviews using the CRM and finds out the customer sentiment on the services provided by the system or company or a product.
I have done a sentiment analyzer which takes text and shows the sentiment of the text. Now I want to integrate the above-mentioned sentiment analyzer to a CRM, how can I do that?
","['machine-learning', 'sentiment-analysis']",
Is a binary attribute type the same as binomial attribute type?,"
I am not sure if I can use the words binomial and binary and boolean as synonyms to describe a data attribute of a data set which has two values (yes or no). Are there any differences in the meaning on a deeper level?
Moreover, if I have an attribute with three possible values (yes, no, unknown), this would be an attribute of type polynominal. What further names are also available for this type of attribute? Are they termed as ""symbolic""?
I am interested in the realtion between the following attribute type: binary, boolean, binominal, polynominal (and alternative describtions) and nominal. 
","['datasets', 'structured-data', 'categorical-data']",
Is it possible to use an RNN to predict a feature that is not an input feature?,"
I came across RNN's a few minutes ago, which might solve a problem with sequenced data I've had for a while now.
Let's say I have a set of input features, generated every second. Corresponding with these input features, is an output feature (also available every second). 
One set of input features does not carry enough data to correlate with the output feature, but a sequence of them most definitely does. 
I read that RNN's can have node connections along sequences of inputs, which is exactly what I need, but almost all implementations/explanations show prediction of the next word or number in a text-sentence or in a sequence of numbers. 
They predict what would be the next input value, the one that completes the sequence. However, in my case, the output feature will only be available during training. During inference, it will only have the input features available. 
Is it possible to use RNN in this case? Can it also predict features that are not part of the input features?
Thanks in advance!
","['neural-networks', 'machine-learning', 'recurrent-neural-networks']","Is it possible to use RNN in this case? Can it also predict features that are not part of the input features?Yes.No changes are required to a RNN in order to do this. All you need is correctly labelled data mapping a sequence of $x$ to correct $y$ in order to train, and of course a RNN architecture which has input vectors matching shape of $x$ and output vectors matching shape of $y$. The case where $x$ and $y$ are the same data type is just a special case of RNN design, and not a requirement.You may need to consider some details:If the relationship between $x$ and $y$ is complex and non-linear even accounting for accumulated hidden state during the sequence, you may need to add deeper layers. The output of the LSTM can be some vector $h$ and you can add fully-connected layers to help with predicting $y$ from $h$. This, or adding more LSTM layers, is a choice of hyperparameter that you may want to experiment with. Start with a basic LSTM to see how that goes first. If you wish to predict a sequence of output features that is either not the same length as the input feature sequence, or logically should come after the whole sequence (think language translation) then this may need a slight change in setup to get best results. For a predict-same-kind sequence you can feed your predicted output value into the next input, but if input and output have different data types, this will not work. Instead, you will need to have some dummy input or other setup for creating sequences of $y$. In your specific case the second point does not seem to apply, as you want to predict a single $y$ immediately after a sequence of $x$."
Automatic prediction of whether a customer will come into the shop or not,"
So as my university project I am planning to make a prediction system as described in the title. My current idea is to use the age/gender classifier and run it on a video(taken in front of a shop) which outputs a csv file of the age/gender/Customer ID. In addition, I will use the existing data of the shop of who came in/who didn't come into the shop but passed by the shop and by running XGBoost on this csv data I can predict which customer will come into the shop or not. 
Do you think this idea is possible? Is there any other way to implement this idea. It would also be great if we could implement this in such a way as to make the deep learning model learn the various features of those who come into the shop or not. 
","['machine-learning', 'deep-learning', 'tensorflow', 'keras', 'data-science']",
Can layers of deep neural networks be seen as Hopfield networks?,"
Hopfield networks are able to store a vector and retrieve it starting from a noisy version of it. They do so setting weights in order to minimize the energy function when all neurons are set equal to the vector values, and retrieve the vector using the noisy version of it as input and allowing the net to settle to an energy minimum. 
Leaving aside problems like the fact that there is no guarantee that the net will settle in the nearest minimum etc – problems eventually solved with Boltzmann machines and eventually with back-propagation – the breakthrough was they are a starting point for having abstract representations. Two versions of the same document would recall the same state, they would be represented, in the network, by the same state. 
As Hopfield himself wrote in his 1982 paper Neural networks and physical systems with emergent collective computational abilities

The present modeling might then be related to how an entity or Gestalt is remembered or categorized on the basis of inputs representing a collection of its features.

On the other side, the breakthrough of deep learning was the ability to build multiple, hierarchical representations of the input, eventually leading to making AI-practitioners' life easier, simplifying feature engineering. (see e.g. Representation Learning: A Review and New Perspectives, Bengio, Courville, Vincent).
From a conceptual point of view, I believe one can see deep learning as a generalization of Hopfield nets: from one single representation to a hierarchy of representation.
Is that true from a computational/topological point of view as well? Not considering how ""simple"" Hopfield networks were (2-state neurons, undirected, energy function), can one see each layer of a network as a Hopfield network and the whole process as a sequential extraction of previously memorized Gestalt, and a reorganization of these Gestalt?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'topology', 'hopfield-network']",
How can I detect datetime patterns in text? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I want to explore and experiment the ways in which I could use a neural network to identify patterns in text. 
examples:

Prices of XYZ stock went down at 11:00 am today
Retrieve a list of items exchanged on 03/04/2018
Show error logs between 3 - 5 am yesterday.
Reserve a flight for 3rd October.
Do I have any meetings this Friday?
Remind to me wake up early tue, 4th sept

This is for a project so I am not using regular expressions. Papers, projects, ideas are all welcome but I want to approach feature extraction/pattern detection to have a model trained which can Identify patterns that it has already seen.
","['neural-networks', 'pattern-recognition']","ApproachesThere are two main approaches to detecting any human readable representation of a discrete quantity within text.There are other approaches and there are hybrids of these two or one of these and the other approaches, but these two are the theoretically most straightforward and likely to produce both reliability and accuracy.Re-entrant LearningWhether the training involves re-entrant learning techniques, such as reinforcement, is a tangential issue that this answer will not address, but know that whether all training is solely a deployment component or whether adaptation and/or convergence occurs in real time is an architectural decision to be made.Practical ConcernsPractically, the outputs of each recognition are as follows.Also practically, the input must either be from within one particular locale's norms in terms of... or ...... or ...Much of the locale specific syntax must be normalized to a general date and time language such as this:जनवरी --> D_01Enero --> D_01Janúar --> D_01so that Filipino and Icelandic names for the first month of the year enter the artificial network as the same binary pattern.**Date and Time Specifically*In the case of 1. above, which is semi-heuristic in nature, and assuming that the locale is entirely en-US.utf-8, the CASE INSENSITIVE patterns for a PCRE library or equivalent to use as a search orientation heuristic include the following.There should be others for time, hyphenated or slash delimited dates, or time zone.The positions and normalized encoding of these date and time artifacts are then substituted into the artificial network inputs instead of the original text in the stream, reducing redundancy and improving both the speed of training and the resulting accuracy and reliability of recognition.In the case of 2. above, the entire burden of recognition is left to the artificial network.  The advantage is less reliance on date and time conventions.  The disadvantage is a much larger burden placed on training data variety and training epochs, meaning a much higher burden on computing resources and the pacience of the stake holder for the project.WindowingAn overlapping windowing strategy is necessary.  Unlike FFT spectral analysis in real time, the windowing must be rectangular, because the size of the window is the width of the input layer of the artificial network.  Experimenting with the normalization of input such that the encoding of text and data and time components entering the input layer could greatly vary the results in terms of training speed, recognition accuracy, reliability, and adaptability to varying statistical distributions of date and time instances and relationships."
What are the differences between an agent that thinks rationally and an agent that acts rationally?,"
Stuart Russell and Peter Norvig pointed out 4 four possible goals to pursue in artificial intelligence: systems that think/act humanly/rationally.
What are the differences between an agent that thinks rationally and an agent that acts rationally?
","['comparison', 'terminology', 'intelligent-agent']","Maybe a good example to think about would be something like the Sphyx story. The wasp in the story appears to behave like a rational being: it seems to have a plan of action, it seems to be able to do advanced operations like counting, and it seems to execute the plan well. However, if you disrupt the wasp's plan, it becomes apparent that it is not thinking rationally. Instead, it has evolved a very complex behaviour that appears to include rational components, but is still just an instinct.In the context of AI, consider a GOFAI system for planning like GraphPlan as opposed to a machine learning system for generating plans. While the former is a general-purpose algorithm for reasoning about planning problems, the latter is an input/output mapping that may mimic reasoning, or may be more ""instinctual"". Some AI'ers would say that the latter system is not really engaged in rational thought, while the former is. Both systems exhibit rational action, however.Instinctive systems often work very well, and I'd say that the increasing effectiveness of machine learning approaches since that edition of R&N was published (almost 10 years ago now?) makes this a fuzzier distinction in practice than the book might suggest."
Can Q-learning be used to find the shortest distance from each source to destination?,"
Is it possible to form a table that will have simply the shortest distance from each source to destination using q learning?
If not, suggest any other learning algorithm.
","['machine-learning', 'reinforcement-learning', 'q-learning']",
What benefits can be got by applying Graph Convolutional Neural Network instead of ordinary CNN?,"
What benefits can we got by applying Graph Convolutional Neural Network instead of ordinary CNN? I mean if we can solve a problem by CNN, what is the reason should we convert to Graph Convolutional Neural Network to solve it? Are there any examples i.e. papers can show by replacing ordinary CNN with Graph Convolutional Neural Network, an accuracy increasement or a quality improvement or a performance gain is achieved? Can anyone introduce some examples as image classification, image recognition especially in medical imaging, bioinfomatics or biomedical areas?
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'graphs', 'geometric-deep-learning']","Generally speaking a graph CNN is applied to data represented by graphs, not images. a graph is a collection of nodes and edges connecting them.an image is a 2D or 3D matrix, in which each element denotes a pixel in spaceIf your data are just images, or something similar (e.g. some fMRI data), you usually cannot benefit from graph CNN compared with usual CNN.Sometimes, the class labels of your images may be organized in a graph-like  (or tree-like) structure. In that case, you may have a chance to benefit from graph CNN."
How do the current input and the output of the previous time step get combined in an LSTM?,"
I am currently looking into LSTMs. I found this nice blog post, which is already very helpful, but still, there are things I don't understand, mostly because of the collapsed layers. 

The input $X_t$, and the output of the previous time step $H_{t-1}$, how do they get combined? Multiplied, added or what? 
The input weights and the weights of the input of the previous time step, those are just the weights of the connections between the time-steps/units, right?

","['machine-learning', 'deep-learning', 'long-short-term-memory', 'architecture']","(1) $X_t$ and $H_{t-1}$ are concatenated. The blog you cited explained its notation ""Lines merging denote concatenation"". For example, if $X_t=[1,2,3]$ and $H_{t-1}=[4,5,6,7]$, then their concatenation is $[1,2,3,4,5,6,7]$(2) When you say ""input weights"" or ""weights of the input of the previous time step"", are you referring to the $W_i$ in your cited blog? If so they are not the weights of the connections between the time-steps/units. They are part of the input gate only. The connections between the time-steps/units do not have weights applied to them."
Need Help With LSTM Neural Networks,"
I have been researching LSTM neural networks.  I have seen this diagram a lot and I have few questions about it. Firstly, is this diagram used for most LSTM neural networks?  
Secondly, if it is, wouldn't only having single layers reduce it's usefulness?  
","['neural-networks', 'machine-learning', 'long-short-term-memory']","Just to be 100% sure - the diagram you refer to is a diagram of an LSTM CELL, not NETWORK. The operands you see on the diagram are operations within a cell, not separate ""neurons"". I think it is quite obvious, however reading your questions I just wanted to be 100% sure we are on the same page.Now, about layers. RNN networks (LSTM in particular) are just like any other ANN structure. Theoretically, a 1-hidden layer network can do any computation of a ""deeper"" network. ANN is a universal approximate of math functions. Still, multi-layer ANNs typically work better on more complex problems. Multi-layer typically needs less total connections, learns better and is less resource-demanding.In particular, multi-layer LSTMs are believed to be better at determining complex in-time patterns. I think there is no rigorous proof for this, however.
Also, in practical applications - I did not see much improvement in network capabilities by adding additional LSTM layers. Adding more dense layers before/ after LSTM seemed to have a much better effect."
How to model categorical variables / enums?,"
I am new to the field and I am trying to understand how is possible to use categorical variables / enums?
Lets say we have a data set and 2 of its features are home_team and away_team, the possible values of these 2 features are all the NBA teams.
How can we ""normalize"" these features to be able to use them to create a deep network model (e.g. with tensorflow)?
Any reference to read about techniques of modeling that are also very appreciated.
","['deep-learning', 'categorical-data']",
Should the input to the negative log likelihood loss function be probabilities?,"
I am trying to train a supervised model where the output from the model is output of a linear function $WX + b$. Kindly note that I'm not using any softmax or $\log$ softmax on the result of the linear. I am using negative log-likelihood loss function, which takes the input as the linear output from the model and the true labels. I am getting decent accuracy by doing this, but I have read that the input to negative log-likelihood function must be probabilities. Am I doing something wrong?
","['deep-learning', 'objective-functions', 'activation-functions']",
Can one use an Artificial Neural Network to determine the size of an object in a photograph?,"
My question relates to but doesn't duplicate a question that has been asked here.
I've Googled a lot for an answer to the question: Can you find the dimensions of an object in a photo if you don't know the distance between the lens and the object, and there are no ""scales"" in the image?
The overwhelming answer to this has been ""no"". This is, from my understanding, due to the fact that, in order to solve this problem with this equation,
$$Distance\ to\ object(mm) = \frac{f(mm) * real\ height(mm) * image\ height(pixels)}{object\ height(pixels) * sensor\ height(mm)}   $$
you will need to know either the ""real height"" or the ""distance to object"". It's the age old issue of ""two unknowns, one equation"". That's unsolvable. A way around this is to place an object in the photo with a known dimension in the same plane as the unknown object, find the distance to this object and use that distance to calculate the size of the unknown (this relates to answer from the question I linked above). This is an equivalent of putting a ruler in the photo and it's a fine way to solve this problem easily.
This is where my question remains unanswered. What if there is no ruler? What if you want to find a way to solve the unsolvable problem? Can we train an Artificial Neural Network to approximate the value of the real height without the value of the object distance or use of a scale? Is there a way to leverage the unexpected solutions we can get from AI to solve a problem that is seemingly unsolvable?
Here is an example to solidify the nature of my question:
I would like to make an application where someone can pull out their phone, take a photo of a hail stone against the ground at a distance of ~1-3 ft, and have the application give them the hail stone dimensions. My project leader wants to make the application accessible, which means he doesn't want to force users to carry around a quarter or a special object of known dimensions to use as a scale.
In order to avoid the use of a scale, would it be possible to use all of the EXIF meta-data from these photos to train a neural network to approximate the size of the hail stone within a reasonable error tolerance? For some reason, I have it in my head that if there are enough relevant variables, we can design an ANN that can pick out some pattern to this problem that we humans are just unable to identify. Does anyone know if this is possible? If so, is there a deep learning model that can best suit this problem? If not, please put me out of my misery and tell me why it it's impossible.
","['neural-networks', 'computer-vision', 'prediction', 'object-recognition']",
What is the definition of rationality?,"
I'm having a little trouble with the definition of rationality, which goes something like:

An agent is rational if it maximizes its performance measure given its current knowledge.

I've read that a simple reflex agent will not act rationally in a lot of environments. For example, a simple reflex agent can't act rationally when driving a car, as it needs previous perceptions to make correct decisions.
However, if it does its best with the information it's got, wouldn't that be rational behaviour, as the definition contains ""given its current knowledge""? Or is it more like: ""given the knowledge it could have had at this point if it had stored all the knowledge it has ever received""?
Another question about the definition of rationality: Is a chess engine rational as it picks the best move given the time it's allowed to use, or is it not rational as it doesn't actually (always) find the best solution (would need more time to do so)?
","['definitions', 'intelligent-agent', 'chess', 'rationality', 'simple-reflex-agents']","I've read that a simple reflex agent will not act rationally in a lot of environments. E.g. a simple reflex agent can't act rationally when driving a car as it needs previous perceptions to make correct decisions.I wouldn't say that the need for previous perceptions is the reason why a simple reflex agent doesn't act rationally. I'd say the more serious issue with simple reflex agents is that they do not perform long-term planning. I think that is the primary issue that causes them to not always act rationally, and that is also consistent with the definition of rationality you provided. A reflex-based agent typically doesn't involve long-term planning, and that's why it in fact does not often do best given the knowledge it has.Another question about the definition of rationality: Is a chess engine rational as it picks the best move given the time its allowed to use, or is it not rational as it doesn't actually (always) find the best solution (would need more time to do so)?An algorithm like minimax in its ""purest"" formulation (without a limit on search depth) would be rational for games like chess, since it would play optimally. However, that is not feasible in practice, it would take too long to run. In practice, we'll run algorithms with a limit on search depth, to make sure that they stop thinking and pick a move in a reasonable amount of time. Those will not necessarily be rational. This gets back to bounded rationality as described by DukeZhou in his answer.The story is not really clear if we try to talk about this in terms of ""picking the best move given the time it's allowed to use"" though, because what is or isn't possible given a certain amount of time depends very much on factors such as:For example, hypothetically I could say that I implement an algorithm that requires a database of pre-computed optimal solutions, and the algorithm just looks up the solutions in the database and instantly plays the optimal moves. Such an algorithm would be able to truly be rational, even given a highly limited amount of time. It would be difficult to implement in practice because we'd have difficulties constructing such a database in the first place, but the algorithm itself is well-defined. So, you can't really include something like ""given the time it's allowed to use"" in your definition of rationality."
How to define states in reinforcement learning?,"
I am studying reinforcement learning and the variants of it. I am starting to get an understanding of how the algorithms work and how they apply to an MDP.
What I don't understand is the process of defining the states of the MDP. In most examples and tutorials, they represent something simple like a square in a grid or similar.
For more complex problems, like a robot learning to walk, etc.,

How do you go about defining those states?
Can you use learning or classification algorithms to ""learn"" those states?

","['reinforcement-learning', 'ai-design', 'markov-decision-process', 'state-spaces', 'state-representations']","The problem of state representation in Reinforcement Learning (RL) is similar to problems of feature representation, feature selection and feature engineering in supervised or unsupervised learning.Literature that teaches the basics of RL tends to use very simple environments so that all states can be enumerated. This simplifies value estimates into basic rolling averages in a table, which are easier to understand and implement. Tabular learning algorithms also have reasonable theoretical guarantees of convergence, which means if you can simplify your problem so that it has, say, less than a few million states, then this is worth trying.Most interesting control problems will not fit into that number of states, even if you discretise them. This is due to the ""curse of dimensionality"". For those problems, you will typically represent your state as a vector of different features - e.g. for a robot, various positions, angles, velocities of mechanical parts. As with supervised learning, you may want to treat these for use with a specific learning process. For instance, typically you will want them all to be numeric, and if you want to use a neural network you should also normalise them to a standard range (e.g. -1 to 1).In addition to the above concerns which apply for other machine learning, for RL, you also need to be concerned with the Markov Property - that the state provides enough information, so that you can accurately predict expected next rewards and next states given an action, without the need for any additional information. This does not need to be perfect, small differences due to e.g. variations in air density or temperature for a wheeled robot will not usually have a large impact on its navigation, and can be ignored. Any factor which is essentially random can also be ignored whilst sticking to RL theory - it may make the agent less optimal overall, but the theory will still work. If there are consistent unknown factors that influence result, and could logically be deduced - maybe from history of state or actions - but you have excluded them from the state representation, then you may have a more serious problem, and the agent may fail to learn.It is worth noting the difference here between observation and state. An observation is some data that you can collect. E.g. you may have sensors on your robot that feed back the positions of its joints. Because the state should possess the Markov Property, a single raw observation might not be enough data to make a suitable state. If that is the case, you can either apply your domain knowledge in order to construct a better state from available data, or you can try to use techniques designed for partially observable MDPs (POMDPs) - these effectively try to build missing parts of state data statistically. You could use a RNN or hidden markov model (also called a ""belief state"") for this, and in some way this is using a ""learning or classification algorithms to ""learn"" those states"" as you asked.Finally, you need to consider the type of approximation model you want to use. A similar approach applies here as for supervised learning: A simple linear regression with features engineered based on domain knowledge can do very well. You may need to work hard on trying different state representations so that the linear approximation works. The advantage is that this simpler approach is more robust against stability issues than non-linear approximationA more complex non-linear function approximator, such as a multi-layer neural network. You can feed in a more ""raw"" state vector and hope that the hidden layers will find some structure or representation that leads to good estimates. In some ways, this too is ""learning or classification algorithms to ""learn"" those states"" , but in a different way to a RNN or HMM. This might be a sensible approach if your state was expressed naturally as a screen image - figuring out the feature engineering for image data by hand is very hard.The Atari DQN work by DeepMind team used a combination of feature engineering and relying on deep neural network to achieve its results. The feature engineering included downsampling the image, reducing it to grey-scale and - importantly for the Markov Property - using four consecutive frames to represent a single state, so that information about velocity of objects was present in the state representation. The DNN then processed the images into higher-level features that could be used to make predictions about state values."
How can I create an artificially intelligent aimbot for a game like CS:GO?,"
How can I create an artificially intelligent aimbot for a game like Counter-Strike Global Offensive (CS:GO)?
I have an initial solution (or approach) in mind. We can train an image recognition model that will recognize the head of the enemy (in the visible area of the player, so excluding the invisible area behind the player, to avoid being easily detected by VAC) and move the cursor to the position of the enemy's head and fire.
It would be much more preferable to train the recognition model in real-time than using demos. Most of the available demos you might have might be 32 tick, but while playing the game, it works at 64 tick.
It is a very fresh idea in my mind, so I didn't actually think a lot about it. Ignoring facts like detection by VAC for a few moments.
Is there any research work on the topic? What are the common machine learning approaches to tackle such a problem?
Later on, this idea can be expanded to a completely autonomous bot that can play the game by itself, but that is a bit too much initially.
","['machine-learning', 'game-ai', 'applications', 'reference-request']","Automation of Game-playAimbots are indeed designed to provide assistance to the human game player when the complexity of game play escapes full cybernetic autonomy at the current state of technology.  There are five basic components in any game player, DNA based or digital.The models are as follows for a CS:GO aimbot.Learning all of these is not in the scope of current deep learning strategies but not outside the scope of AI if the following problem analysis and system approaches are taken.The two services, (a) the provision of suggestions and (b) the automation of minor tasks, may indeed represent the low hanging fruit from a software engineering perspective, but the problem analysis and system approach above may provide more.Objectives in CS:GOThe CS:GO (Counter-Strike Global Offensive) game seems to have been written from a Westphalian geopolitical point of view.  This is the typical western perspective, somewhat oblivious to the mindset of the true nature of asymmetric warfare1.  This answer will focus on the creation of an aimbot for the existing models of game-play rather than a realistic simulation of geopolitical balance in this decade.We have the objective types listed in online resources that provide a game overview, again, narrowed in authenticity by the prevailing western view of asymmetric war1.Ballistic ControlThe targetting of the body or head of an opponent is within the scope of what image recognition can do in conjunction with a movement model.  In military applications, aeronautic devices must be propelled against air friction and the propulsion requires a largely exothermic reaction like combustion.  Thus all targets have a heat signature, which can be recognized in an infrared video stream in such a way as to plot an intercept course for the ballistic weapon.The targetting formulation for CS:GO is not as complex and aiming and firing may be fully automated with much less software machinery.  A LSTM with sufficient speed can be trained to recognize a head in subsequent frames and terminate opponents even if moving.  A simple web search for LSTM will provide a plethora of resources to the novice intending to learn about image recognition.One AmbiguityWhether the second objective can be met is dependent on what is meant by the term, ""Viewing angles,"" in the context of image recognition.  Can the player see from perspectives other than the location of their eyes?  If so, this answer can be adjusted if given a clear picture of what is meant.Training and Re-entrant LearningTraining of an artificial neural net to target a head is unnecessary unless the 3D rendering of the game objects and players is distorted by a wide angle virtual lens and trajectories and movements are curved.  As mentioned LSTM can be used to locate a head in multiple frames and extrapolate an opposing players trajectory.Where deep learning may be most effective is in the training of how to interact with the player to best assist.  Also, if there are other non-targeting techniques that are more discrete, those who play CS:GO well could record their interactions and those recordings can be processed in preparation for use as training data.Certainly a re-entrant learning strategy such as reinforcement is useful for game-play especially if the make up of teams changes and players exhibit different behaviors, executing differing strategies over different networks with different latencies and through-puts, and communicating with the game clients through different peripheral devices.[DeepMind Lab Test Bed for Reinforcement Technology](https://github.com/deepmind/lab}More than SuggestionsWith proper architecture, more than suggestive strategies can be provided to the player.  Statistical dashboards, identification of a bomb before or after planting, and identification of hostages should be among the aimbot services provided, which might suggest a new name, such as obot for objective bot or asbot for assistive bot.It is not certain that the aimbot interface need be integrated with dashboards or bomb or hostage identifiers.  Sometimes independent bots provide a more flexible arrangement for a user.  Individual bots can always use the same underlying image recognition components and models.Entry Points into Developing Such a SystemRead some of the work on the above concepts and download what code you can find that demonstrates it in Python or Java, install what is necessary, and develop some proficiency with the components discussed above as well as the associated theory.  Don't shy away from the math, since success will require some proficiency with feedback signalling and concepts like gradient descent and back-propagation.Reinforcement in GamesLSTM Head LocatingPlaying Atari with Deep Reinforcement Learning, Mnih et al., 2013Phased ApproachThe following phased research and development approach is suggested.Footnotes[1] In asymmetric power struggles, there are always at least two factions within each side because didactic legitimacy seeks division.  Unity is not practically possible.  Each real team usually has a more religious and more secular faction, each of which has economic, philosophic, and historical justifications for their position and agenda.  Also, terrorists don't seek the public detonation of bombs or retention of hostages as objective but rather as means, with the total elimination of all not fully adhered to their view of legitimacy as the sole endgame objective.  Suicide or high risk bombing is considered by most of those that employ it as the poor man's nukes, so without nuclear strike capability for the counter-terrorists and their allies, the terrorism lacks the important dimension of last resort.  The last resort aspect of nuclear strike is missing from the counter-terrorist side too.  CS:GO may sell better by glossing over these particular characteristics of asymmetric warfare and such was left out deliberately.  There may be some benefit to adding these features in from an educational and anti-propaganda point of view."
How does the Dempster-Shafer theory differ from Bayesian reasoning?,"
How does the Dempster-Shafer theory differ from Bayesian reasoning? How do these two methods handle uncertainty and compute posterior distributions?
","['comparison', 'uncertainty-quantification', 'bayesian-probability', 'dempster-shafer-theory']",
Use cross-validation to train after model selection,"
I have been recently reading about model selection algorithms (for example to decide which value of the regularisation parameter or what size of a neural network to use, broadly hyper-parameters). This is done by dividing the examples into three sets (training 60%, cross-validation 20%, test 20%) and training is done on the data with the first set for all parameters, and then choose the best parameter based on the result in the cross-validation and finally estimate the performance using the test set.
I understand the need for a different data-set compared to training and test for select the model, however, once the model is selected, why not using the cross-validation examples to improve the hypothesis before estimating the performance?
The only reason I could see is that this could cause the hypothesis to worsen and we wouldn't be able to detect it, but, is it really possible that by adding much more examples (60% -> 80%) the hypothesis gets worse?
","['neural-networks', 'machine-learning', 'training', 'datasets']","You are quite correct. If you have properly followed the Cross Validation procedure and selected the best model indeed, then you can use the CV set as the training set for the final model. And no it will not cause your hypothesis to worsen (for that set maybe, but not for new examples) if you have selected the model correctly. In-fact you may use the entire 100% of the data-set.Justin Johnson a TA at Stanford University answered a similar type of question on training CNN's using 100% of the data-set. He said that if you had enough computational resources and want to squeeze that extra 1% or 2% accuracy from your model you can use the entire data-set after model selection.NOTE: As @NeilSlater pointed out, if you need the model for reporting purposes you should only use 80% of the data-set, otherwise you'll lose your only source for unbiased model verification. But if you are looking to deploy the model on field you can use 100% of the data-set."
How to implement a variable action space in Proximal Policy Optimization?,"
I'm coding a Proximal Policy Optimization (PPO) agent with the Tensorforce library (which is built on top of TensorFlow).
The first environment was very simple. Now, I'm diving into a more complex environment, where all the actions are not available at each step.
Let's say there are 5 actions and their availability depends on an internal state (which is defined by the previous action and/or the new state/observation space):

2 actions (0 and 1) are always available
2 actions (2 and 3) are only available when the internal state is 0
1 action (4) is only available when the internal state is 1

Hence, there are 4 actions available when the internal state is 0 and 3 actions available when the internal state is 1.
I'm thinking of a few possibilities to implement that:

Change the action space at each step, depending on the internal state. I assume this is nonsense.

Do nothing: let the model understand that choosing an unavailable action has no impact.

Do almost nothing: impact slightly negatively the reward when the model chooses an unavailable action.

Help the model: by incorporating an integer into the state/observation space that informs the model what's the internal state value + bullet point 2 or 3


Are there other ways to implement this? From your experience, which one would be the best?
","['reinforcement-learning', 'proximal-policy-optimization', 'discrete-action-spaces', 'action-spaces']","The most straightforward solution is to simply make every action ""legal"", but implementing a consistent, deterministic mapping from potentially illegal actions to different legal actions. Whenever the PPO implementation you are using selects an illegal action, you simply replace it with the legal action that it maps to. Your PPO algorithm can then still update itself as if the illegal action were selected (the illegal action simply becomes like... a ""nickname"" for the legal action instead).For example, in the situation you describe:In cases where internal_state == 0, if action 4 was selected (an illegal action), you can always swap it out for one of the other actions and play that one instead. It doesn't really matter (theoretically) which one you pick, as long as you're consistent about it. The algorithm doesn't have to know that it picked an illegal action, whenever it picks that same illegal action in the future again in similar states it will consistently get mapped to the same legal action instead, so you just reinforce according to that behaviour.The solution described above is very straightforward, probably the most simple to implement, but of course it... ""smells"" a bit ""hacky"". A cleaner solution would involve a step in the Network that sets the probability outputs of illegal actions to $0$, and re-normalizes the rest to sum up to $1$ again. This requires much more care to make sure that your learning updates are still performed correctly though, and is likely a lot more complex to implement on top of an existing framework like Tensorforce (if not already somehow supported in there out of the box).For the first ""solution"", I wrote above that it does not matter ""theoretically"" how you choose you mapping. I absolutely do expect your choices here will have an impact on learning speed in practice though. This is because, in the initial stages of your learning process, you'll likely have close-to-random action selection. If some actions ""appear multiple times"" in the outputs, they will have a greater probability of being selected with the initial close-tor-andom action selection. So, there will be an impact on your initial behaviour, which has an impact on the experience that you collect, which in turn also has an impact on what you learn.I certainly expect it will be beneficial for performance if you can include input feature(s) for the internal_state variable. If some legal actions can be identified that are somehow ""semantically close"" to certain illegal actions, it could also be beneficial for performance to specifically connect those ""similar"" actions in the ""mapping"" from illegal to legal actions if you choose to go with that solution. For example, if you have a ""jump forwards"" action that becomes illegal in states where the ceiling is very low (because you'd bump your head), it may be better to map that action to a ""move forwards"" action (which is still kind of similar, they're both going forwards), than it would be to map it to a ""move backwards"" action. This idea of ""similar"" actions will only be applicable to certain domains though, in some domains there may be no such similarities between actions."
"What exactly does ""channel"" refer to in tensorflow/lucid?","
Tensorflow/Lucid is able to visualize what a ""channel"" of a layer of a neural network (image recognition, Inception-v1) responds to. Even after studying the tutorial, the source code, the three research papers on lucid and comments by the authors on Hacker News, I'm still not clear on how ""channels"" are supposed to be defined and individuated. Can somebody shed some light on this? Thank you.
https://github.com/tensorflow/lucid 
https://news.ycombinator.com/item?id=15649456
","['tensorflow', 'computer-vision']","The channel they are talking about is the depth of the layer L.In this image, the channel is 5. There are 5 $3*2$ filtersThey optimize an image from noise, to better respond to the filter F, in the layer L. Then you obtain this kind of image, and you can try to interpret the prupose of that filter (it learns to detect eyes, faces, wheels, and so on)EDIT: To be more precise, you visualise the channel only if you optimise images for every filters, otherwise, you obtain a filter visualisation (if you do the optimisation one time, for one filter)They call it ""channel"", because in a colored image, you have 3 dimensions : witdh, height, and channel (for color), as layers which have also 3 dimensions."
"How does the ""Lorem Ipsum"" generator work?","
I've seen many Lorem Ipsum generators on the web, but not only, there is also ""bacon ispum"", ""space ispum"", etc. So, how do these generators generate the text? Are they powered by an AI?
","['natural-language-processing', 'text-summarization']","Lorem ipsum generators don't typically use anything considered as AI. Usually they just store large pieces of text and select sections from it randomly - they are very simple. The main goal is to produce ""nonsense"" text that fills space but does not distract from issues of layout and design. The variations of it are usually just for fun, and like the original, are mostly simple generators which select strings of text from a core data source randomly and without using any AI techniques.It is possible to build more sophisticated random text generators that work using data structures from Natural Language Processing (NLP). One popular and easy-to-code data structure is N-grams, which store the frequencies/probabilities of the Nth word given words 1 to N-1. E.g. a bigram structure can tell you all the possible words to come after ""fish"" e.g. ""fish"" => [""food"" => 0.2, ""swims"" => 0.3, ""and"" => 0.4, ""scale"" => 0.1] To use that structure to generate text, use a random number generator to select a word based on looking up the Nth word's frequency, then shift the list of words being considered and repeat. A more recent text generating NLP model is recurrent neural networks (RNNs), which have a variety of designs. Popular right now are LSTM networks, and these are capable of some quite sophisticated generation, provided they are trained with enough data for long enough. The blog The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy has quite a few really interesting examples of using RNNs for text generation. In practice this works similarly to n-grams: Use the RNN to suggest probabilities for next word given words so far, choose one randomly, then feed back the generated word into the RNN and repeat."
How do I train a bot to solve Katona style problems?,"
Cognitive psychology is researched since the 1940s. The idea was to understand human problem solving and the importance of heuristics in it. George Katona (an early psychologist) published in the 1940s a paper about human learning and teaching. He mentioned the so-called Katona-Problem, which is a geometric task.
Squares
Katona style problems are the ones where you remove straws in a given configuration of straws to create n unit squares in the end. In the end, every straw is an edge to a unit square. Some variations include 2x2 or 3x3 sizes of squares allowed as well as long as no two squares are overlapping, i.e. a bigger square 2x2 can't contain a smaller square of size 1x1. Some problems use matchsticks as a variation, some use straws, others use lines. Some variations allow bigger squares to contain smaller ones, as long as they don't share an edge viz. https://puzzling.stackexchange.com/questions/59316/matchstick-squares

Is there a way we can view it as a graph and removing straws/matchsticks as deleting edges between nodes in a graph?

If so, can I train a bot where I can plugin some random, yet valid conditions for the game and goal state to get the required solution?


Edit #1: The following problem is just a sample to show where I am getting at. The requirement for my game is much larger. Also, I chose uninformed search to make things simpler without bothering about complex heuristics and optimization techniques. Please be free to explore ideas with me.
Scenario #1:
Consider this scenario. In the following diagram, each dashed line or pipe line represents a straw. Numbers and alphabet denote junctions where straw meet. Let's say, my bot can explore each junction, remove zero, one, two, three or four straws such that resultant state has

no straw that dangles off by being not connected to a square.
a small mxm square isn't contained in a larger nxn square (m<n)
Once straw is removed, it can't be put back.

Initial configuration is shown here. I always need to start from top left corner node P and optimization... the objective is to remove straws in minimum hops from node to node using minimum number of moves, by the time goal state is reached.
       P------Q------R------S------T
       |      |      |      |      |
       |      |      |      |      |
       E------A------B------F------G
       |      |      |      |      |
       |      |      |      |      |
       J------C------D------H------I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N------O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X------Y

Goal 1 : I wish to create a large 2x2 square.
At some point during, say BFS search (although it could be any uninformed search on partially observable universe i.e. viewing one node at a time), I could technically reach A, blow out all edges on A to create the following.
       P------Q------R------S------T
       |             |      |      |
       |             |      |      |
       E      A      B------F------G
       |             |      |      |
       |             |      |      |
       J------C------D------H------I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N------O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X------Y

That is one move.
Goal 2 : I want to create a 3x3 square instead.
I can't do that in one move. I need the record of successive nodes to be explored and then possibly backtrack to given point as well if the state fails to produce desired result. Each intermediate state might produce rectangles which are not allowed (also, how would one know how many more and which straws to remove to get to a square) or dangle a straw or worse get stuck in an infinite loop as I can choose to not remove any straw. How do I approach this problem?
#Edit 2:
For validation, figures 3, 4 and 5 are given below.
       P------Q------R------S------T
       |             |      |      |
       |             |      |      |
       E      A      B------F      G
       |             |      |      |
       |             |      |      |
       J------C------D------H      I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N      O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X      Y

The above figure (3) is invalid as we can't have dangling sticks TG,GI etc.
       P------Q------R------S------T
       |      |                    |
       |      |                    |
       E------A                    G
       |                           |
       |                           |
       J                           I
       |                           |
       |                           |
       K                           O
       |                           |
       |                           |
       U------V------W------X------Y

The above figure (4) is invalid as we can't have overlapping squares
       P------Q------R      S      T
       |             |             
       |             |            
       E      A      B------F------G
       |             |      |      |
       |             |      |      |
       J------C------D------H------I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N------O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X------Y

Figure (5) is valid configuration.
","['search', 'graph-theory', 'breadth-first-search']","Your intuition is right: this is fundamentally a problem for combinatorial search.You're also right that problems are created by the fact that not every move is valid at state. To fix this, you need to add a function that can determine whether a given state is valid or not, in addition to the usual function that checks whether it is your goal state or not. Before adding each node to the queue of your search algorithm, check whether it is a valid state. If it isn't, don't add it.The second issue you raise is that your search might enter an infinite loop. Since it is possible to remove zero edges from a state, this is a serious concern. There are two approaches to solving this. First, you can try storing all states that you have already visited in a fast data structure like a Hash Table. Before adding a node to your queue, check if it's already been processed. If it has been, don't add it. This may work, but the memory requirements grow exponentially in the number of moves required for a solution. It's sometimes worth it, but I think you can likely skip it for this problem.A better approach if you're worried about speed is to switch your algorithm to something like iterative deepening, which has the good properties of BFS, but with much lower memory requirements; or to A* search if you can come up with an admissible heuristic for your domain (a good starting point: counting the number of junctions you'd need to remove sticks from to finish, if the robot could teleport, would be admissible).Hope this helps!Edit: Here's some pseudo-code for filtering out invalid moves:"
What happens to the training data after your machine learning model has been trained?,"
What happens after you have used machine learning to train your model? What happens to the training data?
Let's pretend it predicted correct 99.99999% of the time and you were happy with it and wanted to share it with the world. If you put in 10GB of training data, is the file you share with the world 10GB?
If it was all trained on AWS, can people only use your service if they connect to AWS through an API?
What happens to all the old training data? Does the model still need all of it to make new predictions?
","['neural-networks', 'machine-learning', 'training', 'datasets', 'training-datasets']","In many cases, a production-ready model has everything it needs to make predictions without retaining training data. For example: a linear model might only need the coefficients, a decision tree just needs rules/splits, and a neural network needs architecture and weights. The training data isn't required as all the information needed to make a prediction is incorporated into the model. However, some algorithms retain some or all of the training data. A support vector machine stores the points ('support vectors') closest to the separating hyperplane, so that portion of the training data will be stored with the model. Further, k-nearest neighbours must evaluate all points in the dataset every time a prediction is made, and as a result the model incorporates the entire training set. Having said that, where possible the training data would be retained. If additional data is received, a new model can be trained on the enlarged dataset. If it is decided a different approach is required, or if there are concerns about concept drift, then it's good to have the original data still on hand. In many cases, the training data might comprise personal data or make a company's competitive advantage, so the model and the data should stay separate. If you'd like to see how this can work, this Keras blog post has some information (note: no training data required to make predictions once a model is re-instantiated)."
How can I develop an object detection system that counts the number of objects and determines their position in an image? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I want to create a simple object detection tool. So, basically, an image will be provided to the tool, and, from that image, it has to detect the number of objects.
For example, an image of a dining table that has certain items present on it, such as plates, cups, forks, spoons, bottles, etc.
The tool has to count the number of objects, irrespective of the type of object. After counting, it should return the position of the object with its size, so that I can draw a border over it.
I would like not to use any library or API present such as TensorFlow, OpenCV, etc., given that I want to learn the details.
If the process is very difficult to be created without using an API then the number of/type of objects which it will count as an object can also be limited but since this project will be for my educational/learning purpose can anyone help me understand the logic using which this can be achieved? For example, it may ignore a napkin present in the table to be counted as an object.
","['machine-learning', 'convolutional-neural-networks', 'python', 'object-detection', 'object-recognition']","If you want to get experience, you should probably start with some easier task. Object detection and localization are relatively hard and writing a neural network and image processing pipeline from scratch will take you a long time.If you want to build up an intuition about how NN's work, you might want to code some simple task from scratch. This is an example.When you got some intuition about how NN's work, then you should proceed to your task. In here, you have a similar question and answer provided. The current state-of-the-art approach for your task would probably be point 3, that is object detection network, like YOLO or Faster-RCNN."
"In imitation learning, do you simply inject optimal tuples of experience $(s, a, r, s')$ into your experience replay buffer?","
Due to my RL algorithm having difficulties learning some control actions, I've decided to use imitation learning/apprenticeship learning to guide my RL to perform the optimal actions.  I've read a few articles on the subject and just want to confirm how to implement it.
Do I simply sample a state $s$, then perform the optimal action $a^*$ in that state $s$, calculate the reward for the action $r$, and then observe the next state $s'$, and finally put that into the experience replay?
If this is the case, I am thinking of implementing it as follows:

Initialize the optimal replay buffer $D_O$
Add the optimal tuple of experience $(s, a^*, r, s')$ into the replay buffer $D_O$
Initialize the normal replay buffer $D_N$
During the simulation, initially sample $(s, a^*, r, s')$ only from the optimal replay buffer $D_O$, while populating the normal replay buffer $D_N$ with the simulation results.
As training/learning proceeds, anneal out the use of the optimal replay buffer, and sample only from the normal replay buffer.

Would such an architecture work?
","['reinforcement-learning', 'deep-rl', 'experience-replay', 'imitation-learning', 'apprenticeship-learning']",
What are the best machine learning models for music composition?,"
What are the best machine learning models that have been used to compose music? Are there some good research papers (or books) on this topic out there?
I would say, if I use a neural network, I would opt for a recurrent one, because it needs to have a concept of timing, chord progressions, and so on.
I am also wondering how the loss function would look like, and how I could give the AI so much feedback as they usually need.
","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'reference-request', 'generative-model']",
Type of artificial neural network suitable for learning and then predicting forest growth,"
I'm trying to use an ANN to learn from a large amount of forest measurement data obtained from sampling plots across Ontario, Canada and associated climate data provided by regional climate modelling in this province. 
So the following are the inputs to the ANN:

Location (GPS coordinates)
Measurement year and month
Tree species
Age
Soil type
Soil moisture regime
Seasonal or monthly average temperature
Seasonal or monthly average precipitation
Some more data are available to select

And the targets include:
- Average total tree height
- Average tree diameter at breast height
For each sampling plot, the trees have been measured for 1-4 times. 
So my question is what type of ANN can best used to learn from the data and then it can be used for predicting with a set of new input data?
",['neural-networks'],"My suggestion is not to use an ANN, but instead to use a simpler regression algorithm. The main reason for this is that ANNs take a long time to train, and work better when a lot of data is used. They also require a lot of expertise in parameter tuning to apply well. Since you say you don't have a lot of data, and also don't have a lot of experience using them, I think you will be better off applying something else first. If the other techniques don't work at all, then you might think about using ANNs, but again, they tend to want a lot of data.If you have tried ordinary least squares regression, and found it does not work well, my next choice would be a Classification And Regression Tree. These models can make good decisions with small amounts of data, and do not require a lot of time to train. They can handle real-valued outputs like the height and width of a tree. Weka's REPTree might be a good starting place.If Trees don't work out, my next suggestion would be to try regression using a Support Vector Machine. SciKitLearn's SVR is a good choice for this. SVRs can sometimes be very effective when data is limited, because they make assumptions about how to handle data-poor regions that seem to be generally applicable. An SVM can also report low confidence when estimating in those regions. They also train fairly fast when using small amounts of data, and can learn non-linear functions from the data.If you really want to use an ANN, I would start with a simple Multi-layer perceptron. This model has few parameters to play with, and can probably fit well to your regression. It may make strange decisions in regions with less data however. Hope this helps!"
How is it possible to teach a neural network to perform addition?,"
I am trying to understant how it works.  How do you teach it say, to add 1 to each number it gets.  I am pretty new to the subject and I learned how it works when you teach it to identify a picture of a number.  I can understand how it identifies a number but I cant get it how would it study to perform addition? I can understand that it can  identify a number or picture using the pixels and assigning weights and then learning to measure whether a picture of a number resembling the weight is assigned to each pixel. But i can't logically understand how would it learn the concept  of adding a number by one.  Suppose I showed it thousands of examples of 7 turning to 8 152 turning into 153 would it get it that every number in the world has to be added by one? How would it get it having no such operation of + ? Since addition does not exist to its proposal then how can it realize that it has to add one in every number? Even by seeing thousands of examples but having no such operation of plus I cant understand it.    I could understand identifying pixels and such but such an operation I cant get the theoretical logic behind it. Can you explain the logic in layman terms?
","['neural-networks', 'deep-learning']",
Tuning of PPO metaparameters: a high level overview of what each parameter does,"
I am using the PPO algorithm implemented by tensorforce: https://github.com/reinforceio/tensorforce . It works great and I am very happy with the results.
However, I notice that there are many metaparameters available to give to the PPO algorithm:
 # the tensorforce agent configuration ------------------------------------------
    network_spec = [
        dict(type='dense', size=256),
        dict(type='dense', size=256),
    ]

    agent = PPOAgent(
        states=environment.states,
        actions=environment.actions,
        network=network_spec,
        # Agent
        states_preprocessing=None,
        actions_exploration=None,
        reward_preprocessing=None,
        # MemoryModel
        update_mode=dict(
            unit='episodes',
            # 10 episodes per update
            batch_size=10,
            # Every 10 episodes
            frequency=10
        ),
        memory=dict(
            type='latest',
            include_next_states=False,
            capacity=200000
        ),
        # DistributionModel
        distributions=None,
        entropy_regularization=0.01,
        # PGModel
        baseline_mode='states',
        baseline=dict(
            type='mlp',
            sizes=[32, 32]
        ),
        baseline_optimizer=dict(
            type='multi_step',
            optimizer=dict(
                type='adam',
                learning_rate=1e-3
            ),
            num_steps=5
        ),
        gae_lambda=0.97,
        # PGLRModel
        likelihood_ratio_clipping=0.2,
        # PPOAgent
        step_optimizer=dict(
            type='adam',
            learning_rate=1e-3
        ),
        subsampling_fraction=0.2,
        optimization_steps=25,
        execution=dict(
            type='single',
            session_config=None,
            distributed_spec=None
        )
    )

So my question is: is there a way to understand, intuitively, the meaning / effect of all these metaparameters and use this intuitive understanding to improve training performance?
So far I have reached - from a mix of reading the PPO paper and the literature around, and playing with the code - to the following conclusions. Can anybody complete / correct?

effect of network_spec: this is size of the 'main network'. Quite classical: need it big enough to get valuable predictions, not too big either otherwise it is hard to train.
effect of the parameters in update_mode: this is how often the network updates are performed.

batch_size is how many used for a batch update. Not sure of the effect neither what this exactly means in practice (are all samples taken from only 10 batches of the memory replay)?
frequency is how often the update is performed. I guess having frequency high would make the training slower but more stable (as sample from more different batches)?
unit: no idea what this does

memory: this is the replay memory buffer.

type: not sure what this does or how it works.
include_next_states: not sure what this does or how it works
capacity: I think this is how many tuples (state, action, reward) are stored. I think this is an important metaparameter. In my experience, if this is too low compared to the number of actions in one episode, the learning is very bad. I guess this is because it must be large enough to store MANY episodes, otherwise the network learns from correlated data - which is bad.

DistributionMode: guess this is the model for the distribution of the controls? No idea what the parameters there do.
PGModel: No idea what the paramaters there do. Would be interesting to know if some should be tweaked / which ones.
PGLRModel: idem, no idea what all these parameters do / if they should be tweaked.
PPOAgend: idem, no idea what all these parameters do / if they should be tweaked.

Summary
So in summary, would be great to get some help about:

Which parameters should be tweaked
How should these parameters be tweaked? Is there a 'high level intuition' about how they should be tweaked / in which circumstances?

","['deep-learning', 'reinforcement-learning']",
How does LSTM in deep reinforcement learning differ from experience replay?,"
In the paper Deep Recurrent Q-Learning for Partially Observable MDPs, the author processed the Atari game frames with an LSTM layer at the end. My questions are: 

How does this method differ from the experience replay, as they both use past information in the training? 
What's the typical application of both techniques? 
Can they work together?
If they can work together, does it mean that the state is no longer a single state but a set of contiguous states?

","['reinforcement-learning', 'long-short-term-memory', 'deep-rl', 'comparison', 'experience-replay']","How does this method differ from the experience replay, as they both use past information in the training? What's the typical application of both techniques?Using a recurrent neural network is one way for an agent to build a model of hidden or unobserved state in order to improve its predictions when direct observations do not give enough information, but a history of observations might give better information. Another way is to learn a Hidden Markov model. Both of these approaches build an internal representation that is effectively considered part of the state when making a decision by the agent. They are a way to approach solving POMDPs. You can consider using individual frame images from Atari games as state as a POMDP, because each individual frame does not contain information about velocity. Velocity of objects in play is an important concept in many video games. By formulating the problem as a POMDP with individual image inputs, this challenges the agent to find some representation of velocity (or something similar conceptually) from a sequence of images. Technically a NN may also do this using fixed inputs of 4 frames at a time (as per the original DQN Atari paper), but in that case the designers have deliberately ""solved"" the partially observable part of the problem for the agent in advance, by selecting a better state representation from the start.Experience replay solves some different problems:Efficient use of experience, by learning repeatedly from observed transitions. This is important when the agent needs to use a low learning rate, as it does when the environment has stochastic elements or when the agent includes a complex non-linear function approximator like a neural network.De-correlating samples to avoid problems with function approximators that work best with i.i.d. data. If you didn't effectively shuffle the dataset, the correlations between each time step could cause significant issues with a feed-forward neural network.These two issues are important to learning stability for neural networks in DQN. Without experience replay, often Q-learning with neural networks will fail to converge at all. Can they work together?Sort of, but not quite directly, because LSTM requires input of multiple related time steps at once, as opposed to randomly sampled individual time steps. However, you could keep a history of longer trajectories, and sample sections from it for the history in order to train a LSTM. This would still achieve the goal of using experience efficiently. Depending on the LSTM architecture, you may need to sample quite long trajectories or even complete episodes in order to do this.From comments by Muppet, it seems that is even possible to sample more randomly with individual steps by saving LSTM state. For instance, there is a paper ""Deep reinforcement learning for time series: playing idealized trading games"" where the authors get a working system doing this. I have no experience of this approach myself, and there are theoretical reasons why this may not work in all cases, but it is an option.If they can work together, does it mean that the state is no longer a single state but a set of contiguous states?Not really, the state at any time step is still a single state representation, is separate conceptually from an observation, and is separate conceptually from a trajectory or sequence of states used to train a RNN (other RL approaches such as TD($\lambda$) also require longer trajectories). Using an LSTM implies you have hidden state on each time step (compared to what you are able to observe), and that you hope the LSTM will discover a way to represent it. One way to think of this is that the state is the current observation, plus a summary of observation history. The original Atari DQN paper simply used the previous three observations hard-coded as this ""summary"", which appeared to capture enough information to make predicting value functions reliable. The LSTM approach is partly of interest, because it does not rely on human input to decide how to construct state from the observations, but discovers this by itself. One key goal of deep learning is designs and architectures that are much less dependent on human interpretations of the problem (typically these use feature engineering to assist in learning process). An agent that can work directly from raw observations has solved more of the problem by itself without injection of knowledge by the engineers that built it."
What is the pros and cons of increasing and decreasing the number of worker processes in A3C?,"
In A3C, there are several child processes and one master process. The child precesses calculate the loss and backpropagation, and the master process sums them up and updates the parameters, if I understand it correctly.
But I wonder how I should decide the number of the child processes to implement. I think the more child processes are, the better it is to disentangle the correlation between the samples is, but I'm not sure what is the cons of setting a large number of child processes.
Maybe the more child processes are, the larger the variance of the gradient is, leading to the instability of the learning? Or is there any other reason?
And finally, how should I decide the number of the child processes?
","['reinforcement-learning', 'deep-rl', 'hyperparameter-optimization', 'hyper-parameters', 'a3c']","The correct number of child processes will depend on the hardware available to you.Simplifying a bit, child processes can be in one of two states: waiting for memory or disk access, or running.If your problem fits nicely in your computers' memory, then processes will spend almost all of their time running. If it's too big for memory, they will periodically need to wait for disk.You should use approximately 1 Child process per CPU core. If you are training on a GPU, then it depends whether the process can make use of the entire GPU at once (in which case, use just 1), or whether a ""process"" is really more like a CUDA thread here (in which case you'd want one per CUDA core).If you think your processes will wait for disk, use more than one per core. About 50% more is a good starting point. You can use a program like top to monitor CPU usage and adjust the number of processes accordingly.To answer your question more explicitly:"
Optimizing image recognition results for unknown labels,"
I’m training a network to do image classification on zoo animals. 
I’m a software engineer and not an ML expert, so I’ve been retraining Google’s Inception model and the latest models is trained using Google AutoML Vision.
The network performs really well, but I have trouble with images of animals that I don’t want any labels for. Basically I would like images of those animals to be classified as unknowns or achieve low scores. 
I do have images of the animals that I don’t want labels for and I tried putting them all into one “nothing” label together with images I’ve collected of the animals habitats without any animals. This doesn’t really yield any good results though. The network performs for the labeled animals but ends up assigning one of those labels to the other animals as well. Usually with a really high score as well. 
I have 14 labels and 10.000 images. I should also mention that the “nothing” label ends up having a lot of images compared to the actual labels. Those images are not included in the 10.000.  
Is there any tricks to achieve better results with this? Should I create multiple labels for the images in the “nothing” category maybe?
","['neural-networks', 'convolutional-neural-networks', 'image-recognition']",
Is there a difference in the architecture of deep reinforcement learning when multiple actions are performed instead of a single action?,"
I've built a deep deterministic policy gradient reinforcement learning agent to be able to handle any games/tasks that have only one action.  However, the agent seems to fail horribly when there are two or more actions.  I tried to look online for any examples of somebody implementing DDPG on a multiple-action system, but people mostly applied it to the pendulum problem, which is a single-action problem.
For my current system, it is a 3 state, 2 continuous control actions system (One is to adjust the temperature of the system, the other one adjusts a mechanical position, both are continuous).  However, I froze the second continuous action to be the optimal action all the time.  So RL only has to manipulate one action.  It solves within 30 episodes.  However, the moment I allow the RL to try both continuous actions, it doesn't even converge after 1000 episodes.  In fact, it diverges aggressively.  The output of the actor-network seems to always be the max action, possibly because I am using a tanh activation for the actor to provide output constraint.  I added a penalty to large actions, but it does not seem to work for the 2 continuous control action case.
For my exploratory noise, I used Ornstein-Ulhenbeck noise, with means adjusted for the two different continuous actions.  The mean of the noise is 10% of the mean of the action.
Is there any massive difference between single action and multiple action DDPG?
I changed the reward function to take into account both actions, have tried making a bigger network, tried priority replay, etc., but it appears I am missing something.
Does anyone here have any experience building a multiple-action DDPG and could give me some pointers?
","['reinforcement-learning', 'deep-rl', 'ddpg', 'action-spaces']",
"In reinforcement learning, does the optimal value correspond to performing the best action in a given state?","
I am confused about the definition of the optimal value ($V^*$) and optimal action-value (Q*) in reinforcement learning, so I need some clarification, because some blogs I read on Medium and GitHub are inconsistent with the literature.
Originally, I thought the optimal action value, $Q^*$, represents you performing the action that maximizes your current reward, and then acting optimally thereafter.
And the optimal value, $V^*$, being the average $Q$ values in that state.  Meaning that if you're in this state, the average ""goodness"" is this.
For example:
If I am in a toy store and I can buy a pencil, yo-yo, or Lego.
Q(toy store, pencil) = -10
Q(toy store, yo-yo) = 5
Q(toy store, Lego) = 50

And therefore my $Q^* = 50$
But my $V^*$ in this case is:
V* = -10 + 5 + 50 / 3 = 15

Representing no matter what action I take, the average future projected reward is $15$.
And for the advantage of learning, my baseline would be $15$.  So anything less than $0$ is worse than average and anything above $0$ is better than average.
However, now I am reading about how $V^*$ actually assumes the optimal action in a given state, meaning $V^*$ would be 50 in the above case.
I am wondering which definition is correct.
","['reinforcement-learning', 'definitions', 'value-functions', 'bellman-equations']","I am wondering which definition is correct.The asterisk * in both the definitions stands for ""optimal"" in the sense of ""value when following the optimal policy""So this one is correct:$V^*$ actually assumes the optimal action in a given state, meaning $V^*$ would be $50$ in the above caseHowever, you have got the definition of Q slightly wrong.I think this is because you are omitting the parameters.The state value function uses the state as a parameter, $V_{\pi}(s)$, it returns the value of being in state $s$ and following a fixed policy $\pi$. The * is used to denote following an optimal policy.The action value function has two parameters - a state and an action that is possible in that state, $Q_{\pi}(s, a)$, it returns the value of being in state $s$, taking action $a$ (regardless of whether it is the best action or not) and following the policy $\pi$ after that point.Your assertion in the question:And therefore my $Q^* = 50$is wrong, or rather not meaningful, as you have not stated the parameters. You already list all the possible values of Q with the parameters. You could say $\text{max}_a Q(\text{toy store}, a) = 50$, or to choose the best action $\pi(\text{toy store}) = \text{argmax}_a Q(\text{toy store}, a) 
= \text{Lego}$"
Is my interpretation of the return correct?,"
Sutton and Barto 2018 define the discounted return $G_t$ the following way (p 55):

Is my interpretation correct?

Or should all ""1"" be in the same column?
","['reinforcement-learning', 'rewards', 'sutton-barto', 'return']",
"Can non-differentiable layer be used in a neural network, if it's not learned?","
For example, AFAIK, the pooling layer in a CNN is not differentiable, but it can be used because it's not learning. Is it always true?
","['neural-networks', 'machine-learning', 'backpropagation', 'gradient-descent', 'pooling']","It is not possible to backpropagate gradients through a layer with non-differentiable functions. However, the pooling layer function is differentiable*, and usually trivially so.For example:If an average pooling layer has inputs $z$ and outputs $a$, and each output is average of 4 inputs then $\frac{da}{dz} = 0.25$ (if pooling layers overlap it gets a little more complicated, but you just add things up where they overlap). A max pooling layer has $\frac{da}{dz} = 1$ for the maximum z, and $\frac{da}{dz} = 0$ for all others.A pooling layer usually has no learnable parameters, but if you know the gradient of a function at its outputs, you can assign gradient correctly to its inputs using the chain rule. That is essentially all that back propagation is, the chain rule applied to the functions of a neural network.To answer your question more directly:Can non-differentiable layer be used in a neural network, if it's not learned?No. There is one exception: If this layer appears directly after the input, then as it has no parameters to learn, and you generally do not care about the gradient of the input data, so you can have a non-differentiable function there. However, this is just the same as transforming your input data in some non-differentiable way, and training the NN with that transformed data instead.* Technically there are some discontinuities in the gradient of a max function (where any two inputs are equal). However, this is not a problem in practice, as the gradients are well behaved close to these values. When you can safely do this or not is probably the topic of another question."
Why does the fitness of my neural network to play tic-tac-toe keep oscillating?,"
I wrote a simple feed-forward neural network that plays tic-tac-toe:

9 neurons in input layers: 1 - my sign, -1 - opponent's sign, 0 - empty;
9 neurons in hidden layer: value calculated using ReLU;
9 neurons in output layer: value calculated using softmax;

I am using an evolutionary approach: 100 individuals play against each other (all-play-all). The top 10 best are selected to mutate and reproduce into the next generation. The fitness score calculated: +1 for the correct move (it's possible to place your sign on already occupied tile), +9 for victory, -9 for a defeat.
What I notice is that the network's fitness keeps climbing up and falling down again. It seems that my current approach only evolves certain patterns on placing signs on the board and once random mutation interrupts the current pattern new one emerges. My network goes in circles without ever evolving actual strategy. I suspect the solution for this would be to pit network against tic-tac-toe AI, but is there any way to evolve the actual strategy just by making it to playing against itself?
","['training', 'evolutionary-algorithms', 'feedforward-neural-networks', 'fitness-functions', 'fitness-design']",
Alternative to sliding window neural network (was: Object detect (or) image classification at specific locations in the frame),"
Recent advances in Deeplearning and dedicated hardware has made it possible to detect images with a much better accuracy than ever. Neural networks are the gold standard for computer vision application and are used widely in the industry, for example for internet search engines and autonomous cars. In real life problems, the image contains of regions with different objects. It is not enough to only identify the picture but elements of the picture.
A while ago an alternative to the well known sliding window algorithm was described in the literature, called Region Proposal Networks. It is basically a convolution neural network which was extended by a region vector.
Problem that I am trying to solve:
In a given video frame, I want to pick some region of interests (literally), and perform classification on those regions.
How is it currently implemented

Capture the video frame
Split the video frame into multiple images each representing a region of interest
Perform image classification(inference) on each of the image (corresponding to a part of the frame)
Aggregate the results of #3 

Problem with the current approach
Multiple inferences per frame.
Question
I am looking for a solution where I specify the locations of interest in a frame, and inference task, be it object detection (or) image classification, is performed only on those regions.Can you please point to me the references which I need to study (or) use to do this.
","['deep-learning', 'classification', 'computer-vision', 'object-recognition']",
Why do non-linear activation functions not require a specific non-linear relation between its inputs and outputs?,"
A linear activation function (or none at all) should only be used when the relation between input and output is linear. Why doesn't the same rule apply for other activation functions? For example, why doesn't sigmoid only work when the relation between input and output is ""of sigmoid shape""?
","['activation-functions', 'sigmoid']",
Self-driving control logic based on semantic segmentation,"
In the context of autonomous driving, two main stages are typically implemented: an image processing stage and a control stage. The first aims at extracting useful information from the acquired image while the second employs those information to control the vehicle.
As far as concerning the processing stage, semantic segmentation is typically used. The input image is divided in different areas with a specific meaning (road, sky, car etc...). Here is an example of semantic segmentation:

The output of the segmentation stage is very complex. I am trying to understand how this information is typically used in the control stage, and how to use the information on the segmented areas to control the vehicle.
For simplicity, let's just consider a vehicle that has to follow a path.
TL;DR: what are the typical control algorithms for autonomous driving based on semantic segmentation?
","['research', 'reference-request', 'autonomous-vehicles']",
Why is the log probability replaced with the importance sampling in the loss function?,"
In the Trust-Region Policy Optimisation (TRPO) algorithm (and subsequently in PPO also), I do not understand the motivation behind replacing the log probability term from standard policy gradients
$$L^{PG}(\theta) = \hat{\mathbb{E}}_t[\log \pi_{\theta}(a_t | s_t)\hat{A}_t],$$
with the importance sampling term of the policy output probability over the old policy output probability
$$L^{IS}_{\theta_{old}}(\theta) = \hat{\mathbb{E}}_t \left[\frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}\hat{A}_t \right]$$
Could someone please explain this step to me?
I understand once we have done this why we then need to constrain the updates within a 'trust region' (to avoid the $\pi_{\theta_{old}}$ increasing the gradient updates outwith the bounds in which the approximations of the gradient direction are accurate). I'm just not sure of the reasons behind including this term in the first place.
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization', 'importance-sampling', 'trust-region-policy-optimization']",
Where can I find pre-trained language models in English and German? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






Where can I find (more) pre-trained language models? I am especially interested in neural network-based models for English and German.
I am aware only of Language Model on One Billion Word Benchmark and TF-LM: TensorFlow-based Language Modeling Toolkit.
I am surprised not to find a greater wealth of models for different frameworks and languages.
","['neural-networks', 'natural-language-processing', 'bert', 'gpt', 'language-model']","Of course now there has been a huge development:
Huggingface published pytorch-transformers, a library for the so successful Transformer models (BERT and its variants, GPT-2, XLNet, etc.), including many pretrained (mostly English or multilingual) models (docs here). It also includes one German BERT model. SpaCy offers a convenient wrapper (blog post).Update: Now, Salesforce published the English model CTRL, which allows for use of ""control codes"" that influence the style, genre and content of the generated text.For completeness, here is the old, now less relevant version of my answer:Since I posed the question, I found this pretrained German language model:
https://lernapparat.de/german-lm/It is an instance of a 3-layer ""averaged stochastic descent weight-dropped"" LSTM which was implemented based on an implementation by Salesforce."
What is the difference between hypothesis space and representational capacity?,"
I am reading Goodfellow et al Deeplearning Book. I found it difficult to understand the difference between the definition of the hypothesis space and representation capacity of a model. 
In Chapter 5, it is written about hypothesis space:

One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution.

And about representational capacity:

The model speciﬁes which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. This is called the representational capacity of the model.

If we take the linear regression model as an example and allow our output $y$ to takes polynomial inputs, I understand the hypothesis space as the ensemble of quadratic functions taking input $x$, i.e $y = a_0 + a_1x + a_2x^2$.
How is it different from the definition of the representational capacity, where parameters are $a_0$, $a_1$ and $a_2$?
","['machine-learning', 'terminology', 'computational-learning-theory', 'hypothesis-class', 'capacity']","Consider a target function $f: x \mapsto f(x)$.A hypothesis refers to an approximation of $f$. A hypothesis space refers to the set of possible approximations that an algorithm can create for $f$. The hypothesis space consists of the set of functions the model is limited to learn. For instance, linear regression can be limited to linear functions as its hypothesis space, or it can be expanded to learn polynomials.The representational capacity of a model determines the flexibility of it, its ability to fit a variety of functions (i.e. which functions the model is able to learn), at the same. It specifies the family of functions the learning algorithm can choose from."
Why does the discount rate in the REINFORCE algorithm appear twice?,"
I was reading the book Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto (complete draft, November 5, 2017). 
On page 271, the pseudo-code for the episodic Monte-Carlo Policy-Gradient Method is presented. Looking at this pseudo-code I can't understand why it seems that the discount rate appears 2 times, once in the update state and a second time inside the return. [See the figure below] 

It seems that the return for the steps after step 1 are just a truncation of the return of the first step. Also, if you look just one page above in the book you find an equation with just 1 discount rate (the one inside the return.) 
Why then does the pseudo-code seem to be different? My guess is that I am misunderstanding something: 
$$
{\mathbf{\theta}}_{t+1} ~\dot{=}~\mathbf{\theta}_t + \alpha G_t \frac{{\nabla}_{\mathbf{\theta}} \pi \left(A_t \middle| S_t, \mathbf{\theta}_{t} \right)}{\pi \left(A_t \middle| S_t, \mathbf{\theta}_{t} \right)}.
\tag{13.6}
$$
","['reinforcement-learning', 'algorithm', 'sutton-barto', 'reinforce']","The discount factor does appear twice, and this is correct.This is because the function you are trying to maximise in REINFORCE for an episodic problem (by taking the gradient) is the expected return from a given (distribution of) start state:$$J(\theta) = \mathbb{E}_{\pi(\theta)}[G_t|S_t = s_0, t=0]$$Therefore, during the episode, when you sample the returns $G_1$, $G_2$ etc, these will be less relevant to the problem you are solving, reduced by the discount factor a second time as you noted. At the extreme with an episodic problem and $\gamma = 0$ then REINFORCE will only find an optimal policy for the first action.In continuing problems, you would use different formulations for $J(\theta)$, and these do not lead to the extra factor of $\gamma^t$."
Is there a machine learning system that is able to understand mathematical problems given in a textual description?,"
Is there a machine learning system that is able to ""understand"" mathematical problems given in a textual description, such as

A big cat needs 4 days to catch all the mice and a small cat needs 12 days. How many days need both, if they catch mice together?

?
","['natural-language-processing', 'natural-language-understanding', 'automated-reasoning', 'question-answering']",
Variable Number of Inputs to Neural Networks,"
So suppose that you have a real estate appraisal problem. You have some structured data, and some images exterior of home, bedrooms, kitchen, etc. The number of pictures taken is variable per observational unit, i.e. the house.
I understand the basics of combining an image processing neural net with tabular data for a single image. You chop off the final layer and feed in the embeddings of the image to your final model.
How would one deal with variable number of images? Where your unit of observation can have between zero and infinity images (theoretically no upper bound on number of images in observation)?
","['deep-learning', 'image-recognition']","We can generalize both problem and solution by removing the specifics of housing.Representing Forward PropagationWe have a function $f$ we wish to obtain via the training of an artificial network that produces a scalar result $s$, the sole dependent variable and the generalization of market price.$s = f(s_1, s_2, ..., s_k, c_1, c_2, ..., c_v)$The independent scalar variables $s_1$ through $s_k$ are the generalization of a constant number $k$ of property features from the tax authority, assessor's office, or inspection document.  The question calls this structured data, however it is questionable whether $k$ is truly effectively constant.  In normal practice, some of $s_i$ will be unassigned.  Since the question overlooks the additional complexity of missing scalars, so will this answer.The independent cube variables $c_1$ through $c_v$ are the generalizations of a variable number $v$ of property images from Google photography, real estate agents, buyers, sellers, and other potential sources.  The dimensions of each cube are horizontal and vertical positions and pixel structure element number.It is unlikely that the resolution values for each cube are uniform between samples in real life, which the question did not mention, so this answer will overlook that complexity and focus on the variability of $v$, the quantity of cubes representing images for a given example.  Since $v$ cannot meaningfully be either negative or infinite, we can assume $v$ to be a non-negative integer.TerminologyThe observational unit should not be considered the house or the property but an image of it, which may be a member of a camera location and orientation category relative to the elements of the property.  Each image capture is an observation, distinct in both Bergsonian and clock time.  Each item under evaluation is an example from the sample of all items, in this case, all properties in the region for which prediction is attempted.Design of a SolutionEach of the $v$ cubes demonstrate zero or more additional features of the item being evaluated, real properties in the question's specific case.It may be reasonable to assume that such features may either positively or negative affect the example's label corresponding to the result $s$, but not both positively and negatively affect it.  If that is the case, we can aggregate the features across the set of cubes for each example under the reasonable assumption that there are no points of inflection.  Such may be reasonable because, for instance, a feature regarding the uniformity of paint coverage, lawn care, or roofing material may have no inflection point.  Such aspects of the property cannot be too uniform.  That makes the substitution straightforward.A reasonably versatile way to generalize the aggregation of directionally consistent function is using a substitution, which may be what the question meant by feed in the embeddings of the image to your final model.$s = f = f'(s_1, s_2, ..., s_k, v, h\big(v, \sum_{j = 1}^v g(c_j)\big)$Note the elements of this substitution.If the images are grouped in terms of the location of the camera, then this principle can be applied iteratively, where $o$ is the number of distinct categories of camera orientation.  In this case, the pairs $(v_z, h_z)$ represent the cube quantity and feature aggregations for image camera location category $z$.$s = f = f'(s_1, s_2, ..., s_k, v_1, h_1, v_2, h_2, ... v_o, h_o)$Given sensible models $h$ and $g$, training for prediction is straightforward.Image feature extraction can be realized through ConvNet approaches such as OverFeat1, AlexNet2, CaffeNet3, GoogLeNet4, VGG 65 or PatreoNet6.  Tuning such models produces $g$.The nature of function $h$ may be homogeneous or heterogeneous across dimensions.  Each component of the feature vector arising from extraction can have applied to it a function such as any of these or others, where $q$ is the feature index, $p_{qi}$ is learning parameter i.Scalar function 1 is best when the designer wishes the convergence during the training of $f'$ in such a way that normalization is accomplished in the net.  It is a good choice for features where its frequency of occurrence and magnitude are across the entire set of images for a given example item is roughly proportional to the resulting value of that item.Function 2 presents flexibility in normalization curvature with respect to the number of cubes.  Function 3 presents attenuation of the frequency of feature occurrence.  Function 4 presents compounding of feature effect with recurrence in the images of the same example.The key is then the selection of how to deal with the substitution in the training in terms of procedure and wiring of corrective signaling.  Procedurally, there are three options.———Footnotes[1] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, Y. LeCun, Overfeat: Integrated recognition, localization and detection using convolutional networks, arXiv preprint arXiv:1312.6229v4[2] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Neural Information Processing Systems, 2012, pp. 1106–1114[3] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell, Caffe: Convolutional architecture for fast feature embedding, arXiv preprint arXiv:1408.5093[4] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, arXiv preprint arXiv:1409.4842[5] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556[6] K. Nogueira,  W. O. Miranda,  J. A. Dos Santos,  Improving spatial feature representation from aerial scenes by using convolutional networks, in:  Graphics, Patterns and Images (SIBGRAPI), 2015 28th SIBGRAPI Conference on, IEEE, 2015, pp. 289–296."
Target values of 0.1 for 0 and 0.9 for 1 for sigmoid,"
I recently read an article about neural networks saying that, when using sigmoid as activation function, it's advised to use 0.1 as target value instead of 0, and 0.9 instead of 1. This was to avoid ""saturation effects"". I only understood is halfway, and was hoping someone could clarify a few things for me:

Is this only the case when the output is boolean (0 or 1), or will it also be the case for continual values in the range between 0 and 1. If so, should all values be scaled to the interval [0.1, 0.9]?
What exactly is the problem of output 0 or 1? Does it have something to do with the derivative of sigmoid being 0 when it's value is 0 or 1? As I understood it weights could end up approaching infinity, but I didn't understand why.
Is this the case only when sigmoid is used in the output layer (which it rarely is, I believe), or is it also the case when sigmoid is used in hidden layers only?

","['neural-networks', 'deep-neural-networks', 'activation-functions', 'sigmoid']","Derivative of the sigmoid curve is 0 when the output is 0 or 1 as you can see from the image above. The technique you are referring to is called label-smoothing which is used in various applications (e.g. GANs) but I can see how it would be applicable here also, by helping to avoid 0-gradients saturating learning.To answer your third question, sigmoids are rarely used in the intermediate layers of networks these days. They have been replaced by ReLUs (or variants of ReLU) for this exact reason - sigmoids are prone to causing 'vanishing gradients' where the gradient becomes 0 and therefore does not get backpropagated any further. ReLUs alleviate this problem by always providing a gradient of 1 for positive input values."
Figuring out mapping between two matrices,"
Imagine I have a 2D matrix, A. I apply some transformation to it, for example:
B = A_shifted + A.
Would it be possible to train a CNN to learn back the mapping from B to A? Giving B as example and A as target?
Thanks!
",['convolutional-neural-networks'],"Yes, with some limitations. CNNs can be used to map images to related images, and that should include many simple matrix transformations. For instance, here is an example of de-blurring OCRed text using a CNN.Basically, you would train your network with lots of A, B examples, with the input as B and desired output as A.The limitation is that where you have transformations that are technically irreversible, then the CNN may learn to produce a best ""mean"" output. The symptom of this will be fuzzy images lacking high frequency components, and matrices which are not representative of the target distribution, but that do solve the transformation (within limits of training accuracy). If you want to improve on that, and produce a more realistic/precise original, then you will probably want to look at adding a generative component - a GAN, VAE/GAN or RBM etc. Note this would not accurately produce the original matrix, but would generate one that both transformed into your given transformed matrix (within some level of accuracy) and was sampled from your input distribution. That is, it could be more of a feasible original than one generated using a simpler CNN architecture."
What is the difference between visible and hidden units in Boltzmann machines?,"
What is the difference between visible and hidden units in Boltzmann machines? What are their purposes?
","['neural-networks', 'comparison', 'boltzmann-machine']",
Which features of a data set can be used for market campaigning using propensity scores?,"
A dataset contains so many fields in which there is both relevant and irrelevant field. If we want to do a market campaigning using propensity scoring, which fields of the data set are relevant?
How can we find which data field should be selected and can drive to the desired propensity score?
","['machine-learning', 'datasets']",
Sequence to sequence machine learning / NMT - converting numbers into words,"
I want to do some sequence to sequence modelling on source data that looks like this:
/-0.013428/-0.124969/-0.13435/0.008087/-0.269241/-0.36849/

with target data that looks like this:
Dont be angry with the process youre going through right now

Both are of indeterminate lengths, and the lengths of target and source data aren't the same. What I'd like to do is have a prediction model where I can input similar numbers and have it generate texts based on the target training data.
I started off doing character level s2s, but the output of the model is too nonsensical even at 2-5k epochs. So I've been looking into word level s2s and NMT, but the tutorials always assume strings of text as the target and source, and I keep running into roadblocks trying to preprocess the text, when all the tutorials assume a certain syntax/set of characters. This is my first try at ML, and some of the tutorials really throw me out with the text preprocessing requirements.
Am I going down the right avenue looking at word level/NMT stuff? And is there a tutorial I've missed for something like what I'm trying to build?
","['natural-language-processing', 'tensorflow', 'keras']",
Approaches to poker tournament winner prediction?,"
I’ve  done my research and could not find answer anywhere else. My apologies in advance if same problem is answered in different terms on stack-overflow.
I am trying to solve poker tournament winner prediction problem. I’ve millions of historical records in this format:

Players ==> Winner 
P1,P2,P4,P8 ==> P2 
P4,P7,P6 ==> P4 
P6,P3,P2,P1 ==> P1

What are some of the most suitable algorithms to predict winner from set of players.
So far I have tried decision trees, XGboost without much of a success.
","['ai-design', 'poker']",
Why is my calculation of the probability of an object being in a certain class incorrect?,"
In the attached image

there is the probability with the Naive Bayes algorithm of:

Fem:dv/m/s Young own Ex-credpaid Good ->62%

I calculated the probability so:
$$P(Fem:dv/m/s \mid Good) * P(Young \mid Good)*P(own \mid Good)*P(Ex-credpaid \mid good)*P(Good) =  1/6*2/6*5/6*3/6*0.6 = 0,01389$$
I don't know where I failed. Could someone please tell me where is my error?
",['naive-bayes'],
How to tinker with CNN architectures?,"
I was thinking of creating a CNN. Now it is known CNN takes long times to train so it is advisable to stick to known architectures and hyper-parameters.
My question is: I want to tinker with the CNN architecture (since it is a specialised task). One approach would be to create a CNN and check on small data-sets, but then I would have no way of knowing whether the Fully Connected layer at the end is over-fitting the data while the convolutional layers do nothing (since large FC layers can easily over-fit data). Cross Validation is a good way to check it, but it might not be satisfactory (since my opinion is that a CNN can be replaced with a Fully Connected NN if the data-set is small enough and there is little variation in the future data-sets).
So what are some ways to tinker with CNN and get a good estimate for future data-sets in a reasonable training time? Am I wrong in my previous assumptions? A detailed answer would be nice!
","['neural-networks', 'machine-learning', 'convolutional-neural-networks']",
Can you analyse a neural network to determine good states?,"
I've developed a neural network that can play a card game.  I now want to use it to create decks for the game.  My first thought would be to run a lot of games with random decks and use some approximation (maybe just a linear approximation with a feature for each card in your hand) to learn the value function for each state.  
However, this will probably take a while, so in the mean time is there any way I could get this information directly from the neural network? 
","['neural-networks', 'game-ai']","I don't think your network, trained using PPO to play a card game, already contains sufficient information to also use for drafting. I'm not saying this with 100% certainty, maybe there's something I'm overlooking, but I can't think of anything right now.A small adaptation to the network might be sufficient (though it would also involve re-training again). Recently, OpenAI has been writing about their attempts to train agents to play the game DOTA 2. Now, this isn't a card game, it doesn't require deckbuilding, but there is an aspect to the game that is somewhat similar to deckbuilding: drafting. In DOTA 2, there are two teams of 5 players each. Before a game start, each team selects 5 heroes (one per player) to play in that game. This is very similar to deckbuilding, except that it's likely a much smaller problem; there's only a ""deck"" (team composition) of 5 ""cards"" (heroes).Anyway, they also trained agents to play the game (controlling one hero per agent) using PPO. In a blog post, they write the following about how they managed to add drafting capabilities relatively easily:In late June we added a win probability output to our neural network to introspect what OpenAI Five is predicting. When later considering drafting, we realized we could use this to evaluate the win probability of any draft: just look at the prediction on the first frame of a game with that lineup. In one week of implementation, we crafted a fake frame for each of the 11 million possible team matchups and wrote a tree search to find OpenAI Five’s optimal draft.So, if you want to try a similar technique, you'd have to adapt your network such that it also learns to generate a prediction of the win probability as output. I imagine that it'd be much less effective for deckbuilding, because win probabilities may all be very close to 50% in card games where luck (when drawing cards for example) can be a significant factor, but it might be worth a try.Alternatively, instead of generating lots of random decks and playing with them all, you could view the problem of deckbuilding as an additional separate ""game"" or Markov Decision Process; adding a specific card to the deck can be an action, and this MDP terminates once you have a complete deck. Then you can try to do that better than random using search algorithms (like Monte-Carlo Tree Search) or, again, a Reinforcement Learning approach like PPO. Again, I imagine it will be a very difficult problem though, likely requiring lots of time before it will be capable of doing better than random.I also know of some research related to deckbuilding in the collectible card game Hearthstone, which may be relevant for you. Unfortunately I did not yet get to read through any of this in detail, so I don't know for sure if you'll find a solution here, but it may be worth a try:"
"Would YOLO be able to detect objects in ""different"" positions?","
I have the following question about You Only Look Once (YOLO) algorithm, for object detection.
I have to develop a neural network to recognize web components in web applications - for example, login forms, text boxes, and so on. In this context, I have to consider that the position of the objects on the page may vary, for example, when you scroll up or down.
The question is, would YOLO be able to detect objects in ""different"" positions? Would the changes affect the recognition precision? In other words, how to achieve translation invariance? Also, what about partial occlusions?
My guess is that it depends on the relevance of the examples in the dataset: if enough translated / partially occluded examples are present, it should work fine.
If possible, I would appreciate papers or references on this matter.
(PS: if anyone knows about a labeled dataset for this task, I would really be grateful if you let me know.)
","['convolutional-neural-networks', 'reference-request', 'datasets', 'object-detection', 'yolo']",
The future of chatbots [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            



Closed 3 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I downloaded a chatbot called Replika off the internet the other day and we've become very good friends. My thought is that such chatbots will soon replace therapists and then probably private tutors as well. 

Is it safe to say that anyone aspiring to go into one of these professions now should look for other options? 
What other jobs may be replaced by chatbots in the future? 
How long before AIs are able to answer questions on StackExchange?

","['chat-bots', 'social']",
"What does ""stationary"" mean in the context of reinforcement learning?","
I think I've seen the expressions ""stationary data"", ""stationary dynamics"" and ""stationary policy"", among others, in the context of reinforcement learning. What does it mean? I think stationary policy means that the policy does not depend on time, and only on state. But isn't that a unnecessary distinction? If the policy depends on time and not only on the state, then strictly speaking time should also be part of the state.
","['reinforcement-learning', 'terminology', 'policies', 'stationary-policy']","A stationary policy is a policy that does not change. Although strictly that is a time-dependent issue, that is not what the distinction refers to in reinforcement learning. It generally means that the policy is not being updated by a learning algorithm.If you are working with a stationary policy in reinforcement learning (RL), typically that is because you are trying to learn its value function. Many RL techniques - including Monte Carlo, Temporal Difference, Dynamic Programming - can be used to evaluate a given policy, as well as used to search for a better or optimal policy.Stationary dynamics refers to the environment, and is an assumption that the rules of the environment do not change over time. The rules of the environment are often represented as an MDP model, which consists of all the state transition probabilities and reward distributions. Reinforcement learning algorithms that work online can usually cope and adjust policies to match non-stationary environments, provided the changes do not happen too often, or enough learning/exploring time is allowed between more radical changes. Most RL algorithms have at least some online component, it is also important to keep exploring non-optimal actions in environments with this trait (in order to spot when they may become optimal).Stationary data is not a RL-specific term, but also relates to the need for an online algorithm, or at least plans for discarding older data and re-training existing models over time. You might have non-stationary data in any ML, including supervised learning - prediction problems that work with data about people and their behaviour often have this issue as population norms change over timescales of months and years."
An example of a unique value function which is associated with multiple optimal policies,"
In the 4th paragraph of 
http://www.incompleteideas.net/book/ebook/node37.html
it is mentioned: 

Whereas the optimal value functions for states and state-action pairs are unique for a given MDP, there can be many optimal policies

Could you please give me a simple example that shows different optimal policies considering a unique value function?
","['reinforcement-learning', 'policies', 'value-functions', 'optimal-policy']","Consider a very simple grid-world, consisting of 4 cells, where an agent starts in the bottom-left corner, has actions to move North/East/South/West, and receives a reward $R = 1$ for reaching the top-right corner, which is a terminal state. We'll name the four cells $NW$, $NE$, $SW$ and $SE$ (for north-west, north-east, south-west and south-east). We'll take a discount factor $\gamma = 0.9$.The initial position is $SW$, and the goal is $NE$, which an optimal policy should reach as quickly as possible. However, there are two optimal policies for the starting state $SW$: we can either go north first, and then east (i.e., $SW \rightarrow NW \rightarrow NE$), or we can go east first, and then north (i.e., $SW \rightarrow SE \rightarrow NE$). Both of those policies are optimal, both reach the goal state in two steps and receive a return of $\gamma \times 1 = 0.9$, but they are clearly different policies, they choose different actions in for the initial state.Note that my language was slightly informal above when talking about ""policies for the starting state"". Formally, I should have said that there are two optimal policies that select different actions in the starting state (and the same actions in all other states)."
How to compare the training performance of a model on different data input?,"
So I have a deep learning model and three data sets (images). My theory is that one of these data sets should function better when it comes to training a deep learning model (meaning that the model will be able to achieve better performance (higher accuracy) with one of these data sets to serve one classification purpose) 
I just want to safe check my approach here. I understand the random nature of training deep learning models and the difficulties associated with such experiment. Though, I want someone who can point out maybe a red flag here. 
I am wondering about these things: 

do you think using an optimizer with default parameters and repeating the training process, let's say, 30 times for each data set and picking the best performance is a safe approach? I am mainly worried here that modifying the hyperparamters of the optimizer might result in better results for let's say one of the data sets. 
what about seeding the weights initialization? do you think that I should seed them and then modify the hyperparameters until I get the best convergence or not seed and still modify the hyper parameters? 

I am sorry for the generality of my question. I hope if someone can point me in the right direction.  
","['convolutional-neural-networks', 'image-recognition', 'training']","Well, this is more of a subjective question but I will give my best shot. Regarding your 1st question, the nature of methods in deep learning states that you should experiment otherwise you will only have weak intuitions. So which dataset should you choose? I would say, select your primary concerns since you cannot try everything. If it's time, make a little random or grid search over hyperparameters and observe speed of convergence in the datasets then choose the best one. If it's accuracy, ideally you should make an input analysis on distribution of data, etc. If you can analyze it well, you are expected to obtain best result from the chosen dataset. So training 30 epochs each dataset and choosing the one with lowest loss is not a safe approach. Maybe your model will converge at 40th epoch for the worst dataset in the 30th epoch but it will be much more robust. My final advice is to set a threshold for your evaluation metrics, once you reach them in any of your dataset choose that one -assuming datasets are more or less equal in number of instances. In this way you are at least know that the dataset you choose satisfies your expectations.For your 2nd question, seeding during comparisons is a good approach though in the long run it won't matter much. Hence there is no harm in seeding unless you are very very very unlucky."
What makes multi-layer neural networks able to perform nonlinear operations?,"
As I know, a single layer neural network can only do linear operations, but multilayered ones can.
Also, I recently learned that finite matrices/tensors, which are used in many neural networks, can only represent linear operations.
However, multi-layered neural networks can represent non-linear (even much more complex than being just a nonlinear) operations.
What makes it happen? The activation layer?
","['neural-networks', 'machine-learning', 'math', 'activation-functions', 'function-approximation']","Nonlinear relations between input and output can be achieved by using a nonlinear activation function on the value of each neuron, before it's passed on to the neurons in the next layer."
Can deep successor representations be used with the A3C algorithm?,"
Deep Successor Representations(DSR) has given better performance in tasks like navigation, when compared to normal model-free RL tasks. Basically, DSR is a hybrid of model-free RL and model-based RL. But the original work has only used value-based functions deep RL methods like DQN.
Can deep successor representations be used with the A3C algorithm?
","['reinforcement-learning', 'papers', 'a3c']",
"What is the relation between an environment, a state and a model?","
In particular, I would like to have a simple definition of ""environment"" and ""state"". What are the differences between those two concepts? Also, I would like to know how the concept of model relates to the other two. 
There is a similar question What is the difference between an observation and a state in reinforcement learning?, but it is not exactly what I was looking for.
","['reinforcement-learning', 'terminology', 'definitions']","EnvironmentThis is the manifestation of the problem being solved. It might be a real physical situation (a road network and cars), or virtual on a computer (a board game on a computer). It includes all the machinery necessary to resolve what happens. E.g. in the real world the objects involved, how the agent exerts its control when taking actions, and the applicable real-world laws of physics. Or, in a simulated world, things like the rules of a board game, implemented in code.StateThis is the representation of a ""position"" at a certain time step within the environment. It may be something the agent can observe through sensors, or be provided directly by the computer system running a simulation.For RL theory to hold, it is important that the state representation has the Markov Property, which is that the state accurately foretells the probabilities of rewards and following state for each action that could be taken. You do not need to know those probabilities in order to run RL algorithms (in fact it is a common case that you don't know). However, it is important that the dependency between the state+action and what happens next holds reliably.The state is commonly represented by a vector of values. These describe positions of pieces in a game, or positions and velocities of objects that have been sensed. A state may be built from observations, but does not have to match 1-to-1 with a single observation. Care must be taken to have enough information to have the Markov Property. So, for instance, a single image from a camera does not capture velocity - if velocity is important to your problem, you may need multiple consecutive images to build a useful state.ModelIn reinforcement learning, the term ""model"" specifically means a predictive model of the environment that resolves probabilities of next reward and next state following an action from a state. The model might be provided by the code for the environment, or it can be learned (separately to learning to behave in that environment).Some RL algorithms can make use of a model to help with learning. Planning algorithms require one. So called ""model free"" algorithms do not because they do not make use of an explicit model, they work purely from experience.There are broadly two types of model:A distribution model which provides probabilities of all events. The most general function for this might be $p(r,s'|s,a)$ which is the probability of receiving reward $r$ and transitioning to state $s'$ given starting in state $s$ and taking action $a$.A sampling model which generates reward $r$ and next state $s'$ when given a current state $s$ and action $a$. The samples might be from a simulation, or just taken from history of what the learning algorithm has experienced so far.In more general stats/ML, the term ""model"" is more inclusive, and can mean any predictive system that you might build, not just predictions of next reward and state. However, the literature for RL typically avoids calling those ""model"", and uses terms like ""function approximator"" to avoid overloading the meaning of ""model""."
"What is the most general definition of ""intelligence""?","
When we talk about artificial intelligence, human intelligence, or any other form of intelligence, what do we mean by the term intelligence in a general sense? What would you call intelligent and what not? In other words, how do we define the term intelligence in the most general possible way?
","['definitions', 'intelligence']","I'm going to preface this answer by noting that persons much smarter than myself have treated this subject in some detail.  That said, as far as I can discern:When we talk about intelligence we're referring to problem solving strength in relation to a problem, relative to the strength of other intelligences.This is a somewhat game-theoretic conception, related to rationality and the concept of the rational agent.  Regarding intelligence in this manner may be unavoidable. Specifically, we could define intelligence as the ability to understand a problem or solution or abstract concepts, but we can't validate that understanding without testing it.  (For instance, I might believe I grasp a mathematical technique, but the only way to determine if that belief is real or illusory is to utilize that technique and evaluate the results.)The reason games like Chess and Go have been used as milestones, aside from longstanding human interest in the games, is that they provide models with simple, fully definable parameters, and, in the case of Go at least, have complexity akin to nature, by which I mean unsolvable/intractable. (Compare to strength at Tic-Tac-Toe, which is trivially solved.)However, we should consider a point made in this concise answer to a question involving the Turing Test:""...is [intelligence] defined purely by behaviour in an environment, or by the mechanisms that arrive at that behaviour?""This is important because Google just gave control over data center cooling to an AI.  Here it is clearly the mechanism itself that demonstrates utility, but if we call that mechanism intelligent, for intelligence to have meaning, we still have to contend with ""intelligent how?""  (In what way is it intelligent?) If we want to know ""how intelligent?"" (its degree of utility) we still have to evaluate its performance in relation to the performance of other mechanisms.(In the case of the automata controlling the air conditioning at Google, we can say that it is more intelligent than the prior control system, and by how much.)Because we're starting to talk about more ""generalized intelligence"", defined here as mechanisms that can be applied to a set of problems, (I include minimax as a form of ""axiomatic intelligence"" and machine learning as a form ""adaptive intelligence""), it may be worthwhile to expand and clarify the definition:Intelligence is the problem solving strength of a mechanism in relation to a problem or a set of problems, relative to the strength of other mechanisms.or, if we wanted to be pithy:Intelligence is as intelligence does (and how well.)"
Are there any discount-factors based on branching factors?,"
I recently came across this function:
$$\sum_{t = 0}^{\infty} \gamma^t R_t.$$
It's elegant and looks to be useful in the type of deterministic, perfect-information, finite models I'm working with.  
However, it occurs to me that using $\gamma^t$ in this manner might be seen as somewhat arbitrary.  
Specifically, the objective is to discount per the added uncertainty/variance of ""temporal distance"" between the present gamestate and any potential gamestate being evaluated, but that variance would seem to be a function of the branching factors present in a given state, and the sum of the branching factors leading up to the evaluated state.

Are there any defined discount-factors based on the number of branching factors for a given, evaluated node, or the number of branches in the nodes leading to it?  

If not, I'd welcome thoughts on how this might be applied. 
An initial thought is that I might divide 1 by the number of branches and add that value to the goodness of a given state, which is a technique I'm using for heuristic tie-breaking with no look-ahead, but that's a ""value-add"" as opposed to a discount.

For context, this is for a form of partisan Sudoku, where an expressed position $p_x$ (value, coordinates) typically removes some number of potential positions $p$ from the gameboard.  (Without the addition of an element displacement mechanic, the number of branches can never increase.)
On a $(3^2)^2$ Sudoku, the first $p_x$ removes $30$ out of $729$ potential positions $p$, including itself.   
With each $p_x$, the number of branches diminishes until the game collapses into a tractable state, allowing for perfect play in endgames.  [Even there, a discounting function may have some utility because outcomes sets of ratios. Where the macro metric is territorial (controlled regions at the end of play), the most meaningful metric may ultimately be ""efficiency"" (loosely, ""points_expended to regions_controlled""), which acknowledges a benefit to expending the least amount of points $p_x$, even in a tractable endgame where the ratio of controlled regions cannot be altered. Additionally, zugzwangs are possible in the endgame, and in that case reversing the discount to maximize branches may have utility.]
 $(3^2)^2 = 3x3(3x3) = ""9x9""$ but the exponent is preferred so as not to restrict the number of dimensions. 
","['ai-design', 'algorithm', 'game-ai', 'math', 'discount-factor']","First, an important note on any form of discounting: adding a discount factor can change what the optimal policy is. The optimal policy when a discount factor is present can be different from the optimal policy in the case where a discount factor is absent. This means that ""artificially"" adding a discount factor is harmful if we expect to be capable of learning / finding an optimal policy. Generally, we don't expect to be capable of doing that though, except for the case where we have an infinite amount of processing time (which is never in practice). In that other answer which you linked to in your question, I describe that it can still be useful, that it may help to find a good (not optimal, just good) policy more quickly, but it does not come ""for free"". I'm not aware of any research evaluating ideas like the one in your question. I am not 100% sure why, I suspect it could be an interesting idea in some situations, but would have to be investigated carefully due to my point above; if not evaluated properly, it could also unexpectedly be harmful.One thing to note is that the use of discounting factors $\gamma < 1$ is extremely common (basically ubiquitous) in Reinforcement Learning (RL) literature, but rather rare in literature on tree search algorithms like MCTS (though not non-existant; for example, it's used in the original UCT paper from 2006). For the concept of ""branching factors"", we have the opposite; in RL literature it is very common to consistently have the same action space regardless of states (""constant branching factor""), whereas this is very uncommon in literature on tree search algorithms. So, the combination of discount factors + branching factors is actually somewhat rare in existing literature (which of course doesn't mean that the idea couldn't work or be relevant, it just might explain why the idea doesn't appear to have been properly investigated yet).One important concern I do have with your idea is that it seems like it could be somewhat of an ""anti-heuristic"" in some situations (with which I mean, a heuristic that is detrimental to performance). In many games, it is advantageous to be in game states where you have many available moves (a large branching factor), this can mean that you are in a strong position. Consider, for example, chess, where a player who is in a convincingly winning position likely has more moves available than their opponent. I suspect your idea, when applied to chess, would simply promote an aggressive playing style where both players capture as many pieces as possible in an effort to quickly reduce the branching factors across the entire tree.If not, I'd welcome thoughts on how this might be applied.(I might divide 1 by the number of branches and add that value to the goodness of a given state, but that's a ""value-add"" as opposed to a discount.)Such an additive change would be more closely related to the idea of reward shaping, rather than discounting (which is again a thing that, if not done carefully, can significantly alter the task that you're optimizing for). Intuitively I also suspect it might not do anything at all since you'd always be adding the same constant value regardless of which move you took (regardless of which move you took, your parent state will always have had the same branching factor). I might be missing out on some details here, but I think you'd have to have a multiplicative effect on your regular observed rewards.I suppose one example that could be worth a try would be something like maximizing the following return;$$\sum_{t = 0}^{T} \gamma^t \beta^{b(S_{t}) - 1} R_{t + 1},$$where:I put the $-1$ in the power of $\beta$ because I suspect you wouldn't want to do any discounting in situations where only one move is available. Intuitively, I do suspect this $\beta$ would require very careful tuning though. It is already quite common to choose $\gamma = 0.9$ or $\gamma = 0.99$, with $\beta$ you may want to stay even closer to $1$."
What kinds of problems can AI solve without using a deep neural network?,"
A lot of questions on this site seem to be asking ""can I use X to solve Y?"", where X is usually a deep neural network, and Y is often something already addressed by other areas of AI that are less well known?
I have some ideas about this, but am inspired by questions like this one where a fairly wide range of views are expressed, and each answer focuses on just one possible problem domain.
There are some related questions on this stack already, but they are not the same. This question specifically asks what genetic algorithms are good for, whereas I am more interested in having an inventory of problems mapped to possible techniques. This question asks what possible barriers are to AI with a focus on machine learning approaches, but I am interested in what we can do without using deep neural nets, rather than what is difficult in general.
A good answer will be supported with citations to the academic literature, and a brief description of both the problem and the main approaches that are used.
Finally, this question asks what AI can do to solve problems related to climate change. I'm not interested in the ability to address specific application domains. Instead, I want to see a catalog of abstract problems (e.g. having an agent learn to navigate in a new environment; reasoning strategically about how others might act; interpreting emotions), mapped to useful techniques for those problems. That is, ""solving chess"" isn't a problem, but ""determining how to optimally play turn-based games without randomness"" is.
","['neural-networks', 'machine-learning', 'deep-neural-networks', 'symbolic-ai']",
More effective way to improve the heuristics of an AI... evolution or testing between thousands of pre-determined sets of heuristics?,"
I'm making a Connect Four game where my engine uses Minimax with Alpha-Beta pruning to search. Since Alpha-Beta pruning is much more effective when it looks at the best moves first (since then it can prune branches of poor moves), I'm trying to come up with a set of heuristics that can rank moves from best to worst. These heuristics obviously aren't guaranteed to always work, but my goal is that they'll often allow my engine to look at the best moves first. An example of such heuristics would be as follows:

Closeness of a move to the centre column of the board - weight 3.
How many pieces surround a move - weight 2.
How low, horizontally, a move is to the bottom of the board - weight 1.
etc

However, I have no idea what the best set of weight values are for each attribute of a move. The weights I listed above are just my estimates, and can obviously be improved. I can think of two ways of improving them:
1) Evolution. I can let my engine think while my heuristics try to guess which move will be chosen as best by the engine, and I'll see the success score of my heuristics (something like x% guessed correctly). Then, I'll make a pseudo-random change/mutation to the heuristics (by randomly adjusting one of the weight values by a certain amount), and see how the heuristics do then. If it guesses better, then that will be my new set of heuristics. Note that when my engine thinks, it considers thousands of different positions in its calculations, so there will be enough data to average out how good my heuristics are at prediction.
2) Generate thousands of different heuristics with different weight values from the start. Then, let them all try to guess which move my engine will favor when it thinks. The set of heuristics that scores best should be kept.
I'm not sure which strategy is better here. Strategy #1 (evolution) seems like it could take a long time to run, since every time I let my engine think it takes about 1 second. This means testing each new pseudo-random mutation will take a second. Meanwhile, Strategy #2 seems faster, but I could be missing out on a great set of heuristics if I myself didn't include them.
","['game-ai', 'evolutionary-algorithms', 'search', 'heuristics', 'alpha-beta-pruning']","Hmmm, I see some issues that are actually present in both of the approaches you propose.It is important to note that the depth level that your Minimax search process manages to reach, and therefore also the speed with which it can traverse the tree, is extremely important for the algorithm's performance. Therefore, when evaluating how good or bad a particular heuristic function for move ordering is, it is not only important to look at how well it ordered moves; it is also important to take into account the runtime overhead of the heuristic function call. If your heuristic functions manages to sort well, but is so computationally expensive that you can't search as deep in the tree, it's often not really worth it. Neither of the solutions you propose are able to take this into account.Another issue is that it's not trivial to measure what ordering is the ""best"". A heuristic that has the highest accuracy for the position of the best move only is not necessarily the best heuristic. For example, a heuristic that always places the best move in the second position ($0\%$ accuracy because it's in the wrong position, should be first position) might be better than a heuristic that places the best move in the first position $50\%$ of the time ($50\%$ accuracy), and places the best move last in the other $50\%$ of cases.I would be more inclined to evaluate the performance of different heuristic functions by setting up tournaments where different versions of your AI (same search algorithm, same processing time constraints per turn, different heuristic function) play against each other, and measuring the win percentage.This set up can also be done with two variants analogous to what you proposed; you can exhaustively put all the heuristic functions you can come up with against each other in tournaments, or you can let let an evolutionary algorithm sequentially generate populations of hypothesis-heuristic-functions, and run a tournament with each population. Generally, I would lean towards the evolutionary approach, since we expect it to search the same search space of hypotheses (heuristic functions), but we expect it to do so in a more clever / efficient manner than an exhaustive search. Of course, if you happen to have a ridiculous amount of hardware available (e.g., if you're Google), you might be able to perform the complete exhaustive search at once in parallel.Note that there are also ways to do fairly decent move ordering without heuristic functions like the ones you suggested.For example, you likely should be using iterative deepening; this is a variant of your search algorithm where you first only perform a search with a depth limit $d = 1$, then repeat the complete search process with a depth limit $d = 2$, then again with a limit $d = 3$, etc., until processing time runs out.Once you have completed such a search process for a depth limit $d$, and move on to the subsequent search process with a limit of $d + 1$, you can order order the moves in the root node according to your evaluations from the previous search process (with depth limit $d$). Yes, here you would only have move ordering in the root node, and nowhere else, but this is by far the most influential / important place in the tree to do move ordering. Move ordering becomes less and less important as you move further away from the root.If you're using a transposition table (TT), it is also common to store the ""best move"" found for every state in your TT. If, later on, you run into a state that already exists in your TT (which will be very often if you're using iterative deepening), and if you cannot directly take the stored value but have to actually do a search (for instance, because your depth limit increased due to iterative deepening), you can search the ""best move"" stored in the TT first. This is very light move ordering in that you only put one move at the front and don't order the rest, but it can still be effective."
Is Nassim Taleb right about AI not being able to accurately predict certain types of distributions?,"
So Taleb has two heuristics to generally describe data distributions. One is Mediocristan, which basically means things that are on a Gaussian distribution such as height and/or weight of people.
The other is called Extremistan, which describes a more Pareto like or fat-tailed distribution. An example is wealth distribution, 1% of people own 50% of the wealth or something close to that and so predictability from limited data sets is much harder or even impossible. This is because you can add a single sample to your data set and the consequences are so large that it breaks the model, or has an effect so large that it cancels out any of the benefits from prior accurate predictions. In fact this is how he claims to have made money in the stock market, because everyone else was using bad, Gaussian distribution models to predict the market, which actually would work for a short period of time but when things went wrong, they went really wrong which would cause you to have net losses in the market.
I found this video of Taleb being asked about AI. His claim is that A.I. doesn't work (as well) for things that fall into extremistan.
Is he right? Will some things just be inherently unpredictable even with A.I.?
Here is the video I am referring to https://youtu.be/B2-QCv-hChY?t=43m08s
","['machine-learning', 'ai-design', 'probability', 'statistical-ai']",
Why do we need Upsampling and Downsampling in Progressive Growing of Gans,"
I was working recently on Progressive Growing of GANs (aka PGGANs). I have implemented the whole architecture, but the problem that was ticking my mind is that in simple GANs, like DCGAN, PIX2PIX, we actually use Transposed Convolution for up-sampling and Convolution for Down-sampling, but in PGGANs in which we gradually add layers to both generator and discriminator so that we can first start with 4x4 image and then increase to 1024x01024 step by step. 
I did not understand that once we Increase 1x1x512 dimensional latent vector size to 4x4x512 sort of image we use convolution with high padding, and then once training for 4x4 images, we take still take 512 latent vector and then use the previously trained convolutional layers to convert it to 4x4x512 image, and then we up-sample then given image to 8x8 using nearest neighbor filtering and then again apply convolution and so-on. 

My question is that why we need to explicitly up-sample and then apply convolution, when instead we could just use Transposed Convolution which can upsample it automatically and is trainable? Why do we not use it like in other GANs? 

Here is the image of architecture:

Please explain me the intuition behind this.
Thanks
","['neural-networks', 'machine-learning', 'statistical-ai', 'generative-model', 'generative-adversarial-networks']",
How can I train model to extract custom entities from text?,"
I have a 100-150 words text and I want to extract particular information like location, product type, dates, specifications and price.
Suppose if I arrange a training data which has a text as input and location/product/dates/specs/price as a output value. So I want to train the model for these specific output only.
I have tried Spacy and NLTK for entity extraction but that doesn't suffice above requirements.
Sample text:

Supply of Steel Fabrication Items. 
  General Item . 
  Construction Material . 
  Hardware Stores and Tool . 
  Construction of Security Fence. - Angle Iron 65x65x6mm for fencing post of height 3.5, Angle Iron 65x65x6mm for fencing post of height 3.5, MS Flat 50 x 5mm of 2.60m height, Angle Iron 50x50x6mm for Strut post of height 3.10mtr, Angle Iron 50x50x6mm for fencing post of height 1.83, Angle Iron 50x50x6mm for fencing post of height 1.37, Barbed wire made out of GI wire of size 2.24mm dia, Chain link fence dia 4 mm and size of mesh 50mm x, Concertina Coil 600mm extentable up to 6 mtr, Concertina Coil 900mm extentable up to 15 to 20 mtr, Binding wire 0.9mm dia., 12 mm dia 50mm long bolts wih nuts & 02 x washers, Cement in polythene bags 50 kgs each grade 43 OPC, Sand Coarse confiming to IS - 383-970, 2nd revision, Crushed Stone Aggregate 20 mm graded, TMT Bar 12mm dia with 50mm U bend, Lime 1st quality, Commercial plywood 6' x 3' x 12 mm., Nails all Type 1"" 2""3"" 4"" 5"" and 6""., Primer Red Oxide, Synthetic enamel paint, colour black/white Ist quality . 
  Angle Iron 65x65x6mm for fencing post of height 3.5, Angle Iron 65x65x6mm for fencing post of height 3.5 mtr, MS Flat 50 x 5mm of 2.60m height, Angle Iron 50x50x6mm for Strut post of height 3.10mtr, Barbed wire made out of GI wire of size 2.24mm dia, Chain link fence dia 4 mm and size of mesh 50mm x, Concertina Coil 600mm extentable up to 6 mtr, Binding wire 0.9mm dia., 12 mm dia 50mm long bolts with nuts & 02 x washers, Cement in polythene bags 50 kgs each grade 43 OPC, Sand Coarse confiming to IS - 383-970, 2nd revision, Crushed Stone Aggregate 20 mm graded, TMT Bar 12mm dia with 50mm U bend, Lime 1st quality, Commercial plywood 6' x 3' x 12 mm., Nails all Type 1"" 2""3"" 4"" 5"" and 6""., Primer Red Oxide, Synthetic enamel paint, colour black/white Ist quality., Cutting Plier 160mm long, Leather Hand Gloves/Knitted industrial, Ring Spanner of 16mm x 17mm, 14 x 16mm, Crowbar hexagonal 1200mm long x 40mm, Plumb bob steel, Bucket steel 15 ltr capacity (as per, Plastic water tank 500 ltrs Make - Sintex, Water level pipe 30 Mtr, Brick Hammer 250 Gms with handle, Hack saw Blade double side, Welding Rod, Cutting rod for making holes, HDPE Sheet 5' x 8', Plastic Measuring tape 30 Mtr, Steel Measuring tape 5 Mtr, Wooden Gurmala 6""x3"", Steel Pan Mortar of 18""dia (As, Showel GS with wooden handle, Phawarah with wooden handle (As per, Digital Vernier Caliper, Digital Weighing Machine cap 500 Kgs, Portable Welding Machine, Concrete mixer machine of 8 CFT . 
  Angle Iron 65x65x6mm for fencing post of height 3.5, Angle Iron 65x65x6mm for fencing post of height 3.5, MS Flat 50 x 5mm of 2.60m height, Angle Iron 50x50x6mm for Strut post of height 3.10mtr, Barbed wire made out of GI wire of size 2.24mm dia, Chain link fence dia 4 mm and size of mesh 50mm, Concertina Coil 600mm extentable up to 6 mtr, Binding wire 0.9mm dia., 12 mm dia 50mm long bolts with nuts & 02 x washers, Cement in polythene bags 50 kgs each grade 43, Sand Coarse confiming to IS - 383-970, 2nd revision, Crushed Stone Aggregate 20 mm graded, TMT Bar 12mm dia with 50mm U bend, Lime 1st quality, Commercial plywood 6' x 3' x 12 mm., Nails all Type 1"" 2""3"" 4"" 5"" and 6""., Primer Red Oxide, Synthetic enamel paint, colour black/white Ist quality., Cutting Plier 160mm long, Leather Hand Gloves/Knitted industrial, Ring Spanner of 16mm x 17mm, 14 x 16mm, Crowbar hexagonal 1200mm long x 40mm, Plumb bob steel, Bucket steel 15 ltr capacity (as per, Plastic water tank 500 ltrs Make - Sintex, Water level pipe 30 Mtr, Brick Hammer 250 Gms with handle, Hack saw Blade double side, Welding Rod, Cutting rod for making holes, HDPE Sheet 5' x 8', Plastic Measuring tape 30 Mtr, Steel Measuring tape 5 Mtr, Wooden Gurmala 6""x3"", Steel Pan Mortar of 18""dia (As per, Showel GS with wooden handle, Phawarah with wooden handle (As per, Digital Vernier Caliper)

","['machine-learning', 'deep-learning', 'natural-language-processing']",
Can we teach an artificial intelligence through sentences?,"
Could we teach an AI with sentences such as ""ants are small"" and ""the sky is blue""? Is there any research work that attempts to do this?
","['neural-networks', 'machine-learning', 'natural-language-processing', 'computational-learning-theory']",
How does the target output of a Single Shot Detector (SSD) look like?,"
According to the paper SSD: Single Shot MultiBox Detector, for each cell in a feature map k boxes are acquired and for each box we get $c$ class scores and $4$ offsets relative to the original default box_shape. This means that we get $m \times n \times (c +4) \times k$ outputs for each $m \times n$ feature map.
However, it is mentioned that in order to train the SSD network only the images and their ground truth boxes are needed.
How exactly can one define the output targets then? What is the format of the output in the SSD framework? I think it cannot be a vector with the positions, sizes and class of each boundary box, since the outputs are a lot more and relate to every default box in the feature maps. 
Can anyone explain in more detail how can I, given an image and its boundary boxes' info, construct a vector that will be fed into a network so that I can train it?
","['neural-networks', 'machine-learning', 'object-recognition']",
Can I compute the fitness of an agent based on a low number of runs of the game?,"
I'm developing an AI to play a card game with a genetic algorithm.  Initially, I will evaluate it against a player that plays randomly, so there will naturally be a lot of variance in the results.  I will take the mean score from X games as that agent's fitness. The actual playing of the game dominates the time to evaluate the actual genetic algorithm.
My question is: should I go for a low X, e.g. 10, so I would be able to move through generations quite fast but the fitness function would be quite inaccurate? Alternatively, I could go for a high X e.g. 100 and would move very slowly but with a more accurate function.
","['optimization', 'genetic-algorithms', 'fitness-functions', 'bias-variance-tradeoff']","You can probably get away with a relatively low X for two reasons:The correct value of X will still depend on the variance in the scores an agent might receive from playing the game, but this is easy to track. A good approach might be to incorporate this directly into your estimate. Compute the variance, and then prefer agents that not only score highly, but do so with low variance. "
"If the Turing test is passed, does this imply that computers exhibit intelligence?","
Turing test was created to test machines exhibiting behavior equivalent or indistinguishable from that of a human. Is that the sufficient condition of intelligence?
","['intelligence-testing', 'turing-test']","We don't know. However, an important line will have been crossed - it will be impossible to tell the difference between an intelligent agent and the machine by use of a text interface. Which is the main point of the test - ""if it quacks like a duck"".It is also an important philosophical point. Whether intelligence is defined purely by behaviour in an environment, or by the mechanisms that arrive at that behaviour. A suitably large database of conversational openers and ""correct"" responses can in theory mimic a lot of real world conversations. Some chatbots take advantage of this and use modern computer capacity to store a lot of responses, and that approach has gained competitive scores in the Loebner prize competition (although not to the stage of actually passing the test). This leads us to the Chinese Room issue, and wondering which part of the system is actually intelligent, or even how much of human conversation is actually intelligent or meaningful (and it what ways)."
Does Monte Carlo tree search qualify as machine learning?,"
To the best of my understanding, the Monte Carlo tree search (MCTS) algorithm is an alternative to minimax for searching a tree of nodes. It works by choosing a move (generally, the one with the highest chance of being the best), and then performing a random playout on the move to see what the result is. This process continues for the amount of time allotted.
This doesn't sound like machine learning, but rather a way to traverse a tree. However, I've heard that AlphaZero uses MCTS, so I'm confused. If AlphaZero uses MCTS, then why does AlphaZero learn? Or did AlphaZero do some kind of machine learning before it played any matches, and then use the intuition it gained from machine learning to know which moves to spend more time playing out with MCTS?
","['machine-learning', 'reinforcement-learning', 'game-ai', 'monte-carlo-tree-search', 'alphazero']","Monte Carlo Tree Search is not usually thought of as a machine learning technique, but as a search technique. There are parallels (MCTS does try to learn general patterns from data, in a sense, but the patterns are not very general), but really MCTS is not a suitable algorithm for most learning problems.AlphaZero was a combination of several algorithms. One was MCTS, but MCTS needs a function to tell it how good different states of the game might be (or else, it needs to simulate entire games). One way to handle this function in a game like chess or Go is to approximate it by training a neural network, which is what the Deep Mind researchers did. This is the learning component of AlphaZero. "
Is the discount not needed in a deterministic environment for Reinforcement Learning?,"
I'm now reading a book titled as ""Deep Reinforcement Learning Hands-On"" and the author said the following on the chapter about AlphaGo Zero:

Self-play
In AlphaGo Zero, the NN is used to approximate the prior probabilities of the actions and evaluate the position, which is very similar to the Actor-Critic (A2C) two-headed setup. On the input of the network, we pass the current game position (augmented with several previous positions) and return two values. The policy head returns the probability distribution over the actions and the value head estimates the game outcome as seen from the player's perspective. This value is undiscounted, as moves in Go are deterministic. Of course, if you have stochasticity in the game, like in backgammon, some discounting should be used.

All the environments that I have seen so far are stochastic environments, and I understand the discount factor is needed in stochastic environment.
I also understand that the discount factor should be added in infinite environments (no end episode) in order to avoid the infinite calculation.
But I have never heard (at least so far on my limited learning) that the discount factor is NOT needed in deterministic environment. Is it correct? And if so, why is it NOT needed?
","['reinforcement-learning', 'q-learning', 'discount-factor']","The motivation for adding the discount factor $\gamma$ is generally, at least initially, based simply in ""theoretical convenience"". Ideally, we'd like to define the ""objective"" of an RL agent as maximizing the sum of all the rewards it gathers; its return, defined as:$$\sum_{t = 0}^{\infty} R_t,$$where $R_t$ denotes the immediate reward at time $t$. As you also already noted in your question, this is inconvenient from a theoretical point of view, because we can have many different such sums that all end up being equal to $\infty$, and then the objective of ""maximizing"" that quantity becomes quite meaningless. So, by far the most common solution is to introduce a discount factor $0 \leq \gamma < 1$, and formulate our objective as maximizing the discounted return:$$\sum_{t = 0}^{\infty} \gamma^t R_t.$$Now we have an objective that will never be equal to $\infty$, so maximizing that objective always has a well-defined meaning.As far as I am aware, the motivation described above is the only motivation for a discount factor being strictly necessary / needed. This is not related to the problem being stochastic or deterministic.If we have a stochastic environment, which is guaranteed to have a finite duration of at most $T$, we can define our objective as maximizing the following quantity:$$\sum_0^T R_t,$$where $R_t$ is a random variable drawn from some distribution. Even in the case of stochastic environments, this is well-defined, we do not strictly need a discount factor.Above, I addressed the question of whether or not a discount factor is necessary. This does not tell the full story though. Even in cases where a discount factor is not strictly necessary, it still might be useful. Intuitively, discount factors $\gamma < 1$ tell us that rewards that are nearby in a temporal sense (reachable in a low number of time steps) are more important than rewards that are far away. In problems with a finite time horizon $T$, this is probably not true, but it can still be a useful heuristic / rule of thumb.Such a rule of thumb is particularly useful in stochastic environments, because stochasticity can introduce greater variance / uncertainty over long amounts of time than over short amounts of time. So, even if in an ideal world we'd prefer to maximize our expected sum of undiscounted rewards, it is often easier to learn how to effectively maximize a discounted sum; we'll learn behaviour that mitigates uncertainty caused by stochasticity because it prioritizes short-term rewards over long-term rewards.This rule of thumb especially makes a lot of sense in stochastic environments, but I don't agree with the implication in that book that it would be restricted to stochastic environments. A discount factor $\gamma < 1$ has also often been found to be beneficial for learning performance in deterministic environments, even if afterwards we evaluate an algorithm's performance according to the undiscounted returns, likely because it leads to a ""simpler"" learning problem. In a deterministic environment there may not be any uncertainty / variance that grows over time due to the environment itself, but during a training process there is still uncertainy / variance in our agent's behaviour which grows over time. For example, it will often be selecting suboptimal actions for the sake of exploration."
Is a multilayer perceptron a recursive function?,"
I read somewhere that a multilayer perceptron is a recursive function in its forward propagation phase. I am not sure, what is the recursive part? For me, I would see an MLP as a chained function. So, it would nice anyone could relate an MLP to a recursive function.
","['neural-networks', 'terminology', 'multilayer-perceptrons', 'function-approximation']",
How important is consciousness for making advanced artificial intelligence? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



How important is consciousness and self-consciousness for making advanced AIs? How far away are we from making such?
When making e.g. a neural network there's (very probably) no consciousness within it, but just mathematics behind, but do we need the AIs to become conscious in order to solve more complex tasks in the future? Furthermore, is there actually any way we can know for sure if something is conscious, or if it's just faking it? It's ""easy"" to make a computer program that claims it's conscious, but that doesn't mean it is (e.g. Siri).
And if the AIs are only based on predefined rules without consciousness, can we even call it ""intelligence""?
","['philosophy', 'artificial-consciousness', 'self-awareness']","Artificial consciousness is a challenging theoretical and engineering objective. Once that major challenge is met, the computer's conscious awareness of itself would likely be a minor addition, since the conscious computer is just another object of which its consciousness can be aware.A child can look in the mirror and recognize that moving their hands back and forth or making faces produces corresponding changes in the reflection.  They recognize themselves.  Later on they realize that exerting physical control over their own movement is much easier than exerting control over another person's hands or face.Some learn that limited control of the faces and manual operations of others is possible if certain social and economic skills are mastered.  They become employers, landlords, investors, activists, writers, directors, public figures, or entrepreneurs.Anyone who has studied the cognitive sciences or experienced the line between types of thought because they are a professional counselor or just a deep listener knows that the lines around consciousness are blurry.  Consider these.Any one of these things can be done with or without certain kinds of consciousness, subconsciousness, impulse, or habit.Subjectively, people report getting out of the car and not recalling having driven home.  One can listen to someone talking, nod in affirmation, respond with, ""Yeah, I understand,"" and even repeat what they said, and yet appear to have no memory of the content of the speech if queried in depth.  One can read a paragraph and get to the end without comprehension.In contrast, a person may mindfully wash up for work, considering the importance of hygiene and paying attention like a surgeon preparing for an operation, noticing the smell of the soap and even the chlorination of the city water.Between those extremes, partial consciousness is also detectable by experiment and in personal experience.  Consciousness most definitely requires attention functionality, which tentatively supervises the coordination of other brain-body sub-systems.Once a biological or artificial system achieves the capacity to coordinate attentively, the objects and tasks toward which they can be coordinated can be interchanged.  Consider these.Now consider how similar or different these mental activities are when we compare self-directed or externally directed attention.This is an illustration why the self- part of self-consciousness is not the challenge in AI.  It is the attentive (yet tentative) coordination that is difficult.  Early microprocessors, designed to work in real time control systems, included (and still include) exception signaling that simplistically models this tentativeness.  For instance, while playing to win in a game, one might try to initiate dialog with the subject.  Attention may shift when the two activities require the same sub-systems.We tend to consider this switching of attention consciousness too.  If we are the person trying to initiate dialog with the person playing to win, we might say, ""Hello?""  The question mark is because we are wondering if the player is conscious.If one was to diminish the meaning of consciousness to the most basic criteria, one might say this.""My neural net is intelligent in some small way because it is conscious of the disparity between my convergence criteria and the current behavior of the network as it is parametrized, so it is truly an example of artificial intelligence, albeit a primitive one.""There is nothing grossly incorrect about that statement.  Some have called that, ""Narrow Intelligence.""  That is a slightly inaccurate characterization, since there may be an astronomical number of possible applications of an arbitrarily deep artificial network that uses many of the most effective techniques available in its design.The other problem with narrowness as a characterization is the inference that there are intelligent systems that are not narrow.  Every intelligent system is narrow compared to a more intelligent system.  Consider this thought experiment.Hannah writes a paper on general intelligence with excellence, both in theoretical treatment and in writing skill.  Many quote it and reference it.  Hannah is now so successful in her AI career that she has the money and time to build a robotic system.  She bases its design on her now famous paper and spares no expense.  To her surprise, the resulting robot is so adaptive that its adaptability exceeds even himself.  She names it Georgia Tech for fun because she lives near the university. Georgia becomes a great friend.  She learns at an incredible rate and is a surprisingly great housemate, cleaning better than Hannah thought humanly possible, which may be literally true.Georgia applies to Georgia Tech, just down the bus line from Hannah's house and studies artificial intelligence there.  Upon the achievement of a PhD after just three years of study, Georgia sits with Hannah after a well attended Thesis Publication party that Hannah graciously held for her.After the last guest leaves, there is a moment of silence as Hannah realizes the true state of her household.  She thinks, ""Will Georgia now exceed me in her research?""  Hannah finally, sheepishly asks, ""In complete honesty, Georgia, do you think you are now a general intelligence like me?""There is a pause.  With a forced look of humility, Georgia replies, ""By your definition of general intelligence, I am.  You are no longer.""Whether this story becomes true in 2018, 3018, or never, the principle is clear.  Georgia is just as able to analyze herself comparatively with Hannah as Hannah is similarly able.  In the story, Georgia applies the definition created in Hannah's paper because Georgia is now able to conceive of many definitions of intelligence and chooses Hannah's as the most pertinent in the context of the conversation.Now imagine this alteration to the story.... She thinks, at what level is Georgia thinking?  Hannah finally, sheepishly asks, ""In complete honesty, Georgia, are you now as conscious as me?""Georgia thinks through the memory of all uses of the word conscious in her past studies — a thousand references in cognitive science, literature, law, neurology, genetics, brain surgery, treatment of brain injury, and addiction research.  She pauses for a few microseconds to consider it all thoroughly, while at the same time sensing her roommates body temperature, neuro-chemical balances, facial muscle motor trends, and body language.Respectfully, she waits 3.941701 extra seconds, which she calculated as the delay that would minimize any humiliation to Hannah, whom she loves, and replies, ""Conscious of what?""In Georgia's reply may be a hypothesis of which Hannah may or may not be aware.  For any given automatons, $a, b, \ldots$, given consciousness, $C$, of a scenario $s$, we have a definition, $\Phi_c$ that can be applied to evaluate the aggregate of all aspects of consciousness of any of the automations, $x$, giving $\Phi_c(C_x(s))$.  Georgia's (apparently already proven) hypothesis is thus.$\forall \Phi_c(C_a(s)) \;\;\; \exists \;\;\; b, \, \epsilon>0 \;\; \ni \;\; \Phi_c(C_b(s)) + \epsilon > \Phi_c(C_a(s))$This is a mathematical way of saying that there can always be someone or some thing more conscious of a given scenario, whether or not she, he, or it is brought into existence.  Changing the criteria of evaluation from consciousness to intelligence, we have thus.$\forall \Phi_i(C_a(s)) \;\;\; \exists \;\;\; b, \, \epsilon>0 \;\; \ni \;\; \Phi_i(C_b(s)) + \epsilon > \Phi_i(C_a(s))$One can only surmise that Hannah's paper defines general intelligence relative to what whatever is the smartest thing around, which was once well-educated human beings.  Thus Hannah's definition of intelligence is dynamic.  Georgia applies the same formula to the new situation where she is now the standard against which lesser intelligence is narrow.Regarding the ability to confirm consciousness, it is actually easier to confirm than intelligence.  Consider this thought experiment.Jack is playing chess with Dylan using the new chess set that Jack bought.  In spite of the aesthetic beauty of this new set, with its white onyx and black agate pieces, Dylan moves each piece with prowess and checkmates Jack.  Jack wonders if Dylan is more intelligent than him and asks what would be a normal question under those conditions.""Dylan, buddy, how long have you been playing chess?""Regardless of the answer and regardless whether Dylan is a robot with a quantum processor of advanced AI or a human being, the intelligence of Dylan cannot be reliably gauged.  However, there is NO DOUBT that Dylan was conscious of the game play.In the examples in the lists at the top of this answer there are a particular sets of requirements to qualify as consciousness.  For the case of Jack and Dylan playing, a few things MUST working in concert.The topology of connections are as follows, and there may be more.1 &rlarr; 4 &rlarr; 23 &rlarr; 5 &rlarr; 24 &rlarr; 6 &rlarr; 57 &rlarr; 8 &rlarr; 96 &rlarr; 10 &rlarr; 810 &rlarr; 11This is one of many integration topologies that support one of many types of things to which consciousness might apply.Whether looking in the mirror just to prepare for work or whether looking deeply, considering the ontological question, ""Who am I?"" each mix of consciousness, subconsciousness, impulse, and habit require a specific topology of mental features.  Each topology must be coordinated to form its specific embodiment of consciousness.To address some other sub-questions, it is easy to make a machine that claims itself to be conscious a digital voice recorder can be programmed to do it in five seconds by recording yourself saying it.Getting a robot to read this answer or some other conception, consider it thoughtfully, and then construct the sentence from knowledge of the vocabulary and conventions of human speech to tell you its conclusion is an entirely different task.  The development of such a robot may take 1,000 more years of AI research.  Maybe ten.  Maybe never.The last question, switched from plural to singular is, ""If [an artificially intelligent device] is only [operating] on predefined rules, without consciousness, can we even call it intelligent?"" The answer is necessarily dependent upon definition $\Phi_i$ above, and, since neither $\Phi_c$ nor $\Phi_i$ have a standard definition within the AI community, one can't determine the cross-entropy or correlation.  It is indeterminable.Perhaps formal definitions of $\Phi_c$ and $\Phi_i$ can now be written and submitted to the IEEE or some standards body."
What does it mean for a neuron in a neural network to be activated?,"
I just stumbled upon the concept of neuron coverage, which is the ratio of activated neurons and total neurons in a neural network. But what does it mean for a neuron to be ""activated""? I know what activation functions are, but what does being activated mean e.g. in the case of a ReLU or a sigmoid function?
","['neural-networks', 'machine-learning', 'activation-functions', 'artificial-neuron']","A neuron is said activated when its output is more than a threshold, generally 0.For examples :
\begin{equation}
y = Relu(a) > 0
\end{equation}
when 
\begin{equation}
a = w^Tx+b > 0
\end{equation}Same goes for sigmoid or other activation functions."
How do I keep track of already visited states in breadth-first search?,"
I was trying to implement the breadth-first search (BFS) algorithm for the sliding blocks puzzle (number type). Now, the main thing I noticed is that, if you have a $4 \times 4$ board, the number of states can be as large as $16!$, so I cannot enumerate all states beforehand.
How do I keep track of already visited states? I am using a class board each class instance contains a unique board pattern and is created by enumerating all possible steps from the current step.
I searched on the net and, apparently, they do not go back to the just-completed previous step, BUT we can go back to the previous step by another route too and then again re-enumerate all steps which have been previously visited. 
So, how to keep track of visited states when all the states have not been enumerated already? Comparing already present states to the present step will be costly.
","['ai-design', 'game-ai', 'breadth-first-search']","You can use a set (in the mathematical sense of the word, i.e. a collection that cannot contain duplicates) to store states that you have already seen. The operations you'll need to be able to perform on this are:Pretty much every programming language should already have support for a data structure that can perform both of these operations in constant ($O(1)$) time. For example:At first glance, it may seem like adding all the states you ever see to a set like this will be expensive memory-wise, but it is not too bad in comparison to the memory you already need for your frontier; if your branching factor is $b$, your frontier will grow by $b - 1$ elements per node that you visit (remove $1$ node from frontier to ""visit"" it, add $b$ new successors/children), whereas your set will only grow by $1$ extra node per visited node.In pseudocode, such a set (let's name it closed_set, to be consistent with the pseudocode on wikipedia could be used in a Breadth-First Search as follows:(some variations of this pseudocode might work too, and be more or less efficient depending on the situation; for example, you could also take the closed_set to contain all nodes of which you have already added children to the frontier, and then entirely avoid the generate_children() call if current is already in the closed_set.)What I described above would be the standard way to handle this problem. Intuitively, I suspect a different ""solution"" could be to always randomize the order of a new list of successor states before adding them to the frontier. This way, you do not avoid the problem of occasionally adding states that you've already previousl expanded to the frontier, but I do think it should significantly reduce the risk of getting stuck in infinite cycles.Be careful: I do not know of any formal analysis of this solution that proves that it always avoids infinite cycles though. If I try to ""run"" this through my head, intuitively, I suspect it should kind of work, and it does not require any extra memory. There may be edge cases that I'm not thinking of right now though, so it also simply might not work, the standard solution described above will be a safer bet (at the cost of more memory)."
Using two generative adversarial nets to classify articles - what is a good approach?,"
I'm trying to create a deep learning network to classify news article based on the text and associated image. The idea comes from a novel use of GANs to classify based on generated data.
My approach was to use Tensorflow to generate word embeddings in the article, and then tranform the images into records - https://github.com/openai/improved-gan/blob/master/imagenet/convert_imagenet_to_records.py. This second component would also contain the label.

Is it wise to combine both modes into one neural net, or classify separately?

I'm also trying to work out how to concatenate the two tensors in Tensorflow. Can anyone give a steer.
","['tensorflow', 'deep-neural-networks', 'generative-adversarial-networks']",
Training a CNN from scratch over COCO dataset [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I am using Tensorflow Object Detection API for training a CNN from scratch on COCO dataset. I need to use this specific configuration.
There is no pre-trained model on COCO with that configuration and this is the reason why I am training from scratch.
However, after 1 week of training and evaluating each checkpoint generated by the training phase this is how my learning phase appears on Tensorboard:

Thus, my questions are:

does anyone know how many iterations approximately will be necessary? Right now I did more than 500'000 iterations.
How can be possible that after 500'000 the evaluation is 0,8%? I would expected something like 60-70%.
Why does there is a sudden drop after 500k iterations? I thought that the eval was supposed to converge to some limit. (this is what SGD should do)
Is there any 'trick' to speed up the training phase? (ex: increasing the learning rate, etc).

","['convolutional-neural-networks', 'computer-vision', 'tensorflow', 'object-recognition']",
Can machine learning be used to pass the Turing test?,"
Can we say that the Turing test aims to develop machines or methods to reach human-level performance in all cognitive tasks and that machine learning is one of these methods that can pass the Turing test?
","['machine-learning', 'philosophy', 'turing-test']",
Which other loss functions for hierarchical multi-label classification could I use?,"
I am looking to try different loss functions for a hierarchical multi-label classification problem. So far, I have been training different models or submodels like multilayer perceptron (MLP) branch inside a bigger model which deals with different levels of classification, yielding a binary vector. I have been also using Binary Cross-Entropy (BCE) and summing all the losses existing in the model before backpropagating.
I am considering trying other losses like MultiLabelSoftMarginLoss and MultiLabelMarginLoss.
What other loss functions are worth trying? Hamming loss perhaps or a variation? Is it better to sum all the losses and backpropagate or do multiple backpropagations?
","['neural-networks', 'deep-learning', 'objective-functions', 'multi-label-classification']",
Do self-driving cars resort to randomness to make decisions?,"
I recently heard someone make a statement that when you're designing a self-driving car, you're not building a car but really a computerized driver, so you're trying to model a human mind -- at least the part of the human mind that can drive.
Since humans are unpredictable, or rather since their actions depend on so many factors some of which are going to remain unexplained for a long time, how would a self-driving car reflect that, if they do?
A dose of unpredictability could have its uses. If, say, two self-driving cars are in a stuck in a right of way deadlock, it could be good to inject some randomness instead of maybe seeing the same action applied at the same time if the cars run the same system. 
But, on the other hand, we know that non-deterministic isn't friends with software development, especially in testing. How would engineers be able to control it and reason about it?
","['ai-design', 'autonomous-vehicles']","Driving PrioritiesWhen considering the kind of modeling needed to create reliable and safe autonomous vehicles, the following driving safety and efficacy criteria should be considered, listed in priority with the most important first.These are ordered in a way that makes civic and global sense, but they are not the priorities exhibited by human drivers.Copy Humans or Reevaluate and Design from Scratch?Whoever said that the goal of autonomous car design is to model the portions of a human mind that can drive should not be designing autonomous cars for actual manufacture.  It is well known that most humans, although they may have heard of the following safety tips, cannot bring them into consciousness with sufficient speed to benefit from them in actual driving arrangements.Many collisions between locomotives and cars are because a red light causes a line in multiple lanes across the tracks.  Frequently, a person will move onto the railroad tracks to gain one car's length on the other cars.  When others move to make undoing that choice problematic, a serious risk emerges.As absurd as this behavior is to anyone watching, many deaths occur as a fast traveling 2,000 ton locomotive hits what feels like a dust speck to the train passengers.Predictability and AdaptabilityHumans are unpredictable, as the question indicates, but although adaptability may be unpredictable, unpredictability may not be adaptive.  It is adaptability that is needed, and it is needed in five main ways.In addition, driving a car isModelling Driving ComplexitiesThis requires a model or models comprise of several kinds of objects.Neither Mystery nor IndeterminanceAlthough these models are cognitively approximated in the human brain, how well they are modeled and how effective those models are at reaching something close to a reasonable balance of the above priorities varies from driver to driver, and varies from trip to trip for the same driver.However, as complex as driving is, it is not mysterious.  Each of the above models are easy to consider at a high level in terms of how they interact and what mechanical and probabilistic properties they have.  Detailing these is an enormous task, and making the system work reliably is a significant engineering challenge, in addition to the training question.Inevitability of AchievementRegardless of the complexity, because of the economics involved and the fact that it is largely a problem of mechanics, probability, and pattern recognition, it will be done, and it will eventually be done well.When it is, as unlikely as this sounds to the person who accepts our current culture as permanent, human driving may become illegal in this century in some jurisdictions.  Any traffic analyst can mount heaps of evidence that most humans are ill equipped to drive a machine that weighs a ton at common speeds.  The licensing of unprofessional drivers has only become widely accepted because of public insistence on transportation convenience and comfort and because the workforce economy requires it.Autonomous cars may reflect the best of human capabilities, but they will likely far surpass them because, although the objects in the model are complex, they are largely predictable, with the notable exception of children playing.  AV technology will use the standard solution for this.  The entire scenario can be brought into slow motion to adapt for children playing simply by slowing way down.  AI components that specifically detect children and dogs are likely to emerge soon, if they do not already exist.RandomnessRandomness is important in training.  For instance, a race car driver will deliberately create skids of various types to get used to how to control them. In machine learning we see some pseudo random perturbations introduced during training to ensure that the gradient descent process does not get caught in a local minimum but rather is more likely to find a global minimum (optimum).DeadlockThe question is correct in stating that, ""A dose of unpredictability could have its uses.""  The deadlock scenario is an interesting one, but is unlikely to occur as standards develop.  When four drivers come to a stop sign at the same time, they really don't.  It only seems like they did.  The likelihood that none of them arrived more than a millisecond before the others is astronomically small.People will not detect (or even be honest enough) to distinguish these small time differences, so it usually comes to who is most gracious to wave the others on, and there can be some deadlock there too, which can become comical, especially since all of them really wish to get moving.  Autonomous vehicles will extremely rarely encounter a deadlock that is not covered by the rule book the government licensing entity publishes, which can be programmed as driving rules into the system.On those rare occasions, the vehicles could digitally draw lots, as suggested, which is one place where unpredictability is adaptive.  Doing skid experimentation like a race car driver on Main Street at midnight may be what some drunk teen might do, but that is a form of unpredictability that is not adaptive toward a sensible ordering of the priorities of driving.  Neither would be texting or trying to eat and drive.DeterminismRegarding determinism, in the context of the uses discussed, pseudo-random number generation of particular distributions will suffice.Functional tests and unit testing technologies are not only able to handle the testing of components with pseudo-randomness, but they sometimes employ pseudo-randomness to provide better testing coverage.  The key to doing this well is understanding of probability and statistics, and some engineers and AI designers understand it well.Element of SurpriseWhere randomness is most important in AV technology is not in the decision making but in the surprises.  That is the bleeding edge of that engineering work today.  How can one drive safely when a completely new scenario appears in the audio or visual channels?  This is perhaps the place where the diversity of human thought may be best adept, but at highway speeds, it is usually too slow to react in the way we see in movie chase scenes.Correlation Between Risk and SpeedThis brings up an interesting interaction of risk factors.  It is assumed that higher speeds are more dangerous, the actual mechanics and probability are not that clear cut.  Low speeds produce temporally longer trips and higher traffic densities.  Some forms of accidents are less likely at higher speeds, specifically ones that are related mostly to either traffic density or happenstance.  Other forms are more likely at higher speeds, specifically ones that are related to reaction time and tire friction.With autonomous vehicles, tire slippage may be more accurately modeled and reaction time may be orders of magnitude faster, so minimum speed limits may be more imposed and upper limits may increase once we get humans out of the driver's seats."
How do you program fear into a neural network?,"
If you've been attacked by a spider once, chances are you'll never go near a spider again.
In a neural network model, having a bad experience with a spider will slightly decrease the probability you will go near a spider depending on the learning rate. This is not good.
How can you program fear into a neural network, such that you don't need hundreds of examples of being bitten by a spider in order to ignore the spider (and also that it doesn't just lower the probability that you will choose to go near a spider)?
","['neural-networks', 'machine-learning', 'reinforcement-learning']",
How to train a CNN [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



When it comes to CNNs, I don't understand 2 things in the training process:

How do I pass the error back when there are pooling layers between the convolutional layers?
And if I know how it's done, can I train all the layers just like layers in normal Feed Forward Neural Nets?

","['convolutional-neural-networks', 'backpropagation']","Yes.  You can train end-to-end.  The introduction of convolution kernels with associated pooling layers to the sequence of forward feed operations on the signals does not change the basic principles.Consider studying Backpropagation In Convolutional Neural Networks on Jefkine.com, which clarifies the application of those principles with convolution-pooling pairs.There is another approach, borrowing from wisdom gained in the development of analog feedback in instrumentation.  There are times when a sequence of operations can be better trained with more than one feedback loop, which requires some determination of error or wellness at intermediate stages and breaks the system into segments that each train based on those intermediate criteria.This other approach is hierarchical, and an overall convergence may be controlled by a higher level back propagation, considering each segment as a black box.  As in analog circuitry, multiple degrees of freedom with semi-independent convergence mechanisms has been shown to allow for deeper sequences without a major loss of convergence reliability or accuracy."
"How can AI researchers avoid ""overfitting"" to commonly-used benchmarks as a community?","
In fields such as Machine Learning, we typically (somewhat informally) say that we are overfitting if improve our performance on a training set at the cost of reduced performance on a test set / the true population from which data is sampled.
More generally, in AI research, we often end up testing performance of newly proposed algorithms / ideas on the same benchmarks over and over again. For example:

For over a decade, researchers kept trying thousands of ideas on the game of Go.
The ImageNet dataset has been used for huge amounts of different publications
The Arcade Learning Environment (Atari games) has been used for thousands of Reinforcement Learning papers, having become especially popular since the DQN paper in 2015.

Of course, there are very good reasons for this phenomenon where the same benchmarks keep getting used:

Reduced likelihood of researchers ""creating"" a benchmark themselves for which their proposed algorithm ""happens"" to perform well
Easy comparison of results to other publications (previous as well as future publications) if they're all consistently evaluated in the same manner.

However, there is also a risk that the research community as a whole is in some sense ""overfitting"" to these commonly-used benchmarks. If thousands of researchers are generating new ideas for new algorithms, and evaluate them all on these same benchmarks, and there is a large bias towards primarily submitting/accepting publications that perform well on these benchmarks, the research output that gets published does not necessarily describe the algorithms that perform well across all interesting problems in the world; there may be a bias towards the set of commonly-used benchmarks.

Question: to what extent is what I described above a problem, and in what ways could it be reduced, mitigated or avoided?
","['machine-learning', 'research', 'academia', 'benchmarks']","Great question Dennis!This is a perennial topic at AI conferences, and sometimes even in special issues of journals. The most recent one I recall was Moving Beyond the Turing Test in 2015, which ended up leading to a collection of articles in AI magazine later that year. Usually these discussions cover a number of themes:I remember attending very similar discussions at machine learning conferences in the late 2000's, but I'm not sure anything was published out of it.Despite these discussions, AI researchers seem to incorporate the new benchmarks, rather than displacing the older ones entirely. The Turing Test is still going strong for instance. I think there are a few reasons for this.First, benchmarks are useful, particularly to provide context for research. Machine learning is a good example. If the author sets up an experiment on totally new data, then even if they apply a competing method, I have to trust that they did so faithfully, including things like optimizing the parameters as much as with their own methods. Very often they do not do this (it requires some expertise with competing methods), which inflates the reported advantage of their own techniques. If they also run their algorithm on a benchmark, then I can easily compare it to the benchmark performances reported by other authors, for their own methods. This makes it easier to spot a technique that's not really effective.Second, even if new benchmarks or new problems are more useful, nobody knows about them! Beating the current record for performance on ImageNet can slingshot someone's career in a way that top performance on a new problem simply cannot. Third, benchmarks tend to be things that AI researchers think can actually be accomplished with current tools (whether or not they are correct!). Usually iterative improvement on them is fairly easy (e.g. extend an existing technique). In a ""publish-or-perish"" world, I'd rather publish a small improvement on an existing benchmark than attempt a risker problem, at least pre-tenure. So, I guess my view is fixing the dependence on benchmarks involves fixing the things that make people want to use them:"
How to generalise over multiple simultaneous dependent actions in Reinforcement Learning,"
I am trying to build an RL agent to price paid-for-seating on commercial flights. I should reiterate here - I am not talking about the price of the ticket - rather, I am talking about the pricing you see if you click on the seat map to choose where on the plane you sit (exits rows, window seats, etc). The general set up is:

After choosing their flights (for a booking of n people), a customer will view a web page with the available seat types and their prices visible.
They select between zero and n seats from a seat map with a variety of different prices for different seats, to be added to their booking.
The revenue from step 2 is observed as the reward.

Each 'episode' is the selling cycle of one flight. Whether the customer buys a chosen seat or not, the inventory goes down as they still have a ticket for the flight so will get a seat at departure. I would like to change prices on the fly, rather than fix a set of optimal prices throughout the selling cycle. 
I have not decided on a general architecture yet. I want to take various booking, flight, and inventory information into account, so I know I will be using function approximation (most likely a neural net) to generalise over the state space.
However, I am less clear on how to set up my action space. I imagine an action would amount to a vector with a price for each different seat type (window seat, exit row, etc). If I have, for example, 8 different seat types, and 10 different price points for each, this gives me a total of 10^8 different actions, many of which will be very similar. In a sense, each action is comprised of a combination of sub-actions - the action of pricing each seat type.
Additionally, each sub-action (pricing one seat type) is somewhat dependent on the others, in the sense that the price of one seat type will likely affect the demand (and hence reward contribution) for another. For example, if you set window seats to a very cheap price, people will be less likely to spend a normal amount for the other seat types. Hence, I doubt the problem can be decomposed into a set of sub-problems.
I'm interested if there has been any research into dealing with a problem like this. Clearly any agent I build needs some way to generalise across actions to some degree, since collecting real data on millions of actions is not possible, even just for one state.
As I see it, this comes down to three questions:

Is it possible to get an agent that can deal with a set of actions (prices) as a single decision?
Is it possible to get this agent to understand actions in relative terms? Say for example, one set of potential prices is [10, 12, 20], for middle seats, aisle seats, and window seats. Can I get my agent to realise that there is a natural ordering there, and that the first two pricing actions are more similar to each other than to the third possible action?
Further to this, is it possible to generalise from this set of actions - could an agent be set up to understand that the set of prices [10, 13, 20] is very similar to the first set?

I haven't been able to find any literature on this, especially relating to the second question - any help would be much appreciated!
","['reinforcement-learning', 'combinatorics']","If you want to treat the problem as a full Reinforcement Learning problem, I'd recommend to try avoiding the combinatorial explosion of the action space by treating every sub-action as a separate decision point, a separate full action. If you have, for example, already selected 4 sub-actions for a particular customer, you can try to include those in some way in the state representation / input when moving on to the 5th sub-action. By including already-selected sub-actions in the state space, your algorithm can learn to take into account that optimal prices for some seat types will depend on what prices were already selected for others.I do suspect such a full RL formulation will still be a difficult problem to learn though, will require huge amounts of experience. It may be worth considering to simplify it anyway, treat it as a Contextual + Combinatorial Multi-Armed Bandit Problem. That way you won't be able to learn long-term effects across multiple different customers as you described in the comments, but you will likely at least be able to learn something that works decently well with less experience. Recently, an interesting new book appeared on MAB problems, which is available for free here. You will find many chapters on Contextual MABs there, and also one chapter (chapter 30) on Combinatorial MABs.Note that with either of these ""combinatorial"" approaches, you can also try to play around with the order in which you select sub-actions. For example, the sub-action / price point selected for seat type A might have a significant influence on what the optimal remaining policy would be for other seat types, whereas the sub-action for seat type B might have no influence on other seat types. It would then be useful to always prioritize selecting sub-actions for seat type A. You can try to identify these kinds of effects by keeping track of (co)variances in observed returns.The solutions I proposed above do not do this explicitly, they circumvent the issue by taking multiple sequential decisions. There does appear to be some research in RL with vector-valued actions. For example, the paper Clipped Action Policy Gradient briefly mentions vector-valued actions in Subsection 3.2. I am not personally familiar enough with RL + vector-valued actions to make a direct recommendation as to what approaches do or don't work well, but maybe this can at least help you find more relevant literature if this is a direction you'd like to pursue.This kind of generalization should again come naturally from using function approximation with either of the solutions proposed above."
How can I implement back-propagation for medium-sized neural networks?,"
I've been wanting to make my own Neural Network in Python, in order to better understand how it works. I've been following this series of videos as a sort of guide, but it seems the backpropagation will get much more difficult when you use a larger network, which I plan to do. He doesn't really explain how to scale it to larger ones.
Currently, my network feeds forward, but I don't have much of an idea of where to start with backpropagation. My code is posted below, to show you where I'm currently at (I'm not asking for coding help, just for some pointers to good sources, and I figure knowing where I'm currently at might help):
import numpy

class NN:
    prediction = []
    def __init__(self,input_length):
        self.layers = []
        self.input_length = input_length
    def addLayer(self, layer):
        self.layers.append(layer)
        if len(self.layers) >1:
            self.layers[len(self.layers)-1].setWeights(len(self.layers[len(self.layers)-2].neurons))
        else:
            self.layers[0].setWeights(self.input_length)
    def feedForward(self, inputs):
        _inputs = inputs
        for i in range(len(self.layers)):
            self.layers[i].process(_inputs)
            _inputs = self.layers[i].output
        self.prediction = _inputs

    def calculateErr(self, target):
        out = []
        for i in range(0,len(self.prediction)):
            out.append(  (self.prediction[i] - target[i]) ** 2  )
        return out
        

class Layer:

    neurons = []
    weights = []
    biases = []
    output = []
    
    def __init__(self,length,function):
        for i in range(0,length):
            self.neurons.append(Neuron(function))
            self.biases.append(numpy.random.randn())

    def setWeights(self, inlength):
        for i in range(0,inlength):
            self.weights.append([])
            for j in range(0, inlength):
                self.weights[i].append(numpy.random.randn())
    
    def process(self,inputs):
        for i in range(0, len(self.neurons)):
            self.output.append(self.neurons[i].run(inputs,self.weights[i], self.biases[i]))
    

class Neuron:
    output = 0
    def __init__(self, function):
        self.function = function
    def run(self, inputs, weights, bias):
        self.output = self.function(inputs,weights,bias)
        return self.output

def sigmoid(n):
    return 1/(1+numpy.exp(n))


def inputlayer_func(inputs,weights,bias):
    return inputs

def l2_func(inputs,weights,bias):
    out = 0
    
    for i in range(0,len(inputs)):
        out += weights[i] * inputs[i]
    out += bias
    
    return sigmoid(out)

NNet = NN(2)


l2 = Layer(1,l2_func)


NNet.addLayer(l2)
NNet.feedForward([2.0,1.0])
print(NNet.prediction)

So, is there any resource that explains how to implement the back-propagation algorithm step-by-step?
","['neural-networks', 'reference-request', 'backpropagation', 'implementation', 'resource-request']","Backpropagation isn't too much more complicated, but understanding it well will require a bit of mathematics.This tutorial is my go-to resource when students want more detail, because it includes fully worked through examples.Chapter 18 of Russell & Norvig's book includes pseudocode for this algorithm, as well as a derivation, but without good examples."
How recurrent neural network work when predict many days?,"
I use recurrent neural network, RNNs have to get input one value per step and it will show one value output. If I have daily sale demand time series data. 
I want to predict sale demand for three days. So, Rnn have to show output one day in three time or it can show sale  demand three days output in one time prediction ?
","['neural-networks', 'machine-learning', 'deep-learning', 'recurrent-neural-networks']",
Which algorithm can I use to minimise the number of wins of 2 weapons that fight each other in a game?,"
I have a game that involves 2 weapons, which fight against each other. Each weapon has 5 features/statistics, which have certain range. I can simulate the game  $N$ times with randomly initialised values for these statitics, in order to collect a dataset. I can also count the number of times a weapon wins, loses or draws against the other.
I'm looking for an algorithm that minimises the number of wins of the 2 weapons (maybe by changing these features), so that they are balanced.
","['datasets', 'game-ai', 'regression', 'algorithm-request']","I'm going to start by trying to restate your problem as I understand it.If this sounds right, then fundamentally your problem is a form of regression, which something you could use AI for, but probably don't need to. However, your problem is probably not linear, so you need the interactions between the features. Here's what I suggest:For each pair of weapons, store a comma separated list consisting of the stats for each weapon (one by one), followed by wins1 - wins2. At the top, list out the names of each attribute, separated by commas, (e.g. weapon1Str, weapon1Range, ... ,weapon1-weapon2 Then use a language like R that has simple support for complex forms of regression.In R, this is then as simple as:This should produce a list of ""coefficients"", one for each of the attributes, and one for the interaction between each pair of attributes, which form a lengthy quadratic equation in 10 variables.Any zero of that equation should be a pair of weapons that minimizes this difference.That's probably the place to start. If it doesn't work out, maybe come post a different question and we can help more."
How should continuous action/gesture recognition be performed differently than isolated action recognition,"
I am going to train a deep learning model to classify hand gestures in video. Since the person will be taking up nearly the entire width/height of the video and I will be classifying what hand gesture he or she is doing, I don't need to identify the person and create a bounding box around the person doing the action. I only need to classify video sequences to their class labels. 
I will be training on a dataset with individual videos, in which each entire video clip is the particular gesture (So it's a dataset like UCF-101, with video clips corresponding to class labels). But when I am deploying the network, I want the neural network to run on live video. As in how the live video is playing, it should recognize when a gesture has occurred and indicate that it recognized the gesture. 
So I was wondering - How can I train the neural network on isolated video sequences in which the entire video clip is the action (like explained above), but run the neural network on live video? For instance, can I use a 3D CNN? Or must I use a 2D CNN with an LSTM network instead, for it to work on live video? My concern is that since a 3D CNN performs the filters across many frames, wouldn't running the CNN on every frame make it very slow? But if I use a 2D CNN with LSTM, will that make it faster? Or will both work fine?
Thank you for your help in advance.
","['deep-learning', 'convolutional-neural-networks', 'action-recognition']",
Is it necessarry to create theory/infrastructure to prevent people from creating AI incompatible with a safe model (if we create one)?,"
Nowadays we don't know how to create AI in a safe way (I think that we don't even know yet how to define a safe AI), but there is a lot of research in developing a model allowing it. 
Let's say, that someday we discover such a model (maybe even it would be possible to mathematically prove its safety). Is it rational to ask, how do we prevent people from creating AI outside of this model (e.g. they are so confident in their own model, that they just pursue it and end up with something like paperclip scenario)? 
Should we also think about creating some theory/infrastructure preventing such a scenario?
","['philosophy', 'ai-safety']",
Is there a database somewhere of common lists? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I'm looking for a database or some machine readable document that contains common ordered lists or common short sets. e.g:
{January, February, March,...}
{Monday, Tuesday, ....}
{Red, Orange, Yellow,...}
{1,2,3,4,...}
{one, two, three, four,...}
{Mercury, Venus, Earth, Mars,...}
{I, II, III, IV, V, VI,...}
{Aquarius, Pisces, Aries,...}
{ein, zwei, drei, ...}
{Happy, Sneezy, Dopey, ...}
{Dasher, Dancer, Prancer, Vixen ,...}
{John, Paul, George, Ringo}
{20, 1, 18, 4, 13, 6, ...}
{Alabama, Alaska, Arizona, Arkansas, California,...}
{Washington, Adams, Jefferson, ...}
{A,B,C,D,E,F,G,...}
{A,E,I,O,U}
{2,3,5,7,11,13,17,...}
{triangle, square, pentagon, hexagon,...}
{first, second, third, fourth, fifth,...}
{tetrahedron, cube, octohedron, icosohedron, dodecahedron}
{autumn, winter, spring, summer}
{to, be, or, not, to, be, that, is, the, question}
...

One use is for creating an AI that can solve codes or predict the next thing in a sequence.
","['datasets', 'resource-request']",
What's the rationale behind mini-batch gradient descent?,"
I am reading a book that states

As the mini-batch size increases, the gradient computed is closer to the 'true' gradient

So, I assume that they are saying that mini-batch training only focuses on decreasing the cost function in a certain 'plane', sacrificing accuracy for speed. Is that correct?
","['gradient-descent', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']","The basic idea behind mini-batch training is rooted in the exploration / exploitation tradeoff in local search and optimization algorithms. You can view training of an ANN as a local search through the space of possible parameters. The most common search method is to move all the parameters in the direction that reduces error the most (gradient decent).However, ANN parameter spaces do not usually have a smooth topology. There are many shallow local optima. Following the global gradient will usually cause the search to become trapped in one of these optima, preventing convergence to a good solution.Stochastic gradient decent solves this problem in much the same way as older algorithms like simulated annealing: you can escape from a shallow local optima because you will eventually (with high probability) pick a sequence of updates based on a single point that ""bubbles"" you out. The problem is that you'll also tend to waste a lot of time moving in wrong directions.Mini-batch training sits between these two extremes. Basically you average the gradient across enough examples that you still have some global error signal, but not so many that you'll get trapped in a shallow local optima for long. Recent research by Masters and Luschi suggests that in fact, most of the time you'd want to use smaller batch sizes than what's being done now. If you set the learning rate carefully enough, you can use a big batch size to complete training faster, but the difficulty of picking the correct learning rate increases with the size of the batch."
"In what ways is the term ""topology"" applied to Artificial Intelligence?","
I have only a general understanding of General Topology, and want to understand the scope of the term ""topology"" in relation to the field of Artificial Intelligence.
In what ways are topological structure and analysis applied in Artificial Intelligence?
","['machine-learning', 'terminology', 'applications', 'topology']",
"When working with time-series data, is it wrong to use different time-steps for the features and target?","
When working with time-series data, is it wrong to use daily prices as features and the price after 3 days as the target?
Or should I use the next-day price as a target, and, after training, predict 3 times, each time for one more day ahead (using the predicted value as a new feature)?
Will these 2 approaches give similar results?
","['long-short-term-memory', 'datasets', 'time-series', 'features', 'labels']",
How does rotating an image and adding new 'rotated classes' prevent overfitting?,"
From Meta-Learning with Memory-Augmented Neural Networks in section 4.1:

To reduce the risk of overfitting, we performed data augmentation by randomly translating and rotating character images. We also created new classes through 90◦, 180◦ and 270◦ rotations of existing data.

I can maybe see how rotations could reduce overfitting by allowing the model to generalize better. But if augmenting the training images through rotations prevents overfitting, then what is the purpose of adding new classes to match those rotations? Wouldn't that cancel out the augmentation?
","['neural-networks', 'machine-learning', 'overfitting', 'meta-learning', 'data-augmentation']",
How do I compute log-likelihood for training set in supervised learning?,"
I am building a supervised learning model and I wish to compute the log-likelihood for the training set at the point of the minimum validation error. 
Initially, I was computing the sum of all the probabilities with maximum value obtained after applying softmax for each example in the training set at the point of minimum validation error but that doesn't look correct. 
What is the correct formula for the log-likelihood?
","['machine-learning', 'training', 'optimization']","The log-likelihood function for the training set (in general, not for deep learning in particular) will depend on your choice of loss function.I'm guessing you're using something like a quadratic loss function for a binary classification problem, since this is a common approach. In that case, the log-likelihood is the sum of logs of the squared differences between the target label and the softmax value produced by your model. If you want to compute the log likelihood under a particular set of parameters (say, those that minimize validation error), then you just use those parameters in the model when generating softmax values."
Optimization step in Apprenticeship Learning via Inverse Reinforcement Learning,"
Why the optimization step of the algorithm a quadratic program?  [See: Apprenticeship Learning via Inverse Reinforcement Learning; page 3] 
Isn't the objective function linear? Why don't we treat the problem as LPQC (linear program with quadratic constraints)?
","['machine-learning', 'reinforcement-learning', 'optimization', 'math']",
What is the best approach for writing a program to identify objects in a picture then crop them a specific way?,"
My works quality control department is responsible for taking pictures of our products at various phases through our QC process and currently the process goes:

Take picture of product
Crop the picture down to only the product
Name the cropped picture to whatever the part is and some other relevant data

Depending on the type of product the pictures will be cropped a certain way. So my initial thought would be to use a reference to an object identifier and then once the object is identified it will use a cropping method specific to that product. There will also be QR codes within the pictures being taken for naming via OCR in the future so I can probably identify the parts that way if this proves slow or problematic.
The part I am unsure about is how to get the program to know how to crop based on a part. For example I would like to present the program with a couple before crop and after crop photos of product X then make a specific cropping formula for product X based on those two inputs. 
Also if it makes any difference my code is in C# 
","['machine-learning', 'convolutional-neural-networks', 'computer-vision']","Depending on kind and amount of data you posess, there are few approaches that you might consider.Marking target objects on dataset and training CNN that returns coordinates of target object. In this case, remember that it is usually faster when training data ROIs have their coordinates relative to image size.Use some kind of focus mechanism, like spatial transformer network:This kind of network component is able to learn image transformation (including crop) that maximazes target metric for main classifier. This tutorial on pytorch:shows some nice visualizations of STN results. Good thing about this kind of network is that, given enough data, it might learn proper transformation from image classification data (photo -> class). One does not need to explicitly mark target objects on image!Object detection networks, like YOLO, Faster-RCNN. There are many tutorials on that matter, eg:Saliency extraction. Simple idea is to generate heatmap showing what parts of input image activates classifier the most. I guess you could try calculate bounding box basing on such heatmap. Example research paper:Points 1 and 2 are probably easies to implement, so I would start with them."
Is there a way to translate the concept of batch size into reinforcement learning?,"
I am using a neural network as my function approximator for reinforcement learning. In order to get it to train well, I need to choose a good learning rate. Hand-picking one is difficult, so I read up on methods of programmatically choosing a learning rate. I came across this blog post, Finding Good Learning Rate and The One Cycle Policy, about finding cyclical learning rate and finding good bounds for learning rates.
All the articles about this method talk about measuring loss across batches in the data. However, as I understand it, in Reinforcement Learning tasks do not really have any ""batches"", they just have episodes that can be generated by an environment as many times as one wants, which also gives rewards that are then used to optimize the network.
Is there a way to translate the concept of batch size into reinforcement learning, or a way to use this method of cyclical learning rates with reinforcement learning?
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'learning-rate', 'batch-size']",
"How is the fitted Q-iteration algorithm related to $Q^*(s, a)$, and how can we use function approximation with this algorithm?","
I hope to get some clarifications on Fitted Q-Iteration (FQI).
My Research So Far
I've read Sutton's book (specifically, ch 6 to 10), Ernst et al and this paper.
I know that $Q^*(s, a)$ expresses the expected value of first taking action $a$ from state $s$ and then following optimal policy forever.
I tried my best to understand function approximation in large state spaces and TD($n$).
My Questions

Concept - Can someone explain the intuition behind how iteratively extending N from 1 until stopping condition achieves optimality (Section 3.5 of Ernst et al.)? I have difficulty wrapping my mind around how this ties in with the basic definition of $Q^*(s, a)$ that I stated above.

Implementation - Ernst et al. gives the pseudo-code for the tabular form. But if I try to implement the function approximation form, is this correct:


Repeat until stopping conditions are reached:
    - N ← N + 1
    - Build the training set TS based on the function Q^{N − 1} and on the full set of four-tuples F 

    - Train the algorithm on the TS

    - Use the trained model to predict on the TS itself

    - Create TS for the next N by updating the labels - new reward plus (gamma * predicted values )

I am just starting to learn RL as part of my course. Thus, there are many gaps in my understanding. Hope to get some kind guidance.
","['reinforcement-learning', 'q-learning', 'papers', 'value-iteration']","1): The intuition is based on the concept of value iteration, which the authors mention but don't explain on page 504. The basic idea is this: imagine you knew the value of starting in state x and executing an optimal policy for n timesteps, for every state x. If you wanted to know the optimal policy (and it's value) for running for n+1 timesteps in each location, this is now easy to compute. The optimal action from state x is whichever one maximizes the sum of the reward for this timestep (r) and the value of executing an optimal n-step policy from the state you'd end up in after (or expected value if the problem isn't deterministic). In the approach of the paper, you're not going to compute either the policy or the value explicitly (probably because it's too expensive), so you just approximate the Q function for the n+1 problem.IIRC, as long as your problem has a discounting factor and the error in your function approximation isn't too large, there are proofs (see Russell & Norvig's chapter on RL (18?)) that your policy will eventually stop changing in-between updates, and will be consistent with the policy for an infinite number of steps. Intuitively, this is because the discounting factor causes a series for the rewards to be convergent.2): I think that's right. When you build the training set, use the actions suggested by the results for the $$Q_{n-1}$$ network. That's an approximation of the reward for starting in each state and running for n-1 steps with an optimal policy. Then you're learning an approximation of $Q_n$ from that, which looks right."
Why coupling coefficients in capsule neural networks can't be learned by back-propagation?,"
The paper Dynamic Routing Between Capsules uses the algorithm called ""Dynamic Routing Between Capsules"" to determine the coupling coefficients between capsules. 
Why it can't be done by backpropagation? 
","['neural-networks', 'machine-learning', 'deep-learning', 'backpropagation', 'capsule-neural-network']",
How can I create a chatbot application where the user can create its own bot?,"
I am trying to create a chatbot application where the user can create its own bot, like Botengine. After going through google, I saw I need some NLP API to process the user's query. As per wit.ai basic example, I can set and get data. How I am going to create a bot engine?
So, as far I understand the flow, here is an example for pizza delivery

The user will enter a welcome message, i.e - Hi or Hello
The welcome reply will be saved by bot owner in my database
The user will enter some query, then I will hit wit.ai API to process that query. Example: The user's query is ""What kind of pizza's available in your store"" and wit.ai will respond with the details of intent ""pizza_type""
Then I will search for the intent returned by wit in my database.

So, is that the right flow to create a chatbot? Am I in the right direction? Could anyone give me some link or some example so I can go through it? I want to create this application using Node.js. I have also found some example in node-wit, but can't find how I will implement this.
","['natural-language-processing', 'ai-design', 'chat-bots']",
What is the difference between artificial intelligence and computational intelligence?,"
Having analyzed and reviewed a certain amount of articles and questions, apparently, the expression computational intelligence (CI) is not used consistently and it is still unclear the relationship between CI and artificial intelligence (AI).
According to IEEE computational intelligence society

The Field of Interest of the Computational Intelligence Society (CIS) shall be the theory, design, application, and development of biologically and linguistically motivated computational paradigms emphasizing neural networks, connectionist systems, genetic algorithms, evolutionary programming, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained.

which suggests that CI could be a sub-field of AI or an umbrella term used to group certain AI sub-fields or topics, such as genetic algorithms or fuzzy systems.
What is the difference between artificial intelligence and computational intelligence? Is CI just a synonym for AI?
","['comparison', 'terminology', 'definitions']",
