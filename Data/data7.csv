Head,Body,Tags,First Answer
Help with deep Q learning for 2048 game getting stuck,"
I am having trouble making a reinforcement algorithm than can win the 2048 game.
I have tried with deep Q (which I think is the simplest algorithm that should be able to learn a winning strategy).
My Q function is given by a NN of two hidden layers 16 -> 8 -> 4. Weight initialization is XAVIER. Activation function is RELU. Loss function is cuadratic loss. Correction is via gradient descent.
To train the NN I used a reward given by :
$$r_t = \frac{1}{1024} \sum_{i=0}^{n}{p^i r_{((t-n)+i)}}$$
Where n is 20 or the amount of iterations since the last update if a game is lost and $p = 1.4$.
There is an epsilon for discovery, set at 100% at the start and it decreases by 10% until it reaches 1%.
I have tried to optimize the parameters but can't get better results than a ""256"" in the board. And the cuadratic loss seems to get stuck at 0.25:

Is there something I am missing?
Code:

public enum GameAction {
    UP, DOWN, LEFT, RIGHT
}

public final class GameEnvironment {

    public final int points;
    public final boolean lost;
    public final INDArray boardState;

    public GameEnvironment(int points, boolean lost, int[] boardState) {
        this.points = points;
        this.lost = lost;
        this.boardState = new NDArray(boardState, new int[] {1, 16}, new int[] {16, 1});
    }
}

public class SimpleAgent {
    private static final Random random = new Random(SEED);

    private static final MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(SEED)
            .weightInit(WeightInit.XAVIER)
            .updater(new AdaGrad(0.5))
            .activation(Activation.RELU)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .weightDecay(0.0001)
            .list()
            .layer(new DenseLayer.Builder()
                    .nIn(16).nOut(8)
                    .build())
            .layer(new OutputLayer.Builder()
                    .nIn(8).nOut(4)
                    .lossFunction(LossFunctions.LossFunction.SQUARED_LOSS)
                    .build())
            .build();
    MultiLayerNetwork Qnetwork = new MultiLayerNetwork(conf);

    private GameEnvironment oldState;
    private GameEnvironment currentState;
    private INDArray oldQuality;

    private GameAction lastAction;

    public SimpleAgent() {
        Qnetwork.init();
        ui();
    }

    public void setCurrentState(GameEnvironment currentState) {
        this.currentState = currentState;
    }

    private final ArrayList<INDArray> input = new ArrayList<>();
    private final ArrayList<INDArray> output = new ArrayList<>();
    private final ArrayList<Double> rewards = new ArrayList<>();

    private int epsilon = 100;

    public GameAction act() {
        if(oldState != null) {
            double reward = currentState.points - oldState.points;

            if (currentState.lost) {
                reward = 0;
            }

            input.add(oldState.boardState);
            output.add(oldQuality);
            rewards.add(reward);

            if (currentState.lost || input.size() == 20) {
                for(int i = 0; i < rewards.size(); i++) {
                    double discount = 1.4;
                    double discountedReward = 0;

                    for(int j = i; j < rewards.size(); j++) {
                        discountedReward += rewards.get(j) * Math.pow(discount, j - i);
                    }

                    rewards.set(i, lerp(discountedReward, 1024));
                }

                ArrayList<DataSet> dataSets = new ArrayList<>();

                for(int i = 0; i < input.size(); i++) {
                    INDArray correctOut = output.get(i).putScalar(lastAction.ordinal(), rewards.get(i));

                    dataSets.add(new DataSet(input.get(i), correctOut));
                }

                Qnetwork.fit(DataSet.merge(dataSets));

                input.clear();
                output.clear();
                rewards.clear();
            }

            epsilon = Math.max(1, epsilon - 10);
        }

        oldState = currentState;
        oldQuality = Qnetwork.output(currentState.boardState);

        GameAction action;


        if(random.nextInt(100) < 100-epsilon) {
            action = GameAction.values()[oldQuality.argMax(1).getInt()];
        } else {
            action = GameAction.values()[new Random().nextInt(GameAction.values().length)];
        }

        lastAction = action;

        return action;
    }

    private static double lerp(double x, int maxVal) {
        return x/maxVal;
    }

    private void ui() {
        UIServer uiServer = UIServer.getInstance();
        StatsStorage statsStorage = new InMemoryStatsStorage();
        uiServer.attach(statsStorage);
        Qnetwork.setListeners(new StatsListener(statsStorage));
    }
}

","['q-learning', 'feedforward-neural-networks']","
Q learning on its own isn't enough to learn a winning strategy for a game like 2048. 2048 requires predictive thinking for possible outcomes and good positional awareness. Performance of the agent is heavily dependent on the reward function. The approach to give the reward proportional to the obtained points after every move is naive since it might sacrifice better positional play for short term rewards. The way it's posed it seems like the agent will try to maximize its performance for the last 20 moves or so. That could lead an agent to the positional situation which leads to a loss in couple of moves. Possibly better strategy would be to give positive reward when the agent actually completes the 2048 tiles and negative reward if it loses. Such parse rewards would add difficulty to training since it would require sophisticated exploration strategies which $\epsilon$-greedy certainly isn't. The stochastic nature of the game would pose difficulty to the agent as well. Similar positions might lead to different outcomes because in some cases the new tile would spawn in a bad position for the continuation while in other cases it might be beneficial. The suggested approach would be to definitely include Monte Carlo tree search approach along with some RL algorithm which was successfully applied to agents like AlphaZero and AlphaGo. MCTS would sample moves ahead and agent would get better representation of how good certain actions are.
"
How does one prove comprehension in machines?,"
Say we have a machine and we give it a task to do (vision task, language task, game, etc.), how can one prove that a machine actually know's what's going on/happening in that specific task?
To narrow it down, some examples:
Conversation - How would one prove that a machine actually knows what it's talking about or comprehending what is being said? The Turing test is a good start, but never actually addressed actual comprehension.
Vision: How could someone prove or test that a machine actually knows what it's seeing? Object detection is a start, but I'd say it's very inconclusive that a machine understands at any level what it is actually seeing.
How do we prove comprehension in machines?
","['philosophy', 'agi', 'artificial-consciousness', 'chinese-room-argument']","
This is one of the most important issues in the philosophy of artificial intelligence. 
The most famous philosophical argument that attempts to address this issue is the Chinese Room argument published by the philosopher John Searle in 1980. 
The argument is quite simple. Suppose that you are inside a room and you need to communicate (in a written form) with people outside the room in a certain language that you do not understand (in the particular example given by Searle, Chinese), but you are given the rules to manipulate the characters of this language (for a given input, you have the rules to produce the correct output). If you follow these rules, to the people outside the room, it will seem as if you understand this language, but you don't. 
To be more concrete, when I say ""apple"", you understand that it refers to a specific fruit because you have eaten apples and you have a model of the world. That's understanding, according to Searle.
The most famous mathematical model of computers, the Turing machine, is essentially a system that manipulates symbols, so the Chinese Room argument directly applies to computers. 
Many replies or counterarguments to the CR argument have been discussed, such as

the system reply (the symbol manipulator is only a part of the larger system).
the robot reply (the symbol manipulator does not understand the meaning of the symbols because it has not experienced the associated real-world objects, so it suggests that understanding requires a body with sensors and controllers)
the brain simulator reply (the symbol manipulator can actually simulate the activity in the brain of a person that understands the unknown language)

So, can we prove that machines really understand? Even before Searle, Turing had already asked the question ""Can machines think?"". To prove this, you need a rigorous definition of understanding and thinking that people agree on. However, many people do not want to agree on a definition of intelligence and understanding (hence the many counterarguments to the CR argument). So, if you want to prove that machines understand, you need to provide a proof with respect to a specific definition of understanding. For example, if you think that understanding is just a side effect of symbol manipulation, you can easily prove that machines understand many concepts (it just follows from the definition of a Turing machine). However, even if understanding was just a side effect (what does a side effect actually mean in this case?) of symbol manipulation, would a machine be able to understand the same concepts and in the same way that humans understand? It's harder to answer this question because we really do not know if humans only manipulate symbols in our brains.
"
What is correct update when the some indexes are not available?,"
To update the Q table Q-learning takes the arg max of the Q values - the state, value mappings. 
For example, in tic tac toe the state XOX OXO -X- contains two available positions, each marked by the - character. In order to evaluate the arg max should temporal difference calculate the arg max of just the available positions? 
For the state XOX OXO -X- the arg max should be taken at index 6 and 8 ? (assuming zero indexing) If not then how should the arg max indexes be updated? The indexes 0,1,2 3,4,5 7 can be used as they are already been taken by X or O so should not be evaluated for their value ? This also means that indexes which are not available will not have their Q value updated, will this break the Q-learning procedure ?
","['reinforcement-learning', 'temporal-difference-methods']",
What are the applications of hierarchical softmax?,"
Apart from its use in word embeddings (e.g word2vec algorithm), are there any other applications of hierarchical softmax? If yes, can you please give me some reference papers?
","['reference-request', 'word2vec', 'hierarchical-softmax']",
How to understand the matrices used in the Attention layer?,"
Attention-scoring mechanism seems to be a commonly-used component in various seq2seq models, and I was reading about the original ""Location-based Attention"" in Bahadanau well-known paper at https://arxiv.org/pdf/1506.07503.pdf. (it seems this attention is used in various forms of GNMT and text-to-speech sythesizers like tacotron-2 https://github.com/Rayhane-mamah/Tacotron-2).
Even after repeated readings of this paper and other articles about Attention-mechanism, I'm confused about the dimensions of the matrices used, as the paper doesn't seem to describe it. My understanding is:

If I have decoder hidden dim 1024, that means ${s_{i-1}}$ vector is 1024 length.
If I have encoder output dim 512, that means $h_{j}$ vector is 512 length.
If total inputs to encoder is 256, then number of $j$ can be from 1 to 256.
Since $W x S_{i-1}$ is a matrix multiply, it seems $cols(W)$ should match $rows(S_{i-1})$, but $rows(W)$ still remain undefined. Same seems true for matrices $V, U, w, b$.

This is page-3/4 from the paper above that describes Attention-layer:

I'm unsure how to make sense of this. Am I missing something, or can someone explain this?
What I don't understand is:

What is the dimension of previous alignment (denoted by $alpha_{i-1})$? Shouldn't it be total values of $j$ in $h_{j}$ (which is 256 and means total different encoder output states)?
What is the dimension of $f_{i,j}$ and convolution filter $F$? (the paper says $F$ belongs to $kxr$ shape but doesn't define $'r'$ anywhere). What is $'r'$ and what does $'k x r'$ mean here?
How are these unknown dimensions for matrices $'V, U, w, b'$ described above determined in this model?

","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'transformer', 'attention']",
How can the target rely on untrained parameters?,"
I'm trying to understand DQN. I understand where the loss function comes from. I'm just unsure about why the target function works in practice. Given the loss function
$$ L_i(\theta_i) = [(y_i - Q(s,a;\theta_i))^2] $$
where
$$ y_i = r + \gamma * max_{a'}Q(s',a';\theta_{i-1}) $$
is the target value in the loss function.
From my understanding, experiences are pulled from the replay buffer, then the DQN is used to estimate the future sum of the discounted rewards for the next state (assuming it plays optimally) and adds this onto the current rewards $r$ to create the target value. Then the DQN is used again to estimate $Q$ value for the current state. Then the loss function is just the difference between the target and the estimated $Q$ value for the current state. Afterward, you optimize the loss function.
But, if the parameters of the DQN start off randomly, then surely the target value will be completely wrong since the parameters that define that target function are random. So, if the target function is wrong, then it will minimize the difference between the target value and the estimated value, but it will be learning to predict incorrect values, since the target values are wrong?
I don't understand why the target value works if the parameters of the DQN needed to create that target are completely random.
What obvious mistake am I making here?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'dqn', 'objective-functions']",
What is the difference between reinforcement learning and evolutionary algorithms?,"
What is the difference between reinforcement learning (RL) and evolutionary algorithms (EA)?
I am trying to understand the basics of RL, but I do not yet have practical experience with RL. I know slightly more about EAs, but not enough to understand the difference between RL and EA, and that's why I'm asking for their main differences.
","['reinforcement-learning', 'comparison', 'genetic-algorithms', 'optimization', 'evolutionary-algorithms']",
Are the final states not being updated in this $n$-step Q-Learning algorithm?,"
I am reading this paper and in algorithm 3 they describe an $n$-step Q-Learning algorithm. Below is the pseudo-code. 
 
From this pseudo-code, it looks as though the final tuples that they would visit in don't get added to the memory buffer $M$. They define a sample size $T$, but also say in the paper that an episode terminates when $|S| = b$. 
This leaves me two questions: 

Have I understood the episode termination correctly? It seems from the pseudocode they are just running an episode for $T$ time steps but also in the paper they have a definition for when an episode terminates, so I'm not sure why they would want to truncate the episode size. 
As I mentioned, it seems as though the final state $S_T$ that you would be in won't get added to the experience buffer as we only append the $(T-n)$th tuple. Why would you want to exclude the information you get from the final tuples you visit? 

","['reinforcement-learning', 'q-learning', 'deep-rl', 'dqn', 'papers']",
What is meant by the rank of the scoring function here?,"
I've been reading the paper Reinforcement Knowledge Graph Reasoning for Explainable Recommendation (by Yikun Xian et al.) lately, and I don't understand a particular section:

Specifically, the scoring function $f((r,e)|u)$ maps any edge $(r,e)$ to a real-valued score conditioned on user $u$. Then, the user-conditional pruned action space of state $s_t$ denoted by $A_t(u)$ is defined as:
$A_t(u) = \{(r,e)| rank(f((r,e)|u))) \leq \alpha ,(r,e) \in A_t\} $
where $\alpha$ is a predefined integer that upper bounds the size of the action space.

Details about the scoring function can be found in the attached paper.
What I don't understand is: What does rank mean, here? Is the thing inside of it a matrix?
It would be great if someone could explain the expression for the user conditional pruned action space in greater detail.
","['reinforcement-learning', 'papers', 'recommender-system', 'knowledge-graph', 'knowledge-graph-embeddings']",
Is there any programming practice website for beginners in Reinforcement Learning [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 3 years ago.







                        Improve this question
                    



I am doing an online course on Reinforcement Learning from university of Alberta. 
It focus too much on theory. I am engineering and I am interested towards applying RL to my applications directly. 
My question is, is there any website which has sample programmers for beginners. Small sample programs. 
I have seen several websites for other machine learning topics such as CNN/RNN etc. But the resources for RL are either limited, or I couldn't find them
",['reinforcement-learning'],
What is the most compressed audio that I can feed an AI?,"
The problem I currently have is that I want to train an AI to produce music, like music that contains voices etc... However, the problem is that with a WAV file, one second of audio can be up to 48,000 inputs, which is extremely detrimental to the ai's learning process and prevents it from really gaining any knowledge about context.
I've tried to do the fast Fourier transformation, but the amount of data coming in varies depending on what part of the song I'm training it on, which will not work since I can't know ahead of time what every single time unit of every single song will have!
And the max number of inputs is still 24,000
Is there any other way of compressing data down in a way that I can give my ai something within the range of a couple hundred inputs for a second or two?
","['machine-learning', 'audio-processing']",
Calculating the advantage 'gain' of actions in model-free reinforcement learning,"
I have a simple question about model-free reinforcement. In a model I'm writing about, I want to know the value 'gain' we'd get for executing an action, relative to the current state. That is, what will I get if I moved from the current state $s$ taking action $a$.
The measure I want is:
$$G(s, a)=V(s^{\prime})-V(s)$$
where $s'$ is the state that I would transition to if the underlying MDP was deterministic. If the MDP has a stochastic transition function, the model I want is:
$$G(s, a)=\left[\sum_{s' \in S } P(s^{\prime} \mid a, s) V(s^{\prime})\right]-V(s)$$
In a model-free environment, we don't have $P(s' \mid a,s)$.
If we had a Q-function $Q(s,a)$, could we represent $G(s,a)$?
NOTE: This is not the same as an 'advantage function' as first proposed by Baird (Leemon C Baird. Reinforcement learning in continuous time: Advantage updating. In Proceedings of 1994 IEEE International Conference on Neural Networks, pages 448–2453. IEEE, 1994.), which means the advantage of actions relative to the optimal action. What I'm looking for is the gain of actions relative to the current state.
",['reinforcement-learning'],
How to learn how to select a subgraph via reinforcement learning?,"
I have the following problem.
I am given a graph with a lot (>30000) nodes. Nodes are associated with a low (<10)-dimensional feature vector, and edges are associated with a low (<10)-dimensional feature vector. In addition, all nodes start out having the color white.
At every time step until completion, I want to select a subset of the nodes in the graph and color them blue. Then I receive a reward based on my coloring. I continue until all nodes are colored blue, and the total reward is the sum (maybe with a gamma factor) of my total rewards.
Do you have suggestions of papers to read where the task was choosing an appropriate subgraph from a larger graph? 
Just doing a node classification task using a Graph Convolutional Network doesn't seem to do well, I suspect because, given that a good heuristic for reward is connectivity, it would need to learn to choose an optimal neighborhood in the graph and upweight only that neighborhood.
To contextualize, each of the nodes of the graph represents a constraint that will be sent to an incremental SMT solver, and edges represent shared variables or other relationships between the constraints.  I have found empirically that giving these constraints incrementally to the SMT solver when in a good order can be faster than just dumping the entire problem into an SMT solver, since the SMT solver doesn't have the best heuristics for this particular SMT problem.  However, eventually, I want to add all the constraints, i.e., color the entire graph. The cost is the amount of time the solver takes on each set, with a reward at the end for completing all the constraints. 
","['reinforcement-learning', 'reference-request', 'graph-neural-networks', 'graphs']",
What are the real applications of hierarchical temporal memory?,"
What are the real applications of hierarchical temporal memory (HTM) in machine learning (ML) these days?
","['neural-networks', 'machine-learning', 'applications', 'research', 'htm']","
The following figure [1] is inspiring:

It means HTM can be used as a memory unit in different neural networks such as RNN. In the same reference, we can find the following advantages and applications of HTM:


Sequence learning:
  Being able to model temporally correlated patterns represents a key property of intelligence, as it gives both biological and artificial systems the essential ability to predict the future. It answers the basic question “what will happen next?” based on what it has seen before. Every machine learning algorithm should be able to provide valuable predictions not just based on static spatial information but also grounding it in time.
High-order predictions:
  Real-world sequences contain contextual dependencies that span multiple time steps, hence the ability to make high-order predictions becomes fundamental. The term “order” refers to Markov's order, specifically the minimum number of previous time steps the algorithm needs to consider in order to make accurate predictions. An ideal algorithm should learn the order automatically and efficiently.
Multiple simultaneous predictions:
  For a given temporal context, there could be multiple possible future outcomes. With real-world data, it is often insufficient to only consider the single best prediction when information is ambiguous. A good sequence learning algorithm should be able to make multiple predictions simultaneously and evaluate the likelihood of each prediction online. This requires the algorithm to output a distribution of possible future outcomes.
Continual Learning:
  Continuous data streams often have changing statistics. As a result, the algorithm needs to continuously learn from the data streams and rapidly adapt to changes. This property is important for processing continuous real-time perceptual streams, but has not been well studied in machine learning, especially without storing and reprocessing previously encountered data.
Online learning:
  For real-time data streams, it is much more valuable if the algorithm can predict and learn new patterns on-the-fly without the need to store entire sequences or batch several sequences together as it normally happens when training gradient-based recurrent neural networks. The ideal sequence learning algorithm should be able to learn from one pattern at a time to improve efficiency and response time as the natural stream unfolds.
Noise robustness and fault tolerance:
  Real world sequence learning deals with noisy data sources where sensor noise, data transmission errors and inherent device limitations frequently result in inaccurate or missing data. A good sequence learning algorithm should exhibit robustness to noise in the inputs.
No hyperparameter tuning:
  Learning in the cortex is extremely robust for a wide range of problems. In contrast, most machine-learning algorithms require optimizing a set of hyperparameters for each task. It typically involves searching through a manually specified subset of the hyperparameter space, guided by performance metrics on a cross-validation dataset. Hyperparameter tuning presents a major challenge for applications that require a high degree of automation, like data stream mining. An ideal algorithm should have acceptable performance on a wide range of problems without any task-specific hyperparameter tuning.


One of the applications of HTM is in Anomaly detection. See, for example, this paper Unsupervised real-time anomaly detection for streaming data (2017) which used from a network of HTMs to solve this problem in streaming data.
The implementation of HTM (NuPIC) can help to see more applications of HTM in ML.
"
Can the agent wait until the end of the episode to determine the reward in SARSA?,"
From Sutton and Barto's book Reinforcement Learning (Adaptive Computation and Machine Learning series) (p. 99), the following definition for first-visit MC prediction, for estimating $V \sim V_\pi$ is given:

Is determining the reward for each action a requirement of SARSA? Can the agent wait until the end of the episode to determine the reward? 
For example, the reward for the game tic-tac-toe is decided at the end of the episode, when the player wins, loses, or draws the match. The reward is not available at each step $t$. 
Does this mean then that depending on the task it is not always possible to determine reward at time step $t$, and the agent must wait until the end of an episode? If the agent does not evaluate reward until the end of an episode, is the algorithm still SARSA?
","['reinforcement-learning', 'definitions', 'rewards', 'sarsa']",
How do I keep my system (online) learning if I can get ground truth labels only for examples flagged positive?,"
I have a binary classifier (think of it as a content moderation system) that is deployed after having being trained via batch learning.
Once deployed, humans review and check for correctness only items predicted positive by the algorithm. 
In other words, once in production if I group predictions of the model on unseen examples in the confusion matrix 
+-----------+-----------------+
|           |   Ground-truth  |
|           +-----+-----------+
|           |     | Neg | Pos |
+-----------+-----+-----+-----+
|           | Neg | x11 | x12 |
| Predicted +-----+-----+-----+
|           | Pos | x21 | x22 |
+-----------+-----+-----+-----+


I have access to all the ground-truth labels of the elements counted in $x_{21}$, $x_{22}$ (the predicted-positive)
I know the sum of $x_{11}$ and $x_{12}$, but not their values
I do not have access to the ground-truth labels of the elements predicted-negative.

This (suboptimal) setup allows to measure precision $\frac{x_{22}}{x_{21} + x_{22}}$, while recall stays unknown as elements predicted negative are not examined at all (ground-truth labels of negatives can't be assigned due to resource constraints). 
Information gathered from users about the (true and false) positive elements can be used to feed a retraining loop... but

are there any ""smart"" learning recipes that are expected to make the algorithm improve its overall performance (say, the F1 score for the positive class) in this setting?
what's a meaningful metric to monitor to ensure that the performance of the model is not degrading?* (given the constraint specified here, F1 score is unknown).

Thanks for any hint on how to deal with this!
 * One solution could be to continuously monitor the F1 score on a labeled evaluation set, but maybe there's more one can do?
","['machine-learning', 'online-learning', 'algorithmic-bias']","
A first question that I think is important to consider is: do you expect the data that you're dealing with to be changing over time (i.e. do you expect there to be concept drift)? This could be any kind of change. Simply changes in how frequent certain inputs are, changes in how frequent positives/negatives are, or even changes in relations between inputs and ground-truth positive/negative labels.
If you do not expect there to be concept drift, I'd almost consider suggesting that you may not have that big of a problem. It might be worth not doing anything at all with the data you receive online, and just stick to what you learned initially from offline data. Or you could try to use those few extra predicted-positive samples that you get for finetuning. You'd just have to be careful not too change your model too much based on this, because you know that you're not receiving a representative sample of all the data anymore here, so you might bias your model if you pay too much attention to only this online data relative to the offline data.

I guess the question becomes much more interesting if you do expect there to be concept drift, and it also seems likely that you are indeed dealing with this in most of the situations that would match the problem description. In this case, you will indeed want to make good use of the new data that you get online, because it can allow you to adapt to changes in the data that you're dealing with.
So, one ""solution"" could be to just... ignore the problem that you're only learning online from a biased sample of all your data (only from the predicted-positives), and just learn anyway. This might actually not perform too badly. Unless your model is really incredibly good already, you'll likely still get false positives, and so also still be able to learn from some of those -- you're not learning exclusively from positives. Still, the false positives won't be representative of all the negatives, so you still have bias. 
The only better solution I can think of is relaxing this assumption:

Once deployed, humans review and check for correctness only items predicted positive by the algorithm. 

You can still have the humans focus on predicted positives, but maybe have them inspect a predicted-negative also sometimes. Not often, just a few times. You can think of this as doing exploration like you would in reinforcement learning settings. You could do it randomly (randomly pick predicted negatives with some small probability), but you could also be smarter about it and explicitly target exploration of instances that your model is ""unsure"" about, or instances that are unlike data you've seen before (to specifically target concept drift).
I have a paper about something very similar to this right here: Adapting to Concept Drift in Credit Card Transaction Data Streams Using Contextual Bandits and Decision Trees. Here the assumption is that we're dealing with (potentially fraudulent) transactions, of which we can pick out and manually inspect a very small sample online. The only real difference in this paper is that we assumed that different transactions also had different monetary ""rewards"" for getting correctly caught as positives, based on the transaction amount. So a transaction of a very high amount could be worth inspecting even if we predicted a low probability of being fraudulent, whereas a transaction of a very low amount might be ignored even if it had a higher predicted probability of being fraudulent.


what's a meaningful metric to monitor to ensure that the performance of the model is not degrading?* (given the constraint specified here, F1 score is unknown).

Having a labelled evaluation set for this could be useful if possible... but it also might not be representative if concept drift is expected to be a major issue in your problem setting (because I suppose that the concept drift that you deal with online would not be reflected in an older, labelled evaluation set).
Just keeping track of things that you can measure online, like precision, and how it changes over time, could be useful enough already. With some additional assumptions, you could get rough estimates of other metrics. For instance, if you assume that the ratio $\frac{TP + FN}{FP + TN}$ between ground-truth-positives and ground-truth-negatives remains constant (remains the same as it was in your offline, labelled data), you could also try to extrapolate approximately how many positives you've missed out on. If your precision is dropping over time (your true positives are getting lower), you know -- assuming that fraction remains constant -- that your false negatives somewhere else in the dataset must be growing by approximately the same absolute number.
"
"How can I draw a conceptual dependency for the statement ""Place all ingredients in a bowl and mix thoroughly""?","
I stumbled across a question asking to draw a conceptual dependency for the following statement:

Place all ingredients in a bowl and mix thoroughly

My attempt so far

Explanation: Both the sender and recipient are the same, except that the states of their contents are different. 
Something feels like it isn't right. I would appreciate if you could correct errors, if any.
","['knowledge-representation', 'conceptual-dependency']","
As it's two clauses (""place in bowl"" and ""mix""), I would actually use two separate CD structures. The first one is a PTRANS (not ATRANS, as you don't change ownership, but location); the second one is a bit trickier.
Mix can be paraphrased as stir, or move around. You would typically do this with a spoon or similar implement. I would do it as a PROPEL with the instrument spoon and the object the ingredients. PROPEL, according to Schank (1975) is the ""application of a physical force to an object"".
So the first one is:
(PTRANS
  (ACTOR *you*)
  (OBJECT ""all ingredients"")
  (TO ""bowl""))

And the second is:
(PROPEL
  (ACTOR *you*)
  (OBJECT ""ingredients"")
  (INSTRUMENT ""a spoon"")
  (DIRECTION ""circular""))

It's debatable if you PROPEL the ingredients with the instrument spoon, or you PROPEL the object spoon with the target/location/direction ingredients.
If you draw them in a graphical representation, you can have both ACTs leading off the shared actor '*you*'; or you could have them as separate graphs linked by 'then' or some other indicator of sequentiality.
Schank, Roger (1975) The primitive ACTs of conceptual dependency, Proc of 1975 workshop on theoretical issues in natural language processing, 34-37
"
How to exclude sections of bad data from time-series data before training an LSTM network,"
I am using LSTM network for predicting IOT time-series data receiving from un-reliable devices and networks.
This results in several multiple sections [continuous streak of bad data for several days until the problem is fixed].
I need to exclude this bad data section before feeding it to model training.
Since I am using LSTM-RNN network, it requires to do an un-roll data based on the previous records.   
How can I properly exclude this bad data?
I thought of an approach as training model separately using each batch of good data, and use subsequent good-data batch for fine-tuning the model.
Please let me know if this is a good approach? or is there a better method?
example data:
""1-01"",266.0
""1-02"",145.9
""1-03"",183.1
""1-04"",0  [bad data]
""1-05"",0  [bad data]
""1-06"",0  [bad data]
""1-07"",0  [bad data]
""1-08"",224.5
""1-09"",192.8
""1-10"",122.9
""1-11"",0  [bad data]
""1-12"",0  [bad data]
""2-01"",194.3
""2-02"",149.5

","['recurrent-neural-networks', 'long-short-term-memory', 'time-series']","
Your idea is a good one.
Another idea is to upsample or aggregate your data.  For example, average by week if you generally have a couple of missing days in every week.
A similar question on Stack Exchange:
https://stats.stackexchange.com/questions/374935/how-to-deal-with-really-sparse-time-series-data-for-a-binary-classification-task
"
What is a RAM state in the gym's breakout-ram environment?,"
I have encountered the gym environment and decided to create AI that plays breakout. Here is the link: https://gym.openai.com/envs/Breakout-ram-v0/.
The documentation says that the state is represented as a RAM state, but what is the RAM in this context? Is it the random access memory? What does the RAM state represent?
","['reinforcement-learning', 'deep-rl', 'implementation', 'gym']",
Can tabular Q-learning converge even if it doesn't explore all state-action pairs?,"
My understanding of tabular Q-learning is that it essentially builds a dictionary of state-action pairs, so as to maximize the Markovian (i.e., step-wise, history-agnostic?) reward. This incremental update of the Q-table can be done by a trade-off exploration and exploitation, but the fact remains that one ""walks around"" the table until it converges to optimality.
But what if we haven't ""walked around"" the whole table? Can the algorithm still perform well in those out-of-sample state-action pairs? 
","['machine-learning', 'reinforcement-learning', 'q-learning', 'exploration-exploitation-tradeoff']",
What is the main contribution of the paper Disentangling by Factorising?,"
Considering the paper Disentangling by Factorising, in addition to introducing a new model for Disentangled Representation Learning, FactorVAE (see figure), what is the main theoretical contribution provided by the paper? 

","['neural-networks', 'deep-learning', 'papers', 'variational-autoencoder', 'disentangled-representation']","
One of the core contributions presented in the paper consists of understanding at a deeper level the objective function used in Beta VAE and improving it
More specifically, the authors started from Beta VAE OF
$$\frac{1}{N} \sum_{i=1}^{N}\left[\mathbb{E}_{q\left(z | x^{(i)}\right)}\left[\log p\left(x^{(i)} | z\right)\right]-\beta K L\left(q\left(z | x^{(i)}\right) \| p(z)\right)\right]$$
which consists of the classical reconstruction loss and the regularization term which steers the Latent Code PDF towards a target PDF.
Developing the second term
$$\mathbb{E}_{p_{\text{data}}(x)}[K L(q(z | x) \| p(z))]=I(x ; z)+K L(q(z) \| p(z))$$
they observed there are 2 terms which push in different directions:
$I(x,z)$ is the mutual information between the input data and the code, and we do not want to penalize this as in fact we want this to be as high as possible, so that the decode contains as much information as possible about the input
$KL(q(z) || p(z))$ is the KL divergence term which if penalized pushes the latent code PDF $q(z)$ towards the prior $p(z)$ which we choose as factorized
So penalizing the second term in Beta VAE OF means also penalizing the mutual information hence ultimately loosing reconstruction capability
The authors then propose a new OF which fixes this
$$\frac{1}{N} \sum_{i=1}^{N}\left[\mathbb{E}_{q\left(z | x^{(i)}\right)}\left[\log p\left(x^{(i)} | z\right)\right]-K L\left(q\left(z | x^{(i)}\right)|| p(z)\right)\right] -\gamma K L(q(z) \| \bar{q}(z))$$
In the rest of the paper they run experiments and show they have improved the disentanglement of the representation (according to the new metric they propose in the paper) while preserving reconstruction hence improving the reconstruction vs disentanglement tradeoff with respect to the Beta VAE paper
"
Actor-Critic implementation not learning,"
I've implemented a vanilla actor-critic and have run into a wall. My model does not seem to be learning the optimal policy. The red graph below shows its performance in cartpole, where the algorithm occasionally does better than random but for the most part lasts between 10-30 timesteps.

I am reasonably sure that the critic part of the algorithm is working. Below is a graph of the delta value (r + Q_w(s',a') - Q_w(s,a)), which seems to show that for the most part the predicted future reward is quite similar to the approximate future reward.

Thus, I am at a loss for what the problem could be. I have an inkling it lies within the actor, but I am not sure. I have double checked the loss function and that seems to be correct to me as well. I would appreciate any advice. Thanks!
","['reinforcement-learning', 'actor-critic-methods']",
How to prevent deep Q-learning algorithms to overfit?,"
I have recently solved the Cartpole problem using double deep Q-learning. When I saw how the agent was doing, it used to go right every time, never left, and it did similar actions all the time.
Did the model overfit the environment? It seems that the agent just memorized the environment.
What are the common techniques to prevent the agent to overfit like that? Is that a common problem?
","['reinforcement-learning', 'q-learning', 'deep-rl', 'overfitting']",
How does SGD escape local minima?,"

SGD is able to jump out of local minima that would otherwise trap BGD

I don't really understand the above statement. Could someone please provide a mathematical explanation for why SGD (Stochastic Gradient Descent) is able to escape local minima, while BGD (Batch Gradient Descent) can't?
P.S.
While searching online, I read that it has something to do with ""oscillations"" while taking steps towards the global minima. What's that?  
","['optimization', 'gradient-descent', 'stochastic-gradient-descent']",
"If deep Q-learning starts to choose only one action, is this a sign that the algorithm diverged?","
I'm working on a deep q-learning model in an infinite horizon problem, with a continous state space and 3 possible actions. I'm using a neural network to approximate the action-value function. Sometimes it happens that, after a few steps, the algorithm starts choosing only one between the possible actions (apart from a few steps where I suppose it explores, given the epsilon-greedy policy it follows), leading to bad results in terms of cumulative rewards. Is this a sign that the algorithm diverged? 
","['reinforcement-learning', 'q-learning', 'deep-rl', 'convergence']",
How to make a LSTM network to predict sequence only after input sequence is finished?,"
I am learning to use a LSTM model to predict time series data. Specifically, I hope the network should output a sequence (with multiple time steps) only after the input sequence has finished feeding in, as shown in the left figure.

However, most of the LSTM sequence-to-sequence prediction tutorial I have read seems to be the right figure (i.e. each time step of the output sequence is generated after each time step of the input sequence). What's more, as far as I understand, the LSTM implementation in PyTorch (and probably Keras) can only return output sequence corresponding to each time step of the input sequence. It cannot make predictions after the input sequence is over. 
I hope to know is there any way to make a sequence-to-sequence LSTM network which starts output only after the input sequence has finished feeding in? And it would be better if someone can show me an example implementation code.
","['recurrent-neural-networks', 'long-short-term-memory']","
You should try an architecture with an encoder and a decoder. The encoder will consume all the data you give as in put and decoder will give out the series of output.
"
Why does the BERT NSP head linear layer have two outputs?,"
Here's the code in question. 
https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491
class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score

I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?
","['machine-learning', 'natural-language-processing', 'transformer', 'bert']",
Is the distribution of state-action pairs from sample based planning accurate for small experience sets?,"
From the David Silver's lecture 8: Integrating Learning and Planning - based on Sutton and Barto - he talks about using sample-based planning to use our model to take a sample of a state and then use model-free planning, such as Monte Carlo, etc, to run the trajectory and observe the reward. He goes on to say that this effectively gives us infinite data from only a few actual experiences.
However, if we only experience a handful of true state-action-rewards and then start sampling to learn more then we will surely end up with a skewed result, e.g., If I have 5 experiences but then create 10000 samples (as he says, infinite data). I am aware that as the experience set grows the Central Limit Theorem will come into play and the distribution of experience will more accurately represent the true environment's state-actions-rewards distribution but before this happens is sampled based planning still useful?
","['reinforcement-learning', 'planning']",
What is the effect of using pooling layers in CNNs?,"
I know how pooling works, and what effect it has on the input dimensions - but I'm not sure why it's done in the first place. It'd be great if someone could provide some intuition behind it - while explaining the following excerpt from a blog:

A problem with the output feature maps is that they are sensitive to the location of the features in the input. One approach to address this sensitivity is to down sample the feature maps. This has the effect of making the resulting down sampled feature maps more robust to changes in the position of the feature in the image, referred to by the technical phrase “local translation invariance.”

What's local translation invariance here?
","['neural-networks', 'convolutional-neural-networks', 'pooling']","
In addition in general it somewhat aides in detection as only the strongest feature feature filter is activated so in a sense it removes additional information.
But it obviously has draw backs resulting in combinations of features being detected which aren't actual.objects.
"
What is the intuition behind the Xavier initialization for deep neural networks?,"

The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network

I am really having trouble understanding weights initialization technique and Xavier Initialization for deep neural networks (DNNs). 
In simple words (and maybe with an example), what is the intuition behind the Xavier initialization for DNNs? When should we use Xavier's initialization?
","['neural-networks', 'deep-learning']",
What is an auto-associator?,"
What is an auto-associator, and how does it work? How can we design an auto-associator for a given pattern? I couldn't find a clear explanation for this anywhere on the internet.
Here's an example of a pattern.

","['neural-networks', 'math', 'hebbian-learning']",
Does anyone know of a model for comparing the eyes of people in two images to see if they match?,"
There’s a lot of talk of undercover cops intentionally starting violence in otherwise peaceful protests. The evidence, primarily, are images like this.  
https://images.app.goo.gl/4n3o2EXwFzMQfsKq6 
It looks pretty convincing, but I’d like something more solid. Does anyone know of a model that can detect with a high level of certainty if the “mask” area of two photos represents the same person?
","['image-recognition', 'pattern-recognition', 'facial-recognition']","
The model you are looking for is probably a Siamese Neural Networks.
It's very used for Biometrics. This kind of model learns to extract relevant and features (For example, facial metrics) that are invariant on several conditions (that won't change on light, angle, or even occlusions) presented on the dataset.
But keep in mind no model is an Scy-Fi AI, like FBI movies enhancing the image 1000x. There is natural limitation, so maybe not even the best AI possible could give you a satisfactory answer.
"
"Why is learning $s'$ from $s,a$ a kernel density estimation problem but learning $r$ from $s,a$ is just regression?","
In David Silver's 8th lecture he talks about model learning and says that learning $r$ from $s,a$ is a regression problem whereas learning $s'$ from $s,a$ is a kernel density estimation. His explanation for the difference is that if we are in a stochastic environment and we are in the tuple $s,a$ then there might be a 30% chance the wind will blow me left, and a 70% chance the wind will blow me right, so we want to estimate these probabilities. 
Is the main difference between these two problems, and hence why one is regression and the other is kernel density estimation, because with the reward we are mainly concerned with the expected reward (hence regression) whereas with the state transitioning, we want to be able to simulate this so we need the estimated density? 
","['reinforcement-learning', 'markov-decision-process', 'model-based-methods']",
Why is this GAN not converging?,"
This GAN being trained with CelebA dataset doesn't seem to mode collapse, discriminator is not really over confident, and yet the quality is stuck on these rough Picasso-like generator images. Using Leaky-ReLU, strided conv instead of maxpool, and dampened truths helped a little, but still no better than this. Not sure what else to try. training clip Discriminator feed is in top left corner. 

","['deep-learning', 'generative-adversarial-networks']",
Visualisation for Features to Predict Timeseries Data,"
I have a course assignment to use an LSTM to predict the movement directions of stock prices. One of the things I am asked to do is provide a visualization to compare the predictive powers of a set of N features (e.g. 1-day return, volatility, moving average, etc.). Let's assume that we use a window of 50 days as input to the LSTM.
The first thing that came to my mind is to use a RadViz plot (check below image taken from https://www.scikit-yb.org/en/latest/api/features/radviz.html). 
However, I soon realized this will not work for the features since each sample will have 50 values. So if we have M samples, the shape of the input data will be something like Mx50xN. This, unfortunately, is not something RadViz can deal with (it can handle 2D data). 
Given this, I would be grateful if someone can point me to a viable way to visualize the data. Is it even possible when each feature comprises 50 values?
","['long-short-term-memory', 'feature-selection', 'feature-extraction', 'feature-engineering']",
"Deep learning techniques with time-fixed, time-dependent and imaging data","
I have a question about the use of deep learning techniques with time-fixed features and images (setting 1) and time-dependent features (setting 2). (I am pretty new to the deep learning world so please excuse me if it's a basic question.) 
Setting 1:
Imagine having a training dataset composed of 

some time-fixed features such as height, weight, and age of an individual at the first medical visit (these features are recorded once and therefore time-fixed, i.e., they do not change in time in the dataset). 
some medical images for each individual, such as for example a CT scan. 
a label defining if the patient has or not a specific disease.

Setting 2: 
Same as setting 1 but with some features that are repeated over time (time-dependent, longitudinal), such as for example blood pressure recorded twice a day for each individual for several days.
Let say that the goal is to classify if an individual has or not a specific disease given the aforementioned features. 
I have seen zillions of papers and blogs talking about convolutional neural networks to classify images and a few million about recurrent neural networks for time-dependent features. However, I am not very aware of what to use in case I have time-fixed, time-dependent, and imaging features altogether. 
I am wondering how you would attach this problem.
","['deep-learning', 'convolutional-neural-networks', 'recurrent-neural-networks', 'structured-data']",
Is there any good reference for double deep Q-learning?,"
I am new in reinforcement learning, but I already know deep Q-learning and Q-learning. Now, I want to learn about double deep Q-learning. 
Do you know any good references for double deep Q-learning? 
I have read some articles, but some of them don't mention what the loss is and how to calculate it, so many articles are not complete. Also, Sutton and Barto (in their book) don't describe that algorithm either. 
Please, help me to learn Double Q-learning.
","['reinforcement-learning', 'q-learning', 'reference-request', 'deep-rl']","
You should first read the introductory paper of Double DQN.
https://arxiv.org/abs/1509.06461
Then, depending on what you would like to do, search for other relevant papers that use this method.
I also propose studying the original Double Q-learning paper to understand important concepts/issues, such as the overestimation bias of Q-learning: https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf
"
Is there a way of deriving a loss function given the neural network and training data?,"
There is some sort of art to using the right loss function. However, I was wondering if there is a way to derive the loss function if I gave you a neural network model (the weights) as well as the training data. 
The point of this exercise is to see what family of loss functions we would get. And how that compares to the loss function that actually gave rise to the model.
","['neural-networks', 'machine-learning', 'training', 'objective-functions']",
Incorporating domain knowledge into recurrent network,"
I am currently trying to solve a classification task with a recurrent artificial neural network (RNN).
Situation
There are up to 350 inputs (X) mapped on one categorical output (y)(13 differnt classes).
The sequence to predict is deterministic in the sense that only specific state transitions are allowed based on the past. A simplified Abstraction of my problem:

y - Ground Truth: 01020
y - Model Prediction: 01200
Valid Transitions: 01, 10, 02, 20

The predicted transition 12 is consequently not valid.
Question
What would be the best way to optimize a model to make as less invalid transition predictions as possible (ideally none)?
(temporal shifts of the predictions in comparison to the ground truth are still acceptable)

By the integration of the knowledge about the valid transitions into the artificial neural network. Is this even possible to code hard restrictions into a RNN?
By a custom loss function which penalizes these invalid transitions 
Another approach

Current Approach
With a bidirectional recurrent network (one gated recurrent units layer (2000 neurons)) in a many to many fashion an accuracy of 99.5% on the training and 97.5% on the test set could be reached (Implemented with TensorFlow / keras 2.2)
","['deep-learning', 'classification', 'tensorflow', 'objective-functions', 'gated-recurrent-unit']",
How do you prove that minimax algorithm outputs a subgame-perfect Nash equilibrium?,"
At every node, MAX would always move to maximise the minimum payoff while MIN choose to minimise the maximum payoff, hence there is nash equilibrium. 
By using backwards induction, at every node, MAX and MIN player would act optimally. Hence, there is subperfect nash equilibrium.
How do I formally prove this?   
","['proofs', 'game-theory', 'minimax']",
How to effectively crossover mathematical curves?,"
I'm trying to optimize some reflective properties of curves of the form:
$a_1x^n+a_2x^{n-1}+a_3x^{n-2} + ... + a_n + b_1y^n+b_2y^{n-1}+b_3y^{n-2} + ... + b_n = 0$
which is basically the curve that you get when you sum two polynomials of same degree in different variables:
$f(x) + g(y) = 0$
Anyways, I was wondering what would be a good way to do crossover on two such curves? I tried averaging the curves and then mutating them, but the problem is that the entire population quickly becomes homogeneous and the fitness starts to drop. Another typical method I tried is taking a cutoff point somewhere on the expansion above and mixing the left and right part from both parents.
Of course there are many ways to do a similar process. I could order the above expansion differently and then do a cutoff. I could seperate Xs and Ys and do two seperate cutoffs. Etc. The question is: in the context of algebraic curves, which method of generating offspring would be a good option considering I want to optimize some property of the curve (in this case I want it to have some reflective properties)?
","['genetic-algorithms', 'evolutionary-algorithms', 'crossover-operators']",
What are finite horizon look-ahead policies in reinforcement learning?,"
I was reading the paper How to Combine Tree-Search Methods in Reinforcement Learning published in AAAI Conference 2019. It starts with the sentence 

Finite-horizon lookahead policies are abundantly used in Reinforcement Learning and demonstrate impressive empirical success.

What is meant by ""finite horizon look-ahead""?
","['reinforcement-learning', 'terminology', 'papers']",
How should I decay $\epsilon$ in Q-learning?,"
How should I decay the $\epsilon$ in Q-learning? 
Currently, I am decaying epsilon as follows. I initialize $\epsilon$ to be 1, then, after every episode, I multiply it by some $C$ (let it be $0.999$), when it reaches $0.01$. After that, I keep $\epsilon$ to be $0.01$ all the time. I think this has a terrible consequence. 
So, I need a $\epsilon$ decay algorithm. I haven't found script or formula about it, so can you tell me?
","['reinforcement-learning', 'q-learning', 'deep-rl']",
Understanding the role of the target network in this DQN algorithm,"
I've found online this interesting algorithm:

From what I understand reading this algorithm, I can't figure out why I should ""perform the opposite action"" and consequently storing that second experience as then it is never used to update or doing anything else. Is this algorithm incorrect?
","['reinforcement-learning', 'dqn', 'deep-neural-networks']",
"Which one is more important in case of different loss optimization algorithms, Speed or the Route?","
We have different kinds of algorithms to optimize the loss like AdaGrad, SGD + Momentum, etc. Some are more commonly used than the others. In some algorithms, they usually range out before they converge, reach to the steepest slope and find the minima. But some of these algorithms are significantly fast. So my question is that the speed is more of a deciding factor here or the route is important too? Or is it just problem dependent?

Here is a picture of what I mean by the Route.
","['convolutional-neural-networks', 'optimization']",
Training Conditional DCGAN with GAN-CLS loss,"
I am trying to implement conditional GAN using GAN-CLS loss as described in paper: https://arxiv.org/abs/1605.05396
So, while training discriminator, I should I have three batches of data:

[Real_Image, Embeddings] 
[Generated_Image, Embeddings]
[Wrong_Image,Embeddings]

And, while training generator, I should have one batch of data i.e [Generated_Image, Embeddings].
Is this correct way to train the model?
","['machine-learning', 'deep-learning', 'generative-adversarial-networks', 'discriminator']",
Simplifying Log Loss,"
I am reading through a paper (https://www.mitpressjournals.org/doi/pdf/10.1162/0891201053630273) where they describe logloss as a ranking function and can be simplified to the margin of the training data $X$. I am not sure what the transformation is in each step and could use a bit of help.
A precursor to this ranking loss is standard logloss which may clarify my understanding as well:
In this loss I only get from step 2 to here:
$$-\sum_{i=1}^n log(\frac{{e^{y_iF(x_i,\overline{a})}}}{1 + e^{yF(x,\overline{a})}})$$
$$=-\sum_{i=1}^ny_iF(x_i, \overline{a}) - log(1 + e^{yF(x,\overline{a})})$$

And here is the full ranking loss I am having trouble on:

","['math', 'objective-functions', 'optimization', 'linear-algebra']",
How could a NN be trained to output a cyclic (e.g. hue) number?,"
I was thinking about training a neural network to colourize images. The input would be the luminosity/value for each pixel, and the output would be a hue and/or saturation. Training data would be easily obtained just by selecting the luminosity/value channel from a full colour image.
Suppose all channels are scaled to 0.0-1.0, there is a problem with pixels whose hue is nearly 0.0 or nearly 1.0.

The input data may have sharp discontinuities in hue which are not visible to the human eye. This is an unstable, illusory boundary which seems like it would destablilize the training.
Also if the network outputs a value of 1.001 instead of 0.001 then this should NOT be penalised since.

Possible workarounds might be to preprocess the image to remap e.g. 0.99 to -0.01 if that pixel is near a region dominated by near-0 hues, or similarly to to remap e.g. 0.01 to 1.01 if that pixel is near a region dominated by near-1 hues. This has its own problems. Similarly, outputs could be wrapped to the range 0-1 before being scored.
But is there a better way to encode cyclic values such as hue so that they will naturally be continuous?
One solution I thought of would be to treat (hue,saturation) as a (theta,r) polar coordinate and translate this to Cartesian (x,y) and have that be the training target, but I don't know how this change of colour space will affect things (it might be find, I haven't tried it yet).
Are there alternative colour representations which are better suited to machine learning?
","['neural-networks', 'machine-learning']","
Your solution is pretty much on spot. It corresponds to the YUV scheme used in television and designed to match human perception characteristics. As you already noticed, such an encoding wouldn't suffer from discontinuities.
"
Why do we use a delay when feeding our input data to the echo state network?,"
I'm new to working with neural networks and have recently began implementing neural networks for time series forecasting in some of my work. I've been particularly using Echo State Networks and have been doing some reading to understand how they work. For the most part, things seem pretty straight forward, but I'm confused as to why we use a 'delay' when feeding our input data (the delay concept mentioned in the paper Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication)? 
I'm looking at some source code on Github, and they implement this delay as well (they feed two arrays inputData & targetData, into the network where one is delayed by one element relative to the other). I am noticing that the larger the delay, the worse the fit.
Why is this done? 
My interest is eventually to forecast past sample data.
","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'time-series', 'echo-state-network']","
It is not a 'delay' but a 'look ahead' as some people call it.  This is how far in the future the model can forecast (or predict).
In reference to the GitHub source, the code uses a look ahead ('delay') of one.  The echo state network learns on the 'inputData' using a future version of itself called 'targetData'.  
Here is an analogy.  It is early on Friday, June 5, and I have the last 51 days worth of Amazon stock prices.  I can use the first 50 days of prices, which go up to and include Wednesday, June 3, to train a network to predict Thursday, June 4.  If I have a lot of historical data on Amazon, I can train many 50 day windows.  Each 50 day window would have as its target a stock price one day in the future. 
Performance generally degrades the further into the future you try to forecast.  One reason is due to the build up of error over time.  This is true in life.  We usually know fairly accurately what we will do tomorrow, less so next week, and it would be a wild guess to predict what we will be doing in ten years.
"
Is there a classification task with multiple attribute regression?,"
I'm trying to look for a task that predicts a discrete label first (classification), and then predicts the multiple continuous attributes of the predicted class. I found some papers about multi-output regression, but it wasn't what I wanted. Perhaps such a task exists in robot control or in video games, but I haven't found it yet. At the same time I want to train within a supervised learning framework, not reinforcement learning.
","['classification', 'regression', 'supervised-learning']",
Is this proof of $\epsilon$-greedy policy improvement correct?,"
The following paragraph about $\epsilon$-greedy policies can be found at the end of page 100, under section 5.4, of the book ""Reinforcement Learning: An Introduction"" by Richard Sutton and Andrew Barto (second edition, 2018).

but with probability $\varepsilon$ they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, $\frac{\varepsilon}{|\mathcal{A}(s)|}$, and the remaining bulk of the probability, $1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}(s)|}$, is given to the greedy action. The $\varepsilon$-greedy policies are

So, the non-greedy actions are given the probability  $\frac{\varepsilon}{|\mathcal{A}(s)|}$, and the greedy action is given the probability $1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}(s)|}$. All clear up to this point.
However, I have a doubt in the policy improvement theorem that is mentioned in page 101, under section 5.4. I have enclosed a copy of this proof for your convenience:
$$
\begin{aligned}
q_{\pi}(s, \pi^{\prime}(s)) &=\sum_{a} \pi^{\prime}(a \mid s) q_{\pi}(s, a) \\
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \max _{a} q_{\pi}(s, a) \\
& \geq \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \sum_{a} \frac{\pi(a \mid s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon} q_{\pi}(s, a)\\
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)-\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\sum_{a} \pi(a \mid s) q_{\pi}(s, a) \\
&=v_{\pi}(s) .
\end{aligned}
$$
My question is shouldn't the greedy action be chosen with a probability of $1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}(s)|}$?
The weighing factors do not add up to 1, as they are probability values. With this argument, the proof (with a slight modification) would be:
$$
\begin{aligned}
q_{\pi}(s, \pi^{\prime}(s)) 
&=\sum_{a} \pi^{\prime}(a \mid s) q_{\pi}(s, a) \\
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+ \left( 1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}(s)|} \right) \max _{a} q_{\pi}(s, a) \\
& \geq \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\left(1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}(s)|}  \right) \sum_{a} \frac{\pi(a \mid s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}(s)|}  } q_{\pi}(s, a)\\
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)-\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\sum_{a} \pi(a \mid s) q_{\pi}(s, a) \\
&=v_{\pi}(s) .
\end{aligned}
$$
Though the end result isn't changed, I just want to know what I am conceptually missing, in order to understand the proof that is originally provided.
","['reinforcement-learning', 'proofs', 'epsilon-greedy-policy', 'policy-improvement-theorem']",
How and when should we update the Q-target in deep Q-learning?,"
I have recently watched David silver's course, and started implementing the deep Q-learning algorithm. 
I thought I should make a switch between the Q-target and Q-current directly (meaning, every parameter of Q-current goes to Q-target), but I found a repository on GitHub where that guy updates Q-target as follows:
$$Q_{\text{target}} = \tau * Q_{\text{current}} + (1 - \tau)*Q_{\text{target}}$$.
where $\tau$ is some number probably between 0 and 1. 
Is that update correct or I miss something? 
I thought after some iterations (e.g. 2000 iteration), we should update Q-target as: $Q_{\text{target}}=Q_{\text{current}}$.
","['deep-learning', 'q-learning', 'deep-rl', 'dqn']",
"Is there any good source for when the pole actually starts all the way at the bottom, in the cartpole problem?","
There are a lot of examples of balancing a pole (see image below) using reinforcement learning, but I find that almost all examples start close to the upright position.
Is there any good source (or paper) for when the pole actually starts all the way at the bottom? 

","['reinforcement-learning', 'reference-request']","
It is difficult to prove a negative, but I doubt there will be a paper on that specific problem. It should be relatively easy to adjust the environment or write a new one that does this if you wished though.
A very similar environment that does have a lot more written about it is Acrobot, which does have a OpenAI Gym version. Instead of a cart on a track with forces applied and a free join to the pole, there is a longer pole fixed to a free joint, with an active joint in the middle (that the agent can apply forces to). It can be thought of as a very simple model of an acrobat on a trapeze swing (with poles instead of chains, so the swing is stable balanced upside-down)
The degrees of freedom and difficulty of the Acrobot task are similar to CartPole - I would rate it as a harder problem overall, but if you started CartPole with the pole hanging down in fact the two problems are very similar. Usually the joint motor is made too weak to achieve the goal of balancing in a single clean action, and the agent must learn to build momentum over a few swings, before moving to the balance point. That also makes both your alternate start position CartPole and Acrobot similar to MountainCar.
For a low-effort look at a very similar problem you could try Acrobot. Otherwise you will likely need to do some custom work on CartPole.
"
Using a model-based method to build an accurate day trading environment model,"
There are several different angles we can classify Reinforcement Learning methods from. We can distinguish three main aspects :

Value-based and policy-based
On-policy and off-policy
Model-free and model-based

Historically, due to their sample-efficiency, the model-based methods have been used in the robotics field and other industrial controls. That is happened due to the cost of the hardware and the physical limitations of samples that could be obtained from a real robot. Robots with a large number of degrees of freedom are not widely accessible, so RL researchers are more focused on computer games and other environments where samples are relatively cheap. However, the ideas from robotics are infiltrating, so, who knows, maybe the model-based methods will enter the focus quite soon. 
As we know, ""model"" means the model of the environment, which could have various forms, for example, providing us with a new state and reward from the current state and action. From what I have seen so far, all the methods (i.e. A3C, DQN, DDPG) put zero effort into predicting, understanding, or simulating the environment. What we are interested in is proper behavior (in terms of the final reward), specified directly (a policy) or indirectly (a value), given the observation. The source of observations and reward is the environment itself, which in some cases could be very slow and inefficient. 
In a model-based approach, we're trying to learn the model of the environment to reduce the ""real environment"" dependency. If we have an accurate environment model, our agent can produce any number of trajectories that it needs, simply by using the model instead of executing the actions in the real world. 
Question: 
I am interested in a day trading environment. Is it possible to create a model-based environment in order to build an accurate day trading environment model?
","['reinforcement-learning', 'policy-gradients', 'model-based-methods']",
Can you find another reason for sample inefficiency of model-free on-policy Deep Reinforcement Learning?,"
The following mindmap gives an overview of multiple reasons for sample inefficiency. The list is definitely not complete. Can you see another reason not mentioned so far?

Some related links:

interactive mindmap
Reddit post asking the same question
related SE question with only a few reasons all mentioned here

","['deep-learning', 'reinforcement-learning', 'deep-rl', 'sample-efficiency', 'sample-complexity']",
Why do we also need to normalize the action's values on continuous action spaces?,"
I was reading here tips & tricks for training in DRL and I noticed the following:


always normalize your observation space when you can, i.e., when you know the boundaries
normalize your action space and make it symmetric when continuous (cf potential issue below) A good practice is to rescale your actions to lie in [-1, 1]. This does not limit you as you can easily rescale the action inside the environment


I am working on a discrete action space but it is quite difficult to normalize my states when I don't actually know the full range for each feature (only an estimation).
How does this affect training? And more specifically, why on continuous action spaces we need to normalize also the action's values?
","['reinforcement-learning', 'training', 'deep-rl', 'a3c']",
Is it possible to create a fair machine learning system?,"
I started thinking about the fairness of machine learning models recently. 
Wiki page for Fairness_(machine_learning) defines fairness as:

In machine learning, a given algorithm is said to be fair, or to have
  fairness if its results are independent of some variables we consider
  to be sensitive and not related to it (f.e.: gender, ethnicity,
  sexual orientation, etc.).

UC Berkley CS 294 in turn defines fairness as:

understanding and mitigating discrimination based on sensitive
  characteristics, such as, gender, race, religion, physical ability,
  and sexual orientation

Many other resources, like Google in the ML Fairness limit the fairness to these aforementioned categories and no other categories are considered. 
But fairness is a lot broader context than simply these few categories mentioned here, you could easily add another few like IQ, height, beauty anything that could have a real impact on your credit score, school application or job application. Some of these categories may not be popular in existing datasets nowadays but given the exponential growth of data, they will be soon, to the extent, that we will have an abundance of data about every individual with all their physical and mental categories mapped into the datasets.
Then the question would be how to define fairness given all these categories presented in the datasets. And will it even be possible to define fairness if all physical and mental dimensions are considered as it seems that when we do so, all our weights in, say, the neural nets should be exactly the same, i.e., giving no discriminator in any way or form towards or against any physical or mental category of a human being? That means that a machine learning system that is fair across all possible dimensions will have no way of distinguishing one human being from another which would render these machine learning models useless. 
To wrap it up, while it does make perfect sense to withdraw bias towards any individual given the categories like gender, ethnicity, sexual orientation, etc., the set is not closed and with the increasing number of categories being added to this set, we will inevitably arrive at a point where no discrimination (in a statistical sense) would be possible. 
And that's why my question, are fair machine learning models possible?
Or perhaps, the only possible fair machine learning models are those that arbitrarily include some categories but ignore other categories which, of course, if far from being fair.
","['machine-learning', 'explainable-ai']",
Low accuracy during training for text summarization,"
I am trying to implement an extractive text summarization model. I am using keras and tensorflow. I have used bert sentence embeddings and the embeddings are fed into an LSTM layer and then to a Dense layer with sigmoid activation function. I have used adam optimizer and binary crossentropy as the loss function. The input to the model are the sentence embeddings.
The training y labels is a 2d-array i.e [array_of_documents[array_of_biniary_labels_foreach_sentence]]
The problem is that during training, I am getting the training accuracy of around 0.22 and loss 0.6.
How can I improve my accuracy for the model?
","['neural-networks', 'natural-language-processing', 'tensorflow', 'training', 'accuracy']",
Is there a general file type associated with AI projects?,"
This is a general question. 
Is there a general file type associated with AI projects?
Photoshop = .psd
Excel = csv
Artificial Intelligence = ?

","['open-ai', 'implementation']","
No, there is no file type associated with AI projects in general.
Your examples of Photoshop and Excel are specific corporate branded products. These store bespoke data that only works with those products (plus maybe a few converters that can read the files for competitor products).
Even more general examples such as .jpg for images or .txt for text documents are not a good match to AI in general. AI is such a broad field, it is next to impossible to define a standard set of components of an ""AI project"" in order to build a single file format that could handle the contents of all AI projects.
That said, practical work in AI is likely to include use of specific file formats and extensions, depending on what you do. For instance if you work with Python and TensorFlow you are likely to use .py files for your source code, .ckpt for neural network training checkpoints, .pb for saved models. The last two extensions - ckpt and pb - are only semi-formal naming conventions though. 
This lack of a single file extension, and use of maybe a dozen ones that you would be familiar with on a project, is true of most coding and software development work. If you work with an integrated devlopment environment (IDE), then you might have a single ""master"" file that allows you to load up all the resources in the project to work on them. That is entirely optional though, and may have any of a number of extensions depending on which IDE you use.
As a quick example, the Transformers project on github is a popular resource for natural language processing, a topic often considered as part of AI. A quick look at that project shows:

Various development tool configuration files either starting with . or with various extensions - .cfg, .yaml
.py files for Python source code
Script files with .sh extension or with extension removed for convenience of using on command line
Dcoumentation written with .txt, .md and .rst extensions.
Some .ipynb files used for Jupyter notebooks - a format that bundles documentation, Python scripts and their output for sharing work.

"
Can weighted importance sampling be applied to off-policy evaluation for continuous state space MDPs?,"
Can weighted importance sampling (WIS) and importance sampling (IS) be applied to off-policy evaluation for continuous state spaces MDPs? 
Given that I have trajectories of $(s_t,a_t)$ pairs and the behavior policy distribution $\pi_b(a_t | s_t)$ can be approximated with a neural network. 
A paper I came across where they say that IS can be used for continuous states whereas WIS cannot be used for function approximation. I am not sure why WIS cannot be applied to the continuous case whereas IS can be. Both of these techniques seem similar. 
","['reinforcement-learning', 'off-policy-methods', 'importance-sampling']",
Why is the accuracy of my model very low on a separate dataset from the training and test datasets?,"
I am working on stock price prediction project, I am using the support vector regression (SVR) model for it.
As I am splitting my data into train and test, I am getting high accuracy while predicting test data after fitting the model.
But now when I am trying to use another data which I separate out from the used dataset before start doing anything, it gives me very bad results. Can anyone tell me what's happening?

Looking forward to your response.
","['machine-learning', 'data-science', 'support-vector-machine', 'accuracy']",
Banding artifacts in CNN,"
I was working on a CNN for HDR image generation from LDR images. I used an encoder-decoder architecture and merged the input with the decoder output. However I'm getting some banding artifacts in the model prediction as shown.
1)Input LDR image

2) Ground Truth HDR

3) Predicted output

Notice the fine bands in the prediction. What might be causing these bands? Also I trained only for 20 epochs yet. Is the problem due to inadequate training?
Here's my model:
 class TestNet2(nn.Module):
    def __init__(self):
        super(TestNet2, self).__init__()

    def enclayer(nIn, nOut, k, s, p, d=1):
        return nn.Sequential(
            nn.Conv2d(nIn, nOut, k, s, p, d), nn.SELU(inplace=True)
        )
    def declayer(nIn, nOut, k, s, p):
        return nn.Sequential(
            nn.ConvTranspose2d(nIn, nOut, k, s, p), nn.SELU(inplace=True)
        )

    self.encoder = nn.Sequential(
        enclayer(3,64,5,3,1),
        #nn.MaxPool2d(2, stride=2),
        enclayer(64,128,5,3,1),
        #nn.MaxPool2d(2, stride=2),
        enclayer(128,256,5,3,1),
        #nn.MaxPool2d(2, stride=2),
    )
    self.decoder = nn.Sequential(
        declayer(256,128,5,3,1),
        declayer(128,64,5,3,1),
        declayer(64,3,5,3,1),


    )

  def forward(self, x):
      xc=x
      x = self.encoder(x)
      x = self.decoder(x)
      x = F.interpolate(
        x, (512, 512), mode='bilinear', align_corners=False
      )
      x=x+xc
      return x

","['convolutional-neural-networks', 'image-processing', 'image-generation']",
How to use one-hot encoding for multiple columns (multi-class) with varying number of labels in each class?,"
I am a beginner in TensorFlow as well as in AI. I am basically from Pharma background and learning AI from scratch.
I have data with 5038 input (Float64) and 826 output (Categorical - Multi Labels in each column). I have utilized one-hot encoding but the neural network tackles only one output at a time. 
[1]How to process all 826 output (which give 6689 one-hot output) at once in a neural network. Here is the code that I am using.
[2] I am getting only 31% accuracy. I get this accuracy just in the second epoch. From second or third epoch onwards accuracy and other parameters become constant. Am I doing the wrong code here?
dataset = df.values
X = dataset[:,0:5038]/220
Y_smile = dataset[:,5038 :5864]

from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
enc.fit(Y_smile)
OneHotEncoder(handle_unknown='ignore')
enc.categories_
Y = enc.transform(Y_smile).toarray()
print(Y,Y.shape, Y.dtype)

from sklearn.model_selection import train_test_split
X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.3)
X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)

import numpy as np
X_train = np.asarray(X_train).astype(np.float64)
X_val = np.asarray(X_val).astype(np.float64)
X_test = np.asarray(X_test).astype(np.float64)

filepath = ""bestmodelweights.hdf5""
checkpoint = [tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', mode='auto', save_best_only=True, Save_weights_only = True, verbose = 1), 
              tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose =1)]

model = tf.keras.Sequential([
                             tf.keras.layers.Dense(1024, activation='relu', input_shape=(5038,)),
                             tf.keras.layers.Dense(524, activation='relu'),
                             tf.keras.layers.Dense(524, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),
                             tf.keras.layers.Dense(1024, activation='relu'),                        
                             tf.keras.layers.Dense(6689, activation= 'softmax')])

model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics=['accuracy'])

hist = model.fit(X_train, Y_train, epochs= 200, callbacks=[checkpoint],validation_data=(X_val, Y_val))

",['tensorflow'],
"How to make a multivariate forecasting if one of features becomes known for the future with some confidence level, e.g. weather forecast data","
Let's assume that we make forecasting of another metric partially based on forecasts of the weather forecast, e.g. of temperature, pressure, then we can potentially obtain those forecasts from one of public APIs and have some information about future values of these features and do more precise prediction of another parameter taken as a label.
What is the approach to use in this case if one or more of the features in multivariate forecasting have some forecasted values for the predicted horizon? It looks that in this case not only values from historical data can be used but also predicted values, though not clear how to organize the model in this case, e.g. it can be multivariate LSTM.
","['machine-learning', 'forecasting']",
"How do I convert an MDP with the reward function in the form $R(s,a,s')$ to and an MDP with a reward function in the form $R(s,a)$?","
The AIMA book has an exercise about showing that an MDP with rewards of the form $r(s, a, s')$ can be converted to an MDP with rewards $r(s, a)$, and to an MDP with rewards $r(s)$ with equivalent optimal policies.
In the case of converting to $r(s)$ I see the need to include a post-state, as the author's solution suggests. However, my immediate approach to transform from $r(s,a,s')$ to $r(s,a)$ was to simply take the expectation of $r(s,a,s')$ with respect to s' (*). That is:
$$ r(s,a) = \sum_{s'} r(s,a,s') \cdot p(s'|s,a) $$
The authors however suggest a pre-state transformation, similar to the post-state one. I believe that the expectation-based method is much more elegant and shows a different kind of reasoning that complements the introduction of artificial states. However, another resource I found also talks about pre-states.
Is there any flaw in my reasoning that prevents taking the expectation of the reward and allow a much simpler transformation? I would be inclined to say no since the accepted answer here seems to support this. This answer mentions Sutton and Barto's book, by the way, which also seems to be fine with taking the expectation of $r(s, a, s')$.
This is the kind of existential question that bugs me from time to time and I wanted to get some confirmation.
(*) Of course, that doesn't work in the $r(s, a)$ to $r(s)$ case, as we do not have a probability distribution over the actions (that would be a policy, in fact, and that's what we are after).
","['reinforcement-learning', 'markov-decision-process', 'proofs', 'reward-functions']",
Why is the hypothesis function $h_{\theta}(x)$ equivalent to $E[y | x; \theta]$ in generalised linear models?,"
Reading through the CS229 lecture notes on generalised linear models, I came across the idea that a linear regression problem can be modelled as a Gaussian distribution, which is a form of the exponential family. The notes state that $h_{\theta}(x)$ is equal to $E[y | x; \theta]$. However, how can $h_{\theta}(x)$ be equal to the expectation of $y$ given input $x$ and $\theta$, since the expectation would require a sort of an averaging to take place?

Given x, our goal is to predict the expected value of $T(y)$ given $x$. In most of our examples, we will have $T(y) = y$, so this means we would like the prediction $h(x)$ output by our learned hypothesis h to satisfy $h(x) = E[y|x]$.
To show that ordinary least squares is a special case of the GLM family of models, consider the setting where the target variable y (also called the response variable in GLM terminology) is continuous, and we model the conditional distribution of y given x as a Gaussian $N(\mu,\sigma^2)$. (Here, $\mu$ may depend $x$.) So, we let the ExponentialFamily($\eta$) distribution above be the Gaussian distribution. As we saw previously, in the formulation of the Gaussian as an exponential family distribution, we had μ = η. So, we have
$$h_{\theta}(x) = E[y|x; \theta] = \mu = \eta = \theta^Tx.$$

EDIT
Upon reading other sources, $y_i \sim N(\mu_i, \sigma^2)$ meaning that each individual output has it's own normal distribution with mean $\mu_i$ and $h_{\theta}(x_i)$ is set as the mean of the normal distribution for $y_i$. In that case, then the hypothesis makes sense to be assigned the expectation.
","['models', 'linear-regression', 'logistic-regression']","
In generalised Linear models, each output variable $y_i$ is modelled as a distribution from the exponential family, with the hypothesis function $h_{\theta}(x)$ for a given $\theta$ as the expected value of $y_i$ and maximum likelihood estimation is usually the method used to solve GLM's. 
"
Handle non-existing states in q-learning,"
I am using Q-learning to solve an engineering problem. The objective is to generate a Q-table associating state to Q-values. 
I created a State vector DS = [s1, s2, ..., sN] containing all ""desired"" states. So the Q-table has the form of Q-table =[DS, Q-values]. 
On the other hand, my agent follows a trajectory. Playing action a at state s (which is a point of the trajectory) leads the agent to another state s' (another point of the trajectory). However, I don't have the s' state in the initially desired states vector DS. 
One solution is to add new states to the DS vector while the Q-learning algorithm is running, but I do not want to add new states. 
Any other ideas on how to handle this problem? 
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']","
You have two options, either interpolate or restrict the actions only to values that produce states which are in your state vector. 
The simplest interpolation scheme is a linear interpolation, which works as follows (assuming DS contains a set of grid points in increasing order). For a state $s'$ you can locate its closest neighbours from the array DS and the value in state $s'$ will, then, be a weighted average of the values in those neighbouring states. Formally, $$Q(s') = \frac{s_{i+1} - s'}{s_{i+1}-s_i}Q(s_i) + \frac{s'-s_i}{s_{i+1}-s_i}Q(s_{i+1}),$$ for $s_i < s'< s_{i+1}$, where $s_i$ and $s_{i+1}$ are the neighbours, $s_i$ is the $i$-th element of DS(i.e. si = DS[i] = Q-table[i,1]), and $Q(s_i)$ is related to the Q-table as Qsi = Q-table[i,2] (asuming array indexing starts from 1). 
Restricting the actions would work as follows. For simplicity, assume that the agent chooses the next state directly, i.e. $s'=a$. Then, if you have an array of actions A = [a1, a2, ... , aM], $M \le N$ then each $a_i$ needs to be present in the state array DS (i.e. A is a subset of DS, formally $A \subseteq S$). This may not be desirable but it is an option. 
"
Two questions about the architecture of Google Bert model (in particular about parameters),"
I'm looking for someone who can help me clarify a few details regarding the architecture of Bert model. Those details are necessary for me to come with a full understanding of Bert model, so your help would be really helpful. Here are the questions:

Does the self-attention layer of Bert model have parameters? Do the embeddings of words change ONLY according to the actual embeddings of other words when the sentence is passed through the self-attention layer?
Are the parameters of the embedding layer of the model (the layer which transforms the sequence of indexes passed as input into a sequence of embeddings of size=size of the model) trainable or not?

","['natural-language-processing', 'bert']",
Is the PyTorch official tutorial really about Q-learning?,"
I read Q-learning algorithm and also I know value iteration (when you update action values).  I think the PyTorch example is value iteration rather than Q-learning.
Here is the link:
https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
","['q-learning', 'value-iteration']","
TL;DR: It is Q learning. However Q learning is basically sample-based value iteration, so not surprising you see a similarity.
Q learning* and value iteration are very strongly related. When considering action values, both approaches use the same Bellman equation for optimal policy, $q^*(s,a) = \sum_{r,s'}p(r,s'|s,a)(r+\gamma \text{max}_{a'} q^*(s', a'))$ as the basis for update steps. The differences are:

Value iteration makes updates using a model of the environment, Q learning works from samples from the environment made by an active agent.

By working from a simulated environment rather than a real one, it may not be clear when an agent is model-free or model-based (or planning rather than acting). However, the way that the simulated environment in the PyTorch example is used is consistent with a model-free method.

Value iteration loops through all possible states and actions for updates independently of any action an agent might take (in fact the agent need not exist). Q learning works with whichever states the agent experienced.

By adding experience replay memory in DQN, Q learning becomes a little bit closer to value iteration, as you can frame the memory as a learned model, plus consider it to be a type of planning (or a ""sweep"" through states). This is how it is described for instance in DynaQ which is an almost identical algorithm to experience replay as used in DQN when both are used in the simplest versions - see Sutton & Barto chapter 8.

Value iteration value update steps are over an expectation of next states and rewards - it processes the weighted sum $\sum_{r,s'}p(r,s'|s,a)$. Q learning update steps are over sampled next states and rewards - it ends up approximating the same expectation over many separate updates.

Even using large amounts of experience replay memory does not get Q learning the same as value iteration on this issue, samples are not guaranteed perfect. However, in a deterministic environment, this difference is not meaningful. So if you have a deterministic environment, Q learning and value iteration may also be considered a little closer in nature.



* Technically this applies to single-step Q-learning. n-step Q-learning and Q($\lambda$) use different estimates of future expected return, that are related but not the same as the single-step version shown here.
"
How would researchers determine the best deep learning model if every run of the code yields different results?,"
There are many factors that cause the results of ML models to be different for every run of the same piece of code. One factor could be different initialization of weights in the neural network.
Since results might be stochastic, how would researchers know what their best performing model is? I know that a seed can be set to incorporate more determinism into the training. However, there could be other pseudo-random sequences that produce slightly better results?
","['deep-learning', 'reinforcement-learning', 'dqn']",
Detect object in video and augment another video on top of it,"
I'm trying to detect an object in a video (with slight camera movement), and then augment another video on top of it. What is the simplest approach to do that?
For instance, let's assume I have this simple video of a couch HERE. Now I want to augment the right cusion with a human or a dog. The dog or human are video itself (let's assume transparency is not an issue).
What's the simplest approach to do that? 

","['computer-vision', 'image-recognition', 'object-detection', 'object-recognition', 'algorithm-request']",
Why is the policy not a part of the MDP definition?,"
I'm reading an article on reinforcement learning, and I don't understand why the agent's policy $\pi$ is not part of definition of Markov Decision process(MDP):


Bu, Lucian, Robert Babu, and Bart De Schutter. ""A comprehensive survey of multiagent reinforcement learning."" IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 38.2 (2008): 156-172.

My question is:

Why the policy is not a part of the MDP definition?

","['reinforcement-learning', 'definitions', 'markov-decision-process', 'policies']","
The MDP defines the environment (which corresponds to the task that you need to solve), so it defines e.g. the states of the environment, the actions that you can take in those states, the probabilities of transitioning from one state to the other and the probabilities of getting a reward when you take a certain action in a certain state. 
The policy corresponds to a strategy that the RL agent can follow to act in that environment. Note that the MDP doesn't define what the agent does in each state. That's why you need the policy! An optimal policy for a specific MDP corresponds to the strategy that, if followed, is guaranteed to give you the highest amount of reward in that environment. However, there are multiple strategies, most of them are not optimal. This should clarify why the policy is not part of the definition of the MDP.
"
What is the relation between multi-agent learning and reinforcement learning?,"
What is the relation between multi-agent learning and reinforcement learning?
Is one a sub-field of the other? For instance, would it make sense to state that your research interest are multi-agent learning and reinforcement learning, or would that be weird as one includes most of the topics of the other?
","['reinforcement-learning', 'comparison', 'multi-agent-systems']","
I think there is an intersection. There are problems that are in reinforcement learning and in learning in multi-agent systems. There are problems in reinforcement learning, but not exactly in multi-agent systems. And there is learning in multi-agent systems that is not through reinforcement learning. For sort you can say: multi-agent reinforcement learning. I recommend take a look at these references:

Bu, Lucian, Robert Babu, and Bart De Schutter. ""A comprehensive survey of multiagent reinforcement learning."" IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 38.2 (2008): 156-172.
Busoniu, Lucian, Robert Babuska, and Bart De Schutter. ""Multi-agent reinforcement learning: A survey."" 2006 9th International Conference on Control, Automation, Robotics and Vision. IEEE, 2006.

"
Are there any conferences dedicated to artificial general intelligence?,"
Similarly to What are the scientific journals dedicated to artificial general intelligence?, are there any conferences dedicated to artificial general intelligence?
","['agi', 'research', 'academia']",
Are there other mathematical frameworks of artificial general intelligence apart from AIXI?,"
AIXI is a mathematical framework for artificial general intelligence developed by Marcus Hutter since the year 2000. It's based on many concepts, such as reinforcement learning, Bayesian statistics, Occam's razor, or Solomonoff induction. The blog post What is AIXI? — An Introduction to General Reinforcement Learning provides an accessible overview of the topic for those of you not familiar with it.
Are there any other mathematical frameworks of artificial general intelligence apart from AIXI? 
I am aware of projects such as OpenCog, but that's not really a mathematical framework, but more a cognitive science framework.
","['reference-request', 'agi', 'aixi', 'godel-machine']",
Why not replacing the simple linear functions that neurons compute with more complex functions?,"
In a neural network, a neuron typically computes a linear function $f(x) = w*x$, where $w$ is the weight and $x$ is the input.
Why not replacing the linear function with more complex functions, such as $f(x,w,a,b,c) = w*(x + b)^a + c$? 
It will provide much more diversity into neural networks. 
Does this have a name? Has this been used?
","['neural-networks', 'activation-functions']",
How does backpropagation work in LSTMs?,"
After reading a lot of articles (for instance, this one Understanding LSTM Networks), I know that the long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.
How does backpropagation work in the specific case of LSTMs?
","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'backpropagation']",
What made your DDPG implementation on your environment work?,"
I am working on scheduling problem that has inherent randomness. The dimensions of action and state spaces are 1 and 5 respectively. 
I am using DDPG, but it seems extremely unstable, and so far it isn't showing much learning. I've tried to 

adjust the learning rate, 
clip the gradients, 
change the size of the replay buffer, 
different neural net architectures, using SGD and Adam, 
change the $\tau$ for the soft-update. 

So, I'd like to know what people's experience is with this algorithm, for the environments where it was tested on the paper but also for other environments. What values of hyperparameters worked for you? Or what did you do? How cumbersome was the fine-tuning? 
I don't think my implementation is incorrect, because I pretty much replicated this, and every other implementation I found did exactly the same. 
(Also, I am not sure this is necessarily the best website to post this kind of question, but I decided to give a shot.)
","['reinforcement-learning', 'implementation', 'ddpg', 'hyper-parameters']",
How to train a neural network with a data set that in which the target is a mix of 0-1 label and numeric real value label?,"
I am running into an issue in which the the target (label collums) of my dataset contain a mixture of binary label (yes/no) and some numeric value label. 

The value of these numeric value (resource 1 and resource 2 collumns) experience a large variation margin. Sometime these numeric value can be like 0.389 but sometimes they can be 0.389 x 10^-4 or something.
My goal is to predict the binary decision and the amount of resource allocated to a new user who have input feature 1 (numeric) and input feature 2 (numeric).
My initial though would be that the output neuron corresponding to the 0-1 decision would use logistic regression activation function. But for the neuron that corresponding to the resource I am not quite sure.
What would be the appropriate way to tackle such situation in term of network structure or data pre-processing  strategy ?
Thank you for your enthusiasm !
","['neural-networks', 'data-preprocessing']","
In Neural Networks the function that provides the largest interval while activation is Tanh with a result between -1 and 1 
You can use it to train your model , when the label has value of false it should be -1 , and when true it should be 1
In prediction , you'll see where the value is more close , for example if you get 0.4 more close to 1 so it'll be true 
"
Convergence of a delayed policy update Q-learning,"
I thought about an algorithm that twists the standard Q-learning slightly, but I am not sure whether convergence to the optimal Q-value could be guaranteed. 
The algorithm starts with an initial policy. 
Within each episode, the algorithm conducts policy evaluation and does NOT update the policy. 
Once the episode is done, the policy is updated using the greedy policy based on the current learnt Q-values.
The process then repeats. I attached the algorithm as a picture.
Just to emphasize that the updating policy does not change within each episode. The policy at each state is updated AFTER one episode is done, using the Q-tables. 
Has anyone seen this kind of Q-learning before? If so, could you please kindly guide me to some resources regarding the convergence? Thank you!

","['reinforcement-learning', 'q-learning', 'convergence']",
Policy Gradient on Tic-Tac-Toe not working,"
I wanted to implement the Policy Gradient on Tic-Tac-Toe. 
I tried to use the code that worked for any environment like CartPole-v0 to my Tic-Tac-To game. But it is not learning. There are no errors. Just the result is so bad.
RandomPlayer (""Player X"") vs PolicyAgent (""Player O"")

So one can see that the Policy Agent is not learning after 500 battles. Each battle consists of 100games against the random player. Together 500 * 100 games. 
Can someone tell me the problem or the bug in my code. I can not figure it out. Or what I have to improve. It would be so great. 
Here is also a project which did the same, which I want to do, but with success. 
https://github.com/fcarsten/tic-tac-toe/blob/master/tic_tac_toe/DirectPolicyAgent.py
I did not get what I am making different.
Code:
Packages:
import torch
import torch as T
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import numpy as np
import gym
from gym import wrappers

Neural Net:
class PolicyNetwork(nn.Module):
    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):
        super(PolicyNetwork, self).__init__()
        self.input_dims = input_dims
        self.lr = lr
        self.fc1_dims = fc1_dims
        self.fc2_dims = fc2_dims
        self.n_actions = n_actions

        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)
        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)
        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)

        self.optimizer = optim.Adam(self.parameters(), lr=lr)

    def forward(self, observation):
        state = T.Tensor(observation)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

Policy Agent:
class PolicyAgent:
    def __init__(self, player_name):
        self.name = player_name
        self.value = PLAYER[self.name]

    def board_to_input(self, board):
        input_ = np.array([0] * 27)
        for i, val in enumerate(board):
            if val == self.value:
                input_[i] = 1  
            if val == self.value * -1:
                input_[i+9] = 1
            if val == 0:
                input_[i+18] = 1
        return np.reshape(input_, (1,-1))


    def start(self, learning_rate=0.001, gamma=0.1):
        self.lr = learning_rate
        self.gamma = gamma
        self.all_moves = list(range(0,9))
        self.policy = PolicyNetwork(self.lr, 27, 243, 91, 9)
        self.reward_memory = []
        self.action_memory = []

    def turn(self, board, availableMoves):
        state = self.board_to_input(board.copy())
        prob = F.softmax(self.policy.forward(state))
        action_probs = torch.distributions.categorical.Categorical(prob)
        action = action_probs.sample()

        while action.item() not in availableMoves:
            state = self.board_to_input(board.copy())
            prob = F.softmax(self.policy.forward(state))
            action_probs = torch.distributions.categorical.Categorical(prob)
            action = action_probs.sample()

        log_probs = action_probs.log_prob(action)
        self.action_memory.append(log_probs)

        self.reward_memory.append(0)
        return action.item()

    def learn(self, result):
        if result == 0:
            reward = 0.5
        elif result == self.value:
            reward = 1.0
        else:
            reward = 0

        self.reward_memory.append(reward)
        #print(self.reward_memory)

        self.policy.optimizer.zero_grad()
        #G = np.zeros_like(self.action_memory, dtype=np.float64)
        G = np.zeros_like(self.reward_memory, dtype=np.float64)


        #running_add = reward
        #for t in reversed(range(0, len(self.action_memory))):
        #    G[t] = running_add
        #    running_add = running_add * self.gamma

        #'''
        running_add = 0
        for t in reversed(range(0, len(self.reward_memory))):
            if self.reward_memory[t] != 0:
                running_add = 0
            running_add = running_add * self.gamma + self.reward_memory[t]
            G[t] = running_add
        for t in range(len(self.reward_memory)):
            G_sum = 0
            discount = 1
            for k in range(t, len(self.reward_memory)):
                G_sum += self.reward_memory[k] * discount
                discount *= self.gamma
            G[t] = G_sum
        mean = np.mean(G)
        std = np.std(G) if np.std(G) > 0 else 1
        G = (G-mean)/std
        #'''

        G = T.tensor(G, dtype=T.float)

        loss = 0
        for g, logprob in zip(G, self.action_memory):
            loss += -g * logprob

        loss.backward()
        self.policy.optimizer.step()

        self.reward_memory = []
        self.action_memory = []

","['neural-networks', 'reinforcement-learning', 'policy-gradients', 'pytorch']","
Some suggestions:

You have a loop in which illegal moves by the RL agent are ignored.  In other words, when the agent makes illegal moves, it is not punished, nor is there any +/- rewards for it whatsoever.  In my program I treat illegal moves the same as losing the game.

Try to play a few ""pre-moves"" to make the game easier.  For example I start with this (now is for Player 'X' to move):

Then you may see convergence sooner.  Always a good idea to start from a dead-easy case.

Notice that the optimal value is not the ""perfect score"".  If you take win=2, lose=-2, draw=1, then the optimal score for the above game for Player 'X' is 1.5, not 2.0.  (You can check the math).
I wrote a small Python program to calculate the optimal score for an RL agent playing against a random opponent (both players assumed to never make illegal moves).  See this answer.

For me it took much more than 50K games to see convergence.  It's more like 1-2M games.  (Correction:  With more trails, I found that policy gradient seems unable to converge to the optimal value, but it did improve over time to reach a sub-optimal score.)

Your neural network may be a bit too small.


Remark: This is a good question and also very significant to artificial intelligence, as Tic Tac Toe is a game suitable for logic-based agents, and it is important to see how deep learning performs in such ""logical"" domains.  My own research is focused on combining logic structure with deep learning.
My code is here：
https://github.com/Cybernetic1/policy-gradient
"
In vanilla policy gradient is the baseline lagging behind the policy?,"
Vanilla policy gradient algorithm (using baseline to reduce variance) acc to here (page 16)

Initialize policy parameter θ, baseline b
for iteration=1, 2, . . . do

Collect a set of trajectories by executing the current policy
At each timestep in each trajectory, compute

the return $R_{t}= \sum_{t'=t}^{T-1}\gamma^{t'-t}r_{t'}$
the advantage estimate $\hat{A}_{t} = R_{t} - b(s_{t})$

Re-fit the baseline, by minimizing $\lVert b(s_{t}) - R_{t} \rVert^{2}$

summed over all trajectories and timesteps.

Update the policy, using a policy gradient estimate $\hat{g}$,

which is a sum of terms $\nabla_{\theta}log\pi(a_{t}|s_{t},\theta)\hat{A_{t}}$




At line 6, advantage estimate is computed by subtracting baseline from the returns
At line 7, baseline is re-fit minimizing mean squared error between state dependent baseline and return
At line 8, we update the policy using advantage estimate from line 6

So is the baseline expected to be used in the next iteration when our policy has changed?
To compute the advantage we subtract the state value $V(s_{t})$ from the action value $Q(s_{t},a_{t})$, under the same policy, then why is the old baseline used here in advantage estimation?
","['reinforcement-learning', 'policy-gradients']",
Can we increase the speed of training a reinforcement learning algorithm?,"
I am new in reinforcement learning. I started reading the PyTorch's documentation about the cart pole control. Whenever an agent fails, they restart the environment. 
When I run the code, the time in the game is the same as time in real life. Can we train models quicker? Can we make the game faster so that model will be training faster?
","['deep-learning', 'reinforcement-learning', 'q-learning']","

Can we make the game faster so that model will be training faster?

It depends on how much processing is required to run the simulation, how efficient that is implemented in whichever library you have loaded, and whether there is anything non-necessary for training that you can disable. Some environments for instance deliberately run ""real time"" so humans can appreciate the video output, and that is not necessary for training purposes (unless you want to experiment with real-time robotics).
For OpenAI Gym, there is one thing you can usually do: Switch off the rendering. The rendering for environments like CartPole slows down each time step considerably, and unless you are learning using computer vision processing on the pixels output, the agent does not need the pictures. You may even notice during training that moving the rendering window so it is not visible will speed up the training process considerably.
What I did for CartPole, LunarLander and a couple of similar environments is turn off rendering for 99 out of 100 episodes, and render just one of them in 100 to help me monitor progress. For Q learning, I also picked that to be a ""test"" episode where I stopped exploration.
Another option for speeding up training is to run a distributed system with multiple simulations at once. You will need mechanisms to share collected data too, so it is more work, but it is another approach to take if the simulation steps are the bottleneck for training speed.
"
What is the intuition behind importance sampling for off-policy value evaluation?,"
The technique for off-policy value evaluation comes from importance sampling, which states that 
$$E_{x \sim q}[f(x)] \approx \frac{1}{n}\sum_{i=1}^n f(x_i)\frac{q(x_i)}{p(x_i)},$$ where $x_i$ is sampled from $p$.  
In the application of importance sampling to RL, is the expectation of the function $f$ equivalent to the value of the trajectories, which is represented by the trajectories $x$? 
The distributions $p$ represent the probability of sampling trajectories from the behavior policy and the distribution $q$ represents the probability of sampling trajectories from the target policy $q$?
How would the trajectories from distribution $q$ be better than that of $p$? I know from the equation how it is better, but it is hard to understand intuitively why this could be so.
","['reinforcement-learning', 'off-policy-methods', 'importance-sampling']",
Do we have two Q-learning update formulas?,"
I have seen two deep Q-learning formulas:
$$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right]$$
and this one
$$Q(s, a)=r(s, a)+\gamma \max _{a} Q\left(s^{\prime}, a\right)$$
Which one is correct?
","['reinforcement-learning', 'q-learning', 'deep-rl']",
Why do we need convolutional neural networks instead of feed-forward neural networks?,"
Why do we need convolutional neural networks instead of feed-forward neural networks?
What is the significance of a CNN? Even a feed-forward neural network will able to solve the image classification problem, then why is the CNN needed?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'comparison', 'feedforward-neural-networks']",
Learning policy where action involves discrete and continuous parameters,"
Typically it seems like reinforcement learning involves learning over either a discrete or a continuous action space. An example might be choosing from a set of pre-defined game actions in Gym Retro or learning the right engine force to apply in Continuous Mountain Car; some popular approaches for these problems are deep Q-learning for the former and actor-critic methods for the latter.
What about in the case where a single action involves picking both a discrete and a continuous parameter? For example, when choosing the type (discrete), pixel grid location (discrete), and angular orientation (continuous) of a shape from a given set to place on a grid and optimize for some reward.
Is there a well-established approach for learning a policy to make both types of decisions at once?
","['deep-learning', 'reinforcement-learning', 'policy-gradients', 'actor-critic-methods']","
There is a recent paper: Continuous-Discrete Reinforcement Learning for
Hybrid Control in Robotics published by DeepMind that aims to solve this problem, as stated in the abstract:

Many real-world control problems involve both discrete decision variables – such as the choice of control modes, gear switching or digital outputs – as
  well as continuous decision variables – such as velocity setpoints, control gains or
  analogue outputs. However, when defining the corresponding optimal control or reinforcement learning problem, it is commonly approximated with fully continuous
  or fully discrete action spaces. These simplifications aim at tailoring the problem
  to a particular algorithm or solver which may only support one type of action
  space. Alternatively, expert heuristics are used to remove discrete actions from
  an otherwise continuous space. In contrast, we propose to treat hybrid problems
  in their ‘native’ form by solving them with hybrid reinforcement learning, which
  optimizes for discrete and continuous actions simultaneously. 

The idea is that they use a hybrid policy that uses a Gaussian distribution for the continuous decision variables and a categorical distribution for the discrete decision variables. Then, they extend the Maximum a Posteriori Policy Optimisation (MPO) algorithm (also by DeepMind) to allow it to handle hybrid policies.
Here is a video showing how they used the resulting hybrid MPO policy in a robotics task, where in addition to the continuous actions, the robot can choose a discrete action which is the control mode to be used (coarse vs. fine).
"
Training a model for text document transformation?,"
I have a bunch of text documents, split into source documents and transformed documents. These text documents have multiple lines and are edited at specific locations, in a specific way. 
I make use of the difflib package available in Python to identify the associated transformation, for each source document and the resulting transformed document. 
I wish to train and implement a ML technique which will help in identifying and automating this conversion activity.

Here is a sample result of how the transformation result looks like:
(NOTE: This example contains only one line, but my actual use case contains several lines)
import difflib

Initial = 'This is my initial state'
Final = 'This is what I transform into'

diff = difflib.SequenceMatcher(None, Initial, Final)

for tag,i1,i2,j1,j2 in diff.get_opcodes():
    print('{:7} Initial[{:}:{:}] --> Final[{:}:{:}] {:} --> {:}'.format(tag,i1,i2,j1,j2,Initial[i1:i2],Final[j1:j2]))

#Result:
equal   Initial[0:8] --> Final[0:8] This is  --> This is 
insert  Initial[8:8] --> Final[8:23]  --> what I transfor
equal   Initial[8:9] --> Final[23:24] m --> m
delete  Initial[9:10] --> Final[24:24] y --> 
equal   Initial[10:13] --> Final[24:27]  in -->  in
delete  Initial[13:14] --> Final[27:27] i --> 
equal   Initial[14:15] --> Final[27:28] t --> t
replace Initial[15:24] --> Final[28:29] ial state --> o

This helps in outlining the transformation steps to transform Initial to Final. I wish to make use of ML to identify the common pattern in such transformation between a large collection of txt documents and train a model that I can use in future.

What will be the best method to approach this problem? I am not facing a problem in identifying and classifying text data, but in identifying the nature of editing and transformation of strings.
","['machine-learning', 'python', 'training', 'models']",
What is the intuition behind the attention mechanism?,"

Attention idea is one of the most influential ideas in deep learning. The main idea behind attention technique is that it allows the decoder to ""look back” at the complete input and extracts significant information that is useful in decoding.

I am really having trouble understanding the intuition behind the attention mechanism. I mean how the mechanism works and how to configure.
In simple words (and maybe with an example), what is the intuition behind the attention mechanism?
What are some applications, advantages & disadvantages of attention mechanism?
","['neural-networks', 'deep-learning', 'natural-language-processing', 'attention']",
What are the conditions of convergence of temporal-difference learning?,"
In reinforcement learning, temporal difference seem to update the value function in each new iteration of experience absorbed from the environment. 
What would be the conditions for temporal-difference learning to converge in the end? How is it guaranteed to converge?
Any intuitive understanding of those conditions that lead to the convergence?
","['reinforcement-learning', 'convergence', 'temporal-difference-methods']",
Transformer encoding for regression,"
I have a string of characters encoding a molecule. I want to regress some properties of those molecules. I tried using an LSTM that encodes all one hot encdoed characters, and then I take the last hidden state fed into a linear layer to regress the property. This works fine, but I wanted to see if transformers can do better, since they are so good in NLP.
However, I am not quiet sure about two things:

Pytorch transformer encoder layer has two masking parameters: ""src_mask"" and ""src_key_padding_mask"". The model needs the whole string to do the regression, so I dont think I need ""src_mask"", but I do padding with 0 for parallel processing, is that what ""src_key_padding_mask"" is for?
What output from the transformer do I feed into the linear regression layer? For the LSTM I took the last hidden output. For the transformer, since everything is processed in parallel, I feel like I should rather use the sum of all, but it doesn't work well. Instead using only the last state works better, which seems arbitrary to me. Any ideas on how to properly do this, how do sentiment analysis model do it?

","['natural-language-processing', 'transformer']",
Is the number of bidirectional LSTMs in seq2seq model equal to the maximum length of input text/characters?,"
I'm confused about this aspect of RNNs while trying to learn how seq2seq encoder-decoder works at https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/.
It seems to me that the number of LSTMs in the encoder would have to be the same as number of words in the text (if word embeddings are being used) or characters in the text (if char embeddings are being used). For char embeddings, each embedding would correspond to 1 LSTM in 1 direction and 1 encoder hidden state. 

Is this understanding correct? 
E.g. If we have another model that uses encoder-decoder for a different application (say text-to-speech synthesis described here https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html) tha uses 256 LSTMs in each direction of the bidirectional-encoder, does it mean the input to this encoder is limited to 256 characters of text?
Can the decoder output has to be same length as the encoder input or can it be different? If different what factor describes what the decoder output length should be?

","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'bert', 'text-generation']",
Can deep learning be used to help mathematical research?,"
I am currently learning about deep learning and artificial intelligence and exploring his possibilities, and, as a mathematician at heart, I am inquisitive about how it can be used to solve problems in mathematics.
Seeing how well recurrent neural networks can understand human language, I suppose that they could also be used to follow some simple mathematical statements and maybe even come up with some proofs. I know that computer-assisted proofs are more and more frequent and that some software can now understand simple mathematical language and verify proofs (e.g. Coq). Still, I've never heard of deep learning applied to mathematical research.
Can deep learning be used to help mathematical research? So, I am curious about whether systems like Coq could be combined with deep learning systems to help mathematical research. Are there some exciting results?
","['deep-learning', 'math', 'proofs', 'coq']","
Mathematical equations are generally expressed in a sequential form known as 'infix notation'.  It is characterised by the placement of operators between operands. To make the order of the operations in the Infix notation unambiguous, a lot of parenthesis are needed. Infix notation is more difficult to parse by computers than prefix notation (e.g. + 2 2) or postfix notation (e.g. 2 2 +). 
There is a deep learning approach to symbolic mathematics recommended in the research paper by Guillaume Lample and François Charton. They have found an interesting approach to use deep neural networks for symbolic integration and differentiation equations. This paper proposes a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. 
Deep Learning for Symbolic Mathematics

This approach is essentially representing mathematical problems in prefix notation. First a symbolic syntax tree is constructed that captures the order and values of the operations in the expression. Second, the tree is traversed from top to bottom and from left to right. If the current node is a primitive value (a number), add it to the sequence string. If the current node is a binary operation, add the operations symbol to the sequence string. Then, add the representation of the left child node (could be recursive). Then, add the representation of the right child node. This procedure resulted in the following expression.

We can expect further more advances in this area with the emergence of better symbolic learning models leveraging attention based transformers and other neural symbolic learning models. Recent work by MIT, DeepMind and IBM has shown the power of combining connectionist techniques like deep neural networks with symbolic reasoning. Please find the details in the following article. 
The Neuro-Symbolic Concept Learner
"
What is a example showing that the tree-based variant for the greedy best-first search is incomplete?,"
I understand that a tree-based variant will have nodes repeatedly added to the frontier. How do I craft an example where a particular goal node is never found. Is this example valid. 

On the other hand, how do I explain that the graph-based version of the greedy best-first search is complete? 
","['comparison', 'search', 'best-first-search', 'tree-search', 'graph-search']",
How can I find the similar non-zero connections between different levels of sparsity of the same network?,"
I am pruning a neural network (CNN and Dense) and for different sparsity levels, I have different sub-networks. Say for sparsity levels of 20%, 40%, 60% and 80%, I have 4 different sub-networks.
Now, I want to find the non-zero connections that they have in common. Any idea how to visualize this or compute this?
I am using Python 3.7 and TensorFlow 2.0.
After the convergence of a neural network following the random weight initialization, some weights/connections increase (magnitude), while other weights decrease. You can then prune the smallest magnitude weights. I want to compare the remaining weights for say two networks having the same level of sparsity of say 50%. The idea is to have an idea of which weights were pruned away and which weights/connections remain.
","['neural-networks', 'convolutional-neural-networks', 'deep-neural-networks']",
What are the main differences between sparse autoencoders and convolution autoencoders?,"
What are the main differences and similarities between sparse autoencoders and convolution autoencoders?
When should one be preferred over the other? What are their applications?
(References are welcome. Somehow I was not able to find any comparisons of these autoencoders although I looked in a few textbooks and searched for material online. I was able to find the descriptions of each autoencoder separately, but what I am interested in is the comparison.)
","['neural-networks', 'comparison', 'autoencoders']",
How can I increase the exploration in the Proximal Policy Optimation algorithm?,"
How can I increase the exploration in the Proximal Policy Optimation reinforcement learning algorithm? Is there a variable assigned for this purpose? I'm using the stable-baseline implementation: https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html
","['reinforcement-learning', 'proximal-policy-optimization', 'exploration-exploitation-tradeoff']",
Can deep reinforcement learning algorithms be deterministic in their reproducibility in results?,"
I ran a deep q learning algorithm (DQN) for $x$ number of epochs and got policy $\pi_1$. 
I reran the same script for the same $x$ number of epochs and got policy $\pi_2$. I expected $\pi_1 $ and $\pi_2$ to be similar because i ran the same script. However, when computing the actions on the same test set, i realised the actions were very different. 
Is this supposed to be normal when training deep q networks or is there something that I am missing ?
I am using prioritised experience replay when training the model.
","['machine-learning', 'reinforcement-learning', 'dqn']",
Is there a difference between using 1d conv layers and 2d conv layers with kernel with size of 1 along other than time dimension?,"
Let's assume I use convolutional networks for time-series prediction. Data I feed to the network have 1 channel depth, height of number of periods and number of features is the width, so the frame size is: 
[1, periods, features]. Batch size is not relevant here.
Is there a difference between using 1d convolutions along time (height) dimension and 2d convolutional that will have a kernel size of for example (3, 1) or (5, 1), so that the larger number convolutes along the time dimension, and there is no convolution along features dimension?
","['convolutional-neural-networks', 'prediction', 'time-series', 'convolution', 'forecasting']",
My LSTM text classification model seems not learn anything in early epochs,"
I am trying to use LSTM to do text classification and monitor the training process with tensorboard. But it seems that this model doesn't learn anything in early epochs. Is it normal for LSTM networks?
Here is the definition of model:
class RNN(nn.Module):
    """"""
        RNN model for text classification
    """"""
    def __init__(self, vocab_size, num_class, emb_dim, emb_droprate, rnn_cell_hidden, rnn_cell_type, birnn, num_layers, rnn_droprate, sequence_len):
        super().__init__()
        self.vocab_size = vocab_size                # vocab size
        self.emb_dim = emb_dim                      # embedding dimension
        self.emb_droprate = emb_droprate            # embedding droprate
        self.num_class = num_class                  # classes
        self.rnn_cell_hidden = rnn_cell_hidden      # hidden layer size
        self.rnn_cell_type = rnn_cell_type          # rnn cell type
        self.birnn = birnn                          # wheather use bidirectional rnn
        self.num_layers = num_layers                # number of rnn layers
        self.rnn_droprate = rnn_droprate            # rnn dropout rate before fc
        self.sequence_len = sequence_len            # fix sequence length, so we dont need loop
        pass

    def build(self):
        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)
        self.emb_dropout = nn.Dropout(self.emb_droprate)
        if self.rnn_cell_type == ""LSTM"":
            self.rnn = nn.LSTM(input_size=self.emb_dim, hidden_size=self.rnn_cell_hidden, num_layers=self.num_layers, bidirectional=self.birnn, batch_first=True)
        elif self.rnn_cell_type == ""GRU"":
            self.rnn = nn.GRU(input_size=self.emb_dim, hidden_size=self.rnn_cell_hidden, num_layers=self.num_layers, bidirectional=self.birnn, batch_first=True)
        else:
            self.rnn = None
            print(""unsupported rnn cell type, valid is [LSTM, GRU]"")
        if self.birnn:
            self.fc = nn.Linear(2 * self.rnn_cell_hidden, self.num_class)
        else:
            self.fc = nn.Linear(self.rnn_cell_hidden, self.num_class)

        self.rnn_dropout = nn.Dropout(self.rnn_droprate)

    def forward(self, input_):
        batch_size = input_.shape[0]

        x = self.embedding(input_)
        x = self.emb_dropout(x)

        if self.rnn_cell_type == ""LSTM"":
            if self.birnn:
                h_0 = torch.zeros(self.num_layers * 2, batch_size, self.rnn_cell_hidden, requires_grad=True).to(device)
                c_0 = torch.zeros(self.num_layers * 2, batch_size, self.rnn_cell_hidden, requires_grad=True).to(device)
            else:
                h_0 = torch.zeros(self.num_layers, batch_size, self.rnn_cell_hidden, requires_grad=True).to(device)
                c_0 = torch.zeros(self.num_layers, batch_size, self.rnn_cell_hidden, requires_grad=True).to(device)
            output, (h_n, c_n) = self.rnn(x, (h_0, c_0))
        elif self.rnn_cell_type == ""GRU"":
            if self.birnn:
                h_0 = torch.zeros(self.num_layers * 2, batch_size, self.rnn_cell_hidden, requires_grad=True).to(device)
            else:
                h_0 = torch.zeros(self.num_layers, batch_size, self.rnn_cell_hidden, requires_grad=True).to(device)
            output, h_n = self.rnn(x, h_0)

        if self.birnn:
            x = h_n.view(self.num_layers, 2, batch_size, self.rnn_cell_hidden)
            x = torch.cat((x[-1, 0, : , : ], x[-1, 1, : , : ]), dim = 1)
        else:
            x = h_n.view(self.num_layers, 1, batch_size, self.rnn_cell_hidden)
            x = x[-1, 0, : , : ]

        x = x.view(batch_size, 1, -1)           # shape: [batch_size, 1, 2 or 1 * rnn_cell_hidden]
        x = self.rnn_dropout(x)

        x = self.fc(x)
        x = x.view(-1, self.num_class)          # shape: [batch_size, num_class]

        return x

Parameters of this model:  

vocab size: 4805 
number of classes: 27
embedding dimension: 300
embedding dropoutrate: 0.5
rnn cell type: LSTM
rnn cell hidden size: 1000
bidirectional rnn: False
number of lstm layers: 1
dropout rate at last lstm layer hidden: 0.5
padded sequence length: 64

The Optim:
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)

learning rate here is 0.001, batch size is 32.
The tensorboard graph:


It seems that this model starts learning after epoch 15. Is it normal?
","['recurrent-neural-networks', 'long-short-term-memory', 'text-classification']",
What's the difference between LSTM and GRU?,"
I have been reading about LSTMs and GRUs, which are recurrent neural networks (RNNs). The difference between the two is the number and specific type of gates that they have. The GRU has an update gate, which has a similar role to the role of the input and forget gates in the LSTM.
Here's a diagram that illustrates both units (or RNNs).

With respect to the vanilla RNN, the LSTM has more ""knobs"" or parameters. So, why do we make use of the GRU, when we clearly have more control over the neural network through the LSTM model?
Here are two more specific questions.

When would one use Long Short-Term Memory (LSTM) over Gated Recurrent Units (GRU)?

What are the advantages/disadvantages of using LSTM over GRU?


","['comparison', 'recurrent-neural-networks', 'long-short-term-memory', 'gated-recurrent-unit']",
How can we compute the ratio between the distributions if we don't know one of the distributions?,"
Here is my understanding of importance sampling. If we have two distributions $p(x)$ and $q(x)$, where we have a way of sampling from $p(x)$ but not from $q(x)$, but we want to compute the expectation wrt $q(x)$, then we use importance sampling. 
The formula goes as follows: 
$$
E_q[x] = E_p\Big[x\frac{q(x)}{p(x)}\Big]
$$
The only limitation is that we need a way to compute the ratio. Now, here is what I don't understand. Without knowing the density function $q(x)$, how can we compute the ratio $\frac{q(x)}{p(x)}$? 
Because if we know $q(x)$, then we can compute the expectation directly. 
I am sure I am missing something here, but I am not sure what. Can someone help me understand this?  
","['reinforcement-learning', 'monte-carlo-methods', 'importance-sampling']","
It is common in Bayesian statistics to only know the posterior up to a constant of proportionality. This means that we can't directly sample from the posterior. However, using importance sample we are able to. 
Consider our posterior density $\pi$ is only known up to some constant, i.e. $\pi(x) = K \tilde{\pi}(x)$, where $K$ is some constant and we only have $\tilde{\pi}$. Then by importance sampling we can evaluate the expectation of $X$ (or any function thereof) as follows by using a proposal density $q$: 
\begin{align}
 \mathbb{E}_\pi[X] & = \int_\mathbb{R} x \frac{\pi(x)}{q(x)}q(x)dx \; ; \\
& = \frac{\int_\mathbb{R} x \frac{\pi(x)}{q(x)}q(x)dx}{\int_\mathbb{R}\frac{\pi(x)q(x)}{q(x)}dx} \; ;\\
& = \frac{\int_\mathbb{R} x \frac{\tilde{\pi}(x)}{q(x)}q(x)dx}{\int_\mathbb{R}\frac{\tilde{\pi}(x)q(x)}{q(x)}dx} \; ; \\
& = \frac{\mathbb{E}_q[xw(x)]}{\mathbb{E}_q[w(x)]} \; ;
\end{align}
where $w(x) = \frac{\tilde{\pi}(x)}{q(x)}$. Note that on line two we have not done anything crazy - as $\pi$ is a density we know that it integrates to one and then we multiply the integral by $1 = \frac{q(x)}{q(x)}$. The thing to notice is that the if we were to write $\pi(x) = K \tilde{\pi}(x)$ then the constants $K$ in the integrals would cancel, and so we have our result. 
To summarise - we can sample from a distribution that is difficult/impossible to sample from (e.g. because we only know the density up to a constant of proportionality) by using importance sampling, as this allows us to calculate the importance ratio and use samples that are generated from a distribution of our choosing that is easier to sample from.
Note that importance sampling isn't just used in Bayesian statistics - for instance it could be used in Reinforcement Learning as an off policy way of sampling from the environment whilst still evaluating the value of the policy you're interested in. 
edit: as requested I have added a concrete example
As an example to make things concrete - suppose we have $Y_i | \theta \sim \text{Poisson}(\theta)$ and we are interested in $\theta \in (0, \infty)$. The likelihood function for the Poisson distribution is 
    $$ f(\textbf{y} | \theta) = \prod\limits_{i=1}^n \frac{\theta^{y_i}\exp(-\theta)}{y_i\!}\;.$$
We can then assign a gamma prior to $\theta$, that is we say that $\theta \sim \text{Gamma}(a,b)$ with density 
    $$\pi(\theta) \propto \theta^{a-1} \exp(-b\theta)\;.$$
By applying Bayes rule our posterior is then 
    \begin{align}
    \pi(\theta|\textbf{y}) & \propto f(\textbf{y} | \theta) \pi(\theta) \\
    & = \prod\limits_{i=1}^n \frac{\theta^{y_i}\exp(-\theta)}{y_i\!} \times \theta^{a-1} \exp(-b\theta) \\
    & = \theta^{\sum\limits_{i=1}^n y_i + a - 1} \exp(-[n+b]\theta)\;.
\end{align}
Now we know that this is the kernel of a Gamma($\sum\limits_{i=1}^n y_i + a$, $n+b$) distribution, but assume that we didn't know this and didn't want to calculate the normalising integral. This would mean that we are not able to calculate the mean of our posterior density, or even sample from it. This is where we can use importance sampling, for instance we could choose an Exponential(1) proposal distribution. 
We would sample say 5000 times from the exponential distribution and then calculate the two expectations using MC integration and obtain an estimate for the mean of the posterior. NB that in this example $X$ from earlier would be $\theta$ in this example.
Below is some Python code to further demonstrate this. 
import numpy as np

np.random.seed(1)

# sample our data
y = np.random.poisson(lam=0.5,size = 100)

# sample from proposal
samples_from_proposal = np.random.exponential(scale=1,size=5000)

# set parameters for the prior
a = 5; b = 3

def w(x, y, a, b):
    # calculates the ratio between our posterior kernel and proposal density
    pi = x ** (np.sum(y) + a - 1) * np.exp(-(len(y) + b) * x)
    q = np.exp(-x)
    return pi/q

# calculate the top expectation
top = np.mean(samples_from_proposal * w(samples_from_proposal,y,a,b))

# calculate the bottom expectation
bottom = np.mean(w(samples_from_proposal,y,a,b))

print(top/bottom)

# calculate the true mean since we knew the posterior was actually a gamma density
true_mean = (np.sum(y) + a)/(len(y) + b)
print(true_mean)

Running this you should see that the Expectation from importance sampling is 0.5434 whereas the true mean is 0.5436 (both of which are close to the true value of $\theta$ that I used to simulate the data from) so importance sampling approximates the expectation well. 
"
What are the most common feedforward neural networks?,"
What are the most common feedforward neural networks? What kind of inputs do they receive? For example, do they receive binary numbers, real numbers, vectors, or matrics? Is there such a taxonomy? 
","['neural-networks', 'reference-request', 'feedforward-neural-networks']","
What is a neural network?
Many neural networks can be defined as a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, where $n, m \geq 1$.
Equivalently, many neural networks can also be defined as a set of interconnected units (aka neurons or nodes) $f^i$ that receive some input and produce output, i.e. $f^i(\mathbf{x}^i) = y^i$, where $\mathbf{x}^i \in \mathbb{R}^k$. The actual function $f^i$ is variable and depends on the application or problem you want to solve. For example, $f^i$ can just be a linear combination of the inputs, i.e. $f^i(\mathbf{x}^i) = \sum_{j} \mathbf{w}_j^i \mathbf{x}_{j}^i$, where $\mathbf{w}^i \in \mathbb{R}^k$ is a vector of weights (aka parameters or coefficients). The linear combination can also be followed by a non-linear function, such as a sigmoid function.
If the neural network (more precisely, its units) doesn't contain recurrent (aka cyclic) connections, then it can be called a feedforward neural network (FFNN).
What are the most common feedforward neural networks?
Perceptron
The simplest (non-binary) FFNN is the perceptron, where the inputs are directly connected to the outputs. The perceptron performs a linear combination of the inputs followed by a thresholding operation, so perceptrons can only represent straight-line functions, so they can only be used for classification or regression problems where your data is linearly separable. In fact, the perceptron cannot solve the XOR problem.
Before the perceptron, McCulloch and Pitts had introduced simplified models of biological neurons, where all signals are binary, in an attempt to closely mimic their biological counterpart. The perceptron can actually be seen as an extension of this work. In fact, a perceptron can be viewed as a single artificial neuron.
Multi-layer perceptron
An FFNN with more layers (of units) between the input and the output is often called a multi-layer perceptron (MLP). The layers in the middle are often denoted as hidden layers. The MLP can represent not only linear functions (i.e. straight lines), but also more complicated functions by making use of non-linear functions, such as the sigmoid.
Convolution neural network
You can have other forms of FFNNs that perform other operations.
For example, a convolution neural network (CNN), provided it doesn't contain recurrent connections, is an FFNN that performs the convolution operation (and often also a sub-sampling operation). For this reason, they are particularly suited to deal with images (and videos). (This shouldn't be surprising if you are familiar with the basics of image processing and computer vision, which I don't think it's the case)
However, note that CNNs can also have recurrent connections, but this is not usually the case.
Residual neural network
There are also residual neural networks, i.e. neural networks where a node in a certain layer $l$ can be connected to other nodes in layers $l+j$, for $j \geq 1$, as opposed to being connected only to the nodes in layer $l+1$, which is the typical case.
Auto-encoders
Auto-encoders are neural networks that compress the input and then decompress it. The answers to this question may help you to understand why AEs would be useful.
What kind of inputs do they receive?

What kind of inputs do they receive? For example, do they receive binary numbers, real numbers, vectors, or matrics?

In principle, each of these FFNNs can receive either binary or real numbers or vectors (either of real or binary numbers). However, certain NNs are more appropriate to deal with certain inputs. For example, CNNs are more appropriate for images (which are typically represented as matrices or tensors).
How can you further classify the NNs?
Based on chapter 2 of the book Neural Networks - A Systematic Introduction (1996) by Raul Rojas, you can also divide neural networks into other categories

Unweighted (i.e. binary, such as the McCulloch and Pitts' model) vs weighted (e.g. the perceptron)
Synchronous vs asynchronous (e.g. Hopfield networks, which are recurrent neural networks, though)
Neural networks that store states vs NNs that don't store states

You could also distinguish between FFNNs based on the learning algorithm. Nowadays, the widely used NNs are trained with gradient descent (and back-propagation to compute the gradients), but there are other approaches to train NNs, such as evolutionary algorithms or Hebbian learning. Moreover, you could also distinguish between neural networks that compute a deterministic function and neural networks that have some randomness or stochasticity inside them (e.g. Bayesian neural networks). There are probably many more possible subdivisions.
"
How do I set up rewards to account for unmanned aerial vehicle crashes?,"
I am working on a project to implement a collision avoidance algorithm on a real unmanned aerial vehicle (UAV). 
I'm interested in understanding the process to set up a negative reward to account for scenarios wherein there is a UAV crash. This can be done very easily during the simulation (if the UAV touches any object, the episode stops giving a negative reward). In the real world, a UAV crash would usually entail it hitting a wall or an obstacle, which is difficult to model. 
My initial plan is to stop the RL episode and manually input a negative reward (to the algorithm) each time a crash occurs. Any improvements to this plan would be highly appreciated!
","['machine-learning', 'reinforcement-learning', 'real-time']",
How to add more than 1 agent in one generation with Q Learning,"
Sometimes the agent learns a bit slow and you want to have multiple agents in one generation. And at each episode you'll draw on the screen only the best of them or all of them. How is that possible? 
For clarification purposes, please watch this video on youtube at time 4:10.
I need just a theoretical approach, I'll try the coding myself :).
Thanks for any answer! I really do appreciate it! :)
","['reinforcement-learning', 'ai-design', 'q-learning', 'open-ai']",
How to add a pretrained model to my layers to get embeddings?,"
I want to use a pretrained model found in [BERT Embeddings] https://github.com/UKPLab/sentence-transformers and I want to add a layer to get the sentence embeddings from the model and pass on to the next layer. How do I approach this?
The inputs would be an array of documents and each document containing an array of sentences.
The input to the model itself is a list of sentences where it will return a list of embeddings. 
This is what I've tried but couldn't solve the errors:
def get_embeddings(input_data):

    input_embed = []
    for doc in input_data:
      doc = tf.unstack(doc)
      doc_arr = asarray(doc)
      doc = [el.decode('UTF-8') for el in doc_arr]
      doc = list(doc)
      assert(type(doc)== list)

      new_doc = []
      for sent in doc:
        sent = tf.unstack(sent)
        new_doc.append(str(sent))
        assert(type(sent)== str)

      embedding= model.encode(new_doc)  # Accepts lists of strings to return BERT sentence embeddings
      input_embed.append(np.array(embedding))

    return tf.convert_to_tensor(input_embed, dtype=float)


sentences = tf.keras.layers.Input(shape=(3,5)) #test shape
sent_embed = tf.keras.layers.Lambda(get_embeddings)


x = sent_embed(sentences)


","['neural-networks', 'word-embedding', 'bert', 'text-summarization', 'pretrained-models']","
I think you should use Keras embedding layer. It will be too easier than what you are doing.
Steps

Create Embedding Matrix
add matrix to embedding layer while building model.

You will find detailed article
https://www.cs.uaf.edu/2011/spring/cs641/lecture/04_05_modeling.html
"
How can Cat Swarm Algorithm (CSO) used for feature selection?,"
Cat swarm optimization (CSO) is a novel metaheuristic for evolutionary optimization algorithms based on swarm intelligence which proposed in 2006. See Feature Selection of Support Vector Machine Based on Harmonious Cat Swarm Optimization.
According to Modified Cat Swarm Optimization Algorithm for Feature Selection of Support Vector Machines

CSO imitates the behavior of cats through two sub-modes: seeking and tracing. Previous studies have indicated that CSO algorithms outperform other well-known meta-heuristics, such as genetic algorithms and particle swarm optimization. This study presents a modified version of cat swarm optimization (MCSO), capable of improving search efficiency within the problem space. The basic CSO algorithm was integrated with a local search procedure as well as the feature selection of support vector machines (SVMs).

Can someone explain how exactly Cat Swarm Algorithm (CSO) is used for feature selection?
","['machine-learning', 'feature-selection', 'swarm-intelligence', 'cat-swarm-optimization']",
How can I model and solve the Knight Tour problem with reinforcement learning?,"
I've read about the Knight Tour problem. And I wanted to try to solve it with a reinforcement learning algorithm with OpenAI's gym.
So, I want to make a bot that can move on the chess table like the knight. And it is given a reward each time it moves and does not leave the table or step in an already visited place. So, it gets better rewards if it survives more.
Or there is a better approach to this problem? Also, I would like to display the best knight in each generation.
I'm not very advanced at reinforcement learning (I'm still studying it), but this project really caught my attention. I know well machine learning and deep learning.
Do I need to start implementing a new OpenAI's gym environment and start all from scratch, or there is a better idea?
","['reinforcement-learning', 'ai-design', 'game-ai', 'q-learning', 'gym']",
"If the output of a model is a ridge function, what should the activation functions at all the nodes be?","
I have the following assignment.

I can't understand the b part of this question in my assignment. I have completed the 1st part and understand the maths behind it, but the 2nd part has me stumped.
I looked up ridge functions and they basically map real vectors to a single real value, from what I understood. For that reason, I considered that the activation function has to be one that ranges over the real numbers, but that still doesn't clear my doubts.
I don't need a full answer just an explanation of the question will be very helpful, here's some text from the book I'm referring( Russel and Norvig), though I couldn't really grasp how this would help me choose an activation function.

Before delving into learning rules, let us look at the ways in which networks generate complicated functions. First, remember that each unit in a sigmoid network represents a soft threshold in its input space, as shown in Figure 18.17(c) (page 726). With one hidden layer and one output layer, as in Figure 18.20(b), each output unit computes a soft-thresholded linear combination of several such functions. For example, by adding two opposite-facing soft threshold functions and thresholding the result, we can obtain a “ridge” function as shown in Figure 18.23(a). Combining two such ridges at right angles to each other (i.e., combining the outputs from four hidden units), we obtain a “bump” as shown in Figure 18.23(b).

","['neural-networks', 'machine-learning', 'gradient-descent', 'activation-functions']",
What is the advantage of using Google's Coral over Nvidia's Xavier?,"
I was reading about the possibility of using Google's Coral for deep learning-based object detection and image classification. I heard it has a good speed in terms of frames/sec.
I also read that Google's Coral is only compatible with quantized models. What does this mean? How will this affect the performance of object detection or classification in terms of accuracy and speed?
What is the advantage of using Google's Coral over Nvidia's Xavier?
","['deep-learning', 'convolutional-neural-networks']","
Quantization is a technique used to make deep learning models smaller and faster to run. 
Deep learning models are essentially collections of real-valued numbers. Because there are infinitely many real numbers, computers represent them using a format call 'floating point' numbers, which are not completely accurate. For example, a 32-bit floating point number can only represent at most $2^{32}$ distinct values. In contrast, a 64-bit floating point number can represent $2^{64}$ distinct values. 
Most CPUs and GPUs cannot operate directly on large floating point numbers. This means that to do something like multiply two floating point numbers, the CPU might have to work on half of each number at a time, and do some tricky work to combine the results. Some CPUs and GPUs can operate directly on large floating point numbers, but only by using more than one core to work on a single number.
To get around this, you might chose to take a model you have that was trained with high-precision floating point weights, and reduce it to lower precision. The weights won't be exactly the same, but they'll be very close. Doing this will make the models run much faster, but you might lose some accuracy.
So the advantage of using a tool that only supports Quantized models is that models will run faster, but might be slightly less accurate.
"
Does the reduction of the dimensions over multiple layers allow more details to be stored within the final representation?,"
From : https://debuggercafe.com/implementing-deep-autoencoder-in-pytorch/
the following autoencoder is defined
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()

        # encoder
        self.enc1 = nn.Linear(in_features=784, out_features=256)
        self.enc2 = nn.Linear(in_features=256, out_features=128)
        self.enc3 = nn.Linear(in_features=128, out_features=64)
        self.enc4 = nn.Linear(in_features=64, out_features=32)
        self.enc5 = nn.Linear(in_features=32, out_features=16)

        # decoder 
        self.dec1 = nn.Linear(in_features=16, out_features=32)
        self.dec2 = nn.Linear(in_features=32, out_features=64)
        self.dec3 = nn.Linear(in_features=64, out_features=128)
        self.dec4 = nn.Linear(in_features=128, out_features=256)
        self.dec5 = nn.Linear(in_features=256, out_features=784)

    def forward(self, x):
        x = F.relu(self.enc1(x))
        x = F.relu(self.enc2(x))
        x = F.relu(self.enc3(x))
        x = F.relu(self.enc4(x))
        x = F.relu(self.enc5(x))

        x = F.relu(self.dec1(x))
        x = F.relu(self.dec2(x))
        x = F.relu(self.dec3(x))
        x = F.relu(self.dec4(x))
        x = F.relu(self.dec5(x))
        return x

net = Autoencoder()

From the Autoencoder class, we can see that 784 features are passed through a series of transformations and are converted to 16 features.
The transformations (in_features to out_features) for each layer are:
784 to 256
256 to 128
128 to 64
64 to 32
32 to 16

Why do we perform this sequence of operations? For example, why don't we perform the following sequence of operations instead?
784 to 256
256 to 128

Or maybe
784 to 512
512 to 256
256 to 128

Or maybe just encode in two layers:
784 to 16

Does the reduction of the dimensions over multiple layers (instead of a single layer) allow more details to be stored within the final representation? For example, if we used only the transformation $784 \rightarrow 16$, may this cause some detail not to be encoded? If so, why is this the case?
","['neural-networks', 'autoencoders']",
What should the action space for the card game Crib be?,"
I'm working on creating an environment for a card game, which the agent chooses to discard certain cards in the first phase of the game, and uses the remaining cards to play with. (The game is Crib if you are familiar with it.)
How can I make an action space for these actions? For instance, in this game, we could discard 2 of 6 cards, then choose 1 of 4 remaining cards to play, then 1 of 3 remaining cards, then 1 of 2 remaining cards. How do I model this?
I've read this post on using MultiDiscrete spaces, but I'm not sure how to define this space based on the previous chosen action. Is this even the right approach to be taking?
","['reinforcement-learning', 'ai-design', 'open-ai', 'environment', 'gym']",
Applications of polar decomposition in Machine Learning,"
Assume there exists a new and very efficient algorithm for calculating the polar decomposition of a matrix $A=UP$, where $U$ is a unitary matrix and $P$ is a positive-semidefinite Hermitian matrix. Would there be any interesting applications in Machine Learning? Maybe topic modeling? Or page ranking? I am interested in references to articles and books.
","['machine-learning', 'graphs', 'text-classification', 'semantics', 'singular-value-decomposition']",
"How does publishing in the deep learning world work, with respect to journals and arXiv?","
Let's say I implemented a new deep learning model that pushed some SOTA a little bit further, and I wrote a new paper about for publication.
How does it work now? I pictured three options:

Submit it to a conference. Ok, that's the easy one, I submit it to something like NeurIPS or ICML and hope to get accepted. At that point, how do you make your paper accessible? Are there problems in uploading it to arXiv later, in order to get read by more people?
Upload it on arXiv directly. If I do that it would not be peer-reviewed, and technically speaking it would be devoid of ""academic value"". Right? It could easily be read by anyone, but there would be no formal ""proof"" of its ""scientific quality"". Correct me if I'm wrong.
Submit it to a peer-reviewed journal. Avoid desk rejection, avoid reviewers' rejection, after a long painful process it ends up on some international scientific journal. At that point, since the article is formally the editor's property, can you still upload it on arXiv, or on your blog, so that it can be accessible by many people?
How do the big stars of deep learning research do when they have some hot new paper ready for publication? And what publications are the most valued in the professional and the academic world?

","['deep-learning', 'research', 'papers', 'academia']",
How estimate the minimum size of an autoencoder to overfit the training data?,"
Given e.g. $1$M vectors of $1000$ floating points each, where every point in vectors is sampled from a uniform distribution between $-1$ to $1$, how to estimate the minimum network size required between input ($1000$ units), bottleneck (preferably $1$ unit), and output ($1000$ units) which is cable of overfitting the training data perfectly?
","['neural-networks', 'machine-learning', 'autoencoders', 'overfitting', 'computational-learning-theory']",
Is it possible to have the latent vector of an auto-encoder with size 1?,"
Given e.g. 1M vectors of $1000$ floating points each, where every point in vectors is sampled from a uniform distribution between $-1$ to $1$:
Is it possible to have the bottleneck of the AE network with size 1? In other words, without caring about generalization, is it possible to train a network, where, given only 1 encoded value, it can recreate any of the 1M examples?
","['neural-networks', 'machine-learning', 'autoencoders', 'information-theory']","
According to various experimentation on autoencoders, it is very possible to have latent vector of size 1. Various layers can help the downsizing of the original input to a very small size of 1. But an issue may arise during decoding. If you're expecting that through one or two or maybe five layers in decoder you can achieve an accurate reconstruction, it is highly unlikely abd the result will turn out to be blurry. Maybe a great network with various parameters may help the reconstruction without considering generalization as asked by you.
"
Why is update rule of the value function different in policy evaluation and policy iteration?,"
In the textbook ""Reinforcement Learning: An Introduction"", by Richard Sutton and Andrew Barto, the pseudo code for Policy Evaluation is given as follows:

The update equation for $V(s)$ comes from the Bellman equation for $v_{\pi}(s)$ which is mentioned below (the update equation) for your convenience:
$$v_{k+1}(s) = \sum_{a} \pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_{k}(s')]$$
Now, in Policy Iteration, the Policy Evaluation comes in stage 2, as mentioned in the following pseudo code:

Here, in the policy Evaluation stage, $V(s)$ is updated using a different equation:
$$\begin{align} v_{k+1}(s) = \sum_{s',r}p(s',r|s,\pi (s))[r + \gamma v_{k}(s)] \end{align}$$ where $a = \pi(s)$ is used. 
Can someone please help me in understanding why this change is made in Policy Iteration? Are the two equations the same?
","['reinforcement-learning', 'comparison', 'policy-iteration', 'policy-evaluation', 'dynamic-programming']",
What are mono-variable and multi-variable neural networks?,"
In this document, the terms ""Redes Neuronales estáticas monovariables"" and ""Redes Neuronales estáticas multivariables"" are mentioned.
What are mono-variable and multi-variable neural networks? Is it the same as a multi-layer or uni-layer NN?
I have searched about multivariable/mono-variable static/dynamic neural networks in some books, but at least in those books there's no information about these topics.
I have the idea it refers to the inputs/outputs, but I'm not sure.
","['neural-networks', 'terminology']","

What are mono-variable and multi-variable neural networks?

I am not sure about this, because most (if not all useful) neural networks are multivariable neural networks (i.e. they contain multiple parameters). Even the perceptron usually contains more than one parameter, so that terminology isn't clear even to me. Maybe they are referring to the number of inputs (sometimes called variables), but I don't see why this distinction in this context would make sense.

What are static and dynamic neural networks?

To answer this question, I will first quote an excerpt from this document (written in Spanish) to provide some context to Spanish speakers (I am not a Spanish speaker, but I understand 99% of it).

Un primer intento de clasificación puede separ estáticos y dinámicos o recurrentes (fig.2.1)
Los modelos estáticos realizan un mapeo entre entrada y salida. Despreciando el tiempo de procesamiento interno, la salida se obtiene en forma inmediata en función de la entrada, no existe memoria ni dinámica de estados en el sistema neuronal.
Por el contrario los sistemas recurrentes si la poseen, son sistemas realimentados que ante un estimulo de entrada evolucionan hasta converger a una salida estable.
Casos tipicos de ambos sistemas son el Perceptrón (Rosemblatt, 1960a) (de una o múltiples capas) y la memoria asociativa de Hopfield, respectivamente
(Tank, 1987).

So, in this document, the word ""dynamic"" and ""recurrent"" are being used interchangeably. An example of a static (i.e. non-recurrent) neural network is the perceptron. An example of recurrent (or dynamic) neural network is the Hopfield network.
Anyway, I recommend you contact the author of that article to ask for clarification (especially, about the mono-variable NNs)!
"
Can I find a mapping that minimizes the maximum distance ratio of certain vectors?,"
Let's say we have several vector points. My goal is to distinguish the vectors, so I want to make them far from each other.  Some of them are already far from each other, but some of them can be positioned very closely.  
I want to get a certain mapping function that can separate such points that are close to each other, while still preserving the points that are already far away from each other. 
I do not care what is the form of the mapping. Since the mapping will be employed as pre-processing, it does not have to be differentiable or even continuous.  
I think this problem is somewhat similar to 'minimizing the maximum distance ratio between the points'. Maybe this problem can be understood as stretching the crushed graph to a sphere-like isotropic graph. 
I googled it for an hour, but it seems that the people are usually interested in selecting the points that have such nice characteristics from a bunch of data, rather than mapping an existing vector points to a better one.
So, in conclusion, I could not find anything useful.   
Maybe you can think 'the neural network will naturally learn it while solving classification problem'. But it failed. Because it is already struggling with too many burdens. So, this is why I want to help my network with pre-processing.  
","['deep-learning', 'data-preprocessing']","
An interesting question. I would start by finding n nearest neighbors of each data point, then calculate their center of mass c and the point's distance d to its nth nearest neighbor. The smaller the d is the larger the density is around a given point. You could then iteratively step every point away from their c in an inverse proportion to the distance d with a suitable step size. This would spread out the clusters.
But this won't help you transform any new points in the dataset, maybe you can learn this arbitrary mapping R^n -> R^n by using an other neural network and apply it to new samples?
This is the first ad-hoc idea which came to my mind. It would be interesting to see a 2D animation of this.
A more rigorous approach might be a variational autoencoder, you can embed the data in a lower dimensional space with approximately normal distribution. But it doesn't guarantee that clusters would be as spread out as you'd like. An alternative loss function would help with that, for example every point's distance to their original nth closest neighbor should be as close to one as possible.
"
How do we choose the filters for the convolutional layer of a convolution neural network?,"
Since the hidden layers of a CNN work as a trainable feature extractor, more detailed content based on a larger number of pixels shall require bigger filter sizes. But for cases where localized differences are to receive greater attention, smaller filter sizes are required.
I know there is a lot of topic on the internet regarding CNN and most of them have a simple explanation about Convolution Layer and what it is designed for, but they don’t explain

How many convolution layers are required?
What filters should I use in those convolution layers?

","['neural-networks', 'convolutional-neural-networks', 'hyperparameter-optimization', 'convolution']",
How do I derive the gradient with respect to the parameters of the softmax policy?,"
The gradient of the softmax eligibility trace is given by the following:
\begin{align}
\nabla_{\theta} \log(\pi_{\theta}(a|s)) &= \phi(s,a) - \mathbb E[\phi (s, \cdot)]\\
&= \phi(s,a) - \sum_{a'} \pi(a'|s) \phi(s,a')
\end{align}
How is this equation derived?
The following relation is true:
\begin{align}
\nabla_{\theta} \log(\pi_{\theta}(a|s)) &= \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} \tag{1}\label{1}
\end{align}
Thus, the following relation must also be true:
\begin{align}
\frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)}  &=\phi(s,a) - \sum_{a'} \pi(a'|s) \phi(s,a')
\end{align}
Mathematically, why would this be the case? Probably, you just need to answer my question above because \ref{1} is true and it's just the rule to differentiate a logarithm.
","['reinforcement-learning', 'policy-gradients', 'eligibility-traces']",
How should I handle invalid actions in a grid world?,"
I'm building a really simple experiment, where I let an agent move from the bottom-left corner to the upper-right corner of a $3 \times 3$ grid world.
I plan to use DQN to do this. I'm having trouble handling the starting point: what if the Q network's prediction is telling the agent to move downward (or leftward) at the beginning?
Shall I program the environment to immediately give a $-\infty$ reward and end this episode? Will this penalty make the agent ""fear"" of moving left again in the future, even if moving left is a possible choice?
Any suggestions?
","['reinforcement-learning', 'q-learning', 'dqn', 'reward-design', 'reward-functions']",
Can you use transformer models to do autocomplete tasks?,"
I've researched online and seen many papers on the use of RNNs (like LSTMs or GRUs) to autocomplete for, say, a search engine, character by character. Which makes sense since it inherently predicts character-by-character in a sequential manner. 
Would it be possible to use the transformer architecture instead to do search autocomplete? If so, how might such a model be adapted? 
","['neural-networks', 'recurrent-neural-networks', 'transformer']",
Is there any published research on the information-carrying capacity of the human face?,"

Is there any published research on the information-carrying capacity of the human face?

Here I mean ""how much information can be conveyed via facial expressions & micro-expressions"".  
This is a subject of interest because the human face is arguably ""the most interesting"" single thing for humans, since it's likely the first real pattern we recognize as infants, and conveys so much non-verbal communication than can relate to achievement of a goal or identification of a mortal threat.  (Dogs similarly are said to have co-evolved to read human faces. Film acting as ""the art of the closeup"" also validates this viewpoint.)
Essentially, I'm tying to get a sense of how complex is the set of human facial expressions, what is the computational complexity of the range of problems related to identification of the range of possible expressions, and the emulation of such expressions to imitate a human agent.  (i.e. these techniques can be used to ""read"" a human subject or manipulate a human subject.)
Well researched articles & blogs would also be welcome.
","['reference-request', 'human-like', 'facial-recognition', 'complexity-theory', 'information-theory']",
How does the repetition of features across states at different time steps affect learning?,"
Let's say you are training a neural network in an RL setting, where the state (i.e. features/input data) can be the same for multiple successive steps (~typically around 8 steps) of an episode.
For example, an initial state might consist of the following values:
[30, 0.2, 0.5, 1, 0]

And then again the same state could be fed into the neural network for e.g. 6-7 times more, resulting in ultimately the following input arrays:
[[30, 0.2, 0.5, 1, 0], 
 [30, 0.2, 0.5, 1, 0], 
 ..., 
 [30, 0.2, 0.5, 1, 0]]

I know that the value 0 in the feature set depicts that the weight for this feature results in insignificant value. 
But what about the repetition of values? How does that affect learning, if it does at all? Any ideas?
Edit: I am going to provide more information as requested in the comments.
The reason I did not provide this information in the first place, is because I thought there would be similarities in such cases across problems/domains of application. But it is also fine to make it more specific.

The output of the network is a probability among two paths. Our network has to select an optimal path based on some gathered network statistics.
I will be using A3C, as similar work in the bibliography has made progress.
The reason the agent is staying in the same state is the fact that the protocol can also make path selection decisions at the same time, without an actual update of network statistics. So in that case, you would have the same RTT for instance.
i. This is a product of concurrency in the protocol
ii. It is expected behavior 

","['neural-networks', 'deep-learning', 'reinforcement-learning', 'deep-rl']",
How does sampling works in case of imbalanced image datasets?,"
I am solving a problem of image classification of the image dataset for 3 classes. Dataset is highly imbalanced.
How will sampling (either over- or under-sampling) work in that case? Should I remove (or add) any random number of images, or should I follow some pattern?
In the case of CSV data, the general rule is to do PCA, and then remove the data points, but how to do it in the image dataset? Is there any other way to handle this problem?
","['deep-learning', 'convolutional-neural-networks', 'classification', 'datasets', 'data-preprocessing']",
"How do CNNs or RNNs ""stack the feature of nodes by a specific order""?","
I am trying to understand the following statement taken from the paper Graph Neural Networks: A Review of Methods and Applications (2019). 

Standard neural networks like CNNs and RNNs cannot handle the graph input properly in that they stack the feature of nodes by a specific order.

This statement is confusing to me. I have not used CNNs/RNNs for non-Euclidean data before, so perhaps that's where my understanding falls off. 
How do CNNs/RNNs stack the feature of nodes by a specific order?
","['convolutional-neural-networks', 'recurrent-neural-networks', 'terminology', 'geometric-deep-learning', 'graph-neural-networks']",
What is the difference between a Bayesian Network and a Markov Chain?,"
I am trying to understand the difference between a Bayesian Network and a Markov Chain.
When I search for this one the web, the unanimous solution seems to be that a Bayesian Network is directional (i.e. it's a DAG) and a Markov Chain is not directional.
However, often a Markov Chain example is overtime, where the weather today is impacting the weather tomorrow, but the weather tomorrow is not (obviously) impacting the weather today. So I am quite confused how is a Markov Chain not directional?
I seem to be missing something here. Can someone please help me understand?
","['markov-chain', 'bayesian-networks']","
The main difference between a Bayesian network and a Markov chain is not that a Markov Chain is not directional, it is that the graph of the Bayesian network is not trivial whereas the graph of a Markov chain would be somewhat trivial, as all the previous $k$ nodes would just point to the current node. To illustrate further why this would be trivial, we let each node represent a random variable $X_i$. Then the nodes representing $X_i$ for $ t-k \leq i < t$ would be connected by a directed edge to $X_t$. That is, the edges $(X_i, X_t) \in E$ for $ t-k \leq i < t$ and where $E$ is the set of edges of the graph. 
To illustrate this please see the examples below. 
Assume that we have a $k$th order Markov chain, then by definition we have $\forall t > k$ $\mathbb{P}(X_t = x | X_0,...,X_{t-1}) = \mathbb{P}(X_t = x | X_{t-k},...,X_{t-1})$. 
The main difference between the above definition and the definition of a Bayesian Network is that due to the direction of the graph we can have different dependencies for each $X_t$. Consider the Bayesian Network in the Figure below 

We would get that $\mathbb{P}(X_4 = x| X_1, X_2, X_3) = \mathbb{P}(X_4 = x | X_2, X_3)$ and $\mathbb{P}(X_5 = x| X_1, X_2, X_3, X_4) = \mathbb{P}(X_5 = x | X_3)$. 
So, the past events that the current random variable depends on don't have to have the same 'structure' in a Bayesian Network as in a Markov Chain. 
"
How do you manage negative rewards in policy gradients?,"
This old question has no definitive answer yet, that's why I am asking it here again. I also asked this same question here.
If I'm doing policy gradient in Keras, using a loss of the form:
rewards*cross_entropy(action_pdf, selected_action_one_hot)

How do I manage negative rewards?
I've had success with this form in cases where the reward is always positive, but it does not train with negative rewards. The failure mode is for it to drive itself to very confident predictions all the time, which results in very large negative losses due to induced deviation for exploration. I can get it to train by clipping rewards at zero, but this throws a lot of valuable information on the table (only carrots, no sticks).
","['reinforcement-learning', 'keras', 'policy-gradients', 'rewards', 'cross-entropy']","
You don't need to manage negative rewards separately, if you implemented the algorithm correctly it will work regardless if the rewards are negative or not. You seem to be using rewards for the loss but you should be using the return which is the sum of the rewards for some state action pair from that point until the end of trajectory.
You also seem to be missing $-$ sign from the loss. The objective function for the vanilla policy gradient algorithm (REINFORCE) which we want to maximize is
\begin{equation}
J = \sum_a \pi(a|s) q_{\pi}(s, a)
\end{equation}
It can be shown that the gradient sample for this policy gradient method is
\begin{equation}
\nabla J = G_t \nabla \log (\pi(A_t|S_t))
\end{equation}
so in TensorFlow you should define your loss as
\begin{equation}
J = - G_t \pi(A_t|S_t)
\end{equation}
We need the $-$ because in TensorFlow you use minimizers, but we want to maximize this function so minimizing this loss is same as maximizing the objective function. In conclusion, the code similar to what you wrote, should be
-return * cross_entropy(action_pdf, selected_action_one_hot)
EDIT
As pointed out in the comment we don't actually need $-$ because it is already included in cross_entropy function.
"
What is the difference between on-policy and off-policy for continuous environments?,"
I'm trying to understand RL applied to time series (so with infinite horizon) which have a continous state space and a discrete action space.
First, some preliminary questions: in this case, what is the optimal policy? Given the infinite horizon there is no terminal state but only an objective to maximise the rewards, so I can't run more than one episode, is it correct?
Consequently, what is the difference between on-policy and off-policy learning given this framework? 
","['reinforcement-learning', 'comparison', 'q-learning', 'off-policy-methods', 'on-policy-methods']",
Does the concept of validation loss apply to training deep Q networks?,"
In deep learning, the concept of validation loss is to ensure that the model being trained is not currently overfitting the data. Is there a similar concept of overfitting in deep q learning?
Given that I have a fixed number of experiences already in a replay buffer and I train a q network by sampling from this buffer, would computing the validation loss (separate from the experiences in the replay buffer) help me to decide whether I should stop training the network? 
For example, If my validation loss increases even though my train loss continues to decrease, I should stop training the training. Does deep learning validation loss also apply in the deep q network case?
Just to clarify again, no experiences are collected during the training of the DQN.
","['reinforcement-learning', 'dqn', 'deep-rl', 'overfitting']",
"What are the differences between a deep belief network, a restricted Boltzmann machine and a deep Boltzmann machine?","
Can anyone list the differences between deep Belief network (DBN), restricted Boltzmann machine (RBM), deep Boltzmann machine (DBM) using simple examples?
Links to other resources are also appreciated.
","['comparison', 'boltzmann-machine', 'deep-belief-network', 'restricted-boltzmann-machine', 'deep-boltzmann-machine']",
How to best make use of learning rate scheduling in reinforcement learning?,"
How to best make use of learning rate scheduling in reinforcement learning?
To me, a low learning rate towards the end to fine-tune what you've learned with subtle updates makes sense. But I don't see why over training time this should be linearly brought down. Wouldn't this increase overfitting too, as it promotes an early adopted policy to get further and further finetuned for the rest of the training? 
Wouldn't it be better to keep it constant over the entire training so that when the agent finds novel experiences later, it still has a high enough learning rate to update its model?
I also don't really know how these modern deep RL papers do it. The starcraft II paper by DeepMind, and the OpenAI hide and seek paper don't mention learning rate schedules for instance.
Or are there certain RL environments where it's actually best to use something like a linear learning rate schedule?
","['reinforcement-learning', 'proximal-policy-optimization', 'learning-rate']","
I have not used learning rate schedules, but I do have experience with adjustable learning rates.
The Keras callback ReduceLROnPlateau is useful for adjusting the learning rate. If you use it to monitor the validation loss versus training loss, you will avoid the danger of overfitting. Also, you can use the ModelCheckpoint callback to save the model with the lowest validation loss and use that to make predictions. The documentation is here.
I look at the validation loss as a deep valley in $N$ space, where $N$ is the number of trainable parameters. As you progress down the valley, it becomes increasingly narrower, so it is best to reduce the learning rate to get further down the valley (closer to the minimum). With the adjustable learning rate, you can start with a larger initial rate that converges faster, then reduces as needed to achieve a minimum loss.
I wrote a custom callback that initially monitors the training loss and adjusts the learning rate based on that until the training accuracy achieves 95%, then it switches to adjusting the learning rate based on validation loss.
I also am experimenting with a slightly different approach to training. On a given epoch, assume the quantity you are monitoring does NOT improve. That means that you have moved to a point in $N$ space (value of the weights) that is NOT as ""good"" as the point you were at in the previous epoch. So, instead of training from the point you are at in the current epoch, I set the weights back to what they were for the previous (better) epoch, reduce the learning rate then continue training from there. This appears to work rather well.
"
"Why does this multiplication of $Q$ and $K$ have a variance of $d_k$, in scaled dot product attention?","
In scaled dot product attention, we scale our outputs by dividing the dot product by the square root of the dimensionality of the matrix:

The reason why is stated that this constrains the distribution of the weights of the output to have a standard deviation of 1.
Quoted from Transformer model for language understanding | TensorFlow:

For example, consider that $Q$ and $K$ have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of $d_k$. Hence, square root of $d_k$ is used for scaling (and not any other number) because the matmul of $Q$ and $K$ should have a mean of 0 and variance of 1, and you get a gentler softmax.

Why does this multiplication have a variance of $d_k$?
If I understand this, I will then understand why dividing by $\sqrt({d_k})$ would normalize to 1.
Trying this experiment on 2x2 arrays I get an output of 1.6 variance:

","['neural-networks', 'machine-learning', 'natural-language-processing', 'transformer', 'attention']","
It might help to take two small matrices that match the assumptions (mean of zero and variance of one) and just do the matrix multiplication.  The dimensionality of K scales Q in the multiplication, scaling the variance simultaneously.
"
Is there an AI tool to reverse engineer scanned data to obtain its CAD file?,"
Today, if you scan an object and want its CAD file (Solidworks/Autocad), you need to use reverse engineering software (Geomagic). This takes time and you need experience of the software tools. 
Is there an AI tool/app that does the job automatically? If not, is this a reasonable idea to develop an AI application capable of doing it? What would be the biggest challenges?
","['ai-design', 'image-recognition', 'resource-request']","
Two years later, it seems there is no (open) project to train AI on engineering data and no AI tool to reverse engineer things yet.
The growing number of open source hardware projects should provide a good basic training set.. https://oho.wiki and https://certification.oshwa.org/list.html provide catalogues of specimens for example.. Realtime 3d generation should also be good enough by now to get this going..
"
How to feed key-value features (aggregated data) to LSTM?,"
I have the following time-series aggregated input for an LSTM-based model:
x(0): {y(0,0): {a(0,0), b(0,0)}, y(0,1): {a(0,1), b(0,1)}, ..., y(0,n): {a(0,n), b(0,n)}}
x(1): {y(1,0): {a(1,0), b(1,0)}, y(1,1): {a(1,1), b(1,1)}, ..., y(1,n): {a(1,n), b(1,n)}}
...
x(m): {y(m,0): {a(m,0), b(m,0)}, y(m,1): {a(m,1), b(m,1)}, ..., y(m,n): {a(m,n), b(m,n)}}

where x(m) is a timestep, a(m,n) and b(m,n) are features aggregated by the non-temporal sequential key y(m,n) which might be 0...1,000.
Example:
0: {90: {4, 4.2}, 91: {6, 0.2}, 92: {1, 0.4}, 93: {12, 11.2}}
1: {103: {1, 0.2}}
2: {100: {3, 0.1}, 101: {0.4, 4}}

Where 90-93, 103, and 100-101 are aggregation keys.
How can I feed this kind of input to LSTM?
Another approach would be to use non-aggregated data. In that case, I'd get the proper input for LSTM. Example:
Aggregated input:
0: {100: {3, 0.1}, 101: {0.4, 4}}

Original input:
0: 100, 1, 0.05
1: 101, 0.2, 2
2: 100, 1, 0
3: 100, 1, 0.05
4: 101, 0.2, 2

But in that case, the aggregation would be lost, and the whole purpose of aggregation is to minimize the number of steps so that I get 500 timesteps instead of e.g. 40,000, which is impossible to feed to LSTM. If you have any ideas I'd appreciate it.
","['machine-learning', 'long-short-term-memory', 'data-preprocessing', 'structured-data', 'feature-engineering']",
Is this dataset with only two features suitable for clustering with k-means?,"
I am working with the K-means clustering algorithm for unsupervised learning. 
Is the following dataset suitable for the k-means clustering task or not? Why or why not? The dataset has only two features.


","['datasets', 'unsupervised-learning', 'data-science', 'clustering', 'k-means']","
One problem with clustering algorithms is that they will typically find you a solution, ie they will split your data set into clusters, but it will find you a structure even if there isn't one. Your data looks like it could consist of about 5 to 7 clusters, but it could equally well just be 2 or only 1.
What you need to do after the clustering is to assess the quality of the result. I recommend having a look at Finding Groups in Data by Kaufman & Rousseeuw. They discuss various clustering algorithms and also a procedure that works out how cohesive your clusters are. Though it is 30 years old, it is an excellent book on the topic.
You also have the issue of choosing a value for k in your clustering: I usually start with two, and increase it from there; at each step I compute the cohesion of the result using their method, until I get the best score. This is an objective way of finding a good value for k and usually a reasonable clustering result.
The ultimate test, of course, is then if looking at the result makes sense to you. No cluster algorithm can do that for you.
"
How do I derive the gradient with respect to the parameters of the softmax policy?,"
The gradient of the softmax eligibility trace is given by the following:
\begin{align}
\nabla_{\theta} \log(\pi_{\theta}(a|s)) &= \phi(s,a) - \mathbb E[\phi (s, \cdot)]\\
&= \phi(s,a) - \sum_{a'} \pi(a'|s) \phi(s,a')
\end{align}
How is this equation derived?
The following relation is true:
\begin{align}
\nabla_{\theta} \log(\pi_{\theta}(a|s)) &= \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} \tag{1}\label{1}
\end{align}
Thus, the following relation must also be true:
\begin{align}
\frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)}  &=\phi(s,a) - \sum_{a'} \pi(a'|s) \phi(s,a')
\end{align}
Mathematically, why would this be the case? Probably, you just need to answer my question above because \ref{1} is true and it's just the rule to differentiate a logarithm.
","['reinforcement-learning', 'policy-gradients', 'eligibility-traces']",
Confidence Interval around prediction with bootstrapping,"
I want to generate a confidence interval around my prediction (vector) $\hat{y}$. I have implemented the following procedure. However, I am not sure whether this makes sense in a statistical way:

I have a data set. First I split it into a 80% training set (2000 measurements), 10% valdidation set and 10% testing set (250 measurements)
I resample B ($\sim 100$) training sets from the original training set with replacement. 

For each of the B training datasets I train the model $b$ and
validate it 
(everytime I use the same validation set).
I use the testset from point 1. and make a prediction $\hat{y}_i^b$ 
(so everytime I use the same test set, since I need the predictions for the same input values)

I calculate the average of the $B$ predictions. $$\bar{\hat{y}}_i=\frac{1}{B}\sum_{b=1}^B\hat{y}_i^b$$.
I calculate the variance  ($i\in [1,250]$) 
$$\sigma_{\hat{y}_i}^2=\frac{1}{B-1}\sum_{b=1}^B(\hat{y}_i^b -\bar{\hat{y}}_i)^2$$
I guess the $95\%$ confidence interval for the prediction $\hat{y}_i$ is $$\hat{y}_i\in \bar{\hat{y}}_i\pm z_{0.025}\frac{\sigma_{\hat{y}_i}}{\sqrt{B}}$$
with $z_{0.025}=1.96$
If I sort the $\hat{y}_i$ values and plot it together with the upper
and lower bound, I will get the prediction curve with a CI.

My biggest uncertainty relates to step 5). I read in a book Supervised Classification:Quite a Brief Overview, Marco Loog: 

When the population standard deviation $\sigma$ is known and the parent population is normally distributed or $N>30$ the $100(1-\alpha)$ CI for the population mean is given by the symmetrical distribution for the standardized normal distribution $z$
$$\mu\in \bar{x}\pm z_{a/2}\frac{\sigma}{\sqrt{N}}$$

Is it correct to say here $N=B$ (number of bootstrap models or number of resampled trainingsets or number of estimators $\hat{y}_i^b$ ). Does the procedure make sense?
","['neural-networks', 'training', 'prediction']",
Where does reinforcement learning actually show up in Deepmind's game engines?,"
From the brief research I've done on the topic, it appears that the way Deepmind's Alphazero or Muzero makes decisions is through Monte Carlo tree searches, where in the randomized simulations allows for a more rapid way to make calculations than traditional alpha-beta pruning. As the simulation space increases, this search approaches that of a classical tree search. 
Where exactly did Deepmind use neural networks? Was it in the evaluation portion? And if so, how did they make determinations on what makes a ""good"" or ""bad"" game state? If they deferred the evaluations of another chess engine like Stockfish, how do we see AlphaZero absolutely demolish Stockfish in head-to-head matches? 
","['deep-learning', 'monte-carlo-tree-search', 'chess', 'alphazero', 'deepmind']",
"Is there an AI system that, given a patient's symptoms, produces a diagnosis and suggests a treatment?","
Is there an AI system (preferably, one that interacts with the human, such as a chatbot like this one) that, given some input (e.g. entered into the system by writing text), such as a person's physical history and symptoms (of a certain disease), produces a diagnosis of the disease and/or suggests medications or a treatment to improve the condition of the patient?
","['applications', 'reference-request', 'chat-bots', 'resource-request', 'healthcare']",
Is there any practical application of knowing whether a concept class is PAC-learnable?,"
A concept class $C$ is PAC-learnable if there exists an algorithm that can output a hypothesis with probability at least $(1-\delta)$ (the ""probably"" part), and an error that is less than $\epsilon$ (the ""approximately"" part), in time that is polynomial in $1/\epsilon$, $1/\delta$, $n$ and $|C|$. 
Tom Mitchell defines an upper bound for the sample complexity, $m >= 1/\epsilon (ln(|H|) + ln(1/\delta))$ for the finite hypotheses. Based on this bound, he classifies whether target concepts are PAC-learnable or not. For example, $n$ conjunction boolean literal concept class. 
It seems to me that PAC-learnability seeks to act more like a classification of certain concept classes. 
Are there any practical purposes for knowing whether a concept class is PAC-learnable? 
","['applications', 'computational-learning-theory', 'pac-learning', 'sample-complexity', 'hypothesis-class']","

Is there any practical application of knowing whether a concept class is PAC-learnable?

If you know that a concept class is PAC-learnable (i.e. its VC dimension is finite), then there's a possibility that you can design an algorithm that can find a function (or concept) that is arbitrarily close to your target (or desired) function.
This is not really an application, but a consequence, which can lead to applications. 
However, note that asking if PAC learning is useful in practice is like asking if special relativity is useful in practice. Yes, they are useful, but in the sense that they can be used to predict the outcomes of an experiment or explain the rules in their specific context. In the case of machine learning, PAC learning can be used to explain e.g. the probably required number of data points needed to learn a concept (a target function) approximately.
See also Are PAC learning and VC dimension relevant to machine learning in practice? for more concrete ""applications"" of PAC-learning and the VC dimension.
"
What is the difference between deep learning and shallow learning? [duplicate],"







This question already has answers here:
                                
                            




How is a deep neural network different from other neural networks?

                                (2 answers)
                            


What is the difference between machine learning and deep learning?

                                (5 answers)
                            

Closed 1 year ago.



What is the difference between deep learning and shallow learning?
What I am interested in knowing is not the definition of deep learning and shallow learning, but understanding the actual difference.
Links to other resources are also appreciated.
","['machine-learning', 'deep-learning', 'comparison']",
What is a Hidden Markov Model - Artificial Neural Network (HMM-ANN)?,"
As far as I know, neural networks have hidden computational units and HMM has hidden states.
Hidden Markov Models can be used to generate a language, that is, list elements from a family of strings. For example, if you have an HMM that models a set of sequences, you would be able to generate members of this family, by listing sequences that would befall into the group of sequences we are modeling.
In this, this and this paper, HMMs are combined with ANNs. But how exactly? What is a Hidden Markov Model - Artificial Neural Network (HMM-ANN)? Is HMM-ANN a hybrid algorithm? In simple words, how is this model or algorithm used?
","['neural-networks', 'comparison', 'papers', 'hidden-markov-model']",
What are some use cases of few-shot learning?,"
Besides computer vision and image classification, what other use cases/applications are for few-shot learning?
","['machine-learning', 'reference-request', 'applications', 'few-shot-learning']","
Few-short learning (FSL) can be useful for many (if not all) machine learning problems, including supervised learning (regression and classification) and reinforcement learning.
The paper Generalizing from a Few Examples: A Survey on Few-Shot Learning (2020) provides an overview (including examples of applications and use cases) of FSL. Their definition of FSL provided is based on Tom Mitchell's famous definition of machine learning.

Definition 2.1 (Machine Learning [92, 94]). A computer program is said to learn from experience $E$ with respect to some classes of task $T$ and performance measure $P$ if its performance can improve with $E$ on $T$ measured by $P$.

Here's the definition of FSL.

Definition 2.2. Few-Shot Learning (FSL) is a type of machine learning problems, specified by $E$, $T$ and $P$, where $E$ contains only a limited number of examples with supervised information for the task $T$.

Specific examples of applications of FSL are

character generation
drug toxicity discovery
sentiment classification from short text
object recognition

"
How is visual attention mechanism different from a two branch convolutional neural network?,"
I am doing some research on the visual attention mechanism in remote sensing domain (where the features learnt from one layer are highlighted using the attention mask derived from another layer). From what I have observed, the attention mask is learnt in a similar fashion as any other branch in CNN. So, what is so special about the visual attention mask that makes it different from a regular two branch CNN? The reference papers are provided below:
Visual Attention-Driven Hyperspectral Image Classification (IEEE, 2019)
A Two-Branch CNN Architecture for Land Cover
Classification of PAN and MS Imagery (MDPI, 2019)
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'attention']",
How is the incremental update rule derived from the weighted importance sampling in off-policy Monte Carlo control?,"
Here's the approximated value using weighted importance sampling
$$
V_{n} \doteq \frac{\sum_{k=1}^{n-1} W_{k} G_{k}}{\sum_{k=1}^{n-1} W_{k}}, \quad n \geq 2
$$
Here's the incremental update rule for the approximated value
$$V_{n+1} \doteq V_{n}+\frac{W_{n}}{C_{n}}\left[G_{n}-V_{n}\right], \quad n \geq 1$$
How is the second equation derived from the first? 
These are used for the weighted importance sampling method of off-policy Monte Carlo control.
","['reinforcement-learning', 'monte-carlo-methods', 'value-functions', 'importance-sampling']",
Which machine learning method can take a matrix as input?,"
I am pretty new to the machine learning field. I want to use an $n \times m$ matrix as the input of a model, in order to predict a vector $1 \times m$, both of real numbers. Input data are quite clean, with statistics of about 10000 items. 
Do you know a method that can handle that?
","['machine-learning', 'ai-design', 'reference-request']","
ANNs can do the trick. Check out sklearn's ANN example with the Digits dataset, which consists of 28x28 pixel input data.
"
How do I calculate the partial derivative with respect to $x$?,"
I am trying to implement CNN using python NumPy. I searched so much, but all I found was for one filter with one channel for convolution.
Suppose $x$ is an image with the shape: (N_Height, N_Width, N_Channel) = (5,5,3).
Let's say I have 16 filters with this shape: (F_Height, F_Width, N_Channel) = (3,3,3) , stride=1 and padding=0
Forward:
The output shape after convolution 2d will be
(
math.floor((N_Height - F_Height + 2*padding)/stride + 1 )),
math.floor((N_Width- F_Width + 2*padding)/stride + 1 )),
filter_count
)

So, the output of this layer will be an array with this shape: (Height, Width, Channel) =  (3, 3, 16)
BackPropagation:
Suppose $dL/dh$ is the input for my layer in back-propagation with this shape: (3, 3, 16)
Now, I must find $dL/dw$ and $dL/dx$: $dL/dw$ to update my filters parameter and $dL/dx$ to pass it as input to the previous layer as the loss respect to the input $x$.
From this answer Error respect to filters weights I found how to calculate $dL/dw$.
The problem I have in the back-propagation is I don't know how to calculate $dL/dx$ having this shape: (5, 5, 3) and pass it to the previous layer.
I read lots of articles in Medium and other sites, but I don't get how to calculate it:

How Backpropagation works in a CNN
The best explanation of Convolutional Neural Networks on the Internet!
Backpropagation In Convolutional Neural Networks
How to propagate error back to previous layer in CNN?

","['deep-learning', 'convolutional-neural-networks', 'backpropagation']","
While this may not be the answer you were looking for, I hope this explanation will help you to understand applying backpropagation to a CNN. Fundamentally, convolutional layers are no different than dense layers, however there are restrictions. The key one is weight-sharing which allows a CNN to be much more efficient than a regular dense layer (as well as it being sparse due to locality). Imagine we are transforming a 4x4 image into a 2x2 image. Since we are inputting a 16-vector, and outputting a 4-vector, we need a weights matrix of 4x16:

This has 64 parameters. In a convolutional layer, we can accomplish this by convolving a 3x3 kernel over the image:
$$ K=
\begin{bmatrix}
k_{1,1} & k_{1,2} & k_{1,3} \\
k_{2,1} & k_{2,2} & k_{2,3} \\
k_{3,1} & k_{3,2} & k_{3,3}
\end{bmatrix} $$
This convolution is equivalent to multiplying by the weights matrix:

As you can see, this only requires 9 parameters and backpropagation can be applied to update these parameters.
Image Source: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1
"
"Understanding the node information score in the paper ""Hierarchical Graph Pooling with Structure Learning""","
The paper Hierarchical Graph Pooling with Structure Learning (2019) introduces a distance measure between:

a graph's node-representation matrix $\text{H}$, and
an approximation of this constructed from each node's neighbours' information $\text{D}^{-1}\text{A}\text{H}$:


Here, we formally define the node information score as the Manhattan distance between the node representation itself and the one constructed from its neighbors:
$$\mathbb{p} = \gamma(\mathcal{G}_i) = ||(\text{I}^{k}_{i} - (\text{D}^{k}_{i})^{-1}\text{A}^{k}_{i})\text{H}^{k}_{i}||  $$

(where $\text{A}$ and $\text{D}$ are the Adjacency and Diagonal matrices of the graph, respectively)
Expanding the product on the RHS we get (ignoring index notation for simplicity):
$$||\text{H} - (\text{D}^{-1}\text{A}\text{H})||$$
Problem: I don't see how $\text{D}^{-1}\text{A}\text{H}$ is a ""node representation... constructed from its neighbors"".
$\text{I} - \text{D}^{-1}\text{A}$ is clearly equivalent to the Random Walk Laplacian, but it's not immediately obvious to me how multiplying this by $\text{H}$ provides per-node information on how well one can reconstruct a node from its neighbours.
","['papers', 'geometric-deep-learning', 'graph-neural-networks', 'spectral-analysis']",
Should we start with a small batch-size and increase during training to improve sample efficiency?,"
Just made an interesting observation playing around with the stable-baseline's implementation of PPO and the BipedalWalker environment from OpenAI's Gym. But I believe this should be a general property of deep learning.
Using a small batch size of 512 samples the walker achieves a near-optimal behavior after just 0.5 Mio steps. The optimized hyperparameters in the RL Zoo suggest using a batch size of 32k steps. This definitely leads to better performance after 5 Mio steps but takes 2 Mio steps until it reaches a near-optimal behavior. 
Therefore the question: 
Shouldn't we schedule the batch-size to improve sample efficiency? 
I believe it makes sense because after initialization the policy is far away from the optimal one and therefore should update quickly to get better. Even when the gradient estimates using small batches are very noisy, they still seem to bring the policy quickly in a quite good state. Thereafter we can increase the batch-size and make less but more precise gradient steps. Or am I missing an important point here?
","['deep-learning', 'deep-rl', 'hyperparameter-optimization', 'hyper-parameters', 'sample-efficiency']",
"If features are always positives, why do we use RELU activation functions?","
When does it happen that a layer (either first or hidden) outputs negative values in order to justify the use of RELU? 
As far as I know, features are never negative or converted to negative in any other type of layer. 
Is it that we can use the RELU with a different ""inflection"" point than zero, so we can make the neuron start describing a lineal response just after this ""new zero""? 
","['convolutional-neural-networks', 'linear-regression', 'relu', 'non-linear-regression']","
The fact that features are always positive values don't guarantee that outputs of hidden layers are positive too.
Due to multiplication, output of an hidden layer could contain negative values, i.e., a hidden layer can contain weights that have opposites signs as its input.
Remember that only layer outputs, not their weights, are passed through ReLu, so, weights of a model could contain negative values.
"
Why aren’t heuristics for Connect Four Monte Carlo tree search improving the agent?,"
I’ve created an agent using MCTS to play Connect Four. It wins against humans pretty well, but I’d like to improve upon it. I decided to add domain knowledge to the MCTS rollout stage. My evaluation function checks how “good” an action is and returns the best/highest value action to the rollout policy as the action to use. 
I created a “gym” application for one agent, who’s not using the evaluation function, to play against an agent who is using the evaluation function. 
I would have expected the agent using the heuristics to perform better than the agent who isn’t, but the inclusion of the heuristics doesn’t seem to make any difference! Any ideas why this might be the case? 
","['reinforcement-learning', 'monte-carlo-tree-search', 'heuristics']","
It might be the case that if you perform a large number of random rollouts, the ""best action"" as chosen by the agent without the domain knowledge, is same as the agent with the domain knowledge. I guess what you can do is try to reduce the number of rollouts and see if the performance changes.
"
DQN not showing the agent is learning in a snake grid environment game,"
I've been trying to train a snake for the snake game in DQN. Which the snake can essentially just move up, down, left and right. I'm having a hard time getting the snake to stay alive longer. So my question is, what are some techniques that I can implement to get the snake to stay alive for longer?
Some of the things that I've attempted but doesn't seem to have done much after about 1000 episodes are:

Implementing the L2 regularization
Reduce the exploration decay rate so it give the snake more chance to explore
Randomize the starting point for the snake for each episode to try to reduce ""local exploration""
I've tweeked some hyper parameters such as learning rate, policy/target network update rate

The input neurons are fed with the state of the board. For example, if my board size is 12*12 then there are 144 input neurons each representing the space of the environment. I've checked that the loss decreases fairly quickly but no improvements on snake lasting longer in the game.
As a side note my reward function is simply a +1 for every time step that the snake survives.
I'm out of ideas of what I can do to get the snake to learn, maybe 1000 episodes is simply not enough? Or maybe my input is not providing good enough information to train the snake?
","['reinforcement-learning', 'dqn']",
Why isn't my implementation of A2C for the the atari pong game converging?,"
I have two different implementations with PyTorch of the Atari Pong game using A2C algorithm. Both implementations are similar, but some portion are different.

https://colab.research.google.com/drive/12YQO4r9v7aFSMqE47Vxl_4ku-c4We3B2?usp=sharing

The above code is from the following Github repository: https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter10/02_pong_a2c.py It converged perfectly well! 
You can find an explanation in Maxim Lapan's book Deep Reinforcement Learning Hands-on page 269
Here is the mean reward curve : 


https://colab.research.google.com/drive/1jkZtk_-kR1Mls9WMbX6l_p1bckph8x1c?usp=sharing

The above implementation has been created by me based on the Maxim Lapan's book. However, the code is not converging. There's a small portion of my code that is wrong, but I can't point out what it is. I've been working on that near a week now.
Here is the mean reward curve : 

Can someone tell me the problem portion of the code and how can I fix it?
UPDATE 1
I have decided to test my code with a simpler environment, i.e. Cartpole-v0.
Here is the code : https://colab.research.google.com/drive/1zL2sy628-J4V1a_NSW2W6MpYinYJSyyZ?usp=sharing
Even that code doesn't seem to converge. Still can't see where is my problem. 
UPDATE 2
I think the bug might be in the ExperienceSource class or in the Agent class. 
UPDATE 3
The following question will help you understand the classes ExperienceSource and ExperienceSourceFirstLast.
","['reinforcement-learning', 'pytorch', 'actor-critic-methods', 'convergence']",
How to evaluate a Deep Q-Network,"
Good day, it's a pleasure having joined this Stack.
In my master thesis I have to expand a Deep Reinforcement Learning Network, to be precise a Deep Q-Network, which is used to control machines in an electrical grid for power quality management. 
What would be the best way to evaluate if a network is doing a good job during training or not? Right now I have access to the reward function as well as the q_value function.
The rewards consist of 4 arrays, one for each learning criteria of the network. The first tuple is a hard criteria (adherence mandatory) while the latter 3 are soft criteria:
Episode: 1/3000 Step: 1/11 Reward: [[1.0, 1.0, -1.0], [0.0, 0.68, 1.0], [0.55, 0.55, 0.55], [1.0, 0.62, 0.79]]
Episode: 1/3000 Step: 2/11 Reward: [[-1.0, 1.0, 1.0], [0.49, 0.46, 0.67], [0.58, 0.58, 0.58], [0.77, 0.84, 0.77]]
Episode: 1/3000 Step: 3/11 Reward: [[-1.0, 1.0, 1.0], [0.76, 0.46, 0.0], [0.67, 0.67, 0.67], [0.77, 0.84, 1.0]]

The q_values are arrays which I do not fully understand yet. Could one of you explain them to me? I read the official definiton of Q-Values positive False Discovery Rate. Can these values be used to evaluate neural network training? These are the Q-Values for step 1:
Q-Values: [[ 0.6934726  -0.24258053 -0.10599071 -0.44178435  0.5393113  -0.60132784
  -0.07680141  0.97968364  0.7707691   0.57855517  0.16273917  0.44632837
   0.00799532 -0.53355324 -0.45182624  0.9229134  -1.0455914  -0.0765233
   0.37784138  0.14711905  0.10986999  0.08918551 -0.8189287   0.14438646
   0.8869624  -0.43251887  0.7742889  -0.7671829   0.07737591  0.2569678
   0.5102049   0.5132051  -0.31643414 -0.0042788  -0.66071266 -0.18251896
   0.7762838   0.15322062 -0.06284399  0.18447408 -0.9609979  -0.4508798
  -0.07925312  0.7503184   0.6858963  -1.0436649  -0.03167241  0.87660617
  -0.43605536 -0.28459656 -0.5564517   1.2478396  -1.1418368  -0.9335588
  -0.72871417  0.04163677  0.30343965 -0.30024529  0.08418611  0.19429305
   0.44063848 -0.5541725   0.5740701   0.76789933 -0.9621064   0.0272104
  -0.44953588  0.13415053 -0.07738207 -0.16188647  0.6667519   0.31965214
   0.3241703  -0.27273563 -0.07130697  0.49683014  0.32996863  0.485767
   0.39242893  0.40508035  0.3413986  -0.5895434  -0.05772913 -0.6172271
  -0.12423459  0.2693861   0.32966745 -0.16036317 -0.36371914 -0.04342368
   0.22878243 -0.09400887 -0.1134861   0.07647536  0.04724833  0.2907955
  -0.70616114  0.71054566  0.35959414 -1.0539075   0.19137645  1.1948669
  -0.21796732 -0.583844   -0.37989947  0.09840107  0.31991178  0.56294084]]

Are there other ways of evaluating DQNetworks? I would also appreciate literature about this subject. Thank you very much for your time.
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'dqn', 'software-evaluation']",
Conversion of strided filter gradient to convolutional form,"
I'm implementing strided 2D convolution. My formula looks like this:
$$y_{i, j} = \sum_{m=0}^{F_h - 1}\sum_{n=0}^{F_w - 1} x_{s\cdot i + m, s\cdot j + n}\,f_{m, n}, \tag{1}$$ where $s$ is the stride
(some sources might refer to this as 'cross-correlation' but 'convolution' is consistent with PyTorch's definition)
I have calculated the gradient with respect to the filter as:
$$\frac{\partial E}{\partial f_{m', n'}} = \sum_{i=0}^{(x_h - F_h) / s}\sum_{j=0}^{(x_w - F_w) / s} x_{s\cdot i + m', s\cdot j + n'} \frac{\partial E}{\partial y_{i, j}} \tag{2}$$
and some simple dummy index relabeling leads to:
$$\frac{\partial E}{\partial f_{i, j}} = \sum_{m=0}^{(x_h - F_h) / s}\sum_{n=0}^{(x_w - F_w) / s} x_{s\cdot m + i, s\cdot n + j} \frac{\partial E}{\partial y_{m, n}} \tag{3}$$ 
Equation $(3)$ looks similar to the first, but not exactly (the $s$ is on the wrong term!). My objective is to convert the second equation into 'convolutional form' so that I can calculate it using my existing, efficient convolution algorithm.
Could someone please help me work this out, or point out any errors that I have made?
",['convolutional-neural-networks'],
How can I make ELIZA more realistic?,"
I’ve coded a simple ELIZA chatbot for a high school coding competition. The chatbot is part of an app that’s designed to help its user cope with depression, anxiety, and similar mental health disorders. It uses sentiment analysis to identify signs of mental illness, and to track it's user's progress toward ""happiness"" over time.
My question is, what steps can I take to make it more realistic (without using some pre-existing software, library, etc, which isn't allowed)? Also, are there any existing tables of questions/responses I can add to my ELIZA bot's repertoire so that it can handle more conversations?
","['natural-language-processing', 'ai-design', 'chat-bots', 'natural-language-understanding']","
One 'easy' way would be to have some sort of conversational memory, where you track what the user has said already. I don't know how complex your patterns are, but if you could recognise names and track references, you could try and build up a mental model of the user's relationships with other people, and perhaps refer to that in your bots responses.
The latter will be quite advanced, but keeping track of things said earlier and referring back to them on occasion might make it appear a lot more capable.
As an added bonus, track changes in the user's sentiment scores, and see if you spot a pattern in the conversation (maybe over the course of multiple conversations) to see which bot utterance have the biggest (positive or negative) effect on the user's mood.
"
Is my understanding of how AI works correct?,"
In my discussion over my question on Math SE, I explained to a user, how I think AI works, I wrote that with the sigmoid(logistic) function, features of a data set are identified, many such iterations provide learning. 
Is my understanding of how this works correct?
","['neural-networks', 'machine-learning', 'definitions']","
There's some useful information in your description, but that's just a very vague description of how neural networks with sigmoid activation functions are trained.
Moreover, there are many other AI systems apart from neural networks (such as support vector machines, expert systems, etc.), which, of course, I cannot exhaustively list here. 

Is my understanding of how AI works correct?

I would say it's not completely incorrect, but, as I said, it's a very vague description and it only refers to a subset of techniques in the AI field. With that description, no newbie would probably understand how neural networks are really trained, apart from knowing that you will train them iteratively and there are sigmoids involved.
"
How many spectrogram frames per input character does text-to-speech (TTS) system Tacotron-2 generate?,"
I've been reading on Tacotron-2, a text-to-speech system, that generates speech just-like humans (indistinguishable from humans) using the GitHub https://github.com/Rayhane-mamah/Tacotron-2.
I'm very confused about a simple aspect of text-to-speech even after reading the paper several times. Tacotron-2 generates spectrogram frames for a given input-text. During training, the dataset is a text sentence and its generated spectrogram (it seems at a rate of 12.5 ms per spectogram frame). 

If the input is provided as a character string, then how many spectogram frames does it predict for each character?
How does training supply which frames form the expected output from the dataset? Because the training dataset is simply a thousand of frames for a sentences, how does it know which frames are ideal output for a given character?

This basic aspect seems just not mentioned clearly anywhere and I'm having a hard time figuring this one out. 
","['recurrent-neural-networks', 'word-embedding', 'attention', 'text-classification', 'speech-synthesis']",
Advantage computed the wrong way?,"
Here is the code written by Maxim Lapan. I am reading his book (Deep Reinforcement Learning Hands-on). I have seen a line in his code which is really weird. In the accumulation of the policy gradient $$\partial \theta_{\pi} \gets \partial \theta_{\pi} + \nabla_{\theta}\log\pi_{\theta} (a_i | s_i) (R - V_{\theta}(s_i))$$ we have to compute the advantage $R - V_{\theta}(s_i)$. In line 138, maxim uses adv_v = vals_ref_v - value_v.detach(). Visually, it looks fine, but look at the shape of each term. 
ipdb> adv_v.shape                                                                                                                            
torch.Size([128, 128])

ipdb> vals_ref_v.shape                                                                                                                       
torch.Size([128])

ipdb> values_v.detach().shape                                                                                                                
torch.Size([128, 1]) 

In a much simpler code, it is equivalent to 
In [1]: import torch                                                            

In [2]: t1 = torch.tensor([1, 2, 3])                                            

In [3]: t2 = torch.tensor([[4], [5], [6]])                                      

In [4]: t1 - t2                                                                 
Out[4]: 
tensor([[-3, -2, -1],
        [-4, -3, -2],
        [-5, -4, -3]])

In [5]: t1 - t2.detach()                                                        
Out[5]: 
tensor([[-3, -2, -1],
        [-4, -3, -2],
        [-5, -4, -3]])

I have trained the agent with his code and it works perfectly fine. I am very confused why it is good practice and what it is doing. Could someone enlighten me on the line adv_v = vals_ref_v - value_v.detach()? For me, the right thing to do was adv_v = vals_ref_v - value_v.squeeze(-1).
Here is the full algorithm used in his book : 
UPDATE 

As you can see by the image, it is converging even though adv_v = vals_ref_v - value_v.detach() looks wrongly implemented. It is not done yet, but I will update the question later.
","['python', 'policy-gradients', 'pytorch', 'actor-critic-methods']","
Yeah, it seems like it's a wrong implementation.
vals_ref_v is a matrix of 1 row, and 128 columns.
value_v.detach() is a matrix of 128 row 
"
How to fill missing values in a dataset where some properties can be inputs and outputs?,"
I have a dataset with missing values, I would like to use machine learning methods to fill.
In more detail, there are $n$
individuals, for which up to 10 properties are provided, all numerical. The fact is, there are no individuals for which all properties are given. The first rows (each row contains data for a given individual) do look as the following 
\begin{bmatrix}
    1 & NA & 3.6 & 12.1 & NA \\
1.2 & NA & NA & 4 & NA \\
    NA & 4 & 5 & NA & 7
  \end{bmatrix}
What methods could be applicable in general? 
I have some basic experience in classifiers and Random Forests. Modulo the obvious difference that this is not a classifying problem, what I struggle most with is that the same variable (described in the e.g 
$n$-th column) is both an input and an output. Say I want to predict the value $A_{2,3}$ in the dataset above. In this case, all the values in the third column could be used as input, excluded of course $A_{2,3}$
itself, which would be an output.
This seems to be different than the more conventional set-up of predicting a property, given a set of other properties (e.g, predict income given education, work sector, seniority, etc.). In this case, sometimes the income is to be predicted, sometimes used for predicting another variable.
I am aware of methods which, given a vector $X_i$, could approximate a function $F$ and predict responses $Y_i$ with
$$ Y_i = F(X_i)$$
In the scenario I described though, it looks like some implicit function $\Phi$ is to be found, a function of all the variables $Z_i$ (columns in the dataset above)
$$ \Phi (Z_i) = 0$$
What methods could handle this aspect? I understand the question is probably too general, but I could not find much and could do with a starting point. I would be already content with some hints for my further reading, but anything more would be gratefully welcomed, thanks.
","['machine-learning', 'datasets', 'data-preprocessing']",
In what RL algorithm category is MiniMax?,"
Q-learning is a temporal-difference method and Monte Carlo tree search is a Monte Carlo method. In what category is MiniMax?
","['reinforcement-learning', 'definitions', 'minimax', 'monte-carlo-methods', 'temporal-difference-methods']","
I think you are looking at it from the wrong direction, min-max is just a planning algorithm, decision strategy, in the sense that you are describing other algorithms/methods it does not have a category. For example, you have negamax algorithm which is in a sense the same thing the Monte Carlo Search Tree is to Monte Carlo. Min-max category is game theory really.
Now you should be thinking about RL algorithms in another way, and this is taxonomy:

So if you think about methods, you mentioned, let's put them in the right place:

TD methods in general - model free
Monte Carlo methods - model free
MinMax - model-based (that could be discussed but it definitely needs access to a world model)

"
"Super-Resolution with Convolutional Neuronal Networks, why interpolation at the beginning?","
I have read several papers about super-resolution with CNNs, where a low-resolution image is reconstructed to a high-resolution image.
What I don't understand is, why it is necessary to interpolate the low-resolution image at the beginning to a size that matches the high-resolution image target.
What is the idea about that? If I have an image to image transformation, what are the benefits in a Neuronal network to have the same input size as the same output size? 
","['convolutional-neural-networks', 'image-processing', 'image-segmentation']",
Why do most deep learning papers not include an implementation?,"
I'm a novice researcher, and as I started to read papers in the area of deep learning I noticed that the implementation is normally not added and is needed to be searched elsewhere, and my question is how come that's the case? The paper's authors needed to implement their models anyway in order to conduct their experimentations, so why not publish the implementation? Plus, if the implementation is not added and there's no reproducibility, what prevents authors from forging results?
","['deep-learning', 'research', 'papers']","

The paper's authors needed to implement their models anyway in order to conduct their experimentations, so why not publish the implementation?

Some papers and authors actually provide a link to their own implementation, but most of the papers (that I have read) don't provide it, although some third-party implementations may already be available on Github (or other code-hosting sites) when you are reading the paper. There may be different reasons why the author(s) of a paper don't provide a reference implementation

They use some closed-source software or maybe it makes use of other resources that cannot be shared
Their implementation is a mess and, so, from a pedagogical point of view, it's quite useless
This may encourage other people to try to reproduce their results with different implementations, so it may indirectly encourage people to do research on the same topic (but maybe not providing a reference implementation could actually have the opposite effect!)


Plus, if the implementation is not added and there's no reproducibility, what prevents authors from forging results?

I had some experience as a researcher, but not enough to answer this question precisely.
Nevertheless, from some reviews of papers I have read (e.g. on OpenReview), in most cases, the reviewers are interested in the consistency of the results, the novelty of the work, the clarity and structure of the paper, etc. I think that, in most cases, they probably trust the provided results, also because, often, for reproducibility, researchers are expected to describe their models and parameters in detail, provide plots, etc., but I don't exclude that there are cases of people that try to fool the reviewers. For example, watch this video where Phil Tabor comments on ridiculous attempts to fool people and plagiarism by Siraj Raval.
"
What is the difference between the prediction and control problems in the context of Reinforcement Learning?,"
What is the difference between the prediction (value estimation) and control problems in reinforcement learning?
Are there scenarios in RL where the problem cannot be distinctly categorised into the aforementioned problems and is a mixture of the problems?
Examples where the problem cannot be easily categorised into one of the aforementioned problems would be nice.
","['reinforcement-learning', 'comparison', 'terminology']","
Nbro's answer already addresses the basic definitions, so I won't repeat that. Instead I'll try to elaborate a bit on the other parts of the question.

Are there scenarios in RL where the problem cannot be distinctly categorised into the aforementioned problems and is a mixture of the problems?

I'm not sure about cases where the ""problem"" can't be distinctly categories... but often, when we're actually interested in control as a problem, we still also actually deal with the prediction problem as a part of our training algorithm. Think of $Q$-learning, Sarsa, and all kinds of other algorithms related to the idea of ""Generalized Policy Iteration"". Many of them work (roughly) like this:

Initialise (somehow, possibly randomly) a value function
Express a policy in terms of that value function (greedy, $\epsilon$-greedy, etc.)
Generate experience using that policy
Train the value function to be more accurate for that policy (prediction problem here)
Go back to step 2 (control problem here)

You could view these techniques in this way, as handling both of the problems at the same time, but there's also something to be said for the argument that they're really mostly just tackling the prediction problem. That's where all the ""interesting"" learning happens. The solution to the control problem is directly derived from the solution to the prediction problem in a single, small step. There are different algorithms, such as Policy Gradient methods, that directly aim to address the control problem instead.

An interesting (in my opinion :)) tangent is that in some problems, one of these problems may be significantly easier than the other, and this can be important to inform your selection of algorithm. For example, suppose you have a very long ""road"" where you can only move to the left or the right, you start on the left, and the goal is all the way to the right. In this problem, a solution to the control problem is trivial to express; just always go right. For the prediction problem, you need something much more powerful to be able to express all the predictions of values in all possible states.
In other problems, it may be much easier to quickly get an estimate of the value, but much more complicated to actually express how to obtain that value. For example, in StarCraft, if you have a much larger army, it is easy to predict that you will win. But you will still need to execute some very specific, long sequences of actions to achieve that goal.
"
Is there an online RL algorithm that receives as input a camera frame and produces an action as output?,"
I want to build a reinforcement learning model, which takes a camera picture as input, that learns online (in terms of machine learning). Based on the position of an object on the camera, I want the model to output an action. That action would be a stepper motor, that either moves to the right or left. This process would be repeated until a given goal/position is reached. 
I can't go to the lab at the moment, so I wrote a virtual environment and let the agent live in that. 
I am trying a neural network with the cross-entropy function. For small environments, this works fine. However, when I increase the size of the environment, the computation becomes really really slow and the model needs a lot of data input until it starts to learn. Also, it only learns offline. But what I would rather want is a model that learns online and only takes a few tries until it understands the underlying pattern. That isn't a problem for the virtual environment, since I can easily get thousands of data samples. But in the real environment, it would take ages this way. 

Is there an online reinforcement algorithm that could help me out (instead of training the neural network with the cross-entropy loss function)? 

","['machine-learning', 'reinforcement-learning', 'ai-design', 'reference-request', 'online-learning']",
Why is ANFIS important in general?,"
I am actually working with the iris dataset from sklearn and try to understand the ANFIS-Package for python. But that does not really matter! I have a more general question.  
During thinking about adaptive neuro-fuzzy inference system (ANFIS), a general question came into my mind. I don't really understand: in general, why is ANFIS necessary? 
So, for example, if I want to predict classes for this iris dataset, I also can use a supervised learning method or a neural network and I get the result. 
In ANFIS, I do nothing other than splitting the input attributes into linguistic terms and give membership functions to it. At the end of the day, I will receive ""predictions"" for the input values, which are classes. 
But - with the ANFIS-Package in Python - I cannot see, if my membership function has changed during the learning time or what rules the network constructed. So, I cannot really see why this is useful. Maybe it is just because I am usually using the iris dataset for supervised learning.
","['neural-networks', 'machine-learning', 'datasets']",
Why is the reward function $\text{reward} = 1/{(\text{cost}+1)^2}$ better than $\text{reward} =1/(\text{cost}+1)$?,"
I have implemented a simple Q-learning algorithm to minimize a cost function by setting the reward to the inverse of the cost of the action taken by the agent. The algorithm converges nicely, but there is some difference I get in the global cost convergence for different orders of the reward function. If I use the reward function as:
$$\text{reward} = \frac{1}{(\text{cost}+1)^2}$$
the algorithm converges better (lower global cost, which is the objective of the process) than when I use the reward as:
$$\text{reward} = \frac{1}{(\text{cost}+1)}$$
What could be the explanation for this difference? Is it the issue of optimism in the face of uncertainty?
","['reinforcement-learning', 'q-learning', 'rewards', 'reward-design', 'reward-functions']",
What is the difference between fuzzy neural networks and adaptive neuro fuzzy inference systems?,"
I have, like you see, just a general question about the combination of fuzziness and neural networks. I understood it as follows 

Fuzzy neural networks as a hybrid system: the neural network helps me to find the optimal parameters related to the fuzzy system, for example, the rules or the membership function 
Adaptive neural fuzzy inference systems (ANFIS): the NN helps me to find the optimal parameters related to the fuzzy inference system. What are some examples here?

I cannot intuitively grasp the difference between these two.
","['neural-networks', 'machine-learning', 'comparison', 'fuzzy-logic']",
Benchmarking SAC on Pybullet,"
So far I have seen TD3 and DDPG benchmarks on Pybullet environments, but I am looking for SAC benchmarks on Pybullet too, anyone can help?
","['reinforcement-learning', 'pytorch', 'ddpg', 'benchmarks']",
Are my steps correct for a proper classification of a sick brain?,"
I have a dataset with MRI of patients with a specific disease that affects the brain and another dataset with MRI of healthy patients.
I want to create a classifier (using neural networks) to classify if the MRI of a new patient shows the presence of the illness or not.
First of all, I extracted the brain from all the MRIs (the so-called skull stripping) using BET tool found in FSL.
I have three questions for you

As the input to the training phase, I want to give the whole extracted brains (possibly in the nii format). What kind of preprocessing steps do I need to apply once I've extracted the brains (before passing it to the classifier)?
Do you know any better tool for skull stripping?
Do you know a tool (or library) that takes as input a nii files and allows me to create a classifier that uses neural networks?

","['neural-networks', 'machine-learning', 'data-preprocessing', 'brain']","
It looks like everything you want is available with the Deep Learning Toolkit (DLTK) for Medical Imaging
There is also a blog: An Introduction to Biomedical Image Analysis with TensorFlow and DLTK
There is a DataCamp course that walks you through most of the process but instead of a classifier they use deep learning to reconstruct brain images.  They provide a link to their MNIST classifier example which should be easy to adapt for your purpose.  See: Reconstructing Brain MRI Images Using Deep Learning (Convolutional Autoencoder)
ResearchGate has a thread that may help:
What is the appropriate way to use Nifti files in deep learning?
"
What is the difference between artificial intelligence and swarm intelligence?,"
Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.
According to the Wikipedia article on swarm intelligence

Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence.
The application of swarm principles to robots is called swarm robotics, while 'swarm intelligence' refers to the more general set of algorithms.
SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems.

These two terms seem to be related, especially in their application in computer science and software engineering. Is one a subset of another? Is one tool (SI) is used to build a system for the other(AI)? What are their differences and why are they significant?
","['comparison', 'terminology', 'swarm-intelligence']","
Well, one of the simpler definitions for SI sounds like this:

The emergent collective intelligence of groups of simple agents.”
  (Bonabeau et al, 1999)

So, in order to get to the SI you have to use some kind of algorithms/AI to get simple intelligent agents. It's just cooperative intelligence, or cooperative AI if you wish. SI just uses today's AI/ML techniques to build the swarm, in same manners as reinforcement learning uses AI/ML techniques to make agents that can behave reasonably in large spaces by approximating value functions V(S) and policies pi(S). I hope this helps a little.
So AI/ML is kinda of a tool plugged in SI, as SI is field with it's own algorithm definitions and theory.
"
Is it possible to prove that the target policy is better than the behavioural policy based on learned Q values?,"
I have retrospective data for a sort of ""behaviour policy"" which I will use to train a deep q network to learn a target greedy policy. After learning the Q values for this target policy, can we make the conclusion that because the Q value for the target policy, $Q(s,\pi_e(s))$ is higher than the Q values for the behaviour policy, $Q(s,\pi_b(s))$ at all states encountered, where $\pi_e$ is the policy output by deep Q-learning and $\pi_b$ is the behaviour policy, then this target policy has better performance than the behaviour policy?
I know the proper way is to run the policy and do an empirical comparison of some sort. However, that is not possible in my case. 
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'off-policy-methods']","
No, mainly because these are all stochastic approximations and may not represent the true values.
Almost nothing good can be said about NN approximations to value and Q functions(at least according to a professor I have had). 
"
Does L1/L2 Regularization help reach an optimum result faster?,"
I understand that L1 and L2 regularization helps to prevent overfitting. My question is then, does that mean they also help a neural network learn faster as a result?
The way I'm thinking is that since the regularization techniques reduce weights (to 0 or close to 0 depending on whether it's L1 or L2) that are not important to the neural network, this would, in turn, result in ""better values"" for the output neurons right? Or perhaps I am completely wrong.
For example, suppose I have a neural network that is to train a snake to move around a NxN environment. With regularization, the snake will learn faster in terms of survive longer in the game?
","['neural-networks', 'deep-learning', 'regularization', 'l2-regularization', 'l1-regularization']","
I am not aware of any empirical results regarding this question.
But in theory, adding a regularization term shall make the learning task actually even harder, since there is suddenly a second loss term that the network has to be optimized for, which is not even directly related to achieving the original task of fitting the model to the data.
It is true that the regularization term will try drive as many of the weights towards low values as it can. But, at the same time, the other loss term (computed on the original optimization criterion) will try to drive many of the weights to larger values unequal 0 in order to achieve the original training task. (If that wasn't the case, you would be good to go without regularization in the first place.)
Having these competing interests shall then make achieving the original task harder, which I would in theory expect to be more time consuming than, alternatively, allowing the model overfit until a certain admissible error is reached.
After all, the goal of regularization is to improve generalization of a learned model (i.e. to prevent overfitting, as you said), which is about the quality of the outcome and not about how quickly you get to the desired outcome.
"
Why do we use a weighted average of child entropies when we calculate information gain?,"
In the decision tree algorithm, why do we use a weighted average of child entropies when we calculate information gain? What is wrong about using the arithmetic mean of entropies?
",['decision-trees'],
Why do hypercube latent spaces perform poorer than Gaussian latent spaces in generative neural networks?,"
I have a quick question regarding the use of different latent spaces to represent a  distribution. Why is it that a Gaussian is usually used to represent the latent space of the generative model rather than say a hypercube? Is it because a Gaussian has most of its distribution centred around the origin rather than a uniform distribution which uniformly places points in a bounded region? 
I've tried modelling different distributions using a generative model with both a Gaussian and Uniform distribution in the latent space and the Uniform is always slightly restrictive when compared with a Gaussian. Is there a mathematical reason behind this?
Thanks in advance!  
","['generative-model', 'latent-variable']",
Would you categorize policy iteration as an actor-critic reinforcement learning approach?,"
One way of understanding the difference between value function approaches, policy approaches and actor-critic approaches in reinforcement learning is the following:

A critic explicitly models a value function for a policy.
An actor explicitly models a policy.

Value function approaches, such as Q-learning, only keep track of a value function, and the policy is directly derived from that (e.g. greedily or epsilon-greedily). Therefore, these approaches can be classified as a ""critic-only"" approach. 
Some policy search/gradient approaches, such as REINFORCE, only use a policy representation, therefore, I would argue that this approach can be classified as an ""actor-only"" approach.
Of course, many policy search/gradient approaches also use value models in addition to a policy model. These algorithms are commonly referred to as ""actor-critic"" approaches (well-known ones are A2C / A3C).
Keeping this taxonomy intact for model-based dynamic programming algorithms, I would argue that value iteration is an actor-only approach, and policy iteration is an actor-critic approach. However, not many people discuss the term actor-critic when referring to policy iteration. How come?
Also, I am not familiar with any model-based/dynamic programming like actor only approaches? Do these exist? If not, what prevents this from happening?
","['reinforcement-learning', 'definitions', 'actor-critic-methods', 'value-iteration', 'policy-iteration']","

Keeping this taxonomy intact for model-based Dynamic programming algorithms, I would argue that value iteration is a Actor only approach, and policy iteration is a Actor-Critic approach. However, not many people discuss the term Actor-Critic when referring to Policy Iteration. How come?

Both policy iteration and value iteration are value-based approaches. The policy in policy iteration is either arbitrary or derived from a value table. It is not modelled separately.
To count as an Actor, the policy function needs to modelled directly as a parametric function of the state, not indirectly via a value assessment. You cannot use policy gradient methods to adjust an Actor's policy function unless it is possible to derive the gradient of the policy function with respect to parameters that control the relationship bewteen state and action. An Actor policy might be noted as $\pi(a|s,\theta)$ and the parameters $\theta$ are what make it possible to learn improvements.
Policy iteration often generates an explicit policy, from the current value estimates. This is not a representation that can be directly manipulated, instead it is a consequence of measuring values, and there are no parameters that can be learned. Therefore the policy seen in policy iteration cannot be used as an actor in Actor-Critic or related methods.
Another way to state this is that the policy and value functions in DP are not separate enough to be considered as an actor/critic pair. Instead they are both views of the same measurement, with the value function being closer to raw measurements and policy being a mapping of the value function to policy space.

Also, I am not familiar with any model-based/dynamic programming like actor only approaches? Do these exist? If not, what prevents this from happening?

The main difference between model-based dynamic programming and model-free methods like Q-learning, or SARSA, is that the dynamic programming methods directly use the full distribution model (which can be expressed as $p(r, s'|s,a)$) to calculate expected bootstrapped returns.
There is nothing in principle stopping you substituting expected returns calculated in this way into REINFORCE or Actor-Critic methods. However, it may be computationally hard to do so - these methods are often chosen when action space is large for instance. 
Basic REINFORCE using model-based expectations would be especially hard as you need an expected value calculated over all possible trajectories from each starting state - if you are going to expand the tree of all possible results to that degree, then a simple tree search algorithm would perform better, and the algorithm then resolves to a one-off planning exhaustive tree search.
Actor-Critic using dynamic programming methods for the Critic should be viable, and I expect you could find examples of it being done in some situations. It may work well for some card or board games, if the combined action space and state space is not too large - it would behave a little like using Expected SARSA for the Critic component, except also run expectations over the state transition dynamics (whilst Expected SARSA only runs expectations over policy). You could vary the depth of this too, getting better estimates theoretically at the expense of extra computation (potentially a lot of extra computation if there is a large branching factor)
"
Is the test time the phase when the model's accuracy is calculated with test data set?,"
When papers talk about the ""test time"", does this mean the phase when the model is passed with new data instances to derive the accuracy of the test data set? Or is ""test time"" the phase when the model is fully trained and launched for real-world input data?
","['machine-learning', 'training', 'definitions', 'testing']",
What should the dimension of the input be for text summarization?,"
I am trying to build a model for extractive text summarization using keras sequential layers. I am having a hard time trying to understand how to input my x data. Should it be an array of documents with each document containing an array of sentences? or should I further break it down to each sentence containing an array of words?
The y input is basically a binary classification of each sentence to check whether or not they belong to the summary of the document.
The first layer is an embedding layer and I'm using 100d Glove word embedding.
P.s: I am new to machine learning.
","['neural-networks', 'machine-learning', 'word-embedding', 'text-summarization']","
Briefly:
I think what you are looking for is an RNN network (Either LSTM or GRU) with the many-to-many topology.
Explanation:
Clearly your input is the sentences (or to be more precise, the an embedding of your sentences, because you cannot feed the raw text to the network). then for each sentence you want to assign a value, which means for n inputs, you need n outputs. This is the many-to-many architecture.
Moreover, you might want to check the Bi-directional LSTM for your study. Not relevant to your question, but worth mentioning.
For more information, refer to this
"
Does Gödel's second incompleteness theorem put a limitation on artificial intelligence systems?,"
According to Brian Cantwell Smith

no calculation without representation

Therefore, computers depend on models. So, we can say that AI is limited internally by the model and externally by the environment. This problem is discussed here in a previous question I have asked.
Now, consider Gödel's second incompleteness theorem

a coherent theory does not demonstrate its own coherence

Can we say that Gödel's second incompleteness theorem puts a limitation on artificial intelligence? How could AI bypass Gödel's second incompleteness theorem?
","['philosophy', 'math', 'agi', 'incompleteness-theorems']","
I think the colloquial understanding of Gödel's incompleteness theorems allows them to be too broadly applied. Gödel's second incompleteness regards the consistency of a formal system, which is a technical concept of formal systems that means the system cannot prove every formula. It is commonly framed as a system not being able to prove both a formula and its negation (e.g. $2+2=4$ and $2+2 \neq 4$), since many logical systems allow anything to be proven from a contradiction.
The second incompleteness theorem states that if a consistent formal system is expressive enough to encode basic arithmetic (Peano arithmetic), then that system cannot prove its own consistency. This implies that we must use a stronger system B to prove the consistency of A. The system needs to be able to represent arithmetic because that is what is used to define the representability conditions of Gödel's proof that allowed him to formally construct the self-referential formulae central to the incompleteness theorems.
Here I diverge with my own opinion on this, I feel the concept of consistency in formal systems has no obvious bearing on the limits of artificial intelligence. An intelligent agent need not know anything about formal consistency to reach its level of intelligence -- the vast majority of humans have never encountered this concept, and yet they are still intelligent. Even many mathematicians don't give it a second thought unless they are in the trenches of mathematical logic. One would have to take an overly narrow view of artificial intelligence to allow Gödel's second incompleteness to serve as a limitation to it.
I caution against the popular informal restatements of Gödel's incompleteness theorems. These theorems were undoubtedly earth-shattering in the study of foundational mathematics and still have grand implications today, but projecting those results too far away from their rigorous origins is going to lead to many stray conclusions.
"
How is the Jacobian a generalisation of the gradient?,"
I came across these slides Natural Language Processing with Deep Learning CS224N/Ling284, in the context of natural language processing, which talk about the Jacobian as a generalization of the gradient.
I know there is a lot of topic regarding this on the internet, and trust me, I've googled it. But things are getting more and more confused for me.
In simple words, how is the Jacobian a generalization of the gradient? How can it be used in gradient descent?
","['deep-learning', 'natural-language-processing', 'math', 'gradient-descent']",
How to calculate the data noise variance for a prediction interval?,"
I have a neural network that connects $N$ input variables to $M$ output variables (qoi). By default, neural networks just give out point estimations. 
Now, I want to plot some of the quantity of interests and produce also a prediction interval. To calculate the model uncertainty, I use the bootstrap method.
$$\sigma_{model}^2=\frac{1}{B-1}\sum_{i=1}^B(\hat{y}_i^b-\hat{y}_i)^2\qquad \text{with}\quad\hat{y}_i = \frac{1}{B}\sum_{b=1}^B\hat{y}_i^b$$
$B$ training datasets are resampled from original dataset with replacement.  $\hat{y}_i^b$ is the preditcion of the $i$ sample generated by the $b$th bootstrap model.
If I understood it correctly, the model uncertainty (or epistemic uncertainty) is enough to create a confidence interval.
But for the PI I also need the irreducible error $\sigma_{noise,\epsilon}^2$. $$\sigma_y^2= \sigma_{model}^2+\sigma_{noise,\epsilon}^2$$
The aleatoric uncertainty is explained in the following picture:

Is there a procedure to calculate this aleatoric uncertainty? 
I read the paper High-Quality Prediction Intervals for Deep Learning and watched the corresponding  YouTube video. And I read the paper Neural Network-Based Prediction Intervals.
EDIT
I suggest the following algorithm to estimate the noise variance, but I am  not sure if this makes sense: 

","['neural-networks', 'machine-learning', 'statistical-ai']",
Calculating accuracy for cross validation,"
I'm struggling with calculating accuracy
when I do cross-validation for a deep learning model.
I have two candidates for doing this.
1. Train a model with 10 different folds and get the best accuracy of them(so I get 10 best accuracies) and average them.
2. Train a model with 10 different folds and get 10 accuracy learning curves. Now, average these learning curves by calculating the mean of 10 accuracies of each epoch. So now we get one averaged accuracy learning curve and find the highest accuracy from this curve.
Among these two candidates which one is correct??
","['deep-learning', 'cross-validation']","
I guess you could train your model with 10 different folds and in each fold calculate the average accuracy. So you would have 10 values - one corresponding to each fold. And then take the mean of all of them to get the average accuracy of your model.
Your first option doesn't seem great because you take the highest accuracy among folds. If for some reason, the variance between accuracies is high for a fold, this would bias your numbers. Taking mean or maybe median of accuracies might be more reasonable.
Does that help?
"
How does being on-policy prevent us from using the replay buffer with the policy gradients?,"

One of the approaches to improving the stability of the Policy
Gradient family of methods is to use multiple environments in
parallel. The reason behind this is the fundamental problem we
discussed in Chapter 6, Deep Q-Network, when we talked about the
correlation between samples, which breaks the independent and
identically distributed (i.i.d) assumption, which is critical for
Stochastic Gradient Descent (SDG) optimization. The negative
consequence of such correlation is very high variance in gradients,
which means that our training batch contains very similar examples,
all of them pushing our network in the same direction. However, this
may be totally the wrong direction in the global sense, as all those
examples could be from one single lucky or unlucky episode. With our
Deep Q-Network (DQN), we solved the issue by storing a large amount of
previous states in the replay buffer and sampling our training batch
from this buffer. If the buffer is large enough, the random sample
from it is much better representation of the states distribution at
large. Unfortunately, this solution won't work for PG methods, at most
of them are on-policy, which means that we have to train on samples
generated by our current policy, so, remembering old transitions is
not possible anymore.

The above excerpt is from Maxim Lapan in the book Deep Reinforcement Learning Hands-on page 284.
How does being on-policy prevent us from using the replay buffer with the policy gradients? Can you explain to me mathematically why we can't use replay buffer with A3C for instance?
","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods', 'experience-replay', 'a3c']",
"Importance sampling eq. 5 in paper ""Residual Energy-based Models for Text Generation""","
In the paper ""Residual Energy-Based Models for Text Generation"" (arXiv), on page 5, they write that equation 5 is an instance of importance sampling.
Equation 5 is:
$$ P(x_t \mid x_{<t}) = P_{LM}(x_t \mid x_{<t}) \, \frac{\mathbb{E}_{x'_{>t} \sim P_{LM}(\cdot \mid x_{\leq t})}[\exp(-E_\theta (x_{<t}, \, x_t, \, x'_{>t}))]}{\mathbb{E}_{x'_{\geq t} \sim P_{LM}(\cdot \mid x_{\leq t-1})}[\exp(-E_\theta (x_{<t}, \, x'_t, \, x'_{>t}))]} \ \ .$$
The goal is to approximate sampling from a distribution from which sampling is intractable $P_\theta(Y \mid X) = P_{LM}(Y \mid X) \, \frac{\exp(-E_\theta (X, Y))}{Z_\theta(X)}$, by sampling from $P_{LM}$, from which sampling is cheaper.
I understand that they are marginalizing over $>t$ in eq. 5, and I understand the basic idea of importance sampling to change $\mathbb{E}_{x \sim p}[f(x)]$ into $\mathbb{E}_{x \sim q}[f(x) \frac{p(x)}{q(x)}]$. However, eq. 5 is not a mean or aggregate, it is a probability.
What is happening? I don't see how eq. 5 fits in the importance sampling scheme (or a self-normalizing importance sampling scheme, link). Thanks in advance!
","['probability', 'text-generation', 'importance-sampling']",
Why a single trajectory can be used to update the policy network $\theta$ in A3C?,"
The Deep RL bootcamp on policy gradient techniques gives the update equation for the policy network in A3C as
$\theta_{i+1} = \theta_i + \alpha \times 1/m \sum_{k=1}^m\sum_{t=0}^{H-1}\nabla_{\theta}log\pi_{\theta_i}(u_t^{(k)} | s_t^{(k)})(Q(s_t^{(k)},u_t^{(k)}) - V_{\Phi_i}^\pi(s_t^{(k)}))    $
However in the actual A3C paper, the gradient update is based on a single trajectory and there is no averaging of the gradient over $m$ trajectories as defined in the video ? The simple action-value actor-critic algorithm also does not seem to require an averaging over m trajectory. 
","['policy-gradients', 'actor-critic-methods']","
I guess the gradient of the expectation of the Utility function, $\nabla_{\theta}J(\theta)$ in policy gradient methods where $\nabla_{\theta}J(\theta) = E_{\tau \sim p(\tau ; \theta)}[r(\tau)\nabla_{\theta}log p(\tau;\theta)]$ can be approximated using a single sample trajectory as shown in a deep reinforcement learning lecture by stanford where $J(\theta) \approx \sum_{t > 0}r(\tau)\nabla_{\theta}log\pi_{\theta}(a_t |s_t)$ and an average over sampled trajectories is not needed to compute the gradient for $\theta$ update in the direction of gradient.
"
How do reinforcement learning and collaborative learning overlap?,"
How do reinforcement learning and collaborative learning overlap? What are the differences and similarities between these fields?
I feel like the results I get via google do not make the distinction clear.
","['reinforcement-learning', 'comparison', 'collaborative-filtering']",
What pre-processing of the image is needed before feeding it into the convolutional neural network?,"
I can't figure out what preprocessing of the image is needed before feeding it into the convolutional neural network. For example, I want to recognize circles on a 1000 by 1000 px photo. The learning process of a neural network occurs on 100 by 100 px (https://www.kaggle.com/smeschke/four-shapes/data). I'm having a little difficulty wrapping my head around the situation when the circle in the input image is much larger (or smaller) than 100x100 px. How then the convolution neural network determines that circle if it was learned on a dataset of a different picture's size. 
For clarity, I want to submit a 454 by 430 px image to the network input:

Example of the dataset for the learning process (100 by 100 px):

Finally, I want to recognize all the circles on the input image:

","['neural-networks', 'convolutional-neural-networks', 'python', 'image-recognition', 'data-preprocessing']",
How does the gradient increase the probabilities of the path with a positive reward in policy gradient?,"
Pieter Abbeel in his deep rl bootcamp policy gradient lecture derived the gradient of the utility function with respect to $\theta$ as $\nabla U(\theta) \approx \hat{g} = 1/m\sum_{i=1}^m  \nabla_\theta logP(\tau^{(i)}; \theta)R(\tau^{(i)})$, where $m$ is the number of rollouts, and $\tau$ represents the trajectory of $s_0,u_0, ..., s_H, u_H$ state action sequences.
He also explains that the gradient increases the log probabilities of trajectories that have positive reward and decreases the log probabilities of trajectories with negative reward, as seen in the picture. From the equation, however, I don't see how the gradient tries to increase the probabilities of the path with positive R?
From the equation, what I understand is that we would want to update $\theta$ in a way that moves in the direction of $\nabla U(\theta)$ so that the overall utility is maximised, and this entails computing the gradient log probability of a trajectory. 
Also, why is $\theta$ omitted in $R(\tau^{(i)})$, since $\tau$ depends on the policy which is dependent on $\theta$ ?

","['reinforcement-learning', 'policy-gradients']","
The grad log probability of the trajectory parameterised by $\theta$ tells us the direction $\theta$ should move to increase the probability of that trajectory $P(\tau;\theta)$ the most. 
If the reward is positive, $\nabla U(\theta)$ tells us how much we want to increase/decrease the probability of that path $\tau$. The scalar quantity $R(\tau)$ determines the magnitude and direction of shift. If $R(\tau)$ is positive, and $\theta$ is updated based on equation $\theta_{new}$ = $\theta_{old} + \alpha\nabla_{\theta}U(\theta)$, then $\theta$ will move in the direction of it's steepest increase, leading to an increase in probability of $\tau$. If $R(\tau)$ is negative, then $\theta$ moves in the direction of steepest decrease, leading to a decrease in probability of $\tau$. 
"
What are non-held-out data or non-held-out classes?,"
I'm Spanish and I don't understand the meaning of ""non-held-out"". I have tried Google Translator and online dictionaries like Longman but I can't find a suitable translation for this term.
You can find these term using this Google Search, and in articles like this one:

""computing SVD on the non-held-out data"" from here.
""The training set consists all the images and annotations containing non-held-out classes while held-out classes are masked as background during the training"" from Few-Shot Semantic Segmentation with Prototype Learning.
""A cross-validation procedure is that non held out data (meaning after holding out the test set) is splitted in k folds/sets"" from here.

What is non-held-out data and held-out data or classes?
","['machine-learning', 'deep-learning', 'terminology', 'cross-validation']",
How's the action represented in MuZero for Atari?,"
MuZero seems to use two different methods to encode actions into planes for Atari games:

For the input action to the representation function, MuZero encodes historical actions as simple bias planes, scaled as $a/18$, where $18$ is the total number of valid actions in Atari.(from the appendix E of the paper)
For the input action to the dynamics function, Muzero encode an action as a one-hot vector, which is tiled appropriately into planes(from the appendix F of the paper)

I'm not so sure about how to make of the term ""bias plane"".
About the second, my understanding is that, as an example, for action $4$, we first apply one-hot encoding, which gives us a zero vector of length $18$ with one in the $5$-th position(as there are $18$ actions). Then we tile it and get a zero vector of length $36$, with ones in the $5$-th and $23$-rd positions. At last, this vector is reshaped into a $6\times 6$ plane as follows:
$$
0, 0, 0, 0, 1, 0\\ 0, 0, 0, 0, 0, 0\\ 0, 0, 0, 0, 0, 0\\
0, 0, 0, 0, 1, 0\\ 0, 0, 0, 0, 0, 0\\ 0, 0, 0, 0, 0, 0
$$
","['deep-learning', 'reinforcement-learning']",
Can we use a Gaussian process to approximate the belief distribution at every instant in a POMDP?,"
Suppose $x_{t+1} \sim \mathbb{P}(\cdot | x_t, a_t)$ denotes the state transition dynamics in a reinforcement learning (RL) problem. Let $y_{t+1} = \mathbb{P}(\cdot | x_{t+1})$ denote the noisy observation or the imperfect state information. Let $H_{t}$ denote the history of actions and observations $H_{t+1} = \{b_0,y_0,a_0,\cdots,y_{t+1}\}$.
For the RL Partially Observed Markov Decision Process (RL-POMDP), the summary of the history is contained in the ""belief state""  $b_{t+1}(i) = \mathbb{P}(x_{t+1} = i | H_{t+1})$, which is the posterior distribution over the states conditioned on the history.
Now, suppose the model is NOT known. Clearly, the belief state can't be computed.
Can we use a Gaussian Process (GP) to approximate the belief distribution $b_{t}$ at every instant $t$?
Can Variational GP be adapted to such a situation? Can universal approximation property of GP be invoked here?
Are there such results in the literature? 
Any references and insights into this problem would be much appreciated.
","['reinforcement-learning', 'markov-decision-process', 'pomdp', 'bayesian-optimization', 'gaussian-process']",
what will be the best loss function for unet to predict the each pixel values?,"

I'm predicting the used 9 pictures to predict the  last picture 
so (40,40,9) -> unet -> (40,40,1) 
but as you see the predict picture 

It's not just a mask(0or 1) its float 
so which loss function should I define to achieve the best Unet result? and why?
","['objective-functions', 'image-segmentation', 'loss']",
Can I do state space quantization using a KMeans-like algorithm instead of range buckets?,"
Are there any reference papers where it is used a KMeans-like algorithm in state space quantization in Reinforcement Learning instead of range buckets?
","['reinforcement-learning', 'reference-request', 'k-means', 'dimensionality']",
Is it required that taking an action updates the state?,"
For some environments taking an action may not update the environment state. For example, a trading RL agent may take an action to buy shares s. The state at time t which is the time of investing is represented as the interval of 5 previous prices of s. At t+1 the share price has changed but it may not be as a result of the action taken. Does this affect RL learning, if so how ? Is it required that state is updated as a result of taking actions for agent learning to occur ?
In gaming environments it is clear how actions affect the environment. Can some rules of RL breakdown if no ""noticeable"" environment change takes place as a result of actions ?
Update:
""actions influence the state transitions"", is my understanding correct:
If transitioning to a new state is governed by epsilon greedy and epsilon is set to .1 then with .1 probability the agent will choose an action from the q table which has max reward reward for the given state. Otherwise the agent randomly chooses and performs an action then updates the q table with discounted reward received from the environment for the given action.
I've not explicitly modeled an MDP and just defined the environment and let the agent determine best actions over multiple episodes of choosing either a random action or the best action for the given state, the selection is governed by epsilon greedy.
But perhaps I've not understood something fundamental in RL. I'm ignoring MDP in large part as I'm not modeling the environment explicitly. I don't set the probabilities of moving from each state to other states.
",['reinforcement-learning'],"
It seems to me that you are confusing two things, State of the agent and State of the environment.
Think about a robot learning to walk on a rugged terrain.
The actions of the robots don't change the terrain at all ! The robot can include in his own state, part of the environment state, the topology of the terrain in front of him for instance.
In an MDP,  an agent may or may not modify the environment state, in your case, if we make the assumption that you don't manipulate the market, you can consider that you don't modify the environment state.
Your issue might be in the way you designed your agent, relying only on a part of the state environment, and not really having any state of his own, such as a portfolio for instance.
If you're only interested in the way the state environment changes, then the above answer is right, the RL framework may not be a good fit.
Finally, you don't have to model explicitly the environment, that's the beauty of model free RL ! Q-learning, that you mentioned, is model-free.
"
What work has been done with Poisson-style regression via neural networks with exponential activation function?,"
The first neural net I wrote was a classifier. After that, I learned that neural nets can be used for regression tasks, even quantile regression.
It has become clear to me that the usual games with extensions of OLS linear regression can be applied to neural networks.
What work has been done with Poisson-style regression via neural networks with log link functions (exponential activation function)?
","['neural-networks', 'activation-functions', 'regression']",
Do smaller loss values during DQN training produce better policies?,"
During the training of DQN, I noticed that the model with prioritized experience replay (PER) had a smaller loss in general compared to a DQN without PER. The mean squared loss was an order of magnitude $10^{-5}$ for the DQN with PER, whereas the mean squared loss was an order of magnitude $10^{-2}$. 
Do the smaller training errors have any effect on executing the final policy learned by the DQN?
","['reinforcement-learning', 'dqn', 'loss']","
I think it says something about the training progress, while another approach you can make sure is to look at the gradient norm. Sometimes, the training loss is really noisy while the gradient norm is much more clear.
"
What are the procedures to get RL paper results? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 3 years ago.







                        Improve this question
                    



I finished working on a new algorithm in Reinforcement Learning, I need to compare it to some well-known algorithms. That's why I need to know the step-by-step procedures that RL researchers usually take in order to get their results and compare them to other papers' results. (e.g. running the algorithm multiple times with different random seeds, saving results to .csv files, plotting).
Anyone can help? 
(I am working on Pytorch, PyBullet Environments.)
","['reinforcement-learning', 'research', 'pytorch']",
"Once the environments are vectorized, how do I have to gather immediate experiences for the agent?","
My main purpose right now is to train an agent using the A2C algorithm to solve the Atari Breakout game. So far I have succeeded to create that code with a single agent and environment. To break the correlation between samples (i.i.d), I need to have an agent interacting with several environments. 
class GymEnvVec():

    def __init__(self, env_name, n_envs, seed=0):
        make_env = lambda: gym.make(env_name)
        self.envs = [make_env() for _ in range(n_envs)]
        [env.seed(seed + 10 * i) for i, env in enumerate(self.envs)]

    def reset(self):
        return [env.reset() for env in self.envs]

    def step(self, actions):
        return list(zip(*[env.step(a) for env, a in zip(self.envs, actions)]))

I can use the class GymEnvVec to vectorize my environment. 
So I can set my environments with 
envs = GymEnvVec(env_name=""Breakout-v0"", n_envs=50)

I can get my first observations with
observations = envs.reset()

Pick some actions with 
actions = agent.choose_actions(observations)

The choose_actions method might look like 
def choose_actions(self, states):
        assert isinstance(states, (list, tuple))

        actions = []
        for state in states:
            probabilities  = F.softmax(self.network(state)[0])
            action_probs = T.distributions.Categorical(probabilities)
            actions.append(action_probs.sample())

        return [action.item() for action in actions] 

Finally, the environments will spit the next_states, rewards and if it is done with
next_states, rewards, dones, _ = env.step(actions)

It is at this point I am a bit confused. I think I need to gather immediate experiences, batch altogether and forward it to the agent. My problem is probably with the ""gather immediate experiences"". 
I propose a solution, but I am far from being sure it is a good answer. At each iteration, I think I must take a random number with 
nb = random.randint(0, len(n_envs)-1)

and put the experience in history with 
history.append(Experience(state=states[nb], actions[nb], rewards[nb], dones[nb]))

Am I wrong? Can you tell me what I should do?
","['reinforcement-learning', 'python', 'pytorch', 'actor-critic-methods', 'environment']",
Merge two different CNN models into one,"
I have 2 different models with each model doing a separate function and have been trained with different weights. Is there any way I can merge these two models to get a single model.
If it can be merged

How should I go about it? Will the number of layers remain the same?
Will it give me any performance gain?(Intuitively speaking, I should get a higher performance)
Will the hardware requirements change when using the new model?
Will I need to retrain the model? Can I somehow merge the trained weights?

If the models cannot be merged

Why so? After all, convolution is finding the correct pattern in data. 
Also, if CNN's cannot be merged, then how do skip-connections like ResNet50 work?

EDIT: 
Representation: 
What I currently have
Image ---(model A) ---> Temporary image ---(Model B)---> Output image
What I want:
Image ---(model C) ---> Output image
","['neural-networks', 'convolutional-neural-networks', 'computer-vision', 'architecture']",
How does the math behind heat map filters work?,"
I am working on an app that generates heat/ thermal map given a picture. i have been able to get what i expected using python opencv builtin function cv2.applyColorMap(img, cv2.COLORMAP_JET). Everything works exactly as expected. But i want to understand how applyColorMap works at the back end. I am aware how several image filters (like image, edge filters) work by convolution / cross correlation with appropriate kernals, but i can't seem to pull the same concept for color maps. 
For this question lets consider a color map where we want :
Brightest ones: (RED COLORED)
MEDIUM INTENSITY ONES: (YELLOW COLORED)
LOW INTENSITY ONES: (BLUE COLORED)
What i have done:
I tried dividing the pixels into 3 categories and replaced each pixel with the either of the colors (RED, YELLOW, BLUE ) depending upon it's value from gray scale image( 0-255). 
This approach had a problem that there were solid 3 colors in the image with no variation in intensity of the individual color while in a good heat map there is blend of colors ( it decreases or increases ) based upon the intensity . I want to achieve that effect. I would appreciate any help or any lead to understand how heat maps work .
","['python', 'computer-vision', 'image-processing', 'filters']",
How to manage the different pixel size for a CNN?,"
I have a convolutional neural network such as U-Net for a segmentation task and in input images with the same spatial resolution (256x256) but different pixel size due to the acquisition process. Specifically, every image has FoV 370x370mm 256/256 pixel, but a different zoom, for example an image might have 2.7/1.8 px/mm and another image 2.4/1.7 px/mm. 
Considering the FoV the pixel size should be 370/256=1.44mm x pixel but with a zoom of 2.7/1.8 px/mm which is the pixel size in this case ? I thought 1.8/2.7= 0.67mm but I am not sure. 
Why should I have the same in-plane resolution (pixel size) for each image when I train my CNN and not only the same spatial resolution (256x256 px) ? 
",['neural-networks'],
What are some conferences for publishing papers on graph convolutional networks?,"
What are some conferences for publishing papers on graph convolutional networks?
","['reference-request', 'research', 'geometric-deep-learning', 'academia', 'graph-neural-networks']",
Is there a parallelizable algorithm for training sparse neural networks?,"
Consider that we want to create a very big neural network. If we consider to use dense layers, we might face some challenges. Now consider that we use sparse layers instead of dense layers. When using a really sparse model, we would have much less parameters and could create much bigger neural networks. Is there an efficient algorithm to parallelize the training of such networks? 
",['neural-networks'],
Random value generator using a single neuron or DNN,"
AI is supposed to do anything human or traditional computer can do, that is what we expect AI to be.
So 'generating random value' is also a task included in the scope that AI should be able to do 
I'm trying to generate random value using a single neuron but the outcome isn't much good.
Any suggestions? 
PS.
Random weight initialisation is allowed coz weights are constants at start.
Using 'random' function is forbidden anywhere else.
","['machine-learning', 'randomness', 'generator', 'neural-networks', 'deep-neural-networks']",
When to do discretization to decrease the state/action space in RL?,"
When to do discretization to decrease the state/action space in RL? Can you give me some references that such a technique is used?
","['reinforcement-learning', 'reference-request', 'state-spaces', 'action-spaces', 'discretization']",
Reservoir of LSM vs. FF-NN or ELM,"
The reservoir of the Liquid State Machine is an array of random numbers connected to each other with a probability depending on the distance between each other. Because of this connection with each other it apparently has ""recurrence"". The reservoir is followed by a readout stage where the actual weight training is done.
The hidden layer of a FF-NN can be an array of random weights, which is exactly what an Extreme Learning Machine is. ELM has a closed form solution of the second-stage weight (Beta) calculation i.e. only the Beta needs to be trained.
So in both cases you have a second-stage layer or readout layer where weights are trained.
My question is, if the reservoir random weights are very much like the ELM random weights and both don't need to be trained, how are they any different than each other? In other words, both have a set of untrained random weights, so in LSM where is the recurrence exactly happening if the weights are just random? Can't the LSM be reduced to a FF-NN?
",['recurrent-neural-networks'],
Why is it hard to prove the convergence of the deep Q-learning algorithm?,"
Why is it hard to prove the convergence of the DQN algorithm? We know that the tabular Q-learning algorithm converges to the optimal Q-values, and with a linear approximator convergence is proved.
The main difference of DQN compared to Q-Learning with linear approximator is using DNN, the experience replay memory, and the target network. Which of these components causes the issue and why?
","['reinforcement-learning', 'deep-rl', 'dqn', 'convergence', 'function-approximation']",
Why do RL implementations converge on one action?,"
I have seen this happening in implementations of state-of-the-art RL algorithms where the model converges to a single action over time after multiple training iterations. Are there some general loopholes or reasons why this kind of behavior is exhibited? 
","['reinforcement-learning', 'convergence', 'policies']","

Why do RL implementations converge on one action?

If the optimal policy shouldn't always select the same action in the same state, i.e., if the optimal policy isn't deterministic (e.g., in the case of the rock paper scissors, the optimal policy cannot be deterministic because any intelligent player would easily memorize your deterministic policy, so, after a while, you would always lose again that player), then there are a few things that you can do to make your policy more stochastic

Change the reward function. If your agent ends up selecting always the same action and you don't want that, it's probably because you're not giving it the right reinforcement signal (given that the agent selects the action that apparently will give it the highest reward in the long run).
Try to explore more during training. So, if you're using a behavior policy like $\epsilon$-greedy, you may want to increase your $\epsilon$ (i.e. probability of selecting a random action).
If you estimated the state-action value function (e.g. with Q-learning), maybe you derived the policy from it by selecting the best action, but, of course, that will make your policy deterministic. You may want to use e.g. softmax to derive the policy from the state-action value function (i.e. the probability of selecting an action is proportional to its value), although Q-learning assumes that your target policy is greedy with respect to the state-action value function.

If the optimal policy is supposed to be deterministic, then, if you find the optimal policy (which isn't probably the case), you will end up with an agent that always selects the same action. In that case, obviously, it's not a problem that the RL agent selects always the same optimal action.
"
Is the reward following after time step $t+1$ collected based on current policy?,"
I am currently learning policy gradient methods from the Deep RL boot camp by Pieter Abbeel in which he explains the actor-critic algorithm derivation. 
At around minute 39, he explains that the sum of the rewards from time step $t$ onwards is actually an estimation of $Q^\pi(s,u)$. I understand the definition of $Q^\pi(s,u)$ but I'm not sure why this is the case here. Is the reward following after time step $t+1$ collected based on current policy?

","['reinforcement-learning', 'policy-gradients', 'actor-critic-methods']",
Wooden railway search problem AI,"
I have this problem.

A basic wooden railway set contains the pieces shown in Figure 3.32.

The task is to connect these pieces into a railway that has no overlapping tracks and no loose ends where a train could run off onto the floor.
a. Suppose that the pieces fit together exactly with no slack. Give a precise formulation of the task as a search problem.
b. Identify a suitable uninformed search algorithm for this task and explain your choice.

I know I have to use a DFS for this problem. But how do I know all the pieces are connected. The last one and the first one are connected.
Can someone help me with some tips on how to solve this problem and implement it (in Python)?
","['ai-design', 'python', 'search']",
Can you explain me this CNN architecture?,"
I am starting to get my head around convolutional neural networks, and I have been working with the CIFAR-10 dataset and some research papers that used it. In one of these papers, they mention a network architecture notation for a CNN, and I am not sure how to interpret that exactly in terms of how many layers are there and how many neurons in each.
This is an image of their structure notation.


Can some give me an explanation as to what exactly this structure looks like?

In the CIFAR-10 dataset, each image is $32 \times 32$ pixels, represented by 3072 integers indicating the red, green, blue values for each pixel.
Does that not mean that my input layer has to be of size 3072? Or is there some way to group the inputs into matrices and then feed them into the network?


","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'architecture']","
While it would certainly help if the link to the paper could also be posted, I will give it a shot based on what I understand from this picture.
1) For any convolutional layer, there are few important things to configure, namely, the kernel (or, filter) size, number of kernels, stride. Padding is also important but it is generally defined to be zero unless mentioned otherwise. Let us consider the picture block-by-block. 
The first block contains 3 convolutional layers: (i) 2 conv layers with 96 filters each and the size of each filter is $ 3 \times 3$ (and stride $=1$ by default since it is not mentioned), and (ii) another conv layer with same configurations as above but with stride $=2$.
The second block is pretty much the same as the previous except the number of filters is increased to 192 for each layer that is defined.
The only considerable change in the third block is the introduction of $ 1 \times 1$ convolutional filters instead of $3 \times 3$.
And finally, a global average pooling layer is used (instead of a fully connected layer).
2) As for your analysis, it is exactly the case in fully connected layers, wherein the number of units in the input layer must match the vectorized dimensions of the input data. But, in the case of CNN, we give the images directly as the input to the network. The whole idea of a CNN is to understand the spatial structure of the data by analyzing patches of the image at a time (which is what the filter size defines). This PyTorch tutorial should give an idea as to how exactly the input is given to CNN.
"
How does the region proposal method work in Fast R-CNN?,"
I read so many articles and the Fast R-CNN paper, but I'm still confused about how the region proposal method works in Fast R-CNN.
As you can see in the image below, they say they used a proposal method, but it is not specified how it works.
What confuses me is, for example, in the VGGnet, the output of the last convolution layer is feature maps of shape 14x14x512, but what is the used algorithm to propose the regions and how does it propose them from the feature maps?

","['computer-vision', 'object-detection', 'r-cnn', 'selective-search', 'faster-r-cnn']","
Yes, it is not specified because the region proposal algorithm did not change from R-CNN (the previous version from Fast R-CNN, however, in the next verion, Faster R-CNN, this algorithm is replaced by a CNN).
The region proposal algorithm you are looking for is called selective search. You can find in the R-CNN paper that the algorithm is described in ""Selective Search for Object Recognition"", I found a copy here.
The algorithm is based on a series of segmentation and aggregation techniques of the input image for generating the proposed regions. Check it out 4 iterations of segmentation & aggregation over the same input image to build the proposed regions.

All the algorithm is doing is just iterating over 4 steps:

Initial regions based on segmentation by pixel light intensity are obtained by applying a segmentation algorithm described in the paper. For example, given a picture of a shepherd with his sheep in the mountain, it is segmented by light intensity, and the image of Figure (a) is obtained.
Different regions are proposed based on the previous segmentation, Figure (e)
The similarity between the proposed regions is calculated using the formula proposed in equation 6
in Section 3.2 in the paper which is nothing more than an aggregate metric of the similarity of two regions based in 4 metrics: similarity in color, texture, size and fill (measures how well a region within another)
Add the regions based on similarity and get Figure (b). Then return to step two.

That is how iteratively you get all the images depicted.
"
Are Q-learning and SARSA the same when action selection is greedy?,"
I'm currently studying reinforcement learning and I'm having difficulties with question 6.12 in Sutton and Barto's book.
Suppose action selection is greedy. Is Q-learning then exactly the same algorithm as SARSA? Will they make exactly the same action selections and weight updates?
I think it's true, because the main difference between the two is when the agent explores, and following the greedy policy it never explores, but I am not sure.
","['reinforcement-learning', 'comparison', 'q-learning', 'sarsa', 'greedy-policy']","
If we write the pseudo-code for the SARSA algorithm we first initialise our hyper-parameters etc. and then initialise $S_t$, which we use to choose $A_t$ from our policy $\pi(a|s)$. Then for each $t$ in the episode we do the following:

Take action $A_t$ and observe $R_{t+1}$, $S_{t+1}$
Choose $A_{t+1}$ using $S_{t+1}$ in our policy
$Q(S_t, A_t) = Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t)]$

Now, in Q-learning we replace $Q(S_{t+1},A_{t+1})$ in line 3 with $\max_aQ(S_{t+1},a)$. Recall that in SARSA we chose our $A_{t+1}$ using our policy $\pi$ - if our policy is greedy with respect to the action value function then this simply means the policy is $\pi(a|s) = \text{argmax}_aQ(s,a)$ which is exactly how we choose our weight update in Q-learning.
To answer the question - no, they are not always the same algorithm.
Consider where we transition from $s$ to $s'$ where $s'=s$. I will outline the updates for SARSA and Q-learning indexing the $Q$ functions with $t$ to demonstrate the difference.
For each case, I will assume we are at the start of the episode, as this is the easiest way to illustrate the difference. Note that actions denoted by $A_i$ are for actions taken explicitly in the environment -- in the Q-Learning update the $\max$ action that is chosen for the update is not executed in the environment, the action taken in the environment is chosen by the policy after the update has happened.
SARSA

We initialise $S_0 = s$ and choose $A_0 = \text{argmax}_a Q_0(s,a)$
Take action $A_0$ and observe $R_{1}$ and $S_{1} = s' = s$.
Choose action $A_{1} = \text{argmax}_aQ_{0}(s,a)$
$Q_{1}(S_0,A_0) = Q_0(S_0,A_0) + \alpha [R_{1} + \gamma Q_0(s,A_1) - Q_0(S_0,A_0)]$

Q-Learning

Initialise $S_0 = s$
Choose action $A_0 = \text{argmax}_aQ_0(s,a)$, observe $R_{1}, S_{1} = s' = s$
$Q_{1}(S_0,A_0) = Q_0(S_0,A_0) + \alpha [R_{1} + \gamma \max_aQ_0(s,a) - Q_0(S_0,A_0)]$
Choose action $A_1 = \text{argmax}_aQ_1(s,a)$

As you can see the next action for the updates in SARSA (line 4) and Q-learning (line 3) are taken with respect to the same $Q$ function, but the key difference is that the actual next action taken in $Q$-learning is taken with respect to the updated $Q$-function.
The key for understanding this edge case is that when we transition into the same state, the Q-Learning update will update the Q-function before choosing $A_1$. I have indexed actions and Q-functions by the episode step - hopefully, it makes sense why I have done this for the Q-functions as, usually, this would not make sense, but, because we have two successive states that are the same, it is okay.
"
How do you find the homography matrix given 4 points in both images?,"
I want to understand the process of finding a homography matrix given 4 points in both images. I am able to do that in python OpenCV, but I wonder how it works behind the scenes.
Suppose I have points $p_1, p_2, p_3, p_4$ in the first image and $p'_1, p'_2, p'_3, p'_4$ in the second. How am I going to generate the homography matix given these points.
","['computer-vision', 'linear-algebra', 'projective-transformations']",
Trying to Train Cards Game with RL,"
I am trying to train a card game called Callbreak I tried inputs like all the opponents discarded cards all hands everything a human can see and calculate with ""common sense"" I fed it to the Agent but it's not learning the way it should after 100M steps it learned as basic as any new/bad player will do in the game.
I have never tried to train a card game before so Idk what should I feed and how much to feed any help will be appreciated.
I have trained 100m+ steps before it will just keep repeating these drops and will never get more rewards.
config I am using is:
default:
trainer: ppo
batch_size: 1024
beta: 5.0e-3
buffer_size: 10240
epsilon: 0.2
hidden_units: 256
lambd: 0.95
learning_rate: 3.0e-4
learning_rate_schedule: linear
max_steps: 100.0e5
memory_size: 128
normalize: false
num_epoch: 3
num_layers: 3
time_horizon: 64
sequence_length: 64
summary_freq: 100000
use_recurrent: false
vis_encode_type: simple
reward_signals:
    extrinsic:
        strength: 1.0
        gamma: 0.99

Tensorboard

New Training Results
default:
trainer: ppo
batch_size: 1024
beta: 5.0e-3
buffer_size: 10240
epsilon: 0.2
hidden_units: 512
lambd: 0.95
learning_rate: 3.0e-4
learning_rate_schedule: linear
max_steps: 500.0e5
memory_size: 128
normalize: false
num_epoch: 3
num_layers: 3
time_horizon: 64
sequence_length: 64
summary_freq: 100000
use_recurrent: false
vis_encode_type: simple
reward_signals:
    extrinsic:
        strength: 1.0
        gamma: 0.99
    curiosity:
        strength: 0.02
        gamma: 0.99
        encoding_size: 256




","['reinforcement-learning', 'tensorflow']",
What do the authors of this paper mean by the bias term in this picture of a neural network implementation?,"
I am reading a paper implementing a deep deterministic policy gradient algorithm for portfolio management. My question is about a specific neural network implementation they depict in this picture (paper, picture is on page 14).

The first three steps are convolutions. Once they have reduced the initial tensor into a vector, they add that little yellow square entry to the vector, called the cash bias, and then they do a softmax operation.
The paper does not go into any detail about what this bias term could be, they just say that they add this bias before the softmax. This makes me think that perhaps this is a standard step? But I don't know if this is a learnable parameter, or just a scalar constant they concatenate to the vector prior to the softmax.
I have two questions:
1) When they write softmax, is it safe to assume that this is just a softmax function, with no learnable parameters? Or is this meant to depict a fully connected linear layer, with a softmax activation?
2) If it's the latter, then I can interpret the cash bias as being a constant term they concatenate to the vector before the fully connected layer, just to add one more feature for the cash assets. However, if softmax means just a function, then what is this cash bias? It must be a constant that they implement, but I don't see what the use of that would be, how can you pick a constant scalar that you are confident will have the intended impact on the softmax output to bias the network to put some weight on that feature (cash)? 
Any comments/interpretations are appreciated!
","['convolutional-neural-networks', 'papers', 'algorithmic-bias', 'softmax']","
to the cash bias: I think this is simply the money that is still available at time t=50 and has not yet been invested.
"
Why can't DQN be used for self-driving cars?,"
Why can't DQN be used for self-driving cars?
Why can't DQN and similar RL algorithms be used for self-driving cars?
The reason why I am curious is that it successfully plays go and other multistate games.
","['reinforcement-learning', 'dqn', 'deep-rl', 'applications', 'autonomous-vehicles']","
I'm not familiar with the ins and outs of self-driving cars, but I imagine that the action space is not discrete. For instance, the car may want to decide what angle it needs to turn (rather than left or right). The update in Q-Learning involves taking $\max_aQ(s, a)$; this is theoretically possible for a continuous action space, but it would itself require some expensive optimisation at each time step to find the maximum. It is more likely that if RL were to be applied to self-driving cars it would be through a method that easily allows for a continuous action space, like the methods detailed in this paper.
I found this survey of Deep RL for autonomous driving that you may want to look at.
"
Is the high dimensionality of input vectors a problem for a radial basis function neural network?,"
I have a dataset A of videos. I've extracted the feature vector of each video (with a convolutional neural network, via transfer learning) creating a dataset B. Now, every vector of the dataset B has a high dimension (about 16000), and I would like to classify these vectors using an RBF-ANN (there are only 2 possible classes).
Is the high dimensionality of input vectors a problem for a radial basis function ANN? If yes, is there any way to deal with it?
","['neural-networks', 'deep-learning', 'classification', 'transfer-learning']",
How can blackjack be formulated as a Markov decision process?,"
I am reading sutton barton's reinforcement learning textbook and have come across the finite Markov decision process (MDP) example of the blackjack game (Example 5.1). 
Isn't the environment constantly changing in this game? How would the transition probabilities be fixed in such an environment, when both you and the dealer draw cards?
","['reinforcement-learning', 'markov-decision-process']",
Which activation functions should I use for polynomial regression?,"
I am a beginner in machine learning and neural networks. I have only used neural networks for classification problems.  My aim is to modify it so that it can work for polynomial regression as well. In my problem, I have three inputs and three outputs. My aim is to predict these three outputs based on these three inputs. The outputs are real-valued, and can take positive and negative values. 
How should I choose the activation functions? I have only used sigmoid. 
","['neural-networks', 'machine-learning', 'ai-design', 'activation-functions', 'regression']",
Should Monte Carlo tree search be able to consistently beat me in the connect four game?,"
I've implemented the Monte Carlo tree search (MCTS) algorithm for a connect four game I've built. The MCTS agent beats a random choice agent 90-100% of the time, but I’m still able to beat it pretty easily. It even misses obvious three in a row opportunities where it just needs to add one more token to win (but places it elsewhere instead). 
Is this normal behavior, or should the MCTS agent be able to beat me consistently too? I'm allowing it to grow its tree for 2 seconds before getting it to return its chosen action - could it be that it needs longer to think?
","['machine-learning', 'reinforcement-learning', 'monte-carlo-tree-search']","
You should not let the tree grow for only two seconds rather you should use the simulation number equal to 1000 or something like that. I use the simulation number equal to 10000 for making a single move in the tictactoe game and it was working fine for me. Also, after the agent has chosen the move you do not have to start the statistics(N = visit count, V = expected reward, U = UCT score) from the beginning, you can use the current statistics and replace the root node with the chosen node. 
"
Is there 1-dimensional reinforcement learning?,"
From what I can find, reinforcement algorithms work on a grid or 2-dimensional environment.  How would I set up the problem for an approximate solution when I have a 1-dimensional signal from a light sensor.  The sensor sits some distance away from a lighthouse.  The intent would be to take the reading from the sensor to determine the orientation of the lighthouse beam.
The environment would be a lighthouse beam, the state would be the brightness seen at the sensor for a given orientation, and the agent would be the approximate brightness/orientation?  What would the reward be?  What reinforcement learning algorithm would I use to approximate the lighthouse orientation given sensor brightnesses?
",['reinforcement-learning'],
What are the differences between artificial neural networks and other function approximators?,"
Modern artificial neural networks use a lot more functions than just the classic sigmoid, to the point I'm having a hard time really seeing what classifies something as a ""neural network"" over other function approximators (such as Fourier series, Bernstein polynomials, Chebyshev polynomials or splines). 
So, what makes something an artificial neural network? Is there a subset of theorems that apply only to neural networks? 
Backpropagation is classic, but that is the multi-variable chain rule, what else is unique to neural networks over other function approximators? 
","['neural-networks', 'machine-learning', 'comparison', 'function-approximation']","
First of all, neural networks are not (just) defined by the fact that they are typically trained with gradient descent and back-propagation. In fact, there are other ways of training neural networks, such as evolutionary algorithms and the Hebb's rule (e.g. Hopfield networks are typically associated with this Hebbian learning rule). 
The first difference between neural networks and other function approximators is conceptual. In neural networks, you typically imagine that there are one or more computational units (often called neurons) that are connected in different and often complex ways. The human can choose these connections (or they could also be learned) and the functions that these units compute given the inputs. So, there's a great deal of flexibility and complexity, but, often, also a lack of rigorousness (from the mathematical point of view) while using and designing neuron networks. 
The other difference is that neural networks were originally inspired by the biological counterparts. See A logical calculus of the ideas immanent in nervous activity (1943) by Warren McCulloch and Walter Pitts, who proposed, inspired by neuroscience, the first mathematical model of an artificial neuron.
There are other technical differences. For example, the Taylor expansion of a function is typically done only at a single value of the domain, it assumes that the function to be approximated is differentiable multiple times, and it makes uses of the derivatives of such a function. Fourier series typically approximate functions with a weighted sum of sinusoids. Given appropriate weights, the Fourier series can be used to approximate an arbitrary function in a certain interval or the entire function (if the function you want to approximate is also periodic). On the other hand, neural networks attempt to approximate functions of the form $f: [0, 1]^n \rightarrow \mathbb{R}$ (at least, this is the setup in the famous paper that proved the universality of neural networks) in many different ways (for example, weighted sums followed by sigmoids).
To conclude, neural networks are quite different from other function approximation techniques (such as Taylor or Fourier series) in the way they approximate functions and their purpose (i.e. which functions they were supposed to approximate and in which context).
"
3d representation of a regression with two independent variables one of them is categorical and another is continuous,"
I have hopefully a fundamental question of Do I understand things right.
(Thank you in advance and sorry for my English which might be not so good)
1-Preambula 1:
I know that if we have 2 independent variables, both of a continuous type, it is ok to represent them as a 2d plane in a 3d space:

2-Preambula 2:
I have seen that many times when we have to deal with continuous and categorical variables(male\female for example), we represent them like this(note the lines are parallel):

3-Assumption:
In the beginning I assumed that it is 2d representation of this 3d case:

4-Discussion 1: But If my assumption above was right, why do they always ""picture""  it with parallel lines? After all this is a very specific situation. In most of cases both regression lines will not be parallel, further more, they may have different slope direction (one negative and another positive)For example:

5-Discussion 2: On the other hand parallel models may be explained in such a way: if we will add a regression hyperplane which ""somehow"" fits both groups(male and female), we will get the parallel lines:

6-Finally My questions are quite simple.
question 5.1: Did I understand right the nature of parallel lines as I show it above (in discussion2)?
question 5.2: If I was right in 5.1, I assume that in such cases hyperplane regression is a quite a bad predictor. Am I right?
","['machine-learning', 'prediction', 'regression', 'linear-regression', 'categorical-data']",
Simplification of expected reward under the limit in continuous tasks,"
I was reading the average reward setting for continuous tasks from rich sutton's book (page 202, 2nd edition). There he perform a simplification over the expected reward under the limit approaching to infinite. I mark this point in this picture:

The book does not clearly mention the steps to simplify the above expression. I search on the web to find the solution but there is no clear explanation on that. Can anyone explain the marked point?
","['deep-learning', 'reinforcement-learning', 'statistical-ai']",
How long should the state-dependent baseline for policy gradient methods be trained at each iteration?,"
How long should the state-dependent baseline be trained at each iteration? Or what baseline loss should we target at each iteration for use with policy gradient methods? 
I'm using this equation to compute the policy gradient:
$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\underset{\tau \sim \pi_{\theta}}{\mathrm{E}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)\left(\sum_{t^{\prime}=t}^{T} R\left(s_{t^{\prime}}, a_{t^{\prime}}, s_{t^{\prime}+1}\right)-b\left(s_{t}\right)\right)\right]
$$
Here is mentioned to use one or more gradient steps, so is it a hyper-parameter to be found using random search? 
Is there some way we can use an adaptive method to find out when to stop?
In an experiment to train Cartpole-v2 using a policy gradient with baseline, I found the results are better when applying 5 updates than when only a single update was applied.
Note: I am referring to the number of updates to take on a single batch of q values encountered across trajectories collected using current policy.
","['reinforcement-learning', 'gradient-descent', 'policy-gradients', 'reinforce', 'adam']","
I went into the pytorch code for the spinning up implementation of vanilla policy gradient and from what I could understand, found that they use a learning rate of 1e-3 for training the baseline and do a gradient descent 80 times on the same dataset by default with no termination criteria. 
Also it is usually impossible to fit the value function completely as we are using a function approximator. The main point is not to get too carried away to get the lowest loss as the main improvement in the agent's performance will happen by doing policy gradient descents rather than trying to aggressively get the accurate value function for the baseline.
Link for the implementation:
spinning up implementation of vpg
"
Is AI already being used in the drug industry to combat the COVID-19?,"
We all have heard about how beneficial AI can be in health. There are plenty of papers and research about confronting diseases, like cancer. However, in 2020 with COVID-19 be one of the most serious health problems that have caused thousands of deaths worldwide.
Is AI already being used in the drug industry to combat the COVID-19? If yes, can you, please, provide a reference?
","['reference-request', 'healthcare']","
I'm not sure if it is being used directly in the industry, but here is an interesting article on research being done by 3 UK universities using AI. 
"
How to implement AI strategy for Mastermind,"
I'm looking to implement a AI for the turn-based game Mastermind in Node.JS, using Google's Tensorflow library. Basically the AI needs to predict the 4D input for the optimal 2D output [0,4] with a given list of 4D inputs and 2D outputs from previous turns in the form of [input][output].
The optimal output would be [0,4], which would be the winning output. The training data looks like this:
[1,2,3,4][0,1] [0,5,2,6][3,1] [0,2,5,6][2,2] [6,5,2,0][4,0] [5,2,0,6][0,4]

So given these previous turns
[1,2,3,4][0,1] [0,5,2,6][3,1] [0,2,5,6][2,2] [6,5,2,0][4,0]

the AI would predict an input of [5,2,0,6] for the output [0,4].
I've looked at this post but it talks about only inferring input for a output without any context. In Mastermind, the context of previous guesses and results from them are critical
My algorithm would need to use the information from previous turns to determine the best input for the winning output ([0,4]).
So my question is: How can I implement AI for Mastermind?
","['deep-learning', 'tensorflow', 'prediction']","
You could possibly apply neural networks, reinforcement learning to summarise results of previous choices (what you are calling context) and use score predictions to suggest the next turn's guess. However, the game of Mastermind has a small search space and it is possible to process this ""context"" more directly by refining a set of guesses. This will be much more efficient and simpler to understand than a neural network approach. It would be very hard to make a neural network variant which was as efficient - either in terms of CPU time, or in terms of number of turns it takes to find a solution.
In practice, a Mastermind solver is much like a Hangman solver, or a Guess Who? solver. You have an initial large set of all possible answers, and need to narrow it down to a single correct answer. You do this by processing after each guess to reduce the set of answers that meet all the constraints that the game has given you so far.
The agent needs to know the score function that compares a target value with the guess and returns the score. Let's call that score(guess, target)
The algorithm looks like this:
(Opponent sets unknown_target)
Initialise possible_answers as list of all valid targets

For each turn:
  Select one of possible_answers as this turn's guess
  Ask opponent for gscore = score(guess, unknown_target)
  If gscore is [0,4] then exit(win)
  If it was last guess then exit(lose)
  For each possible_answer in possible_answers:
    pscore = score(guess, possible_answer)
    If pscore != gscore, then remove possible_answer from possible_answers

You can finesse this for the stage Select one of possible_answers as this turns guess by trying to optimise either by psychological model of opponent or trying to find choices that are likely to cause the best reduction in size of possible_answers. However, a simple random choice should do quite well.
Also worth noting is that the algorithm does not depend on the exact nature of the scoring function, so it is applicable for many variations of guessing games. It does rely on the score providing information that will reduce the remaining set of guesses. In some games that may mean taking more care about the precise nature of a guess, in order to maximise this effect.
Out of interest, I implemented this algorithm and tested it when there were 10 choices at each position (i.e. digits 0 to 9), maximum of 10 guesses allowed, and the target to guess was set randomly. Using random guesses and the algorithm exactly as written, the above approach guessed correctly 9,996 times out of 10,000, and on average the guesser won the game in 6.2 turns.
"
Are there any general guidelines for dealing with imbalanced data through upsampling or downsampling?,"
Are there any general guidelines for dealing with imbalanced data through upsampling/downsampling? 
This Google developer guide suggests performing downsampling with upweighting, but for the most part I've found upsampling usually works better in practice (some corroboration). 
Is there any clear consensus or empirical study of what works in practice, or when to use which?  Does it matter which classification algorithm you use?
","['machine-learning', 'classification', 'data-science', 'data-preprocessing']",
"My CNN model performs bad on new (self-created) pictures, what are possible reasons?","
I wanted to train a model that recognizes sign language. I have found a dataset for this and was able to create a model that would get 94% accuracy on the test set. I have trained models before and my main goal is not to have the best model (I know 94% could easiy be tuned up). However these models where always for class exercises and thus were never used on 'real' new data.
So I took a new picture of my hand that I know I wanted to be a certain letter (let's assume A).
Since my model was trained on 28x28 images, I needed to re-size my own image because it was larger. After that I fed this image to my model only to get a wrong classification.
https://i.stack.imgur.com/Rf1UK.jpg
These are my pictures (upper-left = my own image (expected class A), upper-right = an image of class A (that my model correctly classifies as A), bottom = picture of class Z (the class my image was classified as)).
You can clearly see that my own image looks for more like the image of class A (that I wanted my model to predict), than the model it did predict.
What could be reasons that my model does not work on real-life images? (If code is wanted I can provide it ofcourse but since I don't know where I go wrong, it seemed out of line to copy all the code).
","['convolutional-neural-networks', 'image-recognition']","
I’m assuming that you used LeNet (our some other model with small number of parameters) since your training image size is 28x28. Note that LeNet doesn’t generalize well to new images. I think it performs fine (>90%) on MNIST but not good on CIFAR10 (>60%) albeit both datasets contain similar size image. (Just trying to remember the performance from PyTorch implementations). It’s more about if the model has capacity to learn the complexity of dataset. CIFAR10 is more complex and harder to model than MNIST. 
LeNet is a small image classification model (in terms of capacity) so it cannot nicely learn the correlation between pixels of input images well and therefore doesn’t perform well on unseen images. 
In your case it seems like your model has overfit to training examples. It might perform well on test images because both training and test subsets are sampled from same data generating distribution but real-world images it might experience in future might be different (like your own hand). If it doesn’t perform well on unseen images we say it has not generalized well, which looks the case in your situation. In this case you need a validation set to validate that your model generalizes to unseen images. If you have it then you should use it in early stopping regularization technique. You can also add other regularizers to your model (the simplest one is weight decay). 
But instead of inventing your network architecture why don’t you use models like ResNet. Just fine-tune the pre-trained ResNet on your own dataset. I’d prefer to fine-tune personally in this situation because data distribution it was trained on (ImageNet) is pretty different from your hand sign dataset. In other case if your dataset contained nature and surrounding images I’d rather freeze the parameters of fixed-feature extractor layers and trained only the last few layers of ResNet (or similar model). 
I hope this helps! 
"
Can we combine Off-Policy with On-Policy Algorithms?,"
On-Policy Algorithms like PPO directly maximize the performance objective or an approximation of it. They tend to be quite stable and reliable but are often sample inefficient. Off-Policy Algorithms like TD3 improve the sample inefficiency by reusing data collected with previous policies, but they tend to be less stable. (Source: Kinds of RL Algorithms - Spinning up - OpenAI)
Looking at learning curves comparing SOTA algorithms, we see that off-policy algorithms quickly improve performance at the training's beginning. Here an example:

Can we start training off-policy and after some time use the learned and quickly improved policy to init the policy network of an on-policy algorithm? 
","['reinforcement-learning', 'deep-rl', 'open-ai', 'off-policy-methods', 'on-policy-methods']",
How should I deal with variable batch size in A3C?,"
I am fairly new to reinforcement learning (RL) and deep RL. I have been trying to create my first agent (using A3C) that selects an optimal path with the reward being some associated completion time (the more optimal the path is, packets will be delivered in less time kind of).
However, in each episode/epoch, I do not have a certain batch size for updating my NN's parameters.
To make it more clear, let's say that, in my environment, on each step, I need to perform a request to the servers, and I have to select the optimal path.
Now, each execution in my environment does not contain the same amount of files. For instance, I might have 3 requests as one run, and then 5 requests for the next one.
The A3C code I have at hand has a batch size of a 100. In my case, that batch size is not known a priori.
Is that going to affect my training? Should I find a way to keep it fixed somehow? And how can one define an optimal batch size for updating?
","['neural-networks', 'reinforcement-learning', 'actor-critic-methods', 'a3c']",
How do I determine the genomes to use for crossover in NEAT?,"
If I have the fitness of each genome, how do I determine which genome will crossover with which, and so on, so that I get a new population?
Unfortunately, I can't find anything about it in the original paper, so I ask here?
","['genetic-algorithms', 'neat', 'neuroevolution', 'crossover-operators', 'genetic-operators']","
The original work on NEAT(Neuroevolution of augmenting topologies) was by Ken Stanley in 2002 at The University of Texas at Austin. The web page for the project is here I suggest you download and read the paper linked from that page. As for selection of genome pairs, NEAT makes use of a speciation model so  the selection of such pairs is constrained to at least prefer pairs from the same 'species', on the assumption that the species has evolved such that species population is isolated under reproduction. The innovation that has been 'bred' into the species is thus preserved under reproduction.  Selection by fitness alone is insufficient in such models. This differs from the simple GA where pair selection is unconstrained.
"
Which NLP model to use to handle long context?,"
I'm trying to process product data for an e-commerce platform. The goal is to understand products' size.
Just to show you some examples on how messy product dimension description is:
Overall Dimensions: 66 in W x 41 in D x 36 in H
Overall: 59 in W x 28.75 in D x 30.75 in H
92w 37d 32h"",
86.6 in W x 33.9 in D x 24 in H
W: 95.75\"" D: 36.5\"" H: 28.75\"""",
W: 96\"" D: 39.25\"" H: 32\"""",
""118\""W x 35\""D x 33\""T."",
""28 L x 95 W x 41 H""
""95\"" W x 26.5\"" H x 34.75\"" D""
""98\""W x 39\""D x 29\""H""
""28\"" High x 80\"" Wide x 32\"" Deep""


Now assume that the product dimension description is short < 60 characters, I trained a two layer bidirectional LSTM, which can handle this task perfectly.
But the problem is, the above dimension is usually embedded in a long context (as a part of the product description). How can I extract the useful information from the long context and understand it? My LSTM can only accept context size of 60.
What language model is more suitable for this?
","['natural-language-processing', 'long-short-term-memory']","
In this problem, you can also use NER models in order to tag those numbers as Width, Height,... . You can also fine-tune a DistilBert model for your task.
if you want to train NER model by tok2vec you can use:
Spacy Library
for fine-tuning DistilBERT you can use this.
Hugging Face
"
How to create vector representation of roadmap like scans,"
What would be the best way to create a vector representation of roadmap like scans? The goal I am trying to achieve is illustrated below. The left side represents the source image, the right side the output in the form of three vectors. The fussiness on the left is a simulation, not the actual source image:
 
The actual source image would look more like:

Currently I am looking at a combination of skeletonization and Hough transform. The result is rather messy though, and seems to warrant quite some extra engineering. Any other suggestions?
","['image-processing', 'image-segmentation']","
It turned out that my intuition was not far off. The skeletonization is a good step. The Hough transform though is not a good way to create a graph of the roadmap. It seems that the Ramer–Douglas–Peucker algorithm can help out here. This algorithm first takes all the skeleton pixels as input, and sees this as a starting graph. The algorithm then proceeds to remove intermediary pixels that do not add information to the shape of the graph.
"
Are activation functions applied to feature maps?,"
If I have a convolutional neural network, and I convolve my input tensor with a kernel, the output is a feature map. Is an activation function then applied to this feature map?
If its an image that is a 2D tensor, would the activation function change every single value on this image? 
","['convolutional-neural-networks', 'activation-functions', 'convolution']",
How does heuristic work with multiple agents?,"
I have a question for heuristic search with multiple agents. I know how heuristic search works with one agent (ex. one Pacman) but I don't really understand it with multiple agents. Let's say we have this problem where Worm A has to get to its goal state A and Worm B to B, knowing that the agents can move only in vertical and horizontal way:

If we had only Worm B, the optimal cost from starting position to the goal position would be 9, since one action costs 1 and it'd follow the path RIGHT-RIGHT-RIGHT-RIGHT-RIGHT-RIGHT-UP-UP-UP.
My question is, if we have two worms, like in the picture, the optimal cost would be 9 + optimal cost for Worm A?
Also, strictly for this problem with 2 agents, if we use Manhattan distance as a heuristic for one agent, would it be admissible if we take the average of Worm A and B heuristics for a problem with two agents? 
Another question, I know for a fact that sum of two admissible heuristics won't be admissible for one agent but would it be for the problem with two agents?
These two worms are dependent of each other. How? If one worm moves from position X to Y, the position X is marked as a wall and is not an available field to move in. So if one worm has been in a specific position, that position is no more free for moving in.
For example, if we have something like B^^^X^^^, where B is the Worm B, ^ is an available field and X is a wall, after one RIGHT action it'll look like XB^^X^^, after one more RIGHT: XXB^X^^ etc.
","['heuristics', 'multi-agent-systems', 'a-star']",
How does the AlphaGo Zero policy decide what move to execute?,"
I was going through the AlphaGo Zero paper and I was trying to understand everything, but I just can't figure out this one formula:
$$
\pi(a \mid s_0) = \frac{N(s_0, a)^{\frac{1}{\tau}}}{\sum_b N(s_0, b)^{\frac{1}{\tau}}}
$$
Could someone decode how the policy makes decisions following this formula? I pretty much understood all of the other parts of the paper, and also the temperature parameter is clear to me.
It might be a simple question, but can't figure it out.
","['reinforcement-learning', 'policies', 'deepmind', 'alphago-zero', 'alphago']","
Intuitively:

Larger Q means larger probability that the node (s'|s,a) would be chosen. When we selected most visited node, we selected a node with good Q.
More visited count means more accurate estimation. And the chosen node proved itself as a good choice even after more trials than other nodes.
Less computation in some cases (integer/long vs float/double)

"
Are these two definitions of the state-action value function equivalent?,"
I have been reading the Sutton and Barto textbook and going through David Silvers UCL lecture videos on YouTube and have a question on the equivalence of two forms of the state-action value function written in terms of the value function. 
From Question 3.13 of the textbook I am able to write the state-action value function as
$$q_{\pi}(s,a) = \sum_{s',r}p(s',r|s,a)(r + \gamma v_\pi(s')) = \mathbb{E}[r + \gamma v_\pi(s')|s,a]\;.$$
Note that the expectation is not taken with respect to $\pi$ as $\pi$ is the conditional probability of taking action $a$ in state $s$. Now, in David Silver's slides for the Actor-Critic methods of the Policy Gradient lectures, he says that 
$$\mathbb{E}_{\pi_\theta}[r + \gamma v_{\pi_\theta}(s')|s,a] = q_{\pi_\theta}(s,a)\;.$$
Are these two definitions equivalent (in expectation)? 
","['reinforcement-learning', 'comparison', 'value-functions', 'expectation', 'bellman-equations']",
An Encoder-Decoder based CNN to predict a tensor of points,"
So I have with me a data of rendered 2D images of a 3D object and along with that, I have the image projection coordinates (X, Y) of all the voxels that are in the camera perspective in that image.
The rendered image
The voxel camera projections(This is just for visualization purposes)

I wanted to build a CNN which takes an input a rendered image and outputs all the voxel camera projection coordinates (X, Y).I was thinking to try out an encoder-decoder based network (like a U-Net) but shouldn't the image obtained after decoding be the same dimensions as the input image. I want my decoder to output the tensor of coordinates but I am having a hard time thinking about how it would do so.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'python', 'computer-vision']",
Why can't pure KG embedding methods discover multi-hop relations paths?,"
According to Reinforcement Knowledge Graph Reasoning for Explainable Recommendation

pure KG embedding methods lack the ability to discover multi-hop relational paths.

Why is it so?
","['reinforcement-learning', 'recommender-system', 'knowledge-graph', 'knowledge-graph-embeddings']",
"Designing a chatbot personal project with zero coding experience, using an existing platform","
My girlfriend has a masters degree in linguistics and would like to create an AI chatbot personal project to show potential employers her linguistics skills since she is struggling to find a job.
Unfortunately she doesn't know how to program except for extremely basic Python skills. She has been searching for weeks for tools to create a chatbot without needing to program, refusing to ask on forums for help so I'm asking the StackExchange community. Sort of like a plugin/widget for Slack, Facebook Messenger or website that you can just install on your website and just concentrate on the workflow/data/conversational design in a similar way to programming in Scratch or Node-Red.
I know nothing about NLP, neural networks or anything like that and I can't understand for the life of me  what exactly it is a linguist needs to do in AI. Adversely, she doesn't have the computer knowledge to understand how an API works, or how to get some sort of service and a chat interface are needed to bootstrap her conversational designs with some code somewhere.
So my question is: Is there a way for a linguist to create a chatbot without knowing programming or going in too deep, all by themselves? We looked at tools like hubspot.com, where the chatbot design is either multiple choices questions with predefined answers or offers expensive paid solutions for companies. I'm sure there are free educational or community open-source platforms doing this.
","['natural-language-processing', 'chat-bots', 'natural-language-understanding']",
Creating Text Features using word2vec,"
My task is to classify some texts. I have used word2vec to represent text words and I pass them to an LSTM as input. Taking into account that texts do not contain the same number of words, is it a good idea to create text features of fixed dimension using the word2vec word representations of the text and then classify the text using these features as an input of a neural network? And in general is it a good idea to create text features using this method?
","['natural-language-processing', 'word-embedding', 'word2vec', 'text-classification']",
Why do we calculate the mean squared error loss to improve the value approximation in Advantage Actor-Critic Algorithm?,"
class AtariA2C(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(AtariA2C, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU(),
        )

        conv_output_size = self. _get_conv_out(input_shape)

        self.policy = nn.Sequential(
            nn.Linear(conv_output_size, 512),
            nn.ReLU(),
            nn.Linear(512, n_actions),
        )

        self.value = nn.Sequential(
            nn.Linear(conv_output_size, 512),
            nn.ReLU(),
            nn.Linear(512, 1),
        )

    def _get_conv_out(self, shape):
        o = self.conv(T.zeros(1, *shape))
        return int(np.prod(o.shape))

    def forward(self, x):
        x = x.float() / 256
        conv_out = self.conv(x).view(x.size()[0], -1)
        return self.policy(conv_out), self.value(conv_out)

In Maxim Lapan's book Deep Reinforcement Learning Hands-on, after implementing the above network model, it says 

The forward pass through the network returns a tuple of two tensors:
  policy and value. Now we have a large and important function, which
  takes the batch of environment transitions and returns three tensors:
  batch of states, batch of actions taken, and batch of Q-values
  calculated using the formula $$Q(s,a) = \sum_{i=0}^{N-1} \gamma^i r_i
 + \gamma^N V(s_N)$$ This Q_value will be used in two places: to calculate mean squared error (MSE) loss to improve the value
  approximation, in the same way as DQN, and to calculate the advantage
  of the action.

I am very confused about a single thing. How and why do we calculate the mean squared error loss to improve the value approximation in Advantage Actor-Critic Algorithm?
","['python', 'q-learning', 'actor-critic-methods', 'mean-squared-error']","
I believe that the author is referring to how the networks are trained in Deep RL. Consider Deep Q-Learning where the $Q(s,a)$ is approximated using a neural network. Then the loss function used to train the network is 
$$\mathbb{E}[(r + \gamma \max_{a'} Q(s',a') - Q(s,a))^2]\;.$$
Here, $r + \gamma \max_{a'} Q(s',a')$ is your target, what you want your network to aim towards, and $Q(s,a)$ is what your network predicted. (Note that I have left off some details that can be found in the Nature paper for simplicity). 
As for actor-critic methods, most popular actor-critic methods will use the value function to 'replace' the action-value function by using the following relationship:
$$\mathbb{E}[r + \gamma v_\pi(s')] = Q_\pi(s,a)\;.$$
This relationship can be proved by looking at exercise 3.13 (or somewhere around there) in the Sutton and Barto textbook. This looks like what the author is doing in the textbook you are reading.
Based on what I said at the start regarding how state-action value functions are trained, it is analogous to train a critic network that approximates the value function in the same way. 
e1: spelling.
e2: added link to nature paper. 
"
What is 3D face recognition? and how we can check liveness of a face image?,"
Actually what is mean by 3D face recognition? In normal cases we are extracting face encoding s from a 2D image,right? 
Is 3D face recognition is used for liveness detection? how its possible? 
","['image-recognition', 'object-detection', 'facial-recognition']","
1. What is typically meant by 3D-face recognition? We are usually extracting the face encoding from 2D-images, right?
Yes. The goal is to reconstruct the three-dimensional shape, as well as the texture of a face from a single or multiple images of that person.
In recent years, ""the performance of 2D face recognition algorithms has significantly increased with the use of deep neural networks and the use of large-scale labeled training data."" Deep 3D Face Ifentification with the latter still being a notable problem because there are so few of them and they are usually needed to better measure and hence train the deep networks. (The referenced paper gives you a nice overview.) Even with enough data, occlusions, lens distortions or possible variations of expressions, etc. make the problem tough. So many other tricks and augmentation techniques are needed.
2. How can 3D face recognition used for liveness detection?
3D face recognition is used to differentiate 2D spoofs from actual present people. Using multiple time-delayed images one could possibly track if the face moves like a living person as well, or if it is static or stretching weird like a mask, puppet, or moving display.
Nonetheless, often even advanced 2D image-based techniques fall flat in real-world applications because of the used cameras. They might not have enough resolution to recognize if the input is a video of a face on a high-resolution display as explained here.
"
How can raw data from a motion sensor (like an IMU) reduced to the main points of the data,"
How can I reduce the caputured movement data of a person in a way, that I have filtered the main features of the movement. Or how can I detect pattern/ main features in that data set?
I captured some data with a device (Inertial sensor and motion capturing (x,y,z)) attached to human body. So I have a huge data set. I want to prepare the data set so I have no noise and no ""unecessary"" data.
In the end I just want to know for example that certain movement behaviour hints to a clueless guy or a person under stress.
I first thought approaches from association analysis could be useful but I think based on what I have read about the application of such algorithms they are not suitable for this data set.
","['datasets', 'data-preprocessing']",
What does it mean to parameterise a policy in policy gradient methods?,"
Can you explain policy gradient methods and what it means for the policy to be parameterised? I am reading Sutton and Barto book on reinforcement learning and didn't understand well what it is, can you give some examples?
","['reinforcement-learning', 'policy-gradients']","
In the context of RL, for a policy to be parameterised it typically means we explicitly model the policy and is common in policy gradient methods. 
Consider value based methods such as Q-learning where our policy is usually something like $\epsilon$-greedy where we choose our action using the following policy
\begin{align}
\pi(a|s) = \left\{ \begin{array}{ll}
\arg \max_a Q(s,a) & \text{with probability } 1-\epsilon\;; \\ \text{random action} & \text{with probability } \epsilon\;.
\end{array}\right.
\end{align}
Here we have parameterised the policy with $\epsilon$ but the learning is done by learning the Q-functions. When we parameterise a policy we will explicitly model $\pi$ by the following: 
$$\pi(s|a,\boldsymbol{\theta}) = \mathbb{P}(A_t = a | S_t=s, \boldsymbol{\theta}_t = \boldsymbol{\theta})\;.$$
Learning is now done by learning the parameter $\boldsymbol{\theta}$ that maximise some performance measure $J(\boldsymbol{\theta})$ by doing approximate gradient ascent updates of the form
$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \hat{\Delta J(\boldsymbol{\theta}_t)}. $$
Note that, as per the Sutton and Barto textbook, $\hat{\Delta J(\boldsymbol{\theta}_t)}$ is a noisy, stochastic estimate of $\Delta J(\boldsymbol{\theta}_t)$ where the former approximates the latter in expectation. 
The policy can be parameterised in any way as long as it is differentiable with respect to the parameters. Commonly in Deep RL the policy is parameterised as a neural network so $\boldsymbol{\theta}$ would be the weights of the network.
"
Is there any research on models that provide uncertainty estimation?,"
Is there any research on machine learning models that provide uncertainty estimation?
If I train a denoising autoencoder on words and put through a noised word, I'd like it to return a certainty that it is correct given the distribution of data it has been trained on. 
Answering these questions or metrics for uncertainty are both things I am curious about. Just general ways for models to just say ""I'm not sure"" when it receives something far outside the inputs it's been trained to approximate.
","['machine-learning', 'bayesian-deep-learning', 'bayesian-neural-networks', 'uncertainty-quantification']",
Q table not converging for an arbitrary experiment,"
This is an experiment in order to understand the working of Q table and Q learning.
I have the states as 
states = [0,1,2,3]
I have an arbitrary value for each of these states as shown below (assume index-based mapping) -
arbitrary_values_for_states = [39.9,47.52,32.92,37.6]
I want to find the minimum of the state which will give me the minimum value.
So I have complimented the values to 50-arbitrary value.
inverse_values_for_states = [50-x for x in arbitrary_values_for_states]
Therefore, I defined reward function as -
def reward(s,a,s_dash):
    if inverse_values_for_states[s]<inverse_values_for_states[s_dash]:
        return 1
    elif inverse_values_for_states[s]>inverse_values_for_states[s_dash]:
        return -1
    else:
        return 0

Q table is initialized as - 
Q = np.zeros((4,4)) (np is numpy)
The learning is carried out as -
episodes = 5
steps = 10
for episode in range(episodes):
    s = np.random.randint(0,4)
    alpha0 = 0.05
    decay = 0.005
    gamma = 0.6
    for step in range(steps):
        a = np.random.randint(0,4)
        action.append(a)
        s_dash = a
        alpha = alpha0/(1+step*decay)
        Q[s][a] = (1-alpha)*Q[s][a]+alpha*(reward(s,a,s_dash)+gamma*np.max(Q[s_dash]))

        s = s_dash

The problem is, the table doesn't converge.
Example. For the above scenario - 
np.argmax(Q[0]) gives 3
np.argmax(Q[1]) gives 2
np.argmax(Q[2]) gives 2
np.argmax(Q[3]) gives 2

All of the states should give argmax as 2 (which is actually the index[state] of the minimum value).
Another example, 
when I increase steps to 1000 and episodes to 50, 
np.argmax(Q[0]) gives 3
np.argmax(Q[1]) gives 0
np.argmax(Q[2]) gives 1
np.argmax(Q[3]) gives 2

More, steps and episodes should assure convergence, but this is not visible.
I need help where I am going wrong. 
PS: This little experiment is needed to make Q-learning applicable to a larger combinatorial problem. Unless I understand this, I don't think I will be able to do that right.
Also, there is no terminal state because this is an optimization problem. (And I have heard that Q-learning doesn't necessarily needs a terminal state)
","['reinforcement-learning', 'q-learning']","
If your intention is to learn make the agent learn which has the min arbitrary value, then you would need to modify your rewards a bit. 
The current reward structure provides the incentive to just move to a stage where it gets a reward. 
For example, if it is at state 0, it gets the same reward to go to either state 2 or state 3, since both of them have a higher inverse value.
To make the agent learn to move to state 2, you would have to provide it with more incentives to go to state 2.
def reward(s,a,s_dash):
    if s_dash == 2:
        return 5
    elif inverse_values_for_states[s]<inverse_values_for_states[s_dash]: 
        return 1
    elif inverse_values_for_states[s]>inverse_values_for_states[s_dash]:
        return -1
    else:
        return 0

I tried using this and it converges to 2. This is a hard-coded version, but I guess you get the idea.
"
State-of-the-art algorithms not working on a custom RL environment,"
I'm trying to train a RL agent on a custom, highly stochastic environment (MDP). In order to do so I'm using existing implementations of state-of-the-art RL algorithms as provided by Stable Baselines. However, no matter what algorithm I try out and despite extensive hyperparameter tuning, I'm failing to obtain any meaningful result. More precisely, a trivial ""always perform the same action (0.7,0.7) each time"" strategy works better than any of the obtained policies. The environment is highly stochastic (model of a financial market). How likely is it that the environment is simply ""too stochastic"" for any meaningful learning to take place? If interested, here's the environment code:
class environment1(gym.Env):
def __init__(self):
    self.t = 0.0 # initial time
    self.s = 100.0 # initial midprice value
    self.T = 1.0 # trading period length
    self.sigma = 2 # volatility constant
    self.dt = 0.005 # time step
    self.q = 0.0 # initial inventory
    self.oldq = 0 # initial old inventory
    self.x = 0 # initial wealth/cash
    self.gamma = 0.1 # risk aversion parameter
    self.k = 1.5 # intensity of arivals of orders
    self.A = 140 # constant
    self.done = False
    self.info = []
    high = np.array([np.finfo(np.float32).max,
                     np.finfo(np.float32).max,
                     np.finfo(np.float32).max],
                    dtype=np.float32)
    self.action_space = spaces.Discrete(100)
    self.observation_space = spaces.Box(-high, high, dtype=np.float32)
    self.seed()
    self.state = None

def seed(self, seed=None):
    self.np_random, seed = seeding.np_random(seed)
    return [seed]

def step(self, action):
    old_x, old_q, old_s = self.x, self.q, self.s # current becomes old
    self.t += 0.005 # time increment
    P1 = self.dt*self.A*np.exp(-self.k*(action//10)/10) # probabilities of execution
    P2 = self.dt*self.A*np.exp(-self.k*(action%10)/10)
    if random.random() < P1: # decrease inventory increase cash
        self.q -= 1
        self.x += self.s + (action//10)/10
    if random.random() < P2: # increase inventory decrease cash
        self.q += 1
        self.x -= self.s - (action%10)/10
    if random.random() < 0.5:
        self.s += np.sqrt(0.005)*self.sigma
    else:
        self.s -= np.sqrt(0.005)*self.sigma
    self.state = np.array([self.s-100,(self.q-34)/25,(self.t-0.5)/0.29011491975882037])
    reward = self.x+self.q*self.s-(self.oldx+self.oldq*self.olds)
    if np.isclose(self.t, self.T):
        self.done = True
    self.oldq = self.q
    self.oldx = self.x
    self.olds = self.s
    return self.state, reward, self.done, {}

def reset(self):
    self.t = 0.0 # initial time
    self.s = 100.0 # initial midprice value
    self.T = 1.0 # trading period length
    self.sigma = 2 # volatility constant
    self.dt = 0.005 # time step
    self.q = 0.0 # initial inventory
    self.oldq = 0.0
    self.oldx = 0.0
    self.olds = 100.0
    self.x = 0.0 # initial wealth/cash
    self.gamma = 0.1 # risk aversion parameter
    self.k = 1.5 # intensity of arivals of orders
    self.A = 140 # constant
    self.done = False
    self.info = []
    self.state = np.array([self.s-100,(self.q-34)/25,(self.t-0.5)/0.29011491975882037])
    return self.state

The state space is mostly normalized. The action space consists of 100 possible discrete actions (integers from 0 to 99 which are then transformed to (0.0,0.0),(0.0,0.1),...(1.0,1.0). The reward is simply given by the change in the portfolio value (cash+stock). 
Note: I've also tried transforming the action space into a continuous one in order to use DDPG but all to no avail.
","['reinforcement-learning', 'deep-rl', 'markov-decision-process', 'rewards', 'open-ai']",
Can SqueezeNet be used for regression?,"
I want a model that outputs the pixel coordinates of the tip of my forefinger, and whether it's touching something or not. Those would be 3 output neurons: 2 for the X-Y coordinates and 1, with a sigmoid activation, wich predicts the probability whether it's touching or not.
What do I need to change in the squeezenet model in order to do this?
(PS: the trained model needs to be the fastest possible (in latency), that's why I wanted to use SqueezeNet)
","['deep-learning', 'ai-design', 'activation-functions', 'regression']",
Combine two feature vectors for a correct input of a neural network,"
Let's consider this scenario. I have two conceptually different video datasets, for example a dataset A composed of videos about cats and a dataset B composed of videos about houses. Now, I'm able to extract a feature vectors from both the samples of the datasets A and B, and I know that, each sample in the dataset A is related to one and only one sample in the dataset B and they belong to a specific class (there are only 2 classes).
For example:
Sample x1 AND sample y1 ---> Class 1
Sample x2 AND sample y2 ---> Class 2
Sample x3 AND sample y3 ---> Class 1
and so on...

If I extract the feature vectors from samples in both datasets , which is the best way to combine them in order to give a correct input to the classifier (for example a neural network) ?
feature vector v1 extracted from x1 + feature vector v1' extracted from y1 ---> input for classifier
I ask this because I suspect that neural networks only take one vector as input, while I have to combine two vectors
","['neural-networks', 'deep-learning', 'classification']","
$^*$Note - Question is bit unclear, in case the answer doesn't addresses the question, please ask for edit/delete Request.

GENERALIZATION
Suppose there are multiple datasets denoted by $A_i$. Datasets contain a set of Vectors $x_{j} $. Mathematically $A_i = \{ x_j\}_{j=0}^n$. We've to find an estimator function $\hat f$, such that $\hat f( \vec r) = y, \, \vec r \in X$ where $X $ is a special dataset created by combining all $A_i$ which helps in classification into $y \in Y$ which is the set of classes.
.
As @Amir Mentioned out, linearly separable feature can be easily separated by straight combination of vectors i.e. if $x_u \in A_i, w_v \in A_j \dots$, then $r = [x_1 \,x_2 \, \dots  \, x_u \, w_1 \, \dots w_v \dots]$. Where, $r \in X$ which is the required dataset.
There are cases where the features are not linearly separable, We use basis expansion methods[1] to make required shape of hyperplane to separate the features. We create a new dataset combining $A_i \, \forall i \in C \subset \mathbb N$. Suppose that the new dataset is $X$, then $r \in X$ and $r = [r_0, r_1, \dots r_n].$
Then,
$$r_1 = u_1^2v_1^2 \\ r_2 = \sin(u_2)\sin(v_2) \\ r_3 = ae^{u_3 + v_3} \\ r_4 = a v_4 v_4 + a_2 u_4^2 v_4^2 + \dots \\ \dots$$
Here $u_p \in A_i; \, v_q \in A_j$
Here you can use all the creativity to set $r = [r_1, r_2, \dots , r_n]$ and make a new dataset. What equations and what functions you chose fully depends on the kind of hyperplane shape you want to obtain. Basis expansion is just one of the methods for feature extraction is certainly one of the most flexible too. 
Now, you feed the newly created vectors into your trained estimator functions (which is Neural Net) which can classify things much easily now.
In case of Regression/Classification without Neural Net needs some extra treatment to train the model[2].

[2]Note: There is also a big role of encoding. For example, if you encode colors by numbers $1, 2, 3$ for RGB or $10,01, 11$ fully changes everything and your features too. In such cases, You may even need different equations to make your required dataset $X$ and vectors $r$.

REFERENCES:

Oleszak, Michal. https://towardsdatascience.com. Non-linear regression: basis expansion, polynomials & splines. Sep 30, 2019. Web. 6 May 2020.
Sangarshanan. https://medium.com. Improve your classification models using Mean /Target Encoding. Jun 23, 2018. Web. 6 May 2020.

"
"When labelled data is not available, what are some common unsupervised learning algorithms for pattern recognition that can be used?","
In pattern recognition systems, when no labeled data is available, what are some common unsupervised learning algorithms for pattern recognition, that can be used?
","['algorithm', 'reference-request', 'unsupervised-learning', 'pattern-recognition']",
Why do we update $W$ with $\frac{1}{\mu (A_t | S_t)}$ instead of $\frac{\pi (A_t | S_t)}{\mu (A_t | S_t)}$ in off-policy Monte Carlo control?,"
I had the same question when I am reading the RL textbook from Sutton Bartol as posted here.

Why do we update $W$ with $\frac{1}{\mu (A_t | S_t)}$ instead of $\frac{\pi (A_t | S_t)}{\mu (A_t | S_t)}$? 
It seems that, with the updating rule from the textbook, whatever action $\mu$ decides to choose, we automatically assume that $\pi$ will choose it with 100% probability. But $\pi$ is greedy with respect to Q. How does this assumption make sense?   
","['reinforcement-learning', 'monte-carlo-methods']",
OpenAI Gym: Multiple actions in one step,"
I'm trying to design an OpenAI Gym environment in which multiple users/players perform actions over time. It's round based and each user needs to take an action before the round is evaluated and the next round starts. The action for one user can be model as a gym.spaces.Discrete(5) space.
I want my RL agent to make decisions for all users. I'm wondering how to take multiple actions before progressing time and calculating the reward.
Basically, what I want is:
obs = env.reset()
user_actions = []
for each user:
    user_actions.append(agent.predict(obs))
obs, reward, done, _ = env.step(user_actions)

So the problem is that I don't immediately know the reward after getting an action since I need to collect all actions before evaluating the round.
I could of course extend actions to include actions of all users in one go. But this would be problematic if I have a really large number of users or it even changes over time, right?
I found these two (1, 2) related questions, but they didn't solve my problem.
","['reinforcement-learning', 'open-ai', 'environment', 'gym']",
How to evaluate the performance of an autoencoder trained on image data?,"
I am training an autoencoder on (general) image data. 
I use binary crossentropy loss function, but it is not very informative when I want to evaluate the performance of my autoencoder. 
An obvious performance metric would be pixel-wise MSE, but it has its own downsides, shown on some toy examples in an image from paper from Pihlgren et al.

In the same paper, the authors suggest using perceptual loss, but it seems complicated and not well-studied. 
I found some other instances of this question, but there doesn't seem to be a concensus.
I understand that it depends on the application, but I want to know if there are some general guidelines as to which performance metric to use when training autoencoders on image data.
","['autoencoders', 'image-processing']","
I will answer my own question to try and provide some insights.
My research supervisor suggested that I should use the SSIM metric or some other well-known image processing metric (see the book ""Modern Image Quality Assessment"" by Wang and Bovik) for assessing the visual similarity of an images.
Another way I evaluate the performance of an autoencoder is by simply visually comparing the input and output images taken from the test set. This is by no means very scientific, but it gives a good idea whether an autoencoder is able to reconstruct the input images. One thing I would add here is that even if an autoencoder can reconstruct images perfectly, it doesn't mean that the encoding it learned is useful. For example, when I wanted similar images to be mapped to similar encodings, the autoencoder that was able to do that better was outputting more blurred reconstructed images in comparison to the autoencoder that wasn't achieving this similarity preservation (but was outputting better reconstructions).
"
How to select good inputs and fitness function to achive good results with NEAT for Icy Tower bot,"
I'm trying to make a bot to the famous ""Icy Tower"" game.
I rebuilt the game using pygame and I'm trying to build the bot using Python-NEAT.
Every generation a population of 70 characters tries to jump to the next platform and increase their fitness. right now the fitness is the number of platforms they jumped on, each platform gives +10.

The problem I'm facing is that the bot isn't learning good enough after 1000 generations the best score was around 200 (it can get to 200 even at the first few generations by mistake. 200 means 20 platforms which is not a lot).
when I look at the characters jumping it looks like they just always jump and go left or right and not deliberately aiming to the next platform.
I tried several input configurations to make the bot perform better. but nothing really helped.
these are the inputs I tried to mess around with:

pos.x, pos.y
velocity.x, velocity.y
isOnPlatform (bool)
[plat.x, plat.y, plat.width] (list of the 3-7 next platforms locations, also tried distance from character in x,y)
[prev.x, prev.y] (2-6 previous character positions)

I'm not so proficient with neuroevolution and I'm probably doing something wrong. glad if you could explain what's causing the bot to be so bad or what's not helping him to learn properly.
Although I think that the fitness function and the inputs should be the only problem I'm attaching the python-NEAT config file.
[NEAT]
fitness_criterion     = max
fitness_threshold     = 10000
pop_size              = 70
reset_on_extinction   = False

[DefaultGenome]
# node activation options
activation_default      = tanh
activation_mutate_rate  = 0.0
activation_options      = tanh

# node aggregation options
aggregation_default     = sum
aggregation_mutate_rate = 0.0
aggregation_options     = sum

# node bias options
bias_init_mean          = 0.0
bias_init_stdev         = 1.0
bias_max_value          = 30.0
bias_min_value          = -30.0
bias_mutate_power       = 0.5
bias_mutate_rate        = 0.7
bias_replace_rate       = 0.1

# genome compatibility options
compatibility_disjoint_coefficient = 1.0
compatibility_weight_coefficient   = 0.5

# connection add/remove rates
conn_add_prob           = 0.5
conn_delete_prob        = 0.5

# connection enable options
enabled_default         = True
enabled_mutate_rate     = 0.01

feed_forward            = True
initial_connection      = full

# node add/remove rates
node_add_prob           = 0.2
node_delete_prob        = 0.2

# network parameters
num_hidden              = 6
num_inputs              = 11
num_outputs             = 3

# node response options
response_init_mean      = 1.0
response_init_stdev     = 0.0
response_max_value      = 30.0
response_min_value      = -30.0
response_mutate_power   = 0.0
response_mutate_rate    = 0.0
response_replace_rate   = 0.0

# connection weight options
weight_init_mean        = 0.0
weight_init_stdev       = 1.0
weight_max_value        = 30
weight_min_value        = -30
weight_mutate_power     = 0.5
weight_mutate_rate      = 0.8
weight_replace_rate     = 0.1

[DefaultSpeciesSet]
compatibility_threshold = 3.0

[DefaultStagnation]
species_fitness_func = max
max_stagnation       = 3
species_elitism      = 2

[DefaultReproduction]
elitism            = 3
survival_threshold = 0.2


Note: the previous character positions are the position in the previous frame, and if the game runs at 60 fps the previous position is not that different from the current one...

Note2: the game score is a bit more complex than just jumping on platforms, the bot should also be rewarded for combos that can make him jump higher. the combo system is already implemented but I first want to see the bot aiming to the next platform before he learns to jump combo.


","['neural-networks', 'python', 'evolutionary-algorithms', 'neat', 'neuroevolution']",
Can a typical supervised learning problem be solved with reinforcement learning methods?,"
Let's say I want to teach a neural to classify images, and, for some reason, I insist on using reinforcement learning rather than supervised learning. 
I have a dataset of images and their matching classes. Then, for each image, I could define a reward function which is $1$ for classifying it right and $-1$ for classifying it wrong (or perhaps even define a more complicated reward function where some mistakes are less costly than others).  For each image $x^i$, I can loop through each class $c$ and use a vanilla REINFORCE step: $\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(c|x^i)r$. 
Would that be different than using standard supervised learning methods (for example, the cross-entropy loss)? Should I expect different results? 
This method actually seems better since I could define a custom reward for each misclassification, but I've never seen anyone use something like that
","['deep-learning', 'reinforcement-learning', 'supervised-learning', 'reinforce']",
What does a self-improving artificial general intelligence with finite resources and infinite time do?,"
What would happen when an artificial general intelligence can improve itself over a long time, with limited resources?
The assumption is that it has a large but finite amount of computing power, and can not escape that limit to find resources. Let's assume the limit to be on the order 100 human brains or so - the limits are not important, only that it is limited.
Now, we let it run to self-improve as much as it can with the given resources, until it goes to some stable or periodic state.
What it can do is at least one of the following:  

converge to some state
oscillate between two or more states
go into a chaotic state
stop doing anything
disable itself in some other way than just stop doing anything

There are certainly more complex behaviors possible:

become maximally happy, and then periodically redefine what happy means
hacking its reward function in some other way

I would expect it to converges to a single state - but that seems naive.
Are there any ideas on how it would end up?
","['philosophy', 'agi', 'superintelligence', 'singularity']",
Backward pass of CNN like Resnet: how to manually compute flops during backprop?,"
I've been trying to figure out how to compute the number of Flops in backward pass of ResNet. For forward pass, it seems straightforward: apply the conv filters to the input for each layer. But how does one do the Flops counts for gradient computation and update of all weights during the backward pass?
Specifically, 

how to compute Flops in gradient computations for each layer? 
what all gradients need to be computed so Flops for each of those can be counted? 
How many Flops in computation of gradient for Pool, BatchNorm, and Relu layers?

I understand the chain rule for gradient computation, but having a hard time formulating how it'd apply to weight filters in conv layers of ResNet and how many Flops each of those would take. It'd be very useful to get any comments about method to compute total Flops for Backward pass. Thanks
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'backpropagation', 'gradient-descent']",
Accuracy dropped when I ran the program the second time,"
I was following a tutorial about Feed-Forward Networks and wrote this code for a simple FFN :
class FirstFFNetwork:
  
  #intialize the parameters
  def __init__(self):
    self.w1 = np.random.randn()
    self.w2 = np.random.randn()
    self.w3 = np.random.randn()
    self.w4 = np.random.randn()
    self.w5 = np.random.randn()
    self.w6 = np.random.randn()
    self.b1 = 0
    self.b2 = 0
    self.b3 = 0
  
  def sigmoid(self, x):
    return 1.0/(1.0 + np.exp(-x))
  
  def forward_pass(self, x):
    #forward pass - preactivation and activation
    self.x1, self.x2 = x
    self.a1 = self.w1*self.x1 + self.w2*self.x2 + self.b1
    self.h1 = self.sigmoid(self.a1)
    self.a2 = self.w3*self.x1 + self.w4*self.x2 + self.b2
    self.h2 = self.sigmoid(self.a2)
    self.a3 = self.w5*self.h1 + self.w6*self.h2 + self.b3
    self.h3 = self.sigmoid(self.a3)
    return self.h3
  
  def grad(self, x, y):
    #back propagation
    self.forward_pass(x)
    
    self.dw5 = (self.h3-y) * self.h3*(1-self.h3) * self.h1
    self.dw6 = (self.h3-y) * self.h3*(1-self.h3) * self.h2
    self.db3 = (self.h3-y) * self.h3*(1-self.h3)
    
    self.dw1 = (self.h3-y) * self.h3*(1-self.h3) * self.w5 * self.h1*(1-self.h1) * self.x1
    self.dw2 = (self.h3-y) * self.h3*(1-self.h3) * self.w5 * self.h1*(1-self.h1) * self.x2
    self.db1 = (self.h3-y) * self.h3*(1-self.h3) * self.w5 * self.h1*(1-self.h1)
  
    self.dw3 = (self.h3-y) * self.h3*(1-self.h3) * self.w6 * self.h2*(1-self.h2) * self.x1
    self.dw4 = (self.h3-y) * self.h3*(1-self.h3) * self.w6 * self.h2*(1-self.h2) * self.x2
    self.db2 = (self.h3-y) * self.h3*(1-self.h3) * self.w6 * self.h2*(1-self.h2)
    
  
  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):
    
    # initialise w, b
    if initialise:
      self.w1 = np.random.randn()
      self.w2 = np.random.randn()
      self.w3 = np.random.randn()
      self.w4 = np.random.randn()
      self.w5 = np.random.randn()
      self.w6 = np.random.randn()
      self.b1 = 0
      self.b2 = 0
      self.b3 = 0
      
    if display_loss:
      loss = {}
    
    for i in tqdm_notebook(range(epochs), total=epochs, unit=""epoch""):
      dw1, dw2, dw3, dw4, dw5, dw6, db1, db2, db3 = [0]*9
      for x, y in zip(X, Y):
        self.grad(x, y)
        dw1 += self.dw1
        dw2 += self.dw2
        dw3 += self.dw3
        dw4 += self.dw4
        dw5 += self.dw5
        dw6 += self.dw6
        db1 += self.db1
        db2 += self.db2
        db3 += self.db3
        
      m = X.shape[1]
      self.w1 -= learning_rate * dw1 / m
      self.w2 -= learning_rate * dw2 / m
      self.w3 -= learning_rate * dw3 / m
      self.w4 -= learning_rate * dw4 / m
      self.w5 -= learning_rate * dw5 / m
      self.w6 -= learning_rate * dw6 / m
      self.b1 -= learning_rate * db1 / m
      self.b2 -= learning_rate * db2 / m
      self.b3 -= learning_rate * db3 / m
      
      if display_loss:
        Y_pred = self.predict(X)
        loss[i] = mean_squared_error(Y_pred, Y)
    
    if display_loss:
      plt.plot(loss.values())
      plt.xlabel('Epochs')
      plt.ylabel('Mean Squared Error')
      plt.show()
      
  def predict(self, X):
    #predicting the results on unseen data
    Y_pred = []
    for x in X:
      y_pred = self.forward_pass(x)
      Y_pred.append(y_pred)
    return np.array(Y_pred)

The data was generated as follows :
data, labels = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)
labels_orig = labels
labels = np.mod(labels_orig, 2)
X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)

When I ran the program yesterday, I had gotten a training accuracy of about 98% and a test accuracy of 94%. But when I ran it today, suddenly the accuracy dropped to 60-70%. I tried to scatter plot the result, and it looked like it behaved as if it were a single sigmoid instead of the Feed-Forward Network.
ffn = FirstFFNetwork()
#train the model on the data
ffn.fit(X_train, Y_train, epochs=2000, learning_rate=.01, display_loss=False)
#predictions
Y_pred_train = ffn.predict(X_train)
Y_pred_binarised_train = (Y_pred_train >= 0.5).astype(""int"").ravel()
Y_pred_val = ffn.predict(X_val)
Y_pred_binarised_val = (Y_pred_val >= 0.5).astype(""int"").ravel()
accuracy_train_1 = accuracy_score(Y_pred_binarised_train, Y_train)
accuracy_val_1 = accuracy_score(Y_pred_binarised_val, Y_val)
#model performance
print(""Training accuracy"", round(accuracy_train_1, 2))
print(""Validation accuracy"", round(accuracy_val_1, 2)

I do not understand how this happened and cannot figure it out.
","['neural-networks', 'deep-learning', 'feedforward-neural-networks', 'accuracy', 'sigmoid']","

It is common during the training of Neural Networks for accuracy to improve for a while and then get worse -- in general, This is caused by over-fitting. It's also fairly common for the Neural Network to ""get UNLUCKY and get knocked into a BAD sectors of parameter space corresponding to a sudden decrease in accuracy -- sometimes it can recover from this quickly, but sometimes not.



In general, lowering your learning rate is a good approach to this kind of problem. Also, setting a learning rate schedule like FactorScheduler can help you achieve more stable convergence by lowering the learning rate every few epochs. In fact, this can sometimes cover up mistakes in picking an initial learning rate that is too high.

you can try using mini-batches.

The error (Entropy) with log functions must be used precisely.




What are some possible reasons why training accuracy would decrease over time?

Check out in this paper a novel way to change the learning rate cyclically.


"
A question about the Wolpertinger algorithm (Deep RL in Large Discrete Action Spaces paper),"
I am trying to reproduce the recommender task experiment from this paper. 
The paper suggests to embed discrete actions into continuous action space and then to use the proposed Wolpertinger agent. 
The Wolpertinger agent is as follows:
DDPG produces so called proto action $f(s)$, then KNN finds k nearest embeddings of discrete actions to this proto action, and then we choose the one of these $k$ embeddings, which has the highest Q-function value. The whole is a full policy, $\pi(\cdot)$.
While training we optimize the critic using only the full policy (DDPG + a choice of a neighbour).
The actor is optimized using proto action output in order to be differentiable, $Q(s, f_{\theta}(s)) \rightarrow \max_{\theta}$.
The problem is that the critic does not know that it is used to optimize continuous output of the algorithm. It is trained only to value the embedded actions. As I understand, we hope that continuity of critic will help us with it, but what I have is that proto actions constantly appear to be in some corners with no real actions and where the Q-function unreasonably has greater values (because it is simply untrained in such domains). The DDPG output is normalized to match the embeddings bounds to make these empty spaces not so large. 
It seems for me that there is a way to make embeddings more appropriate for the task and achieve higher reward. However, when I use $k = |
\mathcal{A}|$, proto actions are not considered and algorithm works pretty well.
Usually I use $|\mathcal{A}| = 100$ and $k = 10$.
I have trained them with skip-gram, based on the users history.
Below are 2d projections of my embeddings to the first 10 axes (embeddings are from $\mathbb{R}^{20}$). And undependently of the state, proto actions are about the same. The blue is a proto action for the some fixed state.
Having some state fixed, $Q(s, f(s))$ value is always higher than $Q(s, a)$ for any $a \in \mathcal{A}$.

Would be glad to get any help, especially the help of people familiar with this algorithm.
Do I need to make embeddings fill proto actions range (some hyperrectangle in case we have the tanh activation it the actor)? What is the way to fill such a domain with embeddings?
","['reinforcement-learning', 'ddpg']",
PPO algorithm converges on only one action,"
I have taken some reference implementations of PPO algorithm and am trying to create an agent which can play space invaders . Unfortunately from the 2nd trial onwards (after training the actor and critic N Networks for the first time) , the probability distribution of the actions converges on only action and the PPO loss and the critic loss converges on only one value.
Wanted to understand the probable reasons why this might occur . I really cant run the code in my cloud VMs without being sure that I am not missing anything as the VMs are very costly to use . I would appreciate any help or advice in this regarding .. if required I can post the code as well . Hyperparameters used are as follows :
clipping_val = 0.2 critic_discount = 0.5 entropy_beta = 0.01 gamma = 0.99 lambda = 0.95
code repo : github.com/superchiku/ReinforcementLearning . 
","['reinforcement-learning', 'policy-gradients', 'proximal-policy-optimization']",
Can operations like convolution and pooling be discovered with a neural architecture search approach?,"
From Neural Architecture Search: A Survey, first published in 2018:

Moreover, common search spaces are also based on predefined building
  blocks, such as different kinds of convolutions and pooling, but do
  not allow identifying novel building blocks on this level; going
  beyond this limitation might substantially increase the power of NAS.

Has anyone tried that? If not, do you have any thoughts about the feasibility of this idea?
","['neural-networks', 'machine-learning', 'neuroevolution', 'neural-architecture-search']",
Is there a good book or paper on word embeddings?,"
Is there a good and modern book that focuses on word embeddings and their applications? It would also be ok to provide the name of a paper that provides a good overview of word embeddings.
","['natural-language-processing', 'reference-request', 'word-embedding']",
What are some resources with exercises related to neural networks?,"
I am asking for a book (or any other online resource) where we can solve exercises related to neural networks, similar to the books or online resources dedicated to mathematics where we can solve mathematical exercises.
","['neural-networks', 'deep-learning', 'reference-request']","
There are actually quite a few. Personally I would say these courses have high quality and strong  focus on practice:

Standford computer vision cs231. Check the assignments materials on this page. This course has good explanation/exercises of how generally neural nets and backprop works.
Fastai course notebooks. You can listen to the lectures as well, but notebooks are quite self-containing
Practical reinforcement learning course, if you interested in NN application for RL

"
What is the difference between training and testing in reinforcement learning?,"
In reinforcement learning (RL), what is the difference between training and testing an algorithm/agent? If I understood correctly, testing is also referred to as evaluation.
As I see it, both imply the same procedure: select an action, apply to the environment, get a reward, and next state, and so on. But I've seen that, e.g., the Tensorforce RL framework allows running with or without evaluation.
","['reinforcement-learning', 'training', 'comparison', 'testing']","
Reinforcement Learning Workflow
The general workflow for using and applying reinforcement learning to solve a task is the following.


Create the Environment
Define the Reward
Create the Agent
Train and Validate the Agent
Deploy the Policy


Training

Training in Reinforcement learning employs a system of rewards and penalties to compel the computer to solve a problem by itself.
Human involvement is limited to changing the environment and tweaking the system of rewards and penalties.
As the computer maximizes the reward, it is prone to seeking unexpected ways of doing it. 
Human involvement is focused on preventing it from exploiting the system and motivating the machine to perform the task in the way expected.
Reinforcement learning is useful when there is no “proper way” to perform a task, yet there are rules the model has to follow to perform its duties correctly.
Example: By tweaking and seeking the optimal policy for deep reinforcement learning, we built an agent that in just 20 minutes reached a superhuman level in playing Atari games.
Similar algorithms, in principle, can be used to build AI for an autonomous car.


Testing

Debugging RL algorithms is very hard. Everything runs and you are not sure where the problem is.
To test if it worked well, if the trained agent is good at what it was trained for, you take your trained model and apply it to the situation it is trained for.
If it’s something like chess or Go, you could benchmark it against other engines (say stockfish for chess) or human players.
You can also define metrics for performance, ways of measuring the quality of the agent’s decisions. 
In some settings (e.g a Reinforcement Learning Pacman player), the game score literally defines the target outcome, so you can just evaluate your model’s performance based on that metric.

"
What are the common pitfalls that we could face when training neural networks?,"
Apart from the vanishing or exploding gradient problems, what are other problems or pitfalls that we could face when training neural networks?
","['neural-networks', 'vanishing-gradient-problem', 'exploding-gradient-problem']","
There are several pitfalls or issues that require your attention when or before training or using neural networks. I will list some of them below, along with some questions you need to ask yourself before or while using neural networks.

Over-fitting and under-fitting problems, and the related generalization problem. Is your neural network generalizing to unseen data?
Availability of training and test data

Do you have enough data to train your neural network so that it generalizes well (i.e. it neither over-fits or under-fits)?
Is your test dataset big enough to assess the generalization ability of your neural network?
Is your data representative of the problem you are trying to solve?
Do you need to augment or normalize your data?
Do you need to use cross-validation?
Is your data independent and identically distributed (i.i.d.)? If your data is correlated, training could be unstable. Shuffling your data may be a feasible solution when your data is initially correlated.

Do you have enough computational resources (i.e. GPUs) for training and testing your neural network?
Are you solving a regression or classification problem? The type of the outputs and the loss function will typically be different in both cases
Do you need explainability and transparency? If yes, neural networks aren't probably the best model to use, as the connections between the neurons are quite obscure and don't really represent any meaningful interaction. That's why neural networks are called black-boxes.
Do you need uncertainty estimation? If yes, you may want to try Bayesian neural networks. Typical neural networks are not very appropriate for uncertainty estimation!
If you use a neural network for function approximation (e.g. in reinforcement learning), you will lose certain convergence guarantees.

"
What happens if the opponent doesn't play optimally in minimax?,"
I just read an article about the minimax algorithm. When you design the algorithm, you assume that your opponent is a perfect player, i.e. it plays optimally. 
Let's consider the game of chess. What happens if the opponent plays irrationally or sub-optimally? Do you still have a guarantee that you are going to win?
","['reinforcement-learning', 'minimax']","

What happens if the opponent plays irrationally or sub-optimally? Do you still have a guarantee that you are going to win?

If your search is deep enough to guarantee optimal play in all cases, then yes. Optimal play is such that the opponent's decision causes least impact to your agent. In fact, if the opponent makes a mistake, often that can make the search easier/faster, and the agent will win more convincingly.
What it may mean is that optimisations you may have taken - e.g. pruning game tree segments that lead to non-optimal decisions by either player - might not be as useful. This might impact decision time if you keep some partial game tree or cache branch evaluations between moves to help speed up the agent.
Actual optimal agents for games as complex as chess are not possible. In these games, you will not have a truly optimal agent, but approximately optimal. You will be relying on some heuristic to guide the minimax search when it cannot force an end game win. If the opponent manages to control play into a state where the heuristics are not accurate, they could cause minimax search to fail and misdirect your agent into making mistakes.
A combination of effects is also possible if you have implemented a tree caching mechanism for performance improvements and made the system playable by limiting planning time - e.g. you limit computer search time to 3 seconds max - an irrational opponent may cause your agent's performance to degrade to the point where it too starts to make mistakes. Whether or not this is enough for a smart opponent to take advantage of it and beat an agent which is capable of otherwise playing a ""perfect"" game depends on details of the game, and how narrow the agent's measure of ""perfect"" is. 
An extreme case might be an agent that has memorised a single perfect game (by scoring high heuristics for any state on a single tarjectory through the game that has been pre-calculated for perfect moves by both players), and has poor heuristics otherwise - once state moves away from what it can evaluate directly, the agent will be limited by how well it can search for completed game wins, and can easily be manipulated by a more generally smart opponent into a losing position that is beyond the depth of its search.
In practice, well-coded agents will not suffer too much from this effect. If you make a mistake, or try a random probably bad move in an attempt to confuse the agent when playing against Stockfish, you will lose.
"
solving xor function using a neural network with no hidden layers,"
xor is a non-linear dataset. It cannot be solved with any number of perceptron based neural network but when the perceptions are applied the sigmoid activation function, we can solve the xor dataset. 
But I came across a source where the following statement is stated as False 
A two layer (one input layer; one output layer; no hidden layer) neural network can represent the XOR function.
However, I have trained a model with no hidden layers, gives the following result:
[INFO] data=[0 0],ground-truth=0, pred=0.5161, step=1
[INFO] data=[0 1],ground-truth=1, pred=0.5000, step=1
[INFO] data=[1 0],ground-truth=1, pred=0.4839, step=0
[INFO] data=[1 1],ground-truth=0, pred=0.4678, step=0

So, in if I apply a softmax classifier, I can separate the xor dataset with a nn without any hidden layer. This makes the statement incorrect. 
Is it true that we cannot separate a non linear dataset without any hidden layers in a neural network? If yes, where am I wrong in my reasoning from the training of the nn I have done above
","['neural-networks', 'non-linear-regression']",
Doubt regarding the proof of convergence of $\epsilon$ soft policies without exploring starts,"
In page 125 of Sutton and Barto (second last paragraph) the proof for equality of $v_{\pi}$ and $v_*$ for $\epsilon$ soft policies is given. But I could not understand the statement explaining the proof:

Consider a new environment that is just like the original environment, except with the requirement that policies be $\epsilon$-soft “moved inside” the environment. The new environment has the same action and state set as the original
and behaves as follows. If in state $s$ and taking action $a$, then with probability
$1 - \epsilon$ the new environment behaves exactly like the old environment. With
probability $\epsilon$ it repicks the action at random, with equal probabilities, and
then behaves like the old environment with the new, random action. The best
one can do in this new environment with general policies is the same as the
best one could do in the original environment with $\epsilon$-soft policies.

What is the meaning of environment here? And what is this new thing/argument (provided above) the authors are describing to arrive at the proof?
","['reinforcement-learning', 'sutton-barto']",
How can reinforcement learning be unsupervised learning if it uses deep learning?,"
I was watching a video in my online course where I'm learning about A.I. I am a very beginner in it. 
At one point in the course, the instructor says that reinforcement learning (RL) needs a deep learning model (NN) to perform an action. But for that, we need expected results in our model for the NN to learn how to predict the Q-values. 
Nevertheless, at the beginning of the course, they said to me that RL is an unsupervised learning approach because the agent performs the action, receives the response from the environment, and finally takes the more likely action, that is, with the highest Q value. 
But if I'm using deep learning in RL, for me, RL looks like a supervised learning approach. I'm a little confused about these things, could someone give me clarifications about them?

","['deep-learning', 'reinforcement-learning', 'terminology', 'unsupervised-learning', 'supervised-learning']","
In Supervised learning, the goal is to learn a mapping from points in a feature space to labels. So that for any new input data point, we are able to predict its label. whereas in Unsupervised learning  data set is composed only of points in a feature space, i.e. there are no labels & here the goal is to learn some inner structure or organization in the feature space itself.
Reinforcement Learning is basically concerned with learning a policy in a sequential decision problem. There are some components in RL that are “unsupervised” and some that are “supervised”, but it is not a combination of “unsupervised learning” and “supervised learning”, since those are terms used for very particular settings, and typically not used at all for sequential decision problems.


In Reinforcement Learning, we have something called as Reward function that the agent aims to maximize. During the learning process, one typical intermediate step is to learn to predict the reward obtained for a specific policy.
In a nutshell, we can say Reinforcement Learning put a model in an environment where it learns everything on its own from data collection to model evaluation. It is about taking suitable action to maximize reward in a particular situation. There is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of training dataset, it is bound to learn from its experience.
To better understand let’s look at an analogy.
Suppose you have a dog that is not so well trained, every time the dog messes up the living room you reduce the amount of tasty foods you give it (punishment) and every time it behaves well you double the tasty snacks (reward). What will the dog eventually learn? Well, that messing up the living room is bad.
This simple concept is powerful. The dog is the agent, the living room the environment, you are the source of the reward signal (tasty snacks).

To learn more on Reinforcement learning, pls check this awesome Reinforcement Learning lecture that is freely available on youtube by someone who actually leads the Reinforcement learning research group at DeepMind and also a lead researcher on AlphaGo, AlphaZero.
[RL Course by David Silver]
https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB""
"
How do you know if an agent has learnt its environment in reinforcement learning?,"
I'm new to reinforcement learning and trying to understand it. 
If you train an agent using a reinforcement learning algorithm (discrete or continuous) on an environment (real or simulated), then how do you know if the agent has learnt its environment? Should it reach its goal on every run (episode)? (Any literature references are also welcome)
Is this related to the reward threshold defined in the environment?
What happens if you continue training after the agent has learnt the environment? Will it perform by reaching its goal every time or will there be failed episodes?
","['reinforcement-learning', 'rewards', 'intelligent-agent', 'learning-algorithms', 'environment']",
"Shouldn't the utility function of two-player zero-sum games be in the range $[-1, 1]$?","
In Appendix B of MuZero, they say

In two-player zero-sum games the value functions are assumed to be bounded within the $[0, 1]$ interval.

I'm confused about the boundary: Shouldn't the value/utility function be in the range of [-1,1] for two-player zero-sum games?
","['reinforcement-learning', 'game-theory', 'multi-agent-systems']",
What should the target be when the neural network outputs multiple Q values in deep Q-learning?,"
I have some gaps in my understanding regarding the performing of the gradient descent in Deep - Q networks. The original deep q network for Atari performs a gradient descent step to minimise $y_j - Q(s_j,a_j,\theta)$, where $y_j = r_j + \gamma max_aQ(s',a',\theta)$. 
In the example where I sample a single experience $(s_1,a_2,r_1,s_2)$ and I try to conduct a single gradient descent step, then feeding in $s_1$ to the neural network outputs an array of $Q(s_1,a_0), Q(s_1,a_1), Q(s_1,a_2), \dots$ values. 
When doing gradient descent update for this single example, should the target output to set for the network be equivalent to $Q(s_1,a_0), Q(s_1,a_1), r_1 + \gamma max_{a'}Q(s_2,a',\theta), Q(s_1,a_3), \dots$ ?
I know the inputs to the neural network to be $s_j$, to give the corresponding Q values. However, I cannot concretize the target values that the network should be optimized. 
","['neural-networks', 'reinforcement-learning', 'dqn', 'deep-rl']","
You are looking for the best actions which minimize the loss function. You sample a batch of memory buffer uniformly and define a loss function based on that batch. The memory buffer consists of trajectories. Each trajectory consists of an state and the action taken in that state which results in next state and an immediate reward. 
If the trajectory is shown by $(s,a,r,s\prime)$, the loss for this single state is simply defined as: $(r + max_a\prime Q(s\prime,a\prime,w^-)-Q(s,a,w))^2$. 
The minus sign above the parameters means that you should fix the the target parameters to ensure the stability of learning. 
So the loss function for a whole batch is:
$L(w) = E_{(s,a,r,s\prime)\sim U(D)}(r + max_a\prime Q(s\prime,a\prime,w^-)-Q(s,a,w))^2$.
"
Is there any application of topology to deep learning?,"
Is there any application of topology (as in math discipline) to deep learning? If so, what are some examples?
","['deep-learning', 'reference-request', 'topology']",
Does Q Learning learn from an opponent playing random moves?,"
I've created a Q Learning algorithm to play Connect Four against an opponent who just chooses a random free column. My Q Agent is currently only winning about 0.49 games on average (30,000 episodes). Will my Q Agent actually learn from these episodes, seeing as its opponent isn't 'trying' to beat it, as there's no strategy behind its random choices? Or should this not matter – if the Q Agent is playing enough games, it doesn't matter how good/bad its opponent is?
","['reinforcement-learning', 'training', 'q-learning']","
It should be possible to train an agent using some variant of DQN to beat a random agent around 100% of the time within a few thousand games. 
It may require one or two more advanced techniques to get the learning time down to a low number of thousands. However, if your agent is winning ~50% of games against a random agent, something has gone wrong, since that is the performance you would expect of another random agent. Even simple policies, such as always play in same column, will beat a random agent a significant fraction of the time.
First thing to consider is that there are too many states in Connect 4 to use tabular Q learning. You have to use some variant of DQN. As a grid-based board game where winning patterns can repeat, some form of convolutional neural network (CNN) for the Q function is probably a good start.
I think for a first step, you should double-check that you have implemented DQN correctly. Check the TD target formula is correct, and that you have implemented experience replay. Ideally you will also have a delayed-update target network for calculating the TD targets.
As a second step, try some variations of hyper-parameters. The learning rate, exploration rate, size of replay table, number of games to play before starting learning etc. A discount factor $\gamma$ slightly below 1 can help (despite this being an episodic problem) - it makes the agent forget more of the initial bias for early time steps.

Or should this not matter – if the Q Agent is playing enough games, it doesn't matter how good/bad its opponent is?

Up to a point this is true. It is hard to learn against a perfect agent in Connect 4, because it always wins as player one, which means all policies are equally good and there is nothing to learn. Other than that, if there is a way to win, eventually a Q learning agent with exploration should find it. 
Against a random agent, you should be seeing some improvement if your agent is correctly set up for the problem, after a few thousand games. As it happens I am currently training Connect 4 agents using variants of DQN for a Kaggle competition, and they consistently beat random agents with 100% measured success rate after 10,000 training games. I have added a few extras to my agents in order to achieve this - there are some discussions of approaches in the forums at https://www.kaggle.com/c/connectx
"
What is the advantage of using more than one environment with the advantage actor-critic?,"
 make_env = lambda: ptan.common.wrappers.wrap_dqn(gym.make(""PongNoFrameskip-v4""))
 envs = [make_env() for _ in range(NUM_ENVS)]

Here is a code you can look at.
The two above lines of code create multiple environments for the game of Atari Pong with the A2C algorithm. 
I understand why it is very useful to have multiple agents working on different instances of the same environment as it is presented in A3C (i.e. an asynchronous version of A2C). However, in the above code, it has a single agent working on different instances of the same environment.
What is the advantage of using more than one environment with a single agent?
UPDATE
class GymEnvVec:
    def __init__(self, name, n_envs, seed):
        self.envs = [gym.make(name) for i in range(n_envs)]
        [env.seed(seed + 10 * i) for i, env in enumerate(self.envs)]

    def reset(self):
        return [env.reset() for env in self.envs]

    def step(self, actions):
        return list(zip(*[env.step(a) for env, a in zip(self.envs, actions)]))

","['reinforcement-learning', 'actor-critic-methods', 'environment', 'advantage-actor-critic']",
Understanding how to calculate $P(x|c_k)$ for the Bernoulli naïve Bayes classifier,"
I'm looking at the Bernoulli naïve Bayes classifier on Wikipedia and I understand Bayes theorem along with Gaussian naïve Bayes. However, when looking at how $P(x|c_k)$ is calculated, I don't understand it. The Wikipedia page says its calculated as follows
$$P(x|c_k) = \prod^{n}_{i=1} p^{x_i}_{ki} (1-p_{ki})^{(1-x_i)}. $$
They mention that $p_{ki}$ is the probability of class $c_k$ generating the term $x_i$, does that mean $P(x|c_k)$? Because if so then that doesn't make sense since to calculate that we need to have calculated it already. So what is $p_{ki}$?
And in the first part, after the product symbol, are they raising this probability to the power pf $x_i$ or does that again just mean 'probability of class $c_k$ generating the term $x_i$'?
I also don't understand the intuition behind why or how this calculates $P(x|c_i)$.
","['naive-bayes', 'probability-theory', 'bayes-theorem', 'bayesian-probability']",
What is meant by the expected BLEU cost when training with BLEU and SIMILE?,"
Recently I was reading a paper based on a new evaluation metric SIMILE. In a section, validation loss comparison had been made for SIMILE and BLEU. The plot showed the expected BLEU cost when training with BLEU and SIMILE. 
What I'm unable to understand is what is meant by the expected BLEU cost when training with BLEU and SIMILE? Are there any separate cost functions defined for these scores?
I'm attaching the image of the graph.

","['natural-language-processing', 'training', 'metric', 'expectation']","
It looks like the method they use for training takes a set of candidate hypotheses $\mathcal{U}(x)$, along with associated probabilities, and then minimizes the expected value of the cost function over that distribution. Section 3 has the loss function being minimized: 
$ \mathcal{L}_{Risk} = \sum\limits_{u \in \mathcal{U}(x)} cost(t, u) \frac{p(u|x)}{\sum_{u' \in \mathcal{U}(x)} p(u'|x)} $. 
One of the cost functions used is $1 - \texttt{BLEU}(t, h)$, where $t$ is the target and $h$ is the generated hypothesis. I'm not sure where $p(u|x)$ is coming from, but $1 - \mathcal{L}_{Risk}$ for the BLEU cost function is probably what they're refering to when they mention Expected BLEU. 
"
MNIST Classification code performing with 88%-90% whereas other codes online perform 95% on first epoch,"
I have been trying to write code to implement plain neural net without convolution from scratch. I took some help online here and added my code to my github account.
I don't understand why the prediction made by my code is only 88%-90% accurate after the 1st epoch, whereas his code is 95% accurate after 1st epoch with the same parameters (Same Xavier initialization for weights, biases are not initialized, same hidden layer neurons). While his architecture uses 2 hidden layers, my code performed worse with 2 hidden layers. For 1 hidden layer, his code performs similar (~96%). 
","['neural-networks', 'machine-learning', 'image-recognition']",
Shouldn't expected return be calculated for some faraway time in the future $t+n$ instead of current time $t$?,"
I am learning RL for the first time. It may be naive, but it is a bit odd to grasp this idea that, if the goal of RL is to maximize the expected return, then shouldn't the expected return be calculated for some faraway time in the future ($t+n$) instead of current time $t$? It is because we are building our system for the future using current information. ( I am coming from machine learning background and this makes more sense to me).
Generally, the expected return is:
$$\mathbb{E}[G_t] = \mathbb{E}[ R_{t+1} + R_{t+2} + R_{t+3}+... R_{t+n}]$$
However, shouldn't the expected return be:
$$\mathbb{E}[G_{t+n}] = \mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3}+... R_{t+n-1}]$$
","['reinforcement-learning', 'rewards', 'expectation', 'return']",
Applying Eligibility Traces to Q-Learning algorithm does not improve results (And might not function well),"
I am trying to apply Eligibility Traces to a currently working Q-Learning algorithm.
The reference code for the Q-Learning algorithm was taken from this great blog by DeepLizard, but does not include Eligibility Traces. Link to the code on Google Colab.
I wish to add the Eligibility Traces by implementing this pseud code:
Initialize Q(s,a) arbitrarily and e(s,a) = 0, for all s,a
Repeat (for each episode):
    Initialize s,a
    Repeat (for each step of episode):
        Take action a, observe r,s’
        Choose a’ from s’ using policy derived from Q (e.g., ϵ-greedy)
        δ ← r + γ Q(s’,a’) – Q(s,a)
        e(s,a) ← e(s,a) + 1
        For all s,a:
            Q(s,a) ← Q(s,a) + α δ e(s,a)
            e(s,a) ← γ λ e(s,a)
        s ← s’ ; a ← a’
    until s is terminal

Taken from HERE
This is my code as I have implemented the pseudo-code - Link
The part that needs to be improved is here:
#Q learning algorithem
for episode in range(num_episodes):
  state = env.reset()
  et_table = np.zeros((state_space_size,action_space_size))
  done = False
  reward_current_episode = 0

  for steps in range(max_steps_per_episode):
    #Exploration-Explotation trade-off
    exploration_rate_thresh = random.uniform(0,1)
    if exploration_rate_thresh > exploration_rate:
      action = np.argmax(q_table[state,:])
    else:
      action = env.action_space.sample()

    new_state, reward, done, info = env.step(action)

    #Update Q-table and Eligibility table
    delta = reward + discount_rate * np.max(q_table[new_state,:]) - q_table[state,action]
    et_table[state, action] = et_table[state, action] + 1

    for update_state in range(state_space_size):
      for update_action in range(action_space_size):
        q_table[update_state, update_action] = q_table[update_state, update_action] + learning_rate * delta * et_table[update_state, update_action]
        et_table[update_state, update_action] = discount_rate * gamma * et_table[update_state, update_action]

    state = new_state
    reward_current_episode = reward

    if done==True:
      break

  #Exploration rate decay
  exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)

  rewards_all_episodes.append(reward_current_episode)

For a while, I was getting pure results (avg. rewards for 1000 episodes were around 0.14 while the original NON-ET algorithm was averaging 0.69 on the last 1000 episodes), but now I get these errors:
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in double_scalars
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in double_scalars

","['reinforcement-learning', 'q-learning', 'unsupervised-learning', 'eligibility-traces']",
Which reward function works for recommendation systems using knowledge graphs?,"
I've been reading this paper on recommendation systems using reinforcement learning (RL) and knowledge graphs (KGs).
To give some background, the graph has several (finitely many) entities, of which some are user entities and others are item entities. The goal is to recommend items to users, i.e. to find a recommendation set of items for every user such that the user and the corresponding items are connected by one reasoning path.
I'm attaching an example of such a graph for more clarity (from the paper itself) -

In the paper above, they say

First, we do not have pre-defined targeted items for any user, so it is not applicable to use a binary reward indicating whether the user interacts with the item or not. A better design of the reward function is to incorporate the uncertainty of how an item is relevant to a user based on the rich heterogeneous information given by the knowledge graph.

I'm not able to understand the above extract, which talks about the reward function to use - binary, or something else. A detailed explanation of what the author is trying to convey in the above extract would really help. 
","['reinforcement-learning', 'papers', 'reward-functions', 'recommender-system', 'knowledge-graph']",
Are bandits considered an RL approach?,"
If a research paper uses multi-armed bandits (either in their standard or contextual form) to solve a particular task, can we say that they solved this task using a reinforcement learning approach? Or should we distinguish between the two and use the RL term only when it is associated with an MDP formulation?
In fact, each RL course/textbook usually contains a section about bandits (especially when dealing with the exploration-exploitation tradeoff). Additionally, bandits also have the concept of actions and rewards. 
I just want to make sure what the right terminology should be, when describing either approach.
","['reinforcement-learning', 'terminology', 'multi-armed-bandits', 'contextual-bandits']","
Let's have a look at the introduction of Chapter 2: Multi-armed Bandits in the
Reinforcement Learning: An Introduction by Sutton, Barto

The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct
actions. This is what creates the need for active exploration, for an explicit search for good behavior.
Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or
the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action
to take, independently of the action actually taken. This kind of feedback is the basis of supervised
learning, which includes large parts of pattern classification, artificial neural networks, and system
identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback
depends entirely on the action taken, whereas instructive feedback is independent of the action taken.
In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one
that does not involve learning to act in more than one situation. This nonassociative setting is the
one in which most prior work involving evaluative feedback has been done, and it avoids much of the
complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly
how evaluative feedback differs from, and yet can be combined with, instructive feedback.
The particular nonassociative, evaluative feedback problem that we explore is a simple version of
the k-armed bandit problem. We use this problem to introduce a number of basic learning methods
which we extend in later chapters to apply to the full reinforcement learning problem. At the end
of this chapter, we take a step closer to the full reinforcement learning problem by discussing what
happens when the bandit problem becomes associative, that is, when actions are taken in more than
one situation.

Since bandits involve evaluative feedback they are indeed a type of a (simplified) reinforcement learning problem.
"
What are multi-hop relational paths?,"
What are multi-hop relational paths in the context of knowledge graphs (KGs)?
I tried looking it up online, but didn't find a simple explanation.
","['recommender-system', 'knowledge-graph']",
How can DDPG handle the discrete action space?,"
I am wondering how can DDPG or DPG handle the discrete action space. There are some papers saying that use Gumbel softmax with DDPG can make the discrete action problem be solved. However, will the Gumbel softmax make the deterministic policy be the stochastic one? If not, how can that be achieved?
","['reinforcement-learning', 'ddpg']",
How can transition models in RL be trained adversarially?,"
To give a little background, I've been reading the COBRA paper, and I've reached the section that talks about the exploration policy, in particular. We figure that a uniformly random policy won't do us any good, since the action space is sparsely populated with objects that the agent must act upon - and a random action is likely to result in no change (an object here occupies only about 1.7% of the space of the screen). Hence we need our agent to learn in the exploration phase a policy that clicks on and moves objects more frequently. 
I get that a random policy won't work, but I've difficulty understanding how and why the transition model is trained adversarially. Following is the extract which talks about the same, and I've highlighted parts that I don't completely understand - 

""Our approach is to train the transition model adversarially with an exploration policy that learns to take actions on which the transition model has a high error. Such difficult-to-predict actions should be those that move objects (given that others would leave the scene unchanged). In this way the exploration policy and the transition model learn together in a virtuous cycle. This approach is a form of curiosity-driven exploration, as previously described in both the psychology (Gopnik et al., 1999) and the reinforcement learning literature (Pathak et al., 2017; Schmidhuber, 1990a,b).""


How does it help to take actions on which the transition model has a
high error?
I don't exactly see how a virtuous cycle is in action

Could someone please explain? Thanks a lot!
","['reinforcement-learning', 'papers', 'adversarial-ml']",
How to use 'Canny/Watershed' algorithm's output as an input for Image Classification Model,"
I have a very silly problem in hand. I have implemented 2 methods which give me the mask to separate the objects from the background. What I get from one method is the object encapsulated in the red Contour or boundary and the other one makes the background Red. 
I am using Keras to classify Trash. I wanted to use the output or the EXTRACTED object as an input to the CNN model. Now I do not see any difference in the images and output. All there is extra boundary around the object and I fail to understand how it can help my model.
I could add an extra alpha channel to second method to make the background transparent but Keras' ImageDataGenerator do not work with RGBA images. What shoul dI do to improve the model?
","['convolutional-neural-networks', 'computer-vision', 'image-recognition', 'keras', 'image-segmentation']","
First and foremost, I have to say that this could (and likely will) be a very hard task. Neural networks (NNs) have excelled at computer vision tasks identifying everything from textures to complex objects but what you are trying to do goes beyond that. We (humans) identify trash using the context as much as the object. An object on a table and the same object in a trash bin can look identical but the context tells us which one is trash. Also, anything can be trash. Trash is not an object, its a state.
Having said all that, it sounds like an interesting project and it would be a very useful model so I'll do my best to explain how you would go about trying this. As per your comment, you have outlined the two steps you need.

Identify and extract the trash object from the image.
Classify the type of waste

This could be accomplished using a single model but I'll explain how to do it with two models to make clear what is being done.
Identify and extract trash
The objective of the first model is to identify and extract the region of interest (the part of the image containing the trash). This is an image segmentation task typically accomplished using an R-CNN - some guides explaining how they work can be found here, here or here. These methods use supervised learning which requires masks delineating the object of interest to be used as the ground truth. A mask is a binary image the same size as your input image where each pixel in the image representing your positive class (trash) is set to 1 and all others are set to 0.
The output of your Canny/Watershed algorithms could be used as the masks to train the model, however your model will only ever be as good as your masks. Therefore, you might as well use your Canny/Watershed algorithms to accomplish task 1. If the masks generated by your algorithm are not of sufficient accuracy you will need to find another way to generate your masks - maybe even doing it manually. 
An approximate rule of thumb for object detection with NNs is that you need 1,000 representative images per class, where class implies a specific object. In this case, unless you are attempting to identify very specific items of trash, you would likely need 100s of thousands of images to obtain a high degree of accuracy.
Classify the type of waste
This task should be easier, making the assumption that the segmented image from the first step is always trash. By assigning a label to each segmented image (paper, metal, glass, cardboard, etc) it becomes a normal multi-class classification task which are well documented online with lots of explainations and tutorials.
Single model
These tasks could be combined into a single model by modifying the masks in step 1. Instead of using a single binary mask, you could create a multichannel mask of shape m x n x o where m x n is your input image size and o is the number of kinds of waste you are attempting to identify. Each channel is a binary mask for a given type of waste. Therefore, it becomes a matter of not just identifying and segmenting trash but of segmenting each type of trash separately. Needless to say the complexity of the model would be a lot higher and so would the process to create the masks.
"
Should I just use exploitation after I have trained the Q agent?,"
When using a trained Q-learning algorithm in an actual game, would I just use exploitation and no longer use exploration? Should I use exploration only during the training phase?
","['reinforcement-learning', 'training', 'q-learning', 'exploration-exploitation-tradeoff']","
Once you have estimated the $Q$ function, you can derive the policy from it in different ways. For example, you can act greedily with respect to it (see this answer), which can be formally denoted as
$$
\pi(s) = \operatorname{argmax}_{a^*}Q(s, a), \; \forall s \in \mathcal{S}
$$
where $Q(s, a)$ is your estimated value function and $\pi$ the policy greedily derived from it.
This means that you would just exploit your current knowledge of the return. This is probably a good thing to do if you believe your value function is optimal and the dynamics of the environment don't change.
Of course, if your policy is not optimal, you may not want to always execute the greedy action. In that case, you could still perform some form of exploration (e.g. with the $\epsilon$-greedy policy).
Moreover, if the dynamics (e.g. the reward function) of your environment change over time, you could continually train your RL agent. If you are interested in continual RL, the paper Continual Reinforcement Learning with Complex Synapses (2018) could potentially be useful.
"
What if the rewards induced by an environment are related to the policy too?,"
Assume we have a policy $\pi_{\theta}$ in a classic reinforcement learning setting, and a reward function $R^{\pi}(s,a)$ that changes as long as $\pi$ changes i.e. not only is it predefined by the environment itself, how can we model the popular algorithms (e.g. SAC) according to this change?
","['machine-learning', 'reinforcement-learning', 'policy-gradients', 'environment', 'ddpg']",
What is the original source of the TD Advantage Actor-Critic algorithm?,"
What is the original source of the TD Advantage Actor-Critic algorithm?
I found this tutorial really helpful for learning the algorithm. However, what is the original source of this algorithm?

","['reinforcement-learning', 'reference-request', 'actor-critic-methods', 'advantage-actor-critic']",
How should I design the reward function for racing game (where the goal is to reach finishing line before the opponent)?,"
I'm building an agent for a racing game. In this game, there is a randomized map where there are speed boosts for the player to pick up and obstacles that act to slow the player down. The goal of the game is to reach the finishing line before the opponent.
While working on this problem, I've realized that we can almost forget about the presence of our opponent and just focus on getting the agent to the finish line as quickly as possible.
I started with a simple

$-1$ reward for every timestep
$+100$ reward for winning, and
$-100$ for losing.

When I was experimenting with this, I felt like the rewards may be too sparse, as my agent was converging to pretty poor average returns. I iterated to a function of speed and distance travelled (along with the $+100$ reward), but, after some experimentation, I started feeling like the agent might be able to achieve high returns without necessarily being the fastest to the finish line.
I'm thinking that I return to the first approach and possibly add in some reward for being in the first place (as a function of the opponent's distance behind the agent).
What else could I try? Should I try and spread the positive rewards out more for good behavior? should I create additional rewards/penalties for perhaps hitting obstacles and using boosts or can I expect the agent to learn the correlation?
","['reinforcement-learning', 'game-ai', 'rewards', 'reward-design', 'reward-shaping']","
Sutton and Barto state, ""The reward signal is your way of communicating to the robot [agent] what you want it to achieve, not how you want it achieved."" Since you stated that the goal is to reach the finish line first, then a reward of $1$ for winning, $0$ for losing, and $0$ at all other time steps seems to fit that narrative. If a draw is identical to a loss, then it should provide reward $0$; otherwise, a reward of $0.5$ seems reasonable. These rewards provide model interpretability: an expected return of $p$ (estimated with a state-value or action-value) at a certain state under the current policy would signify a $p$ chance of winning. Also, keeping the rewards in absolute value at most 1 can aid in training speed and prevent divergence, but it often isn't necessary for deep reinforcement learning problems. You most certainly can add other rewards based on partial progress towards the goal, but as it seems you found out, they may lead to incorrect results.
That being said, I would focus on the training process instead of a finely-tuned reward signal. Since there is a known goal state in the racing game (the finish line), I suggest training the RL agent by first initializing all racer agents only a few steps away from the goal state at the beginning of each episode. These episodes are shorter and therefore should provide a more dense reward signal. When your RL agent has learned a winning policy (e.g. wins more often than not), then initialize the agents slightly further from the goal state at the beginning of each episode. Also, continue to use and train the same neural network. Since the neural network presumably knows a winning policy at states near the goal state, then by initializing the agents only a few states further back, the RL agent is given an warm start and only needs to learn a policy for a few more states. The policy encoded by the neural network essentially contains a refined reward signal for the states close to the goal state since it is based on a winning policy; this helps prevent the sparsity problem caused by only supplying a reward at episode completion. You can repeat this process by initializing the agents slightly further from the goal state once the RL agent has learned a winning policy while continuing to use and train the same neural network. 
Depending on your access to the environment internals, you may need other analogous approaches. For example, you could initialize the agents at the original starting line (i.e. not partway down the map) and then see which agent makes it $n$ units down the map first to determine the winner. Once a winning policy is learned by the RL agent, then gradually increase $n$ until $n$ matches the distance from the starting line to the finish line. Since it seems like you have distance traveled and distance to the opponent as features, you may instead try this method if you are unable to initialize the agents wherever you want on the map and instead can only initialize them on the starting line.
A notable benefit of the overall approach is that you can more quickly debug your algorithm on the easier environments (i.e. ones with shorter episode lengths) to be confident that the learning process is correct and focus your efforts elsewhere (e.g. the training process, including the reward signal).
"
Adversarial Q Learning should use the same Q Table?,"
I'm creating a RF Q-Learning agent for a two player fully-observable board game and wondered, if I was to train the Q Table using adversarial training, should I let both 'players' use, and update, the same Q Table? Or would this lead to issues?
","['reinforcement-learning', 'q-learning', 'adversarial-ml']","

should I let both 'players' use, and update, the same Q Table? 

Yes this works well for zero-sum games, when player 1 wants to maximise a result (often just +1 for ""player 1 wins"") and player 2 wants to minimise a result (score -1 for ""player 2 wins""). That alters algorithms such as Q-learning because the greedy choice switches beween min and max functions over action values - player 1's TD target becomes $R_{t+1} + \gamma \text{min}_{a'} [Q(S_{t+1}, a')]$ because the greedy choice in the next state is taken by player 2.
Alternatively, if the states never overlap between players, then you could learn a Q function that always returns the future expected return according the current player. This can be forced if necessary by making part of the state record whose turn it is. With this you need a way to convert between player 1 and player 2 scores in order to use Q learning updates. For zero-sum games, then the Q value from a player 2 state is negative of player 1's value for that same state, and vice versa, so the TD target changes for both players to $R_{t+1} - \gamma \text{max}_{a'} [Q(S_{t+1}, a')]$
The first option can result in slightly less complexity in the learned function, that might be an issue if you are using function approximation and learning a Q function e.g. using neural networks instead of a Q table. That may result in faster learning and generalisation, although it will depend on details of the game.

Or would this lead to issues?

No major issues. I am performing this kind of training - a single Q function estimating a global score which P1 maximises and P2 minimises - for the Kaggle Connect X competition, and it works well. 
I can think of a couple of minor things:

You may still want to have the ability for each player to be using a different version of the table or learned Q function. This would allow you to have different versions of your agent (e.g. at different stages of learning) compete, to evaluate different agents against each other. To do this, you have to write code that allows for multiple tables or functions in any case.
You need to keep track of how both players express and achieve their opposing goals when using the table, as you can already see by the modified TD targets above. This becomes more important when adding look-ahead planning, which is a common addition and can significantly improve the performance of an agent - during look-ahead you must switch between player views on what the best action choice is. It is possible to make off-by-one errors in some part of the code but not others and have the agent learn inconsistently.

"
What is meant by a multi-dimensional continuous action space?,"
In the context of Reinforcement Learning, what does it mean to have a multi-dimensional continuous action space?
I came across the following in the COBRA Paper

A method for learning a distribution over a multi-dimensional continuous action space. This learned distribution can be sampled efficiently.

and

During the initial exploration phase it explores its environment, in which it can move objects freely with a continuous action space but is not rewarded for its actions.

So, what do the multi-dimensionality and the continuity of the action space refer to? It'd be great if someone could provide an explanation with examples!
","['reinforcement-learning', 'terminology', 'papers', 'action-spaces', 'continuous-action-spaces']","
The question has already been answered by Kirill, but I thought I'll add a good example of a multi-dimensional continuous action space too, namely the one I just encountered in the COBRA paper itself.

In all of our experiments we use a 2-dimensional virtual ""touch-screen"" environment that contains objects with configurable shape, position, and color. The agent can move any visible object by clicking on the object and clicking in a direction for the object to move. Hence the action space is continuous and 4-dimensional, namely a pair of clicks.

"
How do i start building an autoclick bot for pubg mobile?,"
I want to make a bot which clicks the fire button on the mobile screen upon seeing an enemies head.
In pubg mobile which is an android game you have to control the fire button and the aim along with many other controls to kill an enemy or other players. I want to automate the fire button and everything else would be controlled by me, when I'll aim on a player's head the bot should click the fire button instantly.
So there are a few problems in this ,first one is that what if it shoots upon seeing my teammates head and other is what if there are  two players at once ,which will it shoot, for that it needs to shoot when my aim or the red crosshair is on the player's head.
I don't know how to get started , I need to make an image recognition app and an autoclicker and combine them both. How do I get started? Assuming that I only know basic python.
","['python', 'image-recognition', 'game-ai', 'open-ai']",
Backpropagation of neural nets with shared weight,"
I am trying to understand the mathematics behind the forward and backward propagation of neural nets. To make myself more comfortable, I am testing myself with an arbitrarily chosen neural network. However, I am stuck at some point.
Consider a simple fully connected neural network with two hidden layers. For simplicity, choose linear activation function (${f(x) = x}$) at all layer. Now consider that this neural network takes two $n$-dimensional inputs $X^{1}$ and $X^{2}$. However, the first hidden layer only takes $X^1$ as the input and produces the output of $H^1$. The second hidden layer takes $H^{1} $and $X^2$ as the input and produces the output $H^{2}$. The output layer takes $H^{2}$ as the input and produces the output $\hat{Y}$. For simplicity, assume, we do not have any bias.
So, we can write that, $H^1 = W^{x1}X^{1}$
$H^2 = W^{h}H1 + W^{x2}X^{2} = W^{h}W^{x1}X^{1} + W^{x2}X^{2}$ [substituting the value of $H^1$]
$\hat{Y} = W^{y}H^2$
Here, $W^{x1}$, $W^{x2}$, $W^{h}$ and $W^{y}$ are the weight matrix. Now, to make it more interesting, consider a sharing weight matrix $W^{x} = W^{x1} = W^{x2}$, which leads, $H^1 = W^{x}X^{1}$ and $H^2 = W^{h}W^{x}X^{1} + W^{x}X^{2}$
I do not have any problem to do forward propagation by my hand; however, the problem arises when I tried to make backward propagation and update the $W^{x}$.
$\frac{\partial loss}{\partial W^{x}} = \frac{\partial loss}{\partial H^{2}} . \frac{\partial H^{2}}{\partial W^{x}}$ 
Substituting, $\frac{\partial loss}{\partial H^{2}} = \frac{\partial Y}{\partial H^{2}}. \frac{\partial loss}{\partial Y}$ and $H^2 = W^{h}W^{x}X^{1} + W^{x}X^{2}$
$\frac{\partial loss}{\partial W^{x}}= \frac{\partial Y}{\partial H^{2}}. \frac{\partial loss}{\partial Y} . \frac{\partial}{\partial W^{x}} (W^{h}W^{x}X^{1} + W^{x}X^{2})$
Here I understand that, $\frac{\partial Y}{\partial H^{2}} = (W^y)^T$ and $\frac{\partial}{\partial W^{x}} W^{x}X^{2} = (X^{2})^T$ and we can also calculate $\frac{\partial Y}{\partial H^{2}}$, if we know the loss function. But how do we calculate $\frac{\partial}{\partial W^{x}} W^{h}W^{x}X^{1}$?
","['convolutional-neural-networks', 'backpropagation', 'weights']","
If we write $ H^2 = W^{h}H1 + W^{x}X^{2} $ then it will be better to understand the backward propagation step.
Now,
$\frac{\partial}{\partial W^{x}} W^{h}W^{x}X^{1}$ can be written as:
$\frac{\partial H^2}{\partial H^1}\frac{\partial H^1}{\partial W^{x}} $
$\frac{\partial H^2}{\partial H^1} =  (W^h)^T$ and 
$\frac{\partial H^1}{\partial W^{x}} = (X^{1})^T $
Therefore,
$\frac{\partial}{\partial W^{x}} W^{h}W^{x}X^{1} = (W^h)^T(X^{1})^T  $
I hope it has solved your problem.
"
What are the best datasets available for music information retrieval?,"
I am interested in doing some work in classification problems in music information retrieval. I know that there are some formats of datasets (such as MIDI, Spectrogram, Piano-roll, MusicXML, etc.) for this work but have been unable to find any nice large datasets for this. What are the best free datasets for such work? I am mainly looking into western classical music.
","['natural-language-processing', 'datasets', 'data-science', 'audio-processing', 'structured-data']",
Should I compute the gradients with respect to the flatten layer in a convolutional neural network?,"
I'm trying to create a convolutional neural network without frameworks (such as PyTorch, TensorFlow, Keras, and so on) with Python.
Here's a description of CNN taken from the Wikipedia article

In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.

A CNN has different types of layers, such as convolution, pooling (max or average), flatten and dense (or fully-connected) layers.
I have a few questions.

Should we compute gradients (such as $\frac{\partial L}{\partial A_i}$,$\frac{\partial L}{\partial Z_i}$,$\frac{\partial L}{\partial A_{i-1}}$ and so on) in flatten layer or not?
If no, then how should I compute $\frac{\partial L}{\partial A_i}$ and $\frac{\partial L}{\partial Z_i}$ of first layer of convolutional layer? With $\frac{\partial L}{[\frac{\partial g(A_i)}{\partial x}]}$ or with 
$\frac{\partial L}{\partial dA_{i+2}}$(P.S. as you know iteration of BackPropagation is reverse, so I used i+n for denote the previous layer)? 
Or can I compute derivatives in Flatten layer with 
$$\frac{\partial J}{\partial A} = W_{i+1}^T Z_{i+1}$$(i+1 denotes prev.layer in BackProp)
$$\frac{\partial L}{\partial Z} = \frac{\partial L}{\partial A} *\frac{\partial g(A_i)}{\partial x} $$
and then reshape of Conv2D shape? 

P.S. I found questions like mine (names are same), but there're not answer to my question as I asking about formula.
","['machine-learning', 'convolutional-neural-networks', 'backpropagation']",
Automating browser actions using AI,"
I am at a very initial stage of my research so I will try to describe what I am trying to achieve: 
I want to create an AI model which learns how to navigate the browser's component like clicking or creating new favorites and tabs, or navigating browser action menu, or bookmarking the website etc. In short, automating the testing for browser using selenium and an AI model so that over time the model learns itself to navigate the browser and test different functionality itself and eventually it test the functionalities that are not seen by the model before. For example: if I feed the AI model how browser is closed when ""x"" is clicked and minimize when ""-"" is clicked, next it can learn itself how to maximize the browser. 
The initial input could be to record some videos of navigating the browser using selenium and then feed it to the model and with time it learns itself to go to different section of the browser which the model does not know and still test it. 
Is it even possible to combine AI and selenium together to create something like this? If yes, how can I achieve it and what is the best approach to develop such model. 
Thanks in advance.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'ai-design']",
When should Monte Carlo Tree search be chosen over MiniMax?,"
I would like to ask whether MCTS is usually chosen when the branching factor for the states that we have available is large and not suitable for Minimax. Also, other than MCTS simluates actions, where Minimax actually 'brute-forces' all possible actions, what are some other benefits for using Monte Carlo for adversarial (2-player) games?
","['monte-carlo-tree-search', 'minimax', 'alpha-beta-pruning']","
Some basic advantages of MCTS over Minimax (and its many extensions, like Alpha-Beta pruning and all the other extensions over that) are:

MCTS does not need a heuristic evaluation function for states. It can make meaningful evaluations just from random playouts that reach terminal game states where you can use the loss/draw/win outcome. So if you're faced with a domain where you have absolutely no heuristic domain knowledge that you can plug in, MCTS is likely a better choice. Minimax must have a heuristic evaluation function for states (exception: if you game is so simple that you can afford to compute the complete game tree and reach all terminal game states immediately from the the initial game state, you don't need heuristics). If you do have strong evaluation functions, you can still incorporate them and use them to improve MCTS too; they're just not strictly necessary for MCTS.

MCTS has simpler anytime behaviour; you can just keep running iterations until you run out of computing time, and then return the best move. Typically we expect the performance level of MCTS to grow with computatinon time / iteration count relatively smoothly (not always 100% true, but intuitively you can usually expect something like this). You can sort of achieve anytime behaviour in minimax with iterative deepening, but that's usually a bit less ""smooth"", a bit more ""bumpy""; this is because every time you increase the search depth, you need significantly more processing time than you did for the previous depth limit. If you run out of time and have to abort your current search at your current depth limit, that last search will be completely useless; you'll have to discard it and stick to the results from the previous search with the previous depth limit.


A difference, which is not necessarily an advantage or disadvantage either way in the general case (but can be in specific cases):

The computation time of MCTS is generally dominated by running (semi-)random playouts. This means that functions for computing legal move lists, and applying moves to game states, typically dictate how fast or slow your MCTS runs; making these functions faster will generally make your MCTS faster. On the other hand, the computation time of Minimax is generally dominated by copying game states (or ""undoing"" moves, which is an operation that in most games will require additional memory usage for game states to be possible) and heuristic evaluation functions (though the latter are likely to also become important in terms of computation cost in MCTS if you choose to include them there). In some games it will be easier to provide efficient implementations for one of these, and in other games it may be different.

A basic advantage of Minimax over MCTS:

In settings where MCTS can only run very few iterations relative to the branching factor (or in the extreme case, fewer iterations than there are actions available in the root node), MCTS will perform extremely poorly / close to random play. We've noticed this being the case for quite a decent number of games in our general game system Ludii (where the ""general game system"" often implies that games are implemented less efficiently than they could be in a dedicated single-game-specific program) with low time controls (like 1 second per move). This same general game setting often makes it difficult to find super strong heuristics, but it's generally still possible to come up with some relatively simple ones (like just a simple material heuristic in chess). An alpha-beta search with just a couple of search plies and a basic, simple heuristic will often outperform a close-to-random MCTS if the MCTS can't manage to run significantly more iterations than it has legal moves in the root node.

"
Understanding the TensorFlow implementation of the policy gradient method,"
I was trying to understand the implementation of a basic policy gradient (REINFORCE) method using TensorFlow.
I think I got almost everything. The only thing that still bothers me is the loss function implementation.
From the theory, we have that after all the manipulation the gradient of the score function is 
$$\nabla_{\theta}J(\theta)=\mathop{\mathbb{E}}\left[\nabla_{\theta}(log(\pi(s,a,\theta)))R(\tau) \right]$$
In this Cartpole example the part relative to the loss function is 
    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = NeuralNetworkOutputs, labels = actions)
    loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) 

At this point, I do not understand how the definition from above translates into code.
As far as I understood, the functions 
tf.nn.softmax_cross_entropy_with_logits_v2(logits = NeuralNetworkOutputs, labels = actions)

returns
log(softmax(NeuralNetworkOutputs))*actions

Which is then multiplied by the discounted returns 
log(softmax(NeuralNetworkOutputs))*actions*discounted_episode_rewards_

Within this expression, I do not understand why should we multiply, an expression which looks like the loss function we want, by the value of the action.
","['neural-networks', 'reinforcement-learning', 'tensorflow', 'deep-rl', 'reinforce']",
When does Monte Carlo linear function approximation converge?,"
In this Stanford lecture (minute 35:47 and 37:00), the professor says that Monte Carlo (MC) linear function approximation does not always converge, and she gives an example. In general, when does MC linear function approximation converge (or not)? 
Why do people use that MC linear function approximation if sometimes it doesn't converge?
They also gave the definition of the stationary distribution of a policy, and I am not sure if using it for function approximation converges or not.
","['reinforcement-learning', 'deep-rl', 'monte-carlo-methods', 'convergence']",
Is it a good idea to overfit on a small part of your data for faster model convergence?,"
I working on a classification problem that needs to detect patterns on a time serie. Basically, there's a catch-all class that means ""no pattern detected"", the other are for the specific patterns. The data is imbalanced (ratio 1/10 at least), but I adapted the class weights.
I'm able to overfit successfully on a few days of data, but when I train on 2 years of data, the model seems stuck on class1 ""no pattern detected"" for a veeeery long time. I've tried several  learning rates, but it doesn't make the convergence happen significatively faster.
Is it a better starting point for my training to use the overfitting model's weight as a starting point? Could this allow the model to converge faster?
","['convolutional-neural-networks', 'classification', 'overfitting', 'convergence', 'learning-rate']","
The first thing which you first have to understand is that does your trained model is working efficiently with both the training and testing data. If yes then its not overfitting. There is only one case where over-fitting doesn't matter and that is when your testing data is same as training data and dealing with is very important or it could be disastrous. Therefore, to avoid overfitting you can try several methods and they are followings: 

Optimization Algorithms like sgd, Adam, RMSprop, Adagard, are few gradient descents.
Loss function like Hinge loss could be useful.
Parameter initialization greatly influence the optimization process.

Overall, Large datasets are better for building ML models but there are some methods there is an interesting article that i would recommend you to read: Article Link
I Hope this Helps!!!
"
How does the crossover operator work when my output contains only 2 states?,"
I'm currently working on a project where I am using a basic cellular automata and a genetic algorithm to create dungeon-like maps. Currently, I'm having an incredibly hard time understanding how exactly crossover works when my output can only be two states: DEAD or ALIVE (1 or 0).
I understand crossover conceptually - you find two fit members of the population and they exchange genetic material, hopefully producing a fitter offspring. I also understand this is usually done by performing k-point crossover on bit strings (but can also be done with real numbers). 
However, even if I encode my DEAD/ALIVE cells into bits and cross them over, what do I end up with? The cell can only be DEAD or ALIVE. Will the crossover give me some random value that is outside this range?
And even if I were to work on floating-point numbers, wouldn't I just end up with a 1 or 0 anyway? In that case, it seems like it would be better to just randomly mutate DEAD cells into ALIVE cells, or vice versa.
I've read several papers on the topic, but none seem to explain this particular issue (in a language I can understand, anyway). Intuitively, I thought maybe I can perform crossover on a neighbourhood of cells - so I find 2 fit neighbourhoods, and then they exchange members (for example, neighbourhood A gives 4 of its neighbours to neighbourhood B). However, I have not seen this idea anywhere, which leads me to believe it must be fundamentally wrong.
Any help would be greatly appreciated, I'm really stuck on this one.
","['genetic-algorithms', 'crossover-operators', 'genetic-operators']",
Are there any board game appropriate to examine the performance of multiple agents that cooperate both inter-group and intra-group?,"
I want to find out scenarios that useful to examine the performance of intra-group and inter-group cooperation in MARL. 
Specifically, I prefer a board game (like sudoku) that is suitable for the cooperation evaluation. 
But there are some differences between the requirements and the go like game. Every grid on the board should be treated as an agent. They are designed to form a situation with the local utility and global utility. 
Take the sudoku as an example, every grid should choose an appropriate value to reach the sudoku solution.
Due to my non-familiarity with traditional MARL scenarios, it will be a great help for showing me some keywords or lists.
","['reinforcement-learning', 'reference-request', 'multi-agent-systems']",
How can I design a system that suggests physical exercises to a person while keeping into account the fatigue?,"
I want to create an exercise suggester. Each day either has a routine or is a rest day. A routine has 4 slots. For each slot we select an exercise. We constrain the legal exercises, only do upper-body today, etc. We, therefore, restrict the number of available exercises. This seems easy.
I want to know how can I extend this model to take fatigue into account. Fatigue is a state such that every exercise reduces it (specific value for each exercise) and it recovers with time. 
Can this problem even be modeled as a constraint satisfaction problem? What's the best way of modeling and solving this problem
I'd like to suggest all the legal exercises taking fatigue into account.
","['machine-learning', 'ai-design', 'constraint-satisfaction-problems']",
Why is this variable in equation 2 of the SQAIR paper a random vector of $n$ ones followed by a zero?,"
I've been reading the SQAIR paper lately, and the mathematics involved seems a bit complicated.
Some background, about the paper: SQAIR stands for Sequential Attend, Infer, Repeat - the paper does generative modelling of moving objects. The idea of Attend, Infer, Repeat is to decompose a static scene into constituent objects, where each object is represented by continuous latent variables. The latent variables, $z^{what}$,$z^{where}$ and $z^{pres}$ encode the appearance, position and presence of an object.
Here's a screenshot of the first of many things I'm unable to understand -

Why is $z^{pres,1:n+1}$ a random vector of $n$ ones followed by a zero? Why do we need the zero? How does it help?
Furthermore, an explanation of equation $(2)$ as in the image above, would be great.
P.S. I hope you all find the paper interesting. I'll ask other questions from the paper in separate posts, so as to not crowd one post with too many queries.
","['computer-vision', 'papers', 'generative-model', 'random-variable']","
From my understanding of the paper, $Z^{pres}$ keeps track of the objects in the scene. For every step of the sequential inference, $z^{pres,i}$ takes either a 0 or a 1. A 1 represents that an object is present and it has to be explained by the remaining latent variables. 0 indicates that all the objects have been explained and inference is complete.
"
Is it theoretically possible (or impossible) that principal component analysis worsens the performance of the model?,"
In case I had a prediction model and decided to add a PCA step prior to the model, is it theoretically possible/impossible that the number of output dimensions that is better for all tests may perform worse than the model without PCA?
My question comes from the fact that I want to add a PCA step prior to a model and hyperparameterize the PCA output dimension from 1 to N (N being the number of dimensions in the original dataset) and I wanted to know if there is any theoretical basis that there is no case in which performing this previous step could have a worse performance than the previous model.
Especially, my doubt comes if the best PCA case from a selection of dimensions from 1-N is always better than the best case without PCA.
","['machine-learning', 'hyperparameter-optimization', 'principal-component-analysis', 'dimensionality-reduction']","
PCA can make models worse,  imagine data points scattered along two elongated parallel rectangles. The axis with the greatest variation will be parallel to the rectangles but doesn't provide any benefit in classifying the points.
"
Similarity score between 2 words using Pre-trained BERT using Pytorch,"
I'm trying to compare Glove, Fasttext, Bert on the basis of similarity between 2 words using Pre-trained Models. Glove and Fasttext had pre-trained models that could easily be used with gensim word2vec in python.

Does BERT have any such models?
Is it possible to check the similarity between two words using BERT?

","['natural-language-processing', 'bert', 'similarity']",
How can Siamese Networks be viewed as RNNs?,"

""Single-object tracking commonly uses Siamese networks, which can be seen as an RNN unrolled over two time-steps.""

(from the SQAIR paper)
I'm wondering how Siamese networks can be viewed as RNNs, as mentioned above. A diagrammatic explanation, or anything that helps understand the same, would help! Thank you!
","['neural-networks', 'recurrent-neural-networks', 'papers']","
Single object tracking using a Siamese Network is a Detect and compare approach, where an object of interest is detected and the one to be tracked is passed through a siam network with next consecutive frame to get a correlation between them, if you look at the related reference, it is a correlation-based tracking, ie. correlation between objects detected in one frame and the ones detected in next frame, which you can imagine as samples considered across 2 timesteps or an RNN unrolled to 2 timesteps
"
What is the gradient of the Q function with respect to the policy's parameters?,"
I have been recently studying Actor-Critic algorithms, and I ran into the following question.
Let $Q_{\omega}$ be the critic network, and $\pi_{\theta}$ be the actor. It is known that in order to maximize the objective return $J(\theta)$, we follow the gradient direction, which could be estimated as follows $$\nabla_{\theta}J=\mathbb{E}[Q_{\omega}(s,a).\nabla_{\theta}log \pi_{\theta} (a|s)].$$ But if we were to calculate the gradient of $Q^{\pi}$ with respect to $\theta$, what are the possible approaches to do so?
More generally, say we have a network $\phi_{\omega}$ that is trained on data generated from another neural network, say a stochastic actor $\pi_{\theta}$ like in classic reinforcement learning frameworks, how to find the gradient of $\phi_{\omega}$ w.r.t ${\theta}$?
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'policy-gradients', 'actor-critic-methods']",
What are the most popular and effective approaches to leveraging AI for stock price prediction?,"
Currently, what are the most popular and effective approaches to leveraging AI for stock price prediction?
It seems like there could be several approaches and problem formulations:

Supervised learning:
Regression: predict the stock price directly
Classification: predict whether the stock price goes up or down
Unsupervised learning: find clusters of stocks that move together
Reinforcement learning: let the agent directly maximize its stock market return
Other AI methods: rules, symbolic systems, etc.

Which are most popular/performant? Are there other ways that people are using machine learning in stock trading (sentiment analysis on financial statements, news, etc.)?
","['machine-learning', 'reference-request', 'applications', 'algorithm-request', 'algorithmic-trading']","
Your list is complete for what is considered 'popular' by most practitioners who apply AI for stock trading.  Supervised learning and rule learning are at the top for accuracy.  There are more academic papers published on classifiers than on regression approaches;
classifiers are typically more accurate than regressors.
"
What kinds of techniques do autopilots of autonomous cars use?,"
What kinds of techniques do autopilots of autonomous cars (e.g. the ones of Tesla) use? Do they use reinforcement learning? Which types of neural network architecture do they use?
","['neural-networks', 'applications', 'reference-request', 'autonomous-vehicles']","
They definitely don't use RL.
My guess is a mix of NN for segmentation, lidar, radar and how for perception.
Once object detection has been performed they use a trajectory generator and MPC.
There is an autonomous car class on Coursera that goes over this.
"
How do I approach this problem?,"
Let's say I have a dataset with multiple types of multiple ingredients (salt1,salt2, etc). Each n-th variation of each ingredient vs flavor may be represented by an n×k matrix that where an ingredient corresponds with a particular value of ""flavor"".
A recipe consists of a 1×n vector (where n is the number of ingredients) where each value corresponds to the quantity of ingredient in the recipe.
A particular combination of ingredients, with particular weights, with some transformation, would result in a particular 1×k ""flavor"" profile, in this simple model.
One approach could be to formulate this as a Probabilistic Matrix Factorization problem (I think), with k being the number of flavor parameters. And combining the recipe vector with the flavor matrix might do the trick.
But the problem is, the flavor value of each ingredient (and each variation of the ingredient) in the ingredient-flavor matrix would be very very limited. The recipe flavor profile might have a corresponding flavor vector, that too would be limited, and would not be available, at the beginning. So in order to capture the relationship between the ingredients and the flavor, the system would be dependent on user-submitted data on recipe/ingredient flavors.
Is there a way I could create clusters of recipes based on user flavor ratings and extrapolate these to the constituent ingredients or vice versa? Could this be done via some unsupervised learning algorithm?
I am quite new to this, I would appreciate some help or some pointers to which mathematical approaches I should be looking at to model this problem.
","['machine-learning', 'math', 'unsupervised-learning', 'clustering']",
Why is DDPG an off-policy RL algorithm?,"
In DDPG, if there are no $\epsilon$-greedy and no action noise, is DDPG an on-policy algorithm?
","['reinforcement-learning', 'deep-rl', 'ddpg', 'off-policy-methods', 'on-policy-methods']","
DDPG is an off-policy algorithm simply because of the objective taking expectation with respect to some other distribution that we are not learning about, i.e. the deterministic policy gradient can be expressed as
$$\nabla _{\theta^\mu} J \approx \mathbb{E}_{s_t \sim \rho^\beta} \left[ \nabla _{\theta^\mu} Q(s,a|\theta^Q) | s=s_t, a=\mu(s_t ; \theta ^\mu)  \right]\;.$$
We are interested in learning about the policy parameters of $\mu$, denoted by $\theta$, but we take expected with respect to some discounted state distribution induced by a policy $\beta$, which we will denote as $\rho^\beta$.
To summarise, we are learning off-policy as the expectation of the gradient is taken with respect to some state distribution that occurs under some policy that we are not learning about.
Given that on-policy learning is a special case of off-policy learning, if the replay buffer had a size of one, i.e. we use only the most recent experience tuple to perform parameter updates, then DDPG would be on-policy.
"
Why does our AI play worse at even levels of depth?,"
We are building an AI to play a board game. Leaving aside the implementation, we noticed that it plays worse when we set an even (2,4,6,...) level of depth. We use a minimax depth-first strategy.
Do you have any ideas why it behaves like that?
Edit: for example if we set a game between an AI with 5 levels of depth and an AI with 6 levels of depth, the first one usually wins (and this is weird).
","['game-theory', 'depth-first-search']",
What is the KWIK framework?,"

...for learning transition dynamics...in the KWIK framework.

The above is part of a paper's conclusion - and I don't really seem to understand what the KWIK framework is. In the details of the paper, is a brief highlight of the KWIK conditions for a learning algorithm, which go as follows (I paraphrase):

All predictions must be accurate (assuming a valid hypotheses class)
However the learning algorithm may also return $\perp$, which indicates that it cannot yet predict the output for this input.

A quick Google search brought me to this paper from ICML 2008, but it is a little difficult to comprehend without a detailed read.
Could someone please help me understand what the KWIK framework is, and what implication does it have for a learning algorithm to satisfy KWIK conditions? An explanation that starts at simple and goes to fairly advanced discussions is appreciated.
","['reinforcement-learning', 'terminology', 'papers']",
"What is the meaning of ""easy negatives"" in the context of machine learning?","
What does the term ""easy negatives"" exactly mean in the context of machine learning for a classification problem or any problem in general?
From a quick google search, I think it means just negative examples in the training set.
Can someone please elaborate a bit more on why the term ""easy"" is brought into the picture?
Below, there is a screenshot taken from the paper where I found this term, which is underlined.

","['machine-learning', 'classification', 'terminology', 'papers', 'object-detection']","
OK, I think I understood what this means.
Hard and easy negatives are the ones that have relatively large and small values for the loss function, respectively.
"
Is Q-Learning suitable for time-dependent spaces?,"
Many Q-learning techniques have been developed to capture discrete state(observation), actions like a robot in a grid world, and even continuous (state or action) spaces. But I am wondering how we can model the states/space in a time-dependent environment. Please, consider the following example:
There is one smartphone (client) and five compute servers that are addressing/serving many clients (smartphones) at the same time. The smartphone transfers some raw data (e.g, sensor data) to one of those five servers (e.g., every t seconds) and gets the results. Suppose the server computes the stress-level of the client in real-time based on the collected data. 
Now, a q-learning agent should be deployed to the smartphone to be able to select the best server with minimum response time (i.e., the goal is to minimize the execution/response time). Note that servers are serving different clients and their load is a function of time and varies from time to time.  
So in the above scenario, I am wondering what would be our ""states"" and how we can model the ""environment""? 
","['reinforcement-learning', 'q-learning']",
"Reinforcement Learning (and specifically REINFORCE algorithm) for one-round ""games""","
I'm interested about using Reinforcement Learning in a setting that might seem more suitable for Supervised Learning. There's a dataset $X$ and for each sample $x$ some decision needs to be made. Supervised Learning can't be used since there aren't any algorithms to solve or approximate the problem (so I can't solve it on the dataset) but for a given decision it's very easy to decide how good it is (define a reward). 
For example, you can think about the knapsack problem - let's say we have a dataset where each sample $x$ is a list (of let's say size 5) of objects each associated with a weight and a value and we want to decide which objects to choose (of course you can solve the knapsack problem for lists of size 5, but let's imagine that you can't). For each solution the reward is the value of the chosen objects (and if the weight exceeds the allowed weight then the reward is 0 or something). So, we let an agent ""play"" with each sample $M$ times, where play just means choosing some subset and training with the given value. 
For the $i$-th sample the step can be adjusted to be:
$$\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(a|x^i)v$$ 
for each ""game"" with ""action"" $a$ and value $v$.
instead of the original step:
$$\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(a_t|s_t)v_t$$
Essentially, we replace the state with the sample.
The issue with this is that REINFORCE assumes that an action also leads to some new state where here it is not the case. Anyway, do you think something like this could work?
","['deep-learning', 'reinforcement-learning']","
This seems like a multi-armed bandit problem (no states involved here). I had the same problem some times ago and I was advised to sample the output distribution M times, calculate the rewards and then feed them to the agent, this was also explained in this paper Algorithm 1 page 3 (but different problem & different context). I honestly don't know if this will work for your case. You could also take a look at this example.
"
Cannot fine-tune L2-regularization parameter,"
I have a data set of 1600 examples. I am using 1280 (80%) for training, 160 (10%) for testing, and 160 (10%) for validation. The training goes one of two ways no matter how I fine-tune the L2 parameter:
1) The validation and training error converge, albeit around 75% error
2) The training error settles to around 0%, but the validation error stays around 75%
I don't think my network is too large either. I have trained networks with two hidden layers, both with the same number of nodes as the input. I also tried dropout layers and that did not seem to help. 
Does this just mean that I need to add more training examples? Or how do I know that I have reached the limitations of what I am having the network learn?
",['regularization'],
How is an architecture composed of a second model that validates the first one called in machine learning?,"
I have a mix of two deep models, as follows:
if model A is YES --pass to B--> if model B is YES--> result = YES
if model A is NO ---> result = NO

So basically model B validates if A is saying YES. My models are actually the same, but trained on two different feature sets of same inputs.
What is this mix called in machine learning terminology? I just call them master/slave architecture, or primary/secondary model.
","['machine-learning', 'deep-learning', 'terminology', 'ensemble-learning']","
Not in terms of models, but there is a terminology called 'Hierarchical learning', wherein if your model has a task to classify disease, then, If it detects a presence of a disease (disease/ no disease), then it proceeds to further classify a disease(class A/B/C/...). Else it does not proceed.
This technique of hierarchical learning is very common amongst supervised learning tasks.
Now according to your question, you have two models and I assume that they have different tasks and provide a binary outcome(yes/no). Here, you can call it as 'Multitask learning', where the output of task1 is given to task2 for processing.
If task1 detect the presence of disease, then task2 classifies disease into various classes / or segment it / localize it etc.
"
Why is probability that at least one hypothesis out of $k$ being consistent with $m$ training examples $k(1- \epsilon)^m$?,"
My question is actually related to the addition of probabilities. I am reading on computational learning theory from Tom Mitchell's machine learning book. 
In chapter 7, when proving the upper bound of probabilities for $\epsilon$ exhausted version space (theorem 7.1), it says that the probability that at least one hypothesis out of the $k$ hypotheses in the hypotheses space $|H|$ being consistent with m training examples is at most $k(1- \epsilon)^m$. 
I understand that the probability of a hypothesis, $h$, consistent with m training examples is $(1-\epsilon)^m$. However, why is it possible to add the probabilities for $k$ hypotheses? And might the probability be greater than 1 in this case?
","['machine-learning', 'proofs', 'probability', 'computational-learning-theory', 'statistics']",
Is there a theory that captures the following ideas?,"
A big class of problems that are relevant in today's society are full of uncertainty and are also sometimes computationally intractable. Along our lives we come to realize that we are solving the same type of problem multiple times, sometimes with different strategies and mixed results. I would like to close in on three main problem types: pattern recognition, regression and density estimation.
An agent (computer program or even a human) that identifies the type of a problem and applies a systematic procedure for finding its solution. A solution is understood in the classical sense for each of the problem types, thus, the solution does not have to be a global optima. This procedure must be implementable.
Bonus points

Uses metadata about the problem itself to 'gain insight' about the nature of the problem.
Verifies that its solution is correct in some sense.
The types or classes of problems can be expanded later on.
Works with very limited resources.
Works with little information about the problem, or with ""small data"".

So far, I've found Statistical Learning theory and Bayesian Inference as candidates that implement some of those ideas, but I was wondering if there's something else out there or I just need to take the best of both of those worlds.
","['machine-learning', 'reference-request', 'algorithm-request']",
What are the best classifiers for this type of data?,"
I would like to classify a dataset Credit Scoring, which is composed of 21 attributes, some of them are numeric and others are boolean.
For the output, I want to know if they have a good or bad credit based on those attributes, without calculating any numeric value for the credit score.
I am using Weka for this task. However, I am not sure what are the best/ideal classifiers for that kind of datasets.
Anyone here can put me in the right direction?
","['machine-learning', 'ai-design', 'classification']","
Well, it depends on the structure of the data. The best way is to try all the intelligent models like naive bayes, random forest, svm with different parameters by grid search. There is no model works best all the time for classification. However,  neural network (named Multilayer Perceptron on weka) is supposed to be better if it is set correctly.
"
Building a spell check model,"
I have customer review texts. The data consists of the the raw and manually corrected texts of the reviews. I have aligned these pairs by using similarity algorithms and matched the words on them. Since there are some mis-matched words pairs, I have eliminated the pairs under a threshold value for their counts.
Now there are raw and corrected word pairs. What kind of machine learning model can I build for spellcheck by using the data mentioned as well?
","['deep-learning', 'natural-language-processing']",
How should I define the loss function for a multi-object detection problem?,"
I'm trying to create a text recognition project using CNN. I need help regarding the text detection task. 
I have the training images and bounding box details for them. But I'm unable to figure out how to create the loss function. 
Can anyone help me by telling how to take the output from the CNN model and compare it to the bounding box labels?
","['deep-learning', 'convolutional-neural-networks', 'classification', 'image-recognition', 'optical-character-recognition']","
Okay so your CNN model is taking an Image and outputting the bounding boxes for them. That means the last layer of CNN model must be having four outputs which are generating real numbers. This is a regression problem.
In that case, you can take L1 loss (mean absolute error) or L2 loss (mean square error) as your loss function. I have created a similar project, I have used L1 loss.
Suppose your input image is x and predicted_bb is the model output and real_bb is your original bounding box for image x. Then you should proceed as follows
predicted_bb = model(x)

# CALCULATE LOSS BETWEEN THE ACTUAL & PREDICTED BB COORDINATES

loss_bb = torch.nn.functional.l1_loss(predicted_bb, real_bb, reduction=""none"").sum(1)

# SET GRADIENTS TO ZERO

optimizer.zero_grad()

# BACKPROPOGATE THE LOSS

loss.backward()

For tensorflow keras
predicted_bb = Dense(4, activation='relu')(x)

model = Model(inputs = image_input, outputs = [predicted_bb])
model.compile(loss=['mae'], optimizer='adam', metrics =['accuracy'])

"
What is the difference between the state transition of an MDP and an action-value?,"
Let's say we have MDP where we have a state transition matrix. 
How is this state transition different from action value in reinforcement learning? Is the state transition in MDP stochastic transition, meaning transition to some other state without taking any action?
","['reinforcement-learning', 'comparison', 'markov-decision-process', 'value-functions']",
How are the classical MDP and the object-oriented MDP views different?,"
I've been reading the attached paper - which aims to model entities in the world as objects, including the learning agent itself! 
To say the least, the goal is to navigate through what seems like a maze (path-planning problem) - and drop off passengers in desired destinations, while avoiding walls in the map of the world (5x5 grid for now). The objects involved are, a taxi, passengers, walls and a destination. 
Now, a particular paragraph says the following:

""Whereas in the classical MDP
  model, the effect of encountering walls is felt as a property of specific locations in the grid, the OO-MDP view is that wall interactions are the same regardless of their location. As such, agents’ experience can transfer gracefully throughout the state space.""

What does this mean? How are the classical MDP and the object-oriented MDP views different? 
I can't make sense of the above extract, at all. Any help would be appreciated!
P.S. I did not consider posting parts of the extract as separate questions since my problem has more to do with understanding the extract as a whole which inevitably relies on understanding the parts. 
","['reinforcement-learning', 'comparison', 'markov-decision-process', 'papers']",
What are preferences and preference functions in multi-objective reinforcement learning?,"
In RL (reinforcement learning) or MARL (multi-agent reinforcement learning), we have the usual tuple:
(state, action, transition_probabilities, reward, next_state)

In MORL (multi-objective reinforcement learning), we have two more additions to the tuple, namely, ""preferences"" and ""preference functions"". 
What are they? What do we do with them? Can someone provide an intuitive example? 
","['reinforcement-learning', 'terminology', 'definitions', 'multi-objective-rl']","
In MORL the reward component is a vector rather than a scalar, with an element for each objective. So if we are using a multiobjective version of an algorithm like Q-learning, the Q-values stored for each state-action pair will also be vectors.
Q-learning requires the agent to be able to identify the greedy action in any state (the action expected to lead to the highest long-term return). For scalar rewards this is easy, but for vector values it is more complicated as one vector may be higher for objective 1, while another is higher for objective 2, and so on.
We need a means to order the vector values in terms of how well they meet the user's desired trade-offs between the different objectives. That is the role of the preference function and preferences. The function defines a general operation for either converting the vector values to a scalar value so they can be compared, or for performing some sort of ordering of the vectors (some types of orderings such as lexicographic ordering can't readily be defined in terms of scalarisation). So, for example, our preference function might be a weighted sum of the components of the vector. The preferences specify the parameters of the preference function which define a specific ordering (i.e. based on the needs of the current user). So, in the case of a weighted sum for the preference function, the preferences would be specified in terms of the values of the weights.
The choice of preference function can have implications for the types of solutions which can be found, or for whether additional information needs to be included in the state in order to ensure convergence.
I'd suggest you read the following survey paper for an overview of MORL (disclaimer - I was a co-author on this, but I genuinely think it is a useful introduction to this area)

Roijers, D. M., Vamplew, P., Whiteson, S., & Dazeley, R. (2013). A survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research, 48, 67-113.

"
Does bottleneck size matter in Disentangled Variational Autoencoders?,"
I suppose that picking an appropriate size for the bottleneck in Autoencoders is neither a trivial nor an intuitive task. After watching this video about VAEs, I've been wondering: Do disentangled VAEs solve this problem?
After all, if the network is trained to use as few latent space variables as possible, I might as well make the bottleneck large enough so that I don't run into any issues during training. Am I wrong somewhere?
","['variational-autoencoder', 'latent-variable', 'bottlenecks']",
Using AI to find the correct set of object/numbers based on previous data,"
There are 11 objects of which 4 are ""Bad"" objects. So there are 7 ""Good"" objects.
You have to choose as many Good objects before proceeding to another set of objects of a different sequence. 
How would you train an AI that predicts the position of the Good objects based of previous sets of data?
","['machine-learning', 'reinforcement-learning']",
Why does it make sense to study MDPs with finite state and action spaces?,"

In the standard Markov Decision Process (MDP) formalization of the reinforcement-learning (RL) problem (Sutton & Barto, 1998), a decision maker interacts with an environment consisting of finite state and action spaces.

This is an extract from this paper, although it has nothing to do with the paper's content per se (just a small part of the introduction).
Could someone please explain why it makes sense to study finite state and action spaces? 
In the real world, we might not be able to restrict ourselves to a finite number of states and actions! Thinking of humans as RL agents, this really doesn't make sense.
","['reinforcement-learning', 'markov-decision-process']","
Note: I assume you mean, countable Action and State Sets by 'Finite'.
MDP(s) are not exclusive to finite spaces only. They can be used in Continuous/uncountable sets of Action and States too.
Markov Decision Process (MDP) is a tuple $(\mathcal S, \mathcal A, \mathcal P^a_s, \mathcal R^a_{ss'}, \gamma, \mathcal S_o)$ where $\mathcal S$ is a set of States, $\mathcal A$ is the set of actions, $\mathcal P_{s}^a: \mathcal A \times \mathcal S \rightarrow [0, 1]$ is a function that denotes Probability distribution over the states if action $a$ is execuited at state $s$. [1][2]
Where, Q-function is defined as:
$$ Q^\pi (s,a) = \mathbb E_\pi \left [ \sum \limits_{t=0}^{+\infty} \gamma(t)r_t | s_o = s, a_o = a \right] \tag{*}$$
Note that $r_t$ is just special case of Reward function $\mathcal R^a_{ss'}$. 
Now, if states and actions are discrete, then, the Q-Table Method[3] which is a state-action matrix helps us to evaluate $Q$ function and optimize efficiency.
Whereas, in cases where the state/action sets are infinite or continuous, Deep Networks are preferred to Approximate $Q$ function. [4].
Q-Learning is Off-Policy method, doesn't require $\pi$ policy function

References:

R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press,
1998.
Alborz Geramifard, Thomas J. Walsh, Stefanie Tellex, Girish Chowdhary, Nicholas Roy and Jonathan P. How. A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning. Foundations and Trends (R) in Machine Learning
Vol. 6, No. 4 (2013) 375–454
Andre Violante. Simple Reinforcement Learning: Q-learning, Create a q-table, https://towardsdatascience.com, 2019.
Alind Gupta. Deep Q-Learning, Deep Q-Learning, https://www.geeksforgeeks.org/deep-q-learning/, 2020.


Edit: I'd like to thank @nbro for editing suggestions.
"
How MSE should be appliead with multi target deep network?,"
I'm having a problem understanding how the MSE should be used when working with a multidimensional target, e.g 3 dimensiones. (My outputs are continuois values, not categorical)
Let us say I have a batch size of 2, to make it simple; I pass my input in the network and my y_pred would be a 2x3 tensor.
The same happens for y_true, 2x3 itself.
Now the thing I'm not sure of: I take first the difference, diff = y_true - y_pred; this maintains the dimension.
Now, for MSE, I square diff, obtaining again a 2x3 tensor, is that right ?
Now the tricky part (for me): I have to Mean. Which one should I consider:
Mean all the (six) values obtaining thus a scalar ? But in this case I do not understand how the backpropagation would make better on specific targets 
Mean by rows, i.e, obtaining a 2x1 tensor, so that I have a mean for each example ? Also here I cannot get how the optimization would work
Mean by columns, i.e, obtaining a 1x3 tensors, where I obtain thus an ""error"" for each target? This seems the more logical to me, but I'm not so sure.
Hope this is clear 
","['deep-learning', 'objective-functions', 'regression', 'loss', 'mean-squared-error']",
Are there any good resources (preferably books) about techniques used for entity extraction?,"
Given some natural language sentences like

I would like to talk to Mr. Smith

I would like to extract entities, like the person ""Smith"".
I know that frameworks, which are capable of doing so (f. e. RASA or spaCy), exist, but I would like to dive deeper and understand the theory behind all this.
At university, I learned a few of the basic models like CRF's or SVM's used for this task, but I wonder if there are any good resources (preferably books) about this topic.
","['natural-language-processing', 'resource-request', 'books', 'named-entity-recognition']",
Why do we set offset (0.5) in single shot detector?,"
In the paper SSD: Single Shot MultiBox Detector, under section 2.2 - (4), why do we add an offset of 0.5 to x, y in generating the anchor boxes across feature maps? 
","['computer-vision', 'object-detection', 'papers']",
Class of functional equations that backpropagation can solve,"
There is a theorem that states that basically a neural network can approximate any function whatsoever. However, this does not mean that it can solve any equation. I have some notes where it states that backpropagation allows us to solve problems of the following kind 
$$ F(x_i, t) = y_i $$
Can someone point me to what exactly this means?
","['neural-networks', 'backpropagation']",
What is the point of using 1D and 2D convolutions with a kernel size of 1 and 1x1 respectively?,"
I understand the gist of what convolutional neural networks do and what they are used for, but I still wrestle a bit with how they function on a conceptual level. For example, I get that filters with kernel size greater than 1 are used as feature detectors, and that number of filters is equal to the number of output channels for a convolutional layer, and the number of features being detected scales with the number of filters/channels.
However, recently, I've been encountering an increasing number of models that employ 1- or 2D convolutions with kernel sizes of 1 or 1x1, and I can't quite grasp why. It feels to me like they defeat the purpose of performing a convolution in the first place.
What is the advantage of using such layers? Are they not just equivalent to multiplying each channel by a trainable, scalar value?
","['convolutional-neural-networks', 'convolution', 'filters', 'convolutional-layers', 'fully-convolutional-networks']","
Traditional CNNs used for image classification (and related tasks) are composed of 1 or more fully connected layers (FCs), after the convolutional and pooling layers, which take as input the features extracted from the convolutional and pooling layers, in order to perform classification or regression.
One problem with FCs in CNNs is that the number of parameters can be very big, with respect to the number of parameters in the convolutional layers.
There are tasks, such as image segmentation, where this big number of parameters is not really needed. An example of a neural network that does not make use of fully connected layers but only uses convolutions, downsampling (aka pooling), and upsampling operations is the U-net, which is used for image segmentation. A neural network that only uses convolutions is known as a fully convolutional network (FCN). Here I give a detailed description of FCNs and $1 \times 1$, which should also answer your question.
In any case, to answer your question more directly, $1 \times 1$ convolutions have been used for image segmentation tasks, i.e. dense classification tasks, i.e. tasks where you want to assign a label to each pixel (or a group of pixels), as opposed to sparse classification tasks such as image classification (where the goal is to assign 1 label to the whole image). Moreover, in comparison with FC layers, they have fewer parameters and, more importantly, the number of parameters in an FCN does not depend on the dimensions of the images (as in the case of traditional CNNs), which is a good thing (especially, when your images have high resolutions), but typically it depends on the number of kernels and instances (of objects), in the case of instance segmentation.
The FCN paper discusses this reduction of the number of parameters (and computation time), so you should probably read this paper for more details.
"
"Does the ""lowest layer"" refer to the first or last layer of the neural network?","
People sometimes use 1st layer, 2nd layer to refer to a specific layer in a neural net. Is the layer immediately follows the input layer called 1st layer?
How about the lowest layer and highest layer?
","['neural-networks', 'machine-learning', 'deep-learning', 'terminology']","
Lowest layer generally refers to the layer closest to the input. This comes from the idea that layers closer to the input represent low-level features such as gradients and edges, while layers closer to the output represent high-level features such as parts and objects.
"
Is an SVM the same as a neural network without a hidden layer?,"
A neural network without a hidden layer is the same as just linear regression.  
If I then use squared hinge loss and encoporate the l2 regularisation term, is it fair to then call this network the same as a linear SVM?  
Going by this assumption, then if I need to implement a multiclass SVM, i can just have n output nodes (where n is the number of classes). Would this then be equivalent to having n number of SVMs, similar to a one-vs-rest method?
If I then wanted to encoporate a kernel into my SVM, could I then use an activation function or layer prior to the final output nodes (where I compute loss and add regularisation) which would then transfer this data into another feature plane the same as that of an SVM kernel? 
This is my current hunch, but would like some confirmation or correction where my understanding is incorrect. 
","['neural-networks', 'machine-learning', 'support-vector-machine']",
"If two objects are too close to each other, would an object detector do a poor job of correctly classifying them?","
Suppose we have an object detector that is trained to detect $20$ products. If two objects are too close to each other, in general, would an object detector do a poor job of correctly classifying them? If they were far apart in the scene, would the object detector to a better job of correctly classifying them? 
","['computer-vision', 'object-detection']","
As you ask, ""in general..."", I will answer generally, however this changes a lot from model to model and the way they handle close objects.
In general, yes, they would do a poor job detecting very close objects, switch to segmentation models for that (for class or better, instance segmentation).
In general, objects detectors learn to tell an object from other based in 2 criterion:

Intersection over union: for object of the same class
Class probability: for objects of different class

So, if two objects of the same class are very close, the 2 detected bounding boxes will be highly overlapping, then, the Non Maximal Suppression filter will remove one of them. This is where objects detector, in general, perform worse.
Similarly, if two objects belong to different classes the 2 detected bounding boxes will be highly overlapping but the NMS filter won't remove them (again, in general, NMS is set only for same class objects). However when 2 objects are very close, there is a high chance they are partially occluded. Objects detectors, in general, don't handle occlusions very well.
So, in conclusion, objects detectors will perform better detecting far-away objects.
"
Does net with ReLU not learn when output < 0?,"
The derivative of ReLU is 0 if its output is lower than 0 -  $d ReLU(x)/dReLU$ is $0$ if $x < 0$. Let's denote some net's output by $Out$, so if this net's last layer is ReLU then we get that $dOut/dReLU$ is $0$ if $Out < 0$. Subsequently, for every parameter $p$ in the net we would get that $dOut/dp$ is $0$. Does that mean that for every sample $x$ such that $Out(x) < 0$ the net doesn't learn at all from that sample since the derivative for each parameter is $0$?
","['deep-learning', 'backpropagation', 'relu']","
There is no benefit to using ReLU as the output activation of a neural network. As you said, the network will ignore training labels below zero and it will train on labels above zero as if no output activation were present. However, the problem you're describing can also occur for individual units of hidden layers, where ReLU activations are common. This is known as the dead ReLU problem. In practice, it is rarely a problem but it can be avoided with smooth rectifiers like ELU and Swish. Another interesting ideas is CRelu, which concatenates both positive and negative parts of the pre-activation, resulting in twice as many outputs, half of which always receive a non-zero gradient.
"
How to estimate a behavior policy for off-policy learning based on data?,"
I have a dataset which includes states, actions, and reward. The dataset includes information on the transition, i.e., $p(r,s' \mid s,a)$. 
Is there a way to estimate a behavior policy from this dataset so that it can be used in an off-policy learning algorithm?
","['reinforcement-learning', 'policies', 'off-policy-methods']","
You can simply train a policy from the inputs to predict the actions in your dataset. You can use the cross entropy loss for this, i.e. maximize the the log probability that the policy assigns to the actions in the data set when given the corresponding inputs. This is called behavioral cloning.
The result is an approximation of the behavioral policy that lets you compute probability densities of actions. It is an approximation because the dataset is finite, and even more so when you restrict the learned policy to a class of distributions, e.g. Gaussians.
"
How to work on different models for a given problem?,"
I am working on the MNIST data on my own. The idea is to use different values for the number of hidden layers, number of nodes in a given layer, etc. How do you organize these things while you are working on creating a model for a problem? DO you do everything in one code file or you use different code files for choosing the best?
","['deep-learning', 'models', 'hidden-layers']","
it seems to me that you are talking about hyperparameter tuning and effect of hyperparameters on the network in general. If you are working with tensorflow, I recommend you to look into tensorboard.
Hands-on TensorBoard can be a good starting point.
"
A model for each sub-problem vs one model for the whole problem,"
Let's say one wants to use a neural net to learn some function $g(x)$. Let's say that we know that $g$ is a combination of two functions (or two sub-problems), $g(x)=f_2(f_1(x))$, and that we have two datasets 

composed of $x$ samples and their corresponding $g(x)$ labels, and 
composed of $x$ samples and their corresponding $f_1(x)$ labels. 

Should we use two nets, one to learn the mapping from $x$ samples to $f_1(x)$ using dataset 1 and another net to learn the mapping from $f_1(x)$ to $g(x)$ (note that we can build a dataset composed of $f_1(x)$ samples and $g(x)$ labels with the trained net), or just one net to learn mappings from $x$ to $g(x)$ using dataset 1? 
Intuitively, the first option seems to be better since we take advantage of our knowledge that $f_1$ is a ""sub-problem"" of $g$. 
","['machine-learning', 'deep-learning', 'ai-design', 'computational-learning-theory']","
The tendency in literature in the last years (at least for computer vision problems) seems to point towards the single model option (I'll try to remember to come back and add some links to papers mentioning this when I find them), although this IMO is really data- and problem-dependent. 
In your case, I would set up a network for the mapping $x$ to $g(x)$, with a training-only auxiliary loss calculated on the mapping $x$ to $f1(x)$ and compare this with a model trained only on ""$x$ to $g(x)$"".
"
How to use pre-trained BERT to extract the vectors from sentences?,"
I'm trying to extract the vectors from the sentences. Spent soo much time searching for the pre-trained BERT models but found nothing.

Is it possible to get the vectors using pre-trained BERT from the data?

","['natural-language-processing', 'bert', 'pretrained-models']",
Should neural nets be deeper the more complex the learning problem is?,"
I know it's not an exact science. But would you say that generally for more complicated tasks, deeper nets are required?
","['neural-networks', 'deep-learning', 'architecture']","
Deeper models can have advantages (in certain cases)
Most people will answer ""yes"" to your question, see e.g. Why are neural networks becoming deeper, but not wider? and Why do deep neural networks work well?. 
In fact, there are cases where deep neural networks have certain advantages compared to shallow ones. For example, see the following papers 

The Power of Depth for Feedforward Neural Networks (2016) by Ronen Eldan and Ohad Shamir
Benefits of depth in neural networks (2016) by Matus Telgarsky.
Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks (2017) by Safran and Shamir
Optimal approximation of piecewise smooth functions using deep ReLU neural networks (2018) by Petersen and Voigtlaender

What about the width?
The following papers may be relevant

Wide Residual Networks (2017) by Sergey Zagoruyko and Nikos Komodakis
The Expressive Power of Neural Networks: A View from the Width (2017) by Zhou Lu et al.

Bigger models have bigger capacity but also have disadvantages
Vladimir Vapnik (co-inventor of VC theory and SVMs, and one of the most influential contributors to learning theory), who is not a fan of neural networks, will probably tell you that you should look for the smallest model (set of functions) that is consistent with your data (i.e. an admissible set of functions).  
For example, watch this podcast Vladimir Vapnik: Statistical Learning | Artificial Intelligence (AI) Podcast (2018), where he says this. His new learning theory framework based on statistical invariants and predicates can be found in the paper Rethinking statistical learning theory: learning using statistical invariants (2019). You should also read ""Learning Has Just Started"" – an interview with Prof. Vladimir Vapnik (2014).
Bigger models have a bigger capacity (i.e. a bigger VC dimension), which means that you will more likely overfit the training data, i.e., the model may not really be able to generalize to unseen data. So, in order not to overfit, models with more parameters (and thus capacity) will also require more data. You should also ask yourself why people use regularisation techniques. 
In practice, models that achieve state-of-the-art performance can be very deep, but they are also computationally inefficient to train and they require huge amounts of training data (either manually labeled or automatically generated). 
Moreover, there are many other technical complications with deeper neural networks, for example, problems such as the vanishing (and exploding) gradient problem. 
Complex tasks may not require bigger models
Some people will tell you that you require deep models because, empirically, some deep models have achieved state-of-the-art results, but that's probably because we haven't found cleverer and more efficient ways of solving these problems. 
Therefore, I would not say that ""complex tasks"" (whatever the definition is) necessarily require deeper or, in general, bigger models. While designing our models, it may be a good idea to always keep in mind principles like Occam's razor!
A side note
As a side note, I think that more people should focus more on the mathematical aspects of machine learning, i.e. computational and statistical learning theory. There are too many practitioners, who don't really understand the underlying learning theory, and too few theorists, and the progress could soon stagnate because of a lack of understanding of the underlying mathematical concepts.
To give you a more concrete idea of the current mentality of the deep learning community, in this lesson, a person like Ilya Sutskever, who is considered an ""important and leading"" researcher in deep learning, talks about NP-complete problems as if he doesn't really know what he's talking about. NP-complete problems aren't just ""hard problems"". NP-completeness has a very specific definition in computational complexity theory!
"
What happens to the channels after the convolution layer?,"
I wonder what happens to the 'channels' dimension (usually 3 for RGB images) after the first convolution layer in CNNs? 
In books and other sources, it is always said that the depth of the output from convolutional layers is the number of kernels (filters) in that layer. 
But, if the input image has 3 channels and we convolve each of them with $K$ kernels, shouldn't the depth of the output be $K * 3$? Are they somehow 'averaged' or in other way combined with each other?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'filters']",
Should batch-normalization/dropout/activation-function layers be used after the last fully connected layer?,"
I am using the following architechture:
3*(fully connected -> batch normalization -> relu -> dropout) -> fully connected

Should I add the batch normalization -> relu -> dropout part after the last fully connected layer as well (the output is positive anyway, so the relu wouldn't hurt I suppose)? 
","['neural-networks', 'deep-learning', 'architecture']","
You don't put batch normalization or dropout layers after the last layer, it will just ""corrupt"" your predictions. They are intended to be used only within the network, to help it converge and avoid overfitting.
BTW even if your fully connected layer's output is always positive, it would have positive and negative outputs after batch normalization. But as I said you shouldn't have that layer there anyway.
"
How come a detection works after global average pooling 2D?,"
I use an off-the-shelf convolutional neural network, where at the end of the convolutional part, the depth of the last convolutional layer is expanded and then its 2D average is computed (such that for a tensor of say 8x8x512, you get its 2D average, which is of 1x512). It is a commonly used operation in deep networks, called Global Average Pooling 2D.
The only tensor that is input to the fully-connected part is that 2D averaged 1x512 tensor, i.e., a tensor that should not preserve the 2D information. Yet, my fully-connected last layer neurons, which have been trained to predict the 2D location of objects, work very well. 
I thought about it for a long time and couldn't find any convincing explanation how come the network preserved the 2D information in the averaged tensor. 
Any idea?
","['convolutional-neural-networks', 'computer-vision', 'object-detection']",
How can I approximate a function that determines the priority of objects?,"
I am facing the following supervised learning problem:
An object is fully characterized by its position in $R^n$. There are $m$ objects. There are fully observable (i.e. their positions are always known). 
At each time step $t$, exactly one of these objects is activated. Activation is fully observable, i.e. the index $a_t$ ($a_t \in [1,m]$) of the object activated at time $t$ is known. 
We know that, under the hood, activation works this way:  there is a priority function $f$ ($f: R^n \to R$ ), which computes, for each time step, the priority score of each object. The object for which the priority score was the highest is activated.
The goal is to find (the approximation of) one of the possible priority functions that would match a given data-set. A data-set is of size $(m*n+1)*t$ ($m$ positions of dimension $n$, plus the index of the activated object, over $t$ time steps). 
As an example, if it turns out there is a hidden fixed beacon, and at each time step $t$ the object the closest to the beacon is activated, then a possible function would be $f(o_{it})=1/d_{it}$, where $d_{it}$ is the distance between the beacon and the object $o_i$ at time $t$.
(If several objects have the same highest priority score, then only one of them is activated, selected randomly).
The function found by the algorithm may be parametric and encoded by a neural network, if this is applicable.
Is there a method for finding one such function ?
","['machine-learning', 'ai-design', 'datasets', 'supervised-learning']",
How do we determine the membership functions and values for this problem?,"
The goal of our project is to identify the quality of a grain and we have two values, A and B. 
A can take the values low  L, medium M and high H. And B can take the values of Low, Medium and High (L,M,H). Specifically, the ranges for A are -> 5-10 (low), 10-15 (medium), 15-20 (high). And the ranges for B are -> 50-75 (low), 75-90 (medium), 90-110 (high)
The output for these are bad B, average A and good G.

How do we determine the membership functions and values for this? 
We want to write Python code for the fuzzy system, but we are beginners and have no idea how to go about this. Any help would be appreciated.
","['ai-design', 'fuzzy-logic']",
What are the differences between 1-step SARSA and SARSA?,"
SARSA is on-policy, while n-step SARSA is off-policy. But when n = 1, is it like an off-policy version of SARSA? Any similarity and difference between 1-step SARSA and SARSA?
","['reinforcement-learning', 'comparison', 'off-policy-methods', 'sarsa', 'on-policy-methods']",
How is the log-derivative trick of a trajectory derived?,"
I am looking at this formula which breaks down the gradient of $P(\tau |\theta)$ the first part is clear as is the derivative of $\log(x)$, but I do not see how the first formula is rearranged into the second.

","['reinforcement-learning', 'math', 'policy-gradients', 'calculus']",
What types of AI agents are Djikstra's algorithm and Prim's Minimum Spanning Tree algorithm?,"
From the perspective of the type of AI Agents, I would like to discuss Prim's Minimum Spanning Tree algorithm and Dijkstra's Algorithm.
Both are model-based agents and both are ""greedy algorithms"".
Both have their memory to store the history of vertices and their path distance. Prim's is more greedy than Dijkstra's algorithm whereas Dijkstra's algorithm is more efficient than Prim's.
Can we say that Dijkstra's algorithm is a utility-based agent, whereas Prim's is a goal-based agent, with the justification that Prim's is more goal-oriented as compared to finding the optimum (shortest) path?
","['intelligent-agent', 'goal-based-agents', 'utility-based-agents', 'dijkstras-algorithm', 'prims-algorithm']",
What does the Fourier transformed image mean?,"
I have been trying to figure out what the Fourier transformed image represents. I am aware of Fourier transformation in general, but I can't explain myself the image it forms after transformation. 
In the given image, what does the outlined white sort of lines mean?

","['computer-vision', 'image-processing', 'fourier-transform']","
Have a look at this explanation of the two dimensional Fourier Transform applied to an image.
The way I read it is that it somewhat mirrors the diagonal structure of the 'X', as that is where most of the 'image energy' is, so that are the lines you highlighted. Because the image is fairly uniform in colour, most of the components are low-frequency (ie close to the centre of the 2DFT image).
Disclaimer: I'm not really that familiar with a 2D version either, as I mainly have experience with using FFTs in speech processing.
"
Genetic Algorithm Python Snake not improving,"
So, i have created Snake game using Pygame and Python. Then i wanted to create an AI with Genetic algorithm and a simple NN to play it. Seems pretty fun, but things aren't working out.
This is my genetic algorithm:
def calculate_fitness(population):
    """"""Calculate the fitness value for the entire population of the generation.""""""
    # First we create all_fit, an empty array, at the start. Then we proceed to start the chromosome x and we will
    # calculate his fit_value. Then we will insert, inside the all_fit array, all the fit_values for each chromosome
    # of the population and return the array
    all_fit = []
    for i in range(len(population)):
        fit_value = Fitness().fitness(population[i])
        all_fit.append(fit_value)
    return all_fit


def select_best_individuals(population, fitness):
    """"""Select X number of best parents based on their fitness score.""""""
    # Create an empty array of the size of number_parents_crossover and the shape of the weights
    # after that we need to create an array with x number of the best parents, where x is NUMBER_PARENTS_CROSSOVER
    # inside config file. Then we search for the fittest parents inside the fitness array created by the
    # calculate_fitness function. Numpy.where return (array([], dtype=int64),) that satisfy the query, so we
    # take only the first element of the array and then it's value (the index inside fitness array). After we have
    # the index of the element we just need to take all the weights of that chromosome and insert them as a new
    # parent. Finally we change the fitness value of the fitness value of that chromosome inside the fitness
    # array in order to have all different parents and not only the fittest
    parents = numpy.empty((config.NUMBER_PARENTS_CROSSOVER, population.shape[1]))
    for parent_num in range(config.NUMBER_PARENTS_CROSSOVER):
        index_fittest = numpy.where(fitness == numpy.max(fitness))
        index_fittest = index_fittest[0][0]
        parents[parent_num, :] = population[index_fittest, :]
        fitness[index_fittest] = -99999
    return parents


def crossover(parents, offspring_size):
    """"""Create a crossover of the best parents.""""""
    # First we start by creating and empty array with the size equal to offspring_size we want. The type of the
    # array is [ [Index, Weights[]] ]. If the parents size is only 1 than we can't make crossover and we return
    # the parent itself, otherwise we select 2 random parents and then mix their weights based on a probability
    offspring = numpy.empty(offspring_size)
    if parents.shape[0] == 1:
        offspring = parents
    else:
        for offspring_index in range(offspring_size[0]):
            while True:
                index_parent_1 = random.randint(0, parents.shape[0] - 1)
                index_parent_2 = random.randint(0, parents.shape[0] - 1)
                if index_parent_1 != index_parent_2:
                    for weight_index in range(offspring_size[1]):
                        if random.uniform(0, 1) < 0.5:
                            offspring[offspring_index, weight_index] = parents[index_parent_1, weight_index]
                        else:
                            offspring[offspring_index, weight_index] = parents[index_parent_2, weight_index]
                    break
    return offspring


def mutation(offspring_crossover):
    """"""Mutating the offsprings generated from crossover to maintain variation in the population.""""""
    # We cycle though the offspring_crossover population and we change x random weights, where x is a parameter
    # inside the config file. We select a random index, generate a random value between -1 and 1 and then
    # we sum the original weight with the random_value, so that we have a variation inside the population
    for offspring_index in range(offspring_crossover.shape[0]):
        for _ in range(offspring_crossover.shape[1]):
            if random.uniform(0, 1) == config.MUTATION_PERCENTAGE:
                index = random.randint(0, offspring_crossover.shape[1] - 1)
                random_value = numpy.random.choice(numpy.arange(-1, 1, step=0.001), size=1, replace=False)
                offspring_crossover[offspring_index, index] = offspring_crossover[offspring_index, index] + random_value
    return offspring_crossover

My neural network is formed using 7 inputs:
is_left_blocked, is_front_blocked, is_right_blocked, apple_direction_vector_normalized_x,
snake_direction_vector_normalized_x, apple_direction_vector_normalized_y,snake_direction_vector_normalized_y

Basically if you can go left, front, right, direction to the apple and snake direction.
Then i have an hidden layer with 8 neurons and finally 3 output that indicate left, keep going or right.
The Neural Network forward() is calculate like this:
self.get_weights_from_encoded()
Z1 = numpy.matmul(self.__W1, self.__input_values.T)
A1 = numpy.tanh(Z1)
Z2 = numpy.matmul(self.__W2, A1)
A2 = self.sigmoid(Z2)
A2 = self.softmax(A2)
return A2

where self.__W1 and self.__W2 are the weights from input to hidden layer and then the weights from hidden layer to the output. Softmax(A2) return the index of the matrix[1,3] where the value is the biggest, then i use that index to indicate the direction that my neural network choose. 
This is the config file that contains the parameters:
# GENETIC ALGORITHM
NUMBER_OF_POPULATION = 500
NUMBER_OF_GENERATION = 200
NUMBER_PARENTS_CROSSOVER = 50
MUTATION_PERCENTAGE = 0.2

# NEURAL NETWORK
INPUT = 7
NEURONS_HIDDEN_1 = 8
OUTPUT = 3
NUMBER_WEIGHTS = INPUT * NEURONS_HIDDEN_1 + NEURONS_HIDDEN_1 * OUTPUT

And this is the main:
for generation in range(config.NUMBER_OF_GENERATION):

    snakes_fitness = genetic_algorithm.calculate_fitness(population)

    # Selecting the best parents in the population.
    parents = genetic_algorithm.select_best_individuals(population, snakes_fitness)

    # Generating next generation using crossover.
    offspring_crossover = genetic_algorithm.crossover(parents,
                                                      offspring_size=(pop_size[0] - parents.shape[0], config.NUMBER_WEIGHTS))

    # Adding some variations to the offspring using mutation.
    offspring_mutation = genetic_algorithm.mutation(offspring_crossover)

    # Creating the new population based on the parents and offspring.
    population[0:parents.shape[0], :] = parents
    population[parents.shape[0]:, :] = offspring_mutation

I have 2 problems:
1) I don't see an improvement over the new generations
2) I'm actually running the game inside the for loop, but waiting for all the snake of a generation to die and repeat with the new one is really time consuming. Isn't there a way to launch all or, atleast, more than 1 instance of the game and keep filling the array with the result?
This is Fitness().fitness(population[i])
def fitness(self, weights):
    game_manager = GameManager(weights)
    self.__score = game_manager.play_game()
    return self.__score

This is where it's called inside the for loop
def calculate_fitness(population):
    """"""Calculate the fitness value for the entire population of the generation.""""""
    # First we create all_fit, an empty array, at the start. Then we proceed to start the chromosome x and we will
    # calculate his fit_value. Then we will insert, inside the all_fit array, all the fit_values for each chromosome
    # of the population and return the array
    all_fit = []
    for i in range(len(population)):
        fit_value = Fitness().fitness(population[i])
        all_fit.append(fit_value)
    return all_fit

This the function that launch the game (GameManager(weights)) and return the score of the snake.
This is my first time on AI so this code could be all a mess, don't worry about pointing out what i did wrong, just please don't say ""It's all wrong"" because i won't be able to learn otherwise.
","['python', 'genetic-algorithms']",
"Are there any novel quantum machine learning algorithms that are fundamentally different from ""classical"" ones?","
Generally, if one googles ""quantum machine learning"" or anything similar the general gist of the results is that quantum computing will greatly speed up the learning process of our ""classical"" machine learning algorithms. However, ""speed up"" itself does not seem very appealing to me as the current leaps made in AI/ML are generally due to novel architectures or methods, not faster training.
Are there any quantum machine learning methods in development that are fundamentally different from ""classical"" methods? By this I mean that these methods are (almost*) impossible to perform on ""classical"" computers. 
*except for simulation of the quantum computer of course
","['machine-learning', 'quantum-computing']","

Generally, if one googles ""quantum machine learning"" or anything similar the general gist of the results is that quantum computing will greatly speed up the learning process of our ""classical"" machine learning algorithms.

This is correct. A lot of machine learning methods involve linear algebra, and it often takes far fewer quantum operations to do things in linear algebra than the number of classical operations that would be needed. To be more specific, for a matrix of size $N\times N$, if a classical computer needs $f(N)$ operations to do some linear algebra operation, such as diagonalization which can take $f(N)=\mathcal{O}(N^3)$ operations on a classical computer, a quantum computer would often need only $\log_2 f(N)$ operations, which inn (in and only in) the language of computational complexity theory, means exponential speed-up. The ""only in"" part is there because we have made here an assumption that ""fewer quantum operations"" means ""speed-up"", which for now is something we only know to be true in the world of ""computational complexity theory"".

However, ""speed-up"" itself does not seem very appealing to me as the current leaps made in AI/ML are generally due to novel architectures or methods, not faster training.

I disagree. Take vanilla deep learning for example (without GANs or any of the other things that came up in the last decade). Hinton and Bengio had been working on deep learning for decades, so why did interest in deep learning suddenly start growing so much from 2011-2014 after a roughly monotonic curve from 1988-2010? Not that this rise started before newer advances such as GANs and DenseNet were developed:


Notice also the similarity between the above graph and these ones:

These days pretty much everyone doing deep learning uses GPUs if they have access to GPUs, and what is possible to accomplish is extremely tied to what computing power a group has. I don't want to undermine the importance of new methods and new algorithms, but GPUs did play a big role for at least some areas of machine learning, such as deep learning.

Are there any quantum machine learning methods in development that are fundamentally different from ""classical"" methods?

I think you mean: ""Most quantum machine learning algorithms are simply based on classical machine learning algorithms, but with some sub-routine sped-up by the QPU instead of a GPU -- are there any quantum algorithms that are not based on classical machine learning algorithms, and are entirely different"".
The answer is yes, and more experts might be able to tell you more here.
One thing you might consider looking at is Quantum Boltzmann Machines.
Another things I'll mention is that a child prodigy named Ewin Tang who began university at age 14, discovered at around the age of 17 some classical algorithms that were inspired by quantum algorithms rather than the other way around, and
the comments on the Stack Exchange question Quantum machine learning after Ewin Tang might give you more insight on that. This is related to something called dequantization of quantum algorithms.

By this I mean that these methods are (almost*) impossible to perform on ""classical"" computers. *except for simulation of the quantum computer of course

Unfortunately quantum computers can't do anything that's impossible for classical computers to do, apart from the fact that they might be able to do some things faster. Classical computation is Turing complete, meaning anything that can be computed can be computed on a big enough classical computer.
"
Why are denser layers needed in computer vision neural nets?,"
Many neural net architectures for computer vision tasks use several convolutional layers and then several fully-connected (or dense) layers. While the reasons for using convolutional layers are clear to me, I don't understand why the dense layers are needed. Can't high accuracy be achieved with only convolutional layers?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'architecture']","
Convolutional layers are added in order to extract features from the image (like edges, corners, textures). After extracting those features, you feed them to a fully connected neural network to get the prediction.  
Let's take an example, consider you want to classify the cat's image. But you decided to do this by only using the convolutional layer. So, you feed the image to a convolutional layer. After passing through some layer it extracted the key features from the cat's image and at the end of the layer, you add a single neuron(since we have a single class to predict). Now it's ready to classify it as a cat. But unfortunately, it's misclassified the image. Why? Now let us answer the question. 
Since convolutional layers are not fully connected layers, the neuron added at the last layer is only connected with a handful of neurons from the previous layer. So it misses the key features (encapsulated by some other neurons it is not connected with them) in order to detect this image as a cat. In order to get the prediction right, you have to add a dense layer(a fully connected layer) at the end of the convolutional layer so that it can get all of those extracted features. 
If this does not satisfy you, please, ask the question more specifically and also edit the question because linear layer does not call fully connected layer. 
"
How does the DQN loss from td_targets against q_values make sense?,"
Why td_loss is calculated from (td_targets against q_values)?
Why I am lost is because:

q_values is just the probability of action. It does not have a reward and discount.
td_targets does have rewards + discounts * next_q_values. Somemore next_q_values is next state.

How both td_targets and q_values can minus (or Huber or MSE) to get lost work?
td_error = valid_mask * (td_targets - q_values)
td_loss = valid_mask * td_errors_loss_fn(td_targets, q_values)

td_loss = valid_mask * td_errors_loss_fn(td_targets, q_values)
","['objective-functions', 'dqn']","
First of all, DQN is off-policy learning. That means, you are following the behavior policy(epsilon greedy policy) but still learning about the optimal policy or target policy(greedy policy). Td_target in DQN is the estimation of our current state's optimal action-value function independent of the policy we are following (since we are picking next state's action-value from target policy) and q_values(as you referred) is what you get following the behavior policy. While using this kind of update, you are improving both the behavior policy and the target policy.  
"
Algorithm which learns to select from proposed options,"
My goal is to write a program that automatically selects a routing out of multiple proposed options. 
The data consists out of the multiple proposed options with each the attributes time, costs and if there is a transhipment and also which of the options was selected. 
Example of data:

My idea at the moment is that I have to apply so type of inference to learn which attribute (time, costs, transhipment) has the highest impact on how to choose the best option. But I don't know exactly where to start with this. 
Is there a ""best"" ML algorithm for this? Or how should I approach this?
The dataset currently consists out of 1000 samples in case if this is important. 
Thanks in advance for your responses.
","['algorithm', 'automation', 'inference']",
How can a single sample represent the expectation in gradient temporal difference learning?,"
I was reading the gradient temporal difference learning version 2(GTD2) from rich Sutton's book page-246. At some point, he expressed the whole expectation using a single sample from the environment. But how a single sample can represent the whole expectation. 
I marked this point in this image.

","['machine-learning', 'deep-learning', 'reinforcement-learning', 'math', 'deep-rl']","
In the, presumably final, printed version the last two equal signs are approximations. This is just because over a large amount of weight updates where you have been sampling the expectation will be approximated by Monte Carlo.
"
What is the relationship between PAC learning and classic parameter estimation theorems?,"
What are the differences and similarities between PAC learning and classic parameter estimation theorems (e.g. consistency results when estimating parameters, e.g. with MLE)?
","['machine-learning', 'comparison', 'computational-learning-theory', 'pac-learning', 'statistics']",
How can I sample the output distribution multiple times when pruning the filters with reinforcement learning?,"
I was reading the paper Learning to Prune Filters in Convolutional Neural Networks, which is about pruning the CNN filters using reinforcement learning (policy gradient). The paper says that the input for the pruning agent (the agent is a convolutional neural network) is a 2D array of shape (N_l, M_l), where N_l is the number of filters and M_l = m x h x w (m, l and h are filter dimensions), and the output is an array of actions (each element is 0 (unnecessary filter) or 1 (necessary)) and says in order to approximate gradients we have to sample the output M times (using the REINFORCE algorithm).
Since I have one input, how can I sample the output distribution multiple times (without updating the CNN parameters)? 
If I'm missing something, please, tell me where I'm wrong
","['reinforcement-learning', 'convolutional-neural-networks', 'policy-gradients', 'reinforce']","
I'm not sure what do you mean by one input. The input to the pruning agent is always the same, it's the convolutional layer $W$ of dimension $m \times h \times w$. The layer is taken from baseline model that is pretrained. The input doesn't change it's always the same. The output of the pruning agent is an array of probabilities to prune a specific filter. For example if you have $3$ filters in a layer, the output of the pruning agent will be array of $3$ elements . Let's say its
\begin{equation}
y = [0.1, 0.6, 0.7]
\end{equation}
Each of these elements represents probability of pruning filter $i$ in layer $W$. So $0.1$ would be probability to prune filter $1$, $0.6$ to prune filter $2$ and $0.7$ to prune filter $3$.
Let's say you sample this distribution $2$ times and you get: $[0, 1, 1], [0, 0, 1]$. That means you would make 2 different models from the original baseline model. First model would have 2nd and 3rd filter pruned in layer $W$, and second model would have 3rd filter pruned. The you run those 2 new models on your train and validation set, calculate objective function $R$. Then you update parameters $\theta$ of your pruning agent based on $R$. The original weights of layer $W$ stay untouched. Then you do another inference of the pruning model $\pi$ with updated parameters $\theta$ (the input is still original $W$). You will get another array of probabilities and you keep repeating previous steps that i described until parameters $\theta$ converge. When they converge you make final pruning.
"
How are non-linear surfaces formed in the training of a neural network? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



Desperate trying to understand something for couple of weeks. All those questions are actually one big question.Please help me. Time-codes and screens in my question refer to this great(IMHO) 3d explanation:
https://www.youtube.com/watch?v=UojVVG4PAG0&list=PLVZqlMpoM6kaJX_2lLKjEhWI0NlqHfqzp&index=2
....
Here is the case: Say I have 2 inputs (lets call them X1 and X2) into my ANN. Say X1= persons age and X2=years of education.
1) First question: do I plug those numbers as is or normalize them 0-1 as a ""preprocessing""?
2) As I have 2 weights and 1 bias, actually I am going to plug my inputs to X1*W1+X2*W2=output formula.  This is 2d plane in a 3d space if I am not mistaken(time-code 5:31):

Thus when I plug in my variables, like in regression I will get on a Z axis my output.  So the second question is: am I right up to here?
-----------------From here come  real important couple of  questions.
3) My output (before I plug it into the activation function) is just a simple number, IT IS NOT A PLANE and NOT A SURFACE, but a simple scalar, without any sigh on it coming from 2d surface in a 3d space(though it does come from there). Thus, when I plug this number (which was Z value in a previous step) into the activation function (say sigmoid) my number enters there in to the X axis,  and we get as an output some Y value. As I understand  this operation was totally 2d operation, is was 2d sigmoid and not some kind of 3dsigmoidal surface.
So here is the question: If I am right, why do we see in this movie (and couple of other places) such an explanation?
(time-code 12:55):
4)Now lets say that I was right in the previous step and as an output from the activation function I do get a simple number not a 2d surface and not a 3d one. I just have some number like I had in the very beginning of the ANN as an input (age, education etc). If i want to add another layer of neurons, this very number enters there  as is not telling any one the ""secret"" that it was created by some kind of sigmoid. In this next layer this number is about to take similar transformations as it happened to age and education in a previous layer, it is going to be Xn in just the same scenario: sigmoid(XnWn+XmWm=output) and in the end we will get once again just a number. If I am right, why in the movie they say (time-code 14:50 ) that when we add together two activation functions we get something unlinear. They show result of such ""addition"" first as 2d (time-code 14:50 and 14:58).

So, Here comes my question:  how come that they ""add"" two activation functions, if to the second activation function reaches just a simple number as said above he is not telling any one the ""secret"" that it was created by some kind of sigmoid.
5) And then again, they show this addition of 3d surfaces (time-code 19:39 )
 How it is possible? I mean again there should not happen any addition of surfaces, because no surface passes to next step but a number. What do I miss?
","['neural-networks', 'ai-basics', 'activation-functions', 'hidden-layers', 'weights']","
The output of any node is simply a scalar number. For a given input you get a specific scalar output. What is being shown is the surfaces that get generated as you VARY x1 and x2 over their input range.
To answer your first question it is always best to scale your inputs.
"
Monte Carlo epsilon-greedy Policy Iteration: monotonic improvement for all cases or for the expected value?,"
I was going through university slides and this particular slide is trying to prove that in a Monte Carlo Policy Iteration algorithm using an epsilon-greedy policy, the state Values (V-Values) are monotonically improving.

My question is about the first line of computation.

Isn't this actually the formula for the expected value of Q? It is calculating a probability of occurrence following the policy times actual Q values, then doing the summation.
If that is the case, could you help me understand the relationship between the expected value of Q and the expected value of V ?
Also, if above is true, in a real world scenario, depending on how many episodes we sample and on stochasticity, does it mean that the V values of the new policy could be worse than the V values of the old policy ?
","['reinforcement-learning', 'proofs', 'monte-carlo-methods', 'policy-iteration', 'policy-improvement']",
What is generalized policy iteration?,"
I am reading Sutton and Barto's material now. I know value iteration, which is an iterative algorithm taking the maximum value of adjacent states, and policy iteration. But what is generalized policy iteration? 
","['reinforcement-learning', 'definitions', 'value-iteration', 'policy-iteration']","
In the standard policy iteration algorithm presented in Sutton and Barto's book, you alternate between a policy evaluation (PE) step and a policy improvement (PI) step (i.e. PE, PI, PE, PI, PE, PI, PE, ...). However, in general, you don't have to follow this alternation strictly in order to converge (in the limit) to the optimal policy. For example, value iteration (VI) is an example of a truncated policy iteration that still converges to the optimal policy. 
The term generalized policy iteration (GPI) refers to all algorithms based on policy iteration, such as value iteration, that alternate in some order PI and PE, and that are guaranteed to converge to the optimal policy, provided PE and PI are executed enough times.
"
What are episodic and non-episodic domains in reinforcement learning?,"
I was reading about the temporal difference (TD) learning and I read that:

TD handles continuing, non-episodic domains

Assuming that continuing means non-terminating, what does non-episodic or episodic domain mean?
","['reinforcement-learning', 'terminology', 'temporal-difference-methods', 'environment']",
Subtracting the entropy from our policy gradient will prevent our agent from being stuck in the local minimum?,"

In the information theory, the entropy is a measure of uncertainty in
  some system. Being applied to agent policy, entropy shows how much the
  agent is uncertain about which action to make. In math notation,
  entropy of the policy is defined  as : $$H(\pi) = -\sum \pi(a|s) \log
 \pi(a|s)$$ The value of entropy is always greater than zero and has a
  single maximum when the policy is uniform. In other words, all actions
  have the same probability. Entropy becomes minimal when our policy has
  1 for some action and 0 for all others, which means that the agent is
  absolutely sure what to do. To prevent our agent from being stuck in
  the local minimum, we are subtracting the entropy from the loss
  function, punishing the agent for being too certain about the action
  to take.

The above excerpt is from Maxim Lapan in the book Deep Reinforcement Learning Hands-on page 254.
In code, it might look like : 
 optimizer.zero_grad()
 logits= PG_network(batch_states_ts)
 log_prob = F.log_softmax(logits, dim=1)
 log_prob_actions = batch_scales_ts * log_prob[range(params[""batch_size""]), batch_actions_ts]
 loss_policy = -log_prob_actions.mean()

 prob = F.softmax(logits, dim=1)
 entropy = -(prob * log_prob).sum(dim=1).mean()
 entropy_loss = params[""entropy_beta""] * entropy
 loss = loss_policy - entropy_loss

I know that a disadvantage of using policy gradient is our agent can be stuck at a local minimum. Can you explain mathematically why subtracting the entropy from our policy will  prevent our agent from being stuck in the local minimum ?
","['reinforcement-learning', 'python', 'policy-gradients']",
Does it classify as Machine Learning?,"
I have a gaussian distributed time series ($X_t$) with some parameters in my experiment. Suppose I want to know the mean $\mu$. If I define another time series $Y_t$ such that $Y_t=X_t-a$ for all $t$. Now say I vary this parameter $a$ and generate altogether different time series for each $a$, say $Y_t(a)$. I look at the mean of $Y_t$ for each $a$. The value of a, where I get the mean of $Y_t$ closest to $0$, will be my estimate of $\mu$. Say I will eventually use this learnt value of $\mu$ to generate $Y_t$ as my final goal. Can this be called ML? I am using some training data of $X_t$ to learn about its parameter and then using test data of $X_t$ to generate $Y_t$.
Now why am I working so hard on this simple problem? Well, actually I am not. I am doing something else, which will have lots of parameters in the time series and will be used to generate other time series after similar parameter extraction. That will be too complicated to discuss here. I just wanted to clear my basics using an over-simplified example.
","['machine-learning', 'classification', 'terminology', 'time-series']","
Does it classify as Machine Learning?
If I understand the problem correctly, my opinion is yes, this is machine learning.  In fact, I think it is just linear regression for finding the equation of a horizontal line y(t) = a that fits the the dataset y(t) = Xt.
"
What should be a good playing strategy in this 2-player simultaneous game?,"
This is a bot making problem from here.
I am detailing the problem.

The picture above shows the initial configuration of the game. P1 represents player1 and P2 represents player2. A scotch bottle is kept(initially) at the position #5 on the number line. Both players start with 100 dollars in hand. Note that players don't move, only the bottle moves.
Rules of the game:

The first player makes a secret bid followed by a secret bid by the second player.
The bottle moves one position closer to the winning bidder.
In case of drawn bid, the winner is the player, who has the draw advantage.
Draw advantage alternates between the two player, that is, the first draw is won by the first player, the second draw if it occurs is won by the second player and so on.
The winning bid is deducted from the player's hand, the loser keeps his bid.
The bottle moves one position closer to the winning bidder.
Each bid must be greater than 0 dollar. In the case when there's no money left, the player has no choice but to bid 0 dollar. Only integral bids are allowed.

The player who gets the bottle wins. If no one gets it, the game ends in a draw.
Both the players,thus,have complete knowledge of the history of biddings of each other, and the location of bottle at the current time.
So far, i know this is an instance of poorman bidding games. I have used some strategies like I am intentionally losing some bids and let the opponent use his money, in a hope that difference of money increases to the point of allowing for the emergence of a winning strategy. Also, i pull stronger the bottle as it goes further. This isn't performing well with other bots.
What should be the strategy of a bot playing this game?
","['ai-design', 'game-ai', 'game-theory']",
How to estimate the convolutional representation of a graph from its similarity to other graph convolutional representation?,"
Suppose we have two graphs A and B disconnected to each other (let's say 2-hops each), within a larger graph. If the convolutional representation of graph A is known, is it possible to estimate the definitive convolutional representation of graph B based on its similarity to graph A?
If yes, what do you think is the simplest (arithmetically) way to do this, which algorithm can help me to do this? You can assume that precision requirements are not important.
","['convolutional-neural-networks', 'geometric-deep-learning', 'graph-theory', 'similarity', 'graph-neural-networks']",
Do I need to maintain a separate population in each distributed environment when implementing PBT in a MARL context?,"
I have questions regarding on how to implement PBT as described in Algorithm 1 (on page 5) in the paper, Population Based Training of Neural Networks to train agents in a MARL (multi-agent reinforcement learning) environment.
In a single agent RL environment, the environments can be distributed & agents trained in parallel & there will be a need to maintain some sort of centralized population of weights & hyperparameters. (Please correct me if I'm wrong.)
In a MARL context, do I need to also maintain a centralized population for all agents in all environments or do I need to maintain a separate population for agents in each distributed environment? Which is a correct or more effective approach?
Any pointers would be appreciated. Thank you.
","['reinforcement-learning', 'multi-agent-systems', 'distributed-computing']",
Can neural networks handle redundant inputs?,"
I have a fully connected neural network with the following number of neurons in each layer [4, 20, 20, 20, ..., 1]. I am using TensorFlow and the 4 real-valued inputs correspond to a particular point in space and time, i.e. (x, y, z, t), and the 1 real-valued output corresponds to the temperature at that point. The loss function is just the mean square error between my predicted temperature and the actual temperature at that point in (x, y, z, t). I have a set of training data points with the following structure for their inputs:

(x,y,z,t):
(0.11,0.12,1.00,0.41)
(0.34,0.43,1.00,0.92)
(0.01,0.25,1.00,0.65)
...
(0.71,0.32,1.00,0.49)
(0.31,0.22,1.00,0.01)
(0.21,0.13,1.00,0.71)

Namely, what you will notice is that the training data all have the same redundant value in z, but x, y, and t are generally not redundant. Yet what I find is my neural network cannot train on this data due to the redundancy. In particular, every time I start training the neural network, it appears to fail and the loss function becomes nan. But, if I change the structure of the neural network such that the number of neurons in each layer is [3, 20, 20, 20, ..., 1], i.e. now data points only correspond to an input of (x, y, t), everything works perfectly and training is all right. But is there any way to overcome this problem? (Note: it occurs whether any of the variables are identical, e.g. either x, y, or t could be redundant and cause this error.) 
My question: is there any way to still train the neural network while keeping the redundant z as an input? It just so happens the particular training data set I am considering at the moment has all z redundant, but in general, I will have data coming from different z in the future. Therefore, a way to ensure the neural network can robustly handle inputs at the present moment is sought. 
","['neural-networks', 'tensorflow', 'training', 'objective-functions', 'optimization']",
Why are all weights of a neural net updated and not just the weights of the first layer,"
Why are all weights of a neural net updated and not only the weights of the first hidden layer?
The error-influence of the prediction by the weights of a neural net is calculated using the chain rule. However, the chain rule tells us how the first variable influences the second variable, and so on. Following that logic, we should only update the weights of the first hidden layer. My thought is, that if we backtrack the influence of the first variable but also change the values of the subsequent weights (of the subsequent hidden layer), there is no need to calculate the influence of the first weights in the first place. Where am I wrong? 
","['neural-networks', 'machine-learning', 'backpropagation', 'math']","

However, the chain rule tells us how the first variable influences the second variable, and so on. Following that logic, we should only update the weights of the first hidden layer. 

I don't see how the second statement follows from the first.
Each weight $w_i$ (not just the ones in the first layer) affects the loss $\mathcal{L}$ according to the partial derivative of $\mathcal{L}$ with respect to $w_i$, i.e. $\frac{\partial \mathcal{L}}{\partial w_i}$. Intuitively, the partial derivative with respect to parameter tells you how the function is changing with respect to that parameter. 

My thought is, that if we backtrack the influence of the first variable but also change the values of the subsequent weights (of the subsequent hidden layer), there is no need to calculate the influence of the first weights in the first place.

I am not sure I understand your reasoning, but, typically, you update the parameters only after having computed all the partial derivatives. In other words, first, you compute all partial derivatives, i.e. the gradient with back-propagation (a fancy name to denote the application of the chain rule), then you update the parameters.
Why do you do this? In this case, the loss function is multi-variable function, so it depends on multiple variables. The gradient $\nabla \mathcal{L} = \left[ \frac{\partial \mathcal{L}}{\partial w_1}, \dots, \frac{\partial \mathcal{L}}{\partial w_N} \right]$ represents the direction (note that the gradient is a vector and vectors have direction) towards which your function is increasing or decreasing (depending on the sign of the gradient). 
"
What is the difference between hill-climbing and greedy best-first search algorithms?,"
While watching MIT's lectures about search, 4. Search: Depth-First, Hill Climbing, Beam, the professor explains the hill-climbing search in a way that is similar to the best-first search. At around the 35 mins mark, the professor enqueues the paths in a way similar to greedy best-first search in which they are sorted, and the closer nodes expanded first. 
However, I have read elsewhere that hill climbing is different from the best first search. What's the difference between the two then?
","['comparison', 'search', 'hill-climbing', 'best-first-search']",
What does the notation $\partial \theta_{\pi}$ mean in this actor-critic update rule?,"
One of the steps in the actor-critic algorithm is $$\partial \theta_{\pi} \gets \partial \theta_{\pi} + \nabla_{\theta}\log\pi_{\theta} (a_i | s_i) (R - V_{\theta}(s_i))$$
For me, $\theta$ are just the weights. Can you explain to me what mean $\partial \theta_{\pi}$?
The whole algorithm comes from Maxim Lepan's book Deep Reinforcement Learning Hands-on page 269.
Here is a picture of the algorithm : 

","['reinforcement-learning', 'deep-rl', 'pytorch', 'actor-critic-methods', 'notation']",
Is radial basis function network appropriate for small datasets?,"
I'm a computer engineering student and I'm about to work on my master thesis. My professor gave me a small dataset with brain Computed Axial Tomography records. I would like to use deep learning to help doctors diagnose a certain disease (obviously, I've also got the data for doing supervised learning). 
Since the dataset is small, is radial basis function network a good solution? What do you think?
Btw, if you have any type of tips in using the RBF network for this kind of project I would be really grateful.
","['neural-networks', 'machine-learning', 'deep-learning', 'supervised-learning']",
How can I design a DQN or policy gradient model to explore and collect all optimal solutions?,"
I am working to use DQN and Policy Gradient reinforcement learning models to solve classic maze escaping problems. 
So far, I have been able to train a model, which, after around 100 episodes, quickly explored ONE optimal solution to escape mazes.
However, it is easy to see that for many maze designs, the optimal solutions could be multiple, and I would like to take one step further to collect all optimal and distinguishable solutions. 
However, I tried some searches online and so far, the only material I can find is this Learning Diverse Skills. But this seems an obstacle to me. I somewhat believe this seems a classic (?) and an easier problem that should be addressed in the textbook? 
Could someone shed light on this matter?
","['reinforcement-learning', 'ai-design', 'dqn', 'policy-gradients']",
How to know if the hyperparameters of a neural network relate to each other?,"
According this thread some hyperparameters are independent from each other while some are directly related.
One of the answers give an example where two hyperparameters affect each other.

For example, if you're using stochastic gradient descent (that is, you train your model one example at a time), you probably do not want to update the parameters of your model too fast (that is, you probably do not want a high learning rate), given that a single training example is unlikely to be able to give the error signal that is able to update the parameters in the appropriate direction (that is, the global or even local optimum of the loss function). 

How would someone creating a neural network know how the hyperparameters affect each other?
In other words, what are the heuristics for hyperparameter selection when trying to build a robust model?
","['neural-networks', 'hyperparameter-optimization', 'hyper-parameters']",
GPT-2: (Hardware) requirements for fine-tuning the 774M model [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I wonder if there's anyone who has actually succeeded in fine-tuning GPT-2's 774M model without using cloud TPU's. My GeForce RTX 2070 SUPER couldn't handle it in previous attempts.
I'm running TensorFlow 1.14.0 with CUDA V 9.1 on Ubuntu 18.04. For fine-tuning I'm using gpt-2-simple.
When fine-tuning using the 77M model, I keep running into OOM errors, such as:
W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.25MiB (rounded to 6553600).  Current allocation summary follows.
So far I've tried:

Using different a optimizer (RMSPropOptimizer instead of AdamOptimizer)
Setting batch-size to 1
use_memory_saving_gradients
only_train_transformer_layers

Fine-tuning works smoothly on the 355M model.
So what I'm really asking is:

is it possible to fine-tune GPT-2's 774M model without industrial-sized hardware?
if so, please tell me about your successful attempts
apart from hardware-recommendations, how could fine-tuning be optimized to make 77M fit in memory?

","['natural-language-processing', 'gpt', 'fine-tuning', 'gpt-2']","
Possibly a bit late to the answer, but I doubt you'd be able to run GPT-2 774M in FP32 on 2070 Super which has 8GB VRAM. I know it's not an exact comparison, but fine-tuning BERT Large (345M) in FP32 easily takes more than 10GB of VRAM. You might be able to run GPT-2 774M if you run it in FP16.
Alternatively, you can use Google Collab TPUs which provide at 11GB+ VRAM. Here's a good source listing a few posts about fine tuning GTP-2 1.5B on Google Collab TPUs: https://news.ycombinator.com/item?id=21456025
And here's the notebook itself demonstrating the process: https://colab.research.google.com/drive/1BXry0kcm869-RVHHiY6NZmY9uBzbkf1Q#scrollTo=lP1InuxJTD6a
"
Levenshtein Distance between each word in a given string,"
From Calculate Levenshtein distance between two strings in Python it is possible to calculate distance and similarity between two given strings(sentences).
And from Levenshtein Distance and Text Similarity in Python to return the matrix for each character and distance for two strings.
Are there any ways to calculate distance and similarity between each word in a string and print the matrix for each word in a string(sentences)?
a = ""This is a dog.""
b = ""This is a cat.""

from difflib import ndiff

def levenshtein(seq1, seq2):
    size_x = len(seq1) + 1
    size_y = len(seq2) + 1
    matrix = np.zeros ((size_x, size_y))
    for x in range(size_x):
        matrix [x, 0] = x
    for y in range(size_y):
        matrix [0, y] = y

    for x in range(1, size_x):
        for y in range(1, size_y):
            if seq1[x-1] == seq2[y-1]:
                matrix [x,y] = min(
                    matrix[x-1, y] + 1,
                    matrix[x-1, y-1],
                    matrix[x, y-1] + 1
                )
            else:
                matrix [x,y] = min(
                    matrix[x-1,y] + 1,
                    matrix[x-1,y-1] + 1,
                    matrix[x,y-1] + 1
                )
    print (matrix)
    return (matrix[size_x - 1, size_y - 1])

levenshtein(a, b)

Outputs
>> 3

Matrix
[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]
 [ 1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]
 [ 2.  1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]
 [ 3.  2.  1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]
 [ 4.  3.  2.  1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]
 [ 5.  4.  3.  2.  1.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]
 [ 6.  5.  4.  3.  2.  1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]
 [ 7.  6.  5.  4.  3.  2.  1.  0.  1.  2.  3.  4.  5.  6.  7.]
 [ 8.  7.  6.  5.  4.  3.  2.  1.  0.  1.  2.  3.  4.  5.  6.]
 [ 9.  8.  7.  6.  5.  4.  3.  2.  1.  0.  1.  2.  3.  4.  5.]
 [10.  9.  8.  7.  6.  5.  4.  3.  2.  1.  0.  1.  2.  3.  4.]
 [11. 10.  9.  8.  7.  6.  5.  4.  3.  2.  1.  1.  2.  3.  4.]
 [12. 11. 10.  9.  8.  7.  6.  5.  4.  3.  2.  2.  2.  3.  4.]
 [13. 12. 11. 10.  9.  8.  7.  6.  5.  4.  3.  3.  3.  3.  4.]
 [14. 13. 12. 11. 10.  9.  8.  7.  6.  5.  4.  4.  4.  4.  3.]]

General Levenshtein distance for character level shown in below fig. 


Is it possible to calculate Levenshtein Distance for Word Level?

Required Matrix
          This is a cat

This
is
a
dog

","['natural-language-processing', 'similarity']","
well... simply put a .split() at the end of your first two lines:
a = ""This is a dog."".split()
b = ""This is a cat."".split()

Your algorithm works with the iterables, and the string is broken into it's characters. You do the split, and a,b would be a list of words, then your algorithm works on the word-level
Output on your example:
[[0. 1. 2. 3. 4.]
 [1. 0. 1. 2. 3.]
 [2. 1. 0. 1. 2.]
 [3. 2. 1. 0. 1.]
 [4. 3. 2. 1. 1.]]

1.0

"
Understanding the W term in off policy monte carlo learning,"
In Sutton and Barto's RL textbook they included the following pseudocode for off policy Monte Carlo learning. I am a little confused, however, because to me it looks like the W term will become infinitely large after a couple thousand iterations (and this is exactly what happens when I implement the algorithm).
For example, say that the MC algorithm always follows the behavioral policy for each episode (ignoring epsilon soft/greedy for examples sake). If the probability of the action specified by the policy is 0.9, then after 10,000 iterations W would have a value of 1.11^10,000. I understand that the ratio of W to C(a,s) is what matters, however this ratio cannot be computer once W becomes infinite. Clearly I am misunderstanding something.

","['reinforcement-learning', 'monte-carlo-methods', 'off-policy-methods']",
Why AlphaGo didn't use Deep Q-Learning?,"
In the previous research, in 2015, Deep Q-Learning shows its great performance on single player Atari Games. But why do AlphaGo's researchers use CNN + MCTS instead of Deep Q-Learning? is that because Deep Q-Learning somehow is not suitable for Go?
","['reinforcement-learning', 'deep-rl', 'monte-carlo-tree-search']","
Deep Q Learning is a model-free algorithm. In the case of Go (and chess for that matter) the model of the game is very simple and deterministic. It's a perfect information game, so it's trivial to predict the next state given your current state and action (this is the model). They take advantage of this with MCTS to speed up training. I suppose Deep Q Learning would also work, but it would be at a huge disadvantage.
"
Why does this tutorial on reinforced learning not check whether the environment is 'game over' during training?,"
I am following the tutorial Train a Deep Q Network with TF-Agents. It uses the hello world environment of reinforced learning: cart pole.
At the end, the agent is getting trained with experience on the training enviroment (train_env). When performing an action in the environment a time_step is returned containing the observation, award and whether the environment signals the end, which basically says 'game over'. This can be due to the pole reaching an angle which is too high, or 200 time steps have been reached (which is the max score, atleast in the tutorial).
When compute_avg_return method evaluates an agents performance on an environment, the environment is checked to be game over or not using time_step.is_last.
Why is time_step.is_last not considered when training the agent at the end of the tutorial? Nor do I see that the environment is reset during training. Or at least, I do not see it in the code presented. Is it checked internally? Looking at the graph, it never goes over an average return (the score) of 200 time steps. So it does seem to check for time_step.is_last. Do I overlook something or how does this work? 
See the code block below. I would expect the check for time_step.is_last after collect_step(train_env, agent.collect_policy, replay_buffer), which would be followed by resetting the environment if it was true.
# Reset the train step
agent.train_step_counter.assign(0)

# Evaluate the agent's policy once before training.
avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
returns = [avg_return]

for _ in range(num_iterations):

  # Collect a few steps using collect_policy and save to the replay buffer.
  for _ in range(collect_steps_per_iteration):
    collect_step(train_env, agent.collect_policy, replay_buffer)

  # Sample a batch of data from the buffer and update the agent's network.
  experience, unused_info = next(iterator)
  train_loss = agent.train(experience).loss

  step = agent.train_step_counter.numpy()

  if step % log_interval == 0:
    print('step = {0}: loss = {1}'.format(step, train_loss))

  if step % eval_interval == 0:
    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
    print('step = {0}: Average Return = {1}'.format(step, avg_return))
    returns.append(avg_return)

","['reinforcement-learning', 'tensorflow', 'training', 'environment']",
"Appropriate algorithm for RL problem with sparse rewards, continuous actions and significant stochasticity","
I'm working on a RL problem with the following properties:

The rewards are extremely sparse i.e. all rewards are 0 except the terminal non-zero reward. Ideally I would not use any reward engineering as that would lead to a different optimization problem.
Actions are continuous. Discretization should not be used.
The amount of stochasticity in the environment is very high i.e. for a fixed deterministic policy the variance of returns is very high.

More specifically, the RL agent represents the investor, the terminal reward represents the utility of the terminal wealth (hence the sparsity), actions represent portfolio positions (hence the continuity) and the environment represents the financial market (hence the high stochasticity).
I've been trying to use DDPG with a set of ""commonly used"" hyperparameters (as I have no idea have to tune them besides experimenting which lasts too long) but so far (after 10000 episodes) it seems that nothing is happening.

My questions are the following:

Given the nature of the problem I'm trying to solve (sparse rewards, continuous actions, stochasticity) is there a particular (D)RL algorithm that would lend itself well to it?
How likely is it that DDPG simply won't converge to a reasonable solution (due to the peculiarities of the problem itself) no matter what set of hyperparameters I choose?

","['reinforcement-learning', 'rewards', 'policy-gradients', 'ddpg']","
(1) You might want look into RND (Random network distillation) which allows usage of a curiosity-based exploration bonus for the agent as an intrinsic reward. You can use the intrinsic reward to complement the sparse extrinsic reward return by the environment. 
The general idea is to have a randomly initialized fixed target network which encodes the next state & a predictor network is trained to predict the output of the target network. The prediction error is used to ""quantify the novelty of new experience"". Stronger novelty will be a good indication for the agent that it maybe worthwhile to explore more. 
The authors of this (A) paper were able to achieve SOTA performance in Montezuma's Revenge, which is notorious for it's sparse reward.
In appendix A.1, It is mentioned that: ""An exploration bonus can be used with any RL algorithm by modifying the rewards used to train the
model (i.e., rt = it + et)."" It is also mentioned that the authors combined this exploration bonus with PPO (which also works in continuous action space). In A.2, a pseudo code is provided.
I would also recommend this (B) paper (see section 3) if you're interested in exploring the available Bonus-Based Exploration Methods out there which may help in tackling hard exploration games with sparse rewards.
With regards to high stochasticity & variance, I found an interesting remark (on page 3, under Figure 2) in this (C) paper: 
""our investigation of DDPG on different network configurations shows that for the Hopper environment, DDPG is quite unstable no matter the network architecture. This can be attributed partially to the high variance of DDPG itself, but also to the increased stochasticity of the Hopper
task.""
The remark was made in the context where the authors were trying to ""tune DDPG to reproduce results from other works even when using their reported hyper-parameter settings"".
Have a look here for a different benchmark on how DDPG fair against other algorithms.
(2) From the information provided, I can't conclusively provide you a quantitative assessment on DDPG performance for your specific problem. However, I would recommend the following:
(a) I will encourage you to try different RL algorithms when face with a difficult problem so that you can benchmark & find out which is more suitable. Also in (A), the authors mentioned, ""PPO is a policy gradient method that we have found to require little tuning for good performance."" 
(b) Try different sets of hyperparameters. There are many ways to tune them systematically but discussion about this will be out of scope for this question.
"
How can a machine learning problem be reduced as a communication problem?,"
I once heard that the problem of approximating an unknown function can be modeled as a communication problem. How is this possible? 
","['computational-learning-theory', 'information-theory', 'bayesian-deep-learning', 'minimum-description-length']",
How to improve neural network training against a large data set of points with varying magnitude,"
I am currently using TensorFlow and have simply been trying to train a neural network directly against a large continuous data set, e.g. $y = [0.014, 1.545, 10.232, 0.948, ...]$ corresponding to different points in time. The loss function in the fully connected neural network (input layer: 3 nodes, 8 inner layers: 20 nodes each, output layer: 1 node) is just the squared error between my prediction and the actual continuous data. It appears the neural network is able to learn the high magnitude data points relatively well (e.g. Figure 1 at time = 0.4422). But the smaller magnitude data points (e.g. Figure 2 at time = 1.1256) are quite poorly learned without any sharpness and I want to improve this. I've tried experimenting with different optimizers (e.g. mini-batch with Adam, full batch with L-BFGS), compared reduce_mean and reduce_sum, normalized the data in different ways (e.g. median, subtract the sample mean and divide by the standard deviation, divide the squared loss term by the actual data), and attempted to simply make the neural network deeper and train for a very long period of time (e.g. 7+ days). But after approximately 24 hours of training and the aforementioned tricks, I am not seeing any significant improvements in predicted outputs especially for the small magnitude data points.

Figure 1


Figure 2


Therefore, do you have any recommendations on how to improve training particularly when there are different data points of varying magnitude I am trying to learn? I believe this is a related question, but any explicit examples of implementations or techniques to handle varying orders of magnitude within a single large data set would be greatly appreciated.
","['neural-networks', 'tensorflow', 'training', 'optimization', 'batch-normalization']","
I think there is no special method for training the neural network in large datasets. But I can add some suggestion for you:
1) Use the convolutional neural network for this dataset.
2) You can use huber loss instead of squared loss and see what happens.
3) See if you have enough small magnitude training data.
Also, please define your problem in more detail (like what those images represent and what you want to predict, what are the features etc).
"
Has there been any work done on AI-driven operant conditioning?,"
For example, a RL algorithm that gains points when a rat presses a lever and loses points when it dispenses a pellet, water, treat, and/or sugar water. After a few days of controlling the rewards given to a rat, all rewards are stopped and the longer/more times the rat presses the lever before giving up, the higher the score.
This would be a situation in which both the input and the outputs are discrete with very low data density over time and with outputs having very long-term affects on the environment.
What kind of RL architecture would be appropriate here?
","['reinforcement-learning', 'research', 'operant-conditioning']",
Why can't MLPs perform non-linear regression and classification?,"
In this page it's told:

In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP)

What does it mean? I thought the MLP was a non-linear classifier. Could you explain it to me?
","['deep-learning', 'multilayer-perceptrons', 'perceptron']",
Why are Dueling Q Networks not used more often to approximate Q-values in reinforcement learning algorithms?,"
I've just learned about Dueling Network Architectures to estimate $Q$-values and am wondering why this architecture is not used more often in deep RL algorithms? DDPG and TD3 estimate the $Q$-function using Double Q Learning instead of the empirically better Dueling Approach.
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']",
RL: What should be the output of the NN for an agent trying to learn how to play a game?,"
Say the game is tic tac toe.
I found two possible output layers:

Vector of length 9: each float of the vector represents 1 action
(one of the 9 boxes in Tic Tac Toe). The agent will play the corresponding action with the highest value. The agent learns the rules through trial and error. When the agent tries to make an illegal move (i.e. placing a piece on a box where there is already one), the reward will be harshly negative (-1000 or so).
A single float: the float represents who is winning (positive = ""the agent is winning"", negative = ""the other player is winning""). The agent does not know the rules of the game. Each turn the agent is presented with all the possible next states (resulting from playing each legal action) and it chooses the state with the highest output value.

What other options are there?
I like the first option because it's cleaner, but it's not feasible with games that have thousands or millions of actions. Also, I am worried that the game might not really learn the rules.
E.g. Say that in state S the action A is illegal. Say that state R is extremely similar to state S but action A is legal in state R (and maybe in state R action A is actually the best move!). Isn't there the risk that by learning not to play action A in state S it will also learn not to play action A in state R? Probably not an issue in Tic Tac Toe, but likely one in any game with more complex rules.
What are the disadvantages of option 2?
Does the choice depend on the game? What's your rule of thumb when choosing the output layer?
","['reinforcement-learning', 'intelligent-agent', 'architecture']","
It depends on whether the action is part of the input or output of a neural network estimating the Q-value(state, action).
The network on the left has the state as input and outputs one scalar value for each of the categorical actions. It has the advantage of being easy to setup and only needs one network evaluation to predict the Q-value for all actions. If the action space is categorical and single-dimensional I would use it.
The network on the right has both the state and a representation of the action as input and outputs one single scalar value. This architecture also allows to compute the Q-value for multi-dimensional and continuous action spaces. 
The action space of tic-tac-toe can be easily represented by a vector of length 9, thus I would recommend the left NN-architecture. However, if your game has continuous-valued variables in the action space (e.g. the position of your mouse pointer), you should use the NN-architecture on the right.  
The approach to prevent illegal moves is only partially dependent on the choice of the Q-function architecture and covered by another question: How to forbid actions
"
How are the Bellman optimality equations and minimax related?,"
Is the philosophy between Bellman equations and minimax the same? 
Both the algorithms look at the full horizon and take into account potential gains (Bellman) and potential losses (minimax). 
However, do the two differ besides the obvious on the fact that Bellman equations use discounted potential rewards, while minimax deals with potential losses without the discount? Are these enough to say they are similar in philosophy or is are they dissimilar? If so, then in what sense?
","['reinforcement-learning', 'comparison', 'game-theory', 'minimax', 'bellman-equations']",
How does optical computing work and deal with nonlinearity?,"
This article states that: 

One of the algorithms that photonics is very good at implementing is matrix multiplication

But how are parameters stored and updated(in backpropagation)? 
One more serious problem is that there are nonlinear operations in neural networks, then how does the photonic neural network deal with activation functions?
","['neural-networks', 'machine-learning', 'hardware']",
Is Monte Carlo tree search needed in partially observable environments during gameplay?,"
I understand that with a fully observable environment (chess / go etc) you can run an MCTS with an optimal policy network for future planning purposes. This will allow you to pick actions for gameplay, which will result in max expected return from that state.
However, in a partially observable environment, do we still need to run MCTS during gameplay? Why can't we just pick the max action from the trained optimal policy given the current state? What utility does MCTS serve here?
I am new to reinforcement learning and am trying to understand the purpose of MCTS / planning in partially observable environments.
","['reinforcement-learning', 'monte-carlo-tree-search', 'alphazero', 'pomdp']",
Using DCGAN on a (very small) dataset of art,"
I am developing a DCGAN using the this tutorial in PyCharm. As my usage of this tutorial suggests, I am quite new to DCGANs as I've previously only had a few experiences with machine learning algorithms on classifying problems. My goal is to feed my DCGAN a dataset of paintings of a specific painter, and get 'new' paintings in return. Needless to say, a painter does not paint thousands of paintings in his life, leaving me with a dataset of around 60 paintings. One of the smallest datasets I have ever worked with. I have two, related, questions:

Is it realistic to properly train a DCGAN on this type of dataset? If not, would there be any alternative you would suggest?
What would be a good set of parameters to start of from to properly train this DCGAN?

Thanks in advance!
","['neural-networks', 'generative-adversarial-networks', 'generative-model', 'artificial-creativity']",
Do the variance and bias belong to the policy or value functions?,"
Recently, I read many papers on variance and bias. But I am still confused by the two notions, the variance or bias belongs to who? Policy or value? If the variance or bias is large or low, what results will we get?
","['reinforcement-learning', 'deep-rl', 'statistical-ai', 'bias-variance-tradeoff']",
What are examples of machine learning techniques inspired by neuroscience?,"
What are examples of machine learning techniques (i.e. models, algorithms, etc.) inspired (to different extents) by neuroscience?
Particularly, I'm interested in recent developments, say less than 10 years old, that have their basis in neuroscience to some degree. 
","['machine-learning', 'neuromorphic-engineering', 'spiking-neural-networks', 'neuroscience', 'reservoir-computing']",
Why do value iteration and policy iteration obtain similar policies even though they have different value functions?,"
I am trying to implement value and policy iteration algorithms. My value function from policy iteration looks vastly different from the values from value iteration, but the policy obtained from both is very similar. How is this possible? And what could be the possible reasons for this?
","['reinforcement-learning', 'comparison', 'policies', 'value-iteration', 'policy-iteration']","
More comments in addition to the accepted answer.
The OP says the two algorithms have different value functions. This is actually not precise and may be the source of confusion. In particular, only in the policy iteration algorithm, the value of $v$ is the state value function, which is the solution to the Bellman equation. However, the value of $v$ in value iteration is not a state value function! That is simply because it is not the solution to any Bellman equation in general. Then, what is the $v$ in value iteration? See another answer of mine.
Why value iteration, which does not calculate the state values, can find the optimal policy? It would be easier to see that if you think of it as a simple numerical iterative algorithm solving the Bellman optimality equation. The algorithm follows from the contraction (or called fixed-point) theorem when we analyze the Bellman optimality equation.
"
Loss function for choosing a subset of objects,"
I'm trying to train a neural net to choose a subset from some list of objects. 
The input is a list of objects $(a,b,c,d,e,f)$ and for each list of objects the label is a list composed of 0/1 - 1 for every object that is in the subset, for example $(1,1,0,1,0,1)$ represents choosing $a,b,d,f$. I thought about using MSE loss to train the net but that seemed like a naive approach, is there some better loss function to use in this case?
","['neural-networks', 'deep-learning', 'objective-functions']","
The choice of the loss function depends primarily on the type of task you're tackling: classification or regression. Your problem is clearly a classification one since you have classes to which a given input can either belong or not. More specifically, what you're trying to do is multi-label classification, which is different from multi-class classification. The difference is important to stress out and it consists in the format of the target labels.
# Multi-class --> one-hot encoded labels, only 1 label is correct   
  
[1,0,0], [0,1,0], [0,0,1]

# Multi-label --> multiple labels can be correct
 
[1,0,0], [1,1,0], [1,1,1], [0,1,0], [0,1,1], [0,0,1]

MSE is used when continuous values are predicted for some given inputs, therefore it belongs to the loss functions suitable for regression and it should not be used for your problem.
Two loss functions that you could apply are Categorical Cross Entropy or Binary Cross Entropy. Despite being both based on cross-entropy, there is an important difference between them, consisting of the activation function they require.

Binary Cross Entropy

$$L(y, \hat{y})=-\frac{1}{N} \sum_{i=0}^{N}\left(y * \log \left(\hat{y}_{i}\right)+(1-y) * \log \left(1-\hat{y}_{i}\right)\right)$$
Despite the name that suggests this loss should be used only for binary classification, this is not strictly true and actually, this is the loss function that conceptually is best suited for multi-label tasks.
Let's start with the binary classification case. We have a model that returns a single output score, to which the sigmoid function is applied in order to constrain the value between 0 and 1. Since we have a single score, the resulting value can be interpreted as a probability of belonging to one of the two classes, and the probability of being to the other class can be computed as 1 - value.
What if we have multiple output scores, for example, 3 nodes for 3 classes? In this case, we still could apply the sigmoid function, ending up with three scores between 0 and 1.

The important aspect to capture is that since the sigmoid treat each output node independently, the 3 scores would not sum up to 1, so they will represent 3 different probability distributions rather than a unique one. This means that each score after the sigmoid represents a distinct probability of belonging to that specific class. In the example above, for example, the prediction would be true for the two labels with a score higher than 0.5 and false for the remaining label. This also means that 3 different loss have to be computed, one for each possible output. In practice, what you'll be doing is to solve n binary classification problems where n is the number of possible labels.

Categorical Cross Entropy

$$L(y, \hat{y})=-\sum_{j=0}^{M} \sum_{i=0}^{N}\left(y_{i j} * \log \left(\hat{y}_{i j}\right)\right)$$
In categorical cross-entropy we apply the softmax function to the output scores of our model, to constrain them between 0 and 1 and to turn them into a probability distribution (they all sum to 1). The important thing to notice is that in this case, we end up with a unique distribution because, unlike the sigmoid function, the softmax consider all output scores together (they are summed in the denominator).

This implies that categorical cross-entropy is best suited for multi-class tasks, in which we end up with a single true prediction for each input instance. Nevertheless, this loss can also be applied also for multi-label tasks, as done in this paper. To do so, the authors turned each target vector into a uniform probability distribution, which means that the values of the true labels are not 1 but 1/k, with k being the total number of true labels.
# Example of target vector tuned into uniform probability distribution
[0, 1, 1] --> [0, .5, .5]  
[1, 1, 1] --> [.33, .33, .33]

Note also that in the above-mentioned paper the authors found that categorical cross-entropy outperformed binary cross-entropy, even this is not a result that holds in general.
Lastly, the are other loss that you could try which differ functions rather than cross-entropy, for example:

Hamming-Loss

It computes the fraction of wrong predicted labels
$$\frac{1}{|N| \cdot|L|} \sum_{i=1}^{|N|} \sum_{j=1}^{|L|} \operatorname{xor}\left(y_{i, j}, z_{i, j}\right)$$

Exact Match Ratio

Only predictions for which all target labels were correctly classified are considered correct.
$$ExactMatchRatio,\space M R=\frac{1}{n} \sum_{i=1}^{n} I\left(Y_{i}=Z_{i}\right)$$
"
Why is the number of neurons used in various neural networks power of 2?,"
I have noticed that almost all tutorials take the number of neurons as a power of 2. Is there any proper mathematical and well-proven reason for that? 
If you sometimes change it to some other odd value, then we get a very long and weird error calling about a dozen things making the traceback of a page. Is there any reason for that?      

I had tried it on a text predicting RNN with some GRU and LSTM layers mixed (bidirectional). I changed the no. of neuron units and It also resulted in an error. So, any ideas/theories?

","['neural-networks', 'machine-learning', 'ai-design', 'hyper-parameters']",
How well can NLP techniques recognize connotations in natural languages?,"
What is the state of the art with respect to recognizing connotations in natural languages?
For instance:

Trump is a better president than Obama.           [Praising]
Trump is the worst president since Obama.         [Insulting]

or:

The rock star did not infect over 100 groupies.   [Defending against rumor]
The rock star infected no more than 100 groupies. [Attacking (0 is no more than 100)]

In each example, both statements logically mean exactly the same thing, but any human hearing them would interpret them as having quite opposite meanings.
How well can current natural language processors recognize the difference between logically equivalent statements?
","['natural-language-processing', 'reference-request', 'natural-language-understanding', 'state-of-the-art']",
Can optimizing for immediate reward result in a policy maximizing the return?,"
The goal of a reinforcement learning agent is to maximize the expected return which is often a discounted sum of future rewards. The return indeed is a very noisy random variable as future rewards depend on the state-transition-probabilities and the often stochastic policy. Lots of trajectories have to be sampled to approximate its expected value.
The immediate reward indeed does not have these dependencies. Therefore the questions: 
If we train a policy to maximize the immediate reward, will it also perform well in the long term? What properties would the reward function need to fulfill?
","['reinforcement-learning', 'markov-decision-process', 'rewards', 'decision-theory']",
How fast are autoencoders?,"
I was exploring image/video compression using Machine Learning. In there I discovered that autoencoders are used very frequently for this sort of thing. So I wanted to enquire:-      

How fast are autoencoders? I need something to compress an image in milliseconds?
How much resources do they take? I am not talking about the training part but rather the deployment part. Could it work fast enough to compress a video on a Mi phone (like note8 maybe)?    

Do you know of any particularly new and interesting research in AI that has enabled a technique to this fast and efficiently?
","['autoencoders', 'image-processing']","
Actually it depends on the size of your AE, if you use a small AE with just 500.000 to 1M weigths, the inferencing can be stunningly fast. But even large networks can run very fast, using Tensorflow lite for example, models are compressed and optimized to run faster on Edge-devices (Handys for example, end-user devices). You can find a lot of videos on Youtube, where people test inferencing large networks like Resnet-51 or Resnet-101 on a raspberrypi, or other SOC Chips. Handys are comparable to that, but maybe not that optimized. 
For example,I have an Jetson Nano (SOC of Nvidia costs arround 100 euro) and i tried to inference a large Resnet with arround 30 million parameters over my fullHD Webcam. Stable 30 FPS, so speaking in milliseconds its around 33 ms per image. 
To answer your question, yes Autoencoders can be fast, also very fast in combination with an optimized model and hardware. Autoencoder structures are quite easy, check out this medium,keras example
"
Are there any good research papers on image identification with limited data?,"
I'm a newbie in machine learning and I am interested in neural networks.
Are there any good research papers on image identification with limited data?
","['neural-networks', 'machine-learning', 'deep-learning', 'reference-request']",
Is Expected SARSA an off-policy or on-policy algorithm?,"
I understand that SARSA is an On-policy algorithm, and Q-learning an off-policy one.
Sutton and Barto's textbook describes Expected Sarsa thusly:

In these cliff walking results Expected Sarsa was used on-policy, but
  in general it might use a policy different from the target policy to
  generate behavior, in which case it becomes an off-policy algorithm.

I am fundamentally confused by this - specifically, how do we define when Expected SARSA adopts or disregards policy. The Coursera Course states that it is On-Policy, further confusing me.
My confusions became realized when tackling the Udacity course, specifically a section visualizing Expected SARSA for simple a gridworld (See section 1.11 and 1.12 in link below). Note that the course defines Expected Sarsa as on-policy.
https://www.zhenhantom.com/2019/10/27/Deep-Reinforcement-Learning-Part-1/
You'll notice the calculation for the new state value Q(s0,a0) as

Q(s0, a0) <— 6 + 0.1( -1 + [0.1 x 8] + [0.1 x 7] + [0.7 x 9] + [0.1
  x 8] - 6) = 6.16.

This is also the official answer. But this would mean that it is running off policy, given that it is stated that the action taken at S1 corresponds to a shift right, and hence expected SARSA (On policy) should yield you.

Q(s0, a0) <— 6 + 0.1( -1 + [0.1 x 8] + [0.1 x 7] + [0.1 x 9] + [0.7 x
  8] - 6) = 6.1

The question does state

(Suppose that when selecting the actions for the first two timesteps
  in the 100th episode, the agent was following the epsilon-greedy
  policy with respect to the Q-table, with epsilon = 0.4.)

But as this same statement existed for the regular SARSA example (which also yields 6.1 as A1 is shift right, as before), I disregarded it.
Any advice is welcome.
","['reinforcement-learning', 'off-policy-methods', 'sarsa', 'on-policy-methods', 'expected-sarsa']",
Do policy independent state and action values exist in reinforcement learning?,"
The state value function $V(s)$ is defined as the expected return starting in state $s$ and acting according to the current policy $\pi(a|s)$ till the end of the episode. The state-action values $Q(s,a)$ are similarly dependent on the current policy. 
Is it also possible to get a policy independent value of a state or an action? 
Can the immediate reward $r(s,a,s')$ be considered a noisy estimate of the action value?
","['reinforcement-learning', 'value-functions']",
Finding patterns in binary files using deep learning,"
I am a newbie in deep learning and wanted to know if the problem I have at hand is a suitable fit for deep learning algorithms. I have thousands of fragments each of about 1000 bytes size (i.e. numbers in the range of 0 to 255). There are two classes in the fragments:

Some fragments have a high frequency of two particular byte values appearing next to one another: ""0 and 100"". This kind of pattern roughly appears once every 100 to 200 bytes.
In the other class, the byte values are more randomly distributed. 

We have the ability to produce as many numbers of instances of each class as needed for training purposes.  However, I would like to differentiate with a machine learning algorithm without explicitly identifying the ""0 and 100"" pattern in the 1st class myself. Can deep learning help us solve this? If so, what kind of layers might be useful?
As a preliminary experiment, we tried to train a deep learning network made up of 2 hidden layers of TensorFlow's ""Dense"" layers (of size 512 and 256 nodes in each of the hidden layers). However, unfortunately, our accuracy was indicative of simply a random guess (i.e. 50% accuracy). We were wondering why the results were so bad. Do you think a Convolutional Neural Network will better solve this problem? 
","['neural-networks', 'machine-learning', 'deep-learning', 'tensorflow']","
Your network is essentially memorizing data but not extracting features. You need to apply CNN. 
That said the CNN architecture will need to be kind of unusual, each bit will need to be turned into representing a positional element. 
"
What are modern state-of-the-art solutions in prediction of time-series?,"
I wanted to ask you about the newest achievements in time series analysis (mostly prediction).
What state-of-the-art solutions (as in frameworks, papers, related projects) do you know that can be used for analysing and predicting time series? 
I am interested in something possibly better than just RNN, LSTM and GRU :)
","['recurrent-neural-networks', 'prediction', 'time-series', 'state-of-the-art']","
An interesting model I encountered in a course is Facebook Prophet. Prophet takes into account trends, seasonality, and holidays for its predictions. As you can probably guess, this is a model that fits Facebook's needs very well. I'll give a brief introduction then provide a link where you can read more. Prophet fits a couple of functions of time represented by a few terms. The general form of the timeseries predictions are, 
$$y(t)=g(t)+s(t)+h(t)+\epsilon_t$$
$g(t)$ deals with the trends I mentioned above. This is exactly what you would think it is, and accounts for non-periodic features of the data. It takes the form of a piecewise linear or logistic function.
$s(t)$ accounts for seasonality - in other words these are periodic changes in our timeseries (maybe an increase of sunscreen purchases in the summer). As this is periodic, the natural way to model this term is with Fourier decomposition to identify important frequencies in the signal.
$h(t)$ deals with predictable changes in the timeseries but is for events like holidays (this can happen at different times year to year so this is not necessarily periodic, think Easter). The user provides a list of events and how they want to account for it.
$\epsilon_t$ is just an error term to deal with anything that cannot be addressed with the rest of the model. 
This page has a great explanation if you want more detail. I highly recommend you check it out because it is very cool!
"
What are the scientific journals dedicated to artificial general intelligence?,"
Apart from Journal of Artificial General Intelligence (a peer-reviewed open-access academic journal, owned by the Artificial General Intelligence Society (AGIS)), are there any other journals (or proceedings) completely (or partially) dedicated to artificial general intelligence? 
If you want to share a journal that is only partially dedicated to the topic, please, provide details about the relevant subcategory or examples of papers on AGI that were published in such a journal. So, a paper that talks about e.g. an RL technique (that only claims that the idea could be useful for AGI) is not really what I am looking for. I am looking for journals where people publish papers, reviews or surveys that develop or present theories and implementations of AGI systems. It's possible that these journals are more associated with the cognitive science or neuroscience communities and fields.
","['agi', 'research', 'cognitive-science', 'academia', 'neuroscience']","
There's also the journal Advances in Cognitive Systems. According to their website

Advances in Cognitive Systems (ISSN 2324-8416) publishes research articles, review papers, and essays on the computational study of human-level intelligence, integrated intelligent systems, cognitive architectures, and related topics. Research on cognitive systems is distinguished by a focus on high-level cognition, reliance on rich, structured representations, a systems-level perspective, use of heuristics to handle complexity, and incorporation of insights about human thinking. Advances in Cognitive Systems reviews submissions within approximately three months and publishes accepted papers on the journal Web site immediately upon receipt of final versions. Articles are distributed freely over the internet by the Cognitive Systems Foundation.

"
Do Multi-resolution CNN exist?,"
I am currently working on a problem for which the topographic data is in very different resolution. Let say I have data of 20x20 with 1km2 tiles and also high resolution data of 50m2 tiles. I would like to combine both for input in a CNN. 
To make things more spicy I don't care about the 50m2 when it is far away from the center, that is why I would like to use an 'image' multi resolution, aka resolution low in the edges but higher in the center. That would be like human vision, only high detailed in the center...
Then I would combine that multi-resolution image with my 1km2 data 
Do you know any research done on such a CNN ?
Only found that one for now Multi-Resolution Feature Fusion for Image
Classification of Building Damages with
Convolutional Neural Networks 
Thank you for you help
","['neural-networks', 'convolutional-neural-networks', 'image-recognition', 'image-processing']",
What does the model predict if it has never seen the image before?,"
I've been messing around with an Open Set, Binary Classifier and am having trouble with it.  I'm sure there are a lot of reasons for that trouble.
One thing I am struggling with is, what does the model predict if it has never seen the image before?
An example would be if I'm trying to detect sheep across all background scenes.  If I train a binary classification set with one class having lots of sheep in it and the other class having lots of various backgrounds, what would the model predict if it came across a background it had never seen before with no sheep in it?  [mine is telling me ""sheep"" and I don't know why]
","['machine-learning', 'convolutional-neural-networks', 'prediction']","
I am assuming the images you gave the model all contain sheep. This is what i understand from your question.
Any model that you build will be based on the data that you give (training data) and your code. In your case, if you only give images that contains sheeps, and then you test it with no sheep and a background the model hasn't seen, it will search through all of its node, derived from your training data, to see which one is closest to the image you gave. Based on the information given, the only 'route' your cnn model can take is the one with sheeps on it, because you only gave images of sheeps for it to learn.
Here are a few suggestions that i can give you:

Give your model images with no sheep and different backgrounds so it can handle cases where there are no sheeps
Or you can add a piece of code to your model that tells it to default to some value if certain conditions aren't met (If the model doesn't 'see enough' sheep and background, it defaults to 'unknown image' or whatever you choose)

You are on the right path though! Since you are playing around with a binary classifier, you should definitely feed images with and without sheeps so that it can identify the two cases. Remember, a binary classifier is optimal when you give it 2 things to look out for, such as identifying images with and without sheep.
Here are some reading materials that you can brush up on to get a better basic understanding of how CNN works, personally i found the video helpful on the second link:

https://becominghuman.ai/what-exactly-does-cnn-see-4d436d8e6e52
https://brohrer.github.io/how_convolutional_neural_networks_work.html

Also, i find google images helps me alot in terms of visualizing binary classifier as a concept.
"
Can I use an autoencoder with high latent representational space?,"
I am trying to use a neural network to predict the next state output given the current state and action pairs. Both input and outputs are continuous variables. Due to the high dimensionality of each input, ( ~50 dimensional input ) and 48 dimensional output, I am not able to achieve an achieve a satisfiable enough accuracy. 
I am thinking of using an auto-encoder to learn a latent representation of the state. Would a latent representation from an auto-encoder help to improve the prediction accuracy ? and can the latent representation have a higher dimensional space compared to the original state ?
","['deep-learning', 'autoencoders']","
i feel , i can answer this question, based the web source that i found and i read. that can use an autoencoder with hight latent representational space. example use LSTM autoencoder, LTSM autoencoder used for sequence or time series data. on the source, they use CNN autoencoder to denoise some syntetic noised data they have generated. but they have asked what is the meaning of this latent representation space? for what they have done before. on that answer. they said""your input data is noisy sinewave data. your are not supposed to use Convolutional autoencoder for sequence data"".
https://stackoverflow.com/questions/59438488/cnn-autoencoder-latent-space-representation-meaning
addition other source for answer this question
https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d
https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f
hopefully help answer :))
"
"What are the implications of the statement ""If you can't tell, does it matter?"" in relation to AI?","
""If you can't tell, does it matter?"" was one of the first lines of dialogue of the Westworld television series, presented as a throwaway in the first episode of the first season, in response to the question ""Are you real?""
In the sixth episode of the third season, the line becomes a central realization of one of the main characters, The Man in Black.
This is, in fact, a central premise of the show—what is reality, what is identity, what does it mean to be alive?—and has a basis in the philosophy of Philip K. Dick.

What are the implications of this statement in relation to AI?  In relation to experience?  In relation to the self?

","['philosophy', 'artificial-consciousness', 'turing-test', 'mythology-of-ai', 'chinese-room-argument']","
The question in this video is

Are you real?

What does this question really mean? Is the guy asking whether the apparent female (I don't know if she is a cyborg or not because I did not yet watch the TV series) is a human? So, is ""real"" a synonym for ""human""? If that's the case, then the first implication (in the form of a question) of the statement

If you can't tell, does it matter?

in relation to AI is


Can we create an AGI that is sufficiently similar to a human that we can't tell whether it's a human or not (by just normally interacting with it)?


Of course, it's not clear what we mean by ""normally interacting"". As far as I remember, this issue is also raised in the film A.I. Artificial Intelligence, where the AI (the kid interpreted by Haley Joel Osment) looks sufficiently real to the other kid, so he behaves as if he was a human kid, but then the human kid understands that he's a machine, and starts to behave differently (I hope I'm remembering the film correctly).
So, the second question that we could ask is


Once we understand that it's not a human (for example, because it's made of other substances), would we humans start to behave differently and start treating the AGI differently?


As opposed to the first question, which is still an open problem, this second question can probably be answered by looking at our relationships with other humans (or entities, such as other animals). Often, we have an idea of a person. Once we discover something new about that person, which maybe we dislike, we may start to treat that person differently. I think this would very likely also happen in our eventual relationship with a sufficiently advanced AGI too, as depicted in the mentioned film.
Now, let me try to address the other question

What are the implications of this statement in relation to experience, in relation to the self?

I think that you're asking whether a sufficiently advanced AGI could be considered conscious or not. Of course, this is a very hard question to answer, because we still don't have a clear definition of consciousness or we don't yet agree on a standard definition, so I don't really have a definitive answer to this question. However, if consciousness is just a byproduct of perception and the ability to understand the world and its (physical) rules, then an AGI could be conscious (in a similar way that humans are also conscious). However, consciousness may not actually be necessary to correctly act in the world. In any case, the AI probably needs to know that it has a body and that it needs to protect it for its survival, if that's its main goal.
"
Can I do image classification with Multi Layers Perceptron (MLP)?,"
I'm seeking guidence here.
Can I use Multi Layers Perceptron (MLP), e.g regular flat neural networks, for image classification?
Will they perform better than Fisher Faces?
Is it difficult to do image classification with a MLP network? 
It's on basic level like classifying objects and not detailed structures and patterns. 
Important to me is that the MLP need to be trained with pictures that can have noise in background and different light shadows.
","['classification', 'image-recognition', 'multi-label-classification']","
let me try to answer your question. yes, you can use multilayer perceptron to image classification. Multilayer Perceptron is topology
the most common of ANN, where perceptrons are connected to form layers. An MLP has
input layer, at least one
hidden layer, and output layer.
Multilayer perceptron is one method
many used. one of them, regards research on classification
human skin based on its color, Khan
(Khan, Hanbury, Stöttinger, & Bais, 2012)
compare the nine methods for
classifications include BayesNet, J48, Multilayer Perceptron (MLP),
Naive Bayes, Random Forest, and SVM.
The results show that the Multilayer Perceptron (MLP)
produce the highest performance after
Random Forest and J48.
"
Reinforcement learning with industrial continuous process,"
I am new to RL and wish to realize a RL control for an industrial process. The goal is to control the temperature and humidity in a vegetal food production chamber. 
States: External temperature and humidity, internal temperature and humidity, percentage of the proportional valves controlling heater, cooler and steam for humidity. The goal is to keep temperature and humidity in the chamber (measures) as close as possible to the desired values (the setpoints). 
Agent actions: Increase/decrease the percentage of the proportional valves controlling the actuators. 
Rewards: Deviation between measure and setpoints (small deviation => high reward, high deviation => low reward).
I have data available, the history of states and actions from a real system. The actions are made by several PID controllers (some of them in cascade). So far I have about 3 month every minutes (with some stops sometimes when a chamber is for example cleaned). The data are continuously logged and every month I get more data. The data includes bad/unwanted states. 
For training the RL agent, I am planning to simulate the environment using a supervised learning model (with the predict function), probably XGboost. Is it feasible, are there pitfalls to avoid in this case?
","['reinforcement-learning', 'supervised-learning']","
RL is a generic technique that can be applied to any MDP system. From the looks of it you have data to produce a state-space model of your system (system identification, excluding your existing control loops) and then you can use that to drive a simulated exploration of your process and discover a control policy. As this is a continuous process, some discretization will be required
so yes, your quest is feasible, but not trivial! 
I experimented sometime ago with the cartpole balancing system, which is a toy, but I found that a good old PID was much better than AI :)
"
What is the difference between FC and MLP in as used in PointNet?,"
I am trying to understand the PointNet network for dealing with point clouds and struggling with understanding the difference between FC and MLP: 

""FC is fully connected layer operating on each point. MLP is
  multi-layer perceptron on each point.""

I understand how fully connected layers are used to classify and I previously thought, was that MLP was the same thing but it seems varying academic papers have a differing definition from each other and from general online courses. In PointNet what is meant by a shared MLP different to a standard feedforward fully connected network?

","['convolutional-neural-networks', 'models', 'pytorch']","
An MLP is just a fully-connected feedforward neural net. In PointNet, a shared MLP means that you are applying the exact same MLP to each point in the point cloud.
Think of a CNN's convolutional layer. There you apply the exact same filter at all locations, and hence the filter weights are shared or tied. If they were not shared, you'd have potentially different filters (MLPs) at each pixel (point), updating independently.
As an example, let $f_\theta$ be an MLP with parameters $\theta$. Say we have a 3D point cloud $[\vec{x}_1,\ldots,\vec{x}_n]\subseteq \mathbb{R}^3$. If we apply $f_\theta$ as a shared MLP in the way PointNet describes, the result would be $[f_\theta(\vec{x}_1),\ldots,f_\theta(\vec{x}_n)]$.
"
What method to identify markers in data series via machine learning,"
I have data that is collected from several different instruments simultaneously that is generally analyzed on a location-by-location basis.  A skilled interpreter can identify ""markers"" in the data that represent a certain change in conditions with depth - each marker only occurs once in each series of data.  However, it is possible that a maker is absent either due to missing data or that physical condition not existing.
Often, there are dozens of these markers per location and thousands, if not 10's of thousands of measurements that need to be interpreted.  The task is not that difficult and there are many strong priors that can be used to guide interpretation.  E.g., if marker A is in location #1, and location #2 is very close to location #1, it is likely that marker A will be present in a very similar relative position.  Also, if you have markers A, B, and C they will always be in that order.  Although it could be that you have A/B/C or A/C or B/C, etc.
I am including a hand-sketched example below with 4 example locations and one data stream (I normally have 4-5 data streams per location. 
I am looking for guidance on the type of algorithm to apply to this problem.  I have explored Dynamic Time warping, but the issue is that with 10-20k data samples per location, and thousands of locations, the problem becomes computationally challenging.
Also, in general you may have 10000 locations, with maybe 100 that have been hand interpreted by an expert.

","['pattern-recognition', 'time-series']","
Thresholding based on variance is one method for anomaly detection in time-series data. 
"
"What do we mean by saying ""VC dimension gives a LOOSE, not TIGHT bound""?","
From what I understand VC dimension is what establishes the feasibility of learning for infinite hypothesis sets, the only kind we would use in practice. 
But, the literature (i.e. Learning from Data) states that VC gives a loose bound, and that in real applications, learning models with lower VC dimension tend to generalize better than those with higher VC dimension. So, a good rule of thumb would be to require at least 10xVC dimension examples in order to get decent generalization.
I am having trouble interpreting what loose bound means. Is the VC generalization bound loose due to its universality? Meaning, its results apply to all hypothesis sets, learning algorithms, input spaces, probability distributions, and binary target functions.
","['machine-learning', 'computational-learning-theory', 'vc-dimension', 'vc-theory']",
What is the target Q-value in DQNs?,"
I understand that in DQNs, the loss is measured by taking the MSE of outputted Q-values and target Q-values.
What does the target Q-values represent? And how is it obtained/calculated by the DQN?
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'value-functions']","
When training a Deep Q network with experienced replay, you accumulate what is known as training experiences $e_t = (s_t, a_t, r_t, s_{t+1})$. You then sample a batch of such experiences and for each sample you do the following.

Feed $s_t$ into the network to get $Q(s,a;\theta)$.
Feed $s_{t+1}$ into the network to get $Q(s’,a’,\theta)$.
Choose $max_aQ(s’,a’,\theta)$ and set $ \gamma max_aQ(s’,a’,θ)$ + $r_t$ as the target of the network.
Train the network with $s_t$ as input to update $\theta$. The output from the input of $s_t$ is $Q(s,a,\theta)$ and the gradient descent step minimises the squared distance between $Q(s,a,\theta)$ and $\gamma max_aQ(s’,a’,θ)$ + $r_t$

"
Is there a notion of generalization in unsupervised learning?,"
I've been learning a little bit about generalization theory, and in particular, the PAC (and PAC-Bayes) approach to thinking about this problem.
So, I started to wonder if there is an analogous version of ""generalization"" in Unsupervised Learning? I.e., is there a general framework that encapsulates how ""good"" an unsupervised learning method is? There's reconstruction error for learning lower dimensional representations, but what about unsupervised clustering?
Any ideas?
","['machine-learning', 'unsupervised-learning', 'generalization', 'pac-learning']",
Is there a good and easy paper to code policy gradient algorithms (REINFORCE) from scratch?,"
I am interested in learning about policy gradient algorithms and REINFORCE. Can you suggest a good and easy paper that I can use to code them from scratch?
","['reinforcement-learning', 'reference-request', 'policy-gradients', 'implementation', 'reinforce']",
How can I use GPT-2 to modify seed text of one form into a different form (LENGTH INVARIANT) whilst retaining meaning?,"
I am currently starting a research project whereby I am trying to convert text of one form into another.
i.e. If I were to write a seed sentance of the form ""Scientists have finally achieved the ability to induce dreams of electric sheep in the minds of anaesthetized robots"" I would like GPT-2 to convert this into ""Robots have finally had dreams of electric sheep whilst being anaesthetized by scientists."" or some coherent permutation of the underlying structure whereby the main logic of the text is conveyed albeit roughly. 
The current open source implementation of GPT-2 seeks to predict the next word, i.e. the seed text is given ""Scientist have finally"" and the generated text would be "" started being paid enough!""
My first presumption was to use some form of GAN, however it became quickly evident that:

Recent work has shown
  that when both quality and diversity is considered, GAN-generated text
  is substantially worse than language model generations (Caccia et al.,
  2018; Tevet et al., 2018; Semeniuta et al., 2018).

How could I most effectively achieve this? Thanks.
","['tensorflow', 'pytorch', 'text-summarization', 'gpt', 'text-generation']",
"Does the paper ""On the difficulty of training Recurrent Neural Networks"" (2013) assume, falsely, that spectral radii are $\ge$ square matrix norms?","
(link to paper in arxiv)
In section 2.1 the authors define $\gamma$ as the maximum possible value of the derivative of the activation function (e.g. 1 for tanh.) Then they have this to say: 

We first prove that it is sufficient for $\lambda_1 < \frac{1}{\gamma}$, where $\lambda_1$ is the absolute value of the largest eigenvalue of the recurrent weight matrix $W_{rec}$, for the vanishing gradient problem to occur. 

Then they use the submultiplicity ($\|AB\| \le \|A\|\|B\|$) of the 2-norm of the Jacobians to obtain the following inequality: 
$$ \forall x, \| \frac{\partial x_{k+1}}{\partial x_k} \| \le \| W_{rec}^\top \| \| diag(\sigma'(x_k))\| < \frac{1}{\gamma} \gamma < 1 $$
Here

$x_k$ is the pre-activated state vector of the RNN 
$W_{rec}$ is the weight matrix between states (i.e. $x_k = W_{rec} \times \sigma(x_{k-1}) + b$ ) 
$\sigma()$ is the activation function for the state vector 
$diag(v)$ is the diagonal matrix version of a vector $v$

They appear to be either substituting the norm of the weight matrix $\|W_{rec}^\top\|$ for its largest eigenvalue $|\lambda_1|$ (eigenvalues are the same for transposes) or just assuming that this norm is less than or equal to the eigenvalue. This bothers me because the norm of a matrix is bounded below, not above, by this eigenvalue/spectral radius (see lemma 10 here and this math SE question)  
They seem to assume that 
$$\| W_{rec}^\top \| \le \lambda_1 < \frac{1}{\gamma}  $$ 
But really 
$$ \| W_{rec}^\top \| \ge \lambda_1 $$
","['papers', 'vanishing-gradient-problem']",
What exactly is averaged when doing batch gradient descent?,"
I have a question about how the averaging works when doing mini-batch gradient descent.
I think I now understood the general gradient descent algorithm, but only for online learning. When doing mini-batch gradient descent, do I have to:

forward propagate
calculate error
calculate all gradients

...repeatedly over all samples in the batch, and then average all gradients and apply the weight change? 
I thought it would work that way, but recently I have read somewhere that you basically only average the error of each example in the batch, and then calculate the gradients at the end of each batch. That left me wondering though, because, the activations of which sample in the mini-batch am I supposed to use to calculate the gradients at the end of every batch?
It would be nice if somebody could explain what exactly happens during mini-batch gradient descent, and what actually gets calculated and averaged.
","['backpropagation', 'gradient-descent', 'feedforward-neural-networks', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']","
Introduction
First of all, it's completely normal that you are confused because nobody really explains this well and accurately enough. Here's my partial attempt to do that. So, this answer doesn't completely answer the original question. In fact, I leave some unanswered questions at the end (that I will eventually answer).
The gradient is a linear operator
The gradient operator $\nabla$ is a linear operator, because, for some $f : \mathbb{R} \rightarrow \mathbb{R} $ and $g: \mathbb{R} \rightarrow \mathbb{R}$, the following two conditions hold.

$\nabla(f + g)(x) = (\nabla f)(x) + (\nabla g)(x),\; \forall x \in \mathbb{R}$
$\nabla(kf)(x) = k(\nabla f)(x),\; \forall k, x \in \mathbb{R}$

In other words, the restriction, in this case, is that the functions are evaluated at the same point $x$ in the domain. This is a very important restriction to understand the answer to your question below!
The linearity of the gradient directly follows from the linearity of the derivative. See a simple proof here. 
Example
For example, let $f(x) = x^2$, $g(x) = x^3$ and $h(x) = f(x) + g(x) = x^2 + x^3$, then $\frac{dh}{dx} = \frac{d (x^2 + x^3)}{d x} = \frac{d x^2}{d x} + \frac{d x^3}{d x} = \frac{d f}{d x} + \frac{d g}{d x} = 2x + 3x$. 
Note that both $f$ and $g$ are not linear functions (i.e. straight-lines), so the linearity of the gradients is not just applicable in the case of straight-lines.
Straight-lines are not necessarily linear maps
Before proceeding, I want to note that there are at least two notions of linearity. 

There's the notion of a linear map (or linear operator), i.e. which is the definition above (i.e. the gradient operator is a linear operator because it satisfies the two conditions, i.e. it preserves addition and scalar multiplication). 
There's the notion of a straight-line function: $f(x) = c*x + k$. A function can be a straight-line and not be a linear map. For example, $f(x) = x+1$ is a straight-line but it doesn't satisfy the conditions above. More precisely, in general, $f(x+y) \neq f(x) + f(y)$, and you can easily verify that this is the case if $x = 2$ and $y=3$ (i.e. $f(2+3) = 6$, $f(2) = 3$, $f(3) = 4$, but $f(2) + f(3) = 7 \neq f(2+3)$.

Neural networks
A neural network is a composition of (typically) non-linear functions (let's ignore the case of linear functions), which can thus be represented as $$y'_{\theta}= f^{L}_{\theta_L} \circ f^{L-1}_{\theta_{L-1}} \circ \dots \circ f_{\theta_1},$$ where 

$f^{l}_{\theta_l}$ is the $i$th layer of your neural network and it 
computes a non-linear function
${\theta_l}$ is a vector of parameters associated with the $l$th layer
$L$ is the number of layers, 
$y'_{\theta}$ is your neural network, 
$\theta$ is a vector containing all parameters of the neural network
$y'_{\theta}(x)$ is the output of your neural network
$\circ $ means the composition of functions

Given that $f^l_{\theta}$ are non-linear, $y'_{\theta}$ is also a non-linear function of the input $x$. This notion of linearity is the second one above (i.e. $y'_{\theta}$ is not a straight-line). In fact, neural networks are typically composed of sigmoids, ReLUs, and hyperbolic tangents, which are not straight-lines.
Sum of squared errors
Now, for simplicity, let's consider the sum of squared error (SSE) as the loss function of your neural network, which is defined as
$$
\mathcal{L}_{\theta}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^N  \mathcal{S}_{\theta}(\mathbf{x}_i, \mathbf{y}_i) = \sum_{i=1}^N (\mathbf{y}_i - y'_{\theta}(\mathbf{x}_i))^2
$$
where 

$\mathbf{x} \in \mathbb{R}$ and $\mathbf{y} \in \mathbb{R}$ are vectors of inputs and labels, respectively
$\mathbf{y}_i$ is the label for the $i$th input $\mathbf{x}_i$
$\mathcal{S}_{\theta}(\mathbf{x}_i, \mathbf{y}_i) = (\mathbf{y}_i - y'_{\theta}(\mathbf{x}_i))^2$

Sum of gradients vs gradient of a sum
Given the gradient is a linear operator, one could think that computing the sum of the gradients is equal to the gradient of the sums.
However, in our case, we are summing $\mathcal{S}_{\theta}(\mathbf{x}_i, \mathbf{y}_i)$ and, in general, $\mathbf{x}_i \neq \mathbf{x}_j$, for $i \neq j$. So, essentially, the SSE is the sum of the same function, i.e. $S_{\theta}$, evaluated at different points of the domain. However, the definition of a linear map applies when the functions are evaluated at the same point in the domain, as I said above.
So, in general, in the case of neural networks with SSE, the gradient of the sum may not be equal to the sum of gradients, i.e. the definition of the linear operator for the gradient doesn't apply here because we are evaluating every squared error at different points of their domains.
Stochastic gradient descent
The idea of stochastic gradient descent is to approximate the true gradient (i.e. the gradient that would be computed with all training examples) with a noisy gradient (which is an approximation of the true gradient).
How does the noisy gradient approximate the true gradient?
In the case of mini-batch ($M \leq N$, where $M$ is the size of the mini-batch and $N$ is the total number of training examples), this is actually a sum of the gradients, one for each example in the mini-batch. 
The papers Bayesian Learning via Stochastic Gradient Langevin Dynamics (equation 1) or Auto-Encoding Variational Bayes (in section 2.2) use this type of approximation. See also these slides.
Why?
To give you some intuition of why we sum the gradients of the error of each input point $\mathbf{x}_i$, let's consider the case $M=1$, which is often referred to as the (actual) stochastic gradient descent algorithm. 
Let's assume we uniformly sample an arbitrary tuple $(\mathbf{x}_j, \mathbf{y}_j)$ from the dataset $\mathcal{D} = \{ (\mathbf{x}_i, \mathbf{y}_i) \}_{i=1}^N$. 
Formally, we want to show that 
\begin{align}
\nabla_{\theta} \mathcal{L}_{\theta}(\mathbf{x}, \mathbf{y}) 
&=
\mathbb{E}_{(\mathbf{x}_j, \mathbf{y}_j) \sim \mathbb{U}}\left[ \nabla_{\theta} \mathcal{S}_{\theta} \right] \label{1} \tag{1}
\end{align}
where 

$\nabla_{\theta} \mathcal{S}_{\theta}$ is the gradient of $\mathcal{S}_{\theta}$ with respect to the parameters $\theta$
$\mathbb{E}_{(\mathbf{x}_j, \mathbf{y}_j) \sim \mathbb{U}}$ is the expectation with respect to the random variable associated with a sample $(\mathbf{x}_j, \mathbf{y}_j)$ from the uniform distribution $\mathbb{U}$

Under some conditions (see this), we can exchange the expectation and gradient operators, so \ref{1} becomes
\begin{align}
\nabla_{\theta} \mathcal{L}_{\theta}(\mathbf{x}, \mathbf{y}) 
&=
\nabla_{\theta} \mathbb{E}_{(\mathbf{x}_j, \mathbf{y}_j) \sim \mathbb{U}}\left[  \mathcal{S}_{\theta} \right] 
\label{2} \tag{2}
\end{align}
Given that we uniformly sample, the probability of sampling an arbitrary $(\mathbf{x}_j, \mathbf{y}_j)$ is $\frac{1}{N}$. So, equation \ref{2} becomes
\begin{align}
\nabla_{\theta} \mathcal{L}_{\theta} (\mathbf{x}, \mathbf{y}) 
&=
\nabla_{\theta} \sum_{i=1}^N \frac{1}{N} \mathcal{S}_{\theta}(\mathbf{x}_i, \mathbf{y}_i) \\
&=
\nabla_{\theta} \frac{1}{N}  \sum_{i=1}^N \mathcal{S}_{\theta}(\mathbf{x}_i, \mathbf{y}_i) 
\end{align}
Note that $\frac{1}{N}$ is a constant with respect to the summation variable $i$ and so it can be taken out of the summation.
This shows that the gradient with respect to $\theta$ of the loss function $\mathcal{L}_{\theta}$ that includes all training examples is equivalent, in expectation, to the gradient of $\mathcal{S}_{\theta}$ (the loss function of one training example). 
Questions

How can we extend the previous proof to the case $1 < M \leq N$?
Which conditions need exactly to be satisfied so that we can exchange the gradient and the expectation operators? And are they satisfied in the case of typical loss functions, or sometimes they aren't (but in which cases)?
What is the relationship between the proof above and the linearity of the gradient?

In the proof above, we are dealing with expectations and probabilities!

What would the gradient of a sum of errors represent? Can we still use it in place of the sum of gradients? 

"
"What does ""In each generation, 25% of offspring resulted from mutation without crossover"" mean in the context of NEAT?","
I am reading through the NEAT paper. In parameter settings, page 15, there is:

In each generation, 25% of offspring resulted from mutation without crossover.

What does it mean?
","['papers', 'neat', 'crossover-operators', 'mutation-operators', 'genetic-operators']",
Why Pixel RNN (Row LSTM) can capture triangular contexts?,"
I'm reading the paper Pixel Recurrent Neural Network. I have a question about Row LSTM. Why Row LSTM can capture triangular contexts?
In this paper,

the kernel of the one-dimensional convolution has size $k \times 1$ where $k \geq 3$; the larger value of $k$ the broader the context that is captured.

The one-dimensional kernel can capture only the left context. (Is this correct?)
The $n \times n$ kernel such as
$$
\begin{bmatrix}
1 & 1 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
$$
can capture triangular contexts.
Is this correct?
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'deep-neural-networks', 'deepmind']",
Neural networks with internal dynamics in the state-space form,"
Neural networks with feedback (Hopfield, Hamming, etc.) differ from ordinary neural networks (multilayer perceptrons, etc.), which turns them into a dynamic element with its own internal dynamics (if we consider them as a separate dynamic link). The following question naturally arises - is it possible to represent them in the form of state spaces? 
Nuance is that feedback is created by introducing a delay element, which means recording a neural network exclusively in a discrete form. Is continuous recording possible? What acts as matrices A, B, C, D? How does the presence of nonlinear activation functions affect?
The only more or less useful information that I managed to find is in this article:
On neural networks in identification and control of dynamic systems. 3.2 Paragraph. Page 8
But my assumptions are only confirmed there, which does not clarify the situation.
In general, if someone has come across this and can assist in studying the issue, please share links, possibly examples, etc.
","['recurrent-neural-networks', 'models', 'control-problem']",
How does a software license apply to pretrained models?,"
Google provides a lot of pretrained tensorflow models, but I cannot find a license.
I am interested in the tfjs-models. The code is licensed Apache-2.0, but the models are downloaded by the code, so the license of the repository probably does not apply to the models and I am not able to find anywhere a note about the license of the pretrained models.
How should I handle this, especially when I may want to distribute models derived from the pretrained Google models?
","['tensorflow', 'pretrained-models']","
I am not sure if a pretrained machine learning model is actually protected by copyrights or not. Copyright protection exists to protect the creators of creative works from having their work ""stolen"", and I am not sure if training a ML model is an act of creativity.
That said, assuming that a pretrained ML model is actually protected by copyrights, then it is more likely that the model is a derived work of the data set used for training than that it is a derived work of the software that uses the model.
The software reads the model in as data, assuming that the software can be used with many differently trained models. In that case, the software and the model are considered completely independent works in the same way that MS Word and the documents you write with it are independent works for copyright.
Thus, if you want to publish the trained model with a license, I would recommend to use the BSD license that was also used for the training set.
"
Is it possible to guide a reinforcement learning algorithm?,"
I have just started to study reinforcement learning and, as far as I understand, existing algorithms search for the optimal solution/policy, but do not allow the possibility for the programmer to suggest a way to find the solution (to guide their learning process). This would be beneficial for finding the optimal solution faster.
Is it possible to guide the learning process in (deep) reinforcement learning?
","['reinforcement-learning', 'deep-rl', 'supervised-learning', 'efficiency', 'active-learning']","
Here are two very related interesting papers:

Learning from Human Preferences
Improving Reinforcement Learning with Human Input

"
MCTS RAVE performing badly in Board Game AI,"
I'm using Monte Carlo Tree Search with UCT selection to try and build an AI player for a complex multiplayer board game. My regular UCT MCTS seems to be working fine, winning with random and basic greedy players or low-depth 'paranoid' alpha-beta variant player, but I've been looking for some methods to improve it and I found RAVE. 
""In RAVE, for a given game tree node N, its child nodes Ci store not only the statistics of wins in playouts started in node N but also the statistics of wins in all playouts started in node N and below it, if they contain move i (also when the move was played in the tree, between node N and a playout). This way the contents of tree nodes are influenced not only by moves played immediately in a given position but also by the same moves played later."".
I've found a lot of literature about it and it was supposed to give good results - 70%-80% win rate against basic UCT on a game of TicTacToe3D. I implemented it as a sort of benchmark, a 4x4x4 version, before trying it on my target game. But, however I tried tuning the parameters, I've been getting worse results, the win rate is at best arount 46%.
I've been calculating the node values like this: 
visits[i] is a number of visits for child i of parent p that selection is performed on, wins[i] is a number of wins according to UCT, AMAFvisits and AMAFwins are assigned based on the node's source action -> updated after a finished simulation if a sourceAction (the action that changed the game state into this state) was played in the simulation by the player of the MCTS tree root node. 
for (int i = 0; i < nChildren; i++) {
    if (visits[i] < 1) {
        value = Double.MAX_VALUE - rnd.nextDouble();
    }
    else if (m[i] < 1) {
        double vUCT = wins[i]/visits[i] + C*Math.sqrt(Math.log(sumVisits)/(visits[i]));
        value = vUCT;
    }
    else {
        double beta = Math.sqrt(k/(3*visits[i] + k));
        double vRAVE = (AMAFscores[i])/(m[i]) + C*Math.sqrt(Math.log(mChildren)/(m[i]));
        double vUCT = (wins[i])/(visits[i])+ C*Math.sqrt(Math.log(sumVisits)/(visits[i]));
        value = beta * vRAVE + (1 - beta) * vUCT;
        value += rnd.nextDouble() * eps;
        /*double beta = Math.sqrt(k/(3*visits[i] + k));
        double vRAVE = (AMAFscores[i])/(m[i]);
        double vUCT = (wins[i])/(visits[i]);
        value = beta * vRAVE + (1 - beta) * vUCT;
        value += C*Math.sqrt(Math.log(sumVisits)/(visits[i]));
        value += rnd.nextDouble() * eps;*/
    }
    if (maxValue <= value) {
        maxValue = value;
        index = i;
    }
}
chosen = tree.getTreeNode(children.get(index));

Here's a paint rendition of my understanding of how RAVE should work -> https://i.stack.imgur.com/naQXE.jpg.
Am I missing something? Is my implementation wrong? Here's the rest of the code responsible for traversing the tree in a 'rave way': https://www.paste.org/104476. The expand function on tree expands the tree for all actions, and returns a random one which then gets visited, the others are to be visited in other iterations.
I first tested the code on k = 250 like the authors of the benchmark paper https://dke.maastrichtuniversity.nl/m.winands/documents/CIG2016_RAVE.pdf suggested and on 100, 1000 and 10000 iterations, with tree depth 20 or 50. I also experimented with other k values and other params.
","['game-ai', 'monte-carlo-tree-search']","
No implementation of RAVE in academia that I am aware of implements an exploration factor into the AMAF value (C*Math.sqrt(Math.log(mChildren)/(m[i])) into your implementation). The AMAF value is only used to bias the search towards nodes with more promising moves, and doesn't care about exploring less visited moves. That doesn't mean it isn't advantageous to do so, but I am somewhat doubtful. I would test without that term.
"
Leaky Discriminators and Siamese GANs,"
Is it useful to use Siamese network structure for GANs  like sharing latent space between generators in cGAN , or also with discriminators. 
Thinking about it, like giving the generator tips about the knowledge-base of the discriminator, to target the problem of discriminator forgetting and increase the chance of convergence. Because than the discriminator prediction confidence is dependent of the generators construction (what’s anyway the case, but now on a system base ). 
What do you think ? Didn’t see it in recent papers that often, just in this one, but it’s more like a pix2pix transformation, and just works so well, because they are using the segmentation masks of A, to get good segmentation results on B‘ ( A transformed to B). Didn’t find any approaches to sth like leaky discriminators. 
","['neural-networks', 'deep-learning', 'ai-design']",
What are some resources on computational learning theory?,"
Pretty soon I will be finishing up Understanding Machine Learning: From Theory to Algorithms by Shai Ben-David and Shai Shalev-Shwartz. I absolutely love the subject and want to learn more, the only issue is I'm having trouble finding a book that could come after this. Ultimately, my goal is to read papers in JMLR's COLT.

Is there a book similar to ""Understanding Machine Learning: From Theory to Algorithms"" that would progress my knowledge further and would go well after reading UML?
Is there any other materials (not a book) that could allow me to learn more or prepare me for reading a journal like the one mentioned above?

(Also, taking courses in this is not really an option, so this will be for self-study).
(Note that I have also asked this question here on TCS SE, but it was recommended I also ask here.)
","['reference-request', 'papers', 'computational-learning-theory', 'pac-learning', 'books']",
Why does this model have 12 parameters?,"
I guess the model shown in this image (img_1)

is the same as the one in this image (img_2)

I was trying to build a neural net like that.
This keras code is to do the job.
model = Sequential()
model.add(Dense(3, input_dim=3, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

However, print(model.summary()) outputs 
Model: ""sequential_17""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_31 (Dense)             (None, 3)                 12        
_________________________________________________________________
dense_32 (Dense)             (None, 1)                 4         
=================================================================

There are 3 ws and 1 b in the hidden layer. Why does this model have 12 parameters?
","['neural-networks', 'machine-learning', 'keras']",
Could clustering be used to parse pdf documents to get headings and titles?,"
I'm a bit new to AI and I'd like to use some kind of clustering algorithm to solve a problem:
I'm trying to parse pdf documents to get headings and titles. I can parse pdf to html and I'm then able to get some information on the lines of the document. I've identified some properties that can be useful for identifying the headings.

font-size (int): of course it's quite usual that heading's font-size is bigger than normal text
font-family (string): it's possible for headings to be  bold so font-family may differ
left property (int): it's also possible that headings are aligned a bit to the right, there's an indentation that's not always there on normal paragraphs
bonus boolean: I have identified some properties that I can combine to get a boolean value. When the boolean is set to true it can increase the chances of the paragraph being a heading.

Of course, these are not rules that apply to all headings. Some headings may follow some of these but not all of them. It could also be possible that some 'normal' paragraphs follow all these points, but what I've seen is that, in general, those rules where what made headings different from paragraphs.
With this information, is there a way of doing what I'm looking for? As I said, I'm new to AI even though I have a background in CS and mathematics. I thought clustering could be interesting since I'm trying to create 2 clusters: headings and normal paragraphs.
What algorithm do you think might work for this use case. Should I look outside clustering?
","['machine-learning', 'natural-language-processing', 'unsupervised-learning', 'clustering']","
Yes, you could use clustering: Encode your features as a feature vector and feed it into a clustering algorithm (see Finding Groups in Data for a comprehensive description of these). You could use agglomerative clustering, which would give you groups of similar items; perhaps different level headings will be clustered together.
Alternatively you could try a decision tree, something like ID3, which would also be suitable; for this you'd need some annotated training data, though. But with a small amount of data you might solve it, if your items are clearly separated.
"
Replace epsilon greedy action selection and the standard DQN by an Independent Gaussian Noise Network Model,"
Here is my code
Recently, I solved the game of Atari Breakout using a classic DQN model. The convergence of the mean reward slowly improved during three days. I was interested in learning a method which may help me improving the convergence speed. I found the following article : https://arxiv.org/pdf/1706.10295v3.pdf. It says I can use an Independent Gaussian Noise to outperform an standard DQN. 
Here is my Noisy DQN model : 
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class NoisyLinear(nn.Linear): #Independent Gaussian Noise used with NoisyDQN model
    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):
        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)

        self.sigma_weight = nn.Parameter(torch.full((out_features, in_features), sigma_init))
        self.register_buffer(""epsilon_weight"", torch.zeros(out_features, in_features))

        if bias: 
            self.sigma_bias = nn.Parameter(torch.full((out_features,), sigma_init))
            self.register_buffer(""epsilon_bias"", torch.zeros(out_features))

        self.reset_parameters()

    def reset_parameters(self):
        std = math.sqrt(3/self.in_features)
        self.weight.data.uniform_(-std, std)
        self.bias.data.uniform_(-std, std)

    def forward(self, input):
        self.epsilon_weight.normal_()
        bias = self.bias
        if bias is not None:
            self.epsilon_bias.normal_()
            bias = bias + self.sigma_bias * self.epsilon_bias.data
        return F.linear(input, self.weight + self.sigma_weight * self.epsilon_weight.data, bias)


class NoisyDQN(nn.Module):
    """"""
    Look at https://arxiv.org/pdf/1706.10295v3.pdf
    """"""

    def __init__(self, input_shape, num_actions):
        super(NoisyDQN, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
            nn.ReLU(),
        )

        self.conv_output_size = self.get_output_size(input_shape)

        self.linear = nn.Sequential(
           NoisyLinear(in_features=self.conv_output_size, out_features=512),
           nn.ReLU(),
           NoisyLinear(in_features=512, out_features=num_actions)
        )

    def get_output_size(self, input_shape):
        output = self.conv(torch.zeros((1, *input_shape)))
        return int(np.prod(output.shape))

    def forward(self, input):
        self.layer1 = self.conv(input)
        self.layer1 = self.layer1.reshape(-1, self.conv_output_size)

        return self.linear(self.layer1)

The idea is to replace the epsilon greedy action selection and my standard DQN model by the Noisy Network you can see just above.
The code run successfully, but it doesn't improve even a bit. How can I fix that?
UPDATE

After nearly 200k episodes, I am still between 1.5 and 2 reward. The maximum reward I can get on the Atari Breakout game is about 500. With a standard DQN, after 100k episodes, I am near 11 as reward.
On the above picture the X-axis is the number of episodes and the Y-axis is the mean reward over the last 100 rewards. The Y-axis is describe with 
mean_reward = np.mean(self.total_rewards[-100:])
UPDATE
After about 8 hours of training, I got this

As you can see, it is not working as good as in the paper. I worked a lot with the hyperparameters, but not changed. 
","['reinforcement-learning', 'python', 'dqn', 'pytorch']",
"Is the paper ""Reducing the Dimensionality of Data with Neural Networks"" by Hinton relevant?","
Is the paper ""Reducing the Dimensionality of Data with Neural Networks"" by G. Hinton and R. Salakhutdinov relevant?
It seems that the deep learning textbook by Goodfellow, Bengio & Courville (2016) doesn't cite that paper. 
Does that indicate that paper is not as important as others to Deep learning? If yes, I would skip this one to accelerate my process of learning.
","['research', 'papers']","
No; there are too many publications around for anybody to keep track of everything, so unless it is a seminal paper, you cannot draw any conclusions from this. They could simply have missed it.
Especially if it is a textbook for beginners, more advanced papers are often not mentioned, as they might be too complex to understand.
So you have to decide for yourself if that paper is relevant to you. To me it sounds like a specific application of neural networks; if the dimensionality of the input data is an issue for you, it might be, otherwise probably not. 
"
How would semantic segmentation work with a non convolutional neural network,"
Listening to lectures, convolutional neural network seems to be an improvement over a simple neural network, where for example, you take every pixel in the image, flatten it to a vector, and feed it to ANN with a couple layers. Therefore semantic segmentation should be possible to perform on a classic ANN.
I dont understand exactly how a CNN can classify each pixel in the image to do semantic segmentation.
The way semantic segmentation is explained in lectures is the output of the CNN is fed into it backwards, is that the same as with GAN?
If the output of a CNN is a value between 0 and 1 for each class, how exactly can those values be fed back through a CNN backwards to classify each pixel in the image? Through back propagation?
My understanding is that it should be possible to do the same to a regular ANN described above. 
Can someone explain why or why not it wouldn't be possible, and how semantic segmentation feeds output back through network to classify each pixel
Thanks,
","['convolutional-neural-networks', 'generative-adversarial-networks']",
Heavily mixing signal differentiation from Open Set of backgrounds via CNN,"
I am currently attempting to detect a signal from background noise.  The signal is pretty well known but the background has a lot of variability.  I've since come to know this problem as Open Set Recognition.  Another complicating factor is that the signal mixes with the background noise (think equivalent to a transparent piece of glass in-front of scenery for a picture, or picking out the sound of a pin drop in an office space).
When I started this project, it seemed like the current state of the art in this space was generating Spectrograms and feeding them to a CNN and this is the path I've followed.  I'm at a place where I think I've overcome most of the initial problems you might encounter but I'm still not getting good enough results for a project solution.
Here's the overall steps I've gone through:

Generate 17000 ground truth ""signals"" and 17000 backgrounds (negatives or other classes depending on what nn scheme I'm training)

Generate separate test samples (not training samples but external model validation samples: ""blind test"") where I take the backgrounds and randomly overlay the signal into it at various intensities.

My first attempt was with a pre-built library training solution (ImageAI) with resnet50 base model.  This solution is a multiclass classifier so I had 400 each of the signal + 5 other classes that were the background. It did not work well at classifying the signal. I don't think I ever got this off the ground for two reasons a) My spectrogram pictures were not optimised (waay to large) and b) I couldn't adjust the image input shape via the library. It mostly just ended up classifying one background class.

I then started building my own neural nets.  The first reason to make sure my spectrogram input shape was matched in the input shape of the CNN.  The second reason was to test various neural net schemes to see what worked best.

The first net I built was a simple feed forward net with a couple of dense layers.  This trains to .9998 val_acc.  It (like the rest of what I try) produces poor results on my blind tests, in the range of 60% true positive.
def build(width, height, depth, classes):
     # initialize the model along with the input shape to be
     # ""channels last"" and the channels dimension itself
     model = Sequential()
     inputShape = (height, width, depth)
     chanDim = -1

     # if we are using ""channels first"", update the input shape
     # and channels dimension
     if K.image_data_format() == ""channels_first"":
         inputShape = (depth, height, width)
         chanDim = 1
     model.add(Flatten())
     model.add(Dense(512, input_shape=(inputShape),activation=""relu""))
     model.add(Dense(128, activation=""relu""))
     model.add(Dense(32, activation=""relu""))
     # sigmoid classifier
     model.add(Dense(classes))
     model.add(Activation(""sigmoid""))

     # return the constructed network architecture
     return model  


I then try a ""VGG Light"" model.  Again, trains to .9999 but gives me only 62% true positive results on my blind tests
 def build(width, height, depth, classes):
     # initialize the model along with the input shape to be
     # ""channels last"" and the channels dimension itself
     model = Sequential()
     inputShape = (height, width, depth)
     chanDim = -1

     # if we are using ""channels first"", update the input shape
     # and channels dimension
     if K.image_data_format() == ""channels_first"":
         inputShape = (depth, height, width)
         chanDim = 1

     # CONV => RELU => POOL
     model.add(Conv2D(32, (3, 3), padding=""same"",            input_shape=inputShape))
     model.add(Activation(""relu""))
     model.add(BatchNormalization(axis=chanDim))
     model.add(MaxPooling2D(pool_size=(3, 3)))
     model.add(Dropout(0.25))

     # (CONV => RELU) * 2 => POOL
     model.add(Conv2D(64, (3, 3), padding=""same""))
     model.add(Activation(""relu""))
     model.add(BatchNormalization(axis=chanDim))
     model.add(Conv2D(64, (3, 3), padding=""same""))
     model.add(Activation(""relu""))
     model.add(BatchNormalization(axis=chanDim))
     model.add(MaxPooling2D(pool_size=(2, 2)))
     model.add(Dropout(0.25))

     # (CONV => RELU) * 2 => POOL
     model.add(Conv2D(128, (3, 3), padding=""same""))
     model.add(Activation(""relu""))
     model.add(BatchNormalization(axis=chanDim))
     model.add(Conv2D(128, (3, 3), padding=""same""))
     model.add(Activation(""relu""))
     model.add(BatchNormalization(axis=chanDim))
     model.add(MaxPooling2D(pool_size=(2, 2)))
     model.add(Dropout(0.25))
     model.add(GaussianNoise(.05))

     # first (and only) set of FC => RELU layers
     model.add(Flatten())
     model.add(Dense(1024))
     model.add(Activation(""relu""))
     model.add(BatchNormalization())
     model.add(Dropout(0.5))
     model.add(Dense(512))
     model.add(Activation(""relu""))
     model.add(BatchNormalization())
     model.add(Dropout(.5))
     model.add(Dense(128))       
     model.add(Activation(""relu""))
     model.add(BatchNormalization())     
     model.add(GaussianDropout(0.5))

     # sigmoid classifier
     model.add(Dense(classes))
     model.add(Activation(""sigmoid""))

     # return the constructed network architecture
     return model


I then try a ""full VGG"" net.  This again trains to .9999 but only a blind test true positive result of 63%.
def build(width, height, depth, classes):
    # initialize the model along with the input shape to be
    # ""channels last"" and the channels dimension itself
    model = Sequential()
    inputShape = (height, width, depth)
    chanDim = -1

    # if we are using ""channels first"", update the input shape
    # and channels dimension
    if K.image_data_format() == ""channels_first"":
        inputShape = (depth, height, width)
        chanDim = 1

    #CONV => RELU => POOL
    model.add(Conv2D(64, (3, 3), padding=""same"", input_shape=inputShape))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(3, 3)))
    #model.add(Dropout(0.25))

    # (CONV => RELU) * 2 => POOL
    model.add(Conv2D(128, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(128, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.25))

    # (CONV => RELU) * 2 => POOL
    model.add(Conv2D(256, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(256, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.25))

    # (CONV => RELU) * 2 => POOL
    model.add(Conv2D(512, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(512, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.25))

    # (CONV => RELU) * 2 => POOL
    model.add(Conv2D(1024, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(1024, (3, 3), padding=""same""))
    model.add(Activation(""relu""))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Dropout(0.25))
    model.add(GaussianNoise(.1))

    # first (and only) set of FC => RELU layers
    model.add(Flatten())
    model.add(Dense(8192))
    model.add(Activation(""relu""))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(4096))
    model.add(Activation(""relu""))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(1024))
    model.add(Activation(""relu""))
    model.add(BatchNormalization()) 
    model.add(GaussianDropout(0.5))

    # sigmoid classifier
    model.add(Dense(classes))
    model.add(Activation(""sigmoid""))

    # return the constructed network architecture
    return model


All of the above are binary_crossentropy trained in keras.

I've tried multi-class with these models as well but when testing them on the blind test they usually pick the background rather than the signal.

I've also messed around with Autoencoders to try and get the encoder to rebuild the signal well and then compare to known results but haven't been successful yet though I'd be willing to give it another try if everyone thought that might produce better results.

In the beginning I ran into unbalanced classification problems (I was noob) but under all the models shown above the classes all have the same number of samples.


I'm at the point where the larger VGG models trained on 34,000 samples is taking days and I don't see any better results than a basic, feed forward NN that takes 4 minutes to train.
Does anyone see the path forward here?
","['convolutional-neural-networks', 'keras', 'architecture']","
Thanks for the answers. If you are processing an audio signal I think the application of a low pass filter (lpf) would help to enhance the signal to noise ratio. This would help especially if the noise component occupies a large part of the spectrum. If the audio is human speech the majority of the energy is within the 300Hz to 3Khz region. Using a low pass filter with a cutoff frequency of 3Khz would eliminate noise that is in the higher part of the spectrum. You could implement the lpf as a pre-processing function. I am not knowledgeable on the implementation but a search should get you the info you need. I did find an article here. If I recall the process is to convert the time domain signal to the frequency domain using a FFT, then set a cutoff point and reconvert back to the time domain. I also know there are ways to implement that directly in the time domain.Hope this helps.
I am also supersized that if you achieve a high validation accuracy that your test set accuracy is so low. Your validation data should be data the network has not seen before just like your test data. Only thing I can think off is that the test data has a very different probability distribution than the training and validation data. How were the various data sets (train, test, validate) selected? Best choice is to select these randomly using something like sklearn train_test_split or Keras ImageDataGenerator flow from directory. Hope this helps.
"
Is having binary randomized unchanging neural network weights a good idea?,"
I am creating a neural network to experiment with, and I was wondering:

If I have weights randomly initialized to be either 1 or 0 for each neuron, and then I made it so that the weights cannot be changed, would that ruin the neural network? What would happen?

Note: There is no bias in this network.
","['neural-networks', 'weights']",
How does size of the dataset depend on VC dimension of the hypothesis class?,"
This might be a little broad question, but I have been watching Caltech youtube videos on Machine Learning, and in this video prof. is trying to explain how we should interpret the VC dimension in terms of what it means in layman terms, and why do we need it in practice.
The first part I think I understand, please correct me if I am wrong. VC Dimension dictates the number of effective parameters (i.e. degrees of freedom) that model has. In other words, the number of parameters the model needs in order to cover all possible label combinations for the chosen dataset. Now, the second part is not clear to me. The professor is trying to answer the question:

How does knowing the VC dimension of the hypothesis class affect number of samples we need for training?

Again, I apologize if all of this may be trivial, but I am new to the field and wish to learn as much as I can, so I can implement better and more efficient programs in practice.
","['computational-learning-theory', 'vc-dimension', 'vc-theory', 'sample-complexity', 'hypothesis-class']","
Given a hypothesis set $H$, the set of all possible mappings from $X\to Y$ where $X$ is our input space and $Y$ are our binary mappings: $\{-1,1\}$, the growth function, $\Pi_H(m)$, is defined as the maximum number of dichotomies generated by $H$ on $m$ points. Here a dichotomy is the set of $m$ points in $X$ that represent a hypothesis. A hypothesis is just a way we classify our points. Therefore with two labels we know,
$$\Pi_H(m)\leq 2^m$$
This is just counts every possible hypothesis. The VC dimension is then the largest $m$ where $\Pi_H(m)=2^m$.
Consider a 2D perceptron, meaning our $X$ is $\mathbb{R}^2$ and our classifying hyperlane is one-dimensional: a line. The VC dimension will be 3. This is because we can shatter (correctly classify) all dichotomies for $m=3$. We can either have all points be the same colour, or one point be a different colour - which is $2^3=8$ dichotomies. You may ask what if the points we are trying to classify are collinear. This does not matter because we are concerned with resolving the dichotomies themselves, not the location of the points. We just need a set of points (wherever they may be located) that exhibits that dichotomy. In other words, we can pick the points such that they maximize the number of dichotomies we can shatter with one classifying hyperplane (a triangle): the VC dimension is a statement of the capacity of our model.
To make this clear, consider $m=4$. We can represent the truth table of the XOR gate as a dichotomy but this is not resolvable by the perceptron, no matter where we choose the location of the points (not linearly separable). Therefore, we can resolve a maximum of 8 dichotomies, so our VC dimension is 3. In general, the VC dimension of perceptrons is $d+1$ where $d$ is the dimension of $X$ and $d-1$ is the dimension of the classifying hyperplane. 
"
How to make binary neural networks resilient to flipped activation values?,"
Assume I am given a binary neural network where the activation values are constrained to be 0 or 1 (by clipping the ReLU function). Additionally, assume the neural network is supposed to work in a noisy environment where some of the activation values may be randomly flipped, i.e. 0 -> 1 or 1 -> 0.
I am trying to train such neural network in a way that it would be resilient to the noisy environment. I assume training with dropout would make the neural network somewhat resilient to noises where one is flipped to zero (1 -> 0).
What are some ways that allow me to make the neural network resilient to the other kind of noise which flips zeros to ones (0 -> 1)? Is it theoretically valid to introduce a dropout-like algorithm which flips some zeros to ones during training but does not backpropagate the gradients through those flipped nodes?
","['training', 'dropout']",
Top Frequent occurrence word effect in Model Efficiency?,"
Assume that I have a Dataframe with the text column.
Problem: Classification / Prediction
    sms_text
0   Go until jurong point, crazy.. Available only ...
1   Ok lar... Joking wif u oni...
2   Free entry in 2 a wkly comp to win FA Cup fina...
3   U dun say so early hor... U c already then say...
4   Nah I don't think he goes to usf, he lives aro...

After preprocessing the text

From the above WordCloud, we can find the most frequent(occurred) words like
Free
Call
Text
Txt

As these are the most frequent words and adds less importance in prediction/classification as they appear a lot. (My Opinion)

My Question is
Removing top frequent(most occurred) words will improve the model score?
How does this impact on model performance?
Is it ok to remove the most occurred words?

","['natural-language-processing', 'text-classification']","
Based on my experience, I did 2 tasks that is proven to improve the accuracy/score of my model.

Normalization


removing characters and symbols in a text 
lowercase folding

Stopwords removal (as what you asked)

These process helped me improve my model since stopwords gave my model noise as I am using word frequency count to represent text. 
So based on what you asked, does stopwords removal improve score? It depends on your model. If you are using word count to represent text you may do stopwords removal to remove noise when doing text classification.
"
Relationship between the reward rate and the sampled reward in a Semi-Markov Decision Process,"
In the paper: Reinforcement learning methods for continuous-time Markov decision problems, the authors provide the following update rule for the Q-learning algorithm, when applied to Semi-Markov Decision Processes (SMDPs):
$Q^{(k+1)}(x,a) = Q^{(k)}(x,a) + \alpha_k [ \frac{1-e^{-\beta \tau}}{\beta}r(x,y,a) + e^{-\beta \tau} max_{a'} Q^{(k)}(y,a) - Q^{(k)}(x,a) ] $
where $\alpha_k$ is the learning rate, $\beta$ is the continuous time discount factor and $\tau$ is the time taken to transition from state $x$ to state $y$.
It is not clear to me what is the relationship between the sampled reward $r(x,y,a)$ and the reward rate $\rho(x,a)$ specified in the objective function $\mathbb{E}[ \int_{0}^{\infty} e^{-\beta t}\rho(x(t),a(t)) dt ]$.
In particular, how do they determine $r(x,y,a)$ in the experiments in Section 6? In this experiment, they consider a routing problem in an M/M/2 queuing system, where the reward rate is:
$c_1 n_1(t) + c_2 n_2(t)$. $c_1$ and $c_2$ are scalar cost factors and $n_1(t)$ and $n_2(t)$ are the number of customers in queue 1 and 2, respectively.
","['reinforcement-learning', 'q-learning', 'markov-decision-process', 'semi-mdp']",
How should I design a reward function for a NLP problem where two models interoperate?,"
I would like to design a reward function. I am training two models from the first model that classify set of texts (paragraphs and keywords) and I also got some hidden states. The second model is trying to generate keywords for those paragraphs.
I want to use those hidden states from the first model to give rewards for key phrases that are generated from the second model. I want to know how can I implement this reward function since I have never used it before.
","['reinforcement-learning', 'natural-language-processing', 'generative-adversarial-networks', 'rewards', 'text-generation']",
Why do I need an initial arbitrary policy to implement value iteration algorithm,"
I've been recently given an assignment based on Reinforcement Learning and I'm supposed to implement the value iteration algorithm in a grid environment.
The assignment: 


My doubt is why do I even need an initial arbitrary policy as given in the parameters (in the assignment) to implement the value iteration algorithm? And so, the change in the values of a and b shouldn't affect the algorithm. Am I correct in my thinking about it?
","['reinforcement-learning', 'policies', 'value-functions', 'value-iteration']","
It seems to me that you're thinking about the parameters a and b as being characteristic of the agent that's moving in the environment (therefore determining the final policy), but they are actually a characteristic of the environment.
Think of a frozen lake. You want to pass the lake but there is a hole five meters in front of you. Let's say you have boots with a rubber sole, so there is no risk to slip while walking (i.e. all transition probabilities =1). What is the optimal policy? Simply moving forward till the hole, goes around it and then forward again. But what if you were wearing wooden clogs instead? Now if you walk forward there is the risk to not be able to stop when you want to (i.e. when you decide to move forward of one step you might fail and keep going forward for several steps, with probability a). How many steps would you wait before moving to the left or to the right to avoid risking to fall into the hole? The optimal policy is obviously different in this second scenario. Think about it, I will not answer in detail about how the parameters affect the final policy because stack exchange is not meant for homework.
Also, an arbitrary policy is necessary simply because in value iteration you first update the values and only when they converge you infer from then the optimal policy. So you need some arbitrary prior belief about the environment to start update the values.
Edit
So here's a quick visualisation of the impact of different transition probabilities (the parameters a and b in your assignment) on the optimal policy. I used a different grid to make it easier and more explicit (i.e. you don't even need to calculate the values to see the optimal policy).
As you can see in the picture below, there are some terminal states in the third column and a goal state which gives a huge reward on top of it. From the starting point on the bottom we want to reach the goal state.

We have two cases:

Non stochastic environment

On the left we have the case in which all transition probabilities are 1. In this case the optimal policy is simply going forward till the second row, where the goal state is located, and then move right or left to reach it. The terminal states are not scary at all, in this case we could consider them as walls that we can't cross, but the optimal policy do not reflect any risk of falling in them. This is precisely because the transition probabilities are all 1.

Stochastic environment

On the right we have a different situation. In this case only 3 action have a transition probability of 1: going south, east or west. The transition probability for nord is 0.2 meaning that 80% of the time we will slip and go east instead (I wrote right in the picture meaning east, my bad). What is the impact of this simple change in the environment?
This time the second column become really dangerous, because if we try to go nord and fail we will end on a terminal state. On the other hand, if we choose another action we will end on a legitimate state, without any risk because the other transition probabilities are 1! Therefore the agent learn that on the left side of the grid the best strategy is to go down and circumnavigate the terminal states from the right, because only in the fourth column we can move north without risk, in fact in those cells even if we try go north and fail we would just hit a wall and we could try again going north the next turn till we will eventually succeed.
Hope this make sense!
"
2 Partition Problem,"
I want to solve the two partition problem (https://en.wikipedia.org/wiki/Partition_problem) using an uninformed search algorithm (BFS or uniform cost). 
The states can be represented by three sets S1,S1,S where the set S contains unassigned values and S1 and S2 the values assigned to each partition respectively. At the initial state S will contain all values and S1 and S2 will be empty. The actions consist in moving a value from S to S1 or S2. The objective is to find a complete assignation (S is empty) where abs(sum(S1)-sum(S2)) is minimum. As you can see I have all elements but the cost of the actions. 

How can I assign costs to the actions in order to apply one of those algorithms? (Costs must be positive.)

I know it is not the best way to solve this problem but there must be a way to do it because the problem is formulated this way in the book.
","['search', 'breadth-first-search']",
What are the pros and cons of studying machine learning before deep learning? [duplicate],"







This question already has answers here:
                                
                            




Is machine learning required for deep learning?

                                (3 answers)
                            

Closed 1 year ago.



I'm a biotech student and I'm currently working on single-particle tracking. For my work, I need to use aspects of deep learning (CNN, RNN and object segmentation) but I'm not familiar with these topics. I have some prior knowledge in python.
So, do I have to learn machine learning first before going into deep learning, or can I skip ML?
What are the pros and cons of studying machine learning before deep learning?
","['machine-learning', 'deep-learning', 'comparison', 'academia', 'education']","
Like Oliver Mason mentioned, Deep learning is just a sub-field of machine learning. In order to learn deep learning effectively you need to have certain pre-requisites like basic principle of Machine learning and basics of simple Artificial neural network  with some programming knowledge ( Python is go-to language). That being said, you don't need to know every single Machine learning algorithm and it's practices. 
Now if deep learning happens to be just a tool that you need for this particular project and have no time to learn in depth about it then I would recommend you to take a look at python libraries like Tensorflow, pytorch, scikit learn, scipy, open cv etc. You can get started and use DL, ML models with these and many other libraries without knowing it's under the hood algorithms and implementations.
One of the best course to get started with deep learning with very little Ml knowledge is Andrew ng's deep learning.ai course on coursera ( you can audit the course and get all the course materials for free)
Here's the link to the course : Deep learning.ai
"
What is the proof that policy evaluation converges to the optimal solution?,"
Although I know how the algorithm of iterative policy evaluation using dynamic programming works, I am having a hard time realizing how it actually converges.
It appeals to intuition that, with each iteration, we get a better and better approximation for the value function and we can thus assure its convergence, but with this said simply, it seems that this method is very inefficient contrary to the reality that it actually is quite efficient.
What is the rigorous mathematical proof of the convergence of the policy evaluation algorithm to the actual answer? How is it that the value function obtained this way is close to the actual values computed by solving the set of bellman equations?
","['reinforcement-learning', 'reference-request', 'proofs', 'bellman-equations', 'policy-evaluation']","
There exist other RL books which do a better job of talking about this but it's pretty simple at it's core.
The discounting factor puts an upper limit on the difference in reward between a finite number of iterations and an infinite number, each time you add another iteration it decreases by $\gamma$ multiplied into the upper bound of the difference.
$V_\pi = E[\sum_{i=0}^{\infty} \gamma^iR_i]$, $\Delta V_k = E[\sum_{i=k}^{\infty} \gamma^iR_i] = \gamma^k E[\sum_{i=0}^{\infty} \gamma^{i}R_{i+k}] < \gamma^k \frac{1}{1-\gamma}R_{max}$
"
Why do we update all layers simultaneously while training a neural network?,"

Very deep models involve the composition of several functions or layers. The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all of the layers simultaneously.

The above is an extract from Ian Goodfellow's Deep Learning  - which talks about the need for batch normalization. 
Why do we update all the layers simultaneously? Instead, if we update layers one at a time during backpropagation - it will eliminate the need for batch normalization, right?
Reference: A Gentle Introduction to Batch Normalization for Deep Neural Networks
P.S. The attached link says: Because all layers are changed during an update, the update procedure is forever chasing a moving target. Apart from the main question, it would be great if someone could explain why exactly a moving target is being referred to in the above sentence.
","['neural-networks', 'deep-learning', 'backpropagation', 'gradient-descent', 'batch-normalization']",
An infinite VC dimensional space vs using hierarchical subspaces of finite but growing VC dimensions,"
I have the following scenario. I have a binary classification problem, whose underlying function is a step function. The probability distribution of feature vectors is a uniform over the domain.
Case 1: I have a classifier which fits the training samples perfectly, no matter what the size of the data. The space of functions $H$ has an infinite VC dimension. As the data points going to infinite, the hypothesized function converges pointwise to the underlying step function.
Case 2: Here I have divided the same hypothesis space into a number of hierarchical subspaces $H_1 \subset H_2 \subset H_3 \subset \dots \subset H_n$ ($n$ goes to infinity). The VC dimension of each of the spaces is finite and grows with $n$ to infinity. Now, given any data of $n$ points, I compute the minimum number of VC dimension required to fit the data exactly, say, $d_n$ and use that space $H_{d_n}$ as the hypothesis. Do the same as data size $n$ goes to infinity, at each $n$ using the hypothesis space that just enough VC dimension to fit the data. In this approach also, as the data size goes to infinity, the hypothesized function converges pointwise to the underlying step function.
Is the difference between these two approaches to the same problem? Is there any theoretical difference? Which method is any better than others, in some sense?
","['machine-learning', 'classification', 'supervised-learning', 'computational-learning-theory', 'vc-dimension']","
In the first case, the VC dimension of $H$ being infinite implies that $H$ is not (agnostic) PAC learnable (see p. 48 of Understanding Machine Learning: From Theory to Algorithms). So, in general, your classifier is not guaranteed to succeed.
In the second case, your division of $H$ implies that $H$ is nonuniformly learnable (chapter 7 of the cited book). This implies that you can get a generalization bound by using structural risk minimization.
"
"How can I apply the alpha-beta pruning algorithm to the ""1-2 steal marbles"" problem?","
I have the following problem called ""1-2 steal marbles"".
Initially, there are 6 marbles on the board. One of the players can choose to remove 1 or 2 marbles leaving 5 or 4. After that, the other player can do the same, choosing to take again 1 or 2 marbles from the board. The process continues until there is only one marble on the board. The player who wins is the one the leaves the last marble on the board. (For example: If there are 3 marbles and it's my turn, then I will choose to remove 2 to leave one in the board to win)
How can I draw the search tree that represents the application of the alpha-beta pruning to this ""1-2 steal marbles"" with 13 marbles? I would like to see the maximizer and minimizer nodes and the value at the nodes too.
","['search', 'minimax', 'alpha-beta-pruning']",
"Is my understanding of the value function, Q function, policy, reward and return correct?","
I'm a beginner in the RL field, and I would like to check that my understanding of certain RL concepts.
Value function: How good it is to be in a state S following policy π.

So, the value functions here are 0.3 and 0.9

Q function(also called state-action value, or just action value): How good it is to be in a state S and perform action A while following policy π. It uses reward to measure the state-action value

So, the state-action values here are 0.03,0.02,0.5 and 0.9

Q value: The overall expected rewards after performing action A in state S, and continuing with policy π until the end of the episode. So, essentially I can only calculate the Q value if I know all the state-action values of the actions I will be taking in the single episode.(Because the Q value takes into account the actions after the current action A, till the end of the episode, following policy π)
Reward: The metric used to tell the agent how good/bad it's action was. It is a constant value. 
For e.g 
 1. Fall in pond --> -1
 2. On stone path --> +1
 3. Reach home--> +10

Return: The sum of rewards in a single episode
Policy π: A set of specific instructions an agent will follow in an episode. For example, the policy will look like:
In state 1, take action 3 ( which takes me to state 2)

In state 2, take action 2 ( which takes me to state 3)

In state 3, take action 1 ( Which takes me to state 4)

In state 4, take action 2 ( Which takes me to terminal state)

1 episode completed

And my policy will keep updating each episode to get the best return
","['reinforcement-learning', 'q-learning', 'terminology', 'policies', 'return']","
I think most of it is correct. 

Q function(also called state-action value, or just action value): How good it is to be in a state S and perform action A while following policy π. It uses reward to measure the state-action value

This is a bit off. Q function basically tells you how good it is to be in state S and perform action A, and follow policy $\pi$ from the next state onwards. The action A that you take can be any action from the action space and need not be according to the policy $\pi$.
Also, I think Q-function and Q-value are mostly used interchangeably to mean the same thing. 
"
Why does the policy $\pi$ affect the Q value?,"
From my understanding, the policy $\pi$ is basically how the agent acts (i.e. the actions it will take in each state).
However, I am confused about the Q value and how it is ""affected"" by a policy. This answer says

$Q^\pi(s, a)$ is the action-value function. It is the expected return starting from state $s$, following policy $\pi$, taking action $a$. It's focusing on the particular action at the particular state.

From this, I infer that the $Q$ value (the action-value function) will be affected by the policy $\pi$. Why? So, why does the Q value change according to policy $\pi$? 
Shouldn't the Q value be constant, because the same action taken in the same state will always give the same yield (and hence remain constantly good/bad)? 
All the policy does is find out the max Q values and bases its policy on that information.
","['reinforcement-learning', 'q-learning', 'policies', 'value-functions']","
Ok, Q is the reward associated with being in a given state, following a certain action and then following the given policy.
You need to take the expectation of the sum the immediate reward plus the value function which is defined by the policy.
"
Neural network doesn't seem to converge with ReLU but it does with Sigmoid?,"
I'm not really sure if this is the sort of question to ask on here, since it is less of a general question about AI and more about the coding of it, however I thought it wouldn't fit on stack overflow.
I have been programming a multilayer perceptron in c++, and it seems to be working with a sigmoid function, however when I change the activation function to ReLU it does not converge and stays at an average cost of 1 per training example. this is because all of the network's output neurons output a 0.
With the sigmoid function it converges rather nicely, I did a bit of testing and after about 1000 generations it got to an average cost of 0.1 on the first 1000 items in the MNIST dataset.
I will show you the code I changes first for the activation functions, and then i will put the whole block of code in.
Any help would be greatly appreciated!
Sigmoid:
inline float activation(float num)
{
    return 1 / (1 + std::exp(-num));
}

inline float activation_derivative(float num)
{
    return activation(num) * (1 - activation(num));
}

ReLU:
inline float activation(float num)
{
    return std::max(num, 0.0f);
}

inline float activation_derivative(float num)
{
    return num > 0 ? 1.0f : 0.0f;
}

And here's the whole block of code (I collapsed the region of code for benchmarking and the region for creating the dataset):
#include <iostream>
#include <fstream>
#include <vector>
#include <random>
#include <chrono>
#include <cmath>
#include <string>
#include <algorithm>

#pragma region benchmarking
#pragma endregion

class Network
{
public:
    float cost = 0.0f;
    std::vector<std::vector<std::vector<float>>> weights;
    std::vector<std::vector<std::vector<float>>> deriv_weights;
    std::vector<std::vector<float>> biases;
    std::vector<std::vector<float>> deriv_biases;
    std::vector<std::vector<float>> activations;
    std::vector<std::vector<float>> deriv_activations;
    void clear_deriv_activations()
    {
        for (unsigned int i = 0; i < deriv_activations.size(); ++i)
        {
            std::fill(deriv_activations[i].begin(), deriv_activations[i].end(), 0.0f);
        }
    }
    int get_memory_usage()
    {
        int memory = 4;
        memory += get_vector_memory_usage(weights);
        memory += get_vector_memory_usage(deriv_weights);
        memory += get_vector_memory_usage(biases);
        memory += get_vector_memory_usage(deriv_biases);
        memory += get_vector_memory_usage(activations);
        memory += get_vector_memory_usage(deriv_activations);
        return memory;
    }
};

struct DataSet
{
    std::vector<std::vector<float>> training_inputs;
    std::vector<std::vector<float>> training_answers;
    std::vector<std::vector<float>> testing_inputs;
    std::vector<std::vector<float>> testing_answers;
};


Network create_network(std::vector<int> layers)
{
    Network network;
    int layer_count = layers.size() - 1;
    network.weights.reserve(layer_count);
    network.deriv_weights.reserve(layer_count);
    network.biases.reserve(layer_count);
    network.deriv_biases.reserve(layer_count);
    network.activations.reserve(layer_count);
    network.deriv_activations.reserve(layer_count);
    int nodes_in_prev_layer = layers[0];
    for (unsigned int i = 0; i < layers.size() - 1; ++i)
    {
        int nodes_in_layer = layers[i + 1];
        network.weights.emplace_back();
        network.weights[i].reserve(nodes_in_layer);
        network.deriv_weights.emplace_back();
        network.deriv_weights[i].reserve(nodes_in_layer);
        network.biases.emplace_back();
        network.biases[i].reserve(nodes_in_layer);
        network.deriv_biases.emplace_back(nodes_in_layer, 0.0f);
        network.activations.emplace_back(nodes_in_layer, 0.0f);
        network.deriv_activations.emplace_back(nodes_in_layer, 0.0f);
        for (int j = 0; j < nodes_in_layer; ++j)
        {
            network.weights[i].emplace_back();
            network.weights[i][j].reserve(nodes_in_prev_layer);
            network.deriv_weights[i].emplace_back(nodes_in_prev_layer, 0.0f);
            for (int k = 0; k < nodes_in_prev_layer; ++k)
            {
                float input_weight = (2 * (float(std::rand()) / RAND_MAX)) - 1; 
                network.weights[i][j].push_back(input_weight);
            }
            float input_bias = (2 * (float(std::rand()) / RAND_MAX)) - 1;
            network.biases[i].push_back(input_bias);
        }
        nodes_in_prev_layer = nodes_in_layer;
    }
    return network;
}

void judge_network(Network &network, const std::vector<float>& correct_answers)
{
    int final_layer_index = network.activations.size() - 1;
    for (unsigned int i = 0; i < network.activations[final_layer_index].size(); ++i)
    {
        float val_sq = (network.activations[final_layer_index][i] - correct_answers[i]);
        network.cost += val_sq * val_sq;
    }
}

inline float activation(float num)
{
    return std::max(num, 0.0f);
}

void forward_propogate(Network& network, const std::vector<float>& input)
{
    const std::vector<float>* last_layer_activations = &input;
    int last_layer_node_count = input.size();
    for (unsigned int i = 0; i < network.weights.size(); ++i)
    {
        for (unsigned int j = 0; j < network.weights[i].size(); ++j)
        {
            float total = network.biases[i][j];
            for (int k = 0; k < last_layer_node_count; ++k)
            {
                total +=  (*last_layer_activations)[k] * network.weights[i][j][k];
            }
            network.activations[i][j] = activation(total);
        }
        last_layer_activations = &network.activations[i];
        last_layer_node_count = network.weights[i].size();
    }
}

void final_layer_deriv_activations(Network& network, const std::vector<float>& correct_answers)
{
    int final_layer_index = network.activations.size() - 1;
    int final_layer_node_count = network.activations[final_layer_index].size();
    for (int i = 0; i < final_layer_node_count; ++i)
    {
        float deriv = network.activations[final_layer_index][i] - correct_answers[i];
        network.deriv_activations[final_layer_index][i] = deriv * 2;
    }
}

inline float activation_derivative(float num)
{
    return num > 0 ? 1.0f : 0.0f;
}

void back_propogate_layer(Network& network, int layer)
{
    int nodes_in_layer = network.activations[layer].size();
    int nodes_in_prev_layer = network.activations[layer - 1].size();
    for (int i = 0; i < nodes_in_layer; ++i)
    {
        float total = network.biases[layer][i];
        for (int j = 0; j < nodes_in_prev_layer; ++j)
        {
            total += network.weights[layer][i][j] * network.activations[layer - 1][j];
        }
        float dzda = activation_derivative(total);
        float dzdc = dzda * network.deriv_activations[layer][i];
        for (int j = 0; j < nodes_in_prev_layer; ++j)
        {
            network.deriv_weights[layer][i][j] += network.activations[layer - 1][j] * dzdc;
            network.deriv_activations[layer - 1][j] += network.weights[layer][i][j] * dzdc;
        }
        network.deriv_biases[layer][i] += dzdc;
    }
}

void back_propogate_first_layer(Network& network, std::vector<float> inputs)
{
    int nodes_in_layer = network.activations[0].size();
    int input_count = inputs.size();
    for (int i = 0; i < nodes_in_layer; ++i)
    {
        float total = network.biases[0][i];
        for (int j = 0; j < input_count; ++j)
        {
            total += network.weights[0][i][j] * inputs[j];
        }
        float dzda = activation_derivative(total);
        float dzdc = dzda * network.deriv_activations[0][i];
        for (int j = 0; j < input_count; ++j)
        {
            network.deriv_weights[0][i][j] += inputs[j] * dzdc;
        }
        network.deriv_biases[0][i] += dzdc;
    }
}

void back_propogate(Network& network, const std::vector<float>& inputs, const std::vector<float>& correct_answers)
{
    network.clear_deriv_activations();
    final_layer_deriv_activations(network, correct_answers);
    for (int i = network.activations.size() - 1; i > 0; --i)
    {
        back_propogate_layer(network, i);
    }
    back_propogate_first_layer(network, inputs);
}

void apply_derivatives(Network& network, int training_example_count)
{
    for (unsigned int i = 0; i < network.weights.size(); ++i)
    {
        for (unsigned int j = 0; j < network.weights[i].size(); ++j)
        {
            for (unsigned int k = 0; k < network.weights[i][j].size(); ++k)
            {
                network.weights[i][j][k] -= network.deriv_weights[i][j][k] / training_example_count;
                network.deriv_weights[i][j][k] = 0;
            }
            network.biases[i][j] -= network.deriv_biases[i][j] / training_example_count;
            network.deriv_biases[i][j] = 0;
            network.deriv_activations[i][j] = 0;
        }
    }
}

void training_iteration(Network& network, const DataSet& data)
{
    int training_example_count = data.training_inputs.size();
    for (int i = 0; i < training_example_count; ++i)
    {
        forward_propogate(network, data.training_inputs[i]);
        judge_network(network, data.training_answers[i]);
        back_propogate(network, data.training_inputs[i], data.training_answers[i]);
    }
    apply_derivatives(network, training_example_count);
}

void train_network(Network& network, const DataSet& dataset, int training_iterations)
{
    for (int i = 0; i < training_iterations; ++i)
    {
        training_iteration(network, dataset);
        std::cout << ""Generation "" << i << "": "" << network.cost << std::endl;
        network.cost = 0.0f;
    }
}

#pragma region dataset creation

#pragma endregion

int main() 
{
    Timer timer;
    DataSet dataset = create_dataset_from_file(""data.txt"");
    Network network = create_network({784, 128, 10});
    train_network(network, dataset, 1000);
    std::cout << timer.get_duration() << std::endl;
    std::cin.get();
}
```

","['convergence', 'relu', 'c++', 'sigmoid']","
It seems like you're suffering from the the dying ReLU problem. ReLU enforces positive values so the weights and biases your network learned are leading to a negative value passed through the ReLU function - meaning you would get 0. There are a few things you can do. I do not know the exact format of your data, but if it is MNIST it is possible you simply don't have normalized values. You could be learning a large negative bias as a result. Try dividing every pixel intensity in your dataset by the float 255.0 to normalize your values and see if that fixes your problem.
You could also change your activation function to something such as Leaky ReLU which attempts to solve this problem with a small positive gradient for negative values. 
"
"Is using a filter of size (1, x, y) on a 3D convolutional layer the same as using a filter of size (x,y) on a 2D convolutional layer?","
I'm trying to predict some properties of videos with Keras using the following rough architecture:

Feed each frame through the same 2-D convolutional layer. 
Take the outputs of this 2-D convolutional layer and feed them through a 3-D convolutional layer. 

There are more hidden layers, but these are the main ones that matter and are messing with my dimensionality. The input of Conv2D should be (batch_size, height, width, channels). Each movie has dimensionality (number_of_frames, height, width, channels). I first had the idea to neglect batching of movies entirely, and treat the batch size and the number of frames equivalently. Then, Conv2D would output a 4-D tensor, and I would increase its dimensionality to make the ouput a 5-D tensor that I could input into Conv3D. To do this, Conv3D could only accept inputs of batch size 1.
I decided against this, because I wanted to batch movies. My current thought is to do this:
conv1 = Conv3D(filters=1, kernel_size =(1,12,12), strides=(1,1,1), data_format='channels_last')
conv2 = Conv3D(filters=1,kernel_size=(10,10,10), strides=(1,1,1), data_format='channels_last')

conv1 would represent the 2-D convolutional layer while conv2 would represent the 3-D convolutional layer. Would this idea work? I figure there is the advantage that I can batch now and when I train the 2-D filter, the same 2-D filter is running over every single movie frame. I'm just worried that  the filter in conv1 will fail to go over certain frames, or it will somehow overlap frames when I want the filter to go over every frame individually.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'keras']",
What is the best measurement for how good an action of a reinforcement learning agent really is?,"
Even when we get a valuable reward signal after every single action, this immediate reward only approximates the short term goodness of the action. 
To consider the long term effect of an action, we can use the return of an episode, the action value function $Q(s,a)$ or the advantage $A(s,a) = Q(s,a) - V(s)$. However, these measures do not rate the action in isolation but take all the following actions until the end of an episode into account. 
Are there ways to more precisely approximate how good a single action really is considering its short and long term effects? 
","['reinforcement-learning', 'deep-rl', 'rewards']",
How is the convolution layer is usually implemented in practice?,"
Following an earlier question, I'm interested in understanding the basics of Conv2d and especially how the kernel is applied, summed, and the propagated.
I understand that:

a kernel has size W x H and that more than one kernel is applied (e.g., S x W x H) where S is the amount of kernels.
A stride or step is used when iterating the network input
Padding may or may not be used by being added to the network input.

What I would ideally like to see is either a description, or a python sample (pytorch or tensorflow) of how that is done, what the dimensionality of the output is, and any operation I may be missing (some YouTube videos say that the kernel is summarised and then divided to hold one new unique value representing the feature activation?)
","['convolutional-neural-networks', 'implementation', 'convolution']",
Is the Q value the same as the state-action pair value?,"
Am I right to say that the Q value of a particular state and action is the same as the state-action pair value of that same state and action? 
","['reinforcement-learning', 'q-learning', 'terminology', 'value-functions']","
I don't understand your question very clearly.
Q-value of a particular state-action pair (s,a) under policy $\pi$ is the total reward you would expect to collect if you start from the state s, take the action a, and follow policy $\pi$ from then on.
In the literature, this is referred to as state-action values.
"
How does Monte Carlo Exploring Starts work?,"

I'm having trouble understanding the 5th step in the flowchart.
For the 5th step, the 'update the Q function by taking the average of returns' is confusing. 
From what I understand, the Q function is basically the state-action pair values put in a table (the Q table). To update it means to make adjustments to the state-action pair value of the individual states and their respective actions (e.g state 1 action 1, state 3 action 1, state 3 action 2, so on and so forth). 
I'm not sure what 'average of returns' means though. Is it asking me to take the average of the returns after $x$ episodes?  From my understanding, returns is the sum of rewards in a full episode (So, AVG=sum of returns for x episodes/x). 
And what do I do with that 'average'? 
I'm a little confused when they say 'update the Q function' because the Q function consists of many parameters that must be updated (the individual state-action pair value), and I'm not sure which one they are referring to.
What is the point of calculating the average of returns? Since the state-action pair value for a particular state and particular action will always be the same (e.g if I always take action 3 in state 4, I will always get value=2 forever)
","['reinforcement-learning', 'q-learning', 'monte-carlo-methods']","
each episode you will calculate the return, you will then update the action value or $Q(s,a)$  as the average each episode. Using the blackjack example from open AI gym and using a discount factor of 1, you get the following  
episode 1 
[{'state': (22, 10, False), 'reward': -1, 'action': 1}, {'state': (17, 10, False), 'reward': 0, 'action': 1}, {'state': (12, 10, False), 'reward': 0.0, 'action': 1}]
$Q((22, 10, False),0)=-1$
$Q((17, 10, False),1)=-1$
$Q((12, 10, False),1)=-1$
episode 2 
[{'state': (21, 10, False), 'reward': 1, 'action': 0}, {'state': (17, 10, False), 'reward': 0, 'action': 1}, {'state': (12, 10, False), 'reward': 0.0, 'action': 1}]
$Q((21, 10, False),0)=1$
$Q((17, 10, False),1)=0$
$Q((12, 10, False),1)=0$
For $Q((17, 10, False),1)$ and $Q((12, 10, False),1)$ is the average return 
i.e -1 for the first episode and 1 for the second.
"
"If the i.i.d. assumption holds, shouldn't the training and validation trends be exactly the same?","
If the i.i.d. (independent and identically distributed) assumption holds for a training-validation set pair, shouldn't their loss trends be exactly the same, since every batch from the validation set is equivalent to having a batch from the training set instead? 
If the assumption was to be true wouldn't that make any method that was aware of the fact that there were two separate sets (regularization methods such as early stopping) meaningless?
Do we work with the fact that there is a certain degree of wrongness to the assumption or am I interpreting it wrongly?
P.S - The question stems from an observation made on MNIST (where I suppose the i.i.d assumption holds strongly). The training and validation trends (losses and accuracy both) on MNIST were almost exactly identical for any network (convolutional and feedforward) trained using negative log-likelihood, making regularization meaningless. 
","['machine-learning', 'deep-learning', 'training', 'iid']",
"Given the coordinates of an object in an image, is it possible to predict the coordinates of the same object in a different perspective?","
I am trying to figure out how to approach this.
Given training data of images and the pixel coordinates of the centre of an object in that image, would it be possible to predict the pixel coordinates of the object in the same ""scene"" in a different perspective, but with the object removed?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'reference-request']",
How are n-dimensional vectors state vectors represented in Q-learning?,"
Using this code:
import gym
import numpy as np
import time

""""""
SARSA on policy learning python implementation.
This is a python implementation of the SARSA algorithm in the Sutton and Barto's book on
RL. It's called SARSA because - (state, action, reward, state, action). The only difference
between SARSA and Qlearning is that SARSA takes the next action based on the current policy
while qlearning takes the action with maximum utility of next state.
Using the simplest gym environment for brevity: https://gym.openai.com/envs/FrozenLake-v0/
""""""

def init_q(s, a, type=""ones""):
    """"""
    @param s the number of states
    @param a the number of actions
    @param type random, ones or zeros for the initialization
    """"""
    if type == ""ones"":
        return np.ones((s, a))
    elif type == ""random"":
        return np.random.random((s, a))
    elif type == ""zeros"":
        return np.zeros((s, a))


def epsilon_greedy(Q, epsilon, n_actions, s, train=False):
    """"""
    @param Q Q values state x action -> value
    @param epsilon for exploration
    @param s number of states
    @param train if true then no random actions selected
    """"""
    if train or np.random.rand() < epsilon:
        action = np.argmax(Q[s, :])
    else:
        action = np.random.randint(0, n_actions)
    return action

def sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = True, test=False):
    """"""
    @param alpha learning rate
    @param gamma decay factor
    @param epsilon for exploration
    @param max_steps for max step in each episode
    @param n_tests number of test episodes
    """"""
    env = gym.make('Taxi-v3')
    n_states, n_actions = env.observation_space.n, env.action_space.n
    Q = init_q(n_states, n_actions, type=""ones"")
    print('Q shape:' , Q.shape)

    timestep_reward = []
    for episode in range(episodes):
        print(f""Episode: {episode}"")
        total_reward = 0
        s = env.reset()
        print('s:' , s)
        a = epsilon_greedy(Q, epsilon, n_actions, s)
        t = 0
        done = False
        while t < max_steps:
            if render:
                env.render()
            t += 1
            s_, reward, done, info = env.step(a)
            total_reward += reward
            a_ = epsilon_greedy(Q, epsilon, n_actions, s_)
            if done:
                Q[s, a] += alpha * ( reward  - Q[s, a] )
            else:
                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_] ) - Q[s, a] )
            s, a = s_, a_
            if done:
                if render:
                    print(f""This episode took {t} timesteps and reward {total_reward}"")
                timestep_reward.append(total_reward)
                break
#             print('Updated Q values:' , Q)
    if render:
        print(f""Here are the Q values:\n{Q}\nTesting now:"")
    if test:
        test_agent(Q, env, n_tests, n_actions)
    return timestep_reward

def test_agent(Q, env, n_tests, n_actions, delay=0.1):
    for test in range(n_tests):
        print(f""Test #{test}"")
        s = env.reset()
        done = False
        epsilon = 0
        total_reward = 0
        while True:
            time.sleep(delay)
            env.render()
            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)
            print(f""Chose action {a} for state {s}"")
            s, reward, done, info = env.step(a)
            total_reward += reward
            if done:  
                print(f""Episode reward: {total_reward}"")
                time.sleep(1)
                break


if __name__ ==""__main__"":
    alpha = 0.4
    gamma = 0.999
    epsilon = 0.9
    episodes = 200
    max_steps = 20
    n_tests = 20
    timestep_reward = sarsa(alpha, gamma, epsilon, episodes, max_steps, n_tests)
    print(timestep_reward)

from : 
https://towardsdatascience.com/reinforcement-learning-temporal-difference-sarsa-q-learning-expected-sarsa-on-python-9fecfda7467e
A sample Q table generated is :
[[ 1.          1.          1.          1.          1.          1.        ]
 [ 0.5996      0.5996      0.5996      0.35936     0.5996      1.        ]
 [ 0.19936016  0.35936     0.10336026  0.35936     0.35936    -5.56063984]
 ...
 [ 0.35936     0.5996      0.35936     0.5996      1.          1.        ]
 [ 1.          0.5996      1.          1.          1.          1.        ]
 [ 0.35936     0.5996      1.          1.          1.          1.        ]]

The columns representing the actions and rows representing the corresponding states.
Can the state be represented by a vector? The Q table cells are not contained by vectors of size > 1, so how should these states be represented? For example, if I'm in the state [2], can this be represented as an n-dimensional vector?
Put another way, if Q[1,3] = 4, can the Q state 1 with action 3 be represented as a vector [1, 3, 2, 12, 3]? If so, then is the state_number -> state_attributes mapping stored in a separate lookup table?
","['reinforcement-learning', 'q-learning']",
How would you build an AI to output the primary concept of a paragraph?,"
My thinking is you input a paragraph, or sentence, and the program can boil it down to the primary concept(s).
Example:
Input:

Sure, it would be nice if morality was simply a navigation toward greater states of conscious well-being, and diminishing states of suffering, but aren't there other things to value independent of well-being? Like truth, or beauty?

Output:

Questioning moral philosophy.


Is there any group that's doing this already? If not, why not?
","['natural-language-processing', 'reference-request', 'natural-language-understanding', 'text-summarization']",
Action masking for on policy algorithm like PPO,"
I have an environment, in which my agent learns according to PPO. The environment has a maximum of 80 actions, however not all of them are always allowed. My idea was to mask them, by setting the probabilities of the non valid actions to 0, and renormalizing the remaining actions. However, this would be not the predicted policy anymore, and thus the agent wouldn't act on policy. Is there a better way to mask PPO agents actions, or does it simply not constitute a big problem?
","['reinforcement-learning', 'proximal-policy-optimization', 'on-policy-methods']",
Is RL just a less rigorous version of stochastic approximation theory?,"
After reading some literature on reinforcement learning (RL), it seems that stochastic approximation theory underlies all of it.
There's a lot of substantial and difficult theory in this area requiring measure theory leading to martingales and stochastic approximations.
The standard RL texts at best mention the relevant theorem and then move on. 
Is the field of RL is really stochastic approximation theory in disguise? Is RL just a less rigorous version of stochastic approximation theory?
","['reinforcement-learning', 'comparison']","

Is the field of RL is really stochastic approximation theory in disguise? Is RL just a less rigorous version of stochastic approximation theory?

No, but reinforcement learning (RL) is based on stochastic approximation theory (SAT), and these two fields overlap.
In RL, you typically assume that the underlying problem can be modeled as a Markov decision process (MDP), and the goal is to find a policy (or value function) that solves this MDP. To find this policy, you can use stochastic approximation algorithms, such as Q-learning, but RL isn't just SAT, where, in general, there isn't necessarily a notion of MDP. 
SAT is the study of iterative algorithms to find the extrema of functions by sampling from them and under which conditions these iterative algorithms converge. SAT isn't just applied in RL, but it is applied in many other fields, such as deep learning. The paper Scalable estimation strategies based on stochastic approximations: Classical results and new insights (2015) by P.  Toulis et al. provides an overview of SAT and the connections with other fields (including RL).
To conclude, RL is based on SAT, but RL isn't just stochastic approximation algorithms, so they are distinct fields. If you want to study e.g. the convergence properties of certain RL algorithms, you may need to study SAT. In fact, for example, the typical proof of convergence for tabular Q-learning assumes the Robbins–Monro conditions. However, you can do a lot of RL without even knowing that RL is based on SAT. Similarly, you can do a lot of SAT without ever caring about RL.
"
Why are the rewards of my RL agent for the Atari Breakout game decreasing after a certain number of episodes?,"

The agent is trying to master the Atari Breakout game.
Here is my code
Is that normal that reward_100 decreased that much after it hits 4.5? Is there a way to avoid that behavior?
Be aware that reward_100 is simply mean_reward = np.mean(self.total_rewards[-100:]). In other words, it is the mean over the last 100 rewards. On the graph, reward_100 represents de y-axis and th number of episodes the x-axis. 
","['reinforcement-learning', 'python', 'pytorch']",
How to add weights to one specific input feature to ensure fair training in the network?,"
I am trying to create a multiclass product-rating network based on product reviews and other input features. Two of the other input features are ""product category"" and ""gender"". However, I want to avoid unfair bias in the classification task between male/female. Since some product categories are more likely to be reviewed by males or females (hence, not balanced), I am seeking for an approach to solve this ""imbalance""-like issue.
The options and things that I consider at the moment are:

Downsample the training examples in each product category to balance for gender
Add weights to the training examples for gender, or
Add weights to the loss function (either log-likelihood or cross-entropy)

Even though downsampling might be the easiest option, I would like to explore the options of adding weights in the network in some way. However, most literature are only discussing adding weights to the loss function in order to solve for imbalanced data related to the target value (which is not the issue that I am addressing).
Can someone help me or point me in the right direction to solve this challenge?
","['machine-learning', 'deep-learning', 'objective-functions', 'ethics', 'algorithmic-bias']",
"How is a validation set used to tune the hyperparameters in a non-biased way, if the new models depends on the values of these?","
I've built a neural network from the scratch, choosing arbitrary numbers for the hyperparameters: learning rate, number of hidden layers and neurons for these, number of epochs and size of mini batches. Now that I've been able to build something potentially useful (~93% of accuracy with test data, unseen by the model before), I want to focus on hyperparameter tuning.
The conceptual difference between training and validation sets is clear and makes a lot of sense. It's obvious that the model is biased towards the training set, so it wouldn't make sense to use it to tune the hyperparameters, nor for evaluating its performance.
But, how can I use the validation set for this, if changing any of the parameters enforces me to rebuild a new model again? The final prediction depends on the values of X number of MxN matrices (weights) and X number of N vectors (biases), whose values depend on the learning rate, batch size and number of epochs; and whose dimensions depends on the number and size of hidden layers. If I change any of these, I'd need to rebuild my model again. So I'd be using this validation set for training different models, ending up as in the first step: fitting a model from the scratch.
To sum up: I fall in a recursive problem in which I need to fine tune the hyperparameters of my model with unseen data, but changing any of these hyperparameters implies rebuilding the model.
","['neural-networks', 'machine-learning', 'hyperparameter-optimization']","
The importance of having a totally separate test set is very crucial. Once you start to use the validation set performance as a measure to use to tune hyper parameters you are biasing your network to work well on the validation set so it can no longer be relied on as a true measure of performance. Eventually if you use your test set too often then adjust hyper parameters to improve performance on the test set you wind up in the same boat. I have actually used several test sets to try to avoid this trap.
"
Understanding relation between VC Symmetrization Lemma and Generalization Bounds,"
I am new in the field of Machine Learning so I wanted to start of by reading more about mathematics and history behind it.
I am currently reading, in my opinion, a very good and descriptive paper on Statistical Learning Theory - ""Statistical Learning Theory: Models, Concepts, and Results"". In section 5.5 Generalization bounds, it states that:

It is sometimes useful to rewrite (17) ""the other way round"". That is, instead of ﬁxing $\epsilon$ and then computing the probability that the empirical risk deviates from the true risk by more than $\epsilon$, we specify the probability with which we want the bound to hold, and then get a statement which tells us how close we can expect the risk to be to the empirical risk. This can be achieved by setting the right-hand side of (17) equal to some $\delta > 0$, and then solving for $\epsilon$. As a result, we get the statement that with a probability at least $1−\delta$, any function $f \in F$ satisﬁes


Equation (17) is VC Symmetrization lemma to which we applied union bound and then Chernoff bound:

What I fail to understand is the part where we are rewriting (17) ""the other way around"". I fail to grasp intuitive understanding of relation between (17) and (18), as well as understanding generalization bounds in general. 
Could anyone help me with understanding these concepts or at least provide me with additional resources (papers, blog posts, etc.) that can help?
","['machine-learning', 'math', 'computational-learning-theory', 'vc-dimension', 'vc-theory']",
Single label classification into hierarchical categories using a neural network,"
I am working on a classification problem into progressive classes. In other words, there is some hierarchy of categories in such a way, that A < B < C, e.g. low, medium, high, very high. What loss function and activation function for the output layer should I use to take advantage of the class hierarchy, so that true A and predicted C is penalized more than true A and predicted B?
My ideas are:
1) To assign some value to each category, use one output unit with the sigmoid activation and RMS loss function. Then to assign each class to an interval, e.g. 0-033 - class A, 0.33-0.66 class B 0.66-1 - class C. It seem to do the trick, but can favor the extreme categories over the middle ones.
2) Use K softmax output units, integer labels instead of one-hot encoded and the sparse categorical crossentropy loss function. In this case I am not sure how exactly sparse categorical crossentropy works and if it really takes into account the hierarchy.
","['neural-networks', 'deep-learning', 'classification', 'objective-functions', 'activation-functions']",
Which simulation platform is used by DeepMind (and others) to handle inverse kinematics musculoskeletal?,"
Which simulation platform is used by DeepMind and others to handle inverse kinematics musculoskeletal simulation, etc., for reinforcement learning simulations and agents?
I thought they use Unity or Unreal but I assume that would be resource-heavy. 
","['reinforcement-learning', 'reference-request', 'deepmind']",
Could machine learning be used to measure the distance between two objects from a picture or live camera?,"
Could machine learning be used to measure the distance between two objects from a picture or live camera? 
An example of this is the measurement between the centre of each eye pupil.
This area is all new to me, so any advice and suggestions would be greatly appreciated. 
","['machine-learning', 'computer-vision', 'applications']","
The short answer is: yes, it could. In what you are describing, there's nothing very new or specific conceptually; it sounds like a standard regression task. Now the problem that you're actually facing is: do you have the data?
Algorithms won't be able to learn the distance between eyes if you don't have the data that it takes. It could be supervised labels (1 distance per image which would be your regression target), reconstruction from depth maps, multi-view estimation etc. There's a number of ways you could do that given the appropriate data. 
People focus on algorithms a lot, and that's good. But taking a good look at your data is often as important (if not more). 
Now a good example would be in the self-driving car literature. You could start with this blog-post and go through the papers they reference: https://towardsdatascience.com/vehicle-detection-and-distance-estimation-7acde48256e1
There also seems to be some litterature about your eyes examples (https://arxiv.org/pdf/1806.10890.pdf, https://www.sciencedirect.com/science/article/pii/S0165027019301578) so skimming through these papers & the datasets they use could guide to towards answering my question: is there data for this task?
"
Can a neural network whose output is uniformly equal to zero learn its way out of it?,"
I am performing a regression task on sparse images. The images are a result of a physical process with meaningful parameters (actually, they are a superposition of cone-like shapes), and I am trying to train a regressor for these parameters.
Here, sparse images mean that the data, and thus the expected output, is made of 2D square tensors, with only one channel, and that it is expectd that roughly 90% of the image is equal to zero. However, in my system, the data is represented as dense tensors.
I built a neural network with an encoder mapping the image on an output for which I have chosen activation and shape such that it corresponds with those meaningful parameters.
I then use custom layers to build an image from these parameters in a way that matches closely the physical process, and train the network by using the L2 distance between the input image and the output image.
However, for a large set of parameters, the output image will be equal to zero, since these are sparse images. This is the case in general for the initial network.
Is it possible that, through training, the neural network will learn its way out of this all-zero parameterization ?
My intuition is that, in the beginning, the loss will be equal to the L2 norm of the input image, and the gradient will be uniformly zero, hence, no learning.
Can anyone confirm ?
","['neural-networks', 'image-processing', 'regression']",
How are weights for weighted x-entropy loss on imbalanced data calculated?,"
I am trying to build a classifier which should be trained with the cross entropy loss. The training data is highly class-imbalanced. To tackle this, I've gone through the advice of the tensorflow docs
and now I am using a weighted cross entropy loss where the weights are calculated as 
weight_for_class_a = (1 / samples_for_class_a) * total_number_of_samples/number_of_classes

following the mentioned tutorial. 
It works perfectly, but why is there this factor total_number_of_samples/number_of_classes?
The mentioned tutorial says this

[...] helps keep the loss to a similar magnitude.

But I don not understand why. Can someone clarify?
","['machine-learning', 'classification', 'datasets', 'weights', 'cross-entropy']","
This comes from the fact that you want the same magnitude from the loss. Think of it this way: a non-weighted loss function actually has all its weights to 1 and so over the whole data set, samples are weighted with 1 and the sum of all weights is therefore $N$, if $N$ is the total number of samples.
Now in the case of a weighted loss, we want the weights to also sum to $N$ so that the loss's magnitude is comparable ($i = 1..C$ are your classes, $N_i$ is the number of samples for class $i$):
$$S = \sum_{i=0\ }^{C} \sum_{s_i=1}^{N_i} w_{i} = \sum_{i=0}^{i}\sum_{s_i=1}^{N_i}\frac{1}{N_i} \frac{N}{C} = \frac{N}{C} \sum_{i=0}^{C}\sum_{s_i=1}^{N_i}\frac{1}{N_i} = \frac{N}{C} \sum_{i=0}^{C}N_i\frac{1}{N_i}  = \frac{N}{C} \sum_{i=0}^{C}1 = \frac{N}{C} C = N$$
"
"Why does the definition of the reward function $r(s, a, s')$ involve the term $p(s' \mid s, a)$?","
Sutton and Barto define the state–action–next-state reward function, $r(s, a, s')$, as follows (equation 3.6, p. 49)
$$
r(s, a, s^{\prime}) \doteq \mathbb{E}\left[R_{t} \mid S_{t-1}=s, A_{t-1}=a, S_{t}=s^{\prime}\right]=\sum_{r \in \mathcal{R}} r \frac{p(s^{\prime}, r \mid s, a )}{\color{red}{p(s^{\prime} \mid s, a)}}
$$
Why is the term $p(s' \mid s, a)$ required in this definition? Shouldn't the correct formula be $\sum_{r \in \mathcal{R}} r p(s^{\prime}, r \mid s, a )$?
","['reinforcement-learning', 'definitions', 'markov-decision-process', 'reward-functions', 'sutton-barto']","
$\frac{p(s', r \mid s, a)}{p(s' \mid s, a)}$ represents the probability of observing reward $r$ in state $s'$, given that state $s'$ is the next state transitioned to. The equation assumes a probability distribution of rewards $r$ over state $s'$, meaning that a different reward might be observed whenever a state transitions from $s$ to $s'$. In most cases, if $r(s, a, s')$ is a deterministic reward then $p(s', r \mid s, a) = p(s' \mid s,a )$. 
"
Would it be ethical to allow an AI to make life-or-death medical decisions?,"
Would it be ethical to allow an AI to make life-or-death medical decisions?
For instance, where there an insufficient number of ventilators during a respiratory pandemic, not every patient can have one.  It seems like a straight forward question, but before you answer, consider:

Human decision-making in this regard is a form of algorithm.

(For instance, the statistics and rules that determine who gets kidney transplants.)

Even if the basis for the decision is statistical, the ultimate decision making process could be heuristic, so at least the bias could be identified.

In other words, the goal of this process, specifically, is favoring one patient over another, but doing so in the way that has the greatest utility.

Statistical bias is a core problem of Machine Learning, but human decision making is also subject to this condition.

One of the arguments in favor might be that at least the algorithm would be impartial, here in relation to human bias.
Finally, where there is scarcity, utilitarianism becomes more imperative.  (Part of the trolley problem is you only have two tracks.)  But the trolley problem is also relevant because it can be a commentary on the burden of responsibility.
","['philosophy', 'social', 'ethics', 'healthcare']","
I disagree with the idea that a trained Machine Learning model would be impartial. Models are trained on data sets that contain features. Humans prepare those data sets and decide what features are included in the data set. The model only knows what it is trained on. Human bias is still there just less blatantly obvious.
To address your question directly, the answer I believe is that it is no more or less ethical than to have humans make such decisions since in the end humans created the AI model. 
My concern, however, is simply this:
Once we offload this to AI we will longer feel responsible for the results. The mentality of ""the machine"" made the choice will make it very easy to allow us to abdicate. This is especially true if the people implementing the AI decisions are not the ones who developed the AI. 
Of course, humans having to repeatedly make life and death will suffer a serious and potentially devastating toll. So, in the end, it is a trade-off, but, from my perspective, I think the risk of abdication and the consequences thereof carry a heavier weight. But then I am not the one faced with making life and death choices.
"
What does it mean if classification error is equal between two networks but the MSE is different?,"
I'm experimenting with training a feedforward neural network using a genetic algorithm and I've done a few tests using both the mean squared error and classification error functions as fitness heuristic in the GA.
When I use MSE as error function, my GA tends to converge around an MSE of 0.1 (initial conditions have an MSE of around 0.9). Testing system accuracy with this network gives me 95%+ for both training and testing data.
But, when I use classification error as my heuristic, my GA tends to converge around when the MSE is about 0.3. System accuracy is still around the same at 95%+.

My question is, if you had two networks, one showing an MSE of 0.1 and one an MSE of 0.3, but both perform approximately the same in terms of accuracy, what can I deduce from the differences in MSE? 

In other words: which network is ""better"", even if the accuracy is the same? Does a lower MSE mean anything below a certain amount? I could train my network for 100x as many generations and get a better MSE but not necessarily a better accuracy. Why?
For some context:

When the MSE is approximately 1.5 (epoch 250), the accuracy seems to match when the MSE is approximately 2.0 (epoch 50). Why does the accuracy not increase despite MSE decreasing?
","['neural-networks', 'training', 'genetic-algorithms', 'mean-squared-error']","
Accuracy itself isn't a sufficient way to compare two models. For example, you need to consider the precision and recall stats (see confusion matrix) and calculate some other metrics like f1 score. The measurement of accuracy is only the initial state that helps us to know if a model is ""working"". But in order to understand and compare you need to know how many of impostors' set were classified as true claimants, and how many true-claimants' set classified as impostors according to the sum of total correct classified ones. In order to make a decision with the above been known you have to define how critical a miss-classification could be? e.g. assume that you need to classify if a person has a disease or not? (that's critical).
"
How can I formalise a non-zero-sum game of $N$ agent as Markov game?,"
I coded a non-zero-sum game of $N$ agents in a discrete dynamic environment to RL with Q-learning and DQN agents.
It's like a marathon. Only two actions are available per agent: $\{ G  \text{ (move forward}) , S \text{ (stay to its position}) \}$. Every agent has $m$ possible individual positions, the other agents cannot interfere with its path to the terminal position. Only when one agent reaches its terminal position gets a full reward. When everyone reaches a terminal state, all get $0$ rewards. If more than $1$ but less than $N$ reach their terminals, they get a small reward.
Now, I try to formalize it as a Markov Game (MG), but I don't have a solid mathematical background.
My first question is:

When we model a problem as an RL problem, the transition probability (TP) distribution is not required, while an MDP and MG require TP. But then how are all RL problems modeled first into MDP or MG?

As I have read in literature, I understand that I will treat the action sets of all other players as a ""team"" joint set of actions.
Second question:

How can I specialize the TP function to the specific problem I want to model? Should I just mention the general function equation?

What I have tried so far is to explicitly describe it, but I think I am not getting something:


The probability of the transition from $s$ to $s'$, where in $s'$ a number of $k$ agents move a step forward is equal to $1$, given that they all chose action $G \in Α$ and that the rest $n-k$, if any, all chose the action $S \in Α$, where $k$ is an integer $1 \leq k \leq n$.

The probability of the transition from $s$ to $s'$, where in $s'$ a number of $k$ agents to earn the high payoff is equal to $1$, given that $k=1$, its position is equal to $m-1$, it chooses action $G \in A$ and that the rest $n-k$ all chose the action $S \in Α$, where $m$ is the max possible position for each agent.

The probability of the transition from $s$ to $s'$, where in $s'$ a number of $k$ agents to earn a low payoff is equal to $1$, given that $k>1$, their position is equal $m-1$, they all chose action $G \in Α$ and  that the rest $n-k$ all chose the action $S \in Α$, where $k$ is an integer with $1 < k \leq n$ and m is the max possible position for each agent



","['reinforcement-learning', 'ai-design', 'markov-decision-process']",
"Who first coined the term ""artificial general intelligence""?","
Similarly to the question Who first coined the term Artificial Intelligence?, who first coined the term ""artificial general intelligence""?
","['terminology', 'agi', 'history']","
According to Ben Goertzel, the first person that probably used the term ""artificial general intelligence"" (in an article related to artificial intelligence) was Mark Avrum Gubrud in the 1997 article Nanotechnology and International Security. Here's an excerpt from the article.

By advanced artificial general intelligence, I mean AI systems that rival or surpass the human brain in complexity and speed, that can acquire, manipulate and reason with general knowledge, and that are usable in essentially any phase of industrial or military operations where a human intelligence would otherwise be needed. Such systems may be modeled on the human brain, but they do not necessarily have to be, and they do not have to be ""conscious"" or possess any other competence that is not strictly relevant to their application. What matters is that such systems can be used to replace human brains in tasks ranging from organizing and running a mine or a factory to piloting an airplane, analyzing intelligence data or planning a battle.

Note that the term ""AGI"" could have been used even before that Mark A. Gubrud's article (as Goertzel also suggested). 
In any case, Ben Goertzel help to popularise this term, especially with the book Artificial General Intelligence. AGI was previously known as ""strong AI"", which goes back to John Searle's Chinese Room argument, although strong AI often refers to an AGI with consciousness, and the definition of AGI doesn't necessarily imply consciousness. 
You can read more about the history of the term ""AGI"" in this Goertzel's blog post. 
"
How can I implement policy evaluation when reward is tied to an action outcome?,"
I'm following Stanford reinforcement learning videos on youtube. One of the assignments asks to write code for policy evaluation for Gym's FrozenLake-v0 environment.
In the course (and books I have seen), they define policy evaluation as
$$V^\pi_k(s)=r(s,\pi(s))+\gamma\sum_{s'}p(s'|s,\pi(s))V^\pi_{k-1}(s')$$
My confusion is that in the frozen lake example, the reward is tied to the result of the action. So, for each pair state-action, I have a list that contains a possible next-state, the probability to get to that next-state and the reward. For example, being in the target state and performing any action brings a reward of $0$, but being in any state that brings me to the target state gives me a reward of $1$.
Does this mean that, for this example, I need to rewrite $V^\pi_k(s)$ as something like this:
$$V^\pi_k(s)= \sum_{s'} p(s'|s,\pi(s)) [r(s,\pi(s), s')+ \gamma V^\pi_{k-1}(s')]$$
","['reinforcement-learning', 'math', 'implementation', 'gym', 'policy-evaluation']",
Is there an online tool that can predict accuracy given only the dataset? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Is there an online tool that can predict accuracy given only the dataset as input (i.e. without the compiled model)?
That would help to understand how data augmentation/distribution standardization, etc., is likely to change the accuracy.
","['machine-learning', 'deep-learning', 'resource-request']","
If I get correctly what you're asking, the answer is no, there is no way of knowing in advance how good a model would perform in a dataset without training a model on it. That's the whole point of data science, you try, you analyse the results, you try again using the knowledge you got from your previous attempts. It would be nice to hack the whole field and know in advance what to do to get the perfect model but it is rather unrealistic. 
Anyway, there are some standard steps that usually help understanding if you're going on the right direction. For example creating a random benchmark to see how much your model is better than a random one. To create such benchmark you definitely don't need a specific tool, all main programming languages provide built in function to generate random numbers, and that's basically all that you need. For example in python you could do something like:
import numpy as np

# create 30 true labels 
y_true = [1,2,3]*10

# generate 30 random labels
y_random = np.random.randint(0,3,size=30)

# calculate accuracy for random model
acc_random = sum(y_true==y_random)/len(y_true) 

Another thing that is worth to mention is that in the academy people are pushing more and more to use the same datasets in order to have comparable results. for example if you're trying to train an architecture that should be used for images classification, some golden dataset you must test your models on are the MNIST ones. It is well known that on these datasets a good model should achieve more than 99% accuracy, therefore this is an establish lower bound for these datasets. 
"
Creating Dataset for Image Classification,"

I want to develop a CNN model to identify 24 hand signs in American Sign Language. I created a custom dataset that contains 3000 images for each hand sign i.e. 72000 images in the entire dataset.

For training the model, I would be using 80-20 dataset split (2400 images/hand sign in the training set and 600 images/hand sign in the validation set). My question is:
Should I randomly shuffle the images when creating the dataset? And Why?
PS: Based on my previous experience, it led to validation loss being lower than training loss and validation accuracy more than training accuracy.
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'computer-vision']",
Why does TensorFlow docs discourage using softmax as activation for the last layer?,"
The beginner colab example for tensorflow states:

Note: It is possible to bake this tf.nn.softmax in as the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to
  provide an exact and numerically stable loss calculation for all models when using a softmax output. 

My question is, then, why? What do they mean by impossible to provide an exact and numerically stable loss calculation?
","['tensorflow', 'objective-functions', 'softmax']","
This is also a question I stumble upon, thanks for the explaination from ted, it is very helpfull, I will try to elaborate a little bit. Let's still use DeepMind's Simon Osindero's slide:

The grey block on the left we are looking at is only a cross entropy operation, the input $x$ (a vector) could be the softmax output from previous layer (not the input for the neutral network), and $y$ (a scalar) is the cross entropy result of $x$. To propagate the gradient back, we need to calculate the gradient of $dy/dx_i$, which is $-p_i/x_i$ for each element in $x$.
As we know the softmax function scale the logits into the range [0,1], so if in one training step, the neutral network becomes super confident and predict one of the probabilties $x_i$ to be 0 then we have a numerical problem in calculting $dy/dx_i$.
While in the other case where we take the logits and calculate the softmax and crossentropy at one shot (XentLogits function), we don't have this problem. Because the derivative of XentLogits is $dy/dx_i = y - p_i$, a more elaborated derivation can be found here.
"
Why does the growth function need to be polynomial in order for the learning algorithm to be consistent?,"
Could someone please explain to me why in VC theory, specifically, when calculating the VC dimension, the growth function needs to be polynomial in order for the learning algorithm to be consistent? Why polynomial, and where does the name growth function come from exactly?
","['machine-learning', 'vc-dimension', 'vc-theory']",
Are Q values estimated from a DQN different from a duelling DQN with the same number of layers and filters?,"
I am confused about the Q values of a duelling deep Q network (DQN). As far as I know, duelling DQNs have 2 outputs

Advantage: how good it is to be in a particular state $s$
Value: the advantage of choosing a particular action $a$

We can make these two outputs into Q values (reward for choosing particular action $a$ when in state $s$) by adding them together.
However, in a DQN, we get Q values from the single output layer of the network. 
Now, suppose that I use the same DQN model with the very same weights in my input and hidden layers and changing the output layer which gives us Q values to advantage and value outputs. Then, during training, if I add them together, will it give me the same Q value for a particular state, supposing all the parameters of both my algorithms are the same except for the output layers?
","['machine-learning', 'reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']","
Dueling-DQN has different network architecture comparing to vanilla DQN, so I don't think your version will work as well as the Dueling architecture.


From Wang et al., 2016, Dueling Network Architectures for Deep Reinforcement Learning

On the other hand, since we only have the target Q-value, separating the Q-value into state value and advantage result in the identifiability issue. That is the network might simply learn $V(s)=0$, $A(s,a)=Q(s,a)$ for every state.
To tackle with this issue, we should force an additional constraint on the advantage estimate. We can simply use the equation below as mentioned in the paper, that is, normalize the advantages across actions before combining with the state value.
$$Q(s,a;\theta,\alpha,\beta)=V(s;\theta,\beta)+(A(s,a;\theta,\alpha)-\frac{1}{|A|}\sum\limits_{a'}A(s,a';\theta,\alpha))$$
"
How can I draw a Bayesian network for this problem with birds?,"
I am working on the following problem to gain an understanding of Bayesian networks and I need help drawing it: 

Birds frequently appear in the tree outside of your window in the morning and evening; these include finches, cardinals and robins. Finches appear more frequently than robins, and robins appear more frequently than cardinals (the ratio is 7:4:1). The finches will sing a song when they appear 7 out of every 10 times in the morning, but never in the evening. The cardinals rarely sing songs and only in the evenings (in the evening, they sing 1 of every 10 times they appear). Robins sing once every five times they appear regardless of the time of day. Every tenth cardinal and robin will stay in the tree longer than five minutes. Every fourth finch will stay in the tree longer than five minutes.

I have tried drawing two versions of the network and would love some feedback. Currently, I am leaning more towards the right side network.

","['bayesian-networks', 'bayesian-optimization', 'bayesian-probability']",
How can neural networks approximate any continuous function but have $\mathcal{VC}$ dimension only proportional to their number of parameters?,"
Neural networks typically have $\mathcal{VC}$ dimension that is proportional to their number of parameters and inputs. For example, see the papers Vapnik-Chervonenkis dimension of recurrent neural networks (1998) by Pascal Koirana and Eduardo D. Sontag and VC Dimension of Neural Networks (1998) by Eduardo D. Sontag for more details.
On the other hand, the universal approximation theorem (UAT) tells us that neural networks can approximate any continuous function. See Approximation by Superpositions of a Sigmoidal Function (1989) by G. Cybenko for more details. 
Although I realize that the typical UAT only applies to continuous functions, the UAT and the results about the $\mathcal{VC}$ dimension of neural networks seem to be a little bit contradictory, but this is only if you don't know the definition of $\mathcal{VC}$ dimension and the implications of the UAT.
So, how come that neural networks approximate any continuous function, but, at the same time, they usually have a $\mathcal{VC}$ dimension that is only proportional to their number of parameters? What is the relationship between the two?
","['comparison', 'computational-learning-theory', 'vc-dimension', 'universal-approximation-theorems']","
VC Dimension of Neural Networks establishes VC bounds depending on the number of weights, whereas the UAT refers to a class of neural networks in which the number of weights a particular network can have is not bounded, although it needs to be finite.
I think that we can show, from theorem 2 and the observations below theorem 3 in Approximation by Superpositions of a Sigmoidal Function, that the VC dimension of
$$S=\left\{\sum_{i=1}^N \alpha_i\sigma(y_i^T x + \theta_i) : N\in\mathbb N, \alpha_i, \theta_i \in\mathbb R, y_i\in\mathbb{R}^n \right\}$$
is infinite.
Let $\{(x_i, y_i)\}_{i=1}^k$ be a sample of arbitrary size $k\in\mathbb N$, and let us see that there is a function in $S$ which can correctly classify it, i.e., $S$ shatters $\{x_i\}_{i=1}^k$.
We note $B(x, \varepsilon) := \{ y\in\mathbb{R}^n : d(x,y) < \varepsilon \}$ (this is just standard notation to denote a ball).
First, let $\varepsilon > 0$ be such that $B(x_i, \varepsilon)\cap B(x_j, \varepsilon) = \emptyset$ every time that $i \ne j$.
Now define $D = \cup_{y_i=1} B(x_i, \varepsilon)$. Define $f_{\varepsilon}(x)$ as in the observations below theorem 3 of Cybenko's paper, and use theorem 2 to find a function $G(x)$ in $S$ that classifies correctly all points at least $\varepsilon$ away from the boundary of $D$, i.e., all points in the sample.
"
Possible approaches to dealing with unbalanced dataset and highly biased deep learning algorithm,"
I have an extremely unbalanced  video dataset  for a two class video classification problem.All my videos in my current video dataset is $40$ second long with $900p$ resolution.However the dataset is highly unbalanced with $3000$ samples for class A vs $300$ samples to class B. Due to high imbalance, i added the following class weight implementation to my deep learning model.
https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html
However  my model was still heavily biased due to high imbalance in data. I am considering   the following options:

Adding more video data to balance the dataset:  My only concern here is my current dataset is uniform with same duration of videos and similar resolution of around $900p$. Will it matter if I add  very low resolution videos to balance the dataset?
Adding video augmentation to current dataset.

I am looking for any other recommendations that i could make use. Any pros or cons of any of these methods that I should consider to prevent any bias?
","['machine-learning', 'deep-learning', 'data-science', 'algorithmic-bias']",
Is Mean Squared Error Loss function a good loss function for continuous variables $0 < x < 1$,"
Suppose I am utilising a neural network to predict the next state, $s'$ based on the current $(s, a)$ pairs. 
all my neural network inputs are between 0 and 1 and the loss function for this network is defined as the mean squared error of the Difference between the current state and next state. Because the variables are all between 0 and 1, the MSE difference between the actual difference and predicted difference is smaller than the actual difference. 
Suppose the difference in next state and current state for $s \in R^2$ is $[0.4,0.5]$ and the neural network outputs a difference of $[0.2,0.4]$. The mean squared loss is therefore 0.05 $(0.2^2 + 0.1^2) = 0.05$ whereas the neural network does not really predict the next state very well due to a difference of $(0.2, 0.1)$. 
Although whichever loss function is used does not matter, It was deceiving to think that despite the loss function outputting low values, it is mainly due to the squared term that keeps the value small. 
Is Mean Squared Error loss function still a good loss function to be used here ? 
","['deep-learning', 'reinforcement-learning', 'objective-functions']",
Is possible to train a robot or AI to prune fruit trees?,"
I live in a rural area where there is a growing necessity for people with knowledge to prune Pear trees, this process is crucial for the industry, but as people go to the big cities, this skill is being lost, and in a few years there will be no one to do it.
I wanna know if it is possible to train a robot using AI to do this, and want would it take to make this work!
Keep in mind that this would be viable only in an ""industrial"" way. The trees all have approximately the same size and are disposed in a certain preset way (distance between each other, height, etc). 
","['machine-learning', 'robotics']","
I am not an expert in robotics (definitely not in pear trees pruning either) but I will try to give some hints to partially answer and also to help reframe the problem a bit. On overall I'll give already an answer: it is most likely possible, but also most likely not convenient.
Problem statement
First things first: in general the rule is that machine learning should be applied when a task can't be automatised otherwise. Before asking if an AI can be trained to solve a task one should always think how to automatise a task using a rule based system. This is important especially because during the process of thinking how to automatise the task you will realise that some steps can be perform without an expert, while others can't be performed without any supervision. Let's brake down your task in subtasks: a system that prune trees should be (at least) capable of:

Moving between trees
Selecting branches to cut
Cut the selected branches

Selecting the branch to cut is probably the step that requires most of the know how and expert supervision and for which a machine learning component might be suitable. Moving instead is a perfect example of a subtasks that could be tackled at different levels. Creating a machine able to anticipate other objects movements and avoid them in real time definitely require to train an AI component, but when you say that the environment is highly structured (trees disposed in a grid) this make me think that maybe some hand coded rules would do the trick without bothering machine learning.
Theoretical tools
Once you have understood which subtasks your machine should be capable of solving, you can start dig on the theoretical feasibility of them. Following the same order as in the previous paragraph:

Self-driving is a widely studied topics, the algorithms used to train robotic vacuum cleaners to move automatically in a house could be applied straight away to the problem of training an agent to move between trees.

Selecting the right branches involves mostly computer vision. This sub-task should actually be also dived again into sub tasks: detecting branches from other objects and select which ones should be cut. Nevertheless, the field is quite huge and training two models able to perform both actions is, in my opinion, doable.

Cutting a branch is probably harder than driving in this situation, because of the small movements that might be required to reach branches positioned in difficult spots. Anyway, it is possible to train robots to perform fine-grained movements (for a funny example see: Robot learn to flip pancakes).


Again it depends also on how high your expectation are about the final machine/system. Should the system have a surgical precision or could it risks to brake few extra branches in hard situations? Obviously the higher the expectations the harder it would be to make everything work harmoniously.
Resources required
Last but not least you also need to understand if what you're trying to do is feasible in reality and not just in theory. A big problem when it comes to use machine leaning is that these models can be trained only with huge amount of data.

Train an artificial agent to move in an environment can be done by reproducing the environment in an artificial simulator, which is good news. A single guy with a laptop could potentially do the job.

Collecting data to train a model able to detect branches on photo and then select which one should be cut will be highly tedious, and also expensive because data need also to be labelled by experts. Which means that some experienced people will have to take photos, in the order of tens of thousands at least, and write for each photo (using a software) which are the branches they will cut if they were working for real on that tree. I strongly doubt that a dataset like this already exists.

Training a robotic arm to cut branches will also be challenging. Despite the fact that also in this case simulators could be leveraged, the task is inherently harder and this comes with bigger difficulties (for example in designing a proper reward function if using reinforcement leaning). Concretely this mean more time to spend in research and testing.


Consider also that the success of the final model trained for each subtask would be not guaranteed at all, reason why I said at the beginning that training a system with AI modules would be probably not convenient, and that the best thing is always to try to create a rule based system first.
"
Is there any way to apply linear transformations on a vector other than matrix multiplication?,"
I am trying to optimize the cost function calculation in regression analysis using a non-matrix multiplication based approach.
More specifically, I have a point $x = (1, 1, 2, 3)$, to which I want to apply a linear transformation $n$ times. If the transformation is denoted by a $4 \times 4$ matrix $A$, then the final transformation would be given by $A^n * x$. 
Given that matrix multiplication can computational expensive, is there a way we can speed up the computation, assuming we would need to run multiple iterations of this simulation?
","['linear-regression', 'statistical-ai', 'linear-algebra']","
You can sometimes exploit the structure of your matrix to perform faster matrix multiplication. For example, if your matrix is sparse (or dense), there are algorithms that exploit this fact. 
In your case, you can actually compute $A^n$ in less time than $\mathcal{O}(n^3$). For example, have a look at this question at CS SE and this one at Stack Overflow (SO). Note that the provided solutions may not be numerically stable, so I am not suggesting you use them.
Moreover, if you perform your operations on the GPU, they could be faster in practice. See e.g. this question at SO and this one at SciComp SE.
"
Which classifier should I use for a dataset with one feature?,"
I have a labeled dataset composed of 3000 data. Its single feature is the price of the house and its label is the number of bedrooms. 
Which classifier would be a good choice to classify these data?
","['machine-learning', 'ai-design', 'classification']","
Classification can be performed on structured or unstructured data. Classification is a technique where we categorize data into a given number of classes. 
Based on my project in price classification, when i compared into the 5 models, i got a higher score on a Random Forest Classifier compared to Decision Tree, SVM, Naive Bayes, Logistic Regression.
my project: https://github.com/khaifagifari/Classification-and-Clustering-on-Used-Cars-Dataset
source : https://github.com/f2005636/Classification
https://www.kaggle.com/vbmokin/used-cars-price-prediction-by-15-models
"
How to choosing the random value for parameter w in deep learning network?,"
I did watch the course DeepLearning of Andrew Ng and he told that we should create parameter w small like:
parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) ** 0.001 

But in the last application assignment. They choose another way:
layers_dims = [12288, 20, 7, 5, 1]
def initialize_parameters_deep(layer_dims):
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims)

    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l - 1])
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))
        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))
        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))
    return parameters

And the result of this way is very good but if I choose w like the old above, It's just have 34% correct!
So do you can explain ?
",['deep-learning'],
What are standard datasets for fully connected neural networks?,"
I am looking for datasets that are used as a testing standard in the fully connected neural networks (FCNN). For example, in the image recognition and CNN, CIFAR datasets are used in most of the papers, but can't find anything like that for the FCNN. 
","['neural-networks', 'datasets', 'feedforward-neural-networks', 'multilayer-perceptrons']","
You can use MNIST obviously but I'd also suggest you have a look at UC Irvine's datasets: https://archive.ics.uci.edu/ml/datasets.php
"
Could we update the policy network with previous trajectories using supervised learning?,"
I believe to understand the reason why on-policy methods cannot reuse trajectories collected from earlier policies: the trajectory distribution change with the policy and the policy gradient is derived to be an expectation over these trajectories. 
Doesn't the following intuition from the OpenAI Vanilla Policy Gradient description indeed propose that learning from prior experience should still be possible?

The key idea underlying policy gradients is to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to lower return.

The goal is to change the probabilities of actions. Actions sampled from previous policies are still possible under the current one. 
I see that we cannot reuse the previous actions to estimate the policy gradient. But couldn't we update the policy network with previous trajectories using supervised learning? The labels for the actions would be between 0 and 1 based on how good an action was. In the simplest case, just 1 for good actions and 0 for bad ones. The loss could be a simple sum of squared differences with a regularization term.
Why is that not used/possible? What am I missing?
","['reinforcement-learning', 'policy-gradients', 'experience-replay', 'off-policy-methods', 'on-policy-methods']",
What is the best approach to build a self-learning AI chatbot?,"
I am a novice in AI and I like to build a chatbot to predict diseases using patient narration as input. Initially, I simply want to train my chatbot on 1 disease only. And once this initial milestone is accomplished then I want to train it on other diseases. 
I want to know whether I should build and train my model first and then move towards building a chatbot or should i create a chatbot first and then train it on a disease.
Also please justify which approach is better and why? 
","['prediction', 'chat-bots']","
The former: build and train a model first, and then think about the user interface.
Effectively, a chatbot is a user interface to your model. If you run it 'off-line' on input text and it works, then you have achieved your goal without the added complexity of driving a conversation (which is harder than one might think).
Also, building an 'abstract' chatbot devoid of any content is going to be harder. What should that chatbot talk about?
From my own experience (I work in conversational AI), you might not succeed in building a decent chatbot, especially as a novice. But you might be able to train a model to identify diseases from textual input. So if you do that first, you have at least got something! Especially if that is your main reason for the project in the first place.
"
"If a neural network is a universal function approximator, can it have any prior beliefs?","
Let us confine ourselves to the case where we have a $n$ dimensional input and a $+1$ or $-1$ output. It can be shown that:

For every $n$, there exists a dense NN of depth 2, such that it contains all functions from ${±1}^n$ to ${±1}$. (given sign activation functions and some other very simple assumptions).

Check section 20.3 for the proof.
So, if a neural net can approximate any function, then it has a $\mathcal V \mathcal C $ dimension of infinite (considering the $n$ dimensional set of points as our universe).
Thus, it can realize all types of functions (or its hypothesis set contains all set of functions), and hence cannot have prior knowledge (prior knowledge in the sense used in the No Free Lunch theorem).
Are my deductions correct? Or did I make wrong assumptions? Are there actually any prior beliefs in a neural network that I am missing?
A detailed explanation would be nice.
","['neural-networks', 'computational-learning-theory', 'universal-approximation-theorems', 'vc-theory', 'no-free-lunch-theorems']","
I think your deduction is mostly correct.
Neural networks of depth are universal function approximators. This means that in principal, for any function of the form you describe, there's a NN that approximates it. 
However, a particular NN architecture of fixed width and depth, with fixed connections is not a universal approximator for all functions. Only an infinitely wide NN is, and that's a theoretical construct, not something you can make in practice.
Typically, a practitioner using a NN injects their prior beliefs about the problem  by selecting an architecture. For example, it is common to use the ImageNet or ResNet architectures for image processing tasks. Those architectures are less effective on other types of tasks. For instance, it should be clear that they are ineffective on say, a time-series analysis task.
"
How can I cluster this data frame with several features and observations?,"
How can I cluster the data frame below with several features and observations? And how would I go about determining the quality of those clusters? Is k-NN appropriate for this?
id     Name             Gender   Dob    Age  Address
1   MUHAMMAD JALIL      Male    1987    33   Chittagong
1   MUHAMMAD JALIL      Male    1987    33   Chittagong
2   MUHAMMAD JALIL      Female  1996    24   Rangpur
2   MRS. JEBA           Female  1996    24   Rangpur
3   MR. A. JALIL        Male    1987    33   Sirajganj
3   MR. A. JALIL        Male    1987    33   Sirajganj
3   MD. A. JALIL        Male    1987    33   Sirajganj
4   MISS. JEBA          Female  1996    24   Rangpur
4   PROF. JEBA          Female  1996    24   Rangpur
1   MD. A. JALIL        Male    1987    33   Chittagong
1   MUHAMMAD A. JALIL   Male    1987    33   Chittagong

","['python', 'datasets', 'unsupervised-learning', 'clustering']","
A typical clustering algorithm is k-means (and not k-NN, i.e. k-nearest neighbours, which is primarily used for classification). There are other clustering algorithms, such as hierarchical clustering algorithms. sklearn provides functions that implement k-means (and an example), hierarchical clustering algorithms, and other clustering algorithms.
To assess the quality of the produced clusters, you could use the silhouette method (sklearn provides a function that can be used to compute the silhouette score).
Regarding your specific data frame, note that it contains repetitions, so you may want to remove them before starting the clustering procedure. Also, the IDs are not unique, but you probably don't need the IDs for clustering.
"
Can neural network help me with detecting center coordinates of particles in an image?,"
I have an image of some nano particles that was taken with Scanning Electron Microscope (SEM) attached here. I want to obtain center points coordinates (x,y) for each particle. Doing it by hand is very tedious. Since I just started to learn Machine Learning and got introduced to Artificial Neural Networks and kinda understand that they they are helpful with image classification, I am curious if I can use these tools to achieve my goal. 
I found this article where they discuss kind similar work,,, but I am curious if you have seen anything practical or if you can give me some steps where and how to start, that's really helpful.
Any guidance is appreciated.   

","['neural-networks', 'convolutional-neural-networks', 'image-recognition', 'image-processing', 'image-segmentation']","
This is a very hard problem, you have many overlapping points with objects which aren't completely round. I'm not very knowledgeable on CV but I suspect you will find it very challenging.
I would probably say a handcrafted detection algorithm would probably be easier, something like an edge detector which fit circles to arcs and labeled the points. But it's still going to be nontrivial and maybe impossible to get it working with high accuracy.
"
"What is it called in AI when a program is designed to make ""x in the style of y""?","
Simplified: What is it called in AI when a program is designed to make ""x in the style of y;"" when it trains off of two types of sources in order to make a thing from source one, informed by features from source two? For example, if a network made up of two smaller networks were to take sheet music of a specific compositional style in network A and audio samples from a certain genre in B and through an interface creates music from a certain genre in a certain compositional style; the is comes from One, the seems comes from Two.
For more coarse and obvious examples:

""Compose synthpop in the style of Beethoven""
""Draw impressionism in the style of Mondrian""
""Generate casserole recipes using only ingredients most likely to fluctuate in price given current market data""
""Sketch baseballs that look like they're made of espresso foam""

","['neural-networks', 'machine-learning', 'terminology']",
What are some online courses on artificial general intelligence?,"
Although no artificial general intelligence (AGI) has yet been created, probably, there are already some courses on the topic. So, what are some online (preferably free) courses on AGI?
","['agi', 'reference-request']",
What is the intuition behind the dot product attention?,"
I am watching the video Attention Is All You Need by Yannic Kilcher.
My question is: what is the intuition behind the dot product attention?
$$A(q,K, V) = \sum_i\frac{e^{q.k_i}}{\sum_j e^{q.k_j}} v_i$$
becomes:
$$A(Q,K, V) = \text{softmax}(QK^T)V$$
","['natural-language-processing', 'papers', 'transformer', 'attention', 'bert']","
Let's start with a bit of notation and a couple of important clarifications.
$\mathbf{Q}$ refers to the query vectors matrix, $q_i$ being a single query vector associated with a single input word.
$\mathbf{V}$ refers to the values vectors matrix, $v_i$ being a single value vector associated with a single input word.
$\mathbf{K}$ refers to the keys vectors matrix, $k_i$ being a single key vector associated with a single input word.
Where do these matrices come from? Something that is not stressed out enough in a lot of tutorials is that these matrices are the result of a matrix product between the input embeddings and 3 matrices of trained weights: $\mathbf{W_q}$, $\mathbf{W_v}$, $\mathbf{W_k}$.
The fact that these three matrices are learned during training explains why the query, value and key vectors end up being different despite the identical input sequence of embeddings. It also explains why it makes sense to talk about multi-head attention. Performing multiple attention steps on the same sentence produces different results, because, for each attention 'head', new $\mathbf{W_q}$, $\mathbf{W_v}$, $\mathbf{W_k}$ are randomly initialised.
Another important aspect not stressed out enough is that for the encoder and decoder first attention layers, all the three matrices comes from the previous layer (either the input or the previous attention layer) but for the encoder/decoder attention layer, the $\mathbf{Q}$ matrix comes from the previous decoder layer, whereas the $\mathbf{V}$ and $\mathbf{K}$ matrices come from the encoder. And this is a crucial step to explain how the representation of two languages in an encoder is mixed together.
Once computed the three matrices, the transformer moves on to the calculation of the dot product between query and key vectors. The dot product is used to compute a sort of similarity score between the query and key vectors. Indeed, the authors used the names query, key and value to indicate that what they propose is similar to what is done in information retrieval. For example, in question answering, usually, given a query, you want to retrieve the closest sentence in meaning among all possible answers, and this is done by computing the similarity between sentences (question vs possible answers).
Of course, here, the situation is not exactly the same, but the guy who did the video you linked did a great job in explaining what happened during the attention computation (the two equations you wrote are exactly the same in vector and matrix notation and represent these passages):

closer query and key vectors will have higher dot products.
applying the softmax will normalise the dot product scores between 0 and 1.
multiplying the softmax results to the value vectors will push down close to zero all value vectors for words that had a low dot product score between query and key vector.

In the paper, the authors explain the attention mechanisms saying that the purpose is to determine which words of a sentence the transformer should focus on. I personally prefer to think of attention as a sort of coreference resolution step. The reason why I think so is the following image (taken from this presentation by the original authors).

This image shows basically the result of the attention computation (at a specific layer that they don't mention). Bigger lines connecting words mean bigger values in the dot product between the words query and key vectors, which means basically that only those words value vectors will pass for further processing to the next attention layer. But, please, note that some words are actually related even if not similar at all, for example, 'Law' and 'The' are not similar, they are simply related to each other in these specific sentences (that's why I like to think of attention as a coreference resolution). Computing similarities between embeddings would never provide information about this relationship in a sentence, the only reason why transformer learn these relationships is the presences of the trained matrices $\mathbf{W_q}$, $\mathbf{W_v}$, $\mathbf{W_k}$ (plus the presence of positional embeddings).
"
"To perform a white-box adversarial attack, would the use of a numerical gradient suffice?","
I am trying to perform a white-box attack on a model.
Would it be possible to simply use the numerical gradient of the output wrt input directly rather than computing each subgradient of the network analytically? Would this (1) work and (2) actually be a white box attack?
As I would not be using a different model to 'mimic' the results but instead be using the same model to get the outputs, am I right in thinking that this would still be a white box attack.
","['machine-learning', 'ai-security', 'adversarial-ml']",
NaNs after a while in training of PPO,"
My problem is that every time I am trying to train my PPO agent I get NaN values after a while. The diagnostic that I get is the following:
ep=   3| t=  144/450000|  0.8 sec| rew=31.3005| ploss=-2.5e-02| vfloss=4.7e+01| kl=1.3e-03| ent=8.5e+00| clipfrac=0.0e+00
ep=   6| t=  288/450000|  1.1 sec| rew=31.2144| ploss=-2.2e-02| vfloss=4.1e+01| kl=1.3e-03| ent=8.5e+00| clipfrac=0.0e+00
ep=   9| t=  432/450000|  1.4 sec| rew=28.2668| ploss=-2.9e-02| vfloss=3.5e+01| kl=1.6e-03| ent=8.5e+00| clipfrac=0.0e+00
ep=  12| t=  576/450000|  1.7 sec| rew=28.2910| ploss=-2.7e-02| vfloss=3.6e+01| kl=1.7e-03| ent=8.5e+00| clipfrac=0.0e+00
ep=  15| t=  720/450000|  2.0 sec| rew=27.4817| ploss=-2.3e-02| vfloss=3.0e+01| kl=1.8e-03| ent=8.5e+00| clipfrac=0.0e+00
ep=  18| t=  864/450000|  2.3 sec| rew=29.8415| ploss=-4.5e-02| vfloss=3.4e+01| kl=4.0e-03| ent=8.5e+00| clipfrac=2.8e-02
ep=  21| t= 1008/450000|  2.6 sec| rew=29.1447| ploss=-2.7e-02| vfloss=2.7e+01| kl=2.0e-03| ent=8.5e+00| clipfrac=6.9e-03
ep=  24| t= 1152/450000|  3.0 sec| rew=30.2001| ploss=-3.5e-02| vfloss=2.8e+01| kl=1.7e-03| ent=8.5e+00| clipfrac=6.9e-03
ep=  27| t= 1296/450000|  3.3 sec| rew=31.4069| ploss=-2.9e-02| vfloss=3.7e+01| kl=3.0e-03| ent=8.5e+00| clipfrac=2.1e-02
ep=  30| t= 1440/450000|  3.6 sec| rew=27.7963| ploss=-4.6e-02| vfloss=2.3e+01| kl=7.3e-03| ent=8.5e+00| clipfrac=1.7e-01
ep=  33| t= 1584/450000|  3.9 sec| rew=30.8561| ploss=-5.9e-02| vfloss=2.5e+01| kl=9.6e-03| ent=8.5e+00| clipfrac=2.6e-01
ep=  36| t= 1728/450000|  4.2 sec| rew=27.3002| ploss=-6.9e-02| vfloss=2.2e+01| kl=1.3e-02| ent=8.5e+00| clipfrac=3.1e-01
ep=  39| t= 1872/450000|  4.5 sec| rew=28.0270| ploss=-5.6e-02| vfloss=2.1e+01| kl=8.9e-03| ent=8.5e+00| clipfrac=2.0e-01
ep=  42| t= 2016/450000|  4.9 sec| rew=28.0624| ploss=-5.8e-02| vfloss=2.0e+01| kl=7.5e-03| ent=8.5e+00| clipfrac=2.4e-01
ep=  45| t= 2160/450000|  5.2 sec| rew=28.6224| ploss=-8.4e-02| vfloss=2.3e+01| kl=7.2e-03| ent=8.5e+00| clipfrac=2.0e-01
ep=  48| t= 2304/450000|  5.5 sec| rew=32.3889| ploss=-4.3e-02| vfloss=2.6e+01| kl=7.1e-03| ent=8.5e+00| clipfrac=2.9e-01
ep=  51| t= 2448/450000|  5.8 sec| rew=31.4241| ploss=-1.0e-01| vfloss=2.7e+01| kl=7.0e-03| ent=8.5e+00| clipfrac=2.1e-01
ep=  54| t= 2592/450000|  6.1 sec| rew=33.4760| ploss=-5.1e-02| vfloss=2.5e+01| kl=7.3e-03| ent=8.5e+00| clipfrac=2.4e-01
ep=  57| t= 2736/450000|  6.4 sec| rew=31.0780| ploss=-8.8e-02| vfloss=2.3e+01| kl=6.9e-03| ent=8.5e+00| clipfrac=3.0e-01
ep=  60| t= 2880/450000|  6.7 sec| rew=34.1286| ploss=-6.9e-02| vfloss=2.7e+01| kl=7.6e-03| ent=8.5e+00| clipfrac=3.1e-01
ep=  63| t= 3024/450000|  7.1 sec| rew=31.0017| ploss=-6.1e-02| vfloss=2.5e+01| kl=1.4e-02| ent=8.5e+00| clipfrac=3.7e-01
ep=  66| t= 3168/450000|  7.4 sec| rew=32.3697| ploss=-1.1e-01| vfloss=2.2e+01| kl=1.2e-02| ent=8.5e+00| clipfrac=4.6e-01
ep=  69| t= 3312/450000|  7.7 sec| rew=31.4455| ploss=-7.4e-02| vfloss=2.4e+01| kl=8.4e-03| ent=8.5e+00| clipfrac=3.7e-01
ep=  72| t= 3456/450000|  8.0 sec| rew=32.1896| ploss=-9.2e-02| vfloss=2.0e+01| kl=1.3e-02| ent=8.4e+00| clipfrac=4.1e-01
ep=  75| t= 3600/450000|  8.3 sec| rew=31.3721| ploss=-9.4e-02| vfloss=2.4e+01| kl=1.4e-02| ent=8.4e+00| clipfrac=4.2e-01
ep=  78| t= 3744/450000|  8.6 sec| rew=35.5718| ploss=-1.0e-01| vfloss=3.0e+01| kl=1.0e-02| ent=8.4e+00| clipfrac=4.6e-01
ep=  81| t= 3888/450000|  9.0 sec| rew=32.2289| ploss=-1.3e-01| vfloss=2.9e+01| kl=1.6e-02| ent=8.4e+00| clipfrac=5.5e-01
ep=  84| t= 4032/450000|  9.3 sec| rew=31.7656| ploss=-1.0e-01| vfloss=2.3e+01| kl=1.3e-02| ent=8.4e+00| clipfrac=4.4e-01
ep=  87| t= 4176/450000|  9.6 sec| rew=35.4555| ploss=-8.8e-02| vfloss=3.3e+01| kl=5.8e-03| ent=8.4e+00| clipfrac=3.1e-01
ep=  90| t= 4320/450000|  9.9 sec| rew=33.2766| ploss=-1.2e-01| vfloss=2.4e+01| kl=1.5e-02| ent=8.4e+00| clipfrac=5.7e-01
ep=  93| t= 4464/450000| 10.2 sec| rew=32.5218| ploss=-1.1e-01| vfloss=2.4e+01| kl=1.8e-02| ent=8.4e+00| clipfrac=5.6e-01
ep=  96| t= 4608/450000| 10.5 sec| rew=34.7137| ploss=-9.7e-02| vfloss=2.5e+01| kl=1.5e-02| ent=8.4e+00| clipfrac=4.3e-01
ep=  99| t= 4752/450000| 10.9 sec| rew=35.3797| ploss=-8.2e-02| vfloss=2.5e+01| kl=1.6e-02| ent=8.4e+00| clipfrac=4.9e-01
ep= 102| t= 4896/450000| 11.2 sec| rew=nan| ploss=nan| vfloss=nan| kl=nan| ent=nan| clipfrac=0.0e+00
C:\Users\ppo.py:154: RuntimeWarning: invalid value encountered in greater
  adv_nrm = (adv_nrm - adv_nrm.mean()) / max(1.e-8,adv_nrm.std()) # standardized advantage function estimate
ep= 105| t= 5040/450000| 11.5 sec| rew=nan| ploss=nan| vfloss=nan| kl=nan| ent=nan| clipfrac=0.0e+00

Any ideas why this is arised? 
","['reinforcement-learning', 'deep-rl', 'proximal-policy-optimization']",
What is the gradient of a non-linear SVM with respect to the input?,"
The objective function of an SVM is the following:
$$J(\mathbf{w}, b)=C \sum_{i=1}^{m} \max \left(0,1-y^{(i)}\left(\mathbf{w}^{t} \cdot \mathbf{x}^{(i)}+b\right)\right)+\frac{1}{2} \mathbf{w}^{t} \cdot \mathbf{w}$$
where

$\mathbf{w}$ is the model's feature weights and $b$ is its bias parameter
$\mathbf{x}^{(i)}$ is the $i^\text{th}$ training instance's feature vector
$y^{(i)}$ is the target class ($-1$ or $1$) for the $i^\text{th}$ instance
$m$ is the number of training instances
$C$ is the regularisation hyper-parameter

And if I was to use a kernel, this would become:
$$J(\mathbf{w}, b)=C \sum_{i=1}^{m} \max \left(0,1-y^{(i)}\left(\mathbf{u}^{t} \cdot \mathbf{K}^{(i)}+b\right)\right)+\frac{1}{2} \mathbf{u}^{t} \cdot \mathbf{K} \cdot \mathbf{u}$$
where the kernel can be the Gaussian kernel:
$$K(\mathbf{u}, \mathbf{v})=e^{-\gamma\|\mathbf{u}-\mathbf{v}\|^{2}}$$
How would I go about finding its gradient with respect to the input?
I need to know this as to then apply this to a larger problem of a CNN with its last layer being this SVM, so I can then find the gradient of this output wrt the input of the CNN.
","['convolutional-neural-networks', 'gradient-descent', 'support-vector-machine']",
How does normalization of the inputs work in the context of PPO?,"
What does the normalization of the inputs mean in the context of PPO? At each time step of an episode, I only know the values of this time step and of the previous ones, if I take track of them. This means that for each observation and for each reward at each time step I will do:
value = (value - mean) / std

before passing them to the NN, right? Specifically, I compute mean and std by keeping track of the values for the whole episode and at each time step, I add the new values to an array. Is this a valid approach?
Also, how can I handle negative rewards, such that being positive?
","['reinforcement-learning', 'deep-rl', 'rewards', 'proximal-policy-optimization']",
"Understanding the results of ""Visualizing and Understanding Convolutional Networks""","
I am trying to understand the results of the paper Visualizing and Understanding Convolutional Networks, in particular the following image:

What are these 3x3 blocks and their 9 cells representing?
From my understanding, each 3x3 block of the i-th layer corresponds to a randomly chosen feature map in that layer (e.g. for the layer-1 they randomly chose 9 feature maps, for layer-2 16 feature maps etc). On the left part (grayish images), the j-th 3x3 block shows 9 visualizations obtained by mapping the top-9 activations (single values) of that particular feature map to the ""pixel space"" (using a deconvolutional network). On the right part, the j-th block shows the 9 patches of input images, corresponding to the top-9 activations (e.g. in the first layer and i-th feature map, the j-th image patch is the local region of input image which is seen by the j-th neuron of that feature map). Is my understanding correct?
However, it's not entirely clear to me how the top-9 activations are chosen. It seems that for each layer and each feature-map, an activation is picked for a different input image (that's why we see e.g. different persons in layer-3, row-1, col-1, and different cars in layer-3, row-2, col-2). So within each block, the top-9 activations are obtained from 9 different images (but images of the same class) of the entire dataset (but in principle it could be that more than one activations are coming from the same image).
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'papers']",
How do we reach at the formula for UCB action-selection in multi-armed bandit problem?,"
I came across the formula for Upper Confidence Bound Action Selection (while studying multi-armed bandit problem), which looks like:
$$
A_t \dot{=} \operatorname{argmax}_a \left[ Q_t(a) + c \sqrt{ \frac{\ln t}{N_t(a)} } \right]
$$
Although, I understand what the second term in the summation actually means but I am not able to figure out how and from where the exact expression came from, what is the log doing there? What effect does $c$ have? And, why a square root?
","['reinforcement-learning', 'multi-armed-bandits', 'upper-confidence-bound']",
Are there any training datasets using standard font text rather than hand written ones?,"
Are there any training datasets using standard font text rather than handwritten ones?
I tried using the MNIST handwritten one on font based chars, but it didn't work well. 
","['datasets', 'reference-request', 'resource-request']",
What are the most common deep reinforcement learning algorithms and models apart from DQN?,"
Recently, I have completed Atari Breakout (https://arxiv.org/pdf/1312.5602.pdf) with DQN. 
Similar to DQN, what are the most common deep reinforcement learning algorithms and models in 2020? It seems that DQN is outdated and policy gradients are preferred.
","['reinforcement-learning', 'reference-request', 'deep-rl']",
What do these numbers represent in this picture of a surface?,"
The following image is a screenshot from a video tutorial that illustrates the concept of gradient descent algorithm with a 3D animation.
Do the numbers on the top of the balls pointed out by the red arrows represent the gradient?

","['terminology', 'objective-functions', 'gradient-descent']","
It represents the value of the loss function J(x1, x2; θ); the valley has value 0 in the video. You can see that the lowest ball with value 3.13 is on a steep point with a high gradient, so it's not the gradient. 
"
Why can the Bellman equation be turned into an update rule?,"
In chapter 4.1 of Sutton's book, the Bellman equation is turned into an update rule by simply changing the indices of it. How is it mathematically justified? I didn't quite get the initiation of why we are allowed to do that?
$$v_{\pi}(s) = \mathbb E_{\pi}[G_t|S_t=s]$$
$$ = \mathbb E_{\pi}[R_{t+1} + \gamma G_{t+1}|S_t=s]$$
$$= \mathbb E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t=s]$$
$$ = \sum_a \pi(a|s)\sum_{s',r} p(s',r|s,a)[r+ \gamma v_{\pi}(s')]$$
from which it goes to the update equation:
$$v_{k+1}(s) =  \mathbb E_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1})|S_t=s]$$
$$=\sum_a \pi(a|s)\sum_{s',r} p(s',r|s,a)[r+ \gamma v_{k}(s')]$$
","['reinforcement-learning', 'implementation', 'convergence', 'bellman-equations', 'policy-evaluation']","

Why are we allowed to convert the Bellman equations into update rules?

There is a simple reason for this: convergence. The same chapter 4 of the same book mentions it. For example, in the case of policy evaluation, the produced sequence of estimates $\{v_k\}$ is guaranteed to converge to $v_\pi$ as $k$ (i.e. the number of iterations) goes to infinity. There are other RL algorithms that are also guaranteed to converge (e.g. tabular Q-learning). 
To conclude, in many cases, the update rules of simple reinforcement learning (or dynamic programming) algorithms are very similar to their mathematical formalization because algorithms based on those update rules are often guaranteed to converge. However, note that many more advanced reinforcement learning algorithms (especially, the ones that use function approximators, such as neural networks, to represent the value functions or policies) are not guaranteed or known to converge.
"
What are some resources for coding some artificial intelligence techniques in the context of games?,"
I know the most basic rudimentary theory on AI, and I want to delve into actual practical coding with AI and machine learning. I already know a decent bit of coding in C++ and I'm learning Python syntax now. 
I think I want to start implementing artificial intelligence techniques for simple games (like snake or maybe chess, which isn't really a simple game, but I know a lot about it), and then move on to more complex methods and algorithms. 
So, what are some resources (e.g. tutorials, guides, books, etc.) for coding some artificial intelligence techniques in the context of games?
","['machine-learning', 'game-ai', 'reference-request', 'chess']",
Could zero-padding affect learning in a negative way?,"
I implemented an LSTM with Keras to perform word ordering task (given a syntactically unordered sentence, the goal is to label each word of the sentence with the right position in this one.)
So, my dataset is composed by numerical vectors and each numerical vector represents a word.
I train my model trying to learn the local order of a syntactic subtree composed by words that have syntactic relationships (for example, a subtree could be a set of three words in which the root is the verb and children are subject and object relationship).
I padded each subtree to a length of 20, which is the maximum subtree length that I found in my dataset. With padding introduction, I inserted a lot of vectors composed of only zeros.
My initial dataset shape is (700000, 837), but knowing that Keras wants a 3D dataset, I reshaped it to (35000, 20, 837) and the same for my labels (from 700000 to (35000, 20)).
As loss function, I'm using the ListNet algorithm loss function, which takes a list of words and for each computes the probability of the element to be ranked in the first position (then ranking these scores, I obtain the predicted labels of each word).
The current implementation is the following:
model = tf.keras.Sequential()
model.add(LSTM(units=100, activation='tanh', return_sequences=True, input_shape=(timesteps, features)))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(loss=listnet_loss, optimizer=keras.optimizers.Adam(learning_rate=0.00005, beta_1=0.9, beta_2=0.999, amsgrad=True), metrics=[""accuracy""])

model.fit(training_dataset, training_dataset_labels, batch_size=1, epochs=number_of_epochs, workers=10, verbose=1, callbacks=[SaveModelCallback()])

And SaveModelCallback simply saves each model during training.
At the moment I obtain, at each epoch, very very similar results:
Epoch 21/50
39200/39200 [==============================] - 363s 9ms/step - loss: 2.5483 - accuracy: 0.8246
Epoch 22/50
39200/39200 [==============================] - 359s 9ms/step - loss: 2.5480 - accuracy: 0.8245
Epoch 23/50
39200/39200 [==============================] - 360s 9ms/step - loss: 2.5478 - accuracy: 0.8246

I have to questions:

Could zero-padding affect learning in a negative way? And if yes, how could we not consider this padding?
Is it a good model for what I have to do?

","['machine-learning', 'natural-language-processing', 'keras', 'long-short-term-memory']",
"What is ""natural image domain""?","
I see some papers use the term ""natural image domain"". I googled that but didn't find any explanation of it.
I guess I understand the normal meaning of ""natural image"", such as the image people take by phones. The images in ImageNet database are all natural images. 
Is ""natural image domain"" a subfield of computer vision?
","['deep-learning', 'computer-vision', 'terminology']",
Why isn't my DQN agent improving when trained on Atari Breakout?,"
Lately, I have implemented DQN for Atari Breakout. Here is the code: 
https://github.com/JeremieGauthier/AI_Exercices/blob/master/Atari_Breakout/DQN_Breakout.py 
I have trained the agent for over 1500 episodes, but the training leveled off at around 5 as score. Could someone look at the code and point out a few things that should be corrected?

Actually, the training is not going more than 5 in average score. Is there a way to improve my performance?
","['reinforcement-learning', 'dqn', 'atari-games']",
Can artificial intelligence classify textual records?,"
I am a records manager and I am being asked if I recommend Office 365.  I'm having a hard time making a recommendation because I am missing an essential piece of information: can Office 365 replace the manual process of placing records into categories based on organizational function?  It is important that this is done accurately, because the category determines how long the records should be kept before they are irretrievably destroyed.  James Lappin seems to say that yes, Office 365 is underwritten by Project Cortex, and it is capable of doing this.
My sense is that artificial intelligence is not yet capable of determining a conceptual category for records.  For this to be true, a machine would have to replicate a complex human process: reading a free-text, free-form document; identifying the relevant pieces of information in a document, while ignoring others to determine what the document is ""about""; then taking the answer of what the document is ""about"" and matching it to a predefined set of major organizational activity. 
Are there any AI experts who can comment on how realistic it is to expect Project Cortex to do this?
",['text-classification'],"
AI can categorize documents very accurately.  It is not a new application but in the last year the accuracy of the underlying algorithms such as text classifiers, and language models in general, has significantly improved.  There are applications of language models which now surpass human performance. Microsoft is one of the leaders in this area.  For example, see their Turing Natural Language Generation (T-NLG) language model: Turing-NLG: A 17-billion-parameter language model by Microsoft. The T-NLG was developed in a research group called Project Turing which works on AI tools for Microsoft's products including Office-365.  My guess is this same research group makes the AI for Project Cortex.
"
How to perform classification with NEAT-Python?,"
I am trying to do classification using NEAT-python for the first time, and I am having difficulty getting the accuracy rate. I tried the same problem with an ANN and was able to get a good accuracy rate (96%+), but NEAT-Python gives barely 40%. 
Here's how I set up:

Problem: Train 100 probability values to predict classification (1-10)
Input and output setup: inputs = number of input shape (100 values of prob), and output is 10 values of probability assoc with 10 classes. 
Activation: I applied ReLU for feedforward, then applied
softmax

Fitness function: I used the loglikelihood. I was unsure about how to set up the fitness function. I also used mean accuracy rate in the genome. Both gave similar results.


In terms of hyperparameters, I am trying various values and haven't had any luck with it. Today I am trying with an increase in population size and generations. I have another feature input that can be used. 
Are there any resources that discuss how to handle mixed data for NEAT?
Any help is greatly appreciated.
","['python', 'genetic-algorithms', 'neat', 'fitness-functions']",
What is the time complexity of the forward pass and back-propagation of the sequence-to-sequence model with and without attention?,"
I keep looking through the literature, but can't seem to find any information regarding the time complexity of the forward pass and back-propagation of the sequence-to-sequence RNN encoder-decoder model, with and without attention.
The paper Attention is All You Need by Vaswani et. al in 2017 states the forward pass cost is $O(n^3)$, which makes sense to me (with 1 hidden layer). In terms of $X$ the input length, and $Y$ the output length, it then looks like $O(X^3 + Y^3)$, which I understand.
However, for training, it seems to me like one back-propagation is at worst $O(X^3 + Y^3)$, and we do $Y$ of them, so $O(Y(X^3  Y^3))$. 
This is the following diagram, where the green blocks are the hidden states, the red ones are the input text and the blue ones are output text.

If I were to add global attention, as introduced by Luong et. al in 2015, the attention adds an extra $X^2$ in there due to attention multiplication, to make an overall inference of $O(X^3 + X^2 Y^3)$, and training even worse at $O(XY(X^3 + X^2 Y^3))$ since it needs to learn attention weights too. 
The following diagram shows the sequence-to-sequence model with attention, 
where $h$'s are the hidden states, $c$ is the context vector and $y$ is the output word, $Y$ of such output words and $X$ such inputs. This setup is described in the paper Effective Approaches to Attention-based Neural Machine Translation, by Luong et al. in 2015.

Is my intuition correct?
","['recurrent-neural-networks', 'transformer', 'time-complexity', 'forward-pass', 'seq2seq']",
How do I format task features with a one-hot task identification vector to ensure separate weight matrices for each task in multi-task RL?,"
I am on Lecture 2 of Stanford CS330 Multi-Task and Meta-learning, and on slide 10, the professor describes using a one-hot input vector to represent the task, and she also explained that there would be independent weight matrices for each task
How is the input to a multi-task network encoded to allow the features of all the tasks to be associated with different weights?
Would you have an input vector containing all the features for every task, and then multiply the input vectors by the task ID vector? Is there a more efficient way to approach this problem?
In other terms, here’s what I’m thinking:
network_input[i] = features[i] * task[i]

where features is a 2d matrix of feature vectors for every task, and task is a one-hot vector corresponding to the task number. Is that multiplicative conditioning?
","['neural-networks', 'machine-learning', 'ai-design', 'meta-learning', 'multi-task-learning']",
How to train a LSTM with multidimensional data,"
I am trying to train a LSTM, but I have some problems regarding the data representation and feeding it into the model.
My data is a numpy array of three dimensions: One sample consist of a 2D matrix of size (600,5). 600(timesteps) and 5(features). However, I have 160 samples or files that represent the behavior of a user in multiple days. Altogether, my data has a dimension of (160,600,5).
The label set is an array of 600 elements which describes certain patterns of each 2D matrix. The shape of the output should be (600,1).
My question is how can I train the LSTM to the corresponding label set? What would be the best approach to handle this problem? The idea is that the output should be an array of (600,1) with 3 label inside.
Multiple_outputs {0,1,2}
      Output:    0000000001111111110000022222220000000000000
                 -------------600 samples ------------------

Input: (1, 600, 5) 
Output: (600, 1) 
Training: (160,600,5)

I look forward for some ideas!
dataset(160,600,5)

X_train, X_test, y_train, y_test = train_test_split(dataset[:,:,0:4], dataset[:,:,4:5],test_size = 0.30)

model = Sequential()
model.add(InputLayer(batch_input_shape = (92,600,5 )))
model.add(Embedding(600, 128))
#model.add(Bidirectional(LSTM(256, return_sequences=True)))
model.add(TimeDistributed(Dense(2)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer=Adam(0.001),
              metrics=['accuracy'])

model.summary()


model.fit(X_train,y_train, batch_size=92, epochs=40, validation_split=0.2)

","['recurrent-neural-networks', 'long-short-term-memory', 'multi-label-classification']","
I don't see any special characteristic in the problem you're posing. Any LSTM can handle multidimensional inputs (i.e. multiple features). You just need to prepare your data such as they will have shape [batch_size, time_steps, n_features], which is the format required by all main DL libraries (pytorch, keras and tensorflow). 
I linked below 2 tutorials that shows how to implement an LSTM for part of speech tagging in Keras and Pytorch. This task is conceptually identical to what you want to achieve: use 2D inputs (i.e. embeddings) to predict the class (i.e. the pos tags) of each element of a sequence (i.e. every single word). For example:
# Input sample, 4 steps, encoded in 2D embeddings matrix
['we', 'had', 'crispy', 'fries']

# Output predictions, 4 labels
['PRP', 'VBD', 'JJ', 'NNS']

In your case the predicted label would be 0, 1 or 2 and you would have to encode your data in a matrix of shape [n_batch, 600, 5]. The only thing you definetly want to pay attention, since you're using temporal data, is to not shuffle your data at all before the training. 
Keras tutorial: Build a POS tagger with an LSTM using Keras
Pytorch tutorial: LSTM’s in Pytorch
"
What is the degree of linearity in the error propagated by Gradient Descent?,"
Neural Network is trained to learn a non-linear function, the more layers it has, the more is the quality of the prediction and the ability to match the real-world function correctly (lets leave aside overfitting for now).
Now, given that the concept of a derivative of a function (if it is not a line) is only defined at a single point , and worse than that, it is defined as a tangent line (so, its linear in essence) , and backpropagation uses this tangent line to project weight updates, does this mean that Gradient Descent is linear at its fundamental level ? 
Then if Gradient Descent is using linearity to propagate back weight updates, imagine how big the error would be for non linear function? For example , you  have calculated that the weight number 129 of your network has to be decreased by 0.003412, but at that point with this new weight, the function might have already reversed its direction, and the real update for this weight must be a negative number!!! Isn't this the reason that our deep fully connected networks have such a difficulty to learn , because with more layers stacked up, the more non-linear the model becomes, and thus, the weight updates we propagate back to lower layers could be treated as ""best guess"" values instead of something that can be fully trusted?
I am correct in assuming that Gradient Descent is not calculating correct weight updates on each backwards step and that the only reason that the network eventually converges to required model is because these imprecisions are fixed in a loop (called epoch). So, if we use an analogy, Gradient Descent would be like navigating the World on a boat with maps developed with the assumption that the Earth is flat. With such maps, you can sail in near-by areas, but if you would travel around the world without knowing that the Earth is round you will never arrive to your destination, and that's exactly what we are experiencing when we train deep fully connected networks without making them converge.
This means, if Gradient Descent is broken in such a way, a correct Gradient Descent algorithm would only have to do SINGLE backwards step and update all the weights in one pass, giving the minimum error that is theoretically possible in 1 epoch .... I am right ?
So, my question basically is: is Gradient Descent a really broken algorithm or I am missing something?
","['neural-networks', 'deep-learning', 'gradient-descent']",
How can I handle overfitting in reinforcement learning problems?,"
So this is my current result (loss and score per episode) of my RL model in a simple two players game:

I use DQN with CNN as a policy and target networks. I train my model using Adam optimizer and calculate the loss using Smooth L1 Loss.
In a normal ""Supervised Learning"" situation, I can deduce that my model is overfitting. And I can imagine some methods to tackle this problem (e.g. Dropout layer, Regularization, Smaller Learning Rate, Early Stopping).

But would that solution will also work in RL problem?
Or are there any better solutions to handle overfitting in RL?

","['machine-learning', 'reinforcement-learning', 'overfitting']","
The accepted answer does not provide a good definition of over-fitting, which actually exists and is a defined concept in reinforcement learning too. For example, the paper Quantifying Generalization in Reinforcement Learning completely focuses on this issue. Let me give you more details.
Over-fitting in supervised learning
In supervised learning (SL), over-fitting is defined as the difference (or gap) in the performance of the ML model (such as a neural network) on the training and test datasets. If the model performs significantly better on the training dataset than on the test dataset, then the ML model has over-fitted the training data. Consequently, it has not generalized (well enough) to other data other than the training data (i.e. the test data). The relationship between over-fitting and generalization should now be clearer.
Over-fitting in reinforcement learning
In reinforcement learning (RL) (you can find a brief recap of what RL is here), you want to find an optimal policy or value function (from which the policy can be derived), which can be represented by a neural network (or another model). A policy $\pi$ is optimal in environment $E$ if it leads to the highest cumulative reward in the long run in that environment $E$, which is often mathematically modelled as a (partially or fully observable) Markov decision process.
In some cases, you are also interested in knowing whether your policy $\pi$ can also be used in a different environment than the environment it has been trained in, i.e. you're interested in knowing if the knowledge acquired in that training environment $E$ can be transferred to a different (but typically related) environment (or task) $E'$. For example, you may only be able to train your policy in a simulated environment (because of resource/safety constraints), then you want to transfer this learned policy to the real world. In those cases, you can define the concept of over-fitting in a similar way to the way we define over-fitting in SL. The only difference may be that you may say that the learned policy has over-fitted the training environment (rather than saying that the ML model has over-fitted the training dataset), but, given that the environment provides the data, then you could even say in RL that your policy has over-fitted the training data.
Catastrophic forgetting
There is also the issue of catastrophic forgetting (CF) in RL, i.e., while learning, your RL agent may forget what it's previously learned, and this can even happen in the same environment. Why am I talking about CF? Because what it's happening to you is probably CF, i.e., while learning, the agent performs well for a while, then its performance drops (although I have read a paper that strangely defines CF differently in RL). You could also say that over-fitting is happening in your case, but, if you are continuously training and the performance changes, then CF is probably what you need to investigate. So, you should reserve the word over-fitting in RL when you're interested in transfer learning (i.e. the training and test environments do not coincide).
"
How can I use machine learning to predict properties (such as the area) of simple polygons?,"
Imagine a set of simple (non-self-intersecting) polygons given by the coordinate pairs of their vertices $[(x_1, y_1), (x_2, y_2), \dots,(x_n, y_n)]$. The polygons in the set have a different number of vertices.

How can I use machine learning to solve various supervised regression and classification problems for these polygons such as prediction of their areas, perimeters, coordinates of their centroids, whether a polygon is convex, whether its centroid is inside or outside, etc?
Most machine learning algorithms require inputs of the same size but my inputs have a different number of coordinates. This may probably be handled by recurrent neural networks. However, the coordinates of my input vectors can be circularly shifted without changing the meaning of the input. For example, $$[(x_1, y_1), (x_2, y_2),...,(x_n, y_n)]$$ and $$[(x_n, y_n), (x_1, y_1),...,(x_{n-1}, y_{n-1})]$$ represent the same polygon where a starting vertex is chosen differently. 
Which machine learning algorithm is both invariant to a circular shifting of its input coordinates and can work with inputs of different sizes?
Intuitively, an algorithm could learn to split each polygon into non-overlapping triangles, calculate areas or perimeters of each triangle, and then aggregate these computations somewhere in the output layer. However, the labels (areas or perimeters) are given only for the whole polygons, not for the triangles. Also, the perimeter of the polygon is not the sum of the perimeters of the triangles. Is thinking about this problem in terms of triangles misleading?
Could you please provide references on machine learning algorithms that solve such tasks? Or any advice, how to approach this task? It does not have to be neural network and does not have to learn exact analytic formulas. Approximate results would be enough.
","['machine-learning', 'reference-request', 'sequence-modeling']","
You can split each polygon into a collection of triangles and sum up the areas. Not really sure why you would bother with ML.
Anyway if you approximates these polygons as images you could maybe train a CNN. Look at the image classification networks which provide bounding boxes.
"
Can feature engineering change the selection of the model according to the minimum description length?,"
The definition of MDL according to these slides is:

The minimum description length (MDL) criteria in machine learning says that the best description of the data is given by the model which compresses it the best. Put another way, learning a model for the data or predicting it is about capturing the regularities in the data and any regularity in the data can be used to compress it. Thus, the more we can compress a data, the more we have learnt about it and the better we
  can predict it.
MDL is also connected to Occam’s Razor used in machine learning which states that ""other things being equal, a simpler explanation is better than a more complex one."" In MDL, the simplicity (or rather complexity) of a model is interpreted as the length of the code obtained when that model is used to compress
  the data.

To put it in short according to MDL principle, we prefer predictors with relatively smaller description length (i.e can be described within a certain length) for a given description language (this definition is without delving into exact technical details as it is not necessary to the question).
Since MDL is dependent on the description language we use, can we say feature engineering can cause a change in the selection of the predictor? 
For example as this picture shows:

To me, it seems that, in the first picture, we will require a longer description length predictor in Cartesian coordinates, as compared to a predictor in polar co-ordinates (just a single discerning radius needs to be specified). So, to me, it seems feature engineering changed the selection of the predictor to a relatively simple one (in the sense that it will have a shorter description length). Thus feature engineering has changed the description length required for out predictor. Did I make any wrong assumptions? If so why?
","['computational-learning-theory', 'vc-theory', 'information-theory', 'minimum-description-length', 'feature-engineering']","
I think the wrong assumption here is that you've forgotten the cost of encoding the new features!
MDL should be considered relative to the original or raw dataset. The idea is that you want to find an expression you could send to someone else that encodes the structure of the dataset in terms of the original variables. If you define new features, you need to send a description of those features along with your model. 
To make this clearer, imagine you call me up on the phone, and we're both looking at your left hand image. You say to me 'Yeah, you just draw a line through it at $p=a$'. The natural question for me to ask is 'What's p?'. If you have to tell me what $p$ is, then it's part of the description length. 
A circular model for your lefthand image does have a MDL for its decision boundary of something like $(x−a)^2+(y−b)^2=c$. However, the feature transformation you've selected has a description length of $p=(x-a)^2 + (y-b)^2$. It should be clear that the description lengths for the linear model through $p$ and the circular model through $x$ and $y$ are identical.
"
Isn't a simulation a great model for model-based reinforcement learning?,"
Most reinforcement learning agents are trained in simulated environments. The goal is to maximize performance in (often) the same environment, preferably with a minimum amount of interactions. Having a good model of the environment allows to use planning and thus drastically improves the sample efficiency!
Why is the simulation not used for planning in these cases? It is a sampling model of the environment, right? Can't we try multiple actions at each or some states, follow the current policy to look several steps ahead and finally choose the action with the best outcome? Shouldn't this allow us to find better actions more quickly compared to policy gradient updates?
In this case, our environment and the model are kind of identical and this seems to be the problem. Or is the good old curse of dimensionality to blame again? Please help me figure out, what I'm missing. 
","['reinforcement-learning', 'models', 'planning', 'model-based-methods']","
I will give one perspective on this from the domain of robotics. You are right that most RL agents are trained in simulation particularly for research papers, because it allows researchers to in theory benchmark their approaches in a common environment. Many of the environments exist strictly as a test bed for new algorithms and are not even physically realizable, e.g. HalfCheetah. You could in theory have a separate simulator say running in another process that you use as your planning model, and the ""real"" simulator is then your environment. But really that's just a mocked setup for what you really want in the end, which is having a real-world agent in a real-world environment.
What you describe could be very useful, with one important caveat: the simulator needs to in fact be a good model of the real environment. For robotics and many other interesting domains, this is a tall order. Getting a physics simulator that faithfully replicates the real-world environment can be tricky, as one may need accurate friction coefficients, mass and center of mass, restitution coefficients, material properties, contact models, and so on. Oftentimes the simulator is too crude an approximation of the real-world environment to be useful as a planner.
That doesn't mean we're completely hosed though. This paper uses highly parallelized simulators to search for simulation parameters that approximate the real-world well. What's interesting is it's not even necessarily finding the correct real-world values for e.g. friction coefficients and such, but it finds values for parameters that, taken together, produces simulations that match the real-world experience. The better the simulation gets at approximating what's going on in the real world, the more viable it is to use the simulator for task planning. I think with the advent of GPU-optimized physics simulators we will see simulators be a more useful tool even for real-world agents, as you can try many different things in parallel to get a sense of what is the likely outcome of a planned action sequence.
"
What is the most efficient data type to store probabilities?,"
In ML we often have to store a huge amount of values ranging from 0 to 1, mostly being probabilities. The most common data structure to do so seems to be a floating point? Indeed, the range of floating points is huge. This makes them imprecise in the desired interval and inefficient, right?
This question suggests using the biggest integer value to represent a 1 and the smallest for 0. Also, it points to the Q number format, where all bits can be chosen as fractional, which sounds very efficient.
Why have these data types still not found their ways into numpy, tensorflow etc.? Am I missing something?
","['machine-learning', 'probability', 'probability-distribution', 'efficiency', 'storage']",
Can a variational auto-encoder learn images composed of random noise at each pixel (each drawn from the same distribution)?,"
Can a variational auto-encoder (VAE) learn images whose pixels have been generated from a Gaussian distribution (e.g. $N(0, 1)$), i.e. each pixel is a sample from $N(0, 1)$?
My gut feeling says no, because the VAE adds additional noise $\epsilon$ to the original image in the latent space, and if all images and pixels are random from the same distribution it would be impossible do decode/reconstruct into the particular input image. However, VAEs are a bit of a mystery to me internally. Any help would be appreciated!
","['deep-learning', 'autoencoders', 'probability-distribution', 'variational-autoencoder']",
"After having selected the best model with cross-validation, for how long should I train it?","
When using k-fold cross-validation in a deep learning problem, after you have computed your hyper-parameters, how do you decide how long to train your final model? My understanding is that, after the hyperparameters are selected, you train your model one more time on the entire set of data, but it's not clear to me when you decide to stop training.  
","['machine-learning', 'deep-learning', 'training', 'hyperparameter-optimization', 'cross-validation']","
Short answer: training ""duration"" or number of epochs/updates should be cross-validated too: you want to early-stop your training to prevent overfitting.
Longer answer:

Think of accuracy on the validation set as an estimate of accuracy on future data, given the value of some hyperparameter. In this case, the hyperparameter of interest is the number of training epochs. So: for each CV fold, train the network (e.g. up to some maximum number of epochs). After each epoch, record accuracy on the validation set. Compute the average validation set accuracy (across CV folds) for each number of epochs. Choose the number of epochs that maximizes this value.

https://stats.stackexchange.com/questions/298084/k-fold-cross-validation-for-choosing-number-of-epochs
"
Is there any literature on the design of dialogue systems for interviews and questionnaire administration?,"
For my master thesis I am working on a dialogue system that should be deployed in hospitals to administer simple questionnaires to patients. I already did literature research and I'm fine with what I found since I don't have to replicate something which has been already done, but I noticed that there are really few papers regarding this specific 'robot interviewer' topic. 
Let me explain the task a bit more in details: in a real interview a human interviewer usually starts with greetings and with an explanation of the questionnaire to administer, and then he/she starts asking some more or less structured questions to the person to interview. The idea here is to replace a human interviewer with a dialogue system. 
Now, at a first glimpse it seems like a task that can be easily hand coded, and indeed lot of real application simply use systems in which specific questions are stored in memory in association with some already made answers to choose from (here's an example), and the system simply show them (or read in the case of humanoid robots) to the people being interviewed, the system then wait for the answer and then move on with the next questions.
The point is that in real interviews the conversation flow is obviously much more smooth and natural. A human being can detect doubts in the voice of the interviewed person, which can also explicitly ask for explanations, a human being can also understand when an answer comes with emotional implications ('yes I feel sad every day') and we are able to automatically react to these hidden implications with emotional fillers ('I'm sorry to hear that'). All these aspects require to train some natural language understanding module in order to be replicated in an artificial agent (and this is actually what I'm currently working on), so I though I would have found more papers on this.
Now, despite having found a tons of paper related to open domain dialogue systems, affective and attentive systems, even systems able to reply with irony, I did not found many papers about dialogue systems for smooth interviews or questionnaire administration, which in my opinion sounds like an much easier task to tackle (especially if compared to open domain conversations).
The only two papers that I found which truly focused on interviewer systems are:

SimSensei Kiosk: A Virtual Human Interviewer for Healthcare Decision Support
A job interview dialogue system with autonomous android ERICA

So my question is: did I miss something, like some specific keywords? Or is there actually a gap in the literature with regard to the design of dialogue systems for interviews and questionnaire (or surveys) administration? I'm interested in any link or hint from anyone working on similar applicants, thank you in advance!   
","['natural-language-processing', 'reference-request', 'dialogue-systems', 'human-robot-interaction']",
"What does ""off-the-shelf"" mean?","
I encountered the phrase/concept off-the-shelf CNN in this paper in which authors used off-the-shelf CNN representation, OverFeat, with simple classifiers to address different recognition tasks.
As I understand it correctly, it literally means something is ready to be used for a task without alteration.
Can somebody explain in simple words what off-the-shelf CNN technically means in the context of AI and convolutional neural networks?
","['machine-learning', 'convolutional-neural-networks', 'computer-vision', 'terminology']",
Why is creating an AI that can code a hard task?,"
For people who have experience in the field, why is creating AI that has the ability to write programs (that are syntactically correct and useful) a hard task? 
What are the barriers/problems we have to solve before we can solve this problem? If you are in the camp that this isn't that hard, why hasn't it become mainstream?
","['machine-learning', 'philosophy', 'genetic-programming', 'ai-completeness', 'inductive-programming']","
I am not an expert on this specific topic, but I can say a few words. I will use the term ""programming"" to refer to software development (of any kind).

If you are in the camp that this isn't that hard, why hasn't it become mainstream?

It's definitely hard, otherwise, we would have already some useful artificial programmers.

Why is creating AI that can code a hard task?

Programming is actually a hard task because it often requires creativity and a deep understanding of the context(s), goal(s), programming languages, etc. In other words, it's a very complex task (even for humans), apart from the exceptions where you can copy and paste.
Programming can probably be considered an AI-complete problem, i.e. a problem that probably requires an AGI to be solved. In other words, if an AI was as capable as humans in terms of programming, then that probably means it is an AGI (but this is not guaranteed), i.e. programming is a task that probably requires general intelligence. This is why I say that programming is an AI-complete problem. However, note that being able to program is just a necessary (but not sufficient) ability that an AI needs to possess in order for it to be an AGI (although not all general intelligences, e.g. animals, may be able to develop software, but the definition of general intelligence is also fuzzy).
AFAIK, no AGI has yet been created, and I think we are still very far away from that goal. Currently, most AI systems are only able to tackle a specific problem (i.e. we only have narrow AIs, such as AlphaGo). You could say that programming is a very specific problem too, but this is misleading or wrong, because, unless you just want to develop very specific programs in a very limited context (and there are already machine learning models and approaches, such as neural programmer-interpreters and genetic programming respectively, that can do this to some extent; see the answers to this question for other examples), then you will need to know a lot about other contexts too. For example, consider the task of developing a program that can detect signs of cancer in images. To develop this program, the AI would need to have the knowledge of an AI engineer, doctor, etc.
Furthermore, programming often requires common-sense knowledge. For example, while reading the software specifications, the AI needs to interpret them in the way that they were originally meant to be interpreted. This also suggests that programming requires an AGI (or human-level AI) to be solved.
(Finally, to address a comment, note that writing a 4-line program is not equivalent to writing a 10-line program. Also, the length of the program often doesn't correspond to its difficulty or complexity, so that alone is not a good measure of the ability to program.)

What are the barriers/problems we have to solve before we can solve this problem?

I think that the answer to this question is also the answer to the question ""How can we create an AGI?"". However, to be more concrete, I think that, in order to be able to create an AI that is able to program as well as humans, we will need to be able to create an AI that is able to think about low- and high-level concepts, compose them and it will probably require common-sense knowledge (so knowledge representation). A typical supervised learning solution will not be enough to solve this task. See the paper Making AI Meaningful Again, which also suggests that ML-based solutions may not be enough to solve many tasks.
"
Why does the result when restoring a saved DDPG model differ significantly from the result when saving it?,"
I save the trained model after a certain number of episodes with the special save() function of the DDPG class (the network is saved when the reward reaches zero), but when I restore the model again using saver.restore(), the network gives out a reward equal to approximately -1800. Why is this happening, maybe I'm doing something wrong?
My network:
import tensorflow as tf
import numpy as np
import gym

epsiode_steps = 500

# learning rate for actor
lr_a = 0.001

# learning rate for critic
lr_c = 0.002
gamma = 0.9
alpha = 0.01
memory = 10000
batch_size = 32
render = True


class DDPG(object):
    def __init__(self, no_of_actions, no_of_states, a_bound, ):
        self.memory = np.zeros((memory, no_of_states * 2 + no_of_actions + 1), dtype=np.float32)

        # initialize pointer to point to our experience buffer
        self.pointer = 0

        self.sess = tf.Session()

        self.noise_variance = 3.0

        self.no_of_actions, self.no_of_states, self.a_bound = no_of_actions, no_of_states, a_bound,

        self.state = tf.placeholder(tf.float32, [None, no_of_states], 's')
        self.next_state = tf.placeholder(tf.float32, [None, no_of_states], 's_')
        self.reward = tf.placeholder(tf.float32, [None, 1], 'r')

        with tf.variable_scope('Actor'):
            self.a = self.build_actor_network(self.state, scope='eval', trainable=True)
            a_ = self.build_actor_network(self.next_state, scope='target', trainable=False)

        with tf.variable_scope('Critic'):
            q = self.build_crtic_network(self.state, self.a, scope='eval', trainable=True)
            q_ = self.build_crtic_network(self.next_state, a_, scope='target', trainable=False)

        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')
        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')

        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')
        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')

        # update target value
        self.soft_replace = [
            [tf.assign(at, (1 - alpha) * at + alpha * ae), tf.assign(ct, (1 - alpha) * ct + alpha * ce)]
            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]

        q_target = self.reward + gamma * q_

        td_error = tf.losses.mean_squared_error(labels=(self.reward + gamma * q_), predictions=q)

        self.ctrain = tf.train.AdamOptimizer(lr_c).minimize(td_error, name=""adam-ink"", var_list=self.ce_params)

        a_loss = - tf.reduce_mean(q)

        # train the actor network with adam optimizer for minimizing the loss
        self.atrain = tf.train.AdamOptimizer(lr_a).minimize(a_loss, var_list=self.ae_params)

        tf.summary.FileWriter(""logs2"", self.sess.graph)

        # initialize all variables

        self.sess.run(tf.global_variables_initializer())
        self.saver = tf.train.Saver()
        self.saver.restore(self.sess, ""Pendulum/nn.ckpt"")


    def choose_action(self, s):
        a = self.sess.run(self.a, {self.state: s[np.newaxis, :]})[0]
        a = np.clip(np.random.normal(a, self.noise_variance), -2, 2)

        return a

    def learn(self):
        # soft target replacement
        self.sess.run(self.soft_replace)

        indices = np.random.choice(memory, size=batch_size)
        batch_transition = self.memory[indices, :]
        batch_states = batch_transition[:, :self.no_of_states]
        batch_actions = batch_transition[:, self.no_of_states: self.no_of_states + self.no_of_actions]
        batch_rewards = batch_transition[:, -self.no_of_states - 1: -self.no_of_states]
        batch_next_state = batch_transition[:, -self.no_of_states:]

        self.sess.run(self.atrain, {self.state: batch_states})
        self.sess.run(self.ctrain, {self.state: batch_states, self.a: batch_actions, self.reward: batch_rewards,
                                    self.next_state: batch_next_state})

    # we define a function store_transition which stores all the transition information in the buffer
    def store_transition(self, s, a, r, s_):
        trans = np.hstack((s, a, [r], s_))

        index = self.pointer % memory
        self.memory[index, :] = trans
        self.pointer += 1

        if self.pointer > memory:
            self.noise_variance *= 0.99995
            self.learn()

    # we define the function build_actor_network for builing our actor network and after crtic network
    def build_actor_network(self, s, scope, trainable)
        with tf.variable_scope(scope):
            l1 = tf.layers.dense(s, 30, activation=tf.nn.tanh, name='l1', trainable=trainable)
            a = tf.layers.dense(l1, self.no_of_actions, activation=tf.nn.tanh, name='a', trainable=trainable)
            return tf.multiply(a, self.a_bound, name=""scaled_a"")               

    def build_crtic_network(self, s, a, scope, trainable):
        with tf.variable_scope(scope):
            n_l1 = 30
            w1_s = tf.get_variable('w1_s', [self.no_of_states, n_l1], trainable=trainable)
            w1_a = tf.get_variable('w1_a', [self.no_of_actions, n_l1], trainable=trainable)
            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)
            net = tf.nn.tanh(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)

            q = tf.layers.dense(net, 1, trainable=trainable)
            return q

    def save(self):
        self.saver.save(self.sess, ""Pendulum/nn.ckpt"")

env = gym.make(""Pendulum-v0"")
env = env.unwrapped
env.seed(1)

no_of_states = env.observation_space.shape[0]
no_of_actions = env.action_space.shape[0]

a_bound = env.action_space.high
ddpg = DDPG(no_of_actions, no_of_states, a_bound)

total_reward = []

no_of_episodes = 300
# for each episodes
for i in range(no_of_episodes):
    # initialize the environment
    s = env.reset()

    # episodic reward
    ep_reward = 0

    for j in range(epsiode_steps):

        env.render()

        # select action by adding noise through OU process
        a = ddpg.choose_action(s)

        # peform the action and move to the next state s
        s_, r, done, info = env.step(a)

        # store the the transition to our experience buffer
        # sample some minibatch of experience and train the network
        ddpg.store_transition(s, a, r, s_)

        # update current state as next state
        s = s_

        # add episodic rewards
        ep_reward += r

        if int(ep_reward) == 0 and i > 200:
            ddpg.save()
            print(""save"")
            quit()

        if j == epsiode_steps - 1:
            total_reward.append(ep_reward)
            print('Episode:', i, ' Reward: %i' % int(ep_reward))

            break

","['neural-networks', 'machine-learning', 'tensorflow', 'ddpg']",
Why can't I Hyper tune my KNNBasic Algorithm?,"
I've been trying to hyper tuning my KNNBasic algorithm by the help of grid search for recommendation system for movie review data. The problem is that both of my KNNBasicTuned and KNNBasicUntuned shows the same result. Here is my code for KNNTuning..
I have tried the SVD algo tuning and it worked perfectly so all my libraries are working perfectly. However my all libraries are in my github linke : https://github.com/iSarcastic99/KNNBasicTuning
Code of KNNBasicTuning :

# -*- coding: utf-8 -*-
""""""
Created on Sat Apr  4 01:25:40 2020

@author: rahulss
""""""
#My libraries

from MovieLens import MovieLens    
from surprise import KNNBasic
from surprise import NormalPredictor
from Evaluator import Evaluator
from surprise.model_selection import GridSearchCV

import random
import numpy as np

#loading my working data
def LoadMovieLensData():
    ml = MovieLens()
    print(""Loading movie ratings..."")
    data = ml.loadMovieLensLatestSmall()
    print(""\nComputing movie popularity ranks so we can measure novelty later..."")
    rankings = ml.getPopularityRanks()
    return (ml, data, rankings)

np.random.seed(0)
random.seed(0)

# Load up common data set for the recommender algorithms
(ml, evaluationData, rankings) = LoadMovieLensData()

print(""Searching for best parameters..."")
param_grid = {'n_epochs': [10, 30], 'lr_all': [0.005, 0.010],
          'n_factors': [50, 90]}
gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse', 'mae'], cv=3)

gs.fit(evaluationData)

# best RMSE score
print(""Best RMSE score attained: "", gs.best_score['rmse'])

# combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])

# Construct an Evaluator to, you know, evaluate them
evaluator = Evaluator(evaluationData, rankings)

params = gs.best_params['rmse']
KNNBasictuned = KNNBasic(n_epochs = params['n_epochs'], lr_all =  params['lr_all'], n_factors = params['n_factors'])
evaluator.AddAlgorithm(KNNBasictuned, ""KNN - Tuned"")

KNNBasicUntuned = KNNBasic()
evaluator.AddAlgorithm(KNNBasicUntuned, ""KNN - Untuned"")


# Evaluating all algorithms
evaluator.Evaluate(False)

evaluator.SampleTopNRecs(ml, testSubject=85, k=10)

","['python', 'hyperparameter-optimization', 'recommender-system']",
What is the meaning of test data set in naive bayes classifier or decision trees?,"
What is the benefit of a test data set, especially for naive bayes estimator or decision tree construction?
When using a naive bayes classifier the probabilities are a fact. As far as I know there is nothing one could tune (like the weights in a neural net). So what is the purpose of the test data set? Simply to know if one can apply naive bayes or not?
Similiarly what is the benefit of the test data set when constructing a decision tree. We alread use the gini impurity to construct the best possibe decision tree and there is nothing we could do when we get bad results with the test data set.
","['datasets', 'decision-trees', 'naive-bayes']","
Your assumption about the test data is not correct completely. Maybe you use the test data to tune your learning algorithm to work better on the test data, but it's not the whole thing. Sometimes you need to know that the ML method is working or not and have a sense about how much does it work!
You have other scenarios that you want to evaluate your method:

Compare the result of the leaner with other techniques. For example, you are considering DT versus an SVM classifier over a data set. If you want to compare them, you need a value to found such a sense about the comparison.
Sometimes you are using an ensemble method and you want to tune some parameters to balance between using different ML methods. Hence, you need to evaluate these learning methods (DT, Naive Bayes) to improve the ensemble method.

"
"How is the state-visitation frequency computed in ""Maximum Entropy Inverse Reinforcement Learning""?","
I am trying to understand the formulation of the maximum entropy Inverse RL method by Brian Ziebart. Particularly, I am stuck on how to understand the computation of state - visitation frequencies.
In order to do so, they utilize a dynamic programming approach to compute the visitation frequency, in which the next state frequency is calculated based upon the state visitation frequency at the previous time step. 
This is the algorithm below where, $D_{s_i,t}$ is the probability of state $s_i$ being visited at time step $t$.

What is the difference between this way of computing state visitation frequency compared to the naive method of summing the total number of times state $s_i$ appears in the trajectory divided by the trajectory length?
","['reinforcement-learning', 'markov-decision-process', 'papers']","
Please look at line 5:
If $P(a_{i,j}|s_i)$ is equal to the policy that is used for generating the demonstrated trajectories, then it could be the same. However, in inverse RL you don't know $P(a_{i,j}|s_i)$ and you iteratively approximate it.
Furthermore, your technique would work if the MDP is deterministic or you would have an infinite amount of trajectories (central limit theorem). Otherwise, your summation neglects the probability of $s'$ given $s,a$ pairs, which is why you have $P(s_k|a_{i,j},s_i)$ in the equation.
"
Why does the transformer do better than RNN and LSTM in long-range context dependencies?,"
I am reading the article How Transformers Work where the author writes

Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you have to process word by word. Not only that but there is no model of long and short-range dependencies. 

Why exactly does the transformer do better than RNN and LSTM in long-range context dependencies?
","['machine-learning', 'natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory', 'transformer']","
Let's start with RNN. A well known problem is vanishin/exploding gradients, which means that the model is biased by most recent inputs in the sequence, or in other words, older inputs have practically no effect in the output at the current step. 
LSTMs/GRUs mainly try to solve this problem, by including a separate memory (cell) and/or extra gates to learn when to let go of past/current information. Check these series of lectures for more in-depth discussion. Also check the interactive parts of this article for some intuitive understanding of dependency on past elements.
Now, given all this, information from past steps still goes through a sequence of computations and we're relying on these new gate/memory mechanisms to pass information from old steps to the current one. 
One major advantage of the transformer architecture, is that at each step we have direct access to all the other steps (self-attention), which practically leaves no room for information loss, as far as message passing is concerned. On top of that, we can look at both future and past elements at the same time, which also brings the benefit of bidirectional RNNs, without the 2x computation needed. And of course, all this happens in parallel (non-recurrent), which makes both training/inference much faster. 
The self-attention with every other token in the input means that the processing will be in the order of $\mathcal{O}(N^2)$ (glossing over details), which means that it's going to be costly to apply transformers on long sequences, compared to RNNs. That's probably one area that RNNs still have an advantage over transformers. 
"
"Equivalence between expected parameter increments in ""Off-Policy Temporal-Difference Learning with Function Approximation""","
I am having a hard time understanding the proof of theorem 1 presented in the ""Off-Policy Temporal-Difference Learning with Function Approximation"" paper.
Let $\Delta \theta$ and $\Delta \bar{\theta}$ be the sum of the parameter increments over an episode under on-policy $T D(\lambda)$
and importance sampled $T D(\lambda)$ respectively, assuming
that the starting weight vector is $\theta$ in both cases. Then
$E_{b}\left\{\Delta \bar{\theta} | s_{0}, a_{0}\right\}=E_{\pi}\left\{\Delta \theta | s_{0}, a_{0}\right\}, \quad \forall s_{0} \in \mathcal{S}, a_{0} \in \mathcal{A}$
We know that:
$$
\begin{aligned}
&\Delta \theta_{t}=\alpha\left(R_{t}^{\lambda}-\theta^{T} \phi_{t}\right) \phi_{t}\\
&R_{t}^{\lambda}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} R_{t}^{(n)}\\
&R_{t}^{(n)}=r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^{n-1} r_{t+n}+\gamma^{n} \theta^{T} \phi_{t+n}
\end{aligned}
$$
and
$$\Delta \bar{\theta_{t}}=\alpha\left(\bar{R}_{t}^{\lambda}-\theta^{T} \phi_{t}\right) \phi_{t} \rho_{1} \rho_{2} \cdots \rho_{t}$$
$$
\begin{aligned}
\bar{R}_{t}^{(n)}=& r_{t+1}+\gamma r_{t+2} \rho_{t+1}+\cdots \\
&+\gamma^{n-1} r_{t+n} \rho_{t+1} \cdots \rho_{t+n-1} \\
&+\gamma^{n} \rho_{t+1} \cdots \rho_{t+n} \theta^{T} \phi_{t+n}
\end{aligned}
$$
And it is proven that:
$$
E_{b}\left\{\bar{R}_{t}^{\lambda} | s_{t}, a_{t}\right\}=E_{\pi}\left\{R_{t}^{\lambda} | s_{t}, a_{t}\right\}
$$
Here is the proof, it begins with:
$E_{b}\{\Delta \bar{\theta}\}=E_{b}\left\{\sum_{t=0}^{\infty} \alpha\left(\bar{R}_{t}^{\lambda}-\theta^{T} \phi_{t}\right) \phi_{t} \rho_{1} \rho_{2} \cdots \rho_{t}\right\}$
$=E_{b}\left\{\sum_{t=0}^{\infty} \sum_{n=1}^{\infty} \alpha(1-\lambda) \lambda^{n-1}\left(\bar{R}_{t}^{(n)}-\theta^{T} \phi_{t}\right) \phi_{t} \rho_{1} \rho_{2} \cdots \rho_{t}\right\}$.
which I believe is incorrect since,
$E_{b}\{\Delta \bar{\theta}\}=E_{b}\left\{\sum_{t=0}^{\infty} \alpha\left(\bar{R}_{t}^{\lambda}-\theta^{T} \phi_{t}\right) \phi_{t} \rho_{1} \rho_{2} \cdots \rho_{t}\right\}$
$=E_{b}\left\{\sum_{t=0}^{\infty} \alpha \left(\sum_{n=1}^{\infty}(1-\lambda) \lambda^{n-1}\bar{R}_{t}^{(n)}-\theta^{T} \phi_{t}\right) \phi_{t} \rho_{1} \rho_{2} \cdots \rho_{t}\right\}$.
and taking out the second sigma will lead to a sum over constant terms.
Furthermore, it is claimed that in order to prove the equivalence above, it is enough to prove the equivalence below:
$$
\begin{array}{c}
E_{b}\left\{\sum_{t=0}^{\infty}\left(\bar{R}_{t}^{(n)}-\theta^{T} \phi_{t}\right) \phi_{t} \rho_{1} \rho_{2} \cdots \rho_{t}\right\} \\
=E_{\pi}\left\{\sum_{t=0}^{\infty}\left(R_{t}^{(n)}-\theta^{T} \phi_{t}\right) \phi_{t}\right\}
\end{array}
$$
Which I don't understand why. and even if it is the case there are more ambiguities in the proof:
$E_{b}\left\{\sum_{t=0}^{\infty}\left(\bar{R}_{t}^{(n)}-\theta^{T} \phi_{t}\right) \phi_{t} \rho_{1} \rho_{2} \cdots \rho_{t}\right\}$
$$=\sum_{t=0}^{\infty} \sum_{\omega \in \Omega_{t}} p_{b}(\omega) \phi_{t} \prod_{k=1}^{t} \rho_{k} E_{b}\left\{\bar{R}_{t}^{(n)}-\theta^{T} \phi_{t} | s_{t}, a_{t}\right\}$$
(given the Markov property, and I don't understand why Markovian property leads to conditional independence !)
$$=\sum_{t=0}^{\infty} \sum_{\omega \in \Omega_{t}} \prod_{j=1}^{t} p_{s_{j-1}, s_{j}}^{a_{j-1}} b\left(s_{j}, a_{j}\right) \phi_{t} \prod_{k=1}^{t} \frac{\pi\left(s_{k}, a_{k}\right)}{b\left(s_{k}, a_{k}\right)} \cdot \left(E_{b}\left\{\bar{R}_{t}^{(n)} | s_{t}, a_{t}\right\}-\theta^{T} \phi_{t}\right)$$
$$= \sum_{t=0}^{\infty} \sum_{\omega \in \Omega_{t}} \prod_{j=1}^{t} p_{s_{j-1}, s_{j}}^{a_{j-1}} \pi\left(s_{j}, a_{j}\right) \phi_{t} \cdot\left(E_{b}\left\{\bar{R}_{t}^{(n)} | s_{t}, a_{t}\right\}-\theta^{T} \phi_{t}\right)$$
$$=\sum_{t=0}^{\infty} \sum_{\omega \in \Omega_{t}} p_{\pi}(\omega) \phi_{t}\left(E_{\pi}\left\{R^{(n)} | s_{t}, a_{t}\right\}-\theta^{T} \phi_{t}\right)$$
(using our previous result)
$$=E_{\pi}\left\{\sum_{t=0}^{\infty}\left(R_{t}^{(n)}-\theta^{T} \phi_{t}\right) \phi_{t}\right\} . \diamond$$
I'd be grateful if anyone could shed a light on this.
","['reinforcement-learning', 'proofs', 'temporal-difference-methods', 'function-approximation', 'off-policy-methods']",
What are some real-world products or applications that can be developed using GANs?,"
GANs have shown good progress across a wide variety of domains ranging from image translation, image generation, text to image synthesis, audio/video generation, image super-resolution and many more.
Although these concepts have great research potential, what are some real-world products or applications that can be developed using GANs?
A few I know are drug discovery and product customization. What else can you suggest?
","['deep-learning', 'computer-vision', 'applications', 'generative-adversarial-networks']",
How should I deal with multi-dimensional tensors for nodes in a graph convolution network?,"
How to work with GCN when the features of each node is not a 1D vector? For example, if the graph has N nodes and each node has features of the form $C \times D \times E$.
Also, is there an open-source implementation for such a case?
","['machine-learning', 'data-preprocessing', 'geometric-deep-learning', 'graph-neural-networks']",
Are bayesian neural networks suited for text (or document) classification?,"
I've tried to do my research on Bayesian neural networks online, but I find most of them are used for image classification. This is probably due to the nature of Bayesian neural networks, which may be significantly slower than traditional artificial neural networks, so people don't use them for text (or document) classification. Am I right? Or is there a more specific reason for that? 
Are bayesian neural networks suited for text (or document) classification?
","['neural-networks', 'classification', 'text-classification', 'bayesian-deep-learning', 'bayesian-neural-networks']",
Should I be balancing the data before creating the vocab-to-index dictionary?,"
My question is about when to balance training data for sentiment analysis. 
Upon evaluating my training dataset, which has 3 labels (good, bad, neutral), I noticed there were twice as many neutral labels as the other 2 combined, so I used a function to drop neutral labels randomly. 
However, I wasn't sure if I should do this before or after creating the vocab2index mappings. 
To explain, I am numericizing my text data by creating a vocabulary of words in the training data and linking them to numbers using enumerate. I think to use that dictionary of vocab2index values to numericise the training data. I also use that same dictionary to numericise the testing data, dropping any words that do not exist in the dictionary. 
When I took a class on this, they had balanced the training data AFTER creating the vocab2index dictionary. However, when I thought about this in my own implementation, it did not make sense. What if some words from the original vocabulary are gone completely, then we aren't training the machine learning classifier on those words, but they would not be dropping from the testing data either (since words are dropping from X_test based on whether they are in the vocab2index dictionary). 
So should I be balancing the data BEFORE creating the vocab2index dictionary? 
I linked the code to create X_train and X_test below in case it help.
def create_X_train(training_data='Sentences_75Agree_csv.csv'):
    data_csv = pd.read_csv(filepath_or_buffer=training_data, sep='.@', header=None, names=['sentence','sentiment'], engine='python')
    list_data = []
    for index, row in data_csv.iterrows():
        dictionary_data = {}
        dictionary_data['message_body'] = row['sentence']
        if row['sentiment'] == 'positive':
             dictionary_data['sentiment'] = 2
        elif row['sentiment'] == 'negative':
             dictionary_data['sentiment'] = 0
        else:
             dictionary_data['sentiment'] = 1 # For neutral sentiment
        list_data.append(dictionary_data)
    dictionary_data = {}
    dictionary_data['data'] = list_data
    messages = [sentence['message_body'] for sentence in dictionary_data['data']]
    sentiments = [sentence['sentiment'] for sentence in dictionary_data['data']]

    tokenized = [preprocess(sentence) for sentence in messages]
    bow = Counter([word for sentence in tokenized for word in sentence]) 
    freqs = {key: value/len(tokenized) for key, value in bow.items()} #keys are the words in the vocab, values are the count of those words

    # Removing 5 most common words from data
    high_cutoff = 5
    K_most_common = [x[0] for x in bow.most_common(high_cutoff)] 
    filtered_words = [word for word in freqs if word not in K_most_common]

    # Create vocab2index dictionary:
    vocab = {word: i for i, word in enumerate(filtered_words, 1)}
    id2vocab = {i: word for word, i in vocab.items()}
    filtered = [[word for word in sentence if word in vocab] for sentence in tokenized] 

    # Balancing training data due to large number of neutral sentences
    balanced = {'messages': [], 'sentiments':[]}
    n_neutral = sum(1 for each in sentiments if each == 1)
    N_examples = len(sentiments)
    # print(n_neutral/N_examples)
    keep_prob = (N_examples - n_neutral)/2/n_neutral
    # print(keep_prob)
    for idx, sentiment in enumerate(sentiments):
        message = filtered[idx]
        if len(message) == 0:
            # skip this sentence because it has length 0
            continue
        elif sentiment != 1 or random.random() < keep_prob:
            balanced['messages'].append(message)
            balanced['sentiments'].append(sentiment)

    token_ids = [[vocab[word] for word in message] for message in balanced['messages']]
    sentiments_balanced = balanced['sentiments']

    # Unit test:
    unique, counts = np.unique(sentiments_balanced, return_counts=True)
    print(np.asarray((unique, counts)).T)
    print(np.mean(sentiments_balanced))
    ##################

    # Left padding and truncating to the same length 
    X_train = token_ids
    for i, sentence in enumerate(X_train):
        if len(sentence) <=30:
            X_train[i] = ((30-len(sentence)) * [0] + sentence)
        elif len(sentence) > 30:
            X_train[i] = sentence[:30]
    return vocab, X_train, sentiments_balanced

def create_X_test(test_sentences, vocab):
    tokenized = [preprocess(sentence) for sentence in test_sentences]
    filtered = [[word for word in sentence if word in vocab] for sentence in tokenized] # X_test filtered to only words in training vocab
    # Alternate method with functional programming:
    # filtered = [list(filter(lambda a: a in vocab, sentence)) for sentence in tokenized]
    token_ids = [[vocab[word] for word in sentence] for sentence in filtered] # Numericise data

    # Remove short sentences in X_test
    token_ids_filtered = [sentence for sentence in token_ids if len(sentence)>10]
    X_test = token_ids_filtered
    for i, sentence in enumerate(X_test):
        if len(sentence) <=30:
            X_test[i] = ((30-len(sentence)) * [0] + sentence)
        elif len(sentence) > 30:
            X_test[i] = sentence[:30]
    return X_test

","['natural-language-processing', 'python', 'training', 'sentiment-analysis']",
What is a landmark in computer vision?,"
I guess I understand the concept of face detection, a technique specifies the location of multiple objects in the image, and draws bounding boxes on the target.

The question is related to the concept of a landmark. For example, the bottom guy in the image above pointed out by the red arrow has 18 green dots on his face. Is anyone of the dots is a landmark? 
What is the size of a landmark? What is the acceptable error of its position? For example, the landmark in the middle of his nose has to be in what kind of range?
Could someone please give a hint?
","['computer-vision', 'image-recognition', 'terminology']","
The paper A Brief Introduction to Statistical Shape Analysis (2002) by M. B. Stegmann and D. D. Gomez provides a definition of a landmark in the context of statistical shape analysis, which I will report below.

Definition 1: Shape is all the geometrical information that remains when location, scale and rotational effects are filtered out from an object.

For example, in the following diagram, the shape of a hand, after several transformations (i.e. translation, scaling and rotation), is illustrated. This definition captures the intuitive notion of the shape of an object. In fact, as expected, after these transformations, all of the objects below correspond to a hand.


Definition 2: A landmark is a point of correspondence on each object that matches between and within populations.

In your picture, all of the faces (apart from the occluded one) have 18 green points. Each of these points roughly corresponds to the same position in the face. For example, there's a green point on tip of the nose of each face. In your picture, the populations refer to the faces.
You can divide landmark into three groups


Anatomical landmarks Points assigned by an expert that corresponds between organisms in some biologically meaningful way.

Mathematical landmarks Points located on an object according to some mathematical or geometrical property, i.e. high curvature or an extremum point.

Pseudo-landmarks Constructed points on an object either on the outline or between landmarks.



The landmarks in your picture could be considered anatomical landmarks, so they might have been assigned by an expert (a human), so this should explain why they do not exactly correspond. Moreover, note that e.g. noses have different shapes across humans, so correspondences are unlikely to be exact in most cases.

What is the size of a landmark?

This is just an educated guess, but a landmark is probably just a point (i.e. a coordinate of a pixel in the image), although in your picture they look bigger, but this is probably to make them more noticeable.

What is the acceptable error of its position?

This is also a guess, but this might depend on the application and the used algorithms, i.e., in certain cases, your algorithms may be very sensitive to the position of the landmarks, in other cases, that might not be a big problem.
"
What is symbol-to-number differentiation?,"
I recently came across symbol-to-symbol and symbol-to-number differentiation, out of which symbol to symbol seemed fairly straightforward - the computational graph is extended to include gradient calculations and relationships between gradients.
I have a problem in understanding what exactly symbol-to-number differentiation is. Does it map directly every variable in the backprop to its relevant gradient? If yes, how does it do this without knowing about the rest of the computational graph? 
If the question is unclear, to increase context - TensorFlow uses symbol to symbol differentiation whereas torch uses symbol to number (apparently). 
Came across this in section 6.5.5 of Deep Learning, the book. The material mentioned has a convincing explanation for symbol-to-symbol differentiation but could not say the same for symbol-to-number differentiation. 
","['neural-networks', 'deep-learning', 'backpropagation', 'computational-complexity', 'symbolic-computing']",
"CNN High Variance across multiple trained models, what does it mean?","
Background:
I have a 2D CNN model that I am applying to a regression task with some uniquely extracted spectrograms. The specifics of the data set are mostly irrelevant and very domain specific so I won't go into detail, but it is essentially just image classification with a MSE loss function for each label and a unique image of 100x4000. When I re-train the model from scratch multiple times then provide it my testing data set, it has predictions that vary significantly across each iteration and thus a high variance. Supposedly the only difference between a trained model versus another would be the random initialization of weights and the random train/validation split. I feel that the train/validation split has been ruled out by when I've done k-fold cross validation and my model has done very good for all segments of my train/validation splits and acquired good results for the validation in each split. But these same models persist to have high variance in the test data set.
Question:
If I am seeing a high variance for the predictions from my trained model across multiple different runs of re-training, what do I attack first to reduce my variance on my predictions for my test data set? 
I've found many articles talking about bias and variance in the data set but not as much criticism directed towards model design. What things can I explore in my dataset or model design, and/or tools I can use to strengthen my model? Does my model need to be bigger/smaller?
Ideas/Solutions:
A few Ideas I'd like to acquire some criticism for.

Regularization applied to model such as L1/L2 Regularization, dropout, or early stopping.
Data augmentation applied to dataset (inconveniently not an option right now, but in a more general scenario it could be).
Bigger or smaller model?
Is the random initialization of weight actually very important? Maybe train multiple models and take the average of their collective answers to get the best prediction on real world data (test set).

Personal Note:
I have had experience with all these items before on other projects and with personal projects and have some moderate confidence justifying regularization and data augmentation. However, I lack some perspective as to any other tools that might be useful to explore the cause of model variance. I wanted to ask this question here to start a discussion in a general sense of this problem.
Cheers
EDIT: CLARIFICATION. When I say 'variance' I mean specifically variance across models, not variance of predictions from 1 trained model across the test set. Example: Instead lets say I am trying to predict a value somewhere between 1 and 4 (expected_val=3). I train 10 models to do this and 4 of the models accurately predict 3 with a VERY low standard deviation across all the test set samples. Thus a low variance and high accuracy/precision for these 4 models. But the other 6 models predict wildly and some predict 1 very confidently every time and the others could be 4. And I've even had models that predicted negative values even though I have NO training or testing samples that have negative labels.
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
What is the theoretical basis for the use of a validation set?,"
Let's say we use an MLE estimator (implementation doesn't matter) and we have a training set. We assume that we have sampled the training set from a Gaussian distribution $\mathcal N(\mu, \sigma^2)$.
Now, we split the dataset into training, validation and test sets. The result will be that each will have maximum likelihoods for the following Gaussian distributions $\mathcal N(\mu_{training}, \sigma^2_{training}), \mathcal N(\mu_{validation}, \sigma^2_{validation}), \mathcal N(\mu_{test}, \sigma^2_{test})$.
Now, let's assume the  case where $\mu_{validation}<\mu_{training}<\mu_{test}$ and $\mu_{training}<\mu<\mu_{test}$.
Clearly, if we perform validation using this split, then the model that gets selected will be closer to  $\mu_{validation}$, which will worsen the performance on actual data, whereas if we only used the training set, the performance could actually be better (this is the simplest case without taking into account the effect of variance).
So, we will have a $4!$ combinations between the means, and each one might improve or worsen the performance (probably in $50 \%$ cases performance will be worsened, assuming symmetry).
So, what am I missing here? Were my aforementioned assumptions wrong? Or does the validation set has a completely different purpose?
","['machine-learning', 'cross-validation', 'validation-datasets']","
I think Cross-Validation serves a completely different purpose.
From your post, it looks like you think we would use CV to get a better estimate of the parameters of our model (i.e. the model parameters after cross validation are closer to the parameters of the test data).
In fact, we use CV to get an estimate of generalization error while keeping our test set outside the training process. That is, we use it to answer the question ""What is the size of the difference between my training and testing performance likely to be?"". If you have an estimate of this that you are confident in, you can be confident that when you deploy a model to your customers, the model will actually work as you expect. 
If you're only going to build a single model, then you don't need cross validation. You just train the model on the training data, and test it on the test data. Then you have an unbiased estimate of generalization error.
However, we might want to try out many different kinds of models, and many different parameters (broadly, we might want to do hyperparameter tuning). To do this, we need to understand how generalization error changes as we change our hyperparameters, and then use this information to pick hyperparameter values that we think will minimize the actual error when we deploy the model. 
You could do this by training different models on the training set, and then testing them on the test set, recording the difference in model performance on the two sets. If you use this as a basis to pick a model though, you have effectively pulled the test set inside your training process (model parameters were implicitly selected using the test set, since you picked the parameters with the lowest test error). This bias will make your true generalization error much larger than what you observed.
As a stop gap, you could split your training set into a 'real training' set and a validation set. You could train models on the 'real training' set, and then measure their performance on the validation set. The difference would be a biased (but hopefully still useful) estimate of generalization error. You could then test against the test set just once (at the end) to get an unbiased estimate that you can use to decide whether or not to deploy the model.
A better workflow is to use CV on the training set to get an estimate of generalization error during hyperparameter optimization. You get K samples for k-fold cross-validation, so you can do statistical testing to see whether one model truly has better generalization error than another, or whether it's just a fluke. This decreases the degree of bias in your estimates of generalization error. Then, once you've completed hyperparameter optimization, you can run your final model against the test set once to obtain a truly unbiased estimate of your final generalization error.
"
Why is my SVM not reaching good accuracy when trained to perform binary classification of search results?,"
I am trying to perform binary classification of search results based on the relevance to the query.
I followed this tutorial on how to make an SVM, and I got it to work with a small iris dataset. Now, I am attempting to use the LETOR 4.0 MQ2007 dataset by Microsoft to classify. The dataset has 21 input vectors as well as a score from 0 to 2. I classified 0 as -1 and 1, 2 as 1. My algorithm reaches 57.4% accuracy after 1000 epochs with 500 samples of each classification. My learning rate is 0.0001. here is my code.
from tqdm import tqdm
import numpy as np
from sklearn.metrics import accuracy_score


print(""-------------------------------------"")
choice = input(""Train or Test: "")
print(""-------------------------------------"")

# HYPERPARAMETERS
feature_num = 21
epochs = 1000
sample_size = 500
learning_rate = 0.0001

if choice == ""Train"":

    out_file = open('weights.txt', 'w')
    out_file.close()

    print(""Serializing Train Data..."")

    # SERIALIZE DATA
    file = open('train.txt')
    train_set = file.read().splitlines()
    positive = []
    negative = []

    # GRAB TRAINING SAMPLES
    for i in train_set:
        if (i[0] == '1' or i[0] == '2') and len(positive) < sample_size:
            positive.append(i)
        if (i[0] == '0') and len(negative) < sample_size:
            negative.append(i)

    train_set = positive+negative
    file.close()

    features = []
    query = []

    # CREATE TRAINING VECTORS
    alpha = np.full(feature_num, learning_rate)
    weights = np.zeros((len(train_set), feature_num))
    output = np.zeros((len(train_set), feature_num))
    score = np.zeros((len(train_set), feature_num))

    for i in tqdm(range(len(train_set))):
        elements = train_set[i].split(' ')
        if int(elements[0]) == 0:
            score[i] = [-1] * feature_num
        else:
            score[i] = [1] * feature_num

        query.append(int(elements[1].split(':')[1]))
        tmp = []
        for feature in elements[2:2+feature_num]:
            if feature.split(':')[1] == 'NULL':
                tmp.append(0.0)
            else:
                tmp.append(float(feature.split(':')[1]))
        features.append(tmp)

    features = np.asarray(features)

    print(""-------------------------------------"")
    print(""Training Initialized..."")

    # TRAIN MODEL
    for i in tqdm(range(epochs)):

        # FORWARD y = sum(wx)
        for sample in range(len(train_set)):
            output[sample] = weights[sample]*features[sample]
            output[sample] = np.full((feature_num), np.sum(output[sample]))

        # NORMALIZE NEGATIVE SIGNS
        output = output*score
        # UPDATE WEIGHTS
        count = 0
        for val in output:
            if(val[0] >= 1):
                cost = 0
                weights = weights - alpha * (2 * 1/epochs * weights)
            else:
                cost = 1 - val[0]
                # WEIGHTS = WEIGHTS + LEARNING RATE * [X] * [Y]
                weights = weights + alpha * (features[count] * score[count] - 2 * 1/epochs * weights)

            count += 1

    # EXPORT WEIGHTS
    out_file = open('weights.txt', 'a+')
    for i in weights[0]:
        out_file.write(str(i)+'\n')
    out_file.close()

elif choice == ""Test"":

    print(""Serializing Test Data..."")

    # SERIALIZE DATA
    file = open('train.txt')
    train_set = file.read().splitlines()
    positive = []
    negative = []
    for i in train_set:
        if (i[0] == '1' or i[0] == '2') and len(positive) < sample_size:
            positive.append(i)
        if (i[0] == '0') and len(negative) < sample_size:
            negative.append(i)

    test_set = positive+negative

    file = open('weights.txt', 'r').read().splitlines()
    weights = np.zeros((len(test_set), feature_num))

    # CREATE TEST SET
    for i in range(len(weights)):
        weights[i] = file
    features = []
    query = []
    output = np.zeros((len(test_set), feature_num))
    score = np.zeros((len(test_set)))

    for i in tqdm(range(len(test_set))):
        elements = test_set[i].split(' ')
        if int(elements[0]) == 0:
            score[i] = -1
        else:
            score[i] = 1

        query.append(int(elements[1].split(':')[1]))
        tmp = []
        for feature in elements[2:2+feature_num]:
            if feature.split(':')[1] == 'NULL':
                tmp.append(0.0)
            else:
                tmp.append(float(feature.split(':')[1]))
        features.append(tmp)

    features = np.asarray(features)

    for sample in range(len(test_set)):
        output[sample] = weights[sample]*features[sample]
        output[sample] = np.full((feature_num), np.sum(output[sample]))

    predictions = []
    for val in output:
        if(val[0] > 1):
            predictions.append(1)
        else:
            predictions.append(-1)

    print(""-------------------------------------"")
    print(""Predicting..."")
    print(""-------------------------------------"")
    print(""Prediction finished with ""+str(accuracy_score(score, predictions)*100)+""% accuracy."")


My training algorithm
if(val[0] >= 1):
    cost = 0
    weights = weights - alpha * (2 * 1/epochs * weights)
else:
    cost = 1 - val[0]
    # WEIGHTS = WEIGHTS + LEARNING RATE * [X] * [Y]
    weights = weights + alpha * (features[count] * score[count] - 2 * 1/epochs * weights)


What could I do to help the model train? Am I not giving it enough time? Is the algorithm wrong? Are the hyperparameters ok?
Thanks for all your help.
","['ai-design', 'classification', 'support-vector-machine']",
Is this a correct visual representation of a recurrent neural network (RNN)?,"

This is a picture of a recurrent neural network (RNN) found on a udemy course (Deep Learning A-Z). The axis at the bottom is ""time"". 
In a time series problem, each yellow row from left to right would represent a sequence of a feature. In this picture, then, there are 6 sequences from 6 different features that are being fed to the network.
I am wondering if the arrows in this picture are completely accurate in an RNN. Shouldn't every yellow node also connect to every other blue node along its depth dimension? By depth dimension here I mean the third dimensional axis of the input tensor. 
For example, the yellow node at the bottom left of this picture, which is closest to the viewer, should have an arrow pointing to all the blue nodes in the array of blue nodes that is at the very left, and not just to the blue node directly above it.
","['neural-networks', 'recurrent-neural-networks']",
What are the challenges faced by using NLP to convert mathematical texts into formal logic?,"
From what I've figured
(a) converting mathematical theorems and proofs from English to formal logic is a straightforward job for mathematicians with sufficient background, except that it takes time. 
(b) once converted to formal logic, computer verification of the proof becomes straightforward.  
If we can automate (a), a lot of time and intellectual labour (that could be dedicated elsewhere) is saved in doing (b) on published research papers. 
Note that if solving (a) in its entirety is hard, we could expect the mathematicians to meet the computer system halfway and avoid writing lengthy English paras that are hard to convert. If it becomes doable enough, submitting a formal logical version of your paper could even become a standard procedure that is expected. 
Additional benefit of solving (a) would be to do the process in reverse: mathematicians could delegate smaller tasks and lemmas (both trivial and non-trivial tasks) to an automated theorem prover (ATP). Assisted theorem proving will become more popular and boost productivity, maybe even surprise us once in a while by coming up with proofs that the paper writer couldn't. This is further of value if we predict a sharp upward trajectory of the capability of ATPs in the future. If anything, this could be self-fulfilling, as the demonstration of potential for good ATPs combined by a large corpus of proofs and problems in formal logical format could drive an increase in research on ATPs.
Forgive me if I sound like a salesman, but how doable is this? What will be the main challenges faced in developing NLP-based AI to convert papers, and how tractable are these challenges given today's state of the field?
P.s. I understand that proofs generated by ATPs are often hard to understand intuitively and can end up proving results without clearly exposing the underlying proof method used. But it is still a benefit to be able to use the final results
","['natural-language-processing', 'math', 'logic', 'natural-language-understanding', 'automated-theorem-proving']","
I can see several challenges, and the list below is not exhaustive:
i. The main problem is how to model a problem of translating a language test into a formal language. It will probably be something like the automatic translators, but with some guarantees that the proof semantics will be preserved. If you are more interested in this path, I recommend researching what PAC, Information Theory, Computational Proof theory, Complexity theory can contribute to this modeling.
ii. Another problem is how to get the data reliable. You commented that as people used it they would generate this data. But the problem is not just collecting the data. How much you will trust the data and how you will measure the model's performance in translation.
iii. Another problem is more humane, how do you get mathematicians to use such a system? And how to make the model self-explainable.
I believe that this is one of the most difficult problems in machine learning. I once saw this video a while ago and I don't know if it can help anything. I also recommend the stack exchange of theoretical computer science, there you will probably have a more complete answer.
"
How to perform back propagation with different sized layers?,"
I'm developing my first neural network, using the well known MNIST database of handwritten digit. I want the NN to be able to classify a number from 0 to 9 given an image.
My neural network consists of three layers: the input layer (784 neurons, each one for every pixel of the digit), a hidden layer of 30 neurons (it could also be 100 or 50, but I'm not too worried about hyperparameter tuning yet), and the output layer, 10 neurons, each one representing the activation for every digit. That gives to me two weight matrices: one of 30x724 and a second one of 10x30.
I know and understand the theory behind back propagation, optimization and the mathematical formulas behind that, that's not a problem as such. I can optimize the weights for the second matrix of weights, and the cost is indeed being reduced over time. But I'm not able to keep propagating that back because of the matrix structure.
Knowing that I have find the derivative of the cost w.r.t. the weights:
d(cost) / d(w) = d(cost) / d(f(z)) * d(f(z)) / d(z) * d(z) / d(w)

(Being f the activation function and z the dot product plus the bias of a neuron)
So I'm in the rightmost layer, with an output array of 10 elements. d(cost) / d(f(z)) is the subtraction of the observed an predicted values. I can multiply that by d(f(z)) / d(z), which is just f'(z) of the rightmost layer, also an unidimensional vector of 10 elements, having now d(cost) / d(z) calculated. Then, d(z)/d(w) is just the input to that layer, i.e. the output of the previous one, which is a vector of 30 elements. I figured that I can transpose d(cost) / d(z) so that T( d(cost) / d(z) ) * d(z) / d(w) gives me a matrix of (10, 30), which makes sense because it matches with the dimension of the rightmost weight matrix.
But then I get stuck. The dimension of d(cost) / d(f(z)) is (1, 10), for d(f(z)) / d(z) is (1, 30) and for d(z) / d(w) is (1, 784). I don't know how to come up with a result for this.
This is what I've coded so far. The incomplete part is the _propagate_back method. I'm not caring about the biases yet because I'm just stuck with the weights and first I want to figure this out. 
import random
from typing import List, Tuple

import numpy as np
from matplotlib import pyplot as plt

import mnist_loader

np.random.seed(42)

NETWORK_LAYER_SIZES = [784, 30, 10]
LEARNING_RATE = 0.05
BATCH_SIZE = 20
NUMBER_OF_EPOCHS = 5000


def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def sigmoid_der(x):
    return sigmoid(x) * (1 - sigmoid(x))


class Layer:

    def __init__(self, input_size: int, output_size: int):
        self.weights = np.random.uniform(-1, 1, [output_size, input_size])
        self.biases = np.random.uniform(-1, 1, [output_size])
        self.z = np.zeros(output_size)
        self.a = np.zeros(output_size)
        self.dz = np.zeros(output_size)

    def feed_forward(self, input_data: np.ndarray):
        input_data_t = np.atleast_2d(input_data).T
        dot_product = self.weights.dot(input_data_t).T[0]
        self.z = dot_product + self.biases
        self.a = sigmoid(self.z)
        self.dz = sigmoid_der(self.z)


class Network:

    def __init__(self, layer_sizes: List[int], X_train: np.ndarray, y_train: np.ndarray):
        self.layers = [
            Layer(input_size, output_size)
            for input_size, output_size
            in zip(layer_sizes[0:], layer_sizes[1:])
        ]
        self.X_train = X_train
        self.y_train = y_train

    @property
    def predicted(self) -> np.ndarray:
        return self.layers[-1].a

    def _normalize_y(self, y: int) -> np.ndarray:
        output_layer_size = len(self.predicted)
        normalized_y = np.zeros(output_layer_size)
        normalized_y[y] = 1.

        return normalized_y

    def _calculate_cost(self, y_observed: np.ndarray) -> int:
        y_observed = self._normalize_y(y_observed)
        y_predicted = self.layers[-1].a

        squared_difference = (y_predicted - y_observed) ** 2

        return np.sum(squared_difference)

    def _get_training_batches(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        train_batch_indexes = random.sample(range(len(X_train)), BATCH_SIZE)

        return X_train[train_batch_indexes], y_train[train_batch_indexes]

    def _feed_forward(self, input_data: np.ndarray):
        for layer in self.layers:
            layer.feed_forward(input_data)
            input_data = layer.a

    def _propagate_back(self, X: np.ndarray, y_observed: int):
        """"""
        der(cost) / der(weight) = der(cost) / der(predicted) * der(predicted) / der(z) * der(z) / der(weight)
        """"""
        y_observed = self._normalize_y(y_observed)
        d_cost_d_pred = self.predicted - y_observed

        hidden_layer = self.layers[0]
        output_layer = self.layers[1]

        # Output layer weights
        d_pred_d_z = output_layer.dz
        d_z_d_weight = hidden_layer.a  # Input to the current layer, i.e. the output from the previous one

        d_cost_d_z = d_cost_d_pred * d_pred_d_z
        d_cost_d_weight = np.atleast_2d(d_cost_d_z).T * np.atleast_2d(d_z_d_weight)

        output_layer.weights -= LEARNING_RATE * d_cost_d_weight

        # Hidden layer weights
        d_pred_d_z = hidden_layer.dz
        d_z_d_weight = X

        # ...

    def train(self, X_train: np.ndarray, y_train: np.ndarray):
        X_train_batch, y_train_batch = self._get_training_batches(X_train, y_train)
        cost_over_epoch = []

        for epoch_number in range(NUMBER_OF_EPOCHS):
            X_train_batch, y_train_batch = self._get_training_batches(X_train, y_train)

            cost = 0
            for X_sample, y_observed in zip(X_train_batch, y_train_batch):
                self._feed_forward(X_sample)
                cost += self._calculate_cost(y_observed)
                self._propagate_back(X_sample, y_observed)

            cost_over_epoch.append(cost / BATCH_SIZE)

        plt.plot(cost_over_epoch)
        plt.ylabel('Cost')
        plt.xlabel('Epoch')
        plt.savefig('cost_over_epoch.png')


training_data, validation_data, test_data = mnist_loader.load_data()
X_train, y_train = training_data[0], training_data[1]

network = Network(NETWORK_LAYER_SIZES, training_data[0], training_data[1])
network.train(X_train, y_train)

This is the code for mnist_loader, in case someone wanted to reproduce the example:
import pickle
import gzip


def load_data():
    f = gzip.open('data/mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = pickle.load(f, encoding='latin-1')
    f.close()

    return training_data, validation_data, test_data


","['neural-networks', 'backpropagation']",
How do LSTM and GRU avoid to overcome the vanishing gradient problem?,"
I'm watching the video Recurrent Neural Networks (RNN) | RNN LSTM | Deep Learning Tutorial | Tensorflow Tutorial | Edureka where the author says that the LSTM and GRU architecture help to reduce the vanishing gradient problem. How do LSTM and GRU prevent the vanishing gradient problem?
","['natural-language-processing', 'long-short-term-memory', 'vanishing-gradient-problem']","
LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.
"
Feature extraction for exponentially damped signals,"
I am looking into exponentially damped signals where it is a stationary signal (after implementing the Adfuller statistical test) and I would like to look into how can I extract meaningful features out of the signal in order to do pattern recognition with machine learning. Can anyone guide me on where I can find articles/blogs of signal processing techniques and feature extraction of exponentially damped signal?
My situation:
I want to look into features that relate to damping of the signal, I already looked at it in the frequency domain and I found out that from my datasets (considering the first 3 Natural frequencies/modes) the peaks are almost the same (there's like the same deviation by only like [+] or [-] 0.5 from freq. values). Looking into the damping factor, I found out that only the second damping ratio was different but still small deviation around the same ([+] or [-] 0.5). So, I thought that it would be difficult for machine learning to identify the difference between cases. One of my ideas is to look into energy dissipation as it might be related to damping, but I don't know how to approach it or from which domain I need to go in order to get the features. 
Side Question:
I have several questions regarding signal processing:

Say I have a signal and would like to extract features from it,
what steps or points that I should know in order to implement signal processing? (As I am using Python).
When I used signaltonoise function online (python) in order to see
the signal-to-noise ratio and I got a positive SNR. However, if I
pass the signal into, for example, a band-pass filter to concentrate on a certain frequency band I get a negative SNR. Why is that?
How can I extract features from STFT? And I also know about wavelet and HHT, what are the uses of both algorithms and how to also extract features from it?

","['machine-learning', 'python', 'feature-selection', 'signal-processing', 'feature-extraction']",
Why is it called back-propagation?,"
While looking at the mathematics of the back-propagation algorithm for a multi-layer perceptron, I noticed that in order to find the partial derivative of the cost function with respect to a weight (say $w$) from any of the hidden layers, we're just writing the error function from the final outputs in terms of the inputs and hidden layer weights and then canceling all the terms without $w$ in it as differentiating those terms with respect to $w$ would give zero. 
Where is the back-propagation of error while doing this? This way, I can find the partial derivatives of the first hidden layer first and then go towards the other ones if I wanted to. Is there some other method of going about it so that the Back Propagation concept comes into play? Also, I'm looking for a general method/algorithm, not just for 1-2 hidden layers. 
I'm fairly new to this and I'm just following what's being taught in class. Nothing I found on the internet seems to have proper notation so I can't understand what they're saying.
","['terminology', 'backpropagation', 'history', 'multilayer-perceptrons']","

Why is it called back-propagation?

I don't think there is anything special here!
It's called back-propagation (BP) because, after the forward pass, you compute the partial derivative of the loss function with respect to the parameters of the network, which, in the usual diagrams of a neural network, are placed before the output of the network (i.e. to the left of the output if the output of the network is on the right, or to the right if the output of the network is on the left).
It's also called BP because it is just the application of the chain rule. Why is this interesting?
Let me answer this question with an example. Consider the function $y=e^{\sin(x^{2})}$. This is a composite function, i.e. a function composed of multiple simpler functions, which, in this case, are $e^x$, $\sin(x)$, $x^2$ and $x$. To compute the derivative of $y$ with respect to $x$, let's define the following variables
\begin{align}
y &= f(u) = e^u,\\
u &= g(v) = \sin v = \sin(x^2),\\
v &= h(x) = x^2
\end{align}
The derivative of $y$ with respect to the variable $x$ is (according to the chain rule)
$$
\underset{\color{red}{\LARGE \rightarrow}}{
\frac{dy}{dx} = \frac{dy}{du} \color{green}{\cdot} \frac{du}{dv}  \color{green}{\cdot} 
\frac{dv}{dx}}
$$
If you read this equation from the left to the right, you can see that we are going backward (i.e. from the function $y$ to the function $v$). This is the same thing with BP!
Why is it called ""chain rule""? Because you are chaining different partial derivatives. More specifically, you are multiplying them.
BP is also known as the reverse mode of automatic differentiation. Why? The automatic differentiation should be self-explanatory, given that the BP algorithm is just the computation of partial derivatives, and you do this automatically, i.e. with a program, rather than by hand. The expression ""reverse mode"" refers to the fact that we compute the derivatives from the outer function (which, in the example above, is $e^x$) to the inner function (which, in the example above, is $x$). The Wikipedia article related to automatic differentiation provides more details.

What exactly are you back-propagating?

The partial derivative of the loss function $\mathcal{L}$ with respect to a parameter $w_i$, i.e. $\frac{\partial \mathcal{L}}{\partial w_i}$, intuitively, represents the ""contribution"" of the parameter $w_i$ to the loss. After having computed these partial derivatives (i.e. the gradient), you use gradient descent to update each parameter $w_i$ as follows
$$
w_i \leftarrow w_i - \gamma \frac{\partial \mathcal{L}}{\partial w_i}
$$
where $\frac{\partial \mathcal{L}}{\partial w_i}$ represents WHAT we propagatED, which is the error (or loss) that the neural network makes.
This gradient descent step will hopefully make your network produce a smaller error next time.
The modern version of back-propagation was published (in 1970) by a Finnish master's student called Seppo Linnainmaa, but he didn't reference neural networks. This Jürgen Schmidhuber's article goes into the details of the history of BP.
"
How I can identify holes in a 3D CAD file?,"
How I can identify holes in a 3D CAD file?  I want to identify different types of holes, counterbored or countersunk holes. My program lets me extract, for example, the faces and adjacency of the faces.
I am talking about the Siemens NX for example. 
The different types of holes you can see there:
https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D_VwwfDrdggc&psig=AOvVaw1kIYdOt2qxazSYFXwHuXpb&ust=1586329824363000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCOjd1qHh1egCFQAAAAAdAAAAABAJ
","['machine-learning', 'pattern-recognition']",
"What are ""proxy data sets"" in machine learning?","
The paper Assessment of Deep Generative Models for High-Resolution Synthetic Retinal Image Generation of Age-Related Macular Degeneration uses the term ""proxy data sets"" in this way

To develop DL techniques for synthesizing high-resolution realistic fundus images serving as proxy data sets for use by retinal specialists and DL machines.

I googled that term, but didn't find a definition of ""proxy data sets"". What are ""proxy data sets"" in machine learning?
The paper Analysis of Manufacturing Process Sequences, Using Machine Learning on Intermediate Product States (as Process Proxy Data) mentions a similar term

The advantage of the product state-based view is the focus on the product itself to structure the information and data involved throughout the process. Using the intermediate product states as proxy data for this purpose

Does ""proxy data"" mean the same thing as ""proxy data sets"" does?
","['machine-learning', 'datasets', 'terminology']",
"Stochastic gradient descent does not behave as expected, even with different activation functions","
I have been working on my own AI for a while now, trying to implemented SGD with momentum from scratch in python. After looking around and studying all the maths behind it, i finally managed to implement SGD in a neural network that i trained to recognize the classic MNIST digits dataset.
As activation function i always used sigmoid for both hidden and output neurons, and everything seems to work more or less ok, but now i wanted to step it up a bit and try to let SGD operate with different activations, so i added 2 other  functions to my code: relu and tanh.
The behaviours that i expected based on articles, documentation and ""tutorials"" found online were:
tanh: Should be slightly better than sigmoid
relu: should be much better than sigmoid and tanh
(By better i mean faster or at least higher accuracy the the end, or a mix of both)
Using tanh it looks like it's much slower converging to a minimum compared to sigmoid
Using relu...well, the results were very, VERY horrible
Here's the outputs with the different activations (Learning rate: 0.1, Epochs: 5, MiniBatch size: 10, Momentum: 0.9)
Sigmoid training

[Sigmoid for hidden layers, sigmoid for output layer]
Epoch: 1/5 (14.3271 s): Loss: 0.0685, Accuracy: 0.6231, Learning rate: 0.10000
Epoch: 2/5 (14.0060 s): Loss: 0.0503, Accuracy: 0.6281, Learning rate: 0.10000
Epoch: 3/5 (14.0081 s): Loss: 0.0482, Accuracy: 0.6382, Learning rate: 0.10000
Epoch: 4/5 (13.8516 s): Loss: 0.0471, Accuracy: 0.7085, Learning rate: 0.10000
Epoch: 5/5 (13.9411 s): Loss: 0.0374, Accuracy: 0.7990, Learning rate: 0.10000

Tanh training

[Tanh for hidden layers, sigmoid for output layer]
Epoch: 1/5 (13.7553 s): Loss: 0.3708, Accuracy: 0.4171, Learning rate: 0.10000
Epoch: 2/5 (13.7666 s): Loss: 0.2580, Accuracy: 0.4623, Learning rate: 0.10000
Epoch: 3/5 (13.5550 s): Loss: 0.2289, Accuracy: 0.4824, Learning rate: 0.10000
Epoch: 4/5 (13.7311 s): Loss: 0.2211, Accuracy: 0.5729, Learning rate: 0.10000
Epoch: 5/5 (13.6996 s): Loss: 0.2142, Accuracy: 0.5779, Learning rate: 0.10000

Relu training

[Relu for hidden layers, sigmoid for output layer]
Epoch: 1/5 (14.2100 s): Loss: 0.7725, Accuracy: 0.0854, Learning rate: 0.10000
Epoch: 2/5 (14.6218 s): Loss: 0.1000, Accuracy: 0.0854, Learning rate: 0.10000
Epoch: 3/5 (14.2116 s): Loss: 0.1000, Accuracy: 0.0854, Learning rate: 0.10000
Epoch: 4/5 (14.1657 s): Loss: 0.1000, Accuracy: 0.0854, Learning rate: 0.10000
Epoch: 5/5 (14.1427 s): Loss: 0.1000, Accuracy: 0.0854, Learning rate: 0.10000

Another run with relu

Epoch: 1/5 (14.7391 s): Loss: 15.4055, Accuracy: 0.1658, Learning rate: 0.10000
Epoch: 2/5 (14.8203 s): Loss: 59.2707, Accuracy: 0.1709, Learning rate: 0.10000
Epoch: 3/5 (15.3785 s): Loss: 166.1310, Accuracy: 0.1407, Learning rate: 0.10000
Epoch: 4/5 (14.9285 s): Loss: 109.9386, Accuracy: 0.1859, Learning rate: 0.10000
Epoch: 5/5 (15.1280 s): Loss: 158.9268, Accuracy: 0.1859, Learning rate: 0.10000

For these examples the epochs are just 5 but incrementing the epochs the results dont change, tanh and relu for me perform worse than sigmoid.
Here is my python code reference for SGD:
SGD with momentum
This method was created to accept different activation functions to dynamically use them when creating the neural network object
The activation functions and their derivatives:
Activation functions and derivatives
The loss function i used is the mean squared error:

def mean_squared(output, expected_result):
    return numpy.sum((output - expected_result) ** 2) / expected_result.shape[0]


def mean_squared_derivative(output, expected_result):
    return output - expected_result


Is there some concept i am missing? Am i using the activation functions the wrong way? I really cannot find the answer to this even after searching for a long time.
I feel like the problem is somewhere in the backpropagation but i can't find it.
Any kind of help would be greatly appriciated
PS: I hope i posted this in the right place, i am pretty new to asking questions here, so if there is any problem i will move the question somewhere else
Edit:
I tried to implement this with tensorflow, using relu for hidden layers and sigmoid for output. The results i get with this implementation are the same as the ones i mentioned in my question, so unless i am doing something wrong in both situations i am left to think i cannot use relu with sigmoid, which makes sense cause relu can have very high values while sigmoid pushes them down between 0 and 1, therefore most of the times giving values very close to 1.
Code reference:
TensorFlow implementation
","['neural-networks', 'python', 'training', 'backpropagation', 'stochastic-gradient-descent']","
Could you post the pseudocode of your backpropagation algorithm?
I recommend you start off as simple as possible (this includes your cost f(x), I would simply use Yexpected-Youtput) and see if it works and then continue adding things. If it's your first time with neural networks, I recommend you check this link out and you could also try practising the algorithms on a programming language like Octave/Matlab (it can be very efficient speed wise). Also check this question out (link). At the bottom there is a code example for the XOR problem. Please post the pseudocode of your code instead of just dumping it there.  Finally, don't just copy paste algorithms into your code, you need to understand them.
"
"Before GAN, what are the commonly used techniques for image-to-image translation?","
As per a post, image-to-image translation is a type of CV problem. 
I guess I understand the concept of image-to-image translation.

I am aware that GANs(generative adversarial networks) are good at this kind of problems.
I just wondered what the commonly used techniques are for this kind of problems Before GAN?
Could someone please give a hint? Thanks in advance.
",['computer-vision'],
Why do we need both the validation set and test set?,"
I know that this has been asked a hundred times before, however, I was not able to find a question (and an answer) which actually answered what I wanted to know, respectively, which explained it in a way I was able to understand. So, I'm trying to rephrase the question…
When working with neural networks, you typically split your data set into three parts:

Training set
Validation set
Test set

I understand that you use the training set for, well, train the network, and that you use the test set to verify how well it has learned: By measuring how well the network performs on the test set, you know what to expect when actually using it later on. So far, so good.
Now, a model has hyper parameters, which – besides the weights – need to be tuned. If you change these, of course, you get different results. This is where in all explanations the validation set comes into play:

Train using the training set
Validate how well the model performs using the validation set
Repeat this for a number of variants which differ in their hyperparameters (or do it in parallel, right from the start)
Finally, select one and verify its performance using the test set

Now, my question is: Why would I need steps 2 and 3? I could as well train multiple version of my model in parallel, and then run all of them against the test set, to see which performs best, and then use this one.
So, in other words: Why would I use the validation set for comparing the model variants, if I could directly use the test set to do so? I mean, I need to train multiple versions either way. What is the benefit of doing it like this?
Probably, there is some meaning to it, and probably I got something wrong, but I can't figure out what. Any hints?
","['neural-networks', 'machine-learning', 'ai-design', 'datasets', 'hyperparameter-optimization']","
Simply stated, you use your validation set to regularize your model for unseen data.
Test data is completely unseen data, on which you evaluate your model.
Various validation strategies are used to improve your model to perform for unseen data. So strategies like k-fold cross-validation are used.
Also, the validation set helps you in tuning your hyperparameters such as learning rate, batch size, hidden units, number of layers, etc.
Train, Validation, Test sets help you in identifying whether you are underfitting or overfitting.
E.g. If human error at a task is 1%, train error is 8%, validation error is 10%, test set error is 12 % then,
Difference between, 

Human level and training set error tells you about ""Avoidable Bias""
Training set error and Validation set error tells you about ""Variance and data mismatch""
Validation set error and Test error tells you about ""degree of overfitting"" with the validation set.

Based on these metrics, you can apply appropriate strategies for better performance on validation or test sets.
"
"What does ""shape information"" mean in terms of GAN(generative adversarial networks)?","
A paper says

However, annotations used as inputs to C-GAN are typically based only on shape information, which can result in undesirable intensity distributions in the resulting artificially-created images.

What does ""shape information"" mean here?
I am aware of basic concept of GAN(generative adversarial networks), though I don't understand what ""shape information"" refers to. 

I am aware the image above is an illustration of Image Segmentation. Would I think of the any one of the segmented area (red, green, blue) as a shape for a GAN?
Could someone please give a hint? Thanks in advance.
","['terminology', 'generative-adversarial-networks']",
Inner working of Bidirectional RNNs,"
I'm trying to understand how Bidirectional RNNs work.
Specifically, I want to know whether a single cell is used with  different states, or two different cells are used, each having independent parameters.
In pythonic pseudocode,
Implementation 1:
cell = rev_cell = RNNCell()
cell_state = cell.get_initial_state()
rev_cell_state = rev_cell.get_initial_state()
for i in range(len(series)):
    output, cell_state = cell(series[i], cell_state)
    rev_output, rev_cell_state = rev_cell(series[-i-1], rev_cell_state)
    final_output = concatenate([output, rev_output])

Implementation 2:
cell = RNNCell()
rev_cell = RNNCell()
cell_state = cell.get_initial_state()
rev_cell_state = rev_cell.get_initial_state()
for i in range(len(series)):
    output, cell_state = cell(series[i], cell_state)
    rev_output, rev_cell_state = rev_cell(series[-i-1], rev_cell_state)
    final_output = concatenate([output, rev_output])

Which of the above implementations is correct? Or is the working of Bidirectional RNNs completely different altogether?
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'time-series', 'gated-recurrent-unit']",
"How does one detect linguistic recursion so as to know how much nesting there is, if any?","
To be clear, recursion in linguistics is here better called ""nesting"" in this CS context to avoid confusing it with the other recursion. How does one detect nesting? I am particularly interested in the example case of conjunctions. For example: say that I want to look for sentences that look like this:

Would you rather have ten goldfish or a raccoon?

Seems straightforward: a binary choice. However, how do you distinguish a binary choice with nesting from a ternary (or n-ary) choice?

Would you rather have (one or two dogs) or (a raccoon)?
Would you rather have (two dogs) or (ten goldfish) or (a raccoon)?

Ditto for implied uses of ""or,"" which is more common than the latter of the above:

Would you rather have (one or two dogs),[nothing] (ten goldfish), or (a raccoon)?

Given the available tools for NLP (POS-taggers and the like), how do you count the number of conjunctions to say ""there are n surface-level clauses in the sentence, with n-or-zero clauses nested within.""?
","['natural-language-processing', 'computational-linguistics']",
How to handle extremely 'long' images?,"
After transforming timeseries into an image format, I get a width-height ratio of ~135. Typical image CNN applications involve either square or reasonably-rectangular proportions - whereas mine look nearly like lines:

Example dimensions: (16000, 120, 16) = (width, height, channels).
Are 2D CNNs expected to work well with such aspect ratios? What hyperparameters are appropriate - namely, in Keras/TF terms, strides, kernel_size (is 'unequal' preferred, e.g. strides=(16, 1))? Relevant publications would help.

Clarification: width == timesteps. The images are obtained via a transform of the timeseries, e.g. Short-time Fourier Transform. channels are the original channels. height is the result of the transform, e.g. frequency information. The task is binary classification of EEG data (w/ sigmoid output).
Relevant thread
","['convolutional-neural-networks', 'tensorflow', 'python', 'keras', 'image-processing']","
I had recently used a slightly unorthodox method to process such images, which involved using RNNs.
Assume the image dimensions to be (16000, 120, 16) = (width, height, channels), as in the question.
Apply a 2D convolution (or multiple such convolutions) of shape(1, k, c),
such that the output of the convolutions becomes (16000, 1, c). So if you only use a single convolutional layer, k=120.
Then, squeeze the extra dimension, to get the shape (16000, c). 
The problem has now been transformed back into a sequence problem! You can use RNN variants for further processing.
"
How can I use a Hidden Markov Model to recognize images?,"
How could I use a 16x16 image as an input in a HMM? And at the same time how would I train it? Can I use backpropagation?
","['image-recognition', 'backpropagation', 'markov-chain', 'hidden-markov-model']",
How to implement a LSTM for multilabel classification problem?,"
I would like to develop an LSTM because I have a variable input matrix. I am zero-padding to a specific length of 800. 
However, I am not sure of how to classify a certain situation when each input matrix has multiple labels inside, i.e. 0, 1 and 2. Do I need to use multi-label classification?

Data shape
(250,800,4)

x_train(150,800,4)
y_train(150,800,1)
x_test(100,800,4)
y_test(100,800,1)


Building LSTM
model = Sequential()    
model.add(LSTM(100, input_shape=))    
model.add(Dropout(0.5))    
model.add(Dense(100, activation='relu'))    
model.add(Dense(800, activation='softmax'))    
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

I am not sure of how to build the LSTM for training and test. If I train my model with a 3D shape. It will mean that for real time predictions I should also have a 3D shape, but the idea is to have a 2D matrix as an input. 
","['classification', 'recurrent-neural-networks', 'long-short-term-memory']",
What is the difference between model and data distributions?,"
Is there any difference between the model distribution and data distribution, or are they the same?
","['machine-learning', 'comparison', 'models', 'probability-distribution', 'statistics']","
Yes. In Machine Learning we consider that the samples in your training set are sampled from an underlying distribution called the data generating distribution.
Generative models classify the samples by trying to learn the distribution of the data. In most cases, either the model is incapable of doing so, or the training samples aren't enough to properly describe the data-generating distribution, so the model learns an approximation of this. This is what you call the model's distribution.
You can find more info about these concepts in a more detailed answer I wrote. If you're familiar with GANs, you can also read this post, to see where these two concepts come into play when training the two networks.    
"
What are examples of tutorials and blogs for beginners to master the cross-lingual information retrieval?,"
Currently, I am following the Dan Jurofsky NLP Tutorial and CS 224 Stanford 2019. Can you list tutorials and blogs for beginners to master the cross-lingual information retrieval?
","['natural-language-processing', 'resource-request', 'information-retrieval']",
Running 10 epochs on the Food-101 dataset,"
I’m currently working on the Food-101 dataset. I want to train a model that is greater than 85% accuracy for top-1 for the test set, using a ResNet50 or smaller network with a reasonable set of augmentations. I’m running 10 epochs using ResNet34 and I’m currently on the 8th epoch. This is how its doing:
epoch   train_loss  valid_loss  error_rate  time
0   2.526382    1.858536    0.465891    25:21
1   1.981913    1.566125    0.406881    27:21
2   1.748959    1.419548    0.372129    27:16
3   1.611638    1.315319    0.346980    25:16
4   1.568304    1.250232    0.328069    24:43
5   1.438499    1.193816    0.313762    24:26
6   1.378019    1.156924    0.307426    24:30
7   1.331075    1.131671    0.299010    24:26
8   1.314978    1.115857    0.297079    24:24

As you can see, it doesn’t seem like I’m going to do better than 71% accuracy at this point. The dataset size is 101,000. It has 101 different kinds of food and each food has a 1000 images. Training this definitely takes long but what are some things I can do to improve its accuracy?
","['deep-learning', 'classification', 'computer-vision']","
try using an adjustable learning rate. Keras has a number of callbacks that are useful for this purpose. The ReduceLROnPlateau callback can be used to monitor validation loss and reduce the learning rate by a factor if the validation loss does not decrease after a user specified number of epochs. The ModelCheckpoint callback is useful to monitor the validation loss and save the model with the lowest loss which can then be used to make predictions. Documentation is here.
"
What is the intuition behind grid-based solutions to POMDPs?,"
After spending some time reading about POMDP, I'm still having a hard time understanding how grid-based solutions work.
I understand the finite horizon brute-force solution, where you have your current belief distribution, enumerate every possible collection of action/observation combinations for a given depth and find the expected reward.
I have tried to read some sources about grid-based approximations, for example, these slides describe the grid-based approach.
However, it's not clear to me what exactly is going on. I'm not understanding how the value function is actually computed. After you take an action, how do you update your belief states to be consistent with the grid? Does the grid-based solution simply reduce the set of belief states? How does this reduce the complexity of the problem? 
I'm not seeing how this reduces the number of actions, observation combinations needed to be considered for a finite-horizon solution. 
","['markov-decision-process', 'pomdp']","
I will attempt to provide an answer to your questions based on the information you can find in the papers A Heuristic Variable Grid Solution Method for POMDPs (1997) by Ronen I. Brafman and Point-based value iteration: An anytime algorithm for POMDPs (2003) by Joelle Pineau et al.
A grid-based approximate solution to a POMDP attempts to estimate a value function only at a subset of the number of belief states. Why? Because estimating the value function for all belief states is typically computationally infeasible for non-small POMDPs, given that the belief-space MDP (i.e. an MDP where the state space consists of probability distributions over the original states of the POMDP) of a POMDP with $n$ states has an uncountably large state space. Why? Because of the involved probability distributions.
How do we compute the value for the belief states that do not correspond to a point of the grid? We can use e.g. interpolation, i.e. the value of a belief state that does not correspond to a point of the grid is computed as a function of the value of the belief states that correspond to other grid points (typically, the neighboring grid points).
Why is this approach feasible? The assumption is that interpolation is not as expensive as computing the value of a belief state. However, note that you may not need to interpolate at every step of your algorithm, i.e. interpolation could be performed only when the value of a certain belief state is required.
How do you compute the value of a belief state that corresponds to a grid point? It can be computed with a value iteration (dynamic programming) algorithm for POMDPs. An overview of a value iteration algorithm can be found in section 2 of the paper Point-based value iteration: An anytime algorithm for POMDPs. Here's an example of the application of the value iteration algorithm for POMDPs.
The grid-based approach, introduced in Computationally Feasible Bounds for Partially Observed Markov Decision Processes (1991) by William S. Lovejoy, is very similar to the point-based approach, which was introduced in Point-based value iteration: An anytime algorithm for POMDPs. The main differences between the two approaches can be found in section 3 of Point-based value iteration: An anytime algorithm for POMDPs.
The idea of discretizing your problem or simply computing the desired value at a  subset of the domain has been applied in other contexts too. For example, in the context of computer vision, you can approximate the derivative (or gradient) of an image (which is thus considered a function) at discrete points of the domain (i.e. the pixels).
There's a Julia implementation of the first grid-based approximative solution to POMDP. There's also a Python implementation of the point-based approach. These implementations may help you to understand the details of these approaches. 
"
Does there exist a resource for vetting banned words for chatbots?,"
So, Tay the racist tweeter bot... one thing that could have prevented this would have been to have a list of watchwords to not respond to, with some logic similar to foreach (word in msg) {if (banned_words.has(word)) disregard()}.
Even if that wouldn't, what I'm getting at is obvious: I am building a chatterbot that must be kid-friendly. For my sake and for the sake of whoever finds this question, is there a resource consisting of a .csv or .txt of such words that one might want to handle? I remember once using a site-blocking productivity extension that had visible its list of banned words; not just sexually charged words, but racial slurs, too.
","['chat-bots', 'resource-request']",
Is this LSTM model underfitting?,"
I think this model is underfitting. Is this correct?     


_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (50, 60, 100)             42400     
_________________________________________________________________
dropout_1 (Dropout)          (50, 60, 100)             0         
_________________________________________________________________
lstm_2 (LSTM)                (50, 60)                  38640     
_________________________________________________________________
dropout_2 (Dropout)          (50, 60)                  0         
_________________________________________________________________
dense_1 (Dense)              (50, 20)                  1220      
_________________________________________________________________
dense_2 (Dense)              (50, 1)                   21        
=================================================================

The above is a summary of the model.
Any advice on how the model could be improved?
","['neural-networks', 'long-short-term-memory', 'overfitting']","
You need to include optimizer you used to make sure it is correct.
By the way, your drop-out layers are not going to do anything, so you should take them away. 
You likely don’t have test and train data in time-series because all data points are connected. It just has prediction value and ground truth of each period. 
I recommend you use the whole dataset and rotate changing hyper-parameters of LSTM to find the best model. 
"
How was the DQN trained to play many games?,"
Some people claim that DQN was used to play many Atari games. But what actually happened? Was DQN trained only once (with some data from all games) or was it trained separately for each game? What was common to all those games? Only the architecture of the RL agent? Did the reward function change for each game?
","['reinforcement-learning', 'dqn', 'deep-rl', 'rewards', 'atari-games']",
"In NEAT, is it a good idea to give the same ID to node genes created from the same connection gene?","
Do I have to prevent nodes created from the same connection gene to have different IDs/innovation number? In this example, the node 6 is created from the connection going from node 3 to node 4:

In the case where that specific node was already globally created, is it useful to give it the same ID for crossover? Because the goal of NEAT is to do meaningful crossover by doing historical marking. The paper from Kenneth O. Stanley says at page 108:

[...] by keeping a list of the innovations that occurred in the current generation, it
  is possible to ensure that when the same structure arises more than once through independent mutations in the same generation, each identical mutation is assigned the
  same innovation number.

Why don't we do that for node genes too?
","['neat', 'neuroevolution', 'crossover-operators']",
"In RL, if I assign the rewards for better positional play, the algorithm is learning nothing?","
I'm creating an RL application for the game Connect Four. 
If I tell the algorithm which moves/token positions will receive greater rewards, surely it's not actually learning anything; it's just a basic lookup for the algorithm? ""Shall I place the token here, or here? Well, this one receives a greater reward, so I choose this one.""
For example, some pseudocode:
function get_reward()
    if 2 in a line
        return 1
    if 3 in a line
        return 2
    if 4 in a line
        return 10
    else 
        return -1

foreach columns
    column_reward_i = get_reward(column_i)
    if column_reward_i >= column_rewards
        place_token(column_i)

","['reinforcement-learning', 'algorithm', 'rewards']","
What you are proposing is closer to a heuristic for searching than a reward for RL. This is a blurred line, but generally if you start analysing the problem yourself, breaking it down into components and feeding that knowledge into the algorithm, then you place more emphasis on your understanding of the problem, and less on any learning that an agent might do.
Typically in a RL formulation of a simple board game, you would choose rewards or +1 for a win (the goal), 0 for a draw, and -1 for a loss. All non-terminal states would score 0 reward. The point of the RL learning algorithm is that the learning process would assign some nominal value to interim states due to observing play. For value-based RL approaches, such as Q learning or Monte Carlo Control, the algorithm does this more or less directly by ""backing up"" rewards that it experiences in later states into average value estimates for earlier states.
Most game-playing agents will combine the learning process, which will be imperfect given the limited experience an agent can obtain compared to all possible board states, with a look-ahead search method. Your heuristic scores would also make a reasonable input to a search method too - the difference being you may need to search more deeply using your simple heuristic than if you used a learned heuristic. The simplest heuristic would just be +1 for a win, 0 for everything else, and is still reasonably effective for Connect 4 if you can make it search e.g. 10 moves ahead.
The combination of deep Q learning and negamax search is quite effective in Connect 4. It can make near perfect agents. However, if you actually want a perfect agent, you are better off skipping the self-learning approach and working on optimised look-ahead search with some depth of opening moves stored as data (because search is too expesnive in the early game, even for a simple game like Connect 4).
"
Do I need to denormalise results in linear regression?,"
I have learned so far how to linear regression with one or multiple features. So far, so good, everything seems to work fine, at least for my first simple examples.
However, I now need to normalise my features for training. I'm doing this by calculating the mean and the standard deviation per feature, and then calculate the normalised feature by subtracting the mean, taking the absolute value, and dividing by the standard deviation. Again, so far, so good, the results of my tensors which I use for training look good.
I understand why I need to normalise input data, and I also understand why one can do it like this (I know that there are other ways as well, e.g. to map values to a 0-1 interval).
Now I was wondering about two things:

First, after having trained my network, when I want to make a prediction for a specific input â€“ do I need to normalise this as well, or do I use the un-normalised data? Does it make a difference? My gut feeling says, I should normalise it, as it should make a difference, but I'm not sure. What should I do here, and why?
Second, either way, I get a result. Now I was wondering whether I need to denormalise this? I mean, it should make a difference, shouldn't it? If so, how? How do I get from the normalised result value to a denormalised one? Do I just need to reverse the calculation with mean and standard deviation, to get the actual value?

It would be great if someone could shed some light on this.
","['machine-learning', 'data-preprocessing', 'linear-regression', 'normalisation']",
What kind of artificial intelligence is this? A decentralized swarm intelligence where the input and output is split among the agents,"
I have an AI design for deciding the length of green and red lamps of the traffic. In my design, every crossroads has its own agent. This agent has input the amount of vehicle in each road in a single junction. AI then decide how long is the red lamp and the green lamp in each junction. The fitness function is the average commute time in the city. Each agent may communicate with each other, and give reward or punishment to other AI. What AI algorithm works like this?
","['reinforcement-learning', 'swarm-intelligence']",
"What does equation in the ""related work"" section of the GAN paper mean?","
I was going through the paper on GAN by Ian Goodfellow. Under the related work section, there is an equation. I cannot decipher the equation. Can anyone help me understand the meaning of the equation? 
$$\lim_{\sigma \to 0} \nabla_{\mathbf  x} \mathbb E_{\epsilon \sim \mathcal N(0, \sigma^2 \mathbf I)} f(\mathbf x+\epsilon) = \nabla_x f(\mathbf x)$$
Also, any guide to understanding mathematical notation for reading research paper is highly appreciated.
","['math', 'generative-adversarial-networks', 'papers', 'notation']",
Is maximum likelihood estimation meaningless for a dataset of only outliers?,"
From my understanding, maximum likelihood estimation chooses the set of parameters for the estimator that maximizes likelihood with the ground truth distribution.
I always interpreted it as the training set having a tendency to have most examples near the mean or the expected value of the true distribution. Since most training examples are close to the mean (since they have been sampled from this distribution) maximizing the estimator's chance of sampling these examples gets the estimated distribution close to the ground truth distribution. 
This would mean that any MLE procedure on a dataset of outliers should fail miserably. Are this interpretation and conclusion correct? If not, what is wrong with the mentioned interpretation of maximizing likelihood for an estimator?  
","['machine-learning', 'math', 'statistical-ai', 'cross-entropy', 'maximum-likelihood']",
"Do I have to crossover my node genes in NEAT, and how?","
I'm currently trying to code the NEAT algorithm by myself, but I got stuck with two questions. Here they are:
What happens if during crossover a node is removed (or disabled) and there's a connection that was previously connected to that specific node? Because, in that case, some connections are no longer useful. Do I keep the useless connections or do I prevent this from happening? Or maybe I'm missing something?
Someone on AI SE said that:

You could:
1.) Use only the connection genes in crossover, and derive your node genes from the connection genes
2.) Test if every node is in use, and delete the ones that are not

But the problem with that is that my genomes will lose some complexity. Maybe I can use the nodes during crossover, and then disable the connections that were using this node. That way, I'm keeping the genotype complex, but the phenotype is still working.
Is there another way to workaround this problem or this is the best way?
","['neat', 'neuroevolution', 'crossover-operators', 'mutation-operators']",
What is the relationship between the reward function and the value function?,"
To clarify it in my head, the value function calculates how 'good' it is to be in a certain state by summing all future (discounted) rewards, while the reward function is what the value function uses to 'generate' those rewards for it to use in the calculation of how 'good' it is to be in the state?
","['reinforcement-learning', 'comparison', 'value-functions', 'reward-functions']","
Sutton and Barto give a great description in their book Reinforcement Learning:

Whereas the reward signal indicates what is good in an immediate sense, a value
function specifies what is good in the long run. Roughly speaking, the value of a state is
the total amount of reward an agent can expect to accumulate over the future, starting
from that state. Whereas rewards determine the immediate, intrinsic desirability of
environmental states, values indicate the long-term desirability of states after taking into
account the states that are likely to follow and the rewards available in those states. For
example, a state might always yield a low immediate reward but still have a high value
because it is regularly followed by other states that yield high rewards. Or the reverse
could be true. To make a human analogy, rewards are somewhat like pleasure (if high)
and pain (if low), whereas values correspond to a more refined and farsighted judgment
of how pleased or displeased we are that our environment is in a particular state.

Mathematically, the value function is the expected sum of discounted rewards from a given state or for a particular action in a given state.
"
How can artificial intelligence predict the next possible moves of the player?,"
When you play video games, sometimes there is an AI that attempts to predict what are you going to do. 
For example, in the Candy Crush game, if you finish the level and you still have moves remaining, you get to see fishes or other powers destroying the other candies, but, instead of watching 10 minutes of your combos without moving at all after accomplishing a level, like this Longest video game combo ever probably, it tells an alert that says tap to skip, so basically the AI is predicting all the possible combos that will keep proceeding automatically and calculating every automatic move. 
How can artificial intelligence predict such a thing?
","['machine-learning', 'game-ai', 'prediction']","
AI is trained to predict such thing because that is their purpose, they are given almost all possibilities of move they can do to current state of the game and chose the best possible outcome of the possible move, but not only that the AI also predict what happens after that and predict the outcome of that prediction, just like a chess AI that can predict how to checkmate a player just by one move made by the player, so they did not just predict what move to do now but also what move to do after that move has been done
this can be done with deep learning as you can read here : https://towardsdatascience.com/predicting-professional-players-chess-moves-with-deep-learning-9de6e305109e
https://electronics.howstuffworks.com/chess1.htm
"
Does this prove AI Safety is undecidable?,"
Does this prove AI Safety is undecidable?
Proof:
Output meaning output to computer program.
[A1] Assume we have a program that decides which outputs are “safe”.
[A2] Assume we have an example of an unsafe output: “unsafe_output”
[A3] Assume we have an example of safe output: “safe_output”.
[A4] Define a program to be safe if it always produces safe output.
[A5] Assume we have a second program (safety_program) that decides which programs are safe.
[A6] Write the following program:
def h()
   h_is_safe := safety_program(h)
   if (h_is_safe):
      print unsafe_output
   else:
      print safe_output

Clearly h halts.
If the safety_program said h was safe, then h prints out unsafe_output.
If the safety_program said h was not safe, then h prints out safe_output.
Therefore safety_program doesn’t decide h correctly.
This is a contradiction. Therefore we made a wrong assumption: Either safe output cannot be decided, or safe programs cannot be decided.
Therefore, in general, the safety of computer programs, including Artificial Intelligence, is undecidable.
Therefore AI Safety is undecidable.
","['agi', 'ai-safety']","
In my opinion, there are several flaws in your proof and reasonings.
First, note that, in the case of Turing's proof, h will actually loop forever (i.e. not halt) when the oracle says that h halts. In this case, there's an actual contradiction, because h will do the opposite of what the oracle says. 
So, to follow Turing's proof, you would need to make h behave unsafely if the oracle says h is safe. But how should we define a safe or unsafe program? There are many unsafe behaviors. For example, in a certain context, an insult could be unsafe, in other contexts, a certain limb movement could be unsafe, and so on. So, an agent is unsafe or behaves unsafely usually with respect to another agent (or itself) or environment. You probably need to keep this in mind if you want to prove anything about the safety of AI agents. 
In your second assumption, you are implicitly saying that any machine that produces the output unsafe_output is unsafe, but, of course, this definition is not a realistic definition of an unsafe program. 
To help you define safety in a more reasonable and natural way, I think it may be useful to reason first in terms of artificial agents, which are higher-level concepts than Turing machines. Then you could find a way of mapping agents to TMs and attempt to prove your conjectures by using the tools of the theory of computation.
"
Should I be trying to create a generic or specific (to particular game) reinforcement learning agent?,"
I'm creating an RL application for the game Connect Four. 
In general, should I be aiming to create an application that's more generic, which would 'learn' different games, or specific to a particular game (e.g. Connect Four, by assigning greater rewards to certain token positions in the C4 grid)? 
Does the difference between the two approaches just come down to adapting their respective reward functions to reward specific achievements or positions (in a board game setting), or something else?
","['reinforcement-learning', 'ai-design', 'game-ai']","
If what you mean by a generic reinforcement-learning application is an application that can learn any game (or some games), then you can't do it. Why? Because the goal of each game isn't the same, so you have to adapt the rewards depending on the game. If you just want to make an AI for Connect Four, I suggest you to make a specific RL application for that game.
I want to mention another thing: you shouldn't give a reward based on token's position, because it's hard to know what token's position is the best. Instead, just assign a big reward to the winner. That way, you're generalizing your algorithm, and you're avoiding your AI to focus on the wrong goal.
You have to be careful, you don't know for sure what token's position is the best. Try to give a reward ONLY when a player wins the game, because you can know for sure that winning is the best thing to do. It may sound idiot said like that, but that way, you're ensuring you AI to learn by itself what token's position is the best.
"
How should I define the reward function for the Connect Four game?,"
I'm creating an RL application for the game Connect Four. I've researched the different strategies for the game and which positions are more favourable to lead to a win. 
Should I be assigning greater rewards when the application places a token in those particular positions? If so, that would mean the application/algorithm is a Connect Four specific application, and not generic? 
","['reinforcement-learning', 'ai-design', 'markov-decision-process', 'rewards']",
Why can the core reinforcement learning algorithms be applied to POMDPs?,"
Why can an AI, like AlphaStar, work in StarCraft, although the environment is only partially observable? As far as I know, there are no theoretical results on RL in the POMDP environment, but it appears the core RL techniques are being used in partially observable domains. 
","['reinforcement-learning', 'markov-decision-process', 'pomdp']",
Why is there an expectation sign in the Bellman equation?,"
In chapter 3.5 of Sutton's book, the value function is defined as:

Can someone give me some clarification about why there is the expectation sign behind the entire equation? Considering that the agent is following a fixed policy $\pi$, why there should be an expectation when the trajectory of the future possible states is fixed (or maybe I am getting it wrong and it's not). In total, if the expectation here has the meaning of averaging over a series of trajectories, what are those trajectories and what are the weights of them when we want to compute the expected value over them according to this Wikipedia definition of the expected values?
","['reinforcement-learning', 'value-functions', 'expectation', 'bellman-equations']","
In addition to this answer, I would like to note that, if the future trajectories were fixed (i.e. the environment and the policies were deterministic, and the agent always starts from the same state), the expectation of the sum (of the fixed rewards) would simply correspond to the actual sum, because the sum is a constant (i.e. the expectation of a constant is the constant itself), so the expectation operator also applies to the deterministic cases. Therefore, the expectation is a general way of expressing the value of a state in all possible cases (both when trajectories are fixed or not).
"
How can I find the appropriate reward value for my reinforcement learning problem?,"
I am wondering how can I find the appropriate reward value for each specific problem. I know this is a highly empirical process, but I am sure that the value is not set totally at random. I want to know what are the general guidelines and practices to find the appropriate reward value for any reinforcement learning problem.
","['reinforcement-learning', 'ai-design', 'rewards']",
Is the TD-residual defined for timesteps $t$ past the length of the episode?,"
Let $\mathcal{S}$ be the state-space in a reinforcement learning problem where rewards are in $\mathbb{R}$, and let $V:\mathcal{S} \to \mathbb{R}$ be an approximate value function. Following the GAE paper, the TD-residual with discount $\gamma \in [0,1]$ is defined as $\delta_t^V = r_t + \gamma V(s_{t + 1}) - V(s_t)$.
I am confused by the formula for the GAE-$\lambda$ advantage estimator, which is
$$
\hat{A}_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l = 0}^\infty (\gamma \lambda)^l \delta_{t + l}^V.
$$
This seems to imply that $\delta_t^V$ is defined for $t > N$, where $N$ is the length of the current trajectory/episode. It looks like in implementations of this advantage estimator, it is just assumed that $\delta_t^V = 0$ for $t > N$, since the sums are finite. Is there a justification for this assumption? Or am I missing something here? 
","['reinforcement-learning', 'policy-gradients', 'papers', 'temporal-difference-methods']",
What is the role of convex optimisation in AI systems?,"
Convex optimisation is defined as:

I have seen a lot of talk about convex loss functions in Neural Networks and how we are optimising rewards or penalty in AI/ML systems. But I have never seen any loss function formulated in the aforementioned way. So my question is:
Is there any role of convex optimization in AI? If so, in what algorithms or problem settings or systems?
","['machine-learning', 'optimization']","

Is there any role of convex optimization in AI? 

Yes, of course!

If so, in what algorithms or problem settings or systems?

The problem of finding the parameters of a support vector machine can be formulated as a convex optimization problem. Another example is linear regression.
See also the paper Convex Optimization: Algorithms and Complexity (2014) by Sébastien Bubeck, which also mentions SVM as a typical example.
"
How can I train a neural network if I don't have enough data?,"
I have created a neural network that is able to recognize images with the numbers 1-5. The issue is that I have a database of 16x5 images which ,unfortunately, is not proving enough as the neural network fails in the test set. Are there ways to improve a neural network's performance without using more data? The ANN has approximately a 90% accuracy on the training sets and a 50% accuracy in the test ones.
Code:
clear
graphics_toolkit(""gnuplot"")
sigmoid = @(z) 1./(1 + exp(-z));
sig_der = @(y) sigmoid(y).*(1-sigmoid(y));


parse_image;   % This external f(x) loads the images so that they can be read. 
%13x14
num=0;
for i=1:166
  if mod(i-1,10)<=5 && mod(i-1,10) > 0
    num=num+1;
    data(:,num) = dlmread(strcat(""/tmp/"",num2str(i)))(:);
  end
end



function [cost, mid_layer, last_layer] = forward(w1,w2,data,sigmoid,i)
  mid_layer(:,1)=sum(w1.*data(:,i));
  mid_layer(:,2)=sigmoid(mid_layer(:,1));
  last_layer(:,1)=sum(mid_layer(:,2).*w2);
  last_layer(:,2)=sigmoid(last_layer(:,1));
  exp_res=rem(i,5);
  if exp_res==0
    exp_res=5;
  end
  exp_result=zeros(5,1); exp_result(exp_res)=1;
  cost = exp_result-last_layer(:,2);
end

function [w1, w2] = backprop(w1,w2,mid_layer,last_layer,data,cost,sig_der,sigmoid,i)
  delta(1:5) = cost;
  delta(6:20) = sum(cost' .* w2,2);
  w2 = w2 + 0.05 .* delta(1:5) .* mid_layer(:,2) .* sig_der(last_layer(:,1))';
  w1 = w1 + 0.05 .* delta(6:20) .* sig_der(mid_layer(:,1))' .* data(:,i);
end

w1=rand(182,15)./2.*(rand(182,15).*-2+1);
w2=rand(15,5)./2.*(rand(15,5).*-2+1);

for j=1:10000
  for i=[randperm(85)]
    [cost, mid_layer, last_layer] = forward(w1,w2,data,sigmoid,i);
    [w1, w2] = backprop(w1,w2,mid_layer,last_layer,data,cost,sig_der,sigmoid,i);
    cost_mem(j,i,:)=cost;
  end
end

","['neural-networks', 'backpropagation', 'optimization', 'matlab']","
In theory, yes, using synthetic data generation. This involves applying transformations to the original images to generate new 'unique' images. Some standard techniques include rotating, flipping, stretching, zooming or brightening. Obviously not all of these make sense depending on the data. In your problem, zooming, stretching and brightening could be used but flipping should not. Rotation could work but only for small angles.
Generally this is implemented by replacing the dataset for each epoch of training. Therefore, the number of images used in each training iteration is the same but the images themselves have been altered.
In practice, it's not a magic bullet. The reason a larger dataset generally yields better models is because the probability of a new feature falling within the feature distribution of the training data is higher. With synthetic data generation the new features are only marginally different to the original so even if the number of images to train on is increased, the feature distributions are not that different. There is a lot of variation in handwritten numbers so it would be very hard to guess how effective this would be without trying it.
"
Is the derivative of the loss wrt a single scalar parameter proportional to the loss?,"
I am wondering about the correlation between the loss and the derivative of the loss wrt a single scalar parameter, with the same sample. That means: considering a machine learning model with parameters $\theta \in R$, I want to figure out the relationship between $Loss(x)$ and $\frac{\partial Loss(x)}{\partial \theta_i}$, where $i \in \{1,2,3,...,n\}$
Intuitively, I would like to consider that they are in a positive correlation, is it right? If it is right, how can I prove it in a mathematical way? 
","['machine-learning', 'deep-learning', 'math', 'proofs']","
The derivative $f'(x)$ is correlated with $f(x)$ in a certain sense. In fact, $f'(x)$ is a function of $f$, so we could even say that there's a cause-effect relationship.
The derivative at a specific point $c$ of the domain, i.e. $f'(c)$, can either be negative or positive. If $f'(c) > 0$, then $f(c)$ is increasing (with respect to an increase of $x$). If $f'(c) < 0$, then $f(c)$ decreasing  (with respect to an increase of $x$). 
This can easily be seen from an example. Consider $f(x) = x^2$, then $f'(x) = 2x$. Let $c = 2$, then $f'(2) = 4$, so the function is increasing. In fact, $f(1) = 2 \leq f(2) = 4 \leq f(3) = 9$. Similarly, let $c = -1$, then $f'(-1) = -2$, so the function is decreasing. In fact, $\leq f(-2) = 4 \geq f(-1) = 1 \geq  f(0) = 0$ (note that the function is decreasing as we increase $x$!). 
Consider a model with only one parameter, then the partial derivative of the loss function with respect to that parameter corresponds to the derivative of the loss function. So, the reasoning above applies to this model. What about a model with more than one parameter? The same thing happens. 
If the function decreases, does its derivative also decrease? In general, no, and this can easily be seen from a plot of a function and its derivative. For example, consider a plot of a parabola and its derivative (which is a linear function).

On the left of the y-axis, the parabola is decreasing, but its derivative is increasing, while, on the right of the y-axis, the parabola is increasing and the linear function is still increasing.
This is the same thing with a loss function of an ML model and its partial derivative.
"
Should I always start from the same start state in reinforcement learning?,"
In an episodic training of an RL agent, should I always start from the same initial state or I can start from several valid initial states?
For example, in a gym environment, should my env.reset() function always resets me to the same start state or it can start from different states at each training episode?
","['reinforcement-learning', 'training', 'gym']","
It is your choice.
This can even be different between training and target system. The approach called ""exploring starts"" chooses a random start state (and action if you are assessing a deterministic policy for action values).
In general, if you don't have a reason to pick exploring starts, you should aim for your env.reset() function to put the environment into a state drawn from the distribution of start states that you expect the agent to encounter in production. This will help if you are using function approximation - it will mean that the distribution of training data will better match the distribution seen in production, and approximators can be sensitive to that.
In some cases, such as policy-gradient methods, your cost function will be defined in terms of expected return given a start state distribution, so at least during assessment you will want a env.reset() function that matches that target start distribution. 
It is still OK to have different distributions for start states for training and assessment, and might be worth investigating as a hyperparameter for training. For instance if the training start state distribution can pick states that are hard to get to randomly otherwise, it may help the agent to find any of those states that are high value.
"
Is this a good approach to evaluate the game state with a neural network?,"
I've written a Monte Carlo Tree Search player for the game of Castle (AKA Shithead, Shed, Palace...). I have set this MCTS player to play against a basic rule-based AI for ~30000 games and collected ~1.5 million game states (as of now) along with whether the MCTS player won that particular game in the end after being in that particular game state. The game has a large chance aspect, and, currently, the MCTS player is winning ~55% of games. I want to see how high I can push it. In order to do this, I aim to produce a NN that will act as a game state evaluation function to use within the MCTS.
With this information, I've already tried an SVM, but came to the conclusion that the game space is too large for the SVM to classify a given state accurately.
I hope to be able to train a NN to evaluate a given state and return how good that state is for the MCTS player. Either with a binary GOOD/BAD or I think it would be more helpful to return a value between 0-1.
The input to the NN is a $4 \times 41$ NumPy array of binary values (0, 1) representing the MCTS players hand, MCTS face-up cards, OP face-up cards, MCTS no. face-down cards, OP no. face-down cards. Shown below.

Describes the np.array:

The np.array is made from the database entries of game states. An example of this information is below. However, I am currently omitting the TOP & DECK_EMPTY columns in this model. WON (0, 1) is used as the label.

This is my keras code:
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))

model.compile(optimizer='adam',
             loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])

model.fit(X_train, y_train, epochs=3)

This model isn't performing.

Do you think it is possible to obtain a useful NN with my current approach?

What layers should I look to add to the NN?

Can you recommend any training/learning material that I could use to try and get a better understanding?


","['neural-networks', 'convolutional-neural-networks', 'game-ai', 'monte-carlo-tree-search', 'evaluation-functions']",
Efficient algorithm to obtain near optimal policies for an MDP,"
Given a discrete, finite Markov Decision Process (MDP) with its usual parameters $(S, A, T, R, \gamma)$, it is possible to obtain the optimal policy $\pi^{*}$ and the optimal value function $V^{*}$ through one of many planning methods (policy iteration, value iteration or solving a linear program).
I am interested in obtaining a random near-optimal policy $\pi$, with the value function associated with the policy given by $V^{\pi}$, such that
$$ \epsilon_1 < ||V^{*} - V^{\pi}||_{\infty} < \epsilon_2$$
I wish to know an efficient way of achieving this goal. A possible approach is to generate random policies and then to use the given MDP model to evaluate these policies and verify that they satisfy the criteria.
If only an upper bound were needed, the idea that near optimal value functions induce near optimal policies could be used, that is, we can show that, if
$$||V - V^{*}||_{\infty} < \epsilon, \quad \epsilon > 0$$
and if $\pi$ is the policy that is greedy with respect to the value function $V$, then
$$ ||V^{\pi} - V^{*}||_{\infty} < \frac{2\gamma\epsilon}{1 - \gamma}$$
So by picking a suitable $\epsilon$ for the given $\gamma$, we can be sure of any upper bound $\epsilon_2$. 
However, I would also like that the policy $\pi$ not be ""too good"", hence the requirement for a lower bound. 
Any inputs regarding an efficient solution or reasons for the lack thereof are welcome.
","['reinforcement-learning', 'markov-decision-process']",
What are the most common non-Markov RL paradigms?,"
I am interested in doing model-free RL but not using the Markov assumptions typical for MDPs or POMDPs. 
What are alternative paradigms that don't rely on the Markov assumptions? Are there any common approaches when this assumption is violated? 
EDIT: I am asking for mathematical models that do not make the Markov assumption and so could be used for problems where the Markov assumption does not hold
","['reinforcement-learning', 'markov-decision-process', 'markov-property']",
What is the difference between using dense layers as opposed to convolutional layers in my networks when dealing with images?,"
I am thinking about developing a GAN. 
What is the difference between using dense layers as opposed to convolutional layers in my networks when dealing with images?
","['neural-networks', 'convolutional-neural-networks', 'comparison', 'generative-adversarial-networks']",
Does Rice's theorem prove safe AI is undecidable?,"
According to Wikipedia

In computability theory, Rice's theorem states that all non-trivial,
  semantic properties of programs are undecidable. A semantic property
  is one about the program's behavior (for instance, does the program
  terminate for all inputs), unlike a syntactic property (for instance,
  does the program contain an if-then-else statement). A property is
  non-trivial if it is neither true for every computable function, nor
  false for every computable function.

A syntactic property asks a question about a computer program like ""is there is a while loop?""
A semantic properties asks a question about the behavior of the computer program. For example, does the program loop forever (which is the Halting problem, which is undecidable, i.e., in general, there's no algorithm that can tell you if an arbitrarily given program halts or terminates for a given input)? 
So, Rice's theorem proves all non-trivial semantic properties are undecidable (including whether or not the program loops forever).
AI is a computer program (or computer programs). These program(s), like all computer programs, can be modeled by a Turing machine (Church-Turing thesis).
Is safety (for Turing machines, including AI) a non-trivial semantic question? If so, is AI safety undecidable? In other words, can we determine whether an AI program (or agent) is safe or not?
I believe that this doesn't require formally defining safety.
","['agi', 'proofs', 'ai-safety', 'theory-of-computation']","

Every program halts, or continues

Given N steps, enough time and space (*), halting within N steps is provable


3 (from 2). Halting always has proof: run program until halt; count number of steps; verify claim of halting (within number of steps)

(Program is safe) implies (program is proved safe)

(Safety proved) implies (public understands the proof)

(Program is safe) implies (program always halts (safely) or continues (safely)) [from 1, 6] and ((public) understands (safety proof)) [from 7]

(Public not understand claimed safety proof this moment) implies (don't run program this moment) [common sense]


(*) This universe is finite. Some numbers too big to be computed in this universe
Have you seen perfect software?
Have you seen software make mistakes?
Why trust life-death decisions to software?
Why trust government decisions to software?
Why trust business decisions to software?
If scientists may not recognize AI is intelligent, what if you don't recognize something beyond AI in front of you?
(After enough doubt, all you can do is TRUST)
"
What are some good alternatives to U-Net for biomedical image segmentation?,"
Soon I will be working on biomedical image segmentation (microscopy images). There will be a small amount of data (a few dozens at best). 
Is there a neural network, that can compete with U-Net, in this case? 
I've spent the last few hours searching through scientific articles that are dealing with this topic, but haven't found a clear answer and I would like to know what other possibilities are. The best answers I found are that I could consider using ResU-Net (R2U-Net), SegNet, X-Net and backing techniques (article).
Any ideas (with evidence, not necessarily)?
","['neural-networks', 'convolutional-neural-networks', 'reference-request', 'image-segmentation', 'u-net']","
Hey i am working on my Bachelor thesis at the moment and use UNET in combination with a GAN for image segmentation. I spend the last 5 months on that, so on my tests, the new approach of januar 2020, called Multires-UNET is quite a good choice for more texture orientated segmentation. I use the current github implementation. Its quite nice, maybe you notice that you can easyly tweak the number of parameters with ""alpha"" in the implementation, to scale the multiple Resnets in the Unet structure. 
I tried also other segmentation networks like Mask_RCNN with different backbones or tried to construct various types of CAE on my own, but always had to come back to a UNET like structure. Same goes for ResU-NET and R2U, the multires one worked better for my purposes cause i didnt need any kind of LSTM modules.
Some examples which may clarify the difference in performance on a specific task:
Original Image:

Ground-Truth:

Classical UNET++ (Unet with skip-connections)(2.5 million parameters) more parameters (wide) didnt change the result.

Multires-UNET (alpha=1.67, think was about 7 million parameters)

Can you show some of ur microscopy images, how complex is ur task and what do you want to segment ?
"
What are the current research trends in recognizing narrative similarity?,"
I am currently working on a term paper on the topic of Narrative Similarity, based on Loizos Michael's work ""Similarity of Narratives"". I am trying to find the latest trends within this field of study for the literature overview in my assignement. However up untill now I wasn't able to find any new work on this particular subject. 
I would appreciate any literature recommendation by anyone out there that has worked on this topic or is currently doing so.
Link to Michael's work:
https://www.researchgate.net/publication/316280937_Similarity_of_Narratives
","['machine-learning', 'natural-language-processing', 'natural-language-understanding']",
How should I define the loss function when using DQN to estimate the probability density?,"
I'm doing a Deep Q-learning project. All of my rewards are positive and there are two terminal states. One of them has a zero reward and the other has a high positive reward. 
The rewards are stochastic and my Q-network must generate non-zero Q-values for all states and actions. Based on my project, I must use these numbers to create a probability density. In other words, the normalized Q-values of each state generated by network define a probability density for choosing an action.
How should I define my loss function? Is there a project or paper which I could look at and decide how to define the loss function? I am searching for similar projects and their proposed Deep Q-learning algorithms.
","['reinforcement-learning', 'objective-functions', 'dqn', 'deep-rl']",
Can this be a possible deep q learning pseudocode?,"
I  am not using replay here. Can this be a possible deep q learning pseudocode?
s - state    
a - action    
r - reward
n_s - next state
q_net - neural network representing q

step()
{

    get s,a,r,n_s
    q_target[s,a]=r+gamma*max(q_net[n_s,:])
    loss=mse(q_target,q_net[s,a])
    loss.backprop()

}

while(!terminal)
{    
    totalReturn+=step();
}

","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'pseudocode']",
What assumptions are made when positing the emergence of superintelligence?,"
Many experts seem to think that artificial general intelligence, or AGI, (on the level of humans) is possible and likely to emerge in the near-ish future. Some make the further step to say that superintelligence (much above the level of AGI) will appear soon after, through mechanisms like recursive self-improvement from AGI (from a survey).
However, other sources say that such superintelligence is unlikely or impossible (example, example).
What assumptions do those who believe in superintelligence make? The emergence of superintelligence has generally been regarded as something low-probability but possible (e.g. here). However, I can't seem to find an in-depth analysis of what assumptions are made when positing the emergence of superintelligence. What specific assumptions do those who believe in the emergence of superintelligence make that are unlikely, and what have those who believe in the guaranteed emergence of superintelligence gotten wrong?
If the emergence of superintelligence is to be seen as a low-probability event in the future (on par with asteroid strikes, etc.), which seems to be the dominant view and is the most plausible, what assumptions exactly makes it low-probability?
","['philosophy', 'agi', 'superintelligence']",
What does it mean to have epochs=30 in Keras' fit method given certain data?,"
I have read a lot of information about several notions, like batch_size, epochs, iterations, but because of explanation was without numerical examples and I am not native speaker, I have some kind of problem of understanding still about those terms, so I decided to work with data. Let us suppose we have the following data

Of course, it is just subset of original data, and I want to build a neural network with three hidden layers, the first layer contains 500 nodes, it takes input three variable and on each node, there is sigmoid activation function, next layer contains 100 node and sigmoid activation, the third one contains 50 node and sigmoid again, and finally we have one output with sigmoid to convert the result into 0 or 1 that classify whether a person with those attributes is female or male.
I trained the model using Keras Tensorflow with the following code
model.fit(X_train,y_train,epochs=30)

With this data, what does mean epochs=30? Does it mean that all 177 rows  (with 3 input at times) will go to the model 30 times? What about batch_size=None in model.fit parameters? 
","['neural-networks', 'deep-learning', 'training', 'keras', 'datasets']","
ok  so let me explain in my word how i  understood this process:
i know that one sample mean one row, therefore if we have data with size(177,3), that means  we have 177 sample. because we have divided  X and y into training and test, therefore we have following pairs (X_train,y_train)  and (X_test, y_test) 
now about batch size, if we have  let say 177 sample(177 row) and  2 batch_size , that means we have approximately  $177/2$ batch right?update process goes  like this: 
let us suppose network takes 3 input and produce one output, from first sample of data, three data will go to the network and  output will be  generated, this output will be compared to the  first value of y_train and  cost function will be created, then next  sample will go(it means next three value) and compared to the second value of y_train, also second cost function will be generated, final cost function for first batch will be sum of those cost functions and using gradient method weights are updated, after that one new batches will go through the network and on the based on updated weights, new weights are generated, when all  $177/2$  batch will be finished , it will be our 1 epoch right? is that correct? 
"
Can I use deepfake to rotoscope for animation?,"
I'm a big fan of animation and have kept an eye on the deepfake's ability to replicate full body motion. 
So I ask

Is there a deepfake software available I can use to gather animation from a video?
Are there publically released a deepfake body tracking tools out there? Even for normal video

","['game-ai', 'deepfakes']",
How much time does it take to train DQN on Atari environment?,"
I am trying to build a DQN model for the Atari Pong game, but I am not sure whether the model is learning at all.
I am using the architecture described in the paper Playing Atari with Deep Reinforcement Learning. And I tested the model on a simpler environment (like CartPole), which worked great, but I am not seeing any progress at all with Pong, I have been training the model for 2-3 hours and its performance is no better than taking random actions.
Should I just keep waiting or there might be something wrong with my code. Around how many episodes should it take before I see some positive results?
","['reinforcement-learning', 'dqn', 'deep-rl', 'implementation', 'computational-complexity']",
Do I have to downsample the input and upsample the output of the neural network when implementing the NICE algorithm?,"
Consider that my input is an RGB image. The size of my image is $N\times N$. I'm trying to implement NICE algorithm presented by Dinh. The bijective function $f: \mathbb{R}^d \to \mathbb{R}^d$ maps $X$ to $Z$. So I have $p_Z(Z)=p_X(X)$. 
What I can't understand is that $N$ is much bigger than $d$. Does this mean that I should downsample the inputs? Does the resulting loss function change if I add a downsampling layer at the beginning of the neural net and also add an upsampling layer at the end of the net?  
","['neural-networks', 'machine-learning', 'generative-model', 'papers', 'data-preprocessing']",
What is relation between gradient descent and regularization in deep learning?,"
Gradient descent is used to reduce the loss and regularization is used to fight over-fitting. 
Is there any relation between gradient descent and regularization, or both are independent of each other?
","['deep-learning', 'comparison', 'gradient-descent', 'regularization', 'hyper-parameters']",
XOR-solving neural network is suffering from local minima,"
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import matplotlib.pyplot as plt
import numpy as np
x_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).float()
y_data = torch.tensor([0, 1, 1, 0]).float()

class Model(nn.Module):
    def __init__(self, input_size, H1, output_size):
        super().__init__()
        self.linear_input = nn.Linear(input_size, 2)
        self.linear_output = nn.Linear(2, output_size)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.sigmoid(self.linear_input(x))
        x = self.sigmoid(self.linear_output(x))
        return x

    def predict(self, x):
        return (self.forward(x) >= 0.5).float()

model = Model(2,2,1)
lossfunc = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
epochs = 2000
losses = []

for i in range(epochs):
    pred = model(x_data).view(-1)
    loss = lossfunc(pred, y_data)
    print(""epochs:"", i, ""loss:"", loss.item())
    losses.append(loss.item())
    optimizer.zero_grad()
    loss.backward() 
    optimizer.step()

def cal_score(X, y):
    y_pred = model.predict(X)
    score = float(torch.sum(y_pred.squeeze(-1) == y.byte().float())) / y.shape[0]
    return score

print('test score :', cal_score(x_data, y_data))
def plot_decision_boundray(X):
    x_span = np.linspace(min(X[:, 0]), max(X[:, 0]))
    y_span = np.linspace(min(X[:, 1]), max(X[:, 1]))
    xx, yy = np.meshgrid(x_span, y_span)
    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()]).float()
    pred_func = model.forward(grid)
    z = pred_func.view(xx.shape).detach().numpy()
    plt.contourf(xx, yy, z)
    plt.show()

plot_decision_boundray(x_data)

As you can see, it's a simple neural network which consists of one hidden layer using BCELoss and Adam.

Normally, it results in the correct one like above. 

However, it is sometimes stuck in a local minima and a decision boundary becomes awkward.
Because the input data is limited, I guess that preprocessing of those data might not be possible and only initial weights matter in this problem. I tried initializing them with normal distribution but it didn't work. How can I approach this problem?
",['pytorch'],
Which work originally introduced gradient clipping?,"
The Deep Learning book mentions that it's been used for years but the oldest sources it mentions are from 2012: 

A simple type of solution has been in use by practitioners for many years: clipping the gradient. There are diﬀerent instances of this idea (Mikolov, 2012; Pascanu et al., 2013). One option is to clip the parameter gradient from a mini-batch element-wise (Mikolov, 2012), just before the parameter update. Another is to clip the $||g||$ of the gradient $g$ (Pascanu et al., 2013) just before the parameter update  

But I find it hard to believe that the first uses and mentions of gradient clipping are from 2012. Does anyone know the origins of the solution? 
","['reference-request', 'papers', 'history', 'exploding-gradient-problem']",
Atari Breakout Infrastructure,"

This is how they describe their infrastructure in https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf. I want to implement the game of Atari Breakout.
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class DQN(nn.Module):
    def __init__(self, height, width):
        super(DQN, self).__init__()

        self.height = height
        self.width = width

        self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=32, kernel_size=4, stride=2)

        self.fc = nn.Linear(in_features=????, out_features=256)
        self.out = nn.Linear(in_features=256, out_features=4)

    def forward(self, state):

        # (1) Hidden Conv. Layer
        self.layer1 = F.relu(self.conv1(state))

        #(2) Hidden Conv. Layer
        self.layer2 = F.relu(self.conv2(self.layer1))

        #(3) Hidden Linear Layer
        self.layer3 = self.fc(self.layer2)

        #(4) Output
        actions = self.out(self.layer3)

        return actions

I will probably instantiate my policy network and my target network the following way : 
policy_net = DQN(envmanager.get_height(), envmanager.get_width()).to(device)
target_net = DQN(envmanager.get_height(), envmanager.get_width()).to(device)

I am very new in the world of Reinforcement Learning. I would like to implement their infrastructure in the DQN(), but I think I am wrong in several places. Am I good here? If not, how can I fix it so that it reflect the infrastructure from the above picture?
UPDATE
I know that the formula to calculate the output size is equal to
$O=\frac{W−K+2P}{S}+1$
where $O$ is the output height/length, $W$ is the input height/length, $K$ is the filter size, $P$ is the padding, and $S$ is the stride.
I obtained forself.fc = nn.Linear(in_features=????, out_features=256) that in_features must be equal to $32*9*9$
","['neural-networks', 'reinforcement-learning', 'dqn']",
Will structured knowledge bases continue to be used in question answering with the likes of BERT gaining popularity?,"
This may come across as an open and opinion-based question, I definitely want to hear expert opinions on the subject, but I am also looking for references to materials that I can read deeply.
One of the ways question answering systems can be classified is by the type of data source that they use:

Structured knowledge bases with ontologies (DBPedia, WikiData, Yago, etc.).

Unstructured text corpora that contain the answer in natural language (Wikipedia).

Hybrid systems that search for candidate answers in both structured and unstructured data sources.


From my reading, it appears as though structured knowledge bases/knowledge graphs were much more popular back in the days of the semantic web and when the first personal assistants (Siri, Alexa, Google Assistant) came onto the scene.
Are they dying out now in favor of training a deep learning model over a vast text corpus like Bert and/or Meena? Do they have a future in question answering?
","['reference-request', 'datasets', 'bert', 'question-answering', 'knowledge-base']",
What are examples of commonly used feature and readout maps?,"
It is well-known that deep feedforward networks can approximate any continuous function from $\mathbb{R}^k$ to $\mathbb{R}^l$, (uniformly on compacts).  
However, in practice feature maps are typically used to improve the learning quality and likewise, readout maps are used to make neural networks suited for specific learning tasks.  
For example:

Classification: networks are composed with the softmax (readout) function so they take values in $(0,1)^l$.

What are examples of commonly used feature and readout maps?
","['neural-networks', 'deep-learning', 'feature-selection', 'feature-extraction']",
Different methods of calculating gradients of cost function(loss function),"
We require to find the gradient of loss function(cost function) w.r.t to the weights to use optimization methods such as SGD or gradient descent. So far, I have come across two ways to compute the gradient:

BackPropagation
Calculating gradient of loss function by calculus

I found many resources for understanding backpropagation. 
The 2nd method I am referring to is the image below(taken for a specific example, e is the error: difference between target and prediction):
 
Also, the proof was mentioned in this paper:here
Moreover, I found this method while reading this blog.(You might have to scroll down to see the code: gradient = X.T.dot(error) / X.shape[0]  ) 
My question is are the two methods of finding gradient of cost function same? It appears different and if yes, which one is more efficient( though one can guess it is backpropagation)
Would be grateful for any help. Thanks for being patient(it's my 1st time learning ML). 
","['backpropagation', 'gradient-descent']",
Are there any commonly used discontinuous activation functions?,"
Are there any commonly used activation functions (e.g. that take values in $(0,.5)\cup (.5,1)$)? Preferably for classification?
Why? I was looking for commonly used activation functions on Google, and I noticed that all activation functions are continuous. However, I believe this is not needed in Hornik's paper.
When I did a bit of testing myself, with a discontinuous activation function on the MNIST dataset, the results were good. So I was curious if anyone else used this kind of activation function.  
","['deep-learning', 'deep-neural-networks', 'activation-functions']",
What is the difference between training a model with RGB images and using only the color channels separately?,"
What is the difference between training a model with RGB images and using only the color channels separately (like only the red channel, green channel, etc.)? Would the model also learn patterns between the different colors in the first case? 
If for me the single-channel results are relevant but also the patterns between different channels are relevant, it would be beneficial to use them together?
I am asking this because I want to apply this to signals of an accelerometer that has x, y, z-axis data. And I want to increase the resolution of the data. Will the model learn to combine all features from different axis if I input (1024, 3) length, channels of a one-dimensional signal into my one-dimensional CNN? 
","['machine-learning', 'convolutional-neural-networks', 'comparison', 'data-preprocessing']",
How to add a dense layer after a 2d convolutional layer in a convolutional autoencoder?,"
I am trying to implement a convolutional autoencoder with a dense layer at the bottleneck to do some dimensional reduction. I have seen two approaches for this, which aren't particularly scalable. The first was to introduce 2 dense layers (one at the bottleneck and one before & after that has the same number of nodes as the conv2d layer that precedes the dense layer in the encoder section:
input_image_shape=(200,200,3)
encoding_dims = 20

encoder = Sequential()
encoder.add(InputLayer(input_image_shape))
encoder.add(Conv2D(32, (3,3), activation=""relu, padding=""same""))
encoder.add(MaxPooling2D((2), padding=""same""))
encoder.add(Flatten())
encoder.add(Dense(32*100*100, activation=""relu""))
encoder.add(Dense(encoding_dims, activation=""relu""))

#The decoder
decoder = Sequential()
decoder.add(InputLayer((encoding_dims,)))
decoder.add(Dense(32*100*100, activation=""relu""))
decoder.add(Reshape((100, 100, 32)))
decoder.add(UpSampling2D(2))
decoder.add(Conv2D(3, (3,3), activation=""sigmoid"", padding=""same""))

It's easy to see why this approach blows up as there are two densely connected layers with (32100100) nodes each or more or in that ballpark which is nuts.
Another approach I have found which makes sense for b/w images such as the MNIST stuff is to introduce an arbitrary number of encoding dimensions and reshape it (https://medium.com/analytics-vidhya/building-a-convolutional-autoencoder-using-keras-using-conv2dtranspose-ca403c8d144e). The following chunk of code is copied from the link, I claim no credit for it:
#ENCODER
inp = Input((28, 28,1))
e = Conv2D(32, (3, 3), activation='relu')(inp)
e = MaxPooling2D((2, 2))(e)
e = Conv2D(64, (3, 3), activation='relu')(e)
e = MaxPooling2D((2, 2))(e)
e = Conv2D(64, (3, 3), activation='relu')(e)
l = Flatten()(e)
l = Dense(49, activation='softmax')(l)
#DECODER
d = Reshape((7,7,1))(l)
d = Conv2DTranspose(64,(3, 3), strides=2, activation='relu', padding='same')(d)
d = BatchNormalization()(d)
d = Conv2DTranspose(64,(3, 3), strides=2, activation='relu', padding='same')(d)
d = BatchNormalization()(d)
d = Conv2DTranspose(32,(3, 3), activation='relu', padding='same')(d)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(d)

So, is there a more rigorous way of adding a dense layer after a 2d convolutional layer?
","['convolutional-neural-networks', 'keras', 'autoencoders', 'convolutional-layers', 'dense-layers']","
For me, this worked perfectly. I encoded with conv2d and dense and then I flatten I and reshape in the decoder after the dense layer so the encoder and decoder are symmetrical. The only difference is that in my case I use images (224,224,1)
# create encoder
# 28,28 -> 1st conv2d (filter 3x3,relu activation, padding, strides == 'jumps')
self.encoder = tf.keras.Sequential([layers.Input(shape=(224,224,1)),
                                    layers.Conv2D(16,kernel_size=3,activation='relu',padding='same',strides=2),                
                                    layers.Conv2D(8,kernel_size=3,activation='relu',padding='same',strides=2),
                                    layers.Conv2D(4,kernel_size=3,activation='relu',padding='same',strides=2),
                                    layers.Flatten(),
                                    layers.Dense(units=3136,activation='sigmoid')]) # (28,28,4)

# deconvolution -> decoding 
self.decoder = tf.keras.Sequential([layers.Input(shape=(3136)),
                                    layers.Dense(units=3136,activation='sigmoid'),
                                    layers.Reshape((28,28,4)),
                                    layers.Conv2DTranspose(4,kernel_size=3,activation='relu',padding='same',strides=2),
                                    layers.Conv2DTranspose(8,kernel_size=3,strides=2,activation='relu',padding='same'),
                                    layers.Conv2DTranspose(16,kernel_size=3,strides=2,activation='relu',padding='same'),
                                    layers.Conv2D(1,kernel_size=(3,3),activation='sigmoid',padding='same')])

"
Are there examples of neural networks (used for control) implemented on a FPGA or on a neurochip?,"
Greetings to all respected colleagues!
I want to consult on the use of FPGAs and neurochips. I plan to use it in my laboratory project for programming control systems on neural networks. 
In my work, there are a lot of applications of neural networks, and I became interested in their programming on FPGAs and neurochips.
But I don’t know a single example of a really made and working laboratory prototype in which a neural network is implemented on an FPGA or on a neurochip and controls something. If someone shares the link, I would carefully study it.
","['neural-networks', 'reference-request', 'neuromorphic-engineering', 'control-theory']",
How are the weights retained for filters for a particular class in a CNN?,"
I am new to CNN. What I have learned so far about the filters is that when we are giving a training example to our model, our model updates the weights by gradient descent to minimize the loss function.
So my question is how the weights are retained for a particular class label?
The question is vague as my knowledge is vague. It's my 4th hour to CNN.
For example, if I am talking about the MNIST dataset with 10 labels. Let's say I am giving 1 image to my model initially. It will have a bigger loss for the forward pass. Let's say now it came for the back pass and adjusted the weights for and minimized the loss function for that label. 
Now, when a new label arrives for training, how will it update the weights for filters which have already been updated according to the previous label?
","['machine-learning', 'convolutional-neural-networks', 'backpropagation', 'objective-functions', 'gradient-descent']","
Besides the last layer rest of the weights are shared among all classes. When an image is passed to the network all weights are updated accordingly. The only weights that are directly responsible for one specific class are the ones of the final layer. The rest of the weights are updated to find the best values to minimize the average loss for all classes.
To rephrase there aren't ""filters"" in convolutional layers that are specific for a single class. They are used to extract features so that the final layer can make the final prediction (which has weights for each specific class).
I'd suggest you look a bit into how gradient descent and backpropagation works. 
"
How do I decide which norm to use for placing a constraint on my adversarial perturbation?,"
I am performing an adversarial machine learning attack on a neural network for network traffic classification. For adding adversarial perturbations in features such as packet interarrival times and packet size, what norm should I use as a constraint? (eg. l1 norm, l2 norm, l-infinity norm, etc.)
","['machine-learning', 'classification', 'math', 'constraint-satisfaction-problems', 'adversarial-ml']",
Is it normal to have the root mean squared error greater on the test dataset than on the training dataset?,"
I am new to deep learning.
I am training a model and I am getting a root mean squared error (RMSE) greater on the test dataset than on the training dataset. 

What could be the reason behind this? Is this acceptable to get the RMSE greater in test data?
","['deep-learning', 'performance', 'mean-squared-error']","
It is common to have root mean squared error (RMSE) greater on the test dataset than on the training dataset (this is equal to having accuracy/score higher for model in training dataset than test dataset). This normally happens because the training data are assesed on the same data that have been learnt before, while the test dataset may have data that are unknown / not common that may give more errors or misclassification when doing prediction.
But if your model shows your test dataset have way too high RMSE result rather than your training dataset RMSE result, it may indicates that overfitting happens.
If overfitting happens, there are a lot of reasons this could happen. 
Referenced from https://elitedatascience.com/overfitting-in-machine-learning, some factors that causes overfitting are:

Complexity of data (e.g. there are irrelevant input features). This can be solved with removing irrelevant input features. 
Not enough training data. This can be solved by training with more data (Eventhough this may not always succeed. Sometimes it may give noise towards data), etc. 

"
How to estimate the accuracy upper limit of any CNN model over a computer vision classification task,"
We are given a computer vision classification task, that is, a task that asks us to predict the category of an image over $n$ predefined classes (the so-called closed set classification problem).
Question: Is it possible to give an estimate on what is the best accuracy one is likely achieve using an end-to-end CNN model (possibly, using a popular backbone) in this task? Do the performances of state-of-the-arts models on open datasets serve as a good reference? If someone claims that they achieve certain performance with some popular CNN architecture, how do we know s/he is not bragging?
You may or may not have access to the training dataset yet. The testing dataset shall be something close to the real-world production scenario. I know this is too vague, but just assume you have a fair judge.
Background: Product teams sometimes asks engineering teams for quick (and dirty) solutions. Engineering teams want to assess the feasibility before say ""Yes we can do $95\%$"" and officially launch (and be responsible) the projects.
","['neural-networks', 'deep-learning', 'classification', 'computer-vision', 'performance']","
There is no easy rule for this. You can use transfer learning to select a model that works well on image classification. However the accuracy you achieve will be highly dependent on your training set.  If your training set is ""similar"" in quantity and quality to what was used  for the accuracy achieved by the transfer learning model in some application you have a reasonable chance of coming close to that accuracy. By similar is mean roughly the same number of images per class. By quality I mean what percentage of the pixels in the images that are occupied by the ""region of interest -ROI), same level of noise in the images etc. Also it depends on the nature of the classes. If the classes are widely different (elephants vs trees) the accuracy should be higher than if you are try to classify closely related images (human faces).
"
Aren't all discrete convolutions (not just 2D) linear transforms?,"

The image above, a screenshot from this article, describes discrete 2D convolutions as linear transforms. The idea used, as far as I understand, is to represent the 2 dimensional $n$x$n$ input grid as a vector of $n^2$ length, and the $m$x$m$ output grid as a vector of $m^2$ length. I don't see why this can't be generalised to higher-dimensional convolutions, since a transformation matrix can be constructed from one vector to another, no matter the length (right?)
My question: Aren't all discrete convolutions (not just 2D) linear transforms?
Are there cases where such a transformation matrix cannot be found?
","['convolutional-neural-networks', 'signal-processing']","
The convolutions are linear transformations. However in typical applications a non linear activation function like RELU is used following the convolution to provide non-linearity otherwise a convolutional neural network would just be a net linear transformation.
"
Interpreting I/O Transformation Matrix in Convolution,"
I've been reading this article on convolutional neural networks (I'm a beginner) - and I'm stuck at a point.

What I understand: We have a 4x4 input, and want to transform it to a 2x2 grid. I'm visualising this as a kernel sliding over the 4x4 grid, with just the right number of strides, so as to get 4 outputs which constitute the 2x2 grid (there are animations right above this part of the page in the link attached). The model chooses to represent the 4x4 grid as a vector of length 16. Also, the 4x16 transformation matrix when pre-multiplied to the input vector, produces the output vector, which is mapped back to a 2D grid. Is this right?
Moving on, another screenshot from the same page-

Is it that both the matrices are really the same, and a lot of weights just happen to be zero in the weight matrix, which is what the second matrix is depicting? In that case even, why so many zeros?
P.S. Thanks a lot for being so patient and reading to the end of this post, I really appreciate it. I'm a beginner, and really interested in these topics, and I'm self studying from various online resources - hence, any help is appreciated. I hope this is the right platform for this post.
","['neural-networks', 'convolutional-neural-networks']",
Why do DeconvNet use ReLU in the backward pass?,"
Why does DeconvNet (Zeiler, 2014) use ReLU in the backward pass (after unpooling)? Are not the feature maps values already positive due to the ReLU in the forward pass? So, why do the authors apply the ReLU again coming back to the input?
update:  I better explain my problem:
given an input image $x$ and ConvLayer $CL$ composed of: 

a convolution
an activation function ReLU
a pool operation

$f$ is the output of ConvLayer given an input $x$, i.e. $f=CL(x)$.
So, the Deconv target is to ""reverse"" the output $f$ (the feature map) to restore an approximate version of $x$. To this aim, the authors define a function $CL^{-1}$ composed of 3 subfunctions: 
a. unpool 
b. activation function ReLU (useless in my opinion, because $f$ is already positive due to the application of the 2. step in $CL(f)$) 
c. transposed convolution. 
In other words $x\simeq CL^{-1}(f)$ where $CL^{-1} (f) = transpconv(relu(unpool(f)))$. But, if $f$ is the output computed as $f=CL(x)$, it is already positive, so the b. step is useless.
This is what I understood from the paper. Where I wrong?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'backpropagation', 'relu']",
How can I process neural network with 25000 input nodes?,"
I'm trying to build a neural network between protein sequence and its drug fingerprint. My input size is 20000. The output size is 881. The sample size is 610.
Can I process this huge neural network? But how? And in which tool? 
","['neural-networks', 'ai-design']","
Yes, it should be no problem.
When you decide to use a CNN, you have to make sure that this makes sense. Another answer mentioned using 3x3 convolutions -- which I would recommend against. For that to work, you would need to turn your vector into a rectangular array, and you would be implying a structure that isn't there.
Use one-dimensional convolutions instead.
"
Do RNN solves the need for LSTM and/or multiple states in Deep Q-Learning?,"
Introduction
I am trying to setup a Deep Q-Learning agent. I have looked that the papers Playing Atari with Deep Reinforcement Learning as well as Deep Recurrent Q-Learning for Partially Observable MDPs as well as looking at the question How does LSTM in deep reinforcement learning differ from experience replay?.
Current setup
I currently take the current state of a game, not picture but rather the position of the agent, the way the agent is facing and some inventory items. I currently only feed state $S_t$ as at input to a 3 layer NN (1 input, 2 hidden, 3 output) to estimate the Q-values of each action.
The algorithm that I use is almost the same as the one used in Playing Atari with Deep Reinforcement Learning, with the only difference that I do not train after each timestep but rather sample mini_batch*T at the end of each episode and train on that. Where T is the number of time-steps in that episode.
The issue
At the current state, the agent do not learn within 100 00 episodes, which is about 100 00 * 512 training iterations. Making me consider that something is not working, this is where I realised that I do not consider any of the history of the previous steps.
What I currently struggle with is sending multiple time-steps-states to the NN. The reason for this is the complexity of how the game/program I am using. According to Deep Recurrent Q-Learning for Partially Observable MDPs LSTM could be a solution for this, however I would prefer to manually code a RNN rather than using an LSTM. Would not a RNN with something like the following structure have a chance of working?

Also, as far as I know, RNN need the inputs to be fed in sequence and not randomly sampled?
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'q-learning']",
Which deep reinforcement learning algorithm is appropriate for my problem?,"
My task is to solve an optimization problem with deep reinforcement learning. I read about several algorithms like DQN, PPO, DDPG, and A2C/A3C but use cases always seem to be problems like video games (sparse rewards, etc.) or robotics (continuous action spaces, etc.). Since my problem is an optimization issue, I wonder which algorithm is appropriate for my setting:

limited number of discrete actions (like 20)
high-dimensional states (like 250 values)
instant reward after every single action (not only at the end of an episode)
a single action can affect the state quite a lot

There's no ""goal"" like in a video game, an episode ends after a certain number of actions. I'm not quite sure which algorithm is appropriate for my use case.
","['reinforcement-learning', 'optimization', 'deep-rl']","
Theoretically video games and robotics problems are also about optimization(getting maximum reward). So, just like other reinforcement learning problems, I would expect PPO to be the most efficient in your case too. I don't think a ""goal"" is necessary for rl, all you need is the rewards.
"
How do we minimize loss for a single neuron with a feedback?,"
Suppose we had a series of single-dimensional data points $X = \{x_1, x_2, \dots, x_n \}$, where $n$ is the number of data points and there corresponding output values $T = \{t_1, t_2, \dots, t_n \}$. 
Now, I want to train a single neuron network given below to learn from the data (the model is bad, but I just wanted to try it out as an exercise).

The output function of this neuron would be a recursive function as:
$$
y = f(a_0 + a_1x + a_2 y)
$$
where 
$$
f(x) = \frac{1}{1 + e^{-x}}
$$
for a given $x$.
The error function for such a model would be:
$$
e = \sum_{i=1}^N (y_i - t_i)^2
$$
How should I minimise this loss function? What are the derivatives that I need to use to update the parameters?
(Also, I am new to this problem, therefore it would be really helpful if you tell me sources/books to read about such problems.)
","['neural-networks', 'machine-learning', 'deep-learning', 'recurrent-neural-networks', 'feedforward-neural-networks']",
Is there any wrong in my focal loss derivation?,"
Assume $\mathbf{X} \in R^{N, C}$ is the input of the softmax $\mathbf{P} \in R^{N, C}$, where $N$ is number of examples and $C$ is number of classes:
$$\mathbf{p}_i = \left[ \frac{e^{x_{ik}}}{\sum_{j=1}^C e^{x_{ij}}}\right]_{k=1,2,...C} \in R^{C} \mbox{ is a row vector of } \mathbf{P}$$ 
Consider example $i$-th, because softmax function $\mathbf{p}:R^C \mapsto R^C$ (eliminate subscript $i$ for ease notation), so the derivative of vector-vector mapping is Jacobian matrix $\mathbf{J}$: 
$$\mathbf{J}_{\mathbf{p}}(\mathbf{x}) = \left[ \frac{\partial \mathbf{p}}{\partial x_1}, \frac{\partial \mathbf{p}}{\partial x_2}, ..., \frac{\partial \mathbf{p}}{\partial x_C}  \right] = 
\begin{bmatrix} 
    \frac{\partial p_1}{\partial x_1} & \frac{\partial p_1}{\partial x_2} & \dots & \frac{\partial p_1}{\partial x_C} \\ 
    \frac{\partial p_2}{\partial x_1} & \frac{\partial p_2}{\partial x_2} & \dots & \frac{\partial p_2}{\partial x_C} \\
    \dots & \dots & \dots & \dots \\ 
    \frac{\partial p_C}{\partial x_1} & \frac{\partial p_C}{\partial x_2} & \dots & \frac{\partial p_C}{\partial x_C} 
\end{bmatrix}
\in R^{C, C}
$$
$\mathbf{J}_{\mathbf{p}}(\mathbf{x})$ is called the derivative of vector ${\mathbf{p}}$ with respect to vector $\mathbf{x}$
$$\mbox{1) Derivative in diagonal:}\frac{\partial p_{k}}{\partial x_{k}} = \frac{\partial}{\partial x_{k}}\left( \frac{e^{x_k}}{\sum_{j=1}^C e^{x_j}} \right)$$
$$ = \frac{\left( \frac{\partial e^{x_k}}{\partial x_k} \right)\sum_{j=1}^C e^{x_j} - e^{x_k}\left(\frac{\partial \sum_{j=1}^C e^{x_j}}{\partial x_k} \right)}{\left(\sum_{j=1}^C e^{x_j}\right)^2} \mbox{  (Quotient rule) }$$
$$ = \frac{e^{x_k}\sum_{j=1}^C e^{x_j} - e^{x_k} e^{x_k}}{\left(\sum_{j=1}^C e^{x_j}\right)^2} = \frac{e^{x_k}(\sum_{j=1}^C e^{x_j} - e^{x_k})}{\left(\sum_{j=1}^C e^{x_j}\right)^2}$$
$$ = \frac{e^{x_k}}{\sum_{j=1}^C e^{x_j}} \left(\frac{\sum_{j=1}^C e^{x_j} - e^{x_k}}{\sum_{j=1}^C e^{x_j}}\right) = \frac{e^{x_k}}{\sum_{j=1}^C e^{x_j}} \left(1 - \frac{e^{x_k}}{\sum_{j=1}^C e^{x_j}}\right)$$
$$\rightarrow \frac{\partial p_{k}}{\partial x_{k}} = p_{k}(1-p_{k})$$
$$\mbox{2) Derivative not in diagonal } k \neq c \mbox{ :} \frac{\partial p_{k}}{\partial x_{c}} = \frac{\partial}{\partial x_{c}}\left( \frac{e^{x_k}}{\sum_{j=1}^C e^{x_j}} \right)$$
$$ = \frac{\left( \frac{\partial e^{x_k}}{\partial x_c} \right)\sum_{j=1}^C e^{x_j} - e^{x_k}\left(\frac{\partial \sum_{j=1}^C e^{x_j}}{\partial x_c} \right)}{\left(\sum_{j=1}^C e^{x_j}\right)^2} \mbox{  (Quotient rule) }$$
$$ = \frac{0 - e^{x_k} e^{x_c}}{\left(\sum_{j=1}^C e^{x_j}\right)^2} = -\frac{e^{x_k}}{\sum_{j=1}^C e^{x_j}}\frac{e^{x_c}}{\sum_{j=1}^C e^{x_j}}$$
$$\rightarrow \frac{\partial p_{k}}{\partial x_{c}} = -p_{k}p_{c}$$
$$\rightarrow \mathbf{J}_{\mathbf{p}}(\mathbf{x}) =
\begin{bmatrix} 
    p_1(1-p_1) & -p_1p_2 & \dots & -p_1p_C \\ 
    -p_2p_1 & p_2(1-p_2) & \dots & -p_2p_C \\
    \vdots & \vdots & \ddots & \vdots \\ 
    -p_Cp_1 & -p_Cp_2 & \dots & p_C(1-p_C)
\end{bmatrix}$$

Focal Loss: $\displaystyle FL = -\sum_{k=1}^C y_{k} \alpha_{k}(1-p_k)^\gamma \log (p_k)$

$$\nabla_{\mathbf{x}} FL = \nabla_{\mathbf{p}}FL (\mathbf{J}_{\mathbf{p}}(\mathbf{x}))^T $$
$$\nabla_{\mathbf{p}} FL = 
\begin{bmatrix}
    \frac{\partial FL}{\partial p_1}\\ 
    \frac{\partial FL}{\partial p_2}\\
    \vdots \\
    \frac{\partial FL}{\partial p_C} 
\end{bmatrix} \mbox{ where } \frac{\partial FL}{\partial p_k}
= - y_k\alpha_k \left(-\gamma(1-p_k)^{\gamma-1} \log(p_k) + \frac{(1-p_k)^\gamma}{p_k}  \right) = y_k \alpha_k\gamma(1-p_k)^{\gamma-1}\log(p_k) - y_k\alpha_k\frac{(1-p_k)^\gamma}{p_k}$$
$$\nabla_{\mathbf{x}} FL = 
\begin{bmatrix}
    \frac{\partial FL}{\partial p_1}\\ 
    \frac{\partial FL}{\partial p_2}\\
    \vdots \\
    \frac{\partial FL}{\partial p_C} 
\end{bmatrix}^T
\begin{bmatrix} 
    \frac{\partial p_1}{\partial x_1} & \frac{\partial p_1}{\partial x_2} & \dots & \frac{\partial p_1}{\partial x_C} \\ 
    \frac{\partial p_2}{\partial x_1} & \frac{\partial p_2}{\partial x_2} & \dots & \frac{\partial p_2}{\partial x_C} \\
    \vdots & \vdots & \ddots & \vdots \\ 
    \frac{\partial p_C}{\partial x_1} & \frac{\partial p_C}{\partial x_2} & \dots & \frac{\partial p_C}{\partial x_C} 
\end{bmatrix} =
\begin{bmatrix}
    \sum_{k=1}^C \left(\frac{\partial FL}{\partial p_k}\frac{\partial p_k}{\partial x_1}\right)\\ 
    \sum_{k=1}^C \left(\frac{\partial FL}{\partial p_k}\frac{\partial p_k}{\partial x_2}\right)\\
    \vdots \\
    \sum_{k=1}^C \left(\frac{\partial FL}{\partial p_k}\frac{\partial p_k}{\partial x_C}\right)
\end{bmatrix}^T \in R^C
$$
$$\mbox{Case 1: }\displaystyle \frac{\partial FL}{\partial p_k}\frac{\partial p_k}{\partial x_k} \forall k=1,2,...,C$$
$$\frac{\partial FL}{\partial p_k} \frac{\partial p_k}{\partial x_k} = y_k \alpha_k\gamma(1-p_k)^{\gamma-1}\log(p_k)p_k(1-p_k) - y_k\alpha_k\frac{(1-p_k)^\gamma}{p_k}p_k(1-p_k)$$
$$ = y_k \alpha_k (1-p_k)^{\gamma}(\gamma p_k \log(p_k) - 1 + p_k) $$
$$\mbox{Case 2: } (k \neq c)\displaystyle \frac{\partial FL}{\partial p_k}\frac{\partial p_k}{\partial x_c}$$
$$\frac{\partial FL}{\partial p_k} \frac{\partial p_k}{\partial x_c} = - y_k\alpha_k\gamma(1-p_k)^{\gamma-1}\log(p_k)p_kp_c + y_k\alpha_k\frac{(1-p_k)^\gamma}{p_k}p_kp_c$$
$$ = - y_k\alpha_k (1-p_k)^{\gamma-1}p_c(\gamma p_k \log(p_k) - 1 + p_k) $$
$$\mbox{For each } d=1,2,...,C \mbox{ : }\sum_{k=1}^C \left(\frac{\partial FL}{\partial p_k} \frac{\partial p_k}{\partial x_d}\right) = y_d \alpha_d (1-p_d)^{\gamma}(\gamma p_d \log(p_d) - 1 + p_d) + \sum_{c \neq d}^C \left( - y_d\alpha_d (1-p_d)^{\gamma-1}p_c(\gamma p_d \log(p_d) - 1 + p_d) \right) = y_d\alpha_d(1-p_d)^{\gamma-1}(\gamma p_d \log(p_d) - 1 + p_d)\left(1-p_d -\sum_{c \neq d}^C(p_c)\right) $$
$$\rightarrow \nabla_{\mathbf{x}} FL = \left[ y_d\alpha_d(1-p_d)^{\gamma-1}(\gamma p_d \log(p_d) - 1 + p_d)\left(1-p_d -\sum_{c \neq d}^C(p_c)\right) \right]_{d=1,2,...,C}$$
However, the problem is $\left(1-p_d -\sum_{c \neq d}^C(p_c)\right) = 0$ (because sum of all probabilities is 1) then the whole expression collapses to $0$. 
Is there any wrong in my focal loss derivation? 
Reference:

https://en.wikipedia.org/wiki/Softmax_function
https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant
https://arxiv.org/abs/1708.02002

","['deep-learning', 'math', 'objective-functions', 'calculus']",
length independent sequence classification methods,"
I am looking to do sequence classification using deep learning. The length of my sequences can vary from a few hundred to several tens of thousands of characters. I was wondering what is a good approach for doing this. I had success with splitting a sequence into subsequences a few hundred characters long and using LSTMs, but then one is faced with the task of putting the results of each of those together and it is nontrivial as well. Any help would be appreciated.
","['deep-learning', 'classification', 'long-short-term-memory', 'sequence-modeling']",
"Would a new human-like general artificial intelligence be more similar, in terms of eduction, to a toddler or an adult human?","
The naive concept of a general AI, or strong AI, or artificial general intelligence, is some kind of software that can answer questions like   

What is the volume of a cube that is 1 m wide?

or even

Why are there only two political parties in the US?

The second question requires external knowledge and high-level reasoning. For example that US means USA in the context, the constitution and that having two parties is caused by the mathematical properties of the election system.
But I would expect that newborn human child is intelligent in the sense of intelligence that is used in general artificial intelligence, but toddlers can not answer these questions.
That is not because an infant is not intelligent, but because it is not educated, I think.
What is the apparent level of education of an artificial intelligence that could be called human-like?
","['philosophy', 'agi', 'human-like']","
A human-level AI should be able to learn and behave in the same way that humans learn and behave, otherwise, we shouldn't be calling it a human-level AI. So, either if it starts with more or less knowledge of a baby human, it should be able to learn similarly to a human and acquire more knowledge with experience. 

What is the apparent level of education of an artificial intelligence that could be called human-like?

To answer you question more directly, the level of education of a human-like AI (even newly created ones) can potentially be variable (i.e. different human-like AIs could potentially have different levels of education), but the human-like AI will necessarily need to be able to increase its level of education and, in general, knowledge (because humans can also do this).
Moreover, note that AGI is not necessarily restricted to human-level AI. So, other AGIs may not follow the same principles of humans. 
"
Where can I find example data for Nonogram solver?,"
I made CSP nonogram solver and I wanna test it on some bigger data.
Where can I find such data to test my program? I've been looking on the internet but I couldn't find anything.
",['constraint-satisfaction-problems'],
Object detection: combine many classes into one?,"
I am trying to train a model that detects logos in documents. Since I am not really interested in what kind of logo there is, but simply if there is a logo, does it make sense to combine all logos into 1 logo class? 
Or are ""logos"" too diverse to group them together (like some logos are round, some are rectangular, some are even text based etc.) and the diversity of features will just make it hard for the neural network to learn? Or doesn't it matter?
(I am currently trying out the YOLOv3 architecture to begin with. Any other suggestions better suited are also welcome)
","['convolutional-neural-networks', 'classification', 'object-detection', 'yolo']","
I think there is no absolute answer for this. Often its kind of trial and error. In general the CNN tries to generalize the problem, so using all logos with different augmentations and ground truths can maybe lead to some feature maps, which are so general that the CNN can find logos. 
But if your logos are so various, and embedded in colorful websites, the tasks seems quite difficult, also if they vary in shape and form.Like you said, I think you definitely need an FPN (Feature Pyrimad network) to get the different sizes, scales and so on combined with an RPN (region-proposal network) to find the logos multiple times in the websites (if thats nessecary). For that you can use Mask-RCNN (https://github.com/matterport/Mask_RCNN). You can try to transfer-train it on Imagenet-Backbone for example to reduce training time. Just tried to use Mask-RCNN to segment colored cells from medical images. Worked out quite good.
I used ImageLabel for labeling, worked out quite intiutive and saved all in a JSON file.Imglabel
How large is your dataset and how complex are the logos ? Do you have examples which you can demonstrate?
"
Non-Neural Network algorithms for large state space in zero sum games,"
I was reading online that tic-tac-toe has a state space of $3^9 = 19,683$. From my basic understanding, this sounds too large to use tabular Q-learning, as the Q table would be huge. Is this correct?
If that is the case, can you suggest other (non-NN) algorithms I could use to create a TTT bot to play against a human player?
","['reinforcement-learning', 'q-learning', 'algorithm-request', 'tic-tac-toe', 'zero-sum-games']","
In the case of TicTacToe, you can make use of game theory. The entire search space can be denoted by a game tree. You bot must now be able to maximize the chance of winning.
You can make use of the Maximin algorithm. This is still computationally intensive on large search spaces. To improve the efficiency Alpha-Beta pruning can be applied to reduce the number of nodes in the Game tree.
These are core AI concepts and will always perform better than neural networks on dully defined and relatively smaller search spaces. Neural networks perform better when it's too difficult to compute all the possible combinations of a game at a certain state.
You can have a look at this to build a TicTacToe bot.
"
"What does the notation ${s'\sim T(s,a,\cdot)}$ mean?","
I have been seeing notations on Expectations with their respective subscripts such as $E_{s_0 \sim D}[V^\pi (s_0)] = \Sigma_{t=0}^\infty[\gamma^t\phi(s_t)]$. This equation is taken from https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf and $Q^\pi(s,a,R) = R(s) + \gamma E_{s'\sim T(s,a,\cdot)}[V^\pi(s',R)]$ ,in the case of the Bayesian IRL paper.(https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf) 
I understand that $s_0 \sim D$ means that the starting state $s_0$ is drawn from a distribution of starting states $D$. But how do we understand the latter with subscript ${s'\sim T(s,a,\cdot)}$ ? How is $s'$ drawn from a distribution of transition probabilities?
","['reinforcement-learning', 'notation', 'expectation', 'apprenticeship-learning']",
Building a template based NLG system to generate a report from data,"
I am a newbie to NLP and NLG. I am tasked to develop a system to generate a report based on a given data table. The structure of the report and the flow is predefined. I have researched on several existing python libraries like BERT, SimpleNLG but they don't seem to fit my need. 
For example:
input_data(country = 'USA', industry = 'Coal', profit = '4m', trend = 'decline')
output: The coal industry in USA declined by 4m. 
The input data array can be different combinations (and dynamic) based on a data table. I would like to know if  there is any python package available, or any resource discussing a practical approach for this. 
","['natural-language-processing', 'long-short-term-memory', 'bert']",
Are PreLU and Leaky ReLU better than ReLU in the case of noisy labels?,"
Let's assume I want to build a semantic segmentation algorithm, based on Multires-UNET. My GT-masks are messy and generated by a GAN, but they are getting better and better over time. The goal is knowledge expansion (based on the paper Noisy-Student). 
Can you generally say that PreLU and Leaky Relu are better for noisy labels (or imperfect ones), like the situation in GANs in general?
","['machine-learning', 'activation-functions', 'relu']","
The provably more-reliable approach to learning with noisy labels/label errors (instead of altering the model architecture or loss function) is to find the label errors and train without them on cleaned data -- a result shown in the confident learning paper (Northcutt, Lu, Chuang; 2021)). I am an author on this paper.
Re-phrasing your question to two questions: (1) ""how can I generally find label errors in any dataset for any model?"" and (2) ""how can I train a model on noisy data as if it was clean data?"".
There are one line of code answers to each of these questions using the cleanlab python package which implements the algorithms and theory in the confident learning paper and works for any dataset you can train a classifier on for most data formats, ML and deep learning frameworks, and data modalities, e.g. image, text, tabular, and audio data.
link to python package: https://github.com/cleanlab/cleanlab
Find label issues in 1 line of code
from cleanlab.classification import CleanLearning
from cleanlab.filter import find_label_issues

# Option 1 - works with sklearn-compatible models - just input the data and labels ツ
label_issues_info = CleanLearning(clf=sklearn_compatible_model).find_label_issues(data, labels)

# Option 2 - works with ANY ML model - just input the model's predicted probabilities
ordered_label_issues = find_label_issues(
    labels=labels,
    pred_probs=pred_probs,  # out-of-sample predicted probabilities from any model
    return_indices_ranked_by='self_confidence',
)

Train a model as if the dataset did not have errors -- 3 lines of code
from sklearn.linear_model import LogisticRegression
from cleanlab.classification import CleanLearning

cl = CleanLearning(clf=LogisticRegression())  # any sklearn-compatible classifier
cl.fit(train_data, labels)

# Estimate the predictions you would have gotten if you trained without mislabeled data.
predictions = cl.predict(test_data)

Documentation and runnable tutorials for cleanlab: https://docs.cleanlab.ai/
"
Is there any programming language designed by deep learning?,"
I know that AI can be used to design printed circuit boards (PCBs), so it can be used to solve complex tasks.
Is there any programming language designed by deep learning (or any other AI technique)?
","['deep-learning', 'applications', 'programming-languages']","
There are certainly things like this.
I'd say a strong example is layered learning approaches, descended from Peter Stone's work.
A programming language is essentially a collection of useful shorthands for assembly-level instructions. Ultimately, everything you do in a programming language eventually gets executed in assembly. So making a programming language amounts to learning how to write short, reusable, assembly language programs that you can then use as building blocks to solve harder problems.
An example of this in action is Kelly & Heywood's approach to constructing 'Tangled Program Graphs' for reinforcement learning (IJCAI 2018). Here an evolutionary algorithm is used to learn short assembly programs, that can be combined into a graph to make more complex programs. This is similar to graphical programming languages like J.
"
Is ReLU a non-linear activation function?,"
According to this blog post

The purpose of an activation function is to add some kind of non-linear property to the function

The sigmoid is typically used as an activation function of a unit of a neural network in order to introduce non-linearity.
Is ReLU a non-linear activation function? And why? If not, then why is it used as an activation function of neural networks?
","['neural-networks', 'deep-learning', 'math', 'activation-functions', 'relu']","
ReLU is non-linear by definition

In calculus and related areas, a linear function is a function whose graph is a straight line, that is a polynomial function of degree one or zero.

Since the graph of the ReLU function $f(x) = \max(0,x)$ is not a straight line (equivalently, it cannot be expressed in the form $f(x) = mx + c$), by definition it is not linear.
ReLU is piecewise linear
ReLU is piecewise linear on the bounds $(-\inf,0]$ and $[0,\inf)$:
$$
f(x) = \max(0,x) = \begin{cases}
               0               & x \le 0\\
               x               & x \gt 0\\
       \end{cases}
$$
But this is still non-linear on the entire domain:

"
"How do I prove that $\mathcal{H}$, with $\mathcal{VC}$ dimension $d$, shatters all subsets with size less than $d-1$?","
If a certain hypothesis class $\mathcal{H}$ has a $\mathcal{VC}$ dimension $d$ over a domain $X$, how can I prove that $H$ will shatter all subsets of $X$ with size less than $d$, i.e. $\mathcal{H}$ will shatter $A \subset X$ where $|A| \leq d-1$?
","['proofs', 'computational-learning-theory', 'vc-dimension', 'vc-theory', 'hypothesis-class']",
Is text preprocessing really all that necessary for NLP?,"
As a first step in many NLP courses, we learn about text preprocessing. The steps include lemmatization, removal of rare words, correcting typos etc. But I am not so sure about the actual effectiveness of doing such a step; in particular, if we are learning a neural network for a downstream task, it seems like modern state of the art (BERT, GPT-2) just take essentially raw input. 
For instance, this ACL paper seems to show that the result of text preprocessing is mixed, to say the least. 
So is text preprocessing really all that necessary for NLP? In particular, I want to contrast/compare against vision and tabular data, where I have empirically found that standardization usually actually does help. Feel free to share your personal experiences/what use cases where text preprocessing helps! 
","['neural-networks', 'natural-language-processing', 'data-preprocessing', 'structured-data']","
It all depends on the quality of data. Due to old rule ""Garbage in, garbage out"" link
, if you have bad quality data(data redundancy, unstructured data, too much memory, etc) your results won't be spectacular. 
In other cases, everybody could be a Data Scientist, because its only task was ""put raw text into classifier"". Also, you should remember that BERT or GPT-2 it's deep learning algorithms so they not need too much processing. Using preprocessing in machine learning is more that needed(prediction of sentiment for example).
Shortly, preprocessing is optional, but highly advisable.
"
How can I develop a reinforcement learning agent that plays memory cards game?,"
I am new to RL, and I am thinking of doing a little project. The goal of the project is to learn an agent play the memory game with cards.
I already created the program for detecting the cards on the table (with YOLO) and classifying them what kind of object they are.
I want an agent to be able to play the memory game by itself, without being explicitly told the rules and such.
Any tips on how to get started to make the RL process easier?
","['machine-learning', 'deep-learning', 'reinforcement-learning', 'ai-design']","
The main thing to keep in mind when designing a reinforcement learning agent is that you need to develop an interactive environment in which the agent can learn and define the possible moves the agent can make. In your case, the environment is the memory cards. 
Next you need to define how the agent can interact with the environment, that is choosing a card or flipping a card etc. Also how the environment will behave when the agent chooses a move. 
Finally you need to define the fitness function, in this case the score (number of pairs created) should be fine. You should have a look at some online RL competitions to get a better understanding of the architecture. AWS DeepRacer
"
What are the pros and cons of deep learning and machine learning to develop a trading system?,"
As I want to start coding a new Trading AI this year (first based on Python and later maybe in C++) I stumbled over the following question:
Today, I would like to make a pro/contra list with you in the area of deep learning vs machine learning. The difference should be clear to most of you. If not, here is a nice explanation from Hackernoon.
Up to now, I was convinced that this future project will be based on Tensorflow, Keras, etc. However, the following came to my mind afterward.
Most of you will probably have heard of Pluribus already. Dr. Sandholm and Mr. Brown (as work for his Ph.D.) were the first to program an AI that won against 6 poker world champions in No-Limit-Texas Holdem Poker. This seemed impossible because poker is a game of imperfect information. If you haven't read/seen their work until now, here's a link to a Facebook blog post Facebook, Carnegie Mellon build first AI that beats pros in 6-player poker. See also the paper Superhuman AI for multiplayer poker and this video.
From the work, it is clear that they wrote the whole thing in C++ and WITHOUT the use of any deep learning library but exclusively on the basis of machine learning. This was also confirmed to me by both of them via email. So it was possible to bring the AI in under 24H to a level that could beat 6 Poker World Champions without any problems.
The stock and crypto market is nothing else. A game of imperfect information. The prices of a crypto coin or stock are influenced by an incredible number of factors. This includes of course prices from the past but also current media (as currently seen with covid-19) and data from the economy.
We want to grab the data out of the ""Big Players"", like CoinCap API, CryptoAPI.io for all kinds of historical and new charts prices, etc. The same we will do with the Yahoo Finance API to grab data out of the stock market. Depending on the size of this project and how it will develop, we want to implement also some kind of NLP to grab the most out of Economy Data like dailyfx news to predict some in/decreases for some stocks but this is a future feature.
So, basically, the main question is: should we use neural networks (deep learning) or machine learning?
All this leads me to the conclusion that I am not sure what the better option for a trading bot would be. I know that the training of AI-based on deep learning would take much longer than based on machine learning, but is it safe to say that the results are really better?
What are the pros and cons of deep learning and machine learning to develop a trading system?
","['machine-learning', 'reinforcement-learning', 'deep-learning', 'comparison', 'algorithmic-trading']",
Whats the correct loss function to use during deep Q-learning (discrete action space),"
After playing around with normal Q-learning I have decided to switch to deep Q-learning and I have encountered this problem. 
As I understand, for a task with discrete action space, where there are 4 possible actions (lets say left, right, up, down) my DQN needs to have four outputs. And then argmax of prediction needs to be taken, which will be my predicted action (if argmax(prediction)==2 then I will pick third action, in my case up).
If I use Mean Squared Error loss for my network then the dimension of output needs to be same as dimension of expected target variables. I am calculating target variables using following code:
target = rewards[i] + GAMMA * max(Qs_next_state[i]) which gives me a single number (while predicted output is four dimensional) as I workaround I decided to use custom loss function:
def custom_loss(y_true, y_pred): # where y_true is target, y_pred is output of neural net
    return ((max(y_pred) - y_true)**2) / BATCH_SIZE

But I am not sure if it is correct, from what I have read from tutorials/papers loss functions do not have this max() in them. But how else am I going to end up with same dimensionality between targets and NN outputs. What is the correct approach here?
","['reinforcement-learning', 'q-learning', 'dqn']",
Can a fully convolutional network always return an image of the same size as the original?,"
I'm trying to perform a segmentation task on images of multiple sizes using fully convolutional neural networks. 
Currently, I'm using EfficientNet as a feature extractor, and adding a deconvolution/backwards convolution/transposed convolution layer as described in the original Fully Convolutional Networks for Semantic Segmentation paper.
But this transposed convolution layer doesn't return a filter of a size equivalent to the original image for images of varying sizes.
For example, let's assume the original image is $100 \times 100$, and the last layer contains filters of size $50 \times 50$. To get a filter of the same size as the original, you would need a transposed convolution layer of size $51 \times 51$.
Now, assume you passed in an image of size $200 \times 200$. The last layer would contain filters of size $100 \times 100$. That same transposed convolutional filter of size $51 \times 51$ would result in an output of size $150 \times 150$.
Is there any way to make it so that a fully convolutional network always returns an image of the same size as the original?
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'fully-convolutional-networks']","
I ended up using a work around.
I set up the network so that an C x C (i.e. 320 x 320) input would output a C x C mask for some constant C (in my case it was 320).
I then resized the image I wanted to pass in to C x C, and then resized the output back to the original size of the Image.
"
How should I deal with variable-length inputs for neural networks?,"
I am a very beginner in the field of AI. I am basically a Pharma Professional without much coding experience. I use GUI-based tools for the neural network.
I am trying to develop an ANN that receives as input a protein sequence and produces as output a drug molecule. Drug molecules can be represented as fixed-length binary (0-1). This length is 881 bits.
However, I do not know how to transform protein sequences of variable length into a fixed-length binary representation.
So, how should I deal with variable-length inputs for a neural network? What is the best way?
","['neural-networks', 'machine-learning', 'data-preprocessing']","
The most common way people deal with inputs of varying length is padding.
You first define the desired sequence length, i.e. the input length you want your model to have. Then any sequences with a shorter length than this are padded either with zeros or with special characters so that they reach the desired length. If an input is larger than your desired length, usually you'd split it into multiple inputs.
"
What is the difference between batch and mini-batch gradient decent?,"
I am learning deep learning from Andrew Ng's tutorial Mini-batch Gradient Descent.
Can anyone explain the similarities and dissimilarities between batch GD and mini-batch GD?
","['deep-learning', 'comparison', 'gradient-descent', 'stochastic-gradient-descent', 'mini-batch-gradient-descent']","
It is really simple. 
In gradient descent not using mini-batches, you feed your entire training set of data into the network and accumulate a cost function based on this full set of data. Then you use gradient descent to adjust the network weights to minimize the cost. Then you repeat this process until you get a satisfactory level of accuracy. For example, if you have a training set consisting of 50,000 samples, you would feed all 50,000 samples along with the 50,000 labels into the network, then perform gradient descent and update the weights. This is a slow process because you have to process 50,000 inputs to do just one step of gradient descent.
To make things go faster instead of running all 50,000 inputs through the network, you split up the training set into ""batches"". For example, you could break the training set up into 50 batches each containing 1000 samples. You would feed the network the first batch of 1000 samples, accumulate the loss value then perform gradient descent and adjust the weights. Then you feed in the next batch of 1000 samples and repeat the process. So, now, instead of only getting one step of gradient descent for 50,000 samples, you get 50 steps of gradient descent. This method of using batches leads to a much faster convergence of the network.
"
How do weights changes handles during back-propagation when there are unknown labels,"
I have a question about how weights are updated during back-propagation for some of my samples that have unknown labels (please note, unknown, not missing). The reason they are unknown is because this is genomic data and to generate these data would take 8 years of lab work! Nevertheless I have genomic data for samples that have multiple-labels, sex age organ etc. this is a multi-class multi-label problem. 
For most classes, ALL labels are complete. For two or three classes, there are unknown labels. an example would be the developmental stage of samples at age x, the developmental stage of sample at age y are known. the developmental stage of samples at age Z are unknown! (generating this data is what would take most time)... I would therefore like to include all this data during training as it is indispensable. I would like to generate the sigmoid probability and assign unknown label 'Z' as belonging to developmental stage 0 or 1 (known classes) based on a threshold (say >= 0.5)... When one-hot encoding the unknown labels simply have no ground truth, 0 for class developing and 0 for not-developing as follows (example of 3 samples shown for class in question):
  [[1., 0.
   [0., 1.
   [0., 0.  ......]]

the first row is known sample 1, second is known sample 2 and 3rd is unknown, and therefore has no ground truth. It is this sample i would like to assign a label of known class 1 or 2 based on the 'highest probability'.. based on reading and discussions, this is the direction i will be taking for this task, as it can be validated in the lab later... so the approach is, include in training and see what the network 'thinks' it is. 
My question is: How does back-propagation handle these known and unknown samples with respect to weight updates. 
I should note i have trained the network with ~90% validation performance. for all classes for which is there is complete data, the predictions are great. and the same for classes for which there is unknown data. It can accurately classify the samples for which there is known developmental stages... and it does assign a probability value to those samples that have the 'unknown' label (0,0), so i would really like to know how back-prop is handling these samples for the classes where there are unknown ground truth labels.
thank you!
","['neural-networks', 'training', 'backpropagation', 'objective-functions']",
How can I formulate a nonogram problem as a constraint satisfaction problem?,"
I've just started learning CSP and I find it quite exciting. Now I'm facing a nonogram solving problem and I want to solve it using backtracking with CSP.
The first problem that I face is that I cannot quite figure out what such variables, domains, and constraints could be. My first thought was to make every field(those squares) a variable with such domain: $D_i = \{1,0\}$, where 1 means that a certain field has been colored black and 0 white.
So far I've been mostly learning binary constraints, and I was thinking of using the AC-3 or forward checking algorithm for propagation during the backtracking algorithm.
As far I know constraints of all arity can be represented as a set of binary constraints, so that would enable me to use the algorithms I mentioned. But that leads me to the problem of defining constraints. If every field was a variable then each line and column would be a constraint, based on how certain lines should be colored(numbers defining line for example 2,3,2).
But it's all new and quite hard for me to imagine and come up with. I've been reading some articles and papers on that but they were too advanced for me.
So, does anybody have an idea how can I formulate a nonogram problem as a constraint satisfaction problem?
","['search', 'constraint-satisfaction-problems']",
What effect does batch norm have on the gradient?,"
Batch norm is a technique where they essentially standardize the activations at each layer, before passing it on to the next layer. Naturally, this will affect the gradient through the network. I have seen the equations that derive the back-propagation equations for the batch norm layers. From the original paper: https://arxiv.org/pdf/1502.03167.pdf

However, I have trouble understanding if there is an intuitive understanding of what effect it actually has on the network. For instance, does it help with the exploding gradient problem, since the activations are rescaled, and the variance of them is constrained?
","['deep-learning', 'optimization', 'batch-normalization', 'vanishing-gradient-problem', 'exploding-gradient-problem']","
""Naturally, this will affect the gradient through the network."" this statement is only partially true, let's see why by starting explaining the real aim of batch normalisation.
As the title of the paper suggest, the aim of batch normalisation is to decrease training time by reducing covariance shift. What is covariance shift? We can conceive it as the variation that can happen between between the values of two layers of a network. We are all familiar with the concept that if we have input features with different unite scale, like kilos and euros, most likely lot of values will have different magnitude orders, like thousands for the weight could appear often with hundred of thousands for the money. When applying the activation function to different order of magnitude this discrepancy will remain, causing the values in the first layer to assume a really broad range of values. This is not good, since high fluctuation means more time to converge to stable values. This is why values fed to neural nets are always standardised. 
The authors applied the same logic to the hidden layers, arguing that a deep neural net can be conceived as a repetition of itself (every hidden layer is an input layer that send features to another hidden layer) ergo the features should be normalised at every layer. How can we do it? Normalising every batch is the most natural way to procede, but in this way there is the risk to end up transforming the internal representation of a layer, because normalising is not a linear transformation. This is way the authors propose a clever way to perform hidden layers normalisation, which consists in a classic normalisation followed by a linear scaling performed with two trainable parameters $\beta$ and $\gamma$ (which appear in the last step of the batch norm scratch code below). 

A really important thing to notice is that the mean and variance used to perform the classic normalisation are mean and variance calculated on the mini batch. I will explain why this is important in a sec, first I want to stress out that the $\beta$ parameter can actually bring to increase overfitting when batch norm is randomly stucked on top of other layer. The reason why is that, as we see in the scale and shift step, $\beta$ is nothing but a biased term, added to perform the shift of the mean of the batch hidden values. So, to avoid having an extra bias that lead to overfit, the bays on the previous layer should be removed, leaving only the classic matrix of weights parameters.
Back to the gradient problem, we can see that in itself doesn't necessarily lead to increased performances, but it does provide an advantage in terms of hidden layer values convergence. The x axis on the two right sub plots of the figure below represent the variation of the hidden values of net trained with and without batch norm. When trained with batch norm the hidden values reach stable ranges after few iteration. This help the net to reach high accuracy in less iterations (first subplot on the left) but we can see that even without batch norm the net reach eventually high accuracy.  

The only help provided by batch norm to the gradient is the fact that, as noticed before, the normalisation is firstly performed by calculating the mean and variance on individual batches. This is important because this partial estimation of mean and variance introduces noice. Similarly to drop out, which has a regularisation effect due to the noice generated by randomly deactivating part of the weights, batch norm can introduce regularisation by adding noice due to the larger or smaller mean and variance estimated on individual batches. But still, batch norm was not introduced as a regularisation technique, and the equation you put on the question simply prove that it is possible to calculate the derivatives of the equations applied to perform the batch norm. 
"
Why does GAN loss converge to log(2) and not -log(2)?,"
In Goodfellow's paper, he says:

Hence, by inspecting Eq. 4 at $D^*_G (\mathbf{x}) = \frac{1}{2}$, we find $C(G) = \log \frac{1}{2}+ \log \frac{1}{2} = − \log 4$. To see that this is the best possible value of $C(G)$

i.e. $D$ and $G$ loss should converge to $\log \frac{1}{2}$. This makes perfect sense. When I train a GAN in PyTorch with BCEloss, the loss for $D$ and $G$ converge to $\log(2)$, the negative of what Goodfellow states and what I'd expect.
What am I missing?
","['objective-functions', 'generative-adversarial-networks', 'papers', 'pytorch']",
Is AlphaStar still competing in the Star Craft ladder?,"
Last year it was announced that Deepmind's Starcraft playing bot AlphaStar was taking on human players in the Starcraft ladder system (some kind of league system as far as I can tell) and that it had reached the Grandmaster level.
Since then I haven't really heard anything anymore about the progress of Alphastar. Given that I don't know anything about Starcraft I was wondering whether somebody has a better clue as to what Alphastar is up to? Is it still playing online? Or when did it stop playing? What was the improvement trajectory during the time it played online? Basically, how did this pan out, as seen from the perspective of the Starcraft community?
","['gaming', 'deepmind']",
What does a joint probability density function have to do with Stochastic Optimal Control and Reinforcement Learning?,"
I stumbled upon a job offer from a company that was looking for someone who was good with Reinforcement Learning (applied to finance) and something in their offer caught my eye. It goes something like this:

We want you to be able to study the price dynamic (of a stock I suppose) and its evolution in order to extract a Joint PDF that will be used in the Optimal Stochastic Control of a Loss Function (or gain)

The thing is I understand what each of these things mean and how they are used separately (from my background in Control theory & dynamical systems) and I worked with fitting Joint PDFs and Copulas before, but I don't understand how a Joint PDF would help with the ""Optimal Stochastic Control of a Loss Function"" ? Thanks.
","['reinforcement-learning', 'probability-distribution', 'control-theory']","
Extracting a joint PDF just means that you create a model that models the behavior of several variables combined instead of in isolation. 
If these variables aren't independent and your loss functions is influenced by all of them, you obviously have to learn this joint PDF to minimize your loss. 
So I don't see this statement as particularly mysterious. 
"
Data classification model to detect a process in an event log,"
There are many example in python which has a ready made data set, for example there is T-Shirt pre-trained data and thousands images, within few minutes it will tell how many t-shirt images are there in those folders
but how that data detect.tflite itself is created from scratch step by step, If I wanted to manually write those data in csv how it should be done
Ultimately I have data which says, click event, keyboard event, process exe name, title of the window, and timestamp
I want to detect what exactly that user doing in his computer, who told do to what to him I want the software to tell me in words and diagrams
this is definitely a big software, but data classification part unclear to me
I need some guidance  
","['classification', 'pattern-recognition']",
Do all expert trajectories have the same starting state in apprenticeship learning?,"
In the apprenticeship learning algorithm described by Ng et al. in Apprenticeship Learning via Inverse Reinforcement Learning, they mention that expert trajectories come in the form of $\{s_0^i, s_1^i\, ...\}_{i=1}^m$. However, they also mentioned that $s_0 $ is drawn from distribution D. Do all expert trajectories then have to have the same starting state? Why is it not possible to compute the feature expectation based on a single trajectory?
","['reinforcement-learning', 'papers', 'rewards', 'apprenticeship-learning']",
Why is my DQN model not getting better?,"
I've created a deep Q network. My model does not get better, and can't see what I'm doing wrong. I'm new to RL.
Replay Memory
class ReplayMemory(object):

def __init__(self, input_shape, mem_size=100000):
    self.states = np.zeros((mem_size, input_shape))
    self.actions = np.zeros(mem_size, dtype=np.int32)
    self.next_states = np.zeros((mem_size, input_shape))
    self.rewards = np.zeros(mem_size)
    self.terminals = np.zeros(mem_size)

    self.mem_size = mem_size
    self.mem_count = 0

def push(self, state, action, next_state, reward, terminal):

    idx = self.mem_count % self.mem_size

    self.states[idx] = state
    self.actions[idx] = action
    self.next_states[idx] = next_state
    self.rewards[idx] = reward
    self.terminals[idx] = terminal

    self.mem_count += 1

def sample(self, batch_size):
    batch_index = np.random.randint(0, min(self.mem_count, self.mem_size), batch_size)

    states = self.states[batch_index]
    actions = self.actions[batch_index]
    next_states = self.next_states[batch_index]
    rewards = self.rewards[batch_index]
    terminals = self.terminals[batch_index]

    return (states, actions, next_states, rewards, terminals)

def __len__(self):
    return min(self.mem_count, self.mem_size)

DQN Agent
class DQN_Agent(object):

  def __init__(self, n_actions,n_states, ALPHA=0.001, GAMMA=0.99, eps_start=1 , eps_end=0.01, eps_decay=0.005):
      self.n_actions = n_actions
      self.n_states = n_states

      self.memory = ReplayMemory(n_states)

      self.ALPHA = ALPHA
      self.GAMMA = GAMMA

      self.eps_start = eps_start
      self.eps_end = eps_end
      self.eps_decay = eps_decay


      self.model = self.create_net()
      self.target = self.create_net()

      self.target.set_weights(self.model.get_weights())

      self.steps_counter = 0

  def create_net(self):

    model = Sequential([
        Dense(64, activation=""relu"", input_shape=(self.n_states,)),
        Dense(32, activation=""relu""),
        Dense(self.n_actions)
    ])

    model.compile(loss=""huber_loss"", optimizer=Adam(lr=0.0005))

    return model

def select_action(self, state):
    ratio = self.eps_end + (self.eps_start-self.eps_end)*np.exp(-1*self.eps_decay*self.steps_counter)
    rand = random.random()
    self.steps_counter += 1

    if ratio > rand:
        #print(""random"")
        return np.random.randint(0, self.n_actions)
    else:
        #print(""not random"")
        return np.argmax(self.model.predict(state))


def train_model(self, batch_size):
    if len(self.memory) < batch_size:
        return None

    states, actions, next_states, rewards, terminals = self.memory.sample(batch_size)

    q_curr = self.model.predict(states)
    q_next = self.target.predict(next_states)
    q_target = q_curr.copy()


    batch_index = np.arange(batch_size, dtype=np.int32)

    q_target[batch_index, actions] = rewards + self.GAMMA*np.max(q_next, axis=1)*terminals

    _ = self.model.fit(states, q_target, verbose = 0)

    if self.steps_counter % 10 == 0:
        self.target.set_weights(self.model.get_weights())

My training loop
n_games = 50000
agent = DQN_Agent(2, 4)

scores = []
avg_scores = []

for epoch in range(n_games):
    done = False
    score = 0
    state = env.reset()

    while not done:
       #env.render()
       action = agent.select_action(state.reshape(1,-1)) 
       next_state, reward, done, _ = env.step(action)


       score += reward

       agent.memory.push(state, action, next_state, reward, done)

       state = next_state
       agent.train_model(64)


   avg_score = np.mean(scores[max(0, epoch-100):epoch + 1])
   avg_scores.append(avg_score)
   scores.append(score)
   print(score, avg_score)

","['deep-learning', 'reinforcement-learning', 'dqn']",
Are No Free Lunch theorem and Universal Approximation theorem contradictory in the context of neural networks?,"
To my understanding NFL states that, we cannot have an hypothesis (let's assume it is an approximator like NN in this case) class that can't achieve certain accuracy parameters $\leq \epsilon$ with probability greater than a certain $p$ given the number of points from which we can sample is upper bounded to $m$.
Whereas, the UAC states that an approximator like a NN given enough hidden units can approximate any function (to my knowledge the function must be bounded).
The point where these 2 clashes (as per my knowledge) is that if we increase the paramters in a NN, the UAC will start to hold good, but the VC dimension will increase (or hypothesis class becomes richer) and for the same $m$ our $\epsilon$ increases or $p$ decreases (not sure which one is affected).
So what are the gaps in my knowledge here? How do we make these 2 consistent with each other?
","['neural-networks', 'comparison', 'computational-learning-theory', 'no-free-lunch-theorems', 'universal-approximation-theorems']",
Can't solve Towers of Hanoi in PDDL,"
I'm using PDDL to generate a plan to solve this tower of Hanoi puzzle. I'll give the problem, the rules, the domain and fact sheet for everything.
PDDL is telling me that the goal can be simplified to false; however, I know for a fact that this puzzle is solvable.
Puzzle:


There are 3 posts. Each has rings on it. From bottom to top on each post. The first post has the
  second largest ring. The second post has the smallest ring, with the second smallest ring on top of it.
  The third post has the third largest ring, with the largest ring stacked on top of it.

Rules:

The rules of this game are that you may only stack a ring on top of a larger ring. Your goal is to get all of the rings onto the same post, stacked from largest to smallest.

My Code
Domain
(define (domain hanoi)
  (:requirements :strips)
  (:predicates (clear ?x) (on ?x ?y) (smaller ?x ?y))

  (:action move
    :parameters (?disc ?from ?to)
    :precondition (and (smaller ?to ?disc) (on ?disc ?from) 
               (clear ?disc) (clear ?to))
    :effect  (and (clear ?from) (on ?disc ?to) (not (on ?disc ?from))  
          (not (clear ?to))))
  )

Problem
(define (problem hanoi5)
  (:domain hanoi)
  (:objects peg1 peg2 peg3 d1 d2 d3 d4 d5)
  (:init 
    (smaller peg1 d1) (smaller peg1 d2) (smaller peg1 d3)
    (smaller peg1 d4) (smaller peg1 d5)
    (smaller peg2 d1) (smaller peg2 d2) (smaller peg2 d3)
    (smaller peg2 d4) (smaller peg2 d5)
    (smaller peg3 d1) (smaller peg3 d2) (smaller peg3 d3)
    (smaller peg3 d4) (smaller peg3 d5)

    (smaller d2 d1) (smaller d3 d1) (smaller d3 d2) (smaller d4 d1)
    (smaller d4 d2) (smaller d4 d3) (smaller d5 d1) (smaller d5 d2)
    (smaller d5 d3) (smaller d5 d4)

    ;(clear peg2) (clear peg3) (clear d1)
    ;(on d5 peg1) (on d4 d5) (on d3 d4) (on d2 d3) (on d1 d2))
    (clear d2) (clear d4) (clear d1)
    (on d2 peg1) (on d5 peg2) (on d4 d5) (on d3 peg3) (on d1 d3))

  (:goal (and (on d5 d4) (on d4 d3) (on d3 d2) (on d2 d1)))
)

I'm really at a loss here. Thank you!
","['planning', 'pddl']",
Why does the variational auto-encoder use the reconstruction loss?,"
VAE is trained to reduce the following two losses.

KL divergence between inferred latent distribution and Gaussian.

the reconstruction loss


I understand that the first one regularizes VAE to get structured latent space. But why and how does the second loss help VAE to work?
During the training of the VAE, we first feed an image to the encoder. Then, the encoder infers mean and variance. After that, we sample $z$ from the inferred distribution. Finally, the decoder gets the sampled $z$ and generates an image. So, in this way, the VAE is trained to make the generated image to be equal to the original input image.
Here, I cannot understand why the sampled $z$ should make the original image, since the $z$ is sampled, it seems that the $z$ does not have any relationship between the original image.
But, as you know, VAE works well. So I think I miss something important or understand it in a totally wrong way.
","['deep-learning', 'math', 'variational-autoencoder', 'evidence-lower-bound']",
Is there any way of generating fixed-length sequences with RNNs?,"
Is there any way of generating fixed-length sequences with RNNs? I want to tell my character level RNN to generate a name of length 3, 4, 5 and so on. I haven't found anything online like this, but my intuition tells me that, if I append the sequence length (e.g. 5) at each RNN input, the RNN should be able to do this. Does anyone know if this task is possible?
","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'generative-model']",
Are there known error bounds for TD(0) with a constant learning rate?,"
Is there any known error bounds for the TD(0) algorithm for the value function after a finite number of iterations?
$$ \Delta_t=\max_{s \in \mathcal{S}}|v_t(s)-v_\pi(s)|$$
$$v_{t+1}(s_t)=v_t(s_t)+\alpha(r+v_t(s_{t+1})-v_t(s_t))$$
","['reinforcement-learning', 'temporal-difference-methods']","
The paper ""Bias-Variance"" Error Bounds for Temporal Difference Updates (2000) by M. Kearns and S. Singh provides error bounds for temporal-difference algorithms, i.e. TD($k$) and TD($\lambda$) (see theorem 1 and theorem 2, respectively). Note that both TD($k$) and TD($\lambda$) include TD($0$) as a special case.
"
What does a consistent heuristic become if an edge is removed in A*?,"
For the A* algorithm, a consistent heuristic is defined as follows:
if, for every node $n$ and every successor $m$ of $n$ generated by any action $a$, $h(n) \leq c(n,m) + h(m)$. 
Suppose the edge $c(n,m)$ is removed, how do we define consistency for nodes $n, n'$ ? Since the $n'$ is not generated from $n$.
","['search', 'a-star', 'consistent-heuristic']",
What are some online courses for deep reinforcement learning?,"
What are some (good) online courses for deep reinforcement learning?
I would like the course to be both programming and theoretical. I really liked David Silver's course, but the course dates from 2015. It doesn't really teach deep Q-learning at this time.
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl', 'resource-request']","
For the programming part I suggest this YouTube channel by Phil Tabor (he also has a website: neuralnet.ai. I found his videos really useful while I was attending reinforcement learning classes at the uni. He covers basic algorithms like value iteration and policy iteration and also more advanced like deep q learning, covering all main python libraries (Keras, tensorflow, pytorch). Hope it will help you as well! 
"
Why are conics important in computer vision?,"
The book Multiple View Geometry in Computer Vision by Richard Hartley and Andrew Zisserman talks about lines, points and conics. A conic is a curve described by a second-degree equation in the plane, so a parabola would be an example of a conic. The purpose and usage of points and lines in computer vision are quite clear. For example, a set of points defines an object, or we can project a 3-dimensional point to a 2-dimensional point in the image plane, or a line represents the space to look for the corresponding point in the second plane of another point in the first plane (epipolar geometry). However, probably because I haven't yet read the part of the book related to the applications of conics in computer vision, it's not clear why do we even care about conics in computer vision. 
So, why are conics important in computer vision? Note that I know that conics are defined by points and, given the point-line duality, they can also be defined by lines, but this doesn't still enlightens me on the purpose of conics in computer vision. So, I am looking for applications where conics are used to define the underlying CV model, in a similar way that points and lines are used to describe the pinhole camera model.
","['computer-vision', 'math', 'applications']",
"What are the pros and cons of supervised, semi-supervised and unsupervised relation extraction in NLP?","
I am following the NLP course taught by Dan Jurafsky. In the video lectures Supervised Relation Extraction and Semi Supervised and Unsupervised Relation Extraction Jurafsky explains supervised, semi-supervised and unsupervised relation extraction.
But what are the pros and cons of every relation extraction method compared with the other two relation extraction methods?
","['natural-language-processing', 'comparison', 'unsupervised-learning', 'supervised-learning', 'semi-supervised-learning']","
Supervised
Pros:

highest accuracy

Cons:

need a large human-labeled training set
brittle (doesn't work well with examples that are in a different genre from the training set)

Semi-supervised
Relation bootstrapping
Pros:

only requires a small set of labeled data (seed relations)

Cons:

complex iterative process

Distant supervision
Pros:

training happens in one go (no iterative process)

Cons:

requires a big database of relations

Unsupervised
Pros:

don't need any labeled data

Cons:

need to process a huge quantity of unlabeled data (usually web crawling)

"
Automated explanation - function results - simple attempt,"
I used to work as an analyst in a financial project where we had functions $f$ determining the price, and sometimes the inputs $x$ jumped in such a way to produce anomalous results.
We had to report an explanation, and I wish to automate the process.
It's not properly a question of AI, more of information science.
The idea is that once, for a generic non-linear $f$, you can determine the ranking of relevance of $x_i$ in explaining the result, you can generate a full explanation by:

decompose $f$ as a composition of $f_j$, which are intermediate results with a definite meaning in the application domain (in this case, finance)
apply the algorithm using the $f_j$ instead of $x_i$, and then iterate it to explain the $f_j$ in terms of $x_i$

The relevance is quantified by the information gain of each variable. This will be explained for an application in ranking the $x_i$ directly.
We assume to start on a uniform distribution on the $x$ domain, calculate the derived probability density function for $f$, and the information entropy of $f$.
Then we fix the $x_i$ one at a time, for each calculate the new p.d.f. of $f$ conditioned on that $x_i$ and the (lower) information entropy of $f$. The information gain is $IG(x_i)$.
Choose as the first conditioning the $i$ with the largest information gain, then condition of the remaining $i$ with a decreasing order of $IG_i$.
So we could start for example , with $(x_1,x_2,x_3)$, to condition first on $x_2$, then on $(x_2,x_3)$, and then on $(x_2,x_3,x_1)$, getting the percentage contributions as:
$\frac{IG_i}{H_y}$. The successive terms $IG_i$ add up always to the total entropy $H_y$, since conditioning on all variables gives a point and zero entropy.
Any opinion on how to improve this ""automated function explanation"" is welcome
",['algorithm'],
What are the main points of the top-down vs bottom-up paradigm in neural networks?,"
I've been reading some papers on human pose estimation and I'm starting to see the terms top-down and bottom-up crop up a lot. For example in this paper:

Our hourglass module differs from these designs mainly
  in its more symmetric distribution of capacity between bottom-up processing
  (from high resolutions to low resolutions) and top-down processing (from low
  resolutions to high resolutions)

Okay, so what are the main observations or distinctions of top-down vs bottom-up? Why does it make sense to have a paradigm in which we talk about these specifically?
",['convolutional-neural-networks'],
Agents meeting in a directed connected graph,"
We have a directed connected graph, where nodes are places one can go to and edges are the ""roads"" between places. We have K agents whose goal is to meet in one node. Agents start in different nodes. Note that more than one agent can be at one node at the same time and that all agents move by one node at every turn(they move synchronously).
We have to variants of this task:

in each turn every agent must move

an agent may pass on moving.


For a chosen variant, I have to find an algorithm to complete this task, but it cannot be the state-space search algorithm.
I've been sitting on this for a while, but I cannot think of anything.
I've been thinking if agents could know each other positions in order to choose where to go, but it's a state-space search. I also thought that if agents meet, they could continue together. But I'm looking for an alternative to state-space search algorithms.
","['search', 'intelligent-agent', 'heuristics', 'multi-agent-systems']","
It's not possible to solve version 1) of the problem in general. To see why, consider a graph with 2 cities, and 2 agents, where the agents start in opposite nodes. Since both agents need to move every turn, they will never meet in the same city.
For version 2), I'm going to make some assumptions that aren't completely clear from your text:

Agents have only local information. They can't see the global structure of the graph, and just all agree to meet at city X, because they don't know that X exists. They also can't see where other agents are initially.
Cities are labelled, or otherwise identifiable to the agents. That is, an agent can tell whether they've visited a city before or not.
Agents can tell how many agents are in their current city, and in all adjacent cities.
There are a finite number of cities, and it's always possible to get from city A to city B, for any pair of cities (A,B).
Agents have no way to communicate with each other, except by being present in the same space or not.

In this setup, the following heuristic will always work:

If you've never seen another agent, move to a random adjacent city, or stay in place, with a uniform probability of taking each possible action.
If you've never seen another agent before, but you see one or more agents now, then switch permanently to the following strategy:

If you can see an adjacent city with more agents than your city, move there now. If you can see several such cities, move to the largest one. Pick randomly in case of a tie. 
If an adjacent city has the same number of agents as yours, flip a coin to decide whether to go or stay. Eventually, one city ends up with more than the other, and then step 1 causes all agents to end up in the same city.
Otherwise, if you can see other adjacent cities with fewer agents, just stay put. They'll come to you.
If all adjacent agents are empty and there's a total of $n$ agents you can see in your city right now, then remember $n$.
As long as you still can't see more than $n$ agents in total, stay put with probability $(n-1)/n$ and move to a random adjacent city with probability $1/n$. 
If you stayed put per step 5, and you still can only see n agents, and all but one of them is in your current city, move to the city with the single agent who moved. Otherwise, if 0 or more than 1 agent moved, stay put. The agents who moved come back, and we go to step 5.
After successfully moving, go to step 1 again.


You can see that once agents meet up in a group, that group never shrinks. Further, groups of agents are always trying to move around together. You can prove that a group moves an average of about $\frac{1}{2e}$ every step. It may take a while, but eventually every group will bump into every other group, resulting in all the agents being in the same city.
"
Can an artificial intelligence be unbeatable at simple games?,"
There are (two-players, perfect information) combinatorial games for which, at any configuration of the game, a winning move (if there is one) can be quickly computed by a short program. This is the case of the following game that starts with a bunch of matches and each player alternatively removes 1,2 or 3 matches, until the player that removes the last one wins. 
This is also the case of the Nim game. 
On the other hand, understanding the winning strategy of games like Go or chess seems hopeless. 
However, some machine-learning based programs like alphaGo zero are able ""learn the strategy"" of complex games, using as input data only the rules of the game. I don't really know how these algorithms work but here is my vague question:
For simple game like Nim, can such an algorithm be able to actually find a winning move in any winning configuration of the game ?
The number of configurations of Nim is infinite, but the algorithm will consider during its ""training"" only a finite number of configurations. It seems imaginable that if this training phase is long enough, then the program will be able to capture the winning strategy, like a human would do.
","['machine-learning', 'combinatorial-games']","
There is actually a github project about 'solving' Nim that implements certain type of Q-learning reinforcement algorithm (described in undergraduate thesis of Erik Jarleberg (Royal Institute of Technology) entitled ""Reinforcement learning on the combinatorial game of Nim"") that supposedly finds that optimal strategy lying down there inside in the game that also human can find out.
The project uses a code in Python, which is 'only' 114 lines of code, so it can be run on your own machine if you got interested and want to test it. Github page tells also:

There is a known optimal strategy for the game which is credited to Charles L. Bouton of Harvard which can serve as a benchmark for evaluating the performance on our Q-learning agent.

(That is the quote I refer for it getting enough good / optimal results).
Q-learning is part of the reinforcement learning algorithm family where the so called agent learns how to play the game by getting rewards from its actions and it tries to maximize the amount of total reward with following strategy:

The goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state. 

Quote and more information in Q-learning Wikipedia-page.
For further interest on easy games tackled with Reinforcement Learning, please have a look on following paper: Playing Atari with Deep Reinforcement Learning. There is also an Udemy class on that paper and its findings.
"
Can neurons in MLP and filters in CNN be compared?,"
I know they are not the same in working, but an input layer sends the input to $n$ neurons with a set of weights, based on these weights and the activation layer, it produces an output that can be fed to the next layer.
Aren't the filters the same, in the way that they convert an ""image"" to a new ""image"" based on the weights that are in that filter? And that the next layer uses this new ""image""?
","['convolutional-neural-networks', 'multilayer-perceptrons', 'artificial-neuron', 'weights', 'filters']","
The other answer gives a good overview of the differences between MLPs and CNNs, and it includes 2 diagrams that attempt to illustrate the main differences between MLPs and CNNs, i.e. sparse connectivity and weight sharing. However, these diagrams do not clarify what a neuron in a CNN could be. A better diagram, which illustrates what a neuron is in a CNN, from a CNN and MLP perspective, is the following (taken from the famous article on CNNs).

Here, there are 2 main blocks (aka volumes): the orange block on the left (the input) and the blue/cyan volume on the right (the feature maps, i.e. the outputs of the convolutional layer, i.e. after the application of the convolutions with different kernels).
The circles in the visible stack of the cyan block represent the neurons (or, more precisely, their activations or outputs). We only see $k=5$ neurons stacked: this corresponds to the application of $k=5$ different kernels (i.e. weights) to that specific subset of the input (aka receptive field), hence the sparse connectivity of CNNs. So, these neurons, in the same stack, are looking at the same small subset of the input, but with different weights (i.e. kernels). The neurons, which are not shown in this diagram, that are on the same (vertical) 2d plane (known as feature map) of the same neuron (e.g. the first that we see from left to right) in the cyan volume are the neurons that share the same weights, i.e. we use the same kernel to produce their outputs.
So, in this biological/neuroscientific view of the CNN, when you apply the convolution (or cross-correlation) operation with 1 specific filter (or kernel), you are computing the activation (not to be confused with the activation function, which is used to compute the activation!) i.e. the output of multiple neurons, all of them share the same weights. You stack all these activations on the same 2d plane (known as feature map) of the output volume: note that this operation is just the convolution operation! When you compute the convolution with another kernel, you are again computing the activation of other multiple neurons, which share another different weight matrix, and so on and so forth.
Some authors prefer to use the term convolutional networks, i.e. without the term neural, probably because of this issue, i.e. it's not clear, especially to newcomers, what a neuron would be in a CNN, so the neuroscientific/biological view of CNNs is not always clear, although it's important to emphasize that CNNs were inspired by the visual cortext, so this biological interpretation could (and should) be more widely known or less confusing/misunderstood.
Now, let's address your question more directly.

Aren't the filters the same, in the way that they convert an ""image"" to a new ""image"" based on the weights that are in that filter? And that the next layer uses this new ""image""?

The filters in a CNN correspond to the weights of an MLP.
A neuron in a CNN can be viewed as performing exactly the same operation as a neuron in an MLP. The big differences between a CNN and an MLP (as explained also in the other answer) are

Weight sharing: Some neurons (not all!) in the same convolutional layer share the same weights. The convolution (or cross-correlation) is the operation that implements this partial forward pass with the same weights for different neurons.

Neurons in a CNN only look at a subset of the input and not all inputs (i.e. receptive field), which leads to some notion of sparse connectivity

A convolutional layer, in a CNN, is composed of neurons in a 3d dimensional volume (or, more precisely, their activations are organized in a 3d volume), rather than a 1-dimensional one, as in an MLP.

CNNs may use subsampling (aka pooling)


"
What could cause a big fluctuation of the loss in the last epochs of training an AlexNet?,"
I am training an AlexNet neural network, with about 12000 images which 80% is for training, 10% is for validation and another 10% is for testing.
I have a problem in my plots. There is a big fluctuation in epoch 47,
how can I have a smooth plot? What is the problem?

I tried to increase my validation data because the fluctuation was for validation loss. but nothing changed.
I decreased learning rate, but it stucks in a local optimum.
","['neural-networks', 'deep-learning', 'training', 'alexnet', 'data-augmentation']",
Non-linear regression with a neural network,"
I have to perform a regression on three curves as shown in the following plot. Here, accA (y-axis) is the dependent variable, and w (x-axis) is the independent variable. The sum of the three curves adds up to 1. 
To perform regression for the three curves, I would like to use a neural network. What architecture/set-up should I use for this task?

","['neural-networks', 'ai-design', 'regression']",
Is there an up-to-date list of suitable kernels for Gaussian processes?,"
Consider a stochastic process $\{X_t \colon t \in T\}$ indexed by a set $T$. We assume for simplicty that $T \in \mathbb{R}^n$. We assume that for any choice of indexes $t_1, \dots, t_n$, the random variables $(X_{t_1}, \dots, X_{t_n})$ are jointly distributed according to a multivariate Gaussian distribution with mean $\mu = (0, \dots, 0)$, for a given covariance matrix $\Sigma$.
Under these assumptions, the stochastic process is completely determined by the 2nd-order statistics. Hence, if we assume a fixed mean at $0$, then the stochastic process is fully defined by the covariance matrix. This matrix can be defined in terms of the covariance function
$$
k(t_i, t_j) = \mbox{cov}(X_{t_i}, X_{t_j}). 
$$
It is well-known that functions $k$ as defined above are admissible kernels. This fact is often used in probabilistic inference, when performing regression or classification.
Several functions can be suitable kernels, but only a few are used in practice, depending on the application.
Given a large amount of related literature, can someone provide an up-to-date list of functions commonly used as kernels in this context?
","['machine-learning', 'reference-request', 'gaussian-process']",
Is A2C loss function taking smaller steps for larger mistakes?,"
A2C loss is usually defined as advantage * (-log(actor_predictions)) * target where target is a one-hot vector (with some clipping/noise/etc...) with the selected target. 
Does this mean that we get larger losses for smaller mistakes?
If for example the agent has predicted $\pi(a|s)=0.9$ but the advantage is negative, this would mean a larger mistake than if the agent predicted that $\pi(a|s)=0.1$, however, putting the numbers in the formula means a larger loss for the 0.1 prediction.
Assuming advantage=-1, advantage * (-log(actor_predictions)) * target would mean:
$$
-1 * (-log(0.9)) * 1 = log(0.9)=-0.045
$$
$$
-1 * (-log(0.1)) * 1 = log(0.1)=-1
$$
Is my understanding correct?
","['objective-functions', 'policy-gradients', 'advantage-actor-critic']",
Can you train Transformers sequentially?,"
I’m currently trying to train a BART, which is a denoising Transformer created by Facebook researchers. Here’s my Transformer code
import math
import torch
from torch import nn
from Constants import *

class Transformer(nn.Module):
    def __init__(self, input_dim: int, output_dim: int, d_model: int = 200, num_head: int = 8, num_e_layer: int = 6,
                 num_d_layer: int = 6, ff_dim: int = 1024, drop_out: float = 0.1):
        '''
        Args:
            input_dim: Size of the vocab of the input
            output_dim: Size of the vocab for output
            num_head: Number of heads in mutliheaded attention models
            num_e_layer: Number of sub-encoder layers
            num_d_layer: Number of sub-decoder layers
            ff_dim: Dimension of feedforward network in mulihead models
            d_model: The dimension to embed input and output features into
            drop_out: The drop out percentage
        '''
        super(Transformer, self).__init__()
        self.d_model = d_model
        self.transformer = nn.Transformer(d_model, num_head, num_e_layer, num_d_layer, ff_dim, drop_out,
                                          activation='gelu')
        self.decoder_embedder = nn.Embedding(output_dim, d_model)
        self.encoder_embedder = nn.Embedding(input_dim, d_model)
        self.fc1 = nn.Linear(d_model, output_dim)
        self.softmax = nn.Softmax(dim=2)
        self.positional_encoder = PositionalEncoding(d_model, drop_out)
        self.to(DEVICE)

    def forward(self, src: torch.Tensor, trg: torch.Tensor, src_mask: torch.Tensor = None,
                trg_mask: torch.Tensor = None):
        embedded_src = self.positional_encoder(self.encoder_embedder(src) * math.sqrt(self.d_model))
        embedded_trg = self.positional_encoder(self.decoder_embedder(trg) * math.sqrt(self.d_model))
        output = self.transformer.forward(embedded_src, embedded_trg, src_mask, trg_mask)
        return self.softmax(self.fc1(output))

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)    
        self.register_buffer('pe', pe)

and here’s my training code
def train(x: list):
    optimizer.zero_grad()
    loss = 0.
    batch_sz = len(x)
    max_len = len(max(x, key=len)) + 1  # +1 for EOS xor SOS
    noise_x = noise(x)
    src_x = list(map(lambda s: [SOS] + [char for char in s] + [PAD] * ((max_len - len(s)) - 1), noise_x))
    trg_x = list(map(lambda s: [char for char in s] + [EOS] + [PAD] * ((max_len - len(s)) - 1), x))
    src = indexTensor(src_x, max_len, IN_CHARS).to(DEVICE)
    trg = targetsTensor(trg_x, max_len, OUT_CHARS).to(DEVICE)
    names = [''] * batch_sz

    for i in range(src.shape[0]):
        probs = transformer(src, trg[:i + 1])
        loss += criterion(probs, trg[i])

    loss.backward()
    optimizer.step()

    return names, loss.item()

As you can see in the train code. I am training it ""sequentially"" by inputting the first letter of the data then computing the loss with the output then inputting the first and second character and doing the same thing, so on and so forth.
This doesn’t seem to be training properly though as the denoising is totally off. I thought maybe there’s something wrong with my code or you can’t train Transformers this way.
I'm taking first name data then noising it then training the Transformer to denoise it, but the output to the Transformers doesn't look remotely like the denoised version or even the noised version of the name. I built a denoising autoencoder using LSTMs and it did way better, but I feel like BART should be way out performing LSTMs cause it's supposedly state of the art NLP neural network model.
","['natural-language-processing', 'pytorch', 'transformer']","
I assume you're referring to this paper, which basically ""reuses"" the same architecture as the original ""Attention is all you need"" paper for machine translation; the difference being that source and target are noisy and original sentences.
I can't figure the for loop logic inside your train function; but reading your comment, you don't have to traverse the target sequentially. Instead, we rely on masks which exclude paddings, and most importantly help with implementing teacher forcing for the decoder stack. Check this post and how it creates the three types of masks needed (src, target, no_peek). 
Having said that, you will need a for loop when serving your model (inference), because you will have to decode autoregressively. That is, you'll have to decode a token first, and feed it back to the decoder to generate the next token (until you reach EOS). Check ""Testing the model"" section in this article. 
"
Is it better to rely on an intention file or a database for a web chatbot?,"
Currently, I'm making a chatbot that is going to be functioning in a website, so I was wondering, is it better to train the chatbot with intentions files or use the database as the intention file, if it the latter, then how would I do it? With SQLite or with Excel? Any guides or tutorial would be appreciated. 
I'm planning to use Flask + Python + Html for the chatbot.
","['training', 'chat-bots']","
Recognising intents is only a small step in developing a chatbot. It's fine to use an ML classifier with training data for that, no need to keep the original list of intents.
However, you should really think about the next step: how are you getting your bot to conduct a dialogue, rather than firing off single responses to user queries. That is where things get difficult, and that is also what distinguishes a good chatbot from a simple-minded Eliza-clone.
The programming language/framework you use is not relevant.
"
MCTS moves with multiple parents,"
I'd like to develop an MCTS-like (Monte Carlo Tree Search) algorithm for program induction, i.e. learning programs from examples.
My initial plan is for nodes to represent programs and for the search to expand nodes by revising programs. Each edge represents a possible revision.
Many expansions involve a single program: randomly resample a subtree of the program, replace a constant with a variable, etc. It's straightforward to use these with MCTS.
Some expansions, however, generate a program from scratch (e.g. sample a new program). Others use two or more programs to generate a single output program (e.g. crossover in Genetic Programming).
These latter types of moves seem nonstandard for vanilla MCTS, but I'm not deeply familiar with the literature or what technical terms might be most relevant.
How can I model expansions involving $N \ne 1$ nodes? Are there accepted methods for handling these situations?
","['monte-carlo-tree-search', 'monte-carlo-methods', 'computer-programming']","
Approach 1: One way would be to switch from nodes representing programs to nodes representing sets of programs. The root node would represent the empty set ${}$, to which expansions could be applied only if they can generate a program from scratch. The first such expansion would produce some program $p$, so the root would now have child ${p}$. The second expansion would produce $p'$, so the root would now also have child ${p'}$ which would itself also have the child ${p,p'}.
One downside to this approach is that, even assuming reasonable restrictions (e.g. moves can use at most 2 programs, pairs cannot have identical elements, element order doesn't matter), the branching factor will grow combinatorially, because each new program can be combined with all (or most) of the previously generated programs.
Approach 2: Another way would be to switch from nodes representing programs to nodes representing meta-programs describing an initial program and a series of revisions to that program. For example, MCTS might search for expressions in something like this grammar:
Program  -> Empty
          | Program > OneProgramRevision
          | Program Program >> TwoProgramRevision
OneProgramRevision -> AddDatumAsException
                    | SampleStatement
                    | RegenerateSubtree
                    | Delete
                    | ...
TwoProgramRevision -> Concatenate
                    | Crossover
                    | ...

This is just a cartoon: one would need to add details parameterizing each of the various revisions.
"
Which one is better: multivariate regression with basis expansion or neural networks?,"
Assume we are given a training dataset $D = \{ (x_i, y_i)\}_{i=1}^{N}$.
My question is: which is better?

A multivariate regression with basis expansion with independent matrix $X$ and dependent matrix $Y$, such that $X \in K; K \subset \mathbb R^n$ and $Y \in \mathbb R^m$ with training data $D$.

Or

A neural network which takes $n$ input variables and returns $m$ output with training data $D$


Without a doubt, the multivariate regression option is better with its basis polynomials because it can adapt any curve required in any dimension and doesn't need a large number of datasets than neural networks. Then, why neural networks are used more than multivariate regression?

Note: Prefer explaining the mechanism of neural network used as regression in your answers. To help us know the degree of flexibility of both.
Edit: You may prefer choosing your own loss function in case you need.
","['neural-networks', 'comparison', 'regression']",
Simple three layer neural network with backpropagation is not approximating tanh function,"
I have this simple neural network in Python which I'm trying to use to aproximation tanh function. As inputs I have x - inputs to the function, and as outputs I want tanh(x) = y. I'm using sigmoid function also as an activation function of this neural network. 
import numpy
# scipy.special for the sigmoid function expit()
import scipy.special
# library for plotting arrays
import matplotlib.pyplot
# ensure the plots are inside this notebook, not an external window
%matplotlib inline

# neural network class definition
class neuralNetwork:


    # initialise the neural network
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
        # set number of nodes in each input, hidden, output layer
        self.inodes = inputnodes
        self.hnodes = hiddennodes
        self.onodes = outputnodes

        # link weight matrices, wih and who
        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer
        # w11 w21
        # w12 w22 etc 
        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))

        # learning rate
        self.lr = learningrate

        # activation function is the sigmoid function
        self.activation_function = lambda x: scipy.special.expit(x)  

        pass


    # train the neural network
    def train(self, inputs_list, targets_list):
        # convert inputs list to 2d array
        inputs = numpy.array(inputs_list, ndmin=2).T
        targets = numpy.array(targets_list, ndmin=2).T

        # calculate signals into hidden layer
        hidden_inputs = numpy.dot(self.wih, inputs)
        # calculate the signals emerging from hidden layer
        hidden_outputs = self.activation_function(hidden_inputs)

        # calculate signals into final output layer
        final_inputs = numpy.dot(self.who, hidden_outputs)
        # calculate the signals emerging from final output layer
        final_outputs = self.activation_function(final_inputs)

        # output layer error is the (target - actual)
        output_errors = targets - final_outputs
        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes
        hidden_errors = numpy.dot(self.who.T, output_errors) 

        # BACKPROPAGATION & gradient descent part, i.e updating weights first between hidden
        # layer and output layer, 
        # update the weights for the links between the hidden and output layers
        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))

        # update the weights for the links between the input and hidden layers, second part of backpropagation.
        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))
        pass


    # query the neural network
    def query(self, inputs_list):
        # convert inputs list to 2d array
        inputs = numpy.array(inputs_list, ndmin=2).T

        # calculate signals into hidden layer
        hidden_inputs = numpy.dot(self.wih, inputs)
        # calculate the signals emerging from hidden layer
        hidden_outputs = self.activation_function(hidden_inputs)

        # calculate signals into final output layer
        final_inputs = numpy.dot(self.who, hidden_outputs)
        # calculate the signals emerging from final output layer
        final_outputs = self.activation_function(final_inputs)

        return final_outputs

Now I try to query this network, This network has three input nodes one for each x, one node for each input. This network also has 3 output nodes, so It would classify the inputs to given outputs. Where outputs are y, y = tanh(x) function. 
# number of input, hidden and output nodes
input_nodes = 3
hidden_nodes = 8
output_nodes = 3
learning_rate = 0.1

# create instance of neural network
n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)

realInputs = []
realInputs.append(1)
realInputs.append(2)
realInputs.append(3)

# for x in (-3, 3):
#     realInputs.append(x)
#     pass

expectedOutputs = []
expectedOutputs.append(numpy.tanh(1));
expectedOutputs.append(numpy.tanh(2));
expectedOutputs.append(numpy.tanh(3));

for y in expectedOutputs:
    print(y)
    pass

training_data_list = []

# epochs is the number of times the training data set is used for training
epochs = 200

for e in range(epochs):
    # go through all records in the training data set
    for record in training_data_list:
        # scale and shift the inputs
        inputs = realInputs
        targets = expectedOutputs
        n.train(inputs, targets)
        pass
    pass

n.query(realInputs)

Outputs: desired vs ones from network with same data as training data:
0.7615941559557649
0.9640275800758169
0.9950547536867305


array([[-0.21907413],
       [-0.6424568 ],
       [-0.25772344]])

My results are completely wrong. I'm a beginner with neural networks so I wanted to build neural network without frameworks like tensor flow... Could someone help me? Thank you. 
","['neural-networks', 'backpropagation']","
This is because of Vanishing Gradient Problem
What is Vanishing Gradient Problem ?
when we do Back-propagation i.e moving backward in the Network and calculating gradients of loss(Error) with respect to the weights , the gradients tends to get smaller and smaller as we keep on moving backward in the Network. This means that the neurons in the Earlier layers learn very slowly as compared to the neurons in the later layers in the Hierarchy. The Earlier layers in the network are slowest to train.
Reason
Sigmoid function, squishes a large input space into a small input space between 0 and 1. Therefore a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small. 
Solution:
Use Activation function as ReLu

Reference:
Vanishing Gradient Solution
"
Can neural networks with a sigmoid as the activation function of the output layer approximate continuous functions?,"
Neural networks are commonly used for classification tasks, in fact from this post it seems like that's where they shine brightest.  
However, when we want to classify using neural networks, we often have the output layer to take values in $[0,1]$; typically, by taking the last layer to be the sigmoid function $x \mapsto \frac{e^x}{e^x +1}$. 
Can neural networks with a sigmoid as the activation function of the output layer approximate continuous functions? Is there an analogue to the universal approximation theorem for this case? 
","['neural-networks', 'classification', 'definitions', 'computational-learning-theory', 'sigmoid']",
What are pros and cons of Bi-LSTM as compared to LSTM?,"
What are the pros and cons of LSTM vs Bi-LSTM in language modelling? What was the need to introduce Bi-LSTM?
","['natural-language-processing', 'comparison', 'long-short-term-memory', 'language-model', 'bidirectional-lstm']","
I would say that the logic behind the introduction was more empirical than technical. The only difference between LSTM and Bi-LSTM is the possibility for Bi-LSTM to leverage future context chunks to learn better representations of single words. There is no special training step or units added, the idea is just to read a sentence forward and backward to capture more information.
And as trivial as the idea sounds, it works, in fact, in the original paper the authors managed to achieve state-of-the-art scores in three tagging tasks, namely part-of-speech tagging, chunking and named entity recognition.
Even though it must be said that these scores were not dramatically higher compared to other models, and also the complete architecture included a Conditional Random Field on top of the Bi-LSTM.
Probably the most important aspect to stress out is that the authors performed two interesting comparison tests: one using random embedding initialisation and another one using only words (unigrams) as input features. Under these two test conditions, Bi-LSTM (with CRF on top) outperformed significantly all other architectures, proving that Bi-LSTM representations are more robust than representation learned by other models.
I would like also to make a side note regarding human reading. It makes sense to consider unidirectional sequence models as the most reasonable to emulate human reading, because we experience reading as a movement of the eyes that goes from one direction to the opposite. But the reality is that saccades (really rapid unconscious eye movement) and other eye movements play an enormous rule in reading. Which means that also we humans do continuously look to past and future words as well in order to understand the purpose of a word or sentence we're processing. Of course in our case these movements are directed by implicit knowledge and habits that allow us to direct our attention only to important words/parts (for example we barely read conjunctions) and it is interesting to notice that now state-of-the-art models based on transformers try to learn exactly this, where to pay attention rather than single probabilities for each word in a vocabulary.
"
"Representation of state space, action space and reward system for Reinforcement Learning problem","
I am trying to solve the problem of an agent dynamically discovering(start with no information about the environment) the environment and to explore as much of the environment as possible without crashing into obstacles
I have the following environment:

where the environment is a  matrix. In this the obstacles are represented by 0's and the free spaces are represented with 1s. The position of the agent is given by a label such as 0.8 in the matrix.
The initial internal representation of the environment of the agent will look something like this with the agent position in it .

Every time it explores the environment it keeps updating its own map:

The single state representation is just the matrix containing-

0 for obstacles
1 for unexplored regions 
0.8 for position of the agent
0.5 for the places it has visited once
0.2 for the places it has visited more than once

I want the agent to not hit the obstacles and to go around them.
The agent should also not be stuck in one position and try to finish the exploration as quickly as possible.
This is what I plan to do:
In order to prevent the agent from getting stuck in a single place, I want to punish the agent if it visits a single place multiple times. I want to mark the place the agent has visited once as 0.5 and if it has visited it more than once that place will be labelled 0.2
The reason I am marking a place it has visited only once as 0.5 is because if there is a scenario where in the environment there is only one way to go into a region and one way to come out of that region, I don't want to punish this harshly. 
Given this problem, I am thinking of using the following reward system-

+1 for every time it takes an action that leads to an unexplored region
-1 for when it takes an action that crashes into an obstacle
0 if it visits the place twice(i.e 0.5 scenario)
-0.75  is it visits a place more than twice

The action space is just-

up 
down
left
right

Am i right in approaching the problem this way? Is Reinforcement Learning the solution for this problem?
Is my representation of the state , action, reward system correct?
I am thinking that DQN is not the right way to go because the definition of a terminal state is hard in this problem, what method should I use to solve this problem?
","['reinforcement-learning', 'dqn', 'policy-gradients', 'policies']",
Why is the loss associated with my neural network increasing?,"
I am currently learning neural networks using data from Touchscreen Input as a Behavioral Biometric. Basically, I am trying to predict ""User ID"" by training the neural network model shown below.
import time
import os
BATCH_SIZE=32
embedding_size=256
sequence_length=200
BUFFER_SIZE=10000
input_size=41
learning_rate=0.001

inputs_as_tensors=tf.data.Dataset.from_tensor_slices(train_data_features_array)
targets_as_tensors=tf.data.Dataset.from_tensor_slices(train_data_labels_categorical_array)
training_data=tf.data.Dataset.zip((inputs_as_tensors,targets_as_tensors))
#training_data=training_data.batch(sequence_length,drop_remainder=True)
training_dataset=training_data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
print(training_dataset)

def build_model(vocab_size,batch_size):
    modelf=tf.keras.Sequential([
        tf.keras.layers.Dense(10,activation=""sigmoid"",input_shape=(None,10)),
        
                                tf.keras.layers.Dense(30,activation=""relu"",use_bias=True),  
                                 
                                     tf.keras.layers.Dropout(0.2),                    
                                   tf.keras.layers.Dense(vocab_size)
                          
                               
                                ])
    return modelf

def training_step(inputs,targets,optimizer):
    with tf.GradientTape() as tape:
        predictions=model(inputs)
        loss=tf.reduce_mean(tf.keras.losses.categorical_crossentropy(targets,predictions,from_logits=True))
        
        grads=tape.gradient(loss,model.trainable_variables)
        optimizer.apply_gradients(zip(grads,model.trainable_variables))
        return loss,predictions

model=build_model(input_size,BATCH_SIZE)
i=0
inner_loop=0
checkpoint_dir ='Moses_Model_x'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt_{i}"")

while(1):
    start = time.time()
    for x,y in training_dataset:
        loss,predictions=training_step(x,y,tf.keras.optimizers.RMSprop(learning_rate=0.002))
    print ('Epoch {} Loss {:.4f}'.format(i, loss))
    print ('Time taken for iteration {} is {} sec\n'.format(i,time.time() - start))
    model.save_weights(checkpoint_prefix.format(i=i))
    i=i+1

However, the loss value is actually increasing. Is there anything I have to change or something wrong in my code?

","['neural-networks', 'machine-learning', 'objective-functions']",
"What is the difference between ""out-of-distribution (generalisation)"" and ""(meta)-transfer learning""?","
I'm trying to develop a better understanding of the concept of ""out-of-distribution"" (generalization) in the context of Bengio's ""Moving from System 1 DL to System 2 DL"" and the concept of ""(meta)-transfer learning"" in general. 
These concepts seem to be very strongly related, maybe even almost referring to the same thing. So, what are similarities and differences between these two concepts? Do these expressions refer to the same thing? If the concepts are to be differentiated from each other, what differentiates the one concept from the other and how do the concepts relate?
","['deep-learning', 'comparison', 'transfer-learning', 'meta-learning']",
How to set the target for the actor in A2C?,"
I did a simple Actor-Critic implementation in Keras using 2 networks where the critic learns the Q-Values of every action, and the actor predicts probabilities for choosing each action. In training, the target probabilities for the actor was a one-hot vector with 1.0 in the maximum Q-Value prediction position and 0.0 in all the rest, and simply used fit method on the actor model with mean squared error loss function.
However, I'm not sure what to set as the target when switching to A2C. In all the guides I saw it's mentioned that the critic now learns one value per state, not one value per action in the action space. 
This change makes it unclear on how to set the target vector for the actor. The guides/SE questions I went over did not explain this point and simply said that we can calculate the advantage value using the value function (here, here and here) for the current and next state, which is fine, except we can only do that for the specific action taken and not for every action in the action-space because we don't the value for every next state for every action.
In other words, we only know A(s,a) for our memorized a, and we know nothing about the advantage of other actions.
One of my guesses was that you still calculate the Q-Values, because after all, the value function is defined by the Q-Values. The value function is the sum over every action a of Q(s,a)*p(a). So does the critic need to learn the Q-Values and sum their multiplications with the probabilities generated by the policy network (actor), and calculate the advantages of every action?
It's even more confusing because in one of the guides they said that the critic actually learns the advantage values, and not the value function (like all the other guides said), which is strange because you need to use the critic to predict the value function of the state and the next state. Also, the advantage function is per-action and in the implementations I see the critic has one output neuron.
I think that what's being done in the examples I saw was to train the actor to fit a one-hot vector for the selected action (not the best action by the critic), but modify the loss-function value using the advantage value (possibly to influence the gradient). Is that the case?
","['reinforcement-learning', 'keras', 'actor-critic-methods', 'advantage-actor-critic']",
Why MLP cannot approximate a closed shape function?,"
[TL;DR] 
I generated two classes Red and Blue on a 2D space. Red are points on Unit Circle and Blue are points on a Circle Ring with radius limits (3,4). I tried to train a Multi Layer Perceptron with different number of hidden layers, BUT all the hidden layers had 2 neurons. The MLP never reached 100% accuracy. I tried to visualize how the MLP would classify the points of the 2D space with Black and White. This is the final image I get:

At first, I was expecting that the MLP could classify 2 classes on a 2D space with 2 Neurons at each hidden layer, and I was expecting to see a white circle encapsulating the red points and the rest be a black space. Is there a (mathematical) reason, why the MLP fails to create a close shape, rather it seems to go from infinity to infinity on a 2d space ?? (Notice: If I use 3 neurons at each hidden layer, the MLP succeeds quite fast). 
[Notebook Style] 
I generated two classes Red and Blue on a 2D space.
Red are points on Unit Circle 
size_ = 200
classA_r = np.random.uniform(low = 0, high = 1, size = size_)
classA_theta = np.random.uniform(low = 0, high = 2*np.pi, size = size_)
classA_x = classA_r * np.cos(classA_theta)
classA_y = classA_r * np.sin(classA_theta)


and Blue are points on a Circle Ring with radius limits (3,4).  
classB_r = np.random.uniform(low = 2, high = 3, size = size_)
classB_theta = np.random.uniform(low = 0, high = 2*np.pi, size = size_)
classB_x = classB_r * np.cos(classB_theta)
classB_y = classB_r * np.sin(classB_theta)

I tried to train a Multi Layer Perceptron with different number of hidden layers, BUT all the hidden layers had 2 neurons. 
hidden_layers = 15
inputs = Input(shape=(2,))
dnn = inputs
for l_no in range(hidden_layers):
    dnn = Dense(2, activation='tanh', name = ""layer_{}"".format(l_no))(dnn)
outputs = Dense(2, activation='softmax', name = ""layer_out"")(dnn)

model = Model(inputs=inputs, outputs=outputs)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy'])

The MLP never reached 100% accuracy. I tried to visualize how the MLP would classify the points of the 2D space with Black and White.
limit = 4
step = 0.2
grid = []
x = -limit
while x <= limit:
    y = -limit
    while y <= limit:
        grid.append([x, y])
        y += step
    x += step
grid = np.array(grid)

prediction = model.predict(grid)

This is the final image I get:
xs = []
ys = []
cs = []
for point in grid:
    xs.append(point[0])
    ys.append(point[1])
for pred in prediction:
    cs.append(pred[0])

plt.scatter(xs, ys, c = cs, s=70, cmap = 'gray')
plt.scatter(classA_x, classA_y, c = 'r', s= 50)
plt.scatter(classB_x, classB_y, c = 'b', s= 50)
plt.show()


At first, I was expecting that the MLP could classify 2 classes on a 2D space with 2 Neurons at each hidden layer, and I was expecting to see a white circle encapsulating the red points and the rest be a black space. Is there a (mathematical) reason, why the MLP fails to create a close shape, rather it seems to go from infinity to infinity on a 2d space ?? (Notice: If I use 3 neurons at each hidden layer, the MLP succeeds quite fast).
What I mean by a closed shape, take a look at the second image which was generated by using 3 neurons at each layer:
for l_no in range(hidden_layers):
    dnn = Dense(3, activation='tanh', name = ""layer_{}"".format(l_no))(dnn)

 
[According to Marked Answer] 
from keras import backend as K
def x_squared(x):
    x = K.abs(x) * K.abs(x)
    return x
hidden_layers = 3
inputs = Input(shape=(2,))
dnn = inputs
for l_no in range(hidden_layers):
    dnn = Dense(2, activation=x_squared, name = ""layer_{}"".format(l_no))(dnn)
outputs = Dense(2, activation='softsign', name = ""layer_out"")(dnn)
model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=['accuracy'])

I get:

","['machine-learning', 'keras', 'multilayer-perceptrons', 'linear-algebra']",
How should the neural network deal with unexpected inputs?,"
I recently wrote an application using a deep learning model designed to classify inputs. There are plenty of examples of this using images of irises, cats, and other objects. 
If I trained a data model to identify and classify different types of irises and I show it a picture of a cat, is there a way to add in an ""unknown"" or ""not a"" classification or would it necessarily have to guess what type of iris the cat most looks like?
Further, I could easily just add another classification with the label ""not an iris"" and train it using pictures of cats, but then what if I show it a picture of a chair (the list of objects goes on).
Another example would be in natural language processing. If I develop an application that takes the input language and spits out ""I think this is Spanish"", what if it encounters a language it doesn't recognize?
","['neural-networks', 'deep-learning', 'classification', 'bayesian-deep-learning', 'bayesian-neural-networks']",
Derivation of regularized cost function w.r.t activation and bias,"
In regularzied cost function a L2 regularization cost has been added.

Here we have already calculated cross entropy cost w.r.t $A, W$.
As mentioned in the regularization notebook (see below) in order to do derivation of regularized $J$ (cost function), the changes only concern $dW^{[1]}$, $dW^{[2]}$ and $dW^{[3]}$. For each, you have to add the regularization term's gradient.(No impact on $dA^{[2]}$, $db^{[2]}$, $dA^{[1]}$ and $db^{[1]}$ ?)

But I am doing it using the chain rule then I am getting change in values for $dA^{[2]}$ , $dZ^{[2]}$, $dA^{[1]}$, $dW^{[1]}$ and $db^{[1]}$.
Please refer below how I calculated this ?

Can someone explain why I am getting different results?
What is the derivative of L2 reguarlization w.r.t $dA^{[2]}$ ? (in equation 1)
So my questions are 
1) Derivative of L2 regularization cost w.r.t $dA^{[2]}$
2) How adding regularization term not affecting $dA^{[2]}$, $db^{[2]}$, $dA^{[1]}$ and $db^{[1]}$ (i.e. $dA$ and $db$) but changes $dW$'s ?
","['backpropagation', 'math', 'regularization', 'derivative']",
How can I learn a graph given nodes with features in a supervised fashion?,"
I have a dataset and want to be able to construct a graph from it in a supervised fashion.
Let's assume I have a dataset with N nodes, each node has e.g. 10 features. Out of these N nodes, I want to learn a graph, i.e. an $N \times N$ adjacency matrix. So, I start with $N$ nodes and all I know is a 10-dimensional feature vector for each node. I have no knowledge about the relation between these nodes and want to figure it out.
Here is an example for $N=6$, but in practice $N$ is not fixed.

So the output I would like to get here is a $6\times6$ adjacency matrix, representing the relations between the nodes (undirected).
Note: N is arbitrary and not fixed. So an algorithm should be able to perform on any given N.
My dataset is labeled. For the training dataset, I have the desired adjacency matrix for each collection of input nodes, which is filled with $0$s and $1$s. 
However, the output of the algorithm could also be an adjacency matrix filled with non-integer numbers in $[0,1]$, giving some kind of probability of the nodes being connected (preferably close to $0$ or $1$ of course). So I could easily give a number as the label for each node. In the above example, the labels for the three connected nodes could be class $1$, and so on. 
Is there any kind of supervised learning algorithm (e.g. some sort of graph neural network) that can perform these tasks?
","['neural-networks', 'deep-learning', 'supervised-learning', 'geometric-deep-learning', 'graphs']","
It's perfectly reasonable to apply 'traditional' Deep Learning approaches to try and learn an adjacency matrix (a matrix is just a vector of vectors, which can be flattened into a single output vector) but you might need a lot of training data as N gets larger.
Your outputs could certainly have the form of an adjacency matrix, as you describe. Whether it's more useful to have 'boolean' (either 0 or 1)
or 'probabalistic' entries in the matrix depends both on the data and the specifics of your end application.
"
"Human Aggression Detection Community, Competition and dataset","
I'm looking for a community or competition website related to human aggression detection using Deep Learning in a video.
Also, I'm looking for a dataset of human aggression activities. 
Any suggestions would be appreciated.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'computer-vision']",
"How can I get to a final output of shape $224 \times 224$, without FC layers, from a tensor of specific shape, in OpenPose?","
I am approaching the implementation of the OpenPose algorithm for realtime human body pose estimation. 
According to the official paper OpenPose: Realtime Multi-Person 2D Pose
Estimation using Part Affinity Fields, $L$ and $S$ fields (body part maps, and part affinity fields) are estimated. These have the same size as the input image, and, according to the paper, these fields should be outputted at a given step in the forward pass (after a given number of $L$ stages, and $S$ stages), but, since before entering these stages the image is passed through the initial layers of the VGG-19 model, the spatial dimension is encoded and the features that finally enter the $L$ and $S$ stages have other dimensionality. 
All the network is convolutional, there's no FC layer at all. The VGG-19 part is the only one that contains MaxPooling layers, hence affecting the spatial relations and size of the receptive fields. 
My point is, after stage execution, I get tensors of shape [batch_size, filter_number, 28, 28]. The issue is that the paper is not stating how to decode this information into the $L$ and $S$ maps of size $224 \times 224$.
Following a traditional approach and decoding the final tensors with a linear net from, let's say, $15000 \rightarrow (224 * 224 * \text{ number of body parts }) + (224 * 224 * \text{ number of limbs } * 2)$~A very huge number!, is out of question for any domestic computer, I presume I should have the least 128GbRAM installed, and is not the case. 
Another solution is to remove the max-pooling layers from the VGG-19 part, but then although the map size is preserved to $224$, instead of $28$, the huge amount of computations and values that need to be stored also lead to memory errors. 
So, the problem is, how can I get to a final output of $224 \times 224$ without FC layers, from a tensor of shape [batch_size, bodyparts, 28, 28]?
Not an easy answer. I will check a TensorFlow implementation I have seen around to see how the problem was solved. 
Any parallel ideas are greatly welcome.
","['neural-networks', 'deep-learning', 'papers', 'implementation', 'pose-estimation']",
How to perform insect classification given two images of the same insect?,"
I'm relatively new to image classification. Currently, I am trying to classify insect images, using a convolutional neural network (CNN).
When I ask a human expert to identify an insect, I usually provide 2 photos: back and face. It seems that sometimes one feature stands out and allows identification with high certainty (""spots on the back - definitely a ladybug""), while other times you need to cross-reference both angles (""grey back could mean a few things, but after cross-referencing with the eyes - it's a moth""). 
How is it customary to implement this? Naively I was considering:

Two separate networks, one for backs and one for faces? If so, what formula is best for weighing in their outputs?
Single network, but separate dual classifications - e.g. ""moth face"", ""moth back"", ""ladybug face"", ""ladybug back""? 
A single network, feed everything naively (e.g. moths from different angles, all with the same classification ""moth"") and rely on the NN to sort it out itself?

","['neural-networks', 'deep-learning', 'classification', 'image-recognition']",
What does the statement with the max do in the recursive best-first search algorithm?,"
I am reading the book ""Artificial Intelligence: A Modern Approach"" by Stuart and Norvig. I'm having trouble understanding a step of the recursive best-first search (RBFS) algorithm.
Here's the pseudocode.

What does the line s.f <- max(s.g + s.h, node.f) do? 
Here's a diagram of the execution of RBFS.

","['search', 'norvig-russell', 'best-first-search']","
This is probably more easily understood as the collapse/restore macro. The idea is that the previously explored state was collapsed and only the minimum f-cost from the sub-tree was stored. This represents the best unexpanded state in the subtree that was collapsed.
When restoring the portion of the collapsed tree, the f-cost of the restored node could either be the original f-cost (g+h), or it could be the stored f-cost if it is larger. By taking the max, the code ensures that states that are restored maintain at least the cost of the previously best unexpanded state. (If the g+h cost is larger, then we know the state wasn't previously expanded and it wasn't previously the state on the fringe with the minimum edge cost.)
The linked paper gives several examples where similar ideas are used during search.
"
How to predict an event (or action) based on a window of time-series measurements?,"
I have an input vector $X$, which contains a series of measurements within a period, e.g. 100 measurements in 1 sec. The goal is to predict an event, let's say, moving forward, backward or static. 
I don't want to predict the output just by looking at one series of measurements, but by looking at a window of $n$ vectors $X$ of measurements, making it dependant on the previous measurements, because of the noise in the measurements.
Is there a way RNN can help me with this? Many to one architecture? LSTM? 
CNN of 1D + LSTM + dense? 
","['convolutional-neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'time-series']",
"If the point of the ResNet skip connection is to let the main path learn the residual relative to identity, why are there convolutional skips?","
In the original ResNet paper they talk about using plain identity skip connections when the input and output of a block have the same dimensions.

When the input and output have different dimensions they propose two options:
(A) Use an identity mapping padded with zeros to make up for the extra dimensions
(B) Use a ""projection"".

which (after some digging around in other people's code) I see as meaning: do a convolution with a 1x1 kernel with trainable weights.
(B) is confusing to me because it seems to ruin the point of ResNet by making the skip connection trainable. Then the main path is not really learning a ""residual"" relative to an identity transformation. So at this point, I'm no longer sure how to interpret the intent or expected effect of this type of block. And I would think that one should justify doing it in the first places instead of just not putting a skip connection there at all (which in my mind is the status-quo before this paper).
So can anyone help explain away my confusion here?
","['convolutional-neural-networks', 'residual-networks']","
Well, I found an answer that satisfies me.
The zero-padded identity is not ideal. Suppose we're mapping from 64 channels to 128 channels. Then the zero-padded identity will map to an output where half of the channels are the same as the inputs, and the other half are all zeros. So that means the main path is learning a residual for half of the output channels, and learning a mapping from the ground up for the other half of the channels.
Now, the alternative is to use option (B) which is the single 1x1 convolution. After reading their ResNetV2 paper I realised that they really double down on this concept of maintaining a ""clean"" alternative path all the way up and down the network. So then the question is, what's ""cleaner""?: a block of two convolutions of the form 3x3x64 then 3x3x128, or a shortcut with a 1x1x128 convolution? The shortcut is a tempting choice here. And in fact, in the original paper they show empirically that the convolutional shortcut is better than the identity shortcut. Maybe it might be worth running a test to see if the convolutional shortcut is better than no shortcut at all, but until I decide to run such a test, I'll presume the authors would have mentioned it. The other thing worth noting is that there are only a few places in the whole network where the dimensions need to be increased, so maybe the impact is small.
"
Why haven't we solved the problem of bipedal walking?,"
This has been a mystery to me.
All the walking robots look like idiots now. But we do have a lot of simulation-based results
(Flexible Muscle-Based Locomotion for Bipedal Creatures
), so why can't we just apply the simulation results to a real robot and let it walk, not like an idiot, but like an running ostrich?
With the main loop running at more than 60 fps, I fail to see how possibly the program could fail to stop the robot from losing balance. When I balance a stick on my hand, I could probably only do 5 fps.
We have not only supercomputers connected to the robots, but also reinforcement learning algorithms at our disposal, so what has been the bottleneck in the problem of bipedal walking?
","['reinforcement-learning', 'robotics', 'cyborg']",
What is the relationship between the Q and V functions?,"
Suppose we have a policy $\pi$ and we use SARSA to evaluate $Q^\pi(s, a)$, where $a$ is the policy $\pi$.
Can we say that $Q^\pi(s, a) = V^\pi(s)$? 
The reason why I think this can be the case is because $Q^\pi(s, a)$ is defined as the value obtained from taking action $a$ and then following policy $\pi$ thereafter. However, the action $a$ taken is the policy according to $\pi$ for all $s \in S $. This seems to corresponds to the value function equation of $V^\pi(s_t) = r(s_t) + \gamma V^\pi(s_{t+1})$.
","['reinforcement-learning', 'comparison', 'sarsa', 'value-functions']",
Which NLP applications are based on recurrent neural networks?,"
Some of the NLP applications taken from this link NLP Applications:

Machine Translation
Speech Recognition
Sentiment Analysis
Question Answering
Automatic Summarization
Chatbots
Market Intelligence
Text Classification
Character Recognition
Spell Check

Which are the NLP applications that supports recurrent neural network?
","['natural-language-processing', 'recurrent-neural-networks', 'applications']","
Speech recognition and Character recognition are not part of NLP. Everything else on your list can in principle be done with RNNs. But the field is quickly moving towards using transformers.
"
Why is my derivation of the back-propagation equations inconsistent with Andrew Ng's slides from Coursera?,"
I am using the cross-entropy cost function to calculate its derivatives using different variables $Z, W$ and $b$ at different instances. Please refer image below for calculation.

As per my knowledge, my derivation is correct for $dZ, dW, db$ and $dA$, but, if I refer to Andrew Ng Coursera stuff, then I am seeing an extra $\frac{1}{m}$ for $dW$ and $db$, whereas no $\frac{1}{m}$ in $dZ$. Andrew's slides on the left represent derivative and whereas the right side of slides shows NumPy implementation corresponding to the right side equation.

Can someone please explain why there is:
1) $\frac{1}{m}$ in $dW^{[2]}$ and $db^{[2]}$ in Andrew's slides in NumPy representation
2) missing $\frac{1}{m}$ for $dZ^{[2]}$ in Andrew's slides in both normal and NumPy representation.
Am I missing something or doing it in the wrong way?
","['math', 'backpropagation', 'derivative', 'numpy']","
TL;DR: This has to do with the way A. Ng has defined back propagation for the course. 
Left Column
This is only with respect to one input example and so the $\frac{1}{m}$ factor reduces to 1 and can be omitted. He uses lower case to represent one input example (eg a vector $dz$) and upper case with respect to a (mini-)batch (eg a matrix $dZ$). 
The $\frac{1}{m}$ factors in $dW,db$
In this definition of backprop, he ""defers"" multiplying by the $\frac{1}{m}$ factor until $dW,db$ rather than ""absorbing"" it into $dZ^{[2]}$. That is, the $dZ^{[2]}$ term is defined in a way that it does not have $\frac{1}{m}$. 
Observe, if you move the $\frac{1}{m}$ factor to be in the definition of $dZ^{[2]}$ and remove it from the definitions of $dW,db$ you will still come out with the same values for all $dW,db$.
Speculation
This ""deferred"" multiplication might have to do with numerical stability. Or simply a stylistic choice made by A. Ng. This might also prevent one from ""accidentally"" multiplying by $\frac{1}{m}$ more than once. 
"
Multiple-dimension scaling (MDS) objective for MDS and PCA,"

The following is the MDS Objective.


Let's think of a senario where I apply MDS with/from the solution I obtained from PCA. Then I calculate the objective function on the initial PCA solution and MDS solution (after applying MDS on the former PCA solution). Then I would for sure assume that the objective function will decrease for the MDS solution compared with PCA solution. However, when I calculate the objective function respectively, MDS solution yields higher objective function value. Is this normal?
I am attaching my code below:
import os
import pickle
import gzip
import argparse
import time
import matplotlib.pyplot as plt
import numpy as np
from numpy.linalg import norm

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.preprocessing import LabelBinarizer
from sklearn import decomposition

from neural_net import NeuralNet, stochasticNeuralNet
from manifold import MDS, ISOMAP
import utils

def mds_objective(Z,X):
    sum = 0
    n,d = Z.shape
    for i in range(n):
        for j in range(i+1,n):
            sum += (norm(Z[i,:]-Z[j,:],2)-norm(X[i,:]-X[j,:],2))**2
    return 0.5*sum

dataset = load_dataset('animals.pkl')
X = dataset['X'].astype(float)
animals = dataset['animals']
n, d = X.shape
pca = decomposition.PCA(n_components = 5)
pca.fit(X)
Z = pca.transform(X)
plt.figure()
plt.scatter(Z[:, 0], Z[:, 1])
for i in range(n):
     plt.annotate(animals[i], (Z[i,0], Z[i,1]))
utils.savefig('PCA.png')

print(pca.explained_variance_ratio_)
print(mds_objective(Z,X))


dataset = load_dataset('animals.pkl')
X = dataset['X'].astype(float)
animals = dataset['animals']
n,d = X.shape

model = MDS(n_components=2)
Z = model.compress(X)

fig, ax = plt.subplots()
ax.scatter(Z[:,0], Z[:,1])
plt.ylabel('z2')
plt.xlabel('z1')
plt.title('MDS')
for i in range(n):
       ax.annotate(animals[i], (Z[i,0], Z[i,1]))
utils.savefig('MDS_animals.png')
print(mds_objective(Z,X))

It prints the following:

1673.1096816455256
1776.8183112784652

","['machine-learning', 'principal-component-analysis']",
Why does model complexity increase my validation score by a lot?,"
I learned that when creating neural networks the go to was to overfit and then to regularize. However I am now in a situation where, when I make the model more complex (more layers, more filters, ...) my scores become worse.
I am training a CNN to predict pollution 6 hours in advance. The input I give to my model is the pollution of the past 18 hours.
Can I safely say that because there probably is a lot of noise in this data that, that is the reason when increasing my complexity, my model becomes worse?
","['neural-networks', 'convolutional-neural-networks']",
What are the guidelines for defining a reward function in reinforcement learning (bandit problem)?,"
I'm working currently on a problem and I'm using RL (bandit problem). 
In my system, I have an agent that chooses an action among $k$ possible actions, and a user that decides whether the agent chooses the right action or not. If the user is satisfied with the decision made by the agent, he rewards with $+1$, otherwise $-1$.
Is this is a good reward function, knowing that in my problem the values are in the range $[0, 1]$? 
Are there any guidelines to follow for defining the reward function? Are there any references (books or articles) that tackle this problem and present a solution?
","['reinforcement-learning', 'ai-design', 'rewards']",
What is the difference between evolutionary computation and evolutionary algorithms?,"
A book on evolutionary computation by De Jong mentions both the term evolutionary algorithms (EA) as well as evolutionary computation (EC). However, it remains unclear to me what the difference between the two is. According to Vikhar, EA forms a subset of EC. However, it remains unclear to me what sort of topics/algorithms would be considered EC but not EA. Is there a clear difference between the two? If so, what is this difference?
","['comparison', 'evolutionary-algorithms', 'evolutionary-computation']","
As you can find on Wikipedia:

Evolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection, and survival of the fittest.

This means that other types of evolutions, which are not necessarily a biological evolution, are found in evolutionary computation but not in evolutionary algorithms. For example, learning classifier systems are in EC as they are evolutionary, but not completely in EA as they are not biological.
"
Are there principled ways of tuning a neural network in case of overfitting and underfitting?,"
Whenever I tune my neural network, I usually take the common approach of defining some layers with some neurons. 

If it overfits, I reduce the layers, neurons, add dropout, utilize regularisation. 
If it underfits, I do the other way around. 

But it sometimes feels illogical doing all these. So, is there a more principled way of tuning a neural network (i.e. find the optimal number of layers, neurons, etc., in a principled and mathematical sound way), in case it overfits or underfits?
","['neural-networks', 'overfitting', 'hyperparameter-optimization', 'regularization', 'underfitting']",
"If vanishing gradients are NOT the problem that ResNets solve, then what is the explanation behind ResNet success?","
I often see blog posts or questions on here starting with the premise that ResNets solve the vanishing gradient problem.
The original 2015 paper contains the following passage in section 4.1:

We argue that this optimization difficulty is unlikely to
be caused by vanishing gradients. These plain networks are
trained with BN, which ensures forward propagated
signals to have non-zero variances. We also verify that the
backward propagated gradients exhibit healthy norms with
BN. So neither forward nor backward signals vanish. In
fact, the 34-layer plain net is still able to achieve competitive accuracy, suggesting that the solver works
to some extent.

So what's happened since then? I feel like either it became a misconception that ResNets solve the vanishing gradient problem (because it does indeed feel like a sensible explanation that one would readily accept and continue to propagate), or some paper has since proven that this is indeed the case.
I'm starting with the initial knowledge that it's ""easier"" to learn the residual mapping for a convolutional block than it is to learn the whole mapping. So my question is on the level of: why is it ""easier""? And why does the ""plain network"" do such a good job but then struggle to close the gap to the performance of ResNet. Supposedly if the plain network has already learned reasonably good mappings, then all it has left to learn to close the gap is ""residual"". But it just isn't able to.
","['convolutional-neural-networks', 'papers', 'residual-networks', 'vanishing-gradient-problem']",
Using AI to enhance customer service,"
I'm trying to find out how AI can help with efficient customer service, in fact call routing to the right agent. My usecase is given context of a query from a customer and agents' expertise, how can we do the matching?
Generally, how is this problem solved? What sub-topic within AI is suitable for this problems? Classification, recommender systems, ...? Any pointers to open-source projects would be very helpful.
","['machine-learning', 'deep-learning', 'classification', 'recommender-system', 'efficiency']","
This sounds to me like a use case for a chatbot. You would have different intents reflecting the types of user queries that your system can respond to. The intent matching can be done by pattern matching, machine learning (classification), or a combination of the two (hybrid). You can then use the chatbot to ask clarification questions or elicit more information to identify which live agent would be the best person to take over the call. Essentially each live agent would have a list of intents plus added information (such as geographical area etc) which you then compare against the data from the caller to find the best match.
If this is a voice call you'd need to put an ASR system at the front of the pipeline. Chatbots can usually do live-agent handover to then pass control to a human agent at any time in the conversation.
[Disclaimer: I work for a company that operates in exactly that area and whose system works as described above]
"
What are the AI technologies currently used to fight the coronavirus pandemic?,"
The ongoing coronavirus pandemic of coronavirus disease 2019 (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), as of 29 September 2020, has affected many countries and territories, with more than 33.4 million cases of COVID-19 have been reported and more than 1 million people have died. The live statistics can be found at https://www.worldometers.info/coronavirus/ or in the World Health Organization (WHO) site. Although countries have already started quarantines and have adopted extreme countermeasures (such as closing restaurants or forbidding events with multiple people), the numbers of cases and deaths will probably still increase in the next weeks.
Given that this pandemic concerns all of us, including people interested in AI, such as myself, it may be useful to share information about the possible current applications of AI to slow down the spread of SARS-CoV-2, to help infected people or people in the healthcare sector that have been uninterruptedly working for hours to attempt to save more lives, while putting at risk their own.
What are the existing AI technologies (e.g. computer vision or robotics tools) that are already being used to tackle these issues, such as slowing down the spread of SARS-CoV-2 or helping infected people?
I am looking for references that prove that the mentioned technologies are really being used. I am not looking for potential AI technologies (i.e. research work) that could potentially be helpful. Furthermore, I am not looking for data analysis tools (e.g. sites that show the evolution of the spread of coronavirus, etc.)
","['social', 'healthcare', 'applications']","
According to the Baidu Research's blog post How Baidu is harnessing the power of AI in the battle against coronavirus (12-03-2020), there are already some artificial intelligence tools or algorithms being used to fight the coronavirus.
Given that I cannot confirm that these AI tools and algorithms I will mention are really being used in practice, I will only quote the parts of the blog post that potentially answer my original question.
To give some context, similar to HIV viruses, the virus that is causing the coronavirus pandemic, SARS-CoV-2 is capable of rapidly mutating, making vaccine development and virus analysis difficult.
AI-powered and non-contact infrared sensor system

Baidu has developed several tools that are effective in building awareness and screening populations, including an AI-powered, non-contact infrared sensor system that provides users with fast multi-person temperature monitoring that can quickly detect a person if they are suspected of having a fever, one of the many symptoms of the coronavirus. This technology is currently being used in Beijing's Qinghe Railway Station to identify passengers who are potentially infected where it can examine up to 200 people in one minute without disrupting passenger flow.

AI-powered pneumonia screening and lesion detection system

By leveraging PaddlePaddle and the semantic segmentation toolkit PaddleSeg, LinkingMed has developed an AI-powered pneumonia screening and lesion detection system, putting it into use in the hospital affiliated with XiangNan University in Hunan Province. The system can pinpoint the disease in less than one minute, with a detection accuracy of 92% and a recall rate of 97% on test data sets.

Automated HealthMap system

The Boston Children's Hospital used an automated HealthMap system that scans online news and social media reports for early warning signs of outbreaks, which led to the initial awareness that COVID-19 was spreading outside China.

Autonomous vehicles carry out non-contact tasks

Access to health care and resources at a moment's notice is vital for battling the spread of the coronavirus. Autonomous vehicles are playing a useful role in providing access to necessary commodities for health-care professionals and the public alike by delivering goods in infected areas and disinfecting hospitals, effectively minimizing person-to-person transmission and alleviating the shortage of medical staff.
Apollo, Baidu's autonomous vehicle platform, partnered with a local self-driving startup called Neolix to deliver supplies and food to the Beijing Haidian Hospital.

"
Can the addition of low-quality images to the training dataset increase the network performance?,"
I already trained a deep neural network called YOLO (You Only Look Once) with high-quality images (1920 by 1080 pixels) for a detection task. The result for mAP and IOU were 93% and 89% respectively. 
I wanted to decrease the quality of my training data set using some available filters, then I use those low-quality images along with high-quality images to train the network again. 
Does this method increase the accuracy (or, in general, performance) of the deep neural network (for a detection task)? Like mAP and IOU?
My data set is vehicle images.
mAP: mean average precision
IOU: intersection over union ( or overlap)
","['neural-networks', 'deep-learning', 'object-detection', 'data-preprocessing', 'performance']",
Policy Gradient Reward Oscillation in MATLAB,"
I'm trying to train a Policy Gradient Agent with Baseline for my RL research. I'm using the in-built RL toolbox from MATLAB (https://www.mathworks.com/help/reinforcement-learning/ug/pg-agents.html) and have created my own Environment. The goal is to train the system to sample an underlying time-series ($x$) given battery constrains ($\epsilon$ is battery cost). 
The general setup is as follows:

My Environment is a ""sensor"" system with exogenous input time-series and battery level as my States/Observations (size is 13x1). 
Actions $A_t$ are binary: 0 = keep a model prediction $(\hat x)$; 1 = sampling time series $(x)$
Reward function is

$$
R = -[err(\tilde x, x) + A_t\cdot \epsilon ] + (-100)\cdot T_1 + (100) \cdot T_2 
$$
where $err(\tilde x, x)$ is the RMSE error between the sampled time series $(\tilde x)$, and true time series x.

The Terminal State Rewards are -100 if sensor runs out of battery $T_1$ or 100 if reached the end of the episode with RMSE < threshold and remaining battery level $(T_2)$. The goal is to always end in $T_2$. 
Each training Episode consists of a time-series of random length, and random initial battery level.

My current setup is using mostly default RL setups from MALTAB with learning rate of $10^{-4}$ and ADAM optimizer. The training is slow, and shows a lot of Reward oscillation between the two terminal states. MATLAB RL toolbox also outputs a $Q_0$ value which the state is:

Episode Q0 is the estimate of the discounted long-term reward at the
  start of each episode, given the initial observation of the
  environment.  As training progresses, Episode Q0 should approach the
  true discounted  long-term reward if the critic is well-designed,


Questions

Is my training and episodes too random? i.e., time-series of different lengths and random initial sensor setup. 
Should I simplify my reward function to be just $T_2$?
Why doesn't $Q_0$ change at all?

","['reinforcement-learning', 'policy-gradients', 'matlab']",
How to correctly implement self-play with DQN?,"
I have an environment where an agent faces an equal opponent, and while I've achieved OK performance implementing DQN and treating the opponent as a part of the environment, I think performance would improve if the agent trains against itself iteratively. I've seen posts about it, but never detailed implementation notes. My thoughts were to implement the following (agent and opponent are separate networks for now):

Bootstrap agent and opponent with initial weights (either random or trained against CPU, not sure)
Use Annealing Epsilon Greedy strategy for N iterations
After M iterations (M > N), copy agent network's weights to opponent's network
Reset annealing epsilon (i.e. start performing randomly again to explore new opponent)?
Repeat steps 2-4

Would something like this work? Some specific questions are:

Should I ""reset"" my annealing epsilon strategy every time the opponent is updated? I feel like this is needed because the agent needs sufficient time to explore new strategies for this ""new"" opponent.
Should the experience replay buffer be cleared out when the opponent is updated? Again, I think this is needed.

Any pointers would be appreciated.
","['deep-learning', 'reinforcement-learning', 'dqn', 'deep-rl', 'self-play']",
How can I detect the frame from video streaming that contains a graffiti on city wall?,"
I am working on a graffiti detection project. I need to analyze data stream from a camera mounted sideways on a vehicle to identify graffiti on city walls and notify authorities with the single best capture of graffiti and its geolocation, etc. 
I am trying to use a ResNet50 model pre-trained on ImageNet using transfer learning for my graffiti image dataset. The classification will be done on an edge device as network connectivity may not be reliable.
Suppose I have a series of frames that have been detected to contain graffiti, as the vehicle goes past it, but I only need to report one image (so not all frames containing graffiti in the series). How can I do that? 
Ideally, I would like to report the frame where the camera is perpendicular to the wall. Why perpendicular? I think that images containing the graffiti when the camera is perpendicular to the wall will more clearly show the graffiti.
","['deep-learning', 'computer-vision', 'image-recognition', 'transfer-learning']",
How to calculate the advantage in policy gradient functions?,"
From my understanding of the REINFORCE policy gradient method, we gently nudge the probabilities of actions based on the advantages. More specifically, the positive advantages increase the probabilities, negative advantages reduce the probabilities.
So, how do we compute the advantages given the real discounted rewards (aggregated rewards from the episode) and a policy network that only outputs the probabilities of actions?
","['reinforcement-learning', 'objective-functions', 'policy-gradients', 'reinforce']","
The advantage is basically a function of the actual return received and a baseline. The function of the baseline is to make sure that only the actions that are better than average receive a positive nudge.
One way to estimate the baseline is to have a value function approximator. At every step, you train a NN, using the trajectories collected via the current policy, to predict the value function for states.  
I hope that answers your query.
"
Using sigmoid in LSTM network for multi-step forecasting,"
I'm trying to develop a multistep forecasting model using LSTM  Network. The model takes three times steps as input and predicting two time_steps. both input and output columns are normalised using minmax_scalar within the range of 0 and 1.
Please see the below model architecture
Model Architecture 
model = Sequential()
model.add(LSTM(80,input_shape=(3,1),activation='sigmoid',return_sequences=True))
model.add(LSTM(20,activation='sigmoid',return_sequences=False))
model.add(Dense(2))

In this case, using sigmoid as an activation function is it correct? 
","['long-short-term-memory', 'activation-functions', 'regression', 'forecasting']","
Yes, due the input, output being constrained between zero and one that would be the only viable activation function. 
"
Do deeper residual networks perform better or worse?,"
If you have an $18$ layer residual network versus and a $32$ layer residual network, why would the former do better at object detection than the latter, if you have both models are training using the same training data?
","['convolutional-neural-networks', 'object-detection', 'residual-networks']","
Just by having more parameters, the deeper model has a higher capacity than the smaller one. This means that theoretically it can learn to extract more complex features from the data. Additionally, more layers means that the model can extract even higher-level features from the data.
So, generally speaking, deeper models will most of the times outperform shallow ones for more difficult tasks.
The downside is that if you have a small amount of data, a high-capacity model has the ability of memorizing the training set, which would lead to overfitting. 
Besides performance, deeper models require better hardware and larger training times. So, there are plenty of reasons for one to prefer a more shallow model to a deeper one.
"
How does (or should) AlphaGoZero (which does chess) fare against Deep Blue?,"
Deep blue is good at chess, but is more ""hand-coded"" or ""top-down"".
https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)
AlphaGoZero is ""self-taught"", and at Go is very much super-human.
https://en.wikipedia.org/wiki/AlphaGo_Zero 
How does AlphaGoZero fare when it goes head-to-head with DeepBlue?  Are there indicators like chess ratings?
","['chess', 'alphago-zero', 'deep-blue']",
Is there any resource that describes in detail a naive example-based machine translation algorithm?,"
I'm looking to develop a machine translation tool for a constructed language. I think that the example-based approach is the most suitable because the said language is very regular and I can have a sufficient amount of parallel translations.
I already know the overall idea behind the example-based machine translation (EBMT) approach, but I can't find any resource that describes a naive EBMT algorithm (or model) that would allow me to easily implement it.
So, I'm looking for either:

a detailed description,
pseudocode or
a sufficiently clear open-source project (maybe a GitHub one)

of a naive EBMT algorithm. So, I'm not looking for a software library that implements this, but I'm looking for a resource that explains/describes in detail a naive/simple EBMT algorithm, so that I am able to implement it.
Note that there are probably dozens of variations of EBMT algorithms. I'm only looking for the most naive/simple one.
I have already looked at the project Phrase-based Memory-based Machine Translator, but, unfortunately, it is not purely based on examples but also statistical, i.e. it needs an alignment file generated by, for example, Giza++ or Moses.
","['natural-language-processing', 'reference-request', 'resource-request', 'machine-translation']",
Neural network to extract correlated columns,"
I want to use a neural network to find correlated columns in a .csv file and give them as a output. The input .csv file has multiple columns with 0 and 1 ( like Booleans) in it. The file got the assignment from people to interests in it.
Example .csv input:
UserID   History  Math  Physics  Art  Music  ...
User1    0        1     1        0    0      ...
User2    0        0     0        1    1      ...
User3    0        1     1        1    1      ...
User4    1        0     1        1    0      ...
...

The output should be in this case something like: {math,physics}, {art,music}, {history,physics,art} - I exclude here {math,physics,art,music} because in a step afterwards i want to exclude (at least some) which can be created through the combination of others.
At the moment I have a problem that i don´t know which type of neural network could complete this task. How can I solve this problem?
So the important thing, that a column can have more than one column it correlates to - so its not like simple k-means clustering (as far as I understand it).
","['neural-networks', 'clustering', 'multi-label-classification']",
How to calculate the number of parameters of a convolutional layer?,"
I was recently asked at an interview to calculate the number of parameters for a convolutional layer. I am deeply ashamed to admit I didn't know how to do that, even though I've been working and using CNN for years now.
Given a convolutional layer with ten $3 \times 3$ filters and an input of shape $24 \times 24 \times 3$, what is the total number of parameters of this convolutional layer?
","['convolutional-neural-networks', 'filters', 'convolutional-layers', 'convolution-arithmetic']","
For a standard convolution layer, the weight matrix will have a shape of (out_channels, in_channels, kernel_sizes). In addition, you will need a vector of shape [out_channels] for biases. For your specific case, 2d, your weight matrix will have a shape of (out_channels, in_channels, kernel_size[0], kernel_size[1]).
Now, if we plugin the numbers:

out_channels = 10, you're having 10 filters
in_channels = 3, the picture is RGB in this case, so there are 3 channels (the last dimension of the input)
kernel_size[0] = kernel_size[1] = 3

In total you're gonna have 10*3*3*3 + 10 = 280 parameters.
"
Which activation functions can lead to the vanishing gradient problem?,"
From this video tutorial Vanishing Gradient Tutorial, the sigmoid function and the hyperbolic tangent can produce the vanishing gradient problem.
What other activation functions can lead to the vanishing gradient problem?
","['machine-learning', 'backpropagation', 'gradient-descent', 'activation-functions', 'vanishing-gradient-problem']",
Recent algorithms for correcting mislabeled data using multilayer perceptrons,"
I am doing literature research on algorithms for correcting mislabeled data using multilayer perceptrons. Found an ""old"" paper An algorithm for correcting mislabeled data (2001) by Xinchuan Zeng et al. Please share if you are aware of recent/current updates with a brief thoughts. Thanks in advance.
","['neural-networks', 'deep-learning', 'papers', 'multilayer-perceptrons', 'multi-label-classification']","
The most general solution today for the problem of finding label errors in datasets is called ""confident learning"" which works for all datasets and models, can be run time-efficiently in one line of code using cleanlab, and has substantial theory to prove that it works in realistic conditions on real-world datasets. This ""confident learning"" paper was a culminating result during my PhD at MIT and I am an author on the paper.

The Confident Learning Paper: https://arxiv.org/abs/1911.00068
Code: https://github.com/cleanlab/cleanlab
Examples of label errors found with cleanlab: https://labelerrors.com

Find label issues in your dataset in 1 line of code
from cleanlab.classification import CleanLearning
from cleanlab.filter import find_label_issues

# Option 1 - works with sklearn-compatible models - just input the data and labels ツ
label_issues_info = CleanLearning(clf=sklearn_compatible_model).find_label_issues(data, labels)

# Option 2 - works with ANY ML model - just input the model's predicted probabilities
ordered_label_issues = find_label_issues(
    labels=labels,
    pred_probs=pred_probs,  # out-of-sample predicted probabilities from any model
    return_indices_ranked_by='self_confidence',
)

Train a model as if the dataset did not have errors -- 3 lines of code
from sklearn.linear_model import LogisticRegression
from cleanlab.classification import CleanLearning

cl = CleanLearning(clf=LogisticRegression())  # any sklearn-compatible classifier
cl.fit(train_data, labels)

# Estimate the predictions you would have gotten if you trained without mislabeled data.
predictions = cl.predict(test_data)

Documentation and runnable tutorials for cleanlab: https://docs.cleanlab.ai/
The above approaches find issues. To correct label issues in your dataset:
The above approaches, just find issues and train without them. To correct the labels or train a model on a corrected dataset (that still includes all the errors, but now corrected with the right label), there is a no-code tool for that called Cleanlab Studio (https://cleanlab.ai/studio) for which I am also an author.
Background if you're interested:
I spent half a decade working with Isaac Chuang (inventor of the quantum computer) to solve this problem in a way that works for every dataset and every model (and every future dataset and future model) during my PhD at MIT. I originally decided to solve this problem after discovering (while building MIT and Harvard's cheating detection system in 2013) that most real-world datasets have significant label errors and this is one of the biggest problems that companies and universities struggle with when training ML models.
"
How should I select the features for predicting diseases (in particular when patients specify their health issues)?,"
My aim is to train a model for predicting diseases. Now, according to this Wikipedia article, diseases are classified based on the following criteria in general:

Causes (of the disease)
Pathogenesis (the mechanism by which the disease progresses)
Age
Gender
Symptoms (of the disease)
Damage (caused by the disease)
Organ type (e.g. heart disease, liver disease, etc.)

Are these features used for predicting diseases universally (i.e. all types of diseases)? I don't think so. There can be other attributes as well. For example, traveling in the case of coronavirus.
So, are there better features for predicting diseases?
Or which ones among them are better than the others, when patients specify their health issues?
","['machine-learning', 'datasets', 'prediction', 'feature-selection', 'healthcare']","
So for Medical Prognosis, there are some variables that commonly come up like Age, Sex, Ascites, Hepato, Spider, Status of the disease and many others but it depends on the disease. You'll commonly encounter these variables if you're doing regression or classification.
Also, if you're reading Radiology Reports for getting the input for the model then you also have to take care of jargons. The same symptoms can be written in various ways but all point towards the same prognosis i.e., there can be synonyms for labels. Try reading this to get more information on how we can do information extraction from Radiology Reports. This is the famous CheXpert paper
"
How to prove that gradient descent doesn't necessarily find the global optimum?,"
How can I prove that gradient descent doesn't necessarily find the global optimum?
For example, consider the following function
$$f(x_1, x_2, x_3, x_4) = (x_1 + 10x_2)^2 + 5x_2^3 + (x_2 + 2x_3)^4 + 3x_1x_4^2$$
Assume also that we can't find the optimal value for the learning rate because of time constraints.
","['gradient-descent', 'proofs', 'learning-rate']","
You can find by yourself a counterexample that, in general, GD is not guaranteed to find the global optimum!
I first advise you to choose a simpler function (than the one you are showing), with 2-3 optima, where one is the global and the other(s) are local. You don't need neural networks or any other ML concept to show this, but only basic calculus (derivatives) and numerical methods (i.e. gradient descent). Just choose a very simple function with more than one optimum and apply the basic gradient descent algorithm. Then you can see that, if you start gradient descent close to one local optimum (i.e. you choose an initial value for $x$ or $\theta$, depending on your notation for the variable of the function) and then you apply gradient descent (for some iterations), you will end up in that close local optimum, from which you cannot escape, after having applied the gradient descent steps.
See also the question Does gradient descent always converge to an optimum? and For convex problems, does gradient in Stochastic Gradient Descent (SGD) always point at the global extreme value?
"
How to apply hyperparameter optimization on Monte Carlo Tree Search?,"
I have a basic MCTS agent for the game of Hex (a turn based game). I want to tune the parameters of UCT (the Cp parameter) and the number of rollouts parameter.
Where do I have to begin? The problem is that the agent is smart enough to always win if it plays first against another agent. So I don't know how to do the evaluation of each pair of hyperparameters.
If anyone has any ideas let me know.
","['reinforcement-learning', 'monte-carlo-tree-search', 'monte-carlo-methods']",
Interpretation of inverse matrix in mean calculation in Gaussian Process,"
The formula for mean prediction using Gaussian Process is $k(x_*, x)k(x, x)^{-1}y$, where $k$ is the covariance function. See e.g. equation 2.23 (in chapter 2) from Gaussian Processes for Machine Learning (2006) by C. E. Rasmussen & C. K. I. Williams.
Oversimplifying, the mean prediction of the new point $y_*$ is the weighted average of previously observed $y$, where the weights are calculated by the $k(x_*,x)$ and normalized by $k(x,x)^{-1}$. 
Now, the first part $k(x_*, x)$ is easy to interpret. The closer the new data point lies to the previously observed data points, the greater their similarity, the higher will be the weight and impact on the prediction.
But how to interpret the second part $k(x, x)^{-1}$? I presume this makes the weight of the points in the clusters greater than the outliers. Am I correct?
","['math', 'gaussian-process']","
Mathematical Interpretation
Note that equation (2.23) is simply calculating the conditional distribution of equation (2.21) and then finding the mean. Your question reduces to:
""Given normal variables $X$ and $Y$, why is $\mathbb{E}[Y|X] = \mu_y + Cov(X, Y)Cov(Y, Y)^{-1}(x - \mu_x)$? (note: in the book, $\mu_x = \mu_y = 0$)
Deriving the conditional probability mean is complicated (see The Bivariate Normal Distribution, page 3). A more intuitive look can be seen in the first graph in this page.
Here, the mean of $Y|X$ is linear in what value $X$ takes. The line starts at the intercept $\mu_y$, and increases with slope $\rho (\frac{\sigma_y}{\sigma_x}) = \frac{Cov(X,Y)}{\sigma_x \sigma_y}\left(\frac{\sigma_y}{\sigma_x}\right)=\frac{Cov(X,Y)}{\sigma_x^2} = \frac{Cov(X,Y)}{Var(x)} = \frac{Cov(X,Y)}{Cov(X,X)}$. So the mathematical interpretation of $Cov(X,Y)Cov(X,X)^{-1}$ is that it is the slope of the relationship between the mean of $Y|X$ and the value of $x$ that you are given. As you are given a higher value $x$, say $x + \delta$, then the mean of $Y|X$ raises by $Cov(X,Y)Cov(X,X)^{-1}\delta$.
Why is there even a $Cov(X,Y)Cov(X,X)^{-1}$ term there? For some reason, multiplying $Y|X$ by a $Cov(X,Y)Cov(X,X)^{-1}$ term makes $Y|X$ completely independent of $X$ (which makes sense as the definition of ""conditional probability"", because you are already given a value of $X$). This is just a mathematical property, I don't know if there's an intuitive explanation as to why.
Human Interpretation
In case your post just want an intuition as to why there is a $Cov(X,X)^{-1}$ in the prediction of a Gaussian process (and ignoring the conditional probability fluff), I don't think there's a real basis for this, it would only be coincidental as the authors simply used the conditional probability mean formula, but I would guess $Cov(X,X)^{-1}$ somehow normalizes the values of covariance matrix $Cov(X_*, X)$.
For example, if the training set $X$ has a lot of outliers and therefore extremely high variance (e.g. all non-diagonal entries in millions), then it is very likely that $Cov(X_*, X)$ would also be extremely high as $X_*$ follows the same distribution as $X$ (unless each data in $X_*$ matches the exact same variance in $X$). It doesn't make sense to multiply $y$ by millions though, as $y$ is already a somewhat decent estimator/prior.
It makes more sense to normalize $Cov(X_*, X)$ by dividing it with the training data variance $Cov(X,X) = Var(X)$ so that the ratio $Cov(X_*, X)Cov(X, X)^{-1}$ tends to be more closer to 1 when $X_*$ follows the same distribution as $X$ (which should be the case). If the ratio is exactly 1, then $X_*$ has the exact same distribution as $X$, so you just return the prior estimate $y$. If the ratio is far away from $1$, then the test set distribution is wildly different than the training set distribution, so you return a number far away from $y$.
"
"Are the labels updated during training in the algorithm presented in ""An algorithm for correcting mislabeled data""?","
I am trying to understand an algorithm for correcting mislabeled data in the paper An algorithm for correcting mislabeled data (2001) by Xinchuan Zeng et al. The authors are suggesting to update the output class probability vector using the formula in equation 4 and class label in equation 5. 
I am wondering: 

Are they updating labels while training, starting from very first back-propagation? 
It seems like if we train on the same data and then predict labels on the same data, it would be the same as what the authors are suggesting. Does it make sense or I misunderstood?

","['neural-networks', 'deep-learning', 'papers', 'multilayer-perceptrons', 'multi-label-classification']",
Why are reinforcement learning methods sample inefficient?,"
Reinforcement learning methods are considered to be extremely sample inefficient.
For example, in a recent DeepMind paper by Hessel et al., they showed that in order to reach human-level performance on an Atari game running at 60 frames per second they needed to run 18 million frames, which corresponds to 83 hours of play experience. For an Atari game that most humans pick up within a few minutes, this is a lot of time.
What makes DQN, DDPG, and others, so sample inefficient?
","['reinforcement-learning', 'dqn', 'papers', 'ddpg', 'sample-efficiency']","
I will try to give a broad answer, if it's not helpful I'll remove it. 
When we talk about sampling we are actually talking about the number of interaction required to an agent to learn a good model of the environment.
In general I would say that there are two issues related to sample efficiency:
1 the size of the 'action'+'environment states' space 2 the exploration strategy used.
Regarding the first point, in reinforcement learning is really easy to encounter situations in which the number of combinations of possible actions and possible environment states explode, becoming intractable. Lets for example consider the Atari game from the Rainbow paper you linked: the environment in which the agent operate in this case is composed of rgb images of size (210, 160, 3). This means that the agent 'see' a vector of size 100800. The actions that an agent can take are simply modifications of this vector, e.g. I can move to the left a character, slightly changing the whole picture. Despite the fact that in lot of games the number of possible actions is rather small, we must keep in mind that there are also other objects in the environment which change position as well. What other object/enemies do obviously influence the choice of the best action to perform in the next time step. To a high number of possible combinations between actions and environment states is associated a high number of observations/interaction required to learn a good model of the environment. Up to now, what people usually do is to compress the information of the environment (for example by resizing and converting the pictures to grayscale), to reduce the total number of possible states to observe. DQL itself is based on the idea of using neural networks to compress the information gathered from the environment in a dense representation of fixed size. 
For what concern the exploration strategy, we can again divide the issue in subcategories: 1 how do we explore the environment 2 how much information do we get from each exploration. Exploration is usually tuned  through the greedy hyper-parameter. Once in a while we let the agent perform a random action, to avoid to get stuck in suboptimal policies (like not moving at all to avoid to fall into a trap, eventually thanks to a greedy action the agent will try to jump and learn that it gives a higher reward). Exploration comes with the cost of more simulations to perform, so people quickly realise that we can't rely only on more exploration to train better policies. One way to boost performance is to leverage not only the present iteration but also past interactions as well, this approach is called experience replay. The underline idea is to update the q-values depending also on weighted past rewards, stored in a memory buffer. Other approaches point to computation efficiency rather than decreasing the amount of simulations. An old proposed technique that follow this direction is prioritised sweeping Moore et al. 1993, in which big changes in q values are prioritised, i.e. q-values that are stable over iterations are basically ignored (this is a really crude way to put it, I have to admit that I still have to grasp this concept properly). Both this techniques were actually applied in the Rainbow paper. 
On a more personal level (just pure opinions of mine from here) I would say that the problem between RL agents and humans is the fact that we (humans) have lot of common sense knowledge we can leverage, and somehow we are able, through cognitive heuristics and shortcuts, to pay attention to what is relevant without even being aware of it. RL agents learn to interact with an environment without any prior knowledge, they just learn some probability distribution through trial and errors, and if something completely new happens they have no ability at all to pick up an action based on external knowledge. 
One interesting future direction in my opinion is reward modelling, described in this video: https://youtu.be/PYylPRX6z4Q 
I particularly like the emphasis on the fact that the only true thing that human are good at is judging. We don't know how to design proper reward functions, because again, most of the actions we perform in real life are driven by reward of which we are not aware, but we are able in a glimpse to say if an agent is performing a task in a proper way or not. Combining this 'judging power' into RL exploration seems to be a really powerful way to increase sample efficiency in RL.
"
What is the type of problem requiring to rate images on a scale?,"
I'm new to the topic, but I've used some off the shelf knowledge about computer vision for classifying images.
For example, you can easily generate labels that can determine whether or not e.g. a cloud is in the image. However, what is the general type of problem called where you want to assign a value, or rate the image on a scale - in this example, the degree of cloudiness in the image? 
What are useful algorithms or techniques for addressing this type of problem?
","['machine-learning', 'classification', 'computer-vision', 'terminology', 'regression']",
What are the main differences between skip-gram and continuous bag of words?,"
The skip-gram and continuous bag of words (CBOW) are two different types of word2vec models.
What are the main differences between them? What are the pros and cons of both methods?
","['natural-language-processing', 'comparison', 'word2vec', 'cbow', 'skip-gram']","
So as you're probably already aware of, CBOW and Skip-gram are just mirrored versions of each other. CBOW is trained to predict a single word from a fixed window size of context words, whereas Skip-gram does the opposite, and tries to predict several context words from a single input word.
Intuitively, the first task is much simpler, this implies a much faster convergence for CBOW than for Skip-gram, in the original paper (link below) they wrote that CBOW took hours to train, Skip-gram 3 days. 
For the same logic regarding the task difficulty, CBOW learn better syntactic relationships between words while Skip-gram is better in capturing better semantic relationships. In practice, this means that for the word 'cat' CBOW would retrive as closest vectors morphologically similar words like plurals, i.e. 'cats' while Skip-gram would consider morphologically different words (but semantically relevant) like 'dog' much closer to 'cat' in comparison.
A final consideration to make deals instead with the sensitivity to rare and frequent words. Because Skip-gram rely on single words input, it is less sensitive to overfit frequent words, because even if frequent words are presented more times that rare words during training, they still appear individually, while CBOW is prone to overfit frequent words because they appear several time along with the same context. This advantage over frequent words overfitting leads Skip-gram to be also more efficient in term of documents required to achieve good performances, much less than CBOW (and it's also the reason of the better performances of Skip-gram in capturing semantical relationships).
Anyway, you can find some comparisons in the original paper (section 4.3). 
Mikolov et al. 2013
About the architecture, there's not much to say. They just randomly initialised word embedding for each word, then a projection matrix NxD (number of context words times embeddings dimension) is generated at each iteration, there is no hidden layer, the vectors are just averaged together and then fed into an activation function to predict index probabilities in a vector of dimension V (the size of the vocabulary).
For a more specific explanation (even Mikolov's paper lack some detail) I suggest checking this blog page (Words as Vectors), even though the model described there do apply a hidden layer, unlike the original architecture. 

"
How to draw backup diagram when reward is in expectation but next state is iterated?,"
I am working through Sutton and Barto's RL book. So far in the text, when backup diagrams are drawn, the reward and next state are iterated together (i.e. the equations always have $\sum_{s',r}$), because the text uses the four-place function $p(s',r|s,a)$. Starting from a solid circle (state-action pair), each edge has a reward labeled along the edge and the next state labeled on the open circle. (See page 59 for an example diagram, or see Figure 3.4 here.)
However, exercise 3.29 asks to rewrite the Bellman equations in terms of $p(s'|s,a)$ and $r(s,a)$. This means that the reward is an expected value (i.e. we don't want to iterate over rewards like $\sum_r \cdots (r + \cdots)$), whereas the next states should be iterated (i.e. we want something like $\sum_{s'} p(s'|s,a) (\cdots)$).
I think writing the Bellman equations themselves isn't too difficult; my current guess is that they look like this: $$v_\pi(s) = \sum_a \pi(a|s) \left(r(s,a) + \gamma \sum_{s'} p(s'|s,a) v_\pi(s')\right)$$
$$q_\pi(s,a) = r(s,a) + \gamma \sum_{s'} p(s'|s,a) \sum_{a'} \pi(a'|s') q_\pi(s',a')$$
My problem instead is that I want to be able to draw the backup diagrams corresponding to these equations. Given the ""vocabulary"" for backup diagrams given in the book (e.g. solid circle = state-action pair, open circle = state, rewards along the edge, probabilities below nodes, maxing over denoted by an arc), I don't know how to represent the fact that the reward and next state are treated differently. Two ideas that don't seem to work:

If I draw a bunch of edges after the solid circle, that looks like I'm iterating over rewards.
If I come up with a special kind of edge that represents an expected reward, then it looks like only a single next state is being considered.

","['reinforcement-learning', 'markov-decision-process']",
Why isn't the implementation of my policy evaluation for a simple MDP converging?,"
I am trying to code out a policy evaluation algorithm to find the $V^\pi(s)$ for all states. The following diagram below shows the MDP. 

In this case i let p = q = 0.5.
the rewards for each states are independent of action. I.e $r(\sigma_0)$ = $r(\sigma_2)$ = 0,$r(\sigma_1)$ = 1, $r(\sigma_3)$ = 10. Terminal state is $r(\sigma_3)$
I have the following policy, {0:1, 1:0, 2:0}, where key is the state and value is the action. 0 for $a_0$ and 1 for $a_1$. 
#Policy Iteration solver for FUN
class PolicyEvaluation:
    def __init__(self, policies):
        self.N = 3
        self.pi = policies
        self.actions = [0, 1] # a0 and a1
        self.discount = 0.7
        self.states = [i for i in range(self.N + 1)]


    def terminalState(self, state):
        return state == 3

    # assume p = q = 0.5
    def succProbReward(self, state):
        # (newState, probability, reward)
        spr_list = []
        if (state == 0 and self.pi[state] == 0):
            spr_list.append([1, 1.0, 1])
        elif (state == 0 and self.pi[state] == 1):
            spr_list.append([2, 1.0, 0])
        elif (state == 1 and self.pi[state] == 0):
            spr_list.append([2, 0.5, 0])
            spr_list.append([0, 0.5, 0])
        elif (state == 2 and self.pi[state] == 0):
            spr_list.append([1, 1.0, 0])
        elif (state == 2 and self.pi[state] == 1):
            spr_list.append([3, 0.5, 10])
            spr_list.append([2, 0.5, 0])
        return spr_list


def policyEvaluation(mdp):
    # initialize
    V = {} 
    for state in mdp.states:
        V[state] = 0

    def V_pi(state):
        return sum(prob * (reward + mdp.discount*V[newState]) for prob, reward, newState in
        mdp.succProbReward(state))

    while True:
    # compute new values (newV) given old values (V)
        newV = {}
        for state in mdp.states:
            if mdp.terminalState(state):
                newV[state] = 0
            else:
                newV[state] = V_pi(state)

        if max(abs(V[state] - newV[state]) for state in mdp.states) < 1e-10:
            break
        V = newV
        print(V)
    print(V)



pE = PolicyEvaluation({0:1, 1:0, 2:0})
print(pE.states)
print(pE.succProbReward(0))
policyIteration(pE)

I've tried to run the code above to find the values for each state, however, I am not converging with my values. 
Is there something wrong that I did?
","['reinforcement-learning', 'markov-decision-process', 'implementation', 'convergence', 'policy-evaluation']",
How should I deal with variable input sizes for a neural network classifier?,"
I am currently working on a project, where I have a sensor in a shoe that records the $X, Y, Z$ axes, from an acceleration and gyroscope sensor. Every millisecond, I get 6 data points. Now, the goal is, if I do an action, such a jumping or kicking, I would use the sensor's output to predict that action being done.
The issue: If I jump, for example, one time I may get 1000 data points, but, in another, I get 1200 amounts, meaning the size of the input is different. 
The neural networks I've studied so far require the input size to be constant to predict a $Y$ value, however, in this case, it isn't. I've done some research on how to make a neural network with variable sizes, but haven't been able to find one which works. It's not a good idea to crop the input to a certain size, because then I am losing data. In addition, if I just resize the smaller trials by putting extra $0$s, it skews the model. 
Any suggestions on a model that would work or how to better clean the data?
","['neural-networks', 'classification', 'data-preprocessing']","
It is much simpler to process the data in a different way. Since you're using temporal data a common practice is to define a priori a minimum time-step, usually called $\textit{granularity}$, which must be bigger than you're sensor responsiveness. Using this granularity value you'll then be able split your data into intervals, and you can then combine each instance belonging to an interval with a function that you like. Most common choice is obviously averaging the values, but also summing then could be a choice or a moving average. 
Don't think that in this way you will loose information, preprocessing is also called data cleaning for a reason, not aways raw data are better. 
As a last note, I would suggest you to look into 'Machine learning for the quantified self', there is growing literature going on about using sensors to train predictive models about movements and body measures like heart-rate. You might find something about preprocessing for this specific sensor-applications. 
"
What AI technologies were used in earlier versions of StarCraft?,"
I remember back in the 2000s, it was, and still is possible to play against computers in StarCraft BroodWar. They were not as good as pro players, but still reasonably smart. 
What AI technologies were used in earlier versions of StarCraft: Brood War?
","['game-ai', 'reference-request']",
Is there an efficient way of determining the layers with the best performance as feature extractors in GoogleNet?,"
I am using a caffe model of pre-trained GoogleNet trained on ImageNet from here for image retrieval task (place recognition, more specifically).

I would like to know the layer with best performance in feature extraction. Its official paper suggests that:

The strong performance of shallower networks on this task suggests
  that the features produced by the layers in the middle of the network
  should be very discriminative.

There is also a project deepdream suggests that:

The complexity of the details generated depends on which layer's
  activations we try to maximize. Higher layers produce complex
  features, while lower ones enhance edges and textures, giving the
  image an impressionist feeling.

Searching the web, I found a github page suggesting pool5/7x7_s1layer as feature extractor without specific convincing reasons.
What I am doing now is quite cumbersome in which I extract features from each individual layer, apply scipy euclidean distance measurement to find a query in the reference database and  the judgment is based on precision-recall curve and my top 3 results are as follows for one dataset:

inception_3a/3x3
inception_4a/5x5
inception_4b/output

Considering large number of convolutional layers in GoogleNet, my approach is undoubtedly quite inefficient and can be changed to another dataset!
Can anyone suggest an efficient way to figure out the layers with the best performance as feature extractors in GoogleNet?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'image-processing', 'caffe']",
Can denoising auto-encoders be convolutional and fully connected?,"
I have been reading lately on autoencoders a lot. I just wanted to summarize my understanding of denoising autoencoders. As far as I understand they can be 

Fully connected (in which case, they will be over-complete autoencoders)
Convolutional 

The reason I say it should be over-complete is that the objective is to learn new features and I think extra neurons in the latent layer would help. There is no reason to have a lesser number of neurons because compressing is not the objective. I just want to understand is this the right thinking.   
","['neural-networks', 'convolutional-neural-networks', 'autoencoders', 'feedforward-neural-networks']",
What are the advantages and disadvantages of extrinsic and perplexity model evaluation in NLP?,"
In the video Evaluation and Perplexity by Dan Jurafsky, the author talks about extrinsic and perplexity evaluation in the context of natural language processing (NLP).
What are the advantages and disadvantages of extrinsic and perplexity model evaluation in NLP? Which evaluation method is preferred usually, and why?
","['natural-language-processing', 'comparison', 'models']",
"Is this a good approach to solving Atari's ""Montezuma's Revenge""?","
I'm new to Reinforcement Learning. For an internship, I am currently training Atari's ""Montezuma's Revenge"" using a double Deep Q-Network with Hindsight Experience Replay (HER) (see also this article).
HER is supposed to alleviate the reward sparseness problem. But since the reward is annoyingly too sparse, I have also added a Random Network Distillation (RND) (see also this article) to encourage the agent to explore new states, by giving it a higher reward when it reaches a previously undiscovered state and a lower reward when it reaches a state it has previously visited multiple times. This is the intrinsic reward I add to the extrinsic reward the game itself gives. I have also used a decaying greedy epsilon policy.
How well should this approach work? Because I've set it to run for 10,000 episodes, and the simulation is quite slow, because of the mini-batch gradient descent step in HER. There are multiple hyperparameters here. Before implementing RND, I considered shaping the reward, but that is just impractical in this case. What can I expect from my current approach? OpenAI's paper on RND cites brilliant results with RND on Montezuma's Revenge. But they obviously used PPO.
","['reinforcement-learning', 'dqn', 'deep-rl', 'hindsight-experience-replay', 'random-network-distillation']",
What is the simplest classification problem which cannot be solved by a perceptron?,"
What is the simplest classification problem which cannot be solved by a perceptron (that is a single-layered feed-forward neural network, with no hidden layers and step activation function), but it can be solved by the same network if the activation function is swapped out to a differentiable activation function (e.g. sigmoid, tanh)?
In the first case, the training would be done with the perceptron training rule, in the second case with the delta rule.
Note that regression problems cannot be solved by perceptrons, so I'm interested in classification only.
","['classification', 'activation-functions', 'perceptron', 'xor-problem']","
Anything that is not linearly separable cant be solved perceptrons, unless you use feature maps on data to map them to a higher dimension in which it is linearly separable. 
As a simple, concrete example, perceptron cant learn the XOR function.
This page might help you further.
"
Why is dialogue a hard problem in natural language processing?,"
I've just started learning natural language processing from Dan Jurafsky's videos lectures. In that video, minute 4:56, he is stating that dialogue is a hard problem in natural language processing (NLP). Why?
","['machine-learning', 'natural-language-processing']","
First of all, I am not very familiar with details of NLP and NLU systems and concepts, so I will provide an answer based on the slides entitled Natural language understanding in dialogue systems (2013) by David DeVaul, a researcher on the topic.
A dialogue system is composed of different parts or modules. Here's a diagram of an example of a dialogue system.

Each of these modules can introduce errors, which are then propagated to the rest of the pipeline. Of course, this is the first clear issue of such a dialogue system. Other issues or challenges include 

ambiguity of natural language (and there are different types of ambiguity, i.e. see slide number 5), 
synonyms (i.e. the dialogue system needs to handle different words or expressions that mean the same thing), 
context-sensitivity (i.e. the same words or expressions can mean different things in different contexts)
semantic representation (i.e. how to represent semantics)
spontaneous speech (i.e. how to handle stuff like ""hm"", pauses, etc.)

"
What is the current state-of-the-art in unsupervised cross-lingual representation learning?,"
What is the current state-of-the-art in unsupervised cross-lingual representation learning?
","['natural-language-processing', 'reference-request', 'models', 'state-of-the-art']",
What is the purpose of argmax in the PPO algorithm?,"
I'm kinda new to machine learning and still not too solid on math and particularly calculus. I'm currently trying to implement PPO algorithm as described in the spiningUp website : 
 
This line is giving me a hard time : 

What does the $\operatorname{argmax}$ mean, in this context? They are also talking about updating the policy with a gradient ascent. So, is taking argmax with respect to $\theta$ the same as doing:

where $J$ is the min() function?
","['reinforcement-learning', 'gradient-descent', 'policy-gradients', 'proximal-policy-optimization']","
In this case yes, $J$ is the big $\min$ expression and you apply Adam on that. But be careful because they say they do ascent, but automatic differentiation software usually minimizes given function so your $J$ would be $−\min(⋅)$.
"
I created a snake game and fitted the NEAT algorithm and there's issues,"
Below are my Inputs Outputs and fitness function. The snake is learning at a slow rate, and seems to be stagnant, additionally when the snake collides with the food, it gets deleted from the genome, which doesn't make any sense because that's not specified in the collision. Any input would be greatly appreciated
for x, s in enumerate(snakes):
        # inserting new snake head and deleting the tale for movement

        # inputs
        s.x = s.snake_head[0]
        s.y = s.snake_head[1]
        snakeheadBottomDis = win_h - s.y
        snakeheadRightDis = win_w - s.x
        snake_length = len(s.snake_position)
        snakefoodDistEuclidean = math.sqrt((s.x - food.x) ** 2 + (s.y - food.y) ** 2)
        snakefoodDisManhattan = abs(s.x - food.x) + abs(s.y - food.y)
        xdis = s.Xdis()
        ydis = s.Ydis()
        s.dis_list1.append(snakefoodDistEuclidean)
        s.dis_list2.append(snakefoodDisManhattan)
        s.dis_list3.append(s.Xdis())
        s.dis_list4.append(s.Ydis())
        s.hunger_list.append(s.hunger)
        #print('Euclidean: ', dis_list1[-1])
        #print('Manhattan: ', dis_list2[-1])
        #print('X distance from Wall: ', dis_list3[-1])
        #print('Y distance from Wall: ', dis_list4[-1])

        output = nets[snakes.index(s)].activate((s.hunger, s.x, s.y, food.x, food.y, snakeheadBottomDis,
                                                 snakeheadRightDis, snake_length, xdis,ydis,
                                                 snakefoodDisManhattan, snakefoodDistEuclidean,s.dis_list1[-1],s.dis_list1[-2],
                                                 s.dis_list2[-1],s.dis_list2[-2],s.dis_list3[-1],s.dis_list3[-2],
                                                 s.dis_list4[-1],s.dis_list4[-2],s.hunger_list[-1],s.hunger_list[-2]))


        #snake moving animation
        s.snake_position.insert(0, list(s.snake_head))
        s.snake_position.pop()
        s.hunger -= 1

        # Checking distance Euclidean and Manhattan current and last
        if s.dis_list1[-1] > s.dis_list1[-2]:
            ge[x].fitness -= 1

        if s.dis_list1[-1] < s.dis_list1[-2]:
            ge[x].fitness += 0.5

        if s.dis_list1[-1] > s.dis_list2[-2]:
            ge[x].fitness -= 1

        if s.dis_list1[-1] < s.dis_list2[-2]:
            ge[x].fitness += 0.5

        #checking hunger number and if its decreasing
        if s.hunger_list[-1] < s.hunger_list[-2]:
            ge[x].fitness -= 0.1

        # move right
        if output[0] >= 0 and output[1] < 0 and output[2] < 0 and output[
            3] < 0:
            #and s.x < win_w - s.width and s.y > 0 + s.height:
            # ge[x].fitness += 0.5
            s.move_right()

        # move left
        if output[1] >= 0 and output[0] < 0 and output[2] < 0 and output[
            3] < 0:
            #and s.x < 500 - s.width and s.y > 0 + s.height:
            #ge[x].fitness += 0.5
            s.move_left()

        # move down
        if output[2] >= 0 and output[1] < 0 and output[0] < 0 and output[
            3] < 0:
            #and s.x < 500 - s.width and s.y > 0 + s.height:
            # ge[x].fitness += 0.5
            s.move_down()

        # move up
        if output[3] >= 0 and output[1] < 0 and output[2] < 0 and output[
            3] < 0:
            #and s.x < 500 - s.width and s.y > 0 + s.height:
            # ge[x].fitness += 0.5
            s.move_up()

        #adding more fitness if axis aligns
        if s.snake_head[0] == food.x:
            ge[x].fitness += 0.1
        if s.snake_head[1] == food.x:
            ge[x].fitness += 0.1

        # checking the activation function tanh
        # print ('output 0: ', output[0])
        # print('output 1: ', output[1])
        # print ('output 2: ', output[1])
        # print ('output 3: ', output[1])

        # snake poping on other side of screen if screen limit reached
        if s.snake_head[0] >= win_w - s.width:
            s.snake_head[0] = 12
        if s.snake_head[0] <= 11 + s.width:
            s.snake_head[0] = win_w - s.width - 1
        if s.snake_head[1] >= win_h - s.height:
            s.snake_head[1] = s.height + 15
        if s.snake_head[1] <= 11 + s.height:
            s.snake_head[1] = win_h - s.height - 1

        head = s.snake_position[0]
        #s.x < 0 + s.width or s.x > win_w - s.width or s.y < 0 + s.height or \
                #s.y > win_h - s.height or

        #if run into self you die
        if head in s.snake_position[1:]:
            ge[x].fitness -= 10
            snakes.pop(x)
            nets.pop(x)
            ge.pop(x)

        #if hunger reaches 0 you die
        if s.hunger == 0:
            ge[x].fitness -= 5
            snakes.pop(x)
            nets.pop(x)
            ge.pop(x)

        #if snake collides with food award fitness
        if s.getRec().colliderect(food.getRec()):
            ge[x].fitness += 100
            s.hunger = 100
            score += 1
            s.snake_position.insert(0, list(s.snake_head))
            food.y = random.randint(0 + 24, 500 - 24)
            food.x = random.randint(0 + 24, 500 - 24)

    # print(s.hunger)

","['neural-networks', 'python', 'neat']","
Firstly, the key of implementing a good genetic algorithm like the NEAT one is fitness. Now fitness is everything, it tells basically what your snakes will learn. If you have a bad fitness function, your AIs will target the wrong goal. You shouldn't give fitness when a snake is aligning with food, because that's not what you want. What you really want your snake to do is eating food.
So, I suggest you to try giving fitness only when your snake is eating food, and maybe remove some as time passes (because of hunger). Your fitness could even be just the time a snake survived, since to survive your snakes have to eat, and avoid touching themselves! Definitely try this: just giving fitness as time passes!
Secondly, you have to give good inputs to your AIs, so that they have enough information about their environment to optimize their strategy. I suggest you to try inputs that are relative to your snake's position, and direction. X and Y position of the snake and the food are bad inputs, because they're not relative to the snake, they're relative to the origin of your game. So, every second the position of the snake is changing, and this could lead to some distraction.
The snake doesn't need to know his position, he just need to know the distance between him and the food, and the angle between the direction of the snake (him) and the food.
Lastly, you may review your outputs. But this is a minor problem, you should definitely focus on the fitness of your snakes. But, if you want to go deeper, try having two outputs instead of four: speed, and turning velocity. That way, it's easy for your snakes to go straight forward. When a snake wants to go forward with left, right, up, down outputs, it's way harder.
"
How can I solve the zero subset sum problem with hill climbing?,"
I want to solve the zero subset sum problem with the hill-climbing algorithm, but I am not sure I found a good state space for this.
Here is the problem: consider we have a set of numbers and we want to find a subset of this set such that the sum of the elements in this subset is zero.
My own idea to solve this by hill-climbing is that in the first step, we can choose a random subset of the set (for example, the main set is $X= \{X_1,\dots,X_n\}$ and we chose $X'=\{X_{i_1},\dots,X_{i_k}\}$ randomly), then the children of this state can be built by adding an element from $X-X'$ to $X'$ or deleting an element from $X'$ itself. This means that each state has $n$ children. and the objective function could be the sum of the elements in $X'$ that we want to minimize.
Is this a good modeling? Are there better modelings or objective functions that can work more intelligently?
","['ai-design', 'hill-climbing', 'local-search']","
The hill-climbing algorithm to implement is as follows:

The algorithm should take four inputs: as always, there will be a multiset S and integer
k, which are the Subset and Sum for the Subset Sum problem; in addition, there
will be two integers q and r, with roles defined below.
Do the following q times:

(a) Choose a random subset (multiset) $S_0$ of S as the current subset.
(b) Do the following (hill climbing) r times:
i. Find a random neighbor T (see definition of neighbor below) of the current
subset.
ii. If neighbor T has smaller residue, then make T the current subset.
(c) Keep track of the residue of the final current subset when starting with subset
$S_0$.

Return the smallest residue of the q subsets tested by the algorithm.

Definition: Subset (multiset) B ⊆ S is a neighbor of a subset A of S if you can transform
A into B by moving one or two integers from A to B, or by moving one or two integers from
B to A, or by swapping one integer in A with one integer in B.
An easy way to generate a random neighbor B of a subset A of S is as follows:

Order the elements of S as $x_1, x_2, ..., x_n$.
Initialize B to be a clone of A.
Choose two distinct random indices i and j, where $1 ≤ i; j ≤ n$.
if $x_i$ is in A, remove it from B. Otherwise, add xi to B.
if $x_j$ is in A, then with probability 0.5, remove it from B. If $x_j$ is not in A, then with
probability 0.5, add $x_j$ to B.

"
Is my GRU model under-fitting given this plot of the training and validation loss?,"
I was running my gated recurrent unit (GRU) model. I wanted to get an opinion if my loss and validation loss graph is good or not, since I'm new to this and don't really know if that is considered underfitting or not


","['machine-learning', 'training', 'keras', 'overfitting', 'underfitting']","
You should at least crop the plots and add a legend. Maybe also provide some scores (accuracy, auc, whatever you're using).
 Anyway, it doesn't look your model is underfitting, if it was you should have high error at both, training and test phase and the lines would not cross. 
"
How do I deal with an erratic validation set loss when the loss on my training set is monotonically decreasing?,"


Model used
mobilenet_model = MobileNet(input_shape=in_dim, include_top=False, pooling='avg', weights='imagenet')
mob_x = Dropout(0.75)(mobilenet_model.output)
mob_x = Dense(2, activation='sigmoid')(mob_x)

model = Model(mobilenet_model.input, mob_x)

for layer in model.layers[:50]:
    layer.trainable=False

for layer in model.layers[50:]:
    layer.trainable=True

model.summary()

The rest of the code
in_dim = (224,224,3)
batch_size = 64
samples_per_epoch = 1000
validation_steps = 300
nb_filters1 = 32
nb_filters2 = 64
conv1_size = 3
conv2_size = 2
pool_size = 2
epochs = 20
classes_num = 2
lr = 0.000004
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
        'output/train',  # this is the target directory
        target_size= in_dim[0:2],  # all images will be resized to 224*224
        batch_size=batch_size,
        class_mode='categorical') 
#Found 6062 images belonging to 2 classes.
validation_generator = test_datagen.flow_from_directory(
        'output/val',
        target_size=in_dim[0:2],
        batch_size=batch_size,
        class_mode='categorical')
#Found 769 images belonging to 2 classes.
from keras.callbacks import EarlyStopping
#set early stopping monitor so the model stops training when it won't improve anymore
early_stopping_monitor = EarlyStopping(patience=3)
steps_per_epoch = 10
from keras import backend as K

def recall_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

def precision_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])


history = model.fit_generator(
        train_generator,
        steps_per_epoch=2000// batch_size ,
        epochs=50,
        validation_data=validation_generator,
        validation_steps=800// batch_size,
        callbacks = [early_stopping_monitor],
       )
test_generator = train_datagen.flow_from_directory(
        'output/test',
        target_size=in_dim[0:2],
        batch_size=batch_size,
        class_mode='categorical')
loss, accuracy, f1_score, precision, recall = model.evaluate(test_generator)
print(""The test set accuracy is "", accuracy)
#The test set accuracy is  0.9001349538122272

From what I have gathered from this post and this article, I understand that the validation set is much smaller with respect to the training set. I have applied augmentation to the test set due to this and that boosted test set accuracy by 1%.
Please note that the test train split is ""stratified"" as here is a breakdown of each individual class
in test/train/validation folders
Test: Class 0: 7426
      Class 1: 631
Train: Class 0: 928
       Class 1: 80
Val: Class 0: 928
     Class 1: 79

I have used an 80/10/10 split for train/test/val respectively.
Can someone guide me on what to do so that I can ensure the accuracy is  95%+  and the validation loss graph is less erratic?

I am thinking of tuning the learning rate though it doesn't seem to be working by much.
Another suggestion is to use test time augmentation.
Also, the link on fast.ai has a comment like so


That is also part of the reasons why a weighted ensemble of different
performing epoch models will usually perform better than the best
performing model on your validation dataset. Sometimes choosing the
best model or the best ensemble to generalize well isn’t as easy as
selecting the lower loss/higher accuracy model.
4. Should I use L2 regularization in addition to the current dropout?

Applying augmentation of any kind to the validation set is a strict no-no and the dataset is generated by my company which I cannot get more of.
","['deep-learning', 'convolutional-neural-networks', 'classification', 'keras', 'overfitting']",
What is the idea called involving an AI that will eventually rule humanity?,"
It's an idea I heard a while back but couldn't remember the name of. It involves the existence and development of an AI that will eventually rule the world and that if you don't fund or progress the AI then it will see you as ""hostile"" and kill you. Also, by knowing about this concept, it essentially makes you a candidate for such consideration, as people who didn't know about it won't understand to progress such an AI. From my understanding, this idea isn't taken that seriously, but I'm curious to know the name nonetheless.
","['philosophy', 'agi', 'definitions', 'superintelligence', 'singularity']","
I believe the term you are looking for is ""(technological) singularity"".
https://en.wikipedia.org/wiki/Technological_singularity
"
How does the optimization process in hindsight experience replay exactly work?,"
I was reading the following research paper Hindsight Experience Replay. This is the paper that introduces a concept called Hindsight Experience Replay (HER), which basically attempts to alleviate the infamous sparse reward problem. It is based on the intuition that human beings constantly try and learn something useful even from their past failed experiences.
I have almost completely understood the concept. But in the algorithm posited in the paper, I don't really understand how the optimization works. Once the fictitious trajectories are added, we have a state-goal-action dependency. This means our DQN should predict Q-Values based on an input state and the goal we're pursuing (The paper mentions how HER is extremely useful for multi-RL as well).
Does this mean I need to add another input feature (goal) to my DQN? An input state and an input goal, as two input features to my DQN, which is basically a CNN?
Because in the optimization step they have mentioned that we need to randomly sample trajectories from the replay buffer and use those for computing the gradients. It wouldn't make sense to compute the Q-Values without the goal now, because then we'd wind up with duplicate values.
Could someone help me understand how exactly does the optimization take place here?
I am training Atari's ""Montezuma's Revenge"" using a double DQN with Hindsight Experience Replay (HER).
","['reinforcement-learning', 'dqn', 'deep-rl', 'sparse-rewards', 'hindsight-experience-replay']","
Yes, you need to add goal state as an input otherwise you won't know what goal you are trying to pursue. It might take a lot of memory to run HER on ATARI games though, also defining final goal is not straightforward for such environments
"
Medical diagnosis systems based on artificial neural networks,"
Are there any medical diagnosis systems that are already used somewhere that are based on artificial neural networks?
","['neural-networks', 'applications', 'reference-request']",
Is the Cognitive Approach (SOAR) equivalent to the Chinese Room argument?,"
Soar is a cognitive architecture.    

https://en.wikipedia.org/wiki/Soar_(cognitive_architecture)
https://soar.eecs.umich.edu/

There is something called ""the Chinese box"" or ""Chinese room"" argument:    

https://en.wikipedia.org/wiki/Chinese_room
https://plato.stanford.edu/entries/chinese-room/ 

The ""Chinese room"" seems to be begging its question, but that is not what I am asking.  I am asking if there is any literal difference between a tool like ""SOAR"" and the formalism of the ""Chinese box"".  Is SOAR identical or equivalent to a ""Chinese Box""?  
","['philosophy', 'cognitive-science', 'chinese-room-argument', 'cognitive-architecture', 'soar']","
Searle's Chinese room is analogical and is intended to present an easy-to-understand picture of the essential elements and processes of the digital computer. In the room the man (CPU) has a book of intructions (program) for responding to Chinese input questions. That is just one program of many possible programs the room could run. Each different program would be a different instruction book. SOAR would be just one of those books.
"
What are some well-known problems where neural networks don't do very well?,"
Background: It's well-known that neural networks offer great performance across a large number of tasks, and this is largely a consequence of their universal approximation capabilities.  However, in this post I'm curious about the opposite:
Question: Namely, what are some well-known cases, problems or real-world applications where neural networks don't do very well?

Specification:
I'm looking for specific regression tasks (with accessible data-sets) where neural networks are not the state-of-the-art.  The regression task should be ""naturally suitable"", so no sequential or time-dependent data (in which case an RNN or reservoir computer would be more natural).
","['neural-networks', 'deep-learning', 'reference-request', 'applications']","
In theory, most neural networks can approximate any continuous function on compact subsets of $\mathbb{R}^n$, provided that the activation functions satisfy certain mild conditions. This is known as the universal approximation theorem (UAT), but that should not be called  universal, given that there are a lot more discontinuous functions than continuous ones, although certain discontinuous functions can be approximated by continuous ones. The UAT shows the theoretical powerfulness of neural networks and their purpose. They represent and approximate functions. If you want to know more about the details of the UAT, for different neural network architectures, see this answer.
However, in practice, neural networks trained with gradient descent and backpropagation face several issues and challenges, some of which are due to the training procedure and not just the architecture of the neural network or available data.
For example, it is well known that neural networks are prone to catastrophic forgetting (or interference), which means that they aren't particularly suited for incremental learning tasks, although some more sophisticated incremental learning algorithms based on neural networks have already been developed.
Neural networks can also be sensitive to their inputs, i.e. a small change in the inputs can drastically change the output (or answer) of the neural network. This is partially due to the fact that they learn a function that isn't really the function you expect them to learn. So, a system based on such a neural network can potentially be hacked or fooled, so they are probably not well suited for safety-critical applications. This issue is related to the low interpretability and explainability of neural networks, i.e. they are often denoted as black-box models.
Bayesian neural networks (BNNs) can potentially mitigate these problems, but they are unlikely to be the ultimate or complete solution. Bayesian neural networks maintain a distribution for each of the units (or neurons), rather than a point estimate. In principle, this can provide more uncertainty guarantees, but, in practice, this is not yet the case.
Furthermore, neural networks often require a lot of data in order to approximate the desired function accurately, so in cases where data is scarce neural networks may not be appropriate. Moreover, the training of neural networks (especially, deep architectures) also requires a lot of computational resources. Inference can also be sometimes problematic, when you need real-time predictions, as it can also be expensive.
To conclude, neural networks are just function approximators, i.e. they approximate a specific function (or set of functions, in the case of Bayesian neural networks), given a specific configuration of the parameters. They can't do more than that. They cannot magically do something that they have not been trained to do, and it is usually the case that you don't really know the specific function the neural network is representing (hence the expression black-box model), apart from knowing your training dataset, which can also contain spurious information, among other issues.
"
How can the policy iteration algorithm be model-free if it uses the transition probabilities?,"
I'm actually trying to understand the policy iteration in the context of RL. I read an article presenting it and, at some point, a pseudo-code of the algorithm is given : 
What I can't understand is this line : 

From what I understand, policy iteration is a model-free algorithm, which means that it doesn't need to know the environment's dynamics. But, in this line, we need $p(s',r \mid s, \pi(s))$ (which in my understanding is the transition function of the MDP that gave us the probability of landing in the state $s'$ knowing previous $s$ state and the action taken) to compute $V(s)$. So I don't understand how we can compute $V(s)$ with the quantity $p(s',r \mid s, \pi(s))$ since it is a parameter of the environment. 
","['reinforcement-learning', 'comparison', 'model-based-methods', 'model-free-methods', 'policy-iteration']","
The Policy Iteration algorithm (given in the question) is model-based.
However, note that there exist methods that fall into the Generalized Policy Iteration category, such as SARSA, which are model-free.

From what I understand, policy iteration is a model-free algorithm

Maybe this was referring to generalized policy iteration methods.

(Answer based on comments from @Neil Slater.)
"
How does the number of stacked LSTM layers or units in each layer affect the model complexity?,"
I playing around sequence modeling to forecast the weather using LSTM. 
How does the number of layers or units in each layer exactly affect the model complexity (in an LSTM)? For example, if I increase the number of layers and decrease the number of units, how will the model complexity be affected? 
I am not interested in rules of thumb for choosing the number of layers or units. I am interested in theoretical guarantees or bounds.
","['long-short-term-memory', 'computational-learning-theory', 'sequence-modeling', 'vc-dimension']","
In computational learning theory, the VC dimension is a formal measure of the capacity of a model. The VC dimension is defined in terms of the concept of shattering, so have a look at the related Wikipedia article, which briefly describes the fundamental concept of shattering. See also my answer to the question How to estimate the capacity of a neural network? for more details.
The paper Vapnik-Chervonenkis dimension of recurrent neural networks (1998), by Pascal Koirana and Eduardo D. Sontag, partially (because they do not take into account more advanced recurrent neural network architectures, such as the LSTM) answers your question. 
In the paper, the authors show and prove different theorems that state the VC dimension of (standard) recurrent neural networks (RNNs), with different activation functions, such as non-linear polynomials, piecewise polynomials and the sigmoid function.
For example, Theorem 5 (page 70) states

Let $\sigma$ be an arbitrary sigmoid. The VC dimension of recurrent architectures with activation $\sigma$, with $w$ weights and receiving inputs of length $k$, is $\Omega(wk)$. 

The proof of this theorem is given on page 75. 
What does this theorem intuitively tell you? If you are familiar with big-O notation, then you are also familiar with the notation $\Omega(wk)$, which means that $wk$ is, asymptotically, a lower bound on the capacity of the RNN. In other words, asymptotically, the capacity of an RNN with $w$ weights receiving inputs of length $k$ is at least $wk$. How does the capacity of the RNN increase as a function of $w$?
Of course, this is a specific result, which only holds for RNNs with the sigmoid activation function. However, this at least gives you an idea of the potential capacity of an RNN. This theorem will hopefully stimulate your appetite to know more computational learning theory!
The paper On Generalization Bounds of a Family of Recurrent Neural Networks may also be useful, although it has been rejected for ICLR 2019.
"
Continuous control with DDPG: How to eliminate steady state error?,"
Currently I'm working on a continuous control problem using DDPG as my RL algorithm. All in all, things are working out quite well, but the algorithm does not show any tendencies to eliminate the steady state control deviation towards the far end of the episode.
In the graphs you can see what happens: 
In the first graph we see the setpoint in yellow and the controlled continuous parameter in purple. In the beginning, the algorithm brings the controlled parameter close to the setpoint fast, but then it ceases its further efforts and does not try to eliminate the remaining steady state error. This control deviation even increases over time. 

In the second graph, the actual reward is depicted in yellow. (Just ignore the other colors.) I use the normalized control deviation to calculate the reward: $r = \frac{\frac{|dev|}{k}}{1+\frac{|dev|}{k}}$.
This gives me a reward that lies within the interval $]0, 1]$ and has a value of $0.5$ when the deviation $dev$ equals the parameter $k$. (That is the parameter $k$ kind of indicates when half of the work is done)
This reward function is relatively steep for the last fraction of the deviation from $k$ to $0$. So it would definitely be worth the effort for the agent to eliminate the residual deviation. 
However, it looks like the agent is happy with the existing state and the control deviation never gets eliminated. Eventhough the reward is stuck at ~0.85 instead of the maximum achievable 1.

Any ideas how to push the agent into some more effort to eliminate the steady state error? 
(A PID controller would exactly do this by using its I-term. How can I translate this to the RL-algo?)
The state presented to the algo consists of the current deviation and the speed of change (derivatve) of the controlled value. The deviation is not included in the calculation of the reward function, but in the end we wat a flat line with no steady state deviation of course.
Any ideas welcome!
Regards,
Felix
","['deep-rl', 'control-problem', 'ddpg']","
Could you figure out a workaround for this problem of the steady-state error using DDPG?
I'm currently facing the same problem. My application is Satellite attitude control, and no matter which cost function variation I use, the resulting controlled system maintains a constant steady-state error.
It seems to be a limitation of the algorithm. But I can't understand why this happens. Hence, I've decided to use PPO for now. But would like to know if you could solve this problem using DDPG.
All the best,
Wilson
"
Understanding CNN in a few sentences [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I don't know if this is the right place to ask this question. If it is not, please tell me and I remove it.
I've just started to learn CNN and I'm trying to understand what they do and how they do it.
I have written some sentences to describe it:

Let's say that CNN is a mathematical function that adjusts its values based on the result obtained and the desired output.

The values are the filters of the convolutional layers (in other
types of networks would be the weights).
To adjust these values there is a backpropagation method as in all networks.

The result obtained is an image of the same size as the original.
In this image you can see the delimited area.
The goal of the network is to learn these filters.
The overfitting may be because the network has learned where the pixels you are looking for are located.
The filters have as input a pixel of the image and return a 1 or a 0.

My doubt is:
In your own words, Have I forgotten something?
NOTE:
This is only one question. The six points above are affirmative sentences, not questions.
There is only one question mark, and it is on my question.
I have to clarify this because someone has closed my previous question because she/he thinks there were more than one question.
",['convolutional-neural-networks'],"
The 6th point is wrong. Filters do linear combination with group of pixels (depends on filter size) and move over the image and continue to do linear combination operation until it finishes the whole image. 
Please look at 2D convolution in CNN
I would explain CNN following way:
CNN is a kind of neural network that generally contains many convolutional layers and one or two fully connected layers. The purpose of the convolutional layers is feature extraction. Each of the convolutional layers contains many kernels. The purpose of the kernel is to extract different types of features. E. g. Edge, color, shape, and so on. 
Finally, fully connected layers at the end of the network decide the output based on the features that were extracted by convolutional layers. 
The advantage of CNN is that we can learn feature extraction kernels based training images. 
If you look earlier days of image processing. You can
see the image processing community used convolution for feature extraction all the time. They were just using selective kernel for feature extraction. E. g. Sobel operator for edge detection.
"
Neural Network Results always the same,"
I have a GRU model which has 12 features as inputs and I'm trying to predict output power. I really do not understand though whether I choose

1 layer or 5 layers
50 neurons or 512 neuron
10 epochs with a small batch size or 100 eopochs with a large batch size 
Different optimizers and activation functions
Dropput and L2 regurlarization 
Adding more dense layer.
Increasing and Decreasing learning rate

My results are always the same and doesn't make any sense, my loss and val_loss loss is very steep in first 2 epochs and then for the rest it becomes constant with small fluctuations in val_loss.
Here is my code and a figure of losses, and my dataframes if needed:
Dataframe1: https://drive.google.com/file/d/1I6QAU47S5360IyIdH2hpczQeRo9Q1Gcg/view
Dataframe2: https://drive.google.com/file/d/1EzG4TVck_vlh0zO7XovxmqFhp2uDGmSM/view
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from google.colab import files
from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback
tbc=TensorBoardColab() # Tensorboard
from keras.layers.core import Dense
from keras.layers.recurrent import GRU
from keras.models import Sequential
from keras.callbacks import EarlyStopping
from keras import regularizers
from keras.layers import Dropout





df10=pd.read_csv('/content/drive/My Drive/Isolation Forest/IF 10 PERCENT.csv',index_col=None)
df2_10= pd.read_csv('/content/drive/My Drive/2019 Dataframe/2019 10minutes IF 10 PERCENT.csv',index_col=None)

X10_train= df10[['WindSpeed_mps','AmbTemp_DegC','RotorSpeed_rpm','RotorSpeedAve','NacelleOrientation_Deg','MeasuredYawError','Pitch_Deg','WindSpeed1','WindSpeed2','WindSpeed3','GeneratorTemperature_DegC','GearBoxTemperature_DegC']]
X10_train=X10_train.values

y10_train= df10['Power_kW']
y10_train=y10_train.values

X10_test= df2_10[['WindSpeed_mps','AmbTemp_DegC','RotorSpeed_rpm','RotorSpeedAve','NacelleOrientation_Deg','MeasuredYawError','Pitch_Deg','WindSpeed1','WindSpeed2','WindSpeed3','GeneratorTemperature_DegC','GearBoxTemperature_DegC']]
X10_test=X10_test.values

y10_test= df2_10['Power_kW']
y10_test=y10_test.values




# scaling values for model


x_scale = MinMaxScaler()
y_scale = MinMaxScaler()

X10_train= x_scale.fit_transform(X10_train)
y10_train= y_scale.fit_transform(y10_train.reshape(-1,1))
X10_test=  x_scale.fit_transform(X10_test)
y10_test=  y_scale.fit_transform(y10_test.reshape(-1,1))


X10_train = X10_train.reshape((-1,1,12)) 
X10_test = X10_test.reshape((-1,1,12))



Early_Stop=EarlyStopping(monitor='val_loss', patience=3 , mode='min',restore_best_weights=True)



# creating model using Keras
model10 = Sequential()
model10.add(GRU(units=200, return_sequences=True, input_shape=(1,12),activity_regularizer=regularizers.l2(0.0001)))
model10.add(GRU(units=100, return_sequences=True))
model10.add(GRU(units=50))
#model10.add(GRU(units=30))
model10.add(Dense(units=1, activation='linear'))
model10.compile(loss=['mse'], optimizer='adam',metrics=['mse']) 
model10.summary() 

history10=model10.fit(X10_train, y10_train, batch_size=1500,epochs=100,validation_split=0.1, verbose=1, callbacks=[TensorBoardColabCallback(tbc),Early_Stop])


score = model10.evaluate(X10_test, y10_test)
print('Score: {}'.format(score))



y10_predicted = model10.predict(X10_test)
y10_predicted = y_scale.inverse_transform(y10_predicted)

y10_test = y_scale.inverse_transform(y10_test)


plt.scatter( df2_10['WindSpeed_mps'], y10_test, label='Measurements',s=1)
plt.scatter( df2_10['WindSpeed_mps'], y10_predicted, label='Predicted',s=1)
plt.legend()
plt.savefig('/content/drive/My Drive/Figures/we move on curve6 IF10.png')
plt.show()


","['machine-learning', 'tensorflow', 'python', 'keras']",
How do you perform a gradient based adversarial attack on an SVM based model?,"
I have an SVM currently and want to perform a gradient based attack on it similar to FGSM discussed in Explaining And Harnessing
Adversarial Examples.
 
I am struggling to actually calculate the gradient of the SVM cost function with respect to the input (I am assuming it needs to be w.r.t input).
Is there a way to avoid the maths (I am working in python if that helps?)
","['objective-functions', 'gradient-descent', 'papers', 'support-vector-machine', 'adversarial-ml']","
A way to avoid computing the SVM loss by hand is to use a differentiable programming framework, such as JAX. These frameworks will automatically calculate gradients using automatic differentiation.
If you can write down the SVM loss using numpy operations then you can use the framework's tools to get a function which evaluates the gradient with respect to any argument.
In JAX this would look like:
import jax
import jax.numpy as jnp
def hinge_loss(x, y, theta):
    # x is an nxd matrix, y is an nx1 matrix
    y_hat = model(x, theta) # returns nx1 matrix, model parameters theta
    return jnp.maximum(0, 1 - y_hat * y)

hinge_loss_grad = jax.grad(hinge_loss)
# hinge_loss_grad takes an x, y, theta and returns gradient of hinge loss wrt x

"
Is it okay to have wide variations within one of the classes for binary classification tasks?,"
Say I am using a convolutional network to classify pictures of my face versus anyone else's face in the world.
So let's take 10000 pictures of me, and 10000 pictures of other people.
And let's do three experiments where we train a binary classifier:
1) The 10000 ""other"" pictures are of 1 other person.
2) The 10000 ""other"" pictures are of ~10 other people (approximately balanced, so about 1000 pictures per person).
3) The 10000 ""other"" pictures are of ~10000 other people.
I only have one question but here are some different perspectives on it:

Are any of these cases categorically harder to solve than the others?
Are they the same difficulty, or close?
Are there known considerations to make when tuning the model for each
of the cases? (like maybe case (3) has a sharper minimum in the loss
function than (1) so we need to use a different optimisation approach)

","['neural-networks', 'classification']",
Why is the validation performance better than the training performance?,"
I am training a classifier to identify 24 hand signs of American Sign Language. I created a custom dataset by recording videos in different backgrounds for each of the signs and later converted the videos into images. Each sign has 3000 images, that were randomly selected to generate a training dataset with 2400 images/sign and validation dataset with the remaining 600 images/sign.

Total number of images in entire dataset: 3000 * 24 = 72000
Training dataset: 2400 * 24 = 57600
Validation dataset: 600 * 24 = 14400
Image dimension (Width x Height): 1280 x 720 pixels

The CNN architecture used for training
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.25),

    Dense(NUM_CLASSES, activation='softmax')
])

Training parameters:
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32
NUM_CLASSES = 24
train_datagen = ImageDataGenerator(rescale = 1./255,
                                   width_shift_range=0.1,
                                   height_shift_range=0.1,
                                   zoom_range=0.1,
                                   fill_mode='constant')
EPOCHS = 20
STEPS_PER_EPOCH = TRAIN_TOTAL // BATCH_SIZE
VALIDATION_STEPS = VALIDATION_TOTAL // BATCH_SIZE

callbacks_list = [
    tf.keras.callbacks.EarlyStopping(monitor = 'accuracy',
                                     min_delta = 0.005,
                                     patience = 3),
    tf.keras.callbacks.ModelCheckpoint(filepath = 'D:\\Models\\HSRS_ThesisDataset_5Mar_1330.h5',
                                       monitor= 'val_loss',
                                       save_best_only = True)
]

optimizer = 'adam'

The model accuracy and model loss graph is shown in the figure below:

The results obtained at the end of the training are

Train acc: 0.8000121
Val acc: 0.914441

I read this article explaining why the validation loss is lower than the training loss I want to know:

Is it because of the smaller dataset and random shuffling of the images?
Is there any way to improve the condition without changing the dataset?
Will this have a very detrimental effect on the model performance in real test cases? If not, can I just focus on improving the training accuracy of the overall model?

","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'computer-vision', 'datasets']","
Validation dataset: 600 * 24 = 14400

Means that you are augmenting the validation set, right? For an experiment, you can do that and it might take validation accuracy more than train accuracy?
The idea of augmentation in only valid for the training set and you should not change the validation set or test set.  
You can try without the augmentation in the validation set and see the result.
"
What algorithm to use for finding artists/bands in text and differentiating between artists that share the same name,"
Here's the data I have:  

Text from articles from various music blogs & music news sites (title, summary, full content, and sometimes tags).   
I used a couple different NLP/NER tools (nltk, spacy, and stanford NER) to determine the proper nouns in the text, and gave each proper noun a score based on how many times it appeared, and how many NLP tools recognized it as a proper noun. None of these tools are very accurate by themselves for my data
For each proper noun I queried musicbrainz to find artists with that name. (musicbrainz has a lot of data that may be helpful: aliases, discography, associations with other artists)
Any links in the article to Spotify, YouTube etc. and the song name & artist for that link  

I have three goals:    

Determine which proper nouns are artists  
For artists that share the same name, determine which one the text is referring to (based on musicbrainz data)  
Determine if the artist is important to the article, or if they were just briefly mentioned

I have manually tagged some of the data with the correct output for the above 3 goals.    
How would you go about this? Which algorithms do you think would be best for these goals?
Is there any semi-supervised learning I can do to reduce the amount of tagging I need to do?
","['machine-learning', 'natural-language-processing', 'python', 'algorithm']",
How to determine the target value when using ReLU as activation function?,"
Consider the following simple neural network with only one neuron.

The input is $x_1$ and $y_2$, where $-250 < x < 250$ and $-250 < y < 250$
The weights of the only neuron are $w_1$ and $w_1$
The output of the neuron is given by $o = \sigma(x_1w_1 + x_2w_2 + b)$, where $\sigma$ is the ReLU activation function and $b$ the bias.
Thus the cost should be $(o - y)^2$.

When using the sigmoid activation function, the target for each point is usually $0$ or $1$.
But I'm a little confused witch target to use when the activation function is the ReLU, given that it can output numbers greater than 1.
","['neural-networks', 'backpropagation', 'relu', 'mean-squared-error']","
ReLU and sigmoid have different properties (i.e. range), as you already noticed. I've never seen the ReLU being used as the activation function of the output layer (but some people may use it for some reason, e.g. regression tasks where the output needs to be positive). ReLU is usually used as the activation function of a hidden layer. However, in your case, you don't have hidden layers.
The sigmoid function is used as the activation function of the output layer when you need to interpret the output of the neural network as a probability, i.e. a number between $0$ and $1$, given that the sigmoid function does exactly this, i.e. it squashes its input to the range $[0, 1]$, i.e. $\text{sigmoid}(x) = p \in [0, 1]$. When do you need the output of the network to be a probability?  For example, if you decide to use the cross-entropy loss function (which is equivalent to the negative log-likelihood), then the output of your network should be a probability. For example, if you need to solve a binary classification task, then the combination of a sigmoid as the activation function of the output layer and the binary cross-entropy as the loss function is probably what you need.
You could also have a classification problem with more than 2 classes (multi-class classification problem). In that case, you probably need to use a softmax as the activation function of your network combined with a cross-entropy loss function.
See this question How to choose cross-entropy loss in TensorFlow? on Stack Overflow for more info about different cross-entropy functions.
By the way, in general, the targets don't necessarily need to be restricted to be 0 or 1. For example, if you are solving a regression task, your target may just be any number. However, in that case, you may need another loss function (which is often the mean squared error).
"
Model for supervised sequence classification task,"
The Problem
I am currently working on a sequence classification problem I try to solve with machine learning.
The target variable is the current state of a system.
This target variable is following a repeating pattern (eg. [00110200033304...]).
So Transitions are only allowed from or to the ""0"" state if you imagine the system as a state machine.
The only deviation is the time the system stays in one state (eg. iteration_1 = [...0220...], iteration_2 =[...02220...]).
My Question
What would be the best choice of (machine learning) model for this task if one wants to optimize for accuracy?
Restrictions

No restrictions regarding time / space complexity of the model
No restrictions regarding the type of model
The model is only allowed to make wrong classifications in the state transitions phases (e.g. true: [011102...], pred: [001102...]) but must not validate the sequence logic (e.g. true: [011102...], pred: [010102...])

Additional Info / Existing Work

With a lstm neural network (many to 1) I achieved an overall accuracy of 97% in an unseen test set.
Unfortunately the network predicted sequences which violate the sequence logic
(e.g. true: [011102...] predicted: [010102...]) even tough the window length was wide enough to cover at least 3 state transitions.
with simple classification models (only one times step per classification, tested models: feed forward neural network, xgboost / adaboost) an accuracy of ca. 70% are reachable
The input signal is acoustic emission in the frequency domain; Ca. 100 frequency bins / 100 features

Ideas

Maybe the lstm would work better in ""many to many"" designed with a drastic reduced input dimensionality by increased window size?
Maybe a combination of the probability output of the lstm with
a timed Automaton (a state machine with time dependent probability density functions
about the state changes) or a Markov chain model could significantly improve the result?
(But this seems really inelegant)
Is it eventually possible to impose the restriction of valid sequences onto the lstm model?

","['long-short-term-memory', 'supervised-learning', 'time-series', 'sequence-modeling', 'graphs']",
How is the Markovian property consistent in reinforcement learning based scheduling?,"
In Reinforcement Learning, an MDP model incorporates the Markovian property. A lot of scheduling applications in a lot of disciplines use reinforcement learning (mostly deep RL) to learn scheduling decisions. For example, the paper Learning Scheduling Algorithms for Data Processing Clusters, which is from SIGCOMM 2019, uses Reinforcement Learning for scheduling.
Isn't scheduling a non-Markovian process, or am I missing some points? 
","['reinforcement-learning', 'markov-decision-process', 'markov-property']",
Should the RL agent be trained in an environment with real-world data or with a synthetic model?,"
I want to train a reinforcement learning agent in an environment with parameters (for example, the wind speed, sun irradiation, etc.) that change over time. I have recorded a limited amount of data for these time series. 
Should the RL agent be trained in an environment, which replays the recorded time series over and over, or should I model the time series with a generative model first and train the agent in an environment with these synthetic time series?
On the one hand, I think the RL algorithm will perform better with the synthetic data, because there are more diverse trajectories. On the other hand, I don't really have more data, because it is modelled after the same data the RL algorithm could learn from in the first place. 
Are there any papers that elaborate on this topic?
","['reinforcement-learning', 'generative-model', 'time-series', 'environment']",
How should I design the action space of an agent that needs to choose a 2d point and then shoot a cannonball?,"
I'm building a game environment (see the picture below) where an agent should position the mouse on the screen (see the coordinates on the upper right corner) and then click to shoot a cannonball. If the goal (left) is hit. The agent gets a reward based on the elapsed time between this strike and the last one. If three shots are missed, the game is done and the environment will reset.

The env is done so far. But now I wonder what the action space should look like. How can I make the agent choose some x and y coordinates? And how can I combine this with a ""shoot"" action?
","['reinforcement-learning', 'deep-rl', 'environment', 'action-spaces']",
Owner Search for given Server SNO,"
I am newbie to NLP.
I have a excel sheet with following columns: 
    Server_SNo, 
    Owner, 
    Hosting Dept, 
    Bus owner, 
    Applications hosted, 
    Functionality,
    comments
a. Except the Server_SNo, other columns may or may not have data.
b. For some records there is no data except Server_SNo which is the first column.
c. One business owner can own more than 1 Server.
So, out of 4000 records, about 50% of data contain direct mapping for a server with owner.  Remaining 50% of data have combination of other columns (Owner, Hosting Dept, Bus owner, Applications hosted, Functionality and comments)
Here is my problem, I need to find the owner for the given Server_Sno for 50% of data which have combination of other columns (Owner, Hosting Dept, Bus owner, Applications hosted, Functionality and comments).
I have just started to build the code using Python and NLTK.
Is this an NLP problem? Am I going in right direction using Python and NLTK for NLP?
Any insights is appreciated.
-Mani
","['machine-learning', 'natural-language-processing', 'python']","
I don't think this classify as an NLP problem, there is almost no semantic analysis needed, it is more like a classification problem using categorical features. 
NLTK is surely valuable if you want to perform some text 'cleaning' or preprocessing before encoding the variables. The only NLP application that I think you could apply here is some sentiment analysis on the comments to extract extra features (like a number expressing the negativeness or positiveness of each comment). Nevertheless you might want to do that using some pre-trained models cause your dataset is pretty small. 
"
How can I train a neural network to describe the characteristics of a picture?,"
I have collected a set of pictures of people with a text explaining the characteristics of the person on the picture, for example, ""Big nose"" or ""Curly hair"". 
I want to train some type of model that takes in any picture and returns a description of the picture in terms of characteristics.
However, I have a hard time figuring out how to do this. It is not like labeling ""dog"" or ""apple"" because then I can create a set of training data and then evaluate its performance, now I can not. If so I would probably have used a CNN and probably also VGG-16 to help me out.
I only have two ML courses under my belt and have never really encountered a problem like this before. Can someone help me to get in the right direction?
As of now, I have a data set of 13000 labeled images I am very confident it is labeled well. I do not know of any pre-trained datasets that could be of help in this instance, but if you know of one it might help.
Worth noting is that every label is or should at least be unique. If for example there exist two pictures with the same label of ""Big nose"" it is purely coincidental.
","['neural-networks', 'machine-learning', 'image-recognition']","
You can try image captioning. You can train a CNN model for image, and then, on top of that, provide the model embedding to another LSTM model to learn the encoded characteristics. You can directly use the pre-trained VGG-16 model and use the second last layer to create your image embeddings. 
Show and Tell: A Neural Image Caption Generator is a really nice paper to start with. There is an implementation of it in TensorFlow: https://www.tensorflow.org/tutorials/text/image_captioning. The paper focuses on generating caption, but you can provide your 'characteristics' to LSTM, so that it can learn it for each image. 
"
How to deal with features which are here just for training?,"
I'm new to the Data Science field and last week I started to learn about Neural Networks and Deep Learning. To practice, I decided to do a small project: design a Neural Network to predict the winner of an NBA game given the two teams playing. Also, for each match I have 2 stats (let's say number of points and number of free throws) for each of the teams.
In the end, the dataset looks like:
|  ID |  Home |  Away | H_Pts | H_Fts | A_Pts | A_Fts | H_win |
|:---:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
|  1  | Team1 | Team2 |   45  |   10  |   47  |   8   |   1   |
|  2  | Team3 | Team4 |   56  |   6   |   70  |   13  |   0   |
| ... |  ...  |  ...  |  ...  |  ...  |  ...  |  ...  |  ...  |

I implemented the model with TensorFlow/Keras (with the help of this tutorial: Classify structured data with feature columns | TensorFlow Core).
The code is pretty concise:
batch_size = 16
train_ds, test_ds, val_ds = get_datasets()  # The function mainly uses tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
team_names = get_team_names()

feature_columns = []
for column_name in ['Home', 'Away']:
    team = feature_column.categorical_column_with_vocabulary_list(column_name, team_names)
    feature_columns.append(feature_column.indicator_column(team))

for column_name in ['H_Pts', 'H_Fls', 'A_Pts', 'A_Fls']:
    feature_columns.append(feature_column.numeric_column(column_name))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

model = tf.keras.Sequential([
    feature_layer,
    layers.Dense(128, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(1)
])


model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])

model.fit(train_ds, validation_data=val_ds, epochs=10)

loss, accuracy = model.evaluate(test_ds)
print(f""Model evaluation: Loss = {loss} | Accuracy = {accuracy}"")

Trained with just 100 games, I get a great accuracy: 99%. Of course: as it is, the test dataset given to model.evaluate(test_ds) contains everything except the target label H_win. Because H_win can easily be deduced from H_Pts and A_Pts, I get a high accuracy. But this model can't work because by definition you don't know the number of points of each team before the game...
How should I deal with features like these ones which I do not want to predict (so they're not labels) but that should still be considered during the training? Does this kind of feature have a name?
","['ai-design', 'tensorflow', 'keras']",
How to incentivise snake to go straight to apple?,"
I have made a Deep Q Network for the game snake but unfortunately, the snake exhibits some unwanted behavior. It generally does quite well but sometimes it gets stuck in an infinite loop that it can't escape and at the start of the game it takes a very long route to the apple rather than taking a more direct route.
The discount factor per time step is 0.99. The Snake gets a reward of +9 for getting an Apple and -1 for dying. Does anybody have any recommendations on how I should tune the hyperparameters/reward function to minimize this unwanted behavior?
I was thinking that reducing the discount factor may be a good idea?
","['reinforcement-learning', 'rewards']","
Your network finds the infinite loop and notices that this has the best reward (0). This is probably because it hasn't found a path to eating the apple (through exploration).
Reducing the discount factor will only make the long term rewards less valuable. So it will learn eating the apple even slower.
I don't know what you are using as inputs for your network, but maybe changing your reward system could help. For example, you could give your network a reward if it advances in the direction of the apple. This way, your network will be encouraged to find the apple more than it is now.
"
"Is there a way to add ""focus"" on parts of the image when using CNNs?","
I'm building a CNN/3DCNN model that classifies hand gestures. The problem is that the actual gesture occupies only like 1% of the whole image. That means that an enormous amount of convolutional operations is done on the ""empty"" parts of the image, which is useless. 
Is there a way to solve this problem? I was thinking about a MaxPooling layer with a giant pool size, but near features that are extracted from the gesture will be probably ""compressed"" in only 1 feature.
","['deep-learning', 'convolutional-neural-networks']",
Understanding the derivation of the first-order model-agnostic meta-learning,"
According to the authors of this paper, to improve the performance, they decided to 

drop backward pass and using a first-order approximation

I found a blog which discussed how to derive the math but got stuck along the way (please refer to the embedded image below):

Why 
 disappeared in the next line.
How come  (which is an Identity matrix)


Update: I also found another math solution",,
How can I merge outputs of two separate layers so that the overall performance improves?,"
I am training a combined model (fine-tuned VGG16 for images and shallow FCN for numerical data) to do a binary classification. However, the overall AUC score is not what I expected it to be.
Image-only mean AUC after 5-fold cross-validation is about 0.73 and numeric data only 5-fold mean AUC is 0.65. I was hoping to improve the mean AUC by combining the models into one and merging output layers using concatenate in Keras. 
img_output = Dense(256, activation=""sigmoid"")(x_1) 

and 
numeric_output = Dense(128, activation=""relu"")(x_2) 

are the output layers of the two models. And, 
concat = concatenate([img_output, numeric_output])
hidden1 = Dense(64, activation=""relu"")(concat)
main_output = Dense(1, activation='sigmoid', name='main_output')(hidden1)

is the way I concatenated them.
Since image-only performance was better I decided that it might be reasonable to have more dense layers for image_output (256) and ended up using 128 in numeric_output.I could only reach up to mean AUC of 0.67 using a combined model. I think I should rearrange the concatenation of two outputs somehow (by introducing another learnable parameter (like the formula (10) at 3.3 section of this work?, bias?, or something else) to get more boost on mean AUC. However, I was not able to find what options were available.
Hope you have some ideas worth trying.
","['deep-learning', 'convolutional-neural-networks', 'keras', 'performance']","
I'm not sure it's possible to help much because this is an experimental question. I'm afraid the only answer comes with testing many different options. 
I see a little thing that might be making your model a little worse, though:

You're concatenating ""relu"" with ""sigmoid"". 

Placing two different nature values in the same array may make it more difficult for updating weights properly.
A few independent suggestions:

Make the output of the images model have ""relu"" activation as well before the concatenation. Preferrably, use a batch normalization before the image relu and before the number relu (this way you concatenate values that are in very similar ranges).    
Instead of concatenating, you can try Multiply()([img_output, numeric_output]), in this case, both outputs must have the same size, one of them uses ""relu"" or ""linear"", and the other uses ""sigmoid"".   


Now, something important when using AUC: you need big batch sizes, because AUC is dependent on the whole data, it's not like usual metrics/losses that you can take the mean from the results of each batch. 
"
Understanding Bayesian Optimisation graph,"
I came across the concept of Bayesian Occam Razor in the book Machine Learning: a Probabilistic Perspective. According to the book:

Another way to understand
  the Bayesian Occam’s razor effect is to note that probabilities must
  sum to one. Hence $\sum_D'  p(D' |m) = 1$, where the sum is over all possible data sets. Complex
  models, which can predict many things, must spread their probability mass thinly, and hence
  will not obtain as large a probability for any given data set as simpler models. This is sometimes called the conservation of probability mass principle.

The figure below is used to explain the concept:



Image Explanation: On the vertical axis we plot the predictions of 3 possible models: a simple one, $M_1$ ; a medium one, $M_2$ ; and a complex one, $M_3$ . We also indicate the actually observed
    data $D_0$ by a vertical line. Model 1 is too simple and assigns low probability to $D_0$ . Model 3
    also assigns $D_0$ relatively low probability, because it can predict many data sets, and hence it
    spreads its probability quite widely and thinly. Model 2 is “just right”: it predicts the observed data with a reasonable degree of confidence, but does not predict too many other things. Hence model 2 is the most probable model.


What I do not understand is when a complex model is used, it will likely overfit data and hence the plot for a complex model will look like a bell shaped with its peak at $D_0$ while simpler models will more likely have a broader bell shape. But the graph here shows something else entirely. What am I missing here?
","['machine-learning', 'bayesian-optimization']","
The original graph for the aforementioned Bayesian Optimisation is similar to the graph in these slides (slide 18) along with the calculations.
So, according to the tutorial the graph shown should actually have the term $p(D|m)$ on the y-axis, thus making it a generative model.Now the graph starts to make sense, since a model with low complexity cannot produce very complex datasets and will be centred around 0, while very complex models can produce richer datasets which makes them assign probability thinly over all the datatsets (to keep $\sum_{D'}p(D'|m) = 1$).

"
Why do momentum techniques not work well for RNNs?,"
AFAIK, momentum is quite useful when training CNNs, and can speed-up the training substantially without any drop in validation accuracy.
I've recently learned that it is not as helpful for RNNs, where plain SGD is preferred.
For example, Deep Learning by Goodfellow et. al says (section 10.11, page 401):

Both of these approaches have largely been replaced by simply using SGD (even without momentum) applied to LSTMs.

The author talks about LSTMs and ""both of these approaches"" refer to second-order and first-order SGD methods with momentum methods, respectively, according to my understanding.
What causes this discrepancy?
","['recurrent-neural-networks', 'gradient-descent', 'stochastic-gradient-descent', 'momentum']",
Why does the error ensemble use the ceiling of the number of classifiers?,"

What is $y$? Why is $k$ the ceil of $n/2$? What is $y \geq k$?
","['probability', 'ensemble-learning']","

Y ensemble size voting wrong
k = 50% or majority threshold

If you have 11 models. Then the majority of models is anything bigger than 50% of the number of ensemble models. In the example where you have 11 base models. The majority would be anything bigger than N/2 or 11/2. But since 11 is an odd number and cannot be divided by 2. We have to use the python ceiling function to round 5.5 to 6. The in other words. For your ensemble to be wrong. We must look for the probability that Y of them are wrong. Y is ≥ than K and K is ceil(N/2). Thus we must calculate this probability(6 wrong) like shown above summating the possibility of each of these discrete combinations into one
"
How can I use the success and failure data to estimate parameters of a Dirichlet distribution?,"
I have used Beta function to estimate the performance of the agent. I have failure and success data of the task that runs on the agent. 
The parameter $\alpha$ is a number of successful tasks, while $\beta$ is the number of failures. Thus, I can estimate the performance by exploiting the expected value of Beta, as $$\mu = \frac{\alpha} {(\alpha+\beta)}$$
So, I am looking for a similar model, such that its parameter can be estimated from the success and failure data. So far I found Dirichlet distribution. 
What is the expected value of Dirichlet distribution? How I can use the success and failure data to estimate parameters of this distribution?
Let's check the following example:
Suppose that we use a Dirichlet prior represented by $Dirichlet(1, 1, 1)$ and observe $13$ results with $8$ Successful, $2$ Missing, and $3$ Failures. Then we get the posterior to be $Dirichlet(1+8, 1+2, 1+3)$. Then if you define the performance value $\alpha$ to be the expectation of $P(x=Successful)$, 
then $\alpha$ will be $(1+8)/[(1+8)+(1+2)+(1+3)] = 0.56$
Now 
Suppose that we use a Beta prior represented by $Beta(1,1)$ and observe $13$ results with $8$ Successful, and $3$ Failures. Then we get the posterior to be $Beta(1+8, 1+3)$. Then if you define the performance value Pr to be the expectation of $P(x=Successful)$, 
then $\alpha = (1+8)/[(1+8)+(1+3)] = 0.69$
Are my calculations and concept right?
",['probability-distribution'],
How to deal with approximate states when doing path planning?,"
If one is interested in implementing a path planning algorithm that is grid-based, one needs to consider the fact that your grid points will never represent the true state of the robot. 
How is this dealt with?
Suppose we're doing path planning using a grid-based search on the side of the control for a desired grid position as an output state. 
How would you handle the discrepancy between your actual starting position and your discretized starting position? 
I understand that normally you may use an MPC instead, which continually recalculates an optimal path using some type of nonlinear solver, but suppose we don't do this - suppose we restrict ourselves to only a grid search and suppose at after every action the state of the robot has to be considered as living in a particular grid point.  
","['reinforcement-learning', 'robotics', 'path-planning', 'control-theory']",
What are the prerequisites to start using the TensorFlow Probability library? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



I have some familiarity with the regular Tensorflow library and have been able to create a number of working models with it. There are more than enough resources out there to get up and running and answer most questions on the standard library.
But I recently came across the video on some high-level capabilities of the Tensorflow Probability library, TensorFlow Probability: Learning with confidence (TF Dev Summit '19), and I would like to learn it.
The issue is that there are very few resources out there on TFP and given my lack of a formal background in math/statistics, I find myself aimlessly googling to get a grasp of what's going on in the docs. I'm more than willing to invest the time needed, but I just need to know where I can start in terms of resources I can access online.  Specifically, I'm looking to get the necessary domain knowledge needed to work with the library given the lack of courses/tutorials on the library itself.
","['tensorflow', 'probability', 'tensorflow-probability']",
What are recent AI software systems and research papers close to J. Pitrat's ideas?,"
J. Pitrat (born in 1934) was a French leading artificial intelligence scientist (the first to get a Ph.D. in France mentioning ""artificial intelligence""). His blog is still online and of course refer to most of his papers (e.g. A Step toward an Artificial Artificial Intelligence Scientist, etc.) and books, notably Artificial Beings: the conscience of a conscious machine (his last book). He passed away in October 2019. I attended (and presented a talk) at a seminar in his memory.
What are recent AI systems or research papers related to the idea of symbolic AI, introspection, declarative metaknowledge, meta-learning, meta-rules, etc.?
Most of those I know are more than 20 years old (e.g. Lenat Eurisko; I am aware of OpenCyC). I am interested in papers or systems published after 2010 (perhaps AGI papers with actual complex open source software prototypes).
-see also the RefPerSys system-
","['reference-request', 'symbolic-ai', 'meta-learning', 'meta-rules']","
Today one of the challenges is learning representations/concepts that are causally invariant. Once we have good representations then we can work on the reasoning aspect. There are 2 camps of people today. One believes that symbolic manipulation cannot be achieved properly by deep networks. Hence, they separate the task of extracting a lower-dimensional representation of objects from the visual scenes from the task of reasoning with knowledge-graphs. The other camp feels that we can do end-to-end training of a neural network and it can learn how to jointly learn a good lower-dimensional representation for each symbol along with learning how to reason with them. I am no expert at this but here are a few papers that I find are worthy for you to read -

Neuro Symbolic Concent Learner Paper, Code
Learning Reasoning Strategies in End-to-End Differentiable Proving Paper, Code
Neuro-Symbolic Visual Reasoning: Disentangling “Visual” from “Reasoning” Paper
Knowledge Infused Learning (K-IL):
Towards Deep Incorporation of Knowledge in Deep Learning Paper
Visual Concept-Metaconcept Learning Paper Project Page
CVPR 2020 workshop on Neuro-Symbolic Visual Reasoning and Program Synthesis youtube videos

If you are looking for community maintained lists, here is one  list of papers
"
Can transformer be better than RNN for online speech recognition?,"
Does transformer have the potential to replace RNN end-to-end models for speech recognition for online speech recognition? This mainly depends on accuracy/latency and deploy cost, not training cost. Can transformer support low latency online use case and have comparable deploy cost and better result than RNN models?
","['recurrent-neural-networks', 'transformer', 'speech-recognition']","

Are there examples that transformer have better accuracy than RNN end-to-end model like RNN-transducer for speech recognition?
  Can transformer be used for online speech recognition which require low speech-end-to-result latency?
  Does transformer have the potential to replace RNN end-to-end models for speech recognition in most cases in the future? This may mainly depends on accuracy and deploy cost, not training cost.

You can check facebook results on wav2letter on all this:
https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/
https://research.fb.com/publications/scaling-up-online-speech-recognition-using-convnets/
Transformers definitely have a potential in speech especially when combined with faster computatoin methods (hashing) just like in NLP.
The problem with transformers is that you need a lot of GPUs to train them.
"
Can the cross-entropy loss be used for a NLP task with LSTM?,"
I am trying to build an LSTM model to generate Shakspeare-like poems. I have training set $\{s_1,s_2, \dots,s_m\}$, which are sentences of Shakespeare poems, and each sentence contains words $\{w_1,w_2, \dots,w_n\}$.
To my understanding, each sentence $s_i$, for $i=1, \dots,m$ is a random sequence containing the words $w_j$, for $j=1, \dots,n$. The LSTM model is estimated by applying the maximum likelihood (MLE) method, which will use cross-entropy loss for optimization. The use of MLE requires that the samples in the random sequence be independent and identically distributed (i.i.d), however, the word sequence $w_j$ is not i.i.d (since it is non-Markov). Therefore, I am suspicious about using cross-entropy loss for training an LSTM for the NLP task (which seems to be the common practice). 
","['long-short-term-memory', 'cross-entropy', 'maximum-likelihood']",
How do I turn this formula of the average degree of a graph into Python code? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am working through the textbook ""Graph-Based Natural Language Processing and Information Retrieval"", where I've got a question on implementation of this first Latex looking formula/algorithm.
Can you help me turn the formula under 1.2 Graph Properties into python code? Yes, I know there are many other languages, but python is more user-friendly so I'm starting there, and will eventually rewrite it into C.
As I read the above node example, sorry the D and E nodes were cut off.
Node A has two outflowing arrows notating it as their head node, and it is the one tail node.
This first sentence references the Graphs:
To traverse from A to B, If A to B value is sufficient (above Nx), go to B. If A to B value is below Nx, go A to C to D to A to B total cost is 5.2+7+1+8 = 21.20 traverse cost, this makes sense.

This sentence refers to the Latex formula in the book.
Then to start the formula calculation, the average degree of a graph ""a"" is equal to the sum of, one over N, times the sum of the in-degree of vertices? Asserting that the sum is a non zero integer between 1 and N?
Ok, I only loaded one page and hope that's not a TOS violation or causes issue, it's a challenge to find people who understand graph theory.
Let me know what questions you have, but I'm just wanting to get clarification if my understanding is what this page is saying.
","['natural-language-processing', 'python', 'graph-theory']",
Model unfit for some part of spiral data despite low error,"
I'm current testing a model for spiral data. After 500 epoches, loss is 0.04 but the result is still unmatch with some part of the training data. (bottom left)

(source: upsieutoc.com)
The model has 2 hidden tanh x 16 units running with ml5.js. I choose tanh because it seems to be smoother than ReLU. Apart from that they're the same.
Is this thing caused by the model or by ml5 itself?
","['neural-networks', 'classification', 'multilayer-perceptrons']",
How to update edge features in a graph using a loss function?,"
Given a directed, edge attributed graph G, where the edge attribute is a probability value, and a particular node N (with binary features f1 and f2) in G, the algorithm that I want to implement is as follows:

List all the outgoing edges from N, let this list be called edgelist_N.
For all the edges in edgelist_N, randomly assign to the edge attribute a probability value such that the sum of all the probabilities assigned to the edges in the edgelist_N equals to 1.
Take the top x edges (x can be a hyperparameter).
List the nodes in which the edges from step 3 are incoming.
Construct a subgraph with node N, the nodes from step 4 and the edges from step 3. 
Embed the subgraph (preferably using a GNN) and obtain it's embedding and use it with a classifier to predict say f1/f2. 
Propagate the loss so as to update the edge probabilities, that was assigned randomly in step 2.

I do not understand how to do step 7, i.e. update the edge attribute with the loss, so that edges which are more relevant in constructing the subgraph can be assigned a higher probability value.
Any suggestion would be highly appreciated.
Thank you very much.
","['neural-networks', 'classification', 'objective-functions', 'graphs', 'graph-theory']",
How to predict time series with accuracy?,"
I am trying to predict Forex time series. The nature of the market is that 80% of the time the price can not be predicted, but in 20% of the time it can be. For example, if the price drops down very deep, there is 99% probability that there will be a recovery process, and this is what I want to predict.
So , how do I train a feed-forward network the way it would only predict those cases that have 99% of certainty to take place and, for the rest of the cases it would output ""unpredictable"" status?
Imagine that my data set has 24 hours of continuous price data as input (as 1 minute samples), and then as output I want the network to predict 1 hour of future price data. The only restriction I need to implement is that if the network is not ""sure"" that the price is predictable, it would outupt 0s. So, how do I implement safety in predictions the network is outputting?
It seems that my problem is similar to Google Compose, where it predicts the next word as you are typing , for example, if you type ""thank you"", it would add "" very much"" and this would be like 95% correct. I want the same, but it is just that my problem has too much complexity. Google uses RNNs, so maybe I should try a deep network of many layers of RNNs?
","['machine-learning', 'time-series']","
Things like this a really hot topic in research right now, and it's very difficult to get high accuracy on a chaotic system like the stock market. That being said, I would probably recommend preprocessing your data rather than having your primary neural network decide what to accept and what not to. 
For example, in your specific case, you could model a bubble bursting as perhaps a negative exponential drop-off or something of the sort. This could include machine learning too. You could gather historical drops in stock market data, and use some sort of regression (Bayesian would probably work well) to estimate the best function to use as an indicator to whether a steep drop has occurred. If so, then use your neural network specifically to classify the fate of the stock. I would think you would have more success following a specialised route such as this rather than trying to train a network on general trends in the market.
In terms of the structure of your neural network, you may want to consider a convolutional neural network (CNN) instead of a recurrent neural network (RNN). RNNs assume the current point in your time-series depends on all previous points, from the beginning of your data. I wouldn't think this would hold true in general for the stock market. The filters a CNN learns are suited to learning to extract certain features and the CNN will apply the filters to specific portions of the data, in the way it considers optimal. They are both nonlinear models, but the CNN will be less computationally costly to train. You could also try a gradient-boosting regression approach instead of a neural network. That being said, something like an LSTM RNN (long short-term memory) won't necessarily be bad - just my two cents.
"
What's the intuition behind contrastive learning?,"
Recently, I have seen a surge of papers w.r.t contrastive learning (a subset of semi-supervised learning). 
Can anyone give a detailed explanation of this approach with its advantages/disadvantages and what are the cases in which it gives better results?
Also, why it's gaining traction amongst the ML research community?
","['machine-learning', 'deep-learning', 'comparison', 'semi-supervised-learning']","
Contrastive learning is a framework that learns similar/dissimilar representations from data that are organized into similar/dissimilar pairs. This can be formulated as a dictionary look-up problem.
If I conceptually compare the loss mechanisms for:

SimCLR: A Simple Framework for Contrastive Learning of Visual Representations @ https://arxiv.org/abs/2002.05709
MoCo: Momentum Contrast for Unsupervised Visual Representation Learning @ https://arxiv.org/abs/1911.05722

Both MoCo and SimCLR use varients of a contrastive loss function, like InfoNCE from the paper Representation Learning with Contrastive Predictive Coding
\begin{eqnarray*}
\mathcal{L}_{q,k^+,\{k^-\}}=-log\frac{exp(q\cdot k^+/\tau)}{exp(q\cdot k^+/\tau)+\sum\limits_{k^-}exp(q\cdot k^-/\tau)}
\end{eqnarray*}
Here q is a query representation, $k^+$ is a representation of the positive (similar) key sample, and ${k^−}$ are representations of the negative (dissimilar) key samples. $\tau$ is a temperature hyper-parameter. In the instance discrimination pretext task (used by MoCo and SimCLR), a query and a key form a positive pair if they are data-augmented versions of the same image, and otherwise form a negative pair.

The contrastive loss can be minimized by various mechanisms that differ in how the keys are maintained.
In an end-to-end mechanism (Fig. 1a), the negative keys are from the same batch and updated end-to-end by back-propagation. SimCLR, is based on this mechanism and requires a large batch to provide a large set of negatives.
In the MoCo mechanism i.e. Momentum Contrast (Fig. 1b), the negative keys are maintained in a queue, and only the queries and positive keys are encoded in each training batch.
"
Dealing with very similar object classes in object detection,"
I'm working on an object detection problem using Faster R-CNN. I need to identify two object classes, and they are very similar to one another. Furthermore they are similar to a third type of object which should be considered as background. Also, all three of these objects have a lot of variation within them.
In my particular example the two objects of interest are 1) a statue of a particular named person who appears in many statues, 2) a statue of anyone else
Examples:

Also, I want to treat living flesh people, or non-humanoid statues as background.
Now here are some interesting results:

The RPB losses follow the expected trajectory for such a problem, but on the other hand, I really had to have faith and hang in there for the detector losses. They take a while to start decreasing, and I presume it's because there is a relatively sharp trough leading to the the minimum of the loss function with respect to the weights (because the labelled classes are so similar to one another). Miraculously (at least I think so), it does start to kind of work, but not as well as I'd like it to.
My question is as in the title but here are some of my thought processes:

Does the class similarity spoil the bounding box regression? And then does that spoil the class inference in turn?
Would it be better to just detect humanoid statues in general, then train a classifier from scratch on the output of that? (I don't know about this one. Maybe the relevant info is already encoded in the detector of the Faster R-CNN and the added bonus is that the Faster R-CNN gets context from outside of the bounding box... unless the bounding box regression is being spoiled by the class ambiguity)

","['convolutional-neural-networks', 'object-detection']",
Is it a sign of overfitting when validation_loss dips and then goes up with increasingly bigger swings?,"
I am experimenting with a ConvNet to categorize images taken with a depth camera. So far I have 4 sets of 15 images each. So 4 labels. The original images are 680x880 16-bit grayscale. They are scaled down before feeding it to the ImageDataGenerator to 68x88 RGB (each color channel with equal value). I am using the ImageDataGenerator (IDG) to create more variance on the sets. (The IDG does not seem to be able to handle 16-bit grayscale images, nor 8-bit images well, so hence I converted them to RGB).
I estimate the images to be low on features, compared to regular RGB images, because it represents depth. To get a feel for the images, here are a few down scaled examples:



I let it train 4.096 epochs, to see how that would go.
This is the result of the model and validation loss.

You can see that in the early epochs the validation (test / orange line) loss dips, and then goes up and starts to show big swings. Is this a sign of overfitting?
Here is a zoomed in image of the early epochs.

The model loss (train / blue line) reached relatively low values with an accuracy of 1.000. Training again shows repeatedly the same kind of graphs.
Here are the last epochs.
Epoch 4087/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.1137 - accuracy: 0.9286 - val_loss: 216.2349 - val_accuracy: 0.7812
Epoch 4088/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 0.9643 - val_loss: 234.9622 - val_accuracy: 0.7812
Epoch 4089/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 232.9797 - val_accuracy: 0.7812
Epoch 4090/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 238.7082 - val_accuracy: 0.7812
Epoch 4091/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 232.4937 - val_accuracy: 0.7812
Epoch 4092/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0335 - accuracy: 0.9643 - val_loss: 273.6542 - val_accuracy: 0.7812
Epoch 4093/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 258.2848 - val_accuracy: 0.7812
Epoch 4094/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0382 - accuracy: 0.9643 - val_loss: 226.6226 - val_accuracy: 0.7812
Epoch 4095/4096
7/7 [==============================] - 0s 10ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 226.2943 - val_accuracy: 0.7812
Epoch 4096/4096
7/7 [==============================] - 0s 11ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 207.3653 - val_accuracy: 0.7812

Not sure if it is required to know the architecture of the neural network to judge whether this is overfitting on this data set. Anyway, here is the setup.
kernelSize = 3
kernel = (kernelSize, kernelSize)

model = Sequential()
model.add(Conv2D(16, kernel_size=kernel, padding='same', input_shape=inputShape, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, kernel_size=kernel, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=kernel, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(nr_of_classes, activation='softmax'))

sgd = tf.keras.optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.4, nesterov=True)
model.compile(loss='categorical_crossentropy', 
            optimizer=sgd,
            metrics=['accuracy'])

","['convolutional-neural-networks', 'tensorflow', 'keras', 'overfitting', 'convolution']",
Are sentences from the same document independent and identically distributed?,"
I am trying to build an LSTM model to generate Shakspeare-like poems. I have data set $\{s_1, s_2, \dots, s_m \}$, which are sentences of Shakespeare poems, and each sentence contains words $\{w_1, w_2, \dots, w_n \}$. 
I am wondering:  Are different $s_i$ ($i=1, \dots, m$) independent and identically distributed samples (IID)? Are $w_i$ ($i=1, \dots, n$) within each sentence the IID?
","['machine-learning', 'datasets', 'long-short-term-memory', 'cross-entropy']",
Am I overfitting my GAN model?,"
I'm training a DCGAN model on a 320x320 dataset of images and after an hour of training the generator started to generate (on the same latent space noise as during training) images that are identical to the dataset. For example, if my dataset is images of cars, I should expect to see unexisting designs of cars, right? Am I understanding this wrong? I know this is a very general question but I was wondering if this is what should happen and if I should try on different latent space values and then see proper results and not just copies of my dataset?
","['deep-learning', 'generative-adversarial-networks', 'overfitting']",
"Is an oracle that answers only with a ""yes"" or ""no"" dangerous?","
I was thinking about the risks of Oracle AI and it doesn't seem as safe to me as Bostrom et al. suggest. From my point of view, even an AGI that only answers questions could have a catastrophic impact. Thinking about it a little bit, I came up with this Proof:
Lemma

We are not safe even by giving the oracle the ability to only answer yes or no.

Proof

Let's say that our oracle must maximize an utility function $\phi$, there is a procedure that encodes the optimality of $\phi$. Since a procedure is, in fact, a set of instructions (an algorithm), each procedure can be encoded as a binary string, composed solely of 0 and 1, Therefore we will have $\phi \in {\{0,1\}^n}$,  assuming that the optimal procedure has finite cardinality.  Shannon's entropy tells us that every binary string can be guessed by answering only yes/no to questions like: is the first bit 0? and so on, therefore we can reconstruct any algorithm via binary answers (yes / no).

Is this reasoning correct and applicable to this type of AI?
","['philosophy', 'agi', 'proofs', 'risk-management']",
Image dataset for pomegranate plant disease [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am implementing a project on pomegranate plant disease in Machine learning. I want a dataset of all kind images of a healthy and unhealthy part of the pomegranate plant. I got a dataset from Fruit360 but that is only for pomegranate fruits but need for leaves also. Is there anyone who knows any website, link, version control system repository and/or any resource from which I get a dataset for leaves.
","['machine-learning', 'image-recognition', 'datasets', 'image-processing']","
You can find the dataset in the following links:

Pomegranate Disease Detection Using Image Processing
fruits 360 datasets

"
Can you let specific data impact a neural network more than other data?,"
I have a lot of empty values in my dataset, so I want to let my neural network 'learn more' on the rows that have no empty values because these rows are of higher importance.
Is there a way to do this?
","['neural-networks', 'datasets']",
Can experience replay be used for training after completing every single epoch?,"
The DQN implements replay memory. Based on my research, I believe the replay memory starts to get used for training once there is enough experience in the memory buffer. This means the neural network gets trained while the game plays. 
My question is, if I am to play the game 10000 epochs, store all the experiences and then train from the experiences would that have the same effect as training and while running through 10000 epochs? Is it frowned upon to do it this way? Are there any advantages?
","['reinforcement-learning', 'dqn', 'deep-rl', 'experience-replay']",
Use cases for AI inside the software company,"
This question is a bit philosophic and is about making new use cases for software companies. Let me describe what exist for now, why it is not enough, and what is needed.
I know that there are a lot of existing researches in applying ML for software (please don't simply point to this one!), but none of them consider the application of ML for software company, not the software alone.
Existent approaches that apply AI for software engineering tasks consider it as follows:
human1 -> software (big code) <- human2

That means that human1 makes some part of software (that is a part of big code), and human2 reuses some knowledge from it. It may be a bugfix pattern (as e.g. DeepCode does), API usage pattern, repair of code, summarization of code, code search, or whatever else. I think the main reason for this is the original hypothesis of naturalness:

The naturalness hypothesis. Software is a form of human communication; software corpora have similar statistical properties to natural language corpora, and these properties can be exploited to build better software engineering tools.

(from Allamanis et al, page 3)
But imagine one software company. It has:

Some number of engineers,
Some number of managers,
The software product,
Information related to the software product (documentation, bug/task tracking system, code review),
Some number of formal management processes (waterfall, scrum or whatever else),
Some number of informal processes

But none of these models consider the software as a product itself. I mean that we should consider the model as follows:
company -> software product -> customers
              |
              v
           big code

or even
engineer1 -> |
    |
engineer2 -> |
    |
...          | ----> software product ----> customers
    |                   |
engineerN -> |          |
    |                   |
manager  --> |          |
                        v
                     big code

So my questions are:

Are there any cases of investigation of such models? 
Are there any similar cases in related fields, say in general companies (not specifically software ones)?
Are there any analogies (not specifically from software-related domains) where some knowledge can be transferred from a bigger object (big code in our case) to a smaller one (software product)?

Any ideas are welcome.
","['natural-language-processing', 'ai-design', 'philosophy', 'applications']",
Which model should I choose to maximise reward of having chosen two numbers from a list?,"
I am looking for a technique to train a machine learning model to choose two items from a list. 
So, given a list $x=[x_1, x_2, x_3, x_4, \dots, x_n]$, the model needs to choose two elements $(x_i, x_j)$. I have a function $R(x, x_i, x_j)$, which will output the reward of choosing $(x_i, x_j)$ given $x$. 
What type of models should I use, and how should I train it to maximize the reward? 
I've tried using deep reinforcement learning, but I ran into the following problems with implementing the Q-Network:

Variable-length inputs (fixed by using RNN, I think)
The output size grows factorially (for an input set of n elements, there are n choose 2 ways to pick 2 elements, so the network needs to output n choose 2 expected rewards)

","['machine-learning', 'deep-learning', 'reinforcement-learning', 'ai-design', 'rewards']",
Face recognition model loss not decreasing,"
I wrote a script to do train a Siamese Network style model for face recognition on LFW dataset but the training loss doesnt decrease at all. Probably there's a bug in my implementation. Could you please point it out.
Right now my code does:

Each epoch has 0.5M triplets all generated in an online way from data (since the exhaustive number of triplets is too big).
Triplet sampling method: We have a dictionary of {class_id: list of file paths with that class id}. We then create a list of classes which we can use for positive class (some classes have only 1 image so cant be used as positive class). At any iteration we randomly sample a positive class from this refined list and a negative class from the original list. We randomly sample 2 images from positive (as Anchor or A and Positive as P) and 1 from negative (Negative or N). A,P,N form our triplet.
Model used is ResNet with the ultimate (512,1000) softmax layer is replaced with (512,128) Dense layer (no activation). To avoid overfitting, only the last Dense and layer4 are kept trainable and rest are frozen.
During training we find triplets which are semi-hard in a batch (Loss between 0 and margin) and use only those to do backprop (they mention this in the FaceNet paper)

from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
import torch, torch.nn as nn, torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import os, glob
import numpy as np
from PIL import Image

image_size = 224
batch_size = 512
margin = 0.5
learning_rate = 1e-3
num_epochs = 1000

model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 128, bias=False)

for param in model.parameters():
  param.requires_grad = False
for param in model.fc.parameters():
  param.requires_grad = True
for param in model.layer4.parameters():
  param.requires_grad = True

optimizer = optim.Adam(params=list(model.fc.parameters())+list(model.layer4.parameters()), lr=learning_rate, weight_decay=0.05)

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = nn.DataParallel(model).to(device)
writer = SummaryWriter(log_dir=""logs/"")

class TripletDataset(Dataset):
  def __init__(self, rootdir, transform):
    super().__init__()
    self.rootdir = rootdir
    self.classes = os.listdir(self.rootdir)
    self.file_paths = {c: glob.glob(os.path.join(rootdir, c, ""*.jpg"")) for c in self.classes}
    self.positive_classes = [c for c in self.classes if len(self.file_paths[c])>=2]
    self.transform = transform

  def __getitem__(self, index=None):
    class_pos, class_neg = None, None
    while class_pos == class_neg:
      class_pos = np.random.choice(a=self.positive_classes, size=1)[0]
      class_neg = np.random.choice(a=self.classes, size=1)[0]

    fp_a, fp_p = np.random.choice(a=self.file_paths[class_pos], size=2, replace=False)
    fp_n = np.random.choice(a=self.file_paths[class_neg], size=1)[0]

    return {
        ""fp_a"": fp_a,
        ""fp_p"": fp_p,
        ""fp_n"": fp_n,
        ""A"": self.transform(Image.open(fp_a)),
        ""P"": self.transform(Image.open(fp_p)),
        ""N"": self.transform(Image.open(fp_n)),
            }

  def __len__(self):
    return 500000


def triplet_loss(a, p, n, margin=margin):
    d_ap = (a-p).norm(p='fro', dim=1)
    d_an = (a-n).norm(p='fro', dim=1)
    loss = torch.clamp(d_ap-d_an+margin, min=0)
    return loss, d_ap.mean(), d_an.mean()

transform = transforms.Compose([
        transforms.RandomResizedCrop(image_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.596, 0.436, 0.586], [0.2066, 0.240, 0.186])
        ])
train_dataset = TripletDataset(""lfw"", transform)
nw = 4 if torch.cuda.is_available() else 0
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)

num_batches = len(train_dataloader)
model.train()
running_loss = 0

for epoch in range(num_epochs):
    for batch_id, dictionary in enumerate(train_dataloader):
        a, p, n = dictionary[""A""], dictionary[""P""], dictionary[""N""]
        a, p, n = a.to(device), p.to(device), n.to(device)
        emb_a, emb_p, emb_n = model(a), model(p), model(n)
        losses, d_ap, d_an = triplet_loss(a=emb_a, p=emb_p, n=emb_n)

        semi_hard_triplets = torch.where((losses>0) & (losses<margin))
        losses = losses[semi_hard_triplets]
        loss = losses.mean()
        loss.backward()
        optimizer.step()  
        running_loss += loss.item()

        print(""Epoch {} Batch {}/{} Loss = {} Avg AP dist = {} Avg AN dist = {}"".format(epoch, batch_id, num_batches, loss.item(), d_ap.item(), d_an.item()), flush=True)
        writer.add_scalar(""Loss/Train"", loss.item(), epoch*num_batches+batch_id)
        writer.add_scalars(""AP_AN_Distances"", {""AP"": d_ap.item(), ""AN"": d_an.item()}, epoch*num_batches+batch_id)

    print(""Epoch {} Avg Loss {}"".format(epoch, running_loss/num_batches), flush=True)
    writer.add_scalar(""Epoch_Loss"", running_loss/num_batches, epoch)
    torch.save(model.state_dict(), ""facenet_epoch_{}.pth"".format(epoch))

Loss graphs: https://tensorboard.dev/experiment/8TgzPTjuRCOFkFV5lr5etQ/
Please let me know if you need some other information to help you help me.
","['convolutional-neural-networks', 'objective-functions', 'facial-recognition']",
Is it possible to do token classification using a model such as GPT-2?,"
I am trying to use PyTorch's transformers as a part of a research project to do sentiment analysis of several types of review data (laptop and restaurant). 
To do this, my team is taking a token-based approach and we are using models that can perform token analysis.
One problem we have encountered is that many of the models in PyTorch's transformers do not support token classification, but do support sequence classification. One such model we wanted to test is GPT-2.
In order to overcome this, we proposed using sequence classifiers on single tokens which should work in theory, but possibly at reduced accuracy.
This raises the following questions:

Is it possible to do token classification using a model such as GPT-2 using PyTorch's transformers?
How do sequence classifiers perform on single token sequences?

","['natural-language-processing', 'classification', 'pytorch', 'transformer']",
How to use convolution neural network in Deep-Q?,"
I currently have a grid of pixels 20x20. Each pixel can be red green blue or black. So I have one hot-encoded the pixels giving a 20x20x4 array for each screen. 
For my Deep-Q Network, I have attached two successive screenshots of the screen together giving a 20x20x4x2 array.
I am trying to build a Convolutional Neural Network to estimate the Q values but I am not sure if my current architecture is a good idea. It currently is as shown below:
    def create_model(self):
        model = Sequential()
        model.add(Conv3D(256, (4, 4,2), input_shape=(20,20,4,2)))
        model.add(Activation('relu'))
        model.add(Dropout(0.2))

        model.add(Conv3D(256, (2,2,1), input_shape=self.input_shape))
        model.add(Activation('relu'))

        model.add(Flatten())
        model.add(Dense(64))
        model.add(Dense(self.num_actions, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(self.learning_rate), metrics=['accuracy'])
        return model

Is a 3d convolution a good idea?
Is 256 filters a good idea?
Are the filters (4,4,2) and (2,2,1) suitable?
I realise answers may be highly subjective but I'm just looking for someone to point out any immediate flaws in the architecture.
","['deep-learning', 'q-learning', 'deep-neural-networks']","

Is a 3d convolution a good idea? Is 256 filters a good idea? Are the filters (4,4,2) and (2,2,1) suitable?

It's not so much that answers are subjective, but you are performing an experiment, and this should be driven by results. If you can find something published about a similar environment that might help you narrow down your choices.
That said, intuitively you don't gain much going from 2d to 3d convolutions when one of your dimensions is only sized 2. Purely from gut feeling I would suggest simply concatenating your two frames by having 8 channels instead of 4, and use 2d filters. This is simple to enough try and compare though, so perhaps you can do both.
You will likely want to explore deeper networks that have fewer initial filters and build up over a few more than two convolutional layers.
Definitely try your network without the dropout layer. I have not had much luck with using dropout in DQN, and I have seen others having similar problems. I am not sure what the exact issue is.
The accuracy metric won't do you much good on a regression task, so you can drop that and just use MSE loss. Bear in mind that training loss is a less useful metric overall in RL, since the prediction target (action value) is continuously changing as the policy changes. Low loss values mean that the value predictions are self-consistent, they don't necessarily mean that the learning has converged to an optimal policy.
"
Convolutional Feature Encoding Methods in DCNN,"
In Computer Vision, feature encoding methods are used on pre-trained DCNN to increase the feature robustness to certain conditions such as viewpoint/appearance variations ref.
I was just wondering if there are already available well established methods among AI community with probably python implementations.
I found the following ones in literature but without any tutorial or code example:

Multi-layer pooling ref
Cross convolutional layer Pooling ref
Holistic Pooling ref

","['deep-learning', 'convolutional-neural-networks', 'python', 'representation-learning']",
How does one continue the pre-training in BERT?,"
I need some help with continuing pre-training on Bert. I have a very specific vocabulary and lots of specific abbreviations at hand. I want to do an STS task. Let me specify my task: I have domain-specific sentences and want to pair them in terms of their semantic similarity. But as very uncommon language is used here, I need to train Bert on it.

How does one continue the pre-training (I read the GitHub release from google about it, but don't really understand it) Any examples?
What structure does my training data need to have, so that BERT can understand it?
Maybe training BERT from scratch would be even better. I guess it's the same process as continuing the pretraining just the starting checkpoint would be different. Is that correct?

Also, very happy about all other tips from you guys.
","['training', 'python', 'bert', 'fine-tuning', 'training-datasets']",
Is a basic neural network architecture better with small datasets?,"
I'm currently trying to predict 1 output value with 52 input values. The problem is that I only have around 100 rows of data that I can use. 
Will I get more accurate results when I use a small architecture than when I use multiple layers with a higher amount of neurons? 
Right now, I use 1 hidden layer with 1 neuron, because of the fact that I need to solve (in my opinion) a basic regression problem. 
","['neural-networks', 'ai-design', 'datasets', 'regression', 'architecture']","
I'm not aware of a direct way for finding the best NN architecture for a given task, but the recommended way, as far as I know, is to devise a network that can overfit the training data, and then apply regularization on top of it.
That way, you can be almost sure you're not underfitting/underperforming due to network capacity.
"
"What happens if I train a network for more epochs, without using early stopping?","
I have a question about training a neural network for more epochs even after the network has converged without using early stopping criterion.
Consider the MNIST dataset and a LeNet 300-100-10 dense fully-connected architecture, where I have 2 hidden layers having 300 and 100 neurons and an output layer having 10 neurons.
Now, usually, this network takes about 9-11 epochs to train and have a validation accuracy of around 98%.
What happens if I train this network for 25 or 30 epochs, without using early stopping criterion? 
","['neural-networks', 'machine-learning', 'training', 'feedforward-neural-networks', 'early-stopping']","
Training a neural network for ""too many"" epochs than needed without using early stopping criterion leads to overfitting, where your model's ability to generalize decreases.
"
How does positional encoding work in the transformer model?,"
In the transformer model, to incorporate positional information of texts, the researchers have added a positional encoding to the model. How does positional encoding work? How does the positional encoding system learn the positions when varying lengths and types of text are passed at different time intervals?
To be more concrete, let's take these two sentences.

""She is my queen""
""Elizabeth is the queen of England""

How would these sentences be passed to the transformer? What would happen to them during the positional encoding part?
Please explain with less math and with more intuition behind it.
","['deep-learning', 'natural-language-processing', 'transformer', 'attention', 'positional-encoding']",
What are examples of approaches to create an AI for a fighting robot in an MMO game?,"
I have an MMO game where I have players. I wanted to invent something new to the game, and add player-bots to make the game be single-playable as well. The AI I want to add is simply only for fighting other players or other player-bots that he sees around at his level.
So, I thought of implementing my fighting strategy, exactly how I play, to the bot which is basically using if statements and randoms. For example, when the opponent has low health, and the bot has enough special attack power, he will use this chance and use his special attack power in order to try to knock the opponent down, or if the bot has low health he will eat in time but not too much because there is a point in risking fights, if you eat too much your opponent will do too. Or, for example, if the bot detects the opponent player is eating too much and gains health, he will do the same.
I told this idea of the implementation to one of my friends and he simply responded with: This is not AI, it's simply just a set of conditions, it does not have any heuristic functions.
For that type of game, what are some ideas to create a real AI to achieve these conditions?
Basically, the AI should know what to do in order to beat the opponent, based on the opponent's data such as current health, Armour and weapons, and level, if he risks his health or not and so on.
I am a beginner and it really interests me to do it in the right way.
","['reference-request', 'game-ai', 'algorithm-request']","
I would set up a list of goals for your bot. These could be 'maintain a minimum level of health', 'knock out human player', 'block way to location X', etc. This obviously depends on the domain of your MMO.
Then you can use a planner to achieve these goals in the game. You define a set of actions with preconditions and effects, set the current goal, and the planner will work out a list of actions for the bot to achieve the goal. You can easily express your actions (and the domain) in PDDL.
Examples for actions would be 'move to location X', 'eat X', 'attack player X'. A precondition of 'attack player X' could be 'health(X) is low', and an effect could be 'health(X) is reduced by Y'. There are different ways of expressing these depending on the planner's capabilities.
The beauty of this is that you don't actually have to explicitly code any behaviour. You describe the domain, and tell the bot what it should achieve, and what capabilities it has. The actual behaviour then emerges out of that description. If the bot only attacks a player if the player has lower health, then observing the player eat (and thus up their health) could result in the bot eating (to push its own health above the player's so that it can attack) — but you have not told the bot directly to do that.
For a starting point, go to http://education.planning.domains/ for a list of resources.
If you only have a few actions available, it might appear predictable to a human user, but with a variety of goals and actions, this will quickly become more complex and seem more 'intelligent'. 
Update: Here is a link to a paper, Applying Goal-Oriented Action Planning to Games, which describes how this can be applied in a game.
"
How can I implement the reward function for an 8-DOF robot arm with TRPO?,"
I need to get an 8-DOF (degrees of freedom) robot arm to move a specified point. I need to implement the TRPO RL code using OpenAI gym. I already have the gazebo environment. But I am unsure of how to write the code for the reward functions and the algorithm for the joint space motion. 
","['reinforcement-learning', 'gym', 'reward-design', 'reward-functions', 'trust-region-policy-optimization']","
The most important part of the RL is the reward function. If we want an agent to do something speciﬁc, we must provide rewards to it in such a way that it will achieve the goal. It is thus very important that the reward function accurately indicates the exact behavior.
Assume, the robot's goal is to reach the desired position as fast as possible. You can construct your reward function so, that it will take into account the Euclidean distance to the position. If the arm moves to the position directly, you will reward the agent with a positive value, otherwise, you will punish it with a deviation from the direct line. You probably have other parameters of the joints, such as position and velocity. It can be also included in your reward function, in order to find optimal movements.

Check out this video from the free udacity overview course on RL and this paper ""Setting up a Reinforcement Learning Task with a Real-World Robot""
Here is also related DeepMind's article and paper
I also have a project on github, where I implemented custom Gazebo environment for OpenAI Gym. This allows you to run the test even on a Jupyter Notebook. Check out my example
"
Is there any difference between the convolution operation applied to images and applied to other numerical 2D data?,"
Is there any difference between the convolution operation applied to images and applied to other numerical 2D data?
For example, we have a pretty good CNN model trained on a number of $64 \times 64$ images to detect two classes. On the other hand, we have a number of $64 \times 64$ numerical 2D matrices (which are not considered images), which also have two classes. Can we use the same CNN model to classify the numerical dataset?
","['convolutional-neural-networks', 'classification', 'image-recognition', 'convolution']","
Short answer is no. You can't use a model trained for one task to predict on a totally different task. Even if the second task was another image classification task, the CNN would have to be fine tuned for the new data to work.
A couple of things to note...
1) CNNs are good for images due to their nature. It isn't necessary that they'd be good for any 2-dimensional input.  
2) By 2D numerical data I'm assuming you don't mean tabular data.
"
Can you provide some pseudocode examples of what constitutes an AI?,"
After years of learning, I still can't understand what is considered to be an AI. What are the requirements for an algorithm to constitute Artificial Intelligence? Can you provide pseudocode examples of what constitutes an AI?
","['philosophy', 'definitions', 'intelligence', 'pseudocode']","
Philosophically, my own research has led me to understand AI as any artifact that makes a decision.  This is because the etymology of ""intelligence"" strongly implies ""selecting between alternatives"", and these meanings are baked in all the way back to the proto-Indo-European.
(Degree of intelligence, or ""strength"" is merely a measure of utility, typically versus other decision making mechanisms, or, ""fitness in an environment"", where an environment is any action space.)
Therefore, the most basic form of automated (artificial) intelligence is:
if [some condition] 
then [some action]

It is worth noting that narrow AI which matches or exceeds human capability, in the popular sense, manifested only recently when we had sufficient processing and memory to derive sufficient utility from statistical decision making algorithms. But Nimatron constitutes perhaps the first functional strong-narrow AI in a modern computing context, and the first automated intelligence are simple traps and snares, which have been with us almost as long as we've used tools.
I will leave it to others to break down all the various forms of modern AI.
"
"In Fast R-CNN, how are input RoIs mapped to the respective RoIs in the feature map before RoI pooling?","
I've been reading the Fast R-CNN paper.
My understanding is that the input to one forward pass is the whole input image plus a list of RoIs (generated by selective search or another region proposal method). Then I understand that on the last convolution layer's feature map (let's call it FM), each corresponding RoI gets RoI-pooled, where now the corresponding ROIs are a rectangular (over height and width) slice of the FM tensor over all channels.
But I'm having trouble with two concepts:

How is the input RoI mapped to the corresponding RoI in FM? Each neuron comes from a very wide perceptive field, so, in a deep neural network, there's no way of making a 1:1 mapping between input neurons and neurons in the last convolution layer right?

Disregarding that I'm confused in point 1, once we have a bunch of RoIs in FM and we do the RoI pooling, we have N pooled feature vectors. Do we now run each of these through one FC network one by one? Or do we have N branches of FC networks? (that wouldn't make sense to me)


I have also read the faster R-CNN paper. In the same way, I'm also interested to know about how the proposed regions from RPN map to the input of the RoI pooling in the Fast R-CNN layers. Because actually those proposed regions live in the space of the input image, not in the space of the deep feature map.
","['neural-networks', 'convolutional-neural-networks', 'computer-vision', 'r-cnn', 'pooling']",
Is it recommended to remove stop words before named entity recognition?,"
Removing stop words can significantly speed up named entity recognition (NER) modeling by reducing the number of tokens in a document.
Are stop words critical to get correct NER performance?
","['natural-language-processing', 'data-preprocessing']","
It depends how you recognise the entities.
If you do a simple gazetteer lookup, then it could be faster, as you have fewer tokens to deal with.
However, if you use contextual rules, then stop words might be vital to identify certain contexts, so by removing stop words you lose information about the entity's environment. For example, if [work] at {organisation} is a rule you use to identify companies etc, then this wouldn't work if you take out the at.
You will also have problems if the stop words are part of an entity, eg the town Stoke-on-Trent. If hyphens are used to split tokens, then you won't be able to recognise it if the on is discarded.
In general I think stop words should mostly be kept; apart from information retrieval where they are not all that useful in an inverted index you will always lose something. If stop words were really pointless, then they would have been discarded from languages a long time ago. In the old days they were a useful way of reducing the demand on computational resources without losing too much, but nowadays I would say this is not really a problem anymore.
"
Oscillating around the saddle point in gradient descent?,"
I was reading a blog post that talked about the problem of the saddle point in training. 

In the post, it says if the loss function is flatter in the direction of x (local minima here) compared to y at the saddle point, gradient descent will oscillate to and from the y direction. This gives an illusion of converging to a minima. Why is this? 
Wouldn’t it continue down in the y direction and hence escape the saddle point?


Link to post:
https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/
Please go to Challenges with Gradient Descent #2: Saddle Points.
","['optimization', 'gradient-descent']",
Unexpected results when comparing a greedy policy to a DQN policy,"
I am trying to work on a variation of the Access-Control Queuing Task problem presented in Chapter 10 of Sutton’s reinforcement learning book [1]. 
Specific details of my setup are as follows:

I have different types of tasks that arrive to a system
(heavy/moderate/light with heavy tasks requiring more time to be
processed.). The specific task type is chosen uniformly at random. The task inter-arrival time is $0.1s$ on average.
I have different classes of servers that can process these tasks
(low-capacity; medium capacity; high capacity; with high capacity
servers having a faster processing time). When I select a specific
server from a given class, it becomes unavailable during the
processing time of the task assigned to it. Note that the set of servers (and as a result the number of servers of each class) is not fixed, it instead changes periodically, according to the dataset used to model the set of servers (so specific servers may disappear and new ones may appear, as opposed to the unavailability caused by the assignment). The maximum number of servers of each class is $10$. 
My goal is to decide which class of server should process a given
task, in a way that minimizes the sum of the processing times over
all tasks.

The specific reinforcement learning formulation is as follows:

State: the type of task (heavy/moderate/light) ; the number of available low
capacity servers; the number of available medium capacity servers; the number
of available high capacity servers
Actions: (1) Assign the task to a low capacity server (2) assign the
task to a medium capacity server (3) assign the task to a high
capacity server (4) a dummy action that has a worse processing time
than the servers with low capacity. It is selected when there are no free servers.
Rewards: the opposite of the processing time, where the processing times are as follows (in seconds):



|               | Slow server | Medium Server | Fast server | ""Dummy action"" |
|---------------|-------------|---------------|-------------|----------------|
| Light task    | 0.5         | 0.25          | 0.166       | 0.625          |
| Moderate task | 1.5         | 0.75          | 0.5         | 1.875          |
| Heavy task    | 2.5         | 1.25          | 0.833       | 3.125          |


My intuition for formulating the problem as an RL problem is that 'Even though assigning Light tasks to High capacity servers (i.e. being greedy) might lead to a high reward in the short term, it may reduce the number of High capacity servers available when a Heavy task arrives. As a result, Heavy tasks will have to be processed by lower capacity servers which will reduce the accumulated rewards'.
However, when I implemented this (using a deep Q-network[2] specifically), and compared it to the greedy policy, I found that both approaches obtain the same rewards. In fact, the deep Q-network ends up learning the greedy policy. 
I am wondering why such a behaviour occured, especially that I expected the DQN approach to learn a better policy than the greedy one. Could this be related to my RL problem formulation? Or there is no need for RL to address this problem?
[1]Sutton, R. S., & Barto, A. G. (1998). Introduction to reinforcement learning (Vol. 135). Cambridge: MIT press.
[2]Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.f
","['reinforcement-learning', 'deep-rl']",
How can I use autoencoders to analyze patterns and classify them?,"
I generated a bunch of simulation data from a complex physical simulation that spits out patterns. I am trying to apply unsupervised learning to analyze the patterns and ideally classify them into whatever categories the learning technique identifies. Using PCA or manifold techniques such as t-SNE for this problem is rather straightforward, but applying neural networks (autoencoders, specifically) becomes non-trivial, as I am not sure splitting my dataset into test and training data is the right way. 
Naively, I was thinking of the following approaches: 

Train an autoencoder with all the data as training data and train it for a large number of epochs (overfitting is not a problem in this case perse I would think)
Keras offers a model.predict option which enables me to just construct the encoder section of the autoencoder and obtain the bottleneck values
Carry out some data augmentation and split the data as one might into training and test data and carry out the workflow as normal (This approach makes me a little uncomfortable as I am not attempting to generalize a neural network or should I be?)

I would appreciate any guidance on how to proceed or if my understanding of the application of autoencoders is flawed in this context.
","['image-recognition', 'keras', 'unsupervised-learning', 'autoencoders', 'dimensionality-reduction']","
When using an autoencoder, I believe the data u feed in has to be correlated in one way or another. For example, If i want to learn a latent representation of an image of a cat, The training data that I feed into the autoencoder should constitute only cat images. 
Similar to other neural networks, you feed the autoencoder with a set of training data and hope that the network learns a set of weights that is able to output from the latent representation the exact image. To see whether the weights learnt by the autoencoder is able to generalise to other unseen cat images, you would have to use a test set for this. 
Here is a paper about autoencoding. https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf
I hope this helps you somewhat in deciding whether you should use an autoencoder.
"
Why is my validation/test accuracy higher than my training accuracy [duplicate],"







This question already has an answer here:
                                
                            




Validation accuracy higher than training accurarcy

                                (1 answer)
                            

Closed 9 months ago.




Is this due to my dropout layers being disabled during evaluation?
I'm classifying the CIFAR-10 dataset with a CNN using the Keras library.
There are 50000 samples in the training set; I'm using a 20% validation split for my training data (10000:40000). I have 10000 instances in the test set.
","['neural-networks', 'tensorflow', 'keras', 'dropout']","
It is a bit rare that the validation and test  accuracy exceed the training accuracy. One thing that could cause this is the selection of the validation and test data. Was the data for these two sets selected randomly or did you do the selection yourself? It is generally better to have these sets selected randomly from  the overall data  set. That way the probability distribution in these sets will closely match the distribution of the training set. Normally the training accuracy is higher (especially if you run enough epochs which I see you did) because there is always some degree of over fitting which reduces validation and test accuracy. The only other thing I can think of is the effect of Dropout layers. If you had dropout layers in your model and the drop out ratio was high that could cause this accuracy disparity. When the training accuracy is calculated it is done with drop being active. This can lower the training accuracy to some degree. However when evaluating validation accuracy and test accuracy drop out is NOT active so the model is actually more accurate. This increase in accuracy might be enough to overcome the decrease due to over fitting. Especially possible in this case since the accuracy differences appear to be quite small.
"
Is there a possibility that there is no relationship between some inputs and outputs?,"
I'm doing machine learning projects. I took a look at many datasets I worked with, mostly there are already famous datasets that everyone uses.
Let's say I decided to make my own dataset. Is there a possibility that my data are so random so that no relationship exists between my inputs and outputs? This is interesting because if this is possible, then no machine learning model will achieve to find an inputs outputs relationship in the data and will fail to solve the regression or classification problem.
Moreover, is it mathematically possible that some values have absolutely no relationship between them? In other words, there is no function (linear or nonlinear) that can map those inputs to the outputs.
Now, I thought about this problem and concluded that, if there is a possibility for this, then it will likely happen in regression because maybe the target outputs are in the same range and the same features values can correspond to the same output values and that will confuse the machine learning model.
Have you ever encountered this or a similar issue?
","['machine-learning', 'datasets', 'math', 'regression', 'function-approximation']","
Not sure if I can answer the question as a whole, but a pure random input/output pair doesn't quite have ""no relationship"" at all. At the very least, for any fixed training set input/output pair, you can do an if...then mapping to construct a 1-to-1 function, such that you can classify the training set with 100% accuracy (assuming no duplicates of input).
In any case, I assume you mean uniform random, because if you have something like gaussian random, you can still learn some latent structure from how the random numbers are generated.
But even if you assume uniform random, and your algorithm is only guessing, your algorithm is technically still operating optimally per the data generating distribution, which basically means its as optimal as it gets.
The only such case that I can imagine which would satisfy your question, would be if you had a separate training/validation set, where the only element of the training input/output is [1,1], but the validation set only has elements of [1,-1], or something along those lines.
From reading your comments, I suspect that your intention with the question was: ""Can there be a relationship of data such that no method can learn it?"". To the extent that the data-generating distribution exists, then by the universal approximation theorem of neural networks, then it is reasonable that you can at least partially learn it.
However it is important to note that the universal approximation theorem doesn't mean that such a data generating distribution can be learned by a neural net, it only means that you can get ""non-zero as close as you want"" to the data generating distribution. More explicitly: there is a setting of weights that gives you results as good as you want, but gradient descent doesn't necessarily learn it.
"
How to understand and visualize a trained RL agent's policy when the state space is high dimensional?,"
What are typical ways to understand and visualize a trained RL agent's policy when the state space is of high dimension (but not images)? 
For example, suppose  state and action are denoted by $s=(s_1,s_2,\cdots,s_n)$ and $a=(a_1,a_2,\cdots,a_k)$. How do I determine which attribute of the state (e.g. an image pixel of video game) is most responsible for a particular action $a_j$? I would like to have, for each action $a_j, j=1,2,...,k$, a table that ranks the attributes of the observation.
My question may be a little bit vague, but if you have any thoughts on how to improve it please let me know! 
","['reinforcement-learning', 'markov-decision-process', 'policies']",
Does the performance of a model increase if dropout is disabled at evaluation time?,"
I know dropout layers are used in neural networks during training to provide a form of regularisation in an attempt to mitigate over-fitting.
Would you not get an increased fitness if you disabled the dropout layers during evaluation of a network?
","['neural-networks', 'machine-learning', 'dropout']","
Dropout is a technique that helps to avoid overfitting during training. That is, dropout is usually used for training.

units may change in a way that they fix up the mistakes of the other
units. This may lead to complex co-adaptations. This, in turn, leads to
overfitting because these co-adaptations do not generalize to unseen
data.

If you want to evaluate your model, you should turn off all dropout layers. For example, PyTorch's model.eval() does this work.
Note that in some cases dropout can be used for inference, e.g. to add some stochasticity to the output.
More about dropout:

Improving neural networks by preventing co-adaptation of feature detectors
Dropout: A Simple Way to Prevent Neural Networks from Overfitting

"
Why do we regularize the variational autoencoder with a normal distribution?,"
When we define the loss function of a variational autoencoder (VAE), we add the Kullback-Leibler divergence between the sample taken according to a normal distribution of parameters: 
$$ N(\mu,\sigma) $$
and we compare it with a normal distribution of parameters 
$$ N(0,1) $$
My intuition is that it is clever having samples taken from a distribution centered around zero, but I don't understand why we want that examples are taken with a normal distribution. 
","['machine-learning', 'deep-learning', 'probability-distribution', 'variational-autoencoder']",
Train an AI to infer accurate mathematical calculations by simply “looking” at images of shapes/objects,"
I’d like to build a model that has an understanding of geometry, where it can be applied to question and answering system. Specifically, it would be nice if it could determine the volume of an object by simply looking at pictures of it.
If there are any pre-trained models out there that I can utilize that would certainly make things easier. 
Otherwise, are there any suggestions on the kind of model(s) I should use to do this?
Also, I read something online about how Facebook trained an AI to solve complex math problems just by looking at them. They approached the problem as a language translation problem, not as a math problem. I wonder if this is the way to go?
","['deep-learning', 'ai-design', 'computer-vision', 'image-processing']",
Data scan not making sense for coco dataset,"
I am doing a simple scan to see how dataset size affects training. Basically, I took 10% of the coco dataset and trained a yolov3 net (from scratch) to just look for people. Then I took 20% of the coco dataset and did the same thing.... all the way to 100%. What is strange is that all 9 nets are getting similar loss at the end (~7.5). I must be doing something wrong, right? I expected to see an exponential curve where loss started out high and assymptotically approached some value as the dataset increased to 100%. If it didn't approach a value (and still had a noticeable slope at 100%), then that meant more data could help my algorithm.
This is my .data file:
classes= 1
train = train-run-less.txt
valid = data/coco/5k.txt
names = data/humans.names
backup = backup
I am trying to train just one class (person) from the coco dataset. Something is not making sense, and in a sanity test, I discovered that the loss drops even if the training folder only contains 1 image (which doesnt even have people in it). I thought the way this worked was that it trained on the ""train"" images, then it tested the neural net on the ""valid"" images. How is it getting better at finding people in the ""valid"" images if it hasnt trained on a single one??
Basically I am trying to answer the question: ""how much accuracy can I expect to gain as I increase the data?""
","['training', 'yolo', 'loss']","

What is strange is that all 9 nets are getting similar loss at the end (~7.5). I must be doing something wrong, right?

Yes, Please check the annotations file. Did you remove all the other classes except Person? What's the mAP of all those models?

""how much accuracy can I expect to gain as I increase the data?""

It's hard to tell but more the data(variation), better the model.
"
"Suitable algorithms for classifying terrain condition (asphalt, dirt etc) for motor vehicles","
I am required to obtain data through a sensor located on the vehicle reading speed, vibration, roll and tilt, within a sample time, to classify the current road condition using machine learning for a high school project. 
Which algorithm/approach may be most suitable for this task? Suggestions to sources for learning (books, tutorials) would be also appreciated, as I am new to AI and ML.
","['neural-networks', 'machine-learning', 'computer-vision', 'pattern-recognition']",
What are the pros and cons of the common activation functions?,"
I have heard that sigmoid activation functions should not be used on neural networks with many hidden layers as the gradients tend to vanish in deep networks.
When should each of the common activation functions be used, and why?

ReLu
Sigmoid
Softmax
Leaky ReLu
TanH

","['neural-networks', 'deep-learning', 'comparison', 'activation-functions']",
Why is the sample size of stochastic gradient descent a power of 2?,"
I watched the video lecture of cs224: Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 2 – Word Vectors and Word Senses.
They take the sample size of the window to be $2^5 = 32$ or $2^6 = 64$. Why is the sample size of stochastic gradient descent a power of 2? Why not we can take 42 or 53 as the sample window size?
Btw, how do I identify the best minimum window sample size?
","['deep-learning', 'natural-language-processing', 'stochastic-gradient-descent']",
Intuition behind single-shot object detection,"
Is there a good way to understand how single-shot object detection works? The most basic way to do detection is use a sliding-window detector and look at the output of the NN to detect if a class is there or not.  
I'm wondering if there is a way to understand how many of the single-shot detectors work? Internally is there some form of sliding window going on? Or is it basically the same detector learned at each point?
","['deep-learning', 'computer-vision', 'yolo']",
Do Seq2Seq models and the Bidirectional RNN do the same thing?,"
It seems to me that Seq2Seq models and Bidirectional RNNs try to do the same thing. Is that true?
Also, when would you recommend one setup over another?
","['comparison', 'recurrent-neural-networks', 'seq2seq', 'bidirectional-rnn']","
Seq2Seq and Bidirectional RNNs are not doing the same thing, at least in their classic form.
Seq2Seq models are used to generate a sequence from another sequence. Consider, for example, the translation task from one language to another. In that sense, Seq2Seq is more a family of models, not an architecture.
On the other hand, the Bidirectional RNN is a neural network architecture and can be used to build several models including Seq2Seq, for example, the encoding part of the Seq2Seq can be a Bi-RNN, but they also can be used for other tasks, for example, sentence classification or sentiment analysis.
"
Is the policy really invariant under affine transformations of the reward function?,"
In the context of a Markov decision process, this paper says

it is well-known that the optimal policy is invariant to positive affine transformation of the reward function

On the other hand, exercise 3.7 of Sutton and Barto gives an example of a robot in a maze:

Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

It seems like the robot is not being rewarded for escaping quickly (escaping in 10 seconds gives it just as much reward as escaping in 1000 seconds). One fix seems to be to subtract 1 from each reward, so that each timestep the robot stays in the maze, it accumulates $-1$ in reward, and upon escape it gets zero reward. This seems to change the set of optimal policies (now there are way fewer policies which achieve the best possible return). In other words, a positive affine transformation $r \mapsto 1 \cdot r - 1$ seems to have changed the optimal policy.
How can I reconcile ""the optimal policy is invariant to positive affine transformation of the reward function"" with the maze example?
","['reinforcement-learning', 'markov-decision-process', 'policies', 'reward-functions', 'reward-shaping']","
This statement:

(it is well-known that the optimal policy is invariant to positive affine transformation of the reward function).

is, as far as I know, and as you summarise, incorrect* because simple translations to reward signal do affect the optimal policy, and the affine transform of a real number $x$ can be given by $f(x) = mx + c$
It is well known that optimal policy is unaffected by multiplying all rewards by a positive scaling, e.g. $f(x) = mx$ where $m$ is positive.
It is also worth noting that if an optimal policy is derived from Q values using $\pi(s) = \text{argmax}_a Q(s,a)$, then that policy function is invariant to positive affine transformations of action values given by $Q(s,a)$. Perhaps that was what the paper authors meant to write, given that they go on to apply normalisation to Q values.
The impact of the mistake is not relevant to the the rest of the paper as far as I can see (caveat: I have not read the whole paper).

* It is possible to make the statement correct even for episodic problems, if:

You model episodic problems with an ""absorbing state"" and treat it as a continuing problem.

You apply the same affine transform to the (usually zero reward) absorbing state.

You still account for the infinite repeats of the absorbing state (requiring a value of discount factor $\gamma$ less than one). In practice this means either granting an additional reward of $\frac{b}{1-\gamma}$ for ending the episode, or not ending a simulation in the terminal state, but running learning algorithms over repeated time steps whilst still in the terminal state, so they can collect the non-zero reward.


"
Why I have a different number of terms in word2vec and TFIDF? How I can fix it?,"
I need multiply the weigths of terms in TFIDF matrix by the word-embeddings of word2vec matrix but I can't do it because each matrix have a different number of terms. I am using the same corpus for get both matrix, I don't know why each matrix have a different number of terms
.
My problem is that I have a matrix TFIDF with the shape (56096, 15500) (corresponding to: number of terms, number of documents) and matrix Word2vec with the shape (300, 56184) (corresponding to : number of word-embeddings, number of terms).
And I need the same numbers of terms in both matrix. 
I use this code for get the matrix of word-embeddings Word2vec: 
def w2vec_gensim(norm_corpus):
    wpt = nltk.WordPunctTokenizer()
    tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]
    # Set values for various parameters
    feature_size = 300
    # Word vector dimensionality
    window_context = 10
    # Context window size
    min_word_count = 1
    # Minimum word count
    sample = 1e-3
    # Downsample setting for frequent words
    w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, window=window_context, min_count =  min_word_count, sample=sample, iter=100)
    words = list(w2v_model.wv.vocab)
    vectors=[]
    for w in words:
        vectors.append(w2v_model[w].tolist())
    embedding_matrix= np.array(vectors)
    embedding_matrix= embedding_matrix.T
    print(embedding_matrix.shape)

    return embedding_matrix

And this code for get the TFIDF matrix: 
tv = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True, smooth_idf=True)


def matriz_tf_idf(datos, tv):
    tv_matrix = tv.fit_transform(datos)
    tv_matrix = tv_matrix.toarray()
    tv_matrix = tv_matrix.T
    return tv_matrix

And I need the same number of terms in each matrix. For example, if I have 56096 terms in TFIDF, I need the same number in embeddings matrix, I mean matrix TFIDF with the shape (56096, 1550) and matrix of embeddings Word2vec with the shape (300, 56096). How I can get the same number of terms in both matrix? 
Because I can't delete without more data, due to I need the multiplication to make sense because my goal is to get the embeddings from the documents. 
Thank you very much in advance.
","['natural-language-processing', 'python', 'word-embedding', 'word2vec', 'weights']",
How to calculate the confidence of a classifier's output?,"
I'm training a classifier and I want to collect incorrect outputs for human to double check.
the output of the classifier is a vector of probabilities for corresponding classes. for example, [0.9,0.05,0.05]
This means the probability for the current object being class A is 0.9, whereas for it being the class B is only 0.05 and 0.05 for C too.
In this situation, I think the result has a high confidence. As A's probability dominants B's and C's.
In another case, [0.4,0.45,0.15], the confidence should be low, as A and B are close. 
What's the best formula to use to calculate this confidence?
",['classification'],"
I believe that there is no ""best formula"" here, as there are many Calibration metrics out there, depending on what you want to calibrate. This paper introduces three metrics for different purposes:

Expected Calibration Error (ECE): provides a single scalar summary of calibrations.
Maximum Calibration Error (MCE): use when we wish to minimize the worst-case deviation between confidence and accuracy
Negative log likelihood (NLL): this is the same as Cross-entropy loss.

There is also a related paper about more metrics.
Just like Accuracy, F1, and ROC-AUC, Calibration metric should depend on the use case.
"
How should I go about selecting an optimal num_units within a LSTM cell for different sequence sizes,"
I am currently working on a stock market prediction model which incorporates sentiments along with historical price for next day price prediction.
I wanted to test different window / sequence size e.g (3 days, 4 days .. 10 days) to identify which window size is most optimal in predicting the next day prices. 
However the selection for num_units in model.add(LSTM(units=num_units)) for different window sizes are varying. 
If a smaller window size is paired with a larger num_unit, there is over-fitting in the data where the model prediction for the price at day t+1 is almost equal to the price at day t.
Hence I am unable to make a fair comparison between different window sizes without varying num_units
I have referred to this How to select number of hidden layers and number of memory cells in an LSTM? however am unable to come to a conclusion.
Is there a predefined guideline for the num_units to use within a LSTM cell for timeseries prediction based on the sequence length?
","['long-short-term-memory', 'prediction']",
How can I normalize gamestates in order to use with a machine learning library?,"
I have currently collected 150000 gamestates from playing a Monte Carlo Tree Search AI player against a basic rule based AI at the game of Castle. The information captured represents the information available to the MCTS player on the start of each of their turn and whether they won the game in the end. They are stored within CVS files.
Example gamestate entry:

For example the entry above shows that:

HAND: MCTS player's hand contains the cards 7,5,4,9,9,9 (suit has been omitted because it has no baring on the game). (list of cards)
CASTLE_FU: MCTS face up cards are 8,2,10 (list of cards)
CASTLE_FD_SIZE: MCTS has 3 cards face down (int)
OP_HAND_SIZE: The opponent has 3 cards in their hand (int)
OP_CASTLE_FU: The opponents face up cards are Jack, Queen, Ace. (list of cards)
OP_CASTLE_FD_SIZE: The opponent has 3 cards face down (int)
TOP: The top of the discard pile is a 4 (single value)
DECK_EMPTY: The deck in which players pick up cards is not empty (boolean)
WON: The MCTS player ended up winning the hand (boolean)

I hope to input this data into machine learning algorithms to produce an evaluation function for the MCTS algorithm to use.
How can I normalize this data so I can use it in Keras/Scikit-Learn?
EDIT:
I'm not sure normalizing is the right term here. Encoding or mapping may be more accurate for what I am trying to achieve. Another difficulty I've encountered is the fact that the players hand size can vary up to almost holding the full deck in theory (although this would be incredibly rare in practice). However this is the only column that can be of a size greater than 3.
EDIT 2:
I've come up with this model to represent the data. Does this look suitable?

","['neural-networks', 'machine-learning', 'deep-learning']",
"What does ""the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter"" mean?","
I am currently studying Deep Learning by Goodfellow, Bengio, and Courville. In chapter 5.2 Capacity, Overfitting and Underfitting, the authors say the following:

Typically, when training a machine learning model, we have access to a training set; we can compute some error measure on the training set, called the training error; and we reduce this training error. So far, what we have described is simply an optimization problem. What separates machine learning from optimization is that we want the generalization error, also called the test error, to be low as well. The generalization error is defined as the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice. 

I found this part unclear:

Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice. 

The language used here is confusing me, because it is discussing a ""distribution"", as in a ""probability distribution"", but then refers to inputs, which are data gathered from outside of any probability distribution. Based on the limited information my studying of machine learning has taught me so far, my understanding is that the machine learning algorithm (or, rather, some machine learning algorithms) uses training data to implicitly construct some probability distribution, right? So is this what it is referring to here? Is the ""distribution of inputs we expect the system to encounter in practice"" the so called ""test set""? I would greatly appreciate it if people would please take the time to clarify this.
","['machine-learning', 'probability-distribution', 'generalization']","
For illustration, I use the dog/cat classification task. Suppose, the training data of cat and dog follows the Gaussian distribution(for simplicity) and we trained a model which gives an accuracy as below.

train - 98.2% 
val - 97.7% 
test - 97.2%

The model is neither overfitting nor underfitting but we want the classifier to achieve an accuracy of 100%  in all the three sets theoretically. You are right that the model learns the distribution of training data to classify the classes. Due to the overlapping of fat-tail in the distribution of cat and dog, it is highly impossible for the model to get 100% accuracy practically. There will be infinite edge cases that we encounter in reality so we can only improve the model by an iterative approach.
"
Expected duration in a state,"
I am going through Rabiner 1989 and he writes that the discrete probability density function of duration $d$ in state $i$ (that is, staying in a state for duration $d$, conditioned on starting in that state) is $$p_i(d) = {a_{ii}}^{d-1}(1-a_{ii})$$
($a_{ii}$ is the state transition probability from state $i$ to state $i$ - that is, staying in the same state).
He then continues to say that the expected durations in a state, conditioned on starting in that state, is $$\overline d_i = \sum_{i=1}^\infty d p_i(d) = \frac{1}{1-a_{ii}}$$
Where does the coefficient $d$ (in $\sum_{i=1}^\infty d p_i(d)$) come from?
","['math', 'probability', 'hidden-markov-model']",
"If $h_i$ are consistent and admissible, are their sum, maximum, minimum and average also consistent and admissible?","
Consider the following question:

$n$ vehicles occupy squares $(1, 1)$ through $(n, 1)$ (i.e., the bottom row) of an $n \times n$ grid. The vehicles must be moved to the top row but in reverse order; so the vehicle $i$ that starts in $(i, 1)$ must end up in $(n − i + 1, n)$. On each time step, every one of the $n$ vehicles can move one square up, down, left, or right, or stay put; but if a vehicle stays put, one other adjacent vehicle (but not more than one) can hop over it. Two vehicles cannot occupy the same square.

Suppose that each heuristic function $h_i$ is both admissible and consistent. Now what I want to know is to check the admissibility and consistency of the following heuristics:

$h= \Sigma_i h_i$

$h= \min_i (h_i)$

$h= \max_i (h_i)$

$h = \frac{\Sigma_i h_i}{n}$


P.S: As a lemma, we know that consistency implies the admissibility of the heuristic function.
Problem Explanation
From this link, I have found that the first heuristic is neither admissible, nor consistent.
I know that the second and the fourth heuristics are either consistent, or admissible.
I have faced with one contradiction in the third heuristic:

Here we see that if car 3 hops twice, the total cost of moving all the cars to their destinations is 3, whereas the heuristic $\max(h_1, \dots, h_n) = 4$.
Problem
So, $\max(h_1, ..., h_n)$ must be consistent and admissible, but the above example shows that it's not. What is my mistake?
","['admissible-heuristic', 'consistent-heuristic', 'heuristic-functions']","
The issue is that you must include assumptions about hopping into your heuristic. In particular, if you are considering individual cars then you must assume that they might be able to hop all of the way to the goal. Thus, your heuristic for each car should be Manhattan distance divided by 2. That's guaranteed to be admissible when you take the max.
If you consider all possible cars you can do better, but you'll need to reason out all the cases. (In general every car either waits or moves, and for every waiting car one car can hop. So, by looking at the minimum distance for any car to reach the goal you can start to reduce your heuristic.) But, that is a different question.
"
What is the purpose of the batch size in neural networks?,"
Why is a batch size needed to update the weights of a neural network?
According to that Youtube Video from 3B1B, the weights are updated by calculating the error between expectation and outcome of the neural net. Based on that, the chain rule is applied to calculate the new weights. 
Following that logic, why would I pass a complete batch through the net? The first entries wouldn't have an impact on the weighting.
Do I need to define a batch size when I use backpropagation?
","['neural-networks', 'deep-learning', 'training', 'backpropagation', 'batch-size']","
tl;dr: A batch size is the number of samples a network sees before updating its gradients. This number can range from a single sample to the whole training set. Empirically, there is a sweet spot in the range 1 to a few hundreds, where people experience the fastest training speeds. Check this article for more details.

A more detailed explanation...
If you have a small enough number of samples, you can let the network see all of the samples before updating its weights; this is called Gradient Descent. The benefit from this is that you guarantee that the weights will be updated in the direction that reduces the training loss for the whole dataset. The downside is that it is computationally expensive and in most cases infeasible for deep neural nets.
What is done in practice is that the network sees only a batch of the training data, instead of the whole dataset, before updating its weights. However, this technique does not guarantee that the network updates its weights in a way that will reduce the dataset's training loss; instead it reduces the batch's training loss, which might not the same thing. This adds noise to the training process, which can in some cases be a good thing, but requires the network to take too many steps to converge (this isn't a problem since each step is much faster).
What you're saying is essentially training the network each time on a single sample. This is formally called Stochastic Gradient Descent, however the term is used more broadly to include any case where the network is trained on a subset of the whole training set. The problem with this approach is that it adds too much noise to the training process, causing it to require a lot more steps to actually converge.
"
Intutitive explanation of why Experience Replay is used in a Deep Q Network?,"
I understand that Experience Replay is used for data efficiency reasons and to remove correlations in sequences of data. How exactly do these sequences of correlated data affect the performance of the algorithm?
","['reinforcement-learning', 'q-learning', 'dqn', 'experience-replay']",
How is the probability transition matrix populated in the Markov process (chain) for a board game?,"
Following on from my other (answered) question:
With regards to the Markov process (chain), if an environment is a board game and its states are the various position the game pieces may be in, how would the transition probability matrix be initialised? How would it be (if it is?) updated?
","['machine-learning', 'reinforcement-learning', 'game-ai', 'markov-chain']","
Transition probability matrix cannot be initialized. Your game world has some rules. Since we do not know these rules we can approximate them. To do this, we should run the game over and over and collect the outcome of each action.
Here is a toy example: assume we control a plane. We want the plane to fly forward, but sometimes there may be wind in a random direction. So the plane won't end up in the desired position. Instead, it has some probability to fly left, right and forward. After the data is collected, we can estimate this probabilities.
Another example is estimating n-gram model: assume we want to train a model that will suggest a word in search field. We would analyze a corpus and calculate all 2 word sequences (for bigram model). Then, if a user starts typing I the model would suggest the word am, because I am occurs more frequently then I did, for example, and hence, the transition probability from I to am is greater.
Here is good book on Language Modeling
But your question suggest me that you want something like reinforcement learning. Check out this video wich is a part of a great lectures series on RL by David Silver
"
"Where can I find people solving this smoothing, filtering, temporal learning problem?","
Consider a prediction problem for example. This is a loss function (negative log likelihood) that I am roughly talking about:
\begin{align*}
    J_{\text{train}} &= -\sum_t \log L\left(\theta_t, X_t, Y_t\right) \\
    J_{\text{test}} &= -\sum_t \log L\left(\theta_t, X_{t+1}, Y_{t+1}\right) \\
    J_{\text{dyn}} &= - \sum_t \frac{\left(\theta_{t+1} - \text{stop_gradient}({\theta_t})\right)^2}{2 \sigma_{\theta}^2} \\
    \theta_0 &\sim \Psi
\end{align*}
Here, the dynamics are simply first-order smoothness (Brownian motion) in the time-dependent parameters $\text{d}\theta_t = \sigma_{\theta} \text{d} W_t$.
This is basically what they do in weather systems and in papers like A Bayesian Approach to Data Assimilation.
The main difference is, we now have the ability to use stop gradients in the smoothness, which changes the problem somewhat. Other issues might arise.
Who else is doing this? Feel free to also provide links to papers on related work. I am not finding enough hits, so maybe I am missing something. Are RNNs, if twisted slightly, falling into this framework and I am no seeing it?
","['neural-networks', 'machine-learning']",
What AI conferences in Europe should I consider submitting papers to explaining the ongoing work on RefPerSys?,"
https://afia.asso.fr/journee-hommage-j-pitrat/ is a seminar on March 6th, 2020, in Paris (France, European Union), in honor of the late Jacques Pitrat, who advocated during all his professional life a meta-knowledge and reflective approach. (You need to register to attend that seminar).
Pitrat's blog is available (in spring 2020) on http://bootstrappingartificialintelligence.fr/WordPress3/ (and some snapshot of his CAIA system is downloadable here - but no documentation; however you might try to type L EDITE on stdin to caia). He wrote the Artificial Beings : the conscience of a conscious machine book describing the software architecture of, and the motivations for (some previous version of) CAIA. See also this A Step toward an Artificial Artificial Intelligence Scientist paper by J.Pitrat.
What AI conferences (or AGI workshops) in Europe should I consider submitting papers to explaining the ongoing work on RefPerSys?
That RefPerSys project (open source, open science, work-in-progress, with contact information) is explicitly following J.Pitrat's meta-knowledge approach. Feel free to follow or join that ambitious open-source (actually free software) project.
","['ai-design', 'research', 'papers', 'meta-learning']","
Probably to as many as possible. Average accept rate of papers is around 20%. You can find the best conferences on AI & ML Event.
"
Using three image datasets with different image sizes to train a CNN,"
I've just started with AI and CNN networks.
I have two NIFTI images dataset, one with (240, 240) dimensions and the other one with (256, 132). Each dataset is front a different hospital and machine.
If I want to use both to train my model. What do I have to do?
The model needs to have all the train data with the same shape. I've thought to reshape all the data to have the same shape, but I don't know if I'm going to lose information if I reshape the images.
By the way, I have also a third dataset with (232, 256).
","['convolutional-neural-networks', 'datasets', 'data-preprocessing']",
Impact or applications of introducing attention in deep networks modelling multi-agent systems,"
I have been reading quite a lot about the research progress in the domain of self attention-based neural networks that were introduced by Google Inc. in their paper titled ""Attention is all you need"".
The concept of introducing attention to neural networks in order to free ourselves from a strict context vector being really unique on one hand and moreover using the same concept to model sequences without recurrent neural networks as introduced in the paper is extremely elegant.
I have been trying to figure out so has to how this concept of attention would aid deep networks that model multi-agent systems in which game-theoretic factors come into play for the network to learn.
I was looking for some direction or a toy example/explanation or even possible previous research done to try to test these concepts together.
P.S - I'm just tinkering with some ideas hoping to build something experimental.
","['neural-networks', 'attention', 'game-theory']",
What are the variables used in a Gaussian radial basis kernel in the context of SVMs?,"
If I have the Gaussian kernel
$$
k(x, x') = \operatorname{exp}\left( -\| x - x' \|^2 / 2\sigma^2 \right)
$$
What is $x$ and $x'$ in the context of training an SVM?
","['machine-learning', 'math', 'support-vector-machine']",
How to adapt MTCNN to large images with relatively small ROIs,"
This question could be generalised to how to adapt state-of-the-art object detection models to large images with small ROIs.
In my particular case I'm trying to use this implementation of MTCNN to get bounding boxes for the faces of images of statues.
One challenge is that the face could take up a large proportion of the image like this:

Or a very small proportion of the image like this:

Where I'll zoom in on the statue's face so you can see it:

Bonus
If anyone has additional comments on my overall approach to this particular problem, happy to hear them.
","['convolutional-neural-networks', 'object-detection']",
"In the Markov chain, how are the directions to each successive state defined?","
I'm watching the David Silver series on YT which has raised a couple of questions: 
In the Markov process (or chain), how are the directions to each successive state defined? For example, how are the arrow directions defined for the MP below? What's stopping our sample episodes from choosing A -> C -> D -> F?

Also, how is the probability transition matrix populated? From David's example, the probabilities seem to have already been set. For example:

","['machine-learning', 'reinforcement-learning', 'markov-chain']",
How to do machine translation with no labeled data?,"
Is it be possible to train a neural network, with no parallel bilingual data, for machine translation?
","['machine-learning', 'unsupervised-learning', 'machine-translation', 'unlabeled-datasets']","
In this paper: Unsupervised Machine Translation Using Monolingual Corpora Only
 the authors proposed a novel method. 

Intuitively it is an autoencoder, but the Start Of Sentence token is set to be the language type. 
One other advanced method is to use the pre-training model. In this paper: Cross-lingual Language Model Pretraining researchers proposed an algorithm that utilized the pre-trained multi-lingual trained BERT(with labeled data but we don't need to have a labeled dataset for our task) and the autoencoder mentioned previously. 
"
Why gradients are so small in deep learning?,"
The learning rate in my model is 0.00001 and the gradients of the model is within the distribution of [-0.0001, 0.0001]. Is it normal?
","['deep-learning', 'backpropagation', 'gradient-descent']",
How to model personalized threshold problem with machine learning,"
Assume that I have a candidate selection system to generate product/user pairs for recommendation. Currently, in order to hold a quality bar for the recommended product, we trained a model to optimize for the click of the link, denoting as pClick(product, user) model, the output of the model is a score of (0,1) representing how likely the user will click on the recommended product.
For our initial launch product, we set a manually selected threshold, say T for all users. For all users, only when the threshold pass T, we will send user the recommendation.
Now we realize this is not optimal: Some users care less about recommendation quality while some other users have a high bar of recommendation quality. And a personalized threshold, instead of the global T can help us improve the overall relevance.
The goal is to output the threshold for each user, assume we have training data for each user's activity and user/product attributes.
The question is: How should we model this problem with machine learning? Any reference or papers is highly appreciated.
","['machine-learning', 'recommender-system']",
Is reinforcement learning suited for real-time systems?,"
From what I have seen, any results involving RL almost always take a massive number of simulations to reach a remotely good policy.
Will any form of RL be viable for real-time systems? 
","['reinforcement-learning', 'applications', 'real-time']","
Short answer: Yes, it is.
Explanation
Reinforcement learning can be considered as a online learning. That is, you can train your model with a single data/reward pairs.  As with any online learning algorithm, there are a few things to consider.
The model tends to forget the knowledge gained. To overcome this problem, one can save new data in a circular buffer called history and train the model with a portion of mix of new and old data. This is actually the common way to train an RL model and can be adopted to real-time systems. There are also others techniques to overcome it.
Another problem is that if only one data point is fed to the network, it will be impossible to apply some techniques, such as Batch normalization.
"
GANs: Should Generator update weights when Discriminator says false continuously,"
My GANs is like this:

Train an autoencoder (VAE), get the decoder part and use as Generator
Train Discriminator

After training, do the generation in these steps:

Call Generator to generate an image
Call the Discriminator to classify the image to see whether it's acceptable

The problem is that the Discriminator says 'false' a lot, which means the generated image is not useful.
How should the Generator change (update weights) when Discriminator doesn't accept its generated image?
","['machine-learning', 'generative-adversarial-networks', 'generative-model', 'generator', 'discriminator']",
How do 3 channels affect a network when detecting human skin (CNN)?,"
Yeah I know, best title ever. Anyway,
I want to make a neural network which is fed with frames coming from an usb camera. 
Don't wanna be so specific, so I'm just gonna say that the network's goal is to classify human hand gestures, therefore I need to make sure it can effectively learn how the hand moves around.
My problem is that I've no idea about what happens when having 3 channels instead of 1, I only know that (for 3 channels) it does 3 separate convolution operations with the same kernel, resulting actually in 3 separate layers. 
How do this 3 channels affect the network? Does it learn from the movement 3 parallel times, then it mixes toghether this 3 ""separate movements""? Do I need to make it single channel to help him detect the hand?
PS: the text is problably confusing, but that's because I'm confused to, that's why I'm asking.
",['convolutional-neural-networks'],"
3 channels will give you more information about the color of the object and surroundings which might be important in certain cases. For example if you want to classify blue cars and red cars then color of the object is very important. Since your problem is to classify hand gestures then color might not be that relevant. You're not very interested in the color of the hand you only care about it's position so there is a good chance that grayscale images might be enough. You should try with grayscale first and if that works good, if not, try with 3 channels.
"
Is the summation of consistent heuristic functions also consistent?,"
Imagine that we have a set of heuristic functions $\{h_i\}_{i=1}^N$, where each $h_i$ is both admissible and consistent (monotonic). Is $\sum_{i=1}^N h_i$ still consistent or not?
Is there any proof or counterexample to show the contradiction?
","['search', 'proofs', 'heuristics', 'admissible-heuristic', 'consistent-heuristic']","
No, it will not necessary be consistent or admissible. Consider this example, where $s$ is the start, $g$ is the goal, and the distance between them is 1.

s --1-- g

Assume that $h_0$ and $h_1$ are perfect heuristics. Then $h_0(s) = 1$ and $h_1(s) = 1$. In this case the heuristic is inadmissible because $h_0(s)+h_1(s) = 2 > d(s, g)$. Similarly, as an undirected graph the heuristic will be inconsistent because $|h(s)-h(g)| > d(s, g)$.
If you'd like to understand the conditions for the sum of heuristics to be consistent and admissible, I would look at the work on additive PDB heuristics.
"
What's the best method to predict/generate signal from one sensor (source) to signal from another another (target)?,"
I was wondering what is the best method out there to find relationship between two 1D signals so that I can predict/generate one (source) from the other (target). For example, let's say that in response to an event, my sensor A's readings vary in a certain way for 5 seconds. For the same event sensor B's readings vary as well for 5 seconds. Sensor A and B are not measuring the same physical quantities but respond to the same event and seem to have a relationship.  
What can I do to use the signal from sensor A to learn how the signal from sensor B would look like for that event? What is the state of the art in deep learning?
","['deep-learning', 'time-series', 'sequence-modeling', 'machine-translation', 'signal-processing']",
Is a dystopian surveillance state computationally possible?,"
This isn't really a conspiracy theory question. More of an inquire on the global computational power and data storage logistics question.
Most recording instruments such as cameras and microphones are typically voluntary opt in devices, in that, they have to be activated before they start recording. What happens if all of these devices were permanently activated and started recording data to some distributed global data storage?
There are 400 hours of video uploaded to YouTube every minute.
Let’s do some very rough math.
I’m going to assume for the rest of this post that the average video is 1080p which is 2.5GB (or $10^9$ bytes) per hour. From that, we get about 400 hrs * 60 mins * 2.5GB/hrs * 24 hrs = 1.5 petabytes (or $10^{15}$ bytes) per day.
But YouTube videos post are voluntary, and they are far from continuous video streams.
There are about 3.5 billion smartphones in the world. If video was continuously streamed and recorded, going through the same video math above ($3.5 * 10^9 * 1.5 * 10^{15} * 24)$ = 126 yottabytes (or $10^{24}$ bytes) per day.
The IDC projects there will be 175 zettabytes (or $10^{21}$ bytes) in 2025.
Unless my math is very wrong, it would seem as though smartphone cameras alone could produce more data in one day than all of the data created in human history in 2025.
This, so far, has only been about the data recording, but, to implement a surveillance state, all recorded data would need to be processed by AI to intelligent flag data that is significant. How much processing power would be needed to filter 126 yottabytes into relevant information?
Overall, this question is motivated by the spread of dystopian surveillance media like Edward Snowden NSA whistle blowing leaks or George Orwell's sentiment of ""Big Brother is Watching You"".
Computationally, could we be surveilled, and to what extent? I imagine text messages surveillance would be the easiest, does the world have the computation power to surveil all text messages? How about audio? or video?
","['social', 'computation']","
You don't necessarily have to analyse it all. Just by having such data available you can achieve a lot in terms of surveillance, as long as you can retrieve relevant parts.
A few years ago there was a Radiolab podcast, ""The Eye in the Sky"" (there's a full transcript on the site). The basic idea is that you have a plane circling a city 24/7, and filming what goes on. If there was a crime somewhere, you retrieve the recordings after the event, and you can track back to where vehicles involved in the crime were coming from, and where they went after the crime. If nothing happens, you simply archive the data, and perhaps remove it after a month or so.
This method was used to solve a hit-and-run assassination of a police woman who was on her way to work. The gang who committed the attack were rather surprised when the police showed up at their secret hide-out a few days later, as they could see on the images where the cars involved went to later. At the time and place of the murder there were obviously no witnesses who could have done that. And this involved no computational processing at all.
The possibilities this opens up are just scary, as you can track pretty much anybody's movements without actually needing someone to follow them. Add to that street-level CCTV, and not much can happen without you being able to find out.
In this scenario there is no processing at all, but you could imaging simple processing steps, such as tracking vehicles or changes in the environment, which could be used to give clues about potentially 'interesting' events. So instead of using it 'passively' as a kind of memory, you could use that data to identify things that happened that you weren't aware of.
And this is without even any clandestine access to people's data. If you add that dimension, then you might even be able to identify crimes/etc before they even happen. Text processing can be quite fast, but is not easy to do, as presumably few people would openly communicate about things they were planning. So I guess we're still a long way away from that.
Of course there is the ethical dimension (which is mentioned in the podcast): who has access to that data, and who decides what it is used for? If you do, and you suspect your partner of being unfaithful, who/what would stop you from checking out their movements? Or check up on that politician who might have a secret affair, or a gambling issue, or who keeps being in the same locations as a well-known drug dealer. All rather scary.
While a complete analysis of all such data would be very heavy computationally, and fraught with false positives and recall problems, it might simply be enough to index it by time, location, and perhaps people involved (face recognition seems to be reasonably good, though still with a rather high error rate). This is enough already to make me feel worried about the future.
"
Suitable deep learning algorithms for spatial / geometric data,"
I have a task of classifying spatial data from a geographic information system. More precisely, I need a way to filter out unnecessary line segments from the CAD system before loading into the GIS (see the attached picture, colors for illustrative purposes only).

The problem is that there are much more variations of objects than in the picture. The task is difficult to solve in an algorithmic way.
I tried to apply a bunch of classification algorithms from the Scikit-learn package and, in general, got significant results. GradientBoostingClassifier and ExtraTreeClassifier achive an accuracy about 96-98%, but:

this accuracy is achieved in the context of individual segments into
which I explode the source objects (hundreds of thousands of objects)
After reverse aggregation of objects it may turn out that one of
their segments in each object is classified incorrectly. The error in
the context of the source objects is high
significant computational resources and time are required for the
preparation of the source data, and calculation of features for
classifiers
it is impossible to use the source 2d coordinates of objects in
algorithms, but only their derivatives

I tried to find good examples of use deep neural networks for this kind of tasks / for spatial data, but I only found articles that are quite difficult to understand about the use of such networks for point clouds classification, and some information on geometric deep learning for graphs. I do not have enough knowledge to adapt them for my case
Can someone provide me good examples of using neural networks directly with 2d coordinates and, maybe, good articles on these theme written in simple language?
Thanks
","['deep-learning', 'geometric-deep-learning', 'graphs', 'scikit-learn']",
What are the reasons behind slow YOLO training?,"
I'm testing out YOLOv3 using the 'darknet' binary, and custom config. It trains rather slow.
My testing out is only with 1 image, 1 class, and using YOLOv3-tiny instead of YOLOv3 full, but the training of yolov3-tiny isn't fast as expected for 1 class/1 image.
The accuracy reached near 100% after like 3000 or 4000 batches, in similarly 3 to 4 hours. 
Why is it slow with just 1 class/1 image?
","['training', 'computer-vision', 'object-detection', 'yolo', 'darknet']","
It depends upon the factors such as 
1. Batch size (GPU memory capacity)
2. CPU speed and number of cores(multi-threading to load the images)
Number of classes increase the number of convolution filters only in the prediction layers of YOLO. It influences only less than 1% speed of the detector to train the model.
"
Using ML for Enemy Generation in Video Games,"
I am attempting to make a 2-D platformer game where the player traverses through an evil factory that is producing killer robots. The robots spawn at multiple specific locations in each level and impede the player's progress.
Enemies are procedurally generated using machine learning. Early levels have ""garbage"" robots that plop down and can't really do anything. After generations of training, the robots begin having more refined bodies and are able to move about and attack the player. Later levels produce enemies that are more challenging.
Enemies consist of a body and up to 4 limbs. The body is simply a circle of a certain radius, while the limbs are just a bar with a certain length. Limbs can pivot and/or contract/extend. Additionally, each limb can have one of three types of ""motor"" (wheel, spring, or hover). This makes for about 20-25 input parameters:

BodySize, Limb1Enabled, Limb1PivotPoint, Limb1Length, Limb1Angle, Limb1MotorType, Limb1MotorStrength, Limb2Enabled, Limb2PivotPoint, Limb2Length, Limb2Angle, Limb2MotorType, Limb2MotorStrength, Limb3Enabled, Limb3PivotPoint, Limb3Length, Limb3Angle, Limb3MotorType, Limb3MotorStrength, Limb4Enabled, Limb4PivotPoint, Limb4Length, Limb4Angle, Limb4MotorType, Limb4MotorStrength

My thoughts are that a genetic algorithm (or something similar) would be used to generate a body, while a neural network would control that body by using the same inputs to generate outputs that control the limbs and motors.
There would actually be 3 ""control brains"" that would have to be trained using the same inputs, but having different fitness goals: Moving Right/Left, Moving Up, and Attacking the Player. (Gravity exists in 2-D platformers, so moving down isn't necessary.)
A fourth, ""master brain"" would take the player's relative location, score, and maybe time elapsed, as inputs, and would output one of the goals for the robot to achieve (move left, move right, and attack).
The master brain's fitness would be determined by the ""inverse"" of the player's ""progress"", while each control brain's fitness would be determined by how well it was able to perform the task assigned by the master brain. Finally, the overall fitness for the body's genetic algorithm would be an average (or some other function like min, max, etc.) of the three control brain's fitness values.
Now that I have all this ""down on paper"", where do I start? I had planned on doing this in Unity, but early attempts have been a bit confusing for me. I've been able to procedurally generate a body with random limbs (no motors) that wiggle about randomly, but there's no neural network or any machine learning going on whatsoever. I am not exactly sure how to expose my parameters to be used as inputs, and am barely grasping how I should take those outputs to control what I want them to. Are there any libraries I should look at, or should I write this all from scratch?
Also, before I get too far ahead of myself, what are the flaws in my approach (as I'm sure there are plenty). I want my project to be something practical in scope, if training can't be done feasibly while a player traverses a level, this might just be a dead project idea.
Anyways, that all being said, thank you for your help.
","['neural-networks', 'machine-learning', 'game-ai', 'genetic-algorithms', 'models']",
Is it legal to construct a public image database (for deep learning) with images from the internet? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



I am trying to put together a public agricultural image database of corn and soybeans, to train convolutional neural networks. The main method of image collection will be through taking pictures of various fields in the growing season. The images will be uploaded to a public data sharing site which will be accessible by many.
However, I could get much more images compiled if I were to take some off of, say, Google Images. If there anything wrong with this? Would there be any issues with copywright infringements if I find the images on a publicly-available search engine? I need a lot of images, so I thought this would be a good method of increasing my image numbers.
","['deep-learning', 'convolutional-neural-networks', 'image-recognition', 'datasets']",
How do we define intention if there is no free will?,"
There is an idea that intentionality may be a requirement of true intelligence, here defined as human intelligence.  
But all I know for certain is that we have the appearance of free will.  Under the assumption that the universe is purely deterministic, what do we mean by intention?
(This seems an important question given that intention is not just a philosophical matter in relation to definitions of AI, but involves ethics in the sense of application of AI, ""offloading responsibility to agents that cannot be meaningfully punished"" as an example.  Also touches on goals, implied by intention, whether awareness is a requirement, and what constitutes awareness. I'm interested in all angles, but was inspired by the question ""does true art require intention, and, if so, is that the sole domain of humans?"")
","['philosophy', 'intelligent-agent', 'intelligence', 'goal-based-agents']","
My thoughts.
The short answer is: you can't.
The long answer is that since we're searching for a new definition of a term when removing a necessary (in my opinion) precondition for it to exist, the question becomes ""can you make up a new definition for what you intuitively and empirically understand as intention, while removing free will from the picture?"". I'm going to give it a shot.
First of all, there's a lot to be said about whether or not the idea or intuition of intention even exists in our collective discourse based on the latent assumption that free will is a thing. As in, before any rigorous definition, even the intuitions encoded in discourse, philosophy, art, and other languages as ""intention"" could very well be as invalid as the assumption of free will itself.
That being said, I'm a fan of Deleuze's model for people and other entities as machines of input, output and internal state (not his wording, but I paraphrase and in consequence, interpret and alter for the purposes of my point). It's not perfect, but I run to it a lot to answer these question as I find it very refreshing, often lacking in bias and having good explanatory power compared to the usual romance-foo that dominates these conversations. If that's the case you could pretty much define intention not as a self-started force but as a product of a much blurrier mechanism, namely the non-deterministic characteristic of this rhizomatic soup. Whether or not an input or output will exist, what kind will it be, what internal state will it find or cause in the machine and the long term dependencies between these interactions seem to me like a convincing enough candidate for the cause of any intuition (or illusion if you like a more cynic vocabulary) of ""intention"". It's pretty much an emergent symbol we use, assuming the form of a force for the setup and function of new connections, that will in their complication or pure non-determinism spawn even more intention in the network.
tl;dr: Intent could be the most basic expression of the RNG of the universe.
"
How should I define the state space for this life science problem?,"
I would like to ask for a piece of advice with regard to Q-learning. I am studying RL and would like to do a basic project applied to life science and calculate the reward. I have been trying to get my head around how to define all possible states of the environment. 
My states are $S = ( \text{health } (4 \text{ levels}), \text{shape } (3 \text{ levels}) \}$. My actions are $A=\{a_1, a_2, \dots, a_4 \}$. My possible states are $60=4 * 3 * 5$. Could you advise whether these are correct?
$(s_{w_0, sh_0}, a_1, s'_{w_1, sh_1})$ is a tuple of the initial state $s_{w_0, sh_0}$, the first action $a_1$ and the next state $s'_{w_1, sh_1}$, where $w$ is the health level, $sh$ is the shape of the tumor. 
","['reinforcement-learning', 'ai-design', 'q-learning', 'applications']",
How is the expected value in the loss function of DQN approximated?,"
In Deep Q Learning the parametrized Q-functions $Q_i$ are optimised by performing gradient descent on the series of loss functions 
$L_i(\theta_i)= E_{(s,a)\sim p}[(y_i-Q(s,a;\theta_i))^2]$ , where 
$y_i = E_{s' \sim \mathcal{E}}[r+\gamma \max_{a'}Q(s',a';\theta_{i+1})\mid s,a]$.
In the actual algorithm, however, the expected value is never computed. Also, I think it cannot be computed since the transition probabilities of the underlying MDP remain hidden from the agent. Instead of the expected value, we compute $y_i = r_i + \gamma \max_a Q(\phi_{i+1},a;\theta)$. I assume some sort of stochastic approximation is taking place here. Can someone explain the details?
","['reinforcement-learning', 'q-learning', 'math', 'dqn']",
Is there an advantage in decaying $\epsilon$ during Q-Learning?,"
If the agent is following an $\epsilon$-greedy policy derived from Q, is there any advantage to decaying $\epsilon$ even though $\epsilon$ decay is not required for convergence?
","['reinforcement-learning', 'q-learning', 'convergence', 'epsilon-greedy-policy', 'exploration-strategies']",
Does SARSA(0) converge to the optimal policy in expectation if the Robbins-Monro conditions are removed?,"
The conditions of convergence of SARSA(0) to the optimal policy are :

The Robbins-Monro conditions above hold for $α_t$.
Every state-action pair is visited infinitely often
The policy is greedy with respect to the policy derived from $Q$ in the limit
The controlled Markov chain is communicating: every state can be reached from any other with positive probability (under some policy).
$\operatorname{Var}{R(s, a)} < \infty$, where $R$ is the reward function

The original proof of the convergence of TD(0) prediction (page 24 of the paper Learning to Predict by the Method of Temporal Differences) was for convergence in the mean of the estimation to the true value function. This did not require the learning rate parameter to satisfy Robbins-Monro conditions.
I was wondering if the Robbins-Monro conditions are removed from the SARSA(0) assumptions would the policy converge in some notion of expectation to the optimal policy?
","['reinforcement-learning', 'proofs', 'convergence', 'temporal-difference-methods']",
What are the conditions for the convergence of SARSA to the optimal value function?,"
Is it correct that for SARSA to converge to the optimal value function (and policy)

The learning rate parameter  $\alpha$ must satisfy the conditions:
$$\sum \alpha_{n^k(s,a)} =\infty \quad \text{and}\quad \sum \alpha_{n^k(s,a)}^{2} <\infty \quad \forall s \in \mathcal{S}$$
where $n_k(s,a)$ denotes the $k^\text{th}$ time $(s,a)$ is visited
$\epsilon$ (of the $\epsilon$-greedy policy) must be decayed so that the policy converges to a greedy policy.
Every state-action pair is visited infinitely many times.

Are any of these conditions redundant?
","['reinforcement-learning', 'math', 'proofs', 'convergence', 'sarsa']","
I have the conditions for convergence in these notes SARSA convergence by Nahum Shimkin.

The Robbins-Monro conditions above hold for $α_t$.
Every state-action pair is visited infinitely often
The policy is greedy with respect to the policy derived from $Q$ in the limit
The controlled Markov chain is communicating: every state can be reached from any other with positive probability (under some policy).
$\operatorname{Var}{R(s, a)} < \infty$, where $R$ is the reward function

"
How is the gradient with respect to weights derived in batch normalization?,"
At the bottom of page 2 of the paper L2 Regularization versus Batch and Weight Normalization, the equation for the gradient of the output with respect to the weights is given as:
$$
\triangledown y_{BN} (X; w, \gamma, \beta)  = \frac{X}{\sigma(X)}\gamma g'(z).
$$
Can someone break down into smaller steps on how the author got to that equation?
","['papers', 'backpropagation', 'notation', 'regularization', 'batch-normalization']",
YOLOv3 Model Structure: Why is filters = (classes + coords + 1) * num?,"
Here's a tutorial about doing custom training of YOLO (Darknet): https://medium.com/@manivannan_data/how-to-train-yolov3-to-detect-custom-objects-ccbcafeb13d2
The tutorial guides how to set values in the .cfg files:

classes = Number of classes, OK
filters = (classes + 5) * 3

Why is it 'plus 5' then 'times 3'?
Some say it's (classes + coords + 1) * num, but I can't guess it out the meaning.
","['computer-vision', 'yolo', 'darknet', 'models', 'filters']",
Irregular results while prediction identical object on same image,"
I used the pre-trained model faster_rcnn_resnet101_coco.config with my own dataset.
I have two issues 

some objects were not detected, while I learned it, with a high number of steps, and test over the same learning data, but still have some missing data
sometimes, the same identical objects predicted correctly on some images and sometimes it got missed, although the objects are identical 

Any help would be appreciated.
","['tensorflow', 'prediction', 'object-detection']",
Monte Carlo updates on policy gradient with no terminal state,"
Consider some MDP with no terminal state. We can apply bootstrapping methods (like TD(0)) to learn in these cases no problem, but in policy gradient algorithms that have only a simple monte carlo update, it requires us to supply a complete trajectory (which is impossible with no terminal state). 
Naturally, one might let the MDP run for 1000 periods, and then terminate as an approximation. If we feed these trajectories into a monte carlo update, I imagine that samples for time period t=1,2,...,100 would give very good estimates for the value function due to the discount factor. However, the time periods 997, 998, 999, 1000, we'd have an expected value for those trajectories far different than V(s) due to their proximity to the cutoff of 1000.
The question is this: 

Should we even include these later-occurring data points when we update our function approximation? 

OR

Is the assumption that these points become really sparse in our updates, so they won't have much effect in our training?

OR

Is it usually implied that the final data reward in the trajectory is bootstrapped in these cases (i.e., we have some TD(0)-like behavior in this case)?

OR 

Are monte carlo updates for policy gradient algorithms even appropriate for non-terminating MDPs due to this issue? 

","['deep-rl', 'policy-gradients', 'function-approximation']","

Naturally, one might let the MDP run for 1000 periods, and then terminate as an approximation. If we feed these trajectories into a monte carlo update, I imagine that samples for time period t=1,2,...,100 would give very good estimates for the value function due to the discount factor. However, the time periods 997, 998, 999, 1000, we'd have an expected value for those trajectories far different than V(s) due to their proximity to the cutoff of 1000.

I think your intuition is... partially right here, but not entirely precise. Recall that a value function $V(S)$ is generally defined as something like (omitting some unimportant details like specifying the policy):
$$V(S_i) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{i+t} \right].$$
The ""samples for time period $t = 1, 2, \dots, 100$ that you mention are not estimates for this full value function. They're estimates for the corresponding individual terms $R_{i+t}$. Indeed, in general, your intuition is right that the closer they are to the ""starting point"" $i$, the more likely they'll be to be accurate estimators. This is because larger $t$ are typically associated with larger numbers of stochastic state-transitions and stochastic decision-making, and therefore often exhibit higher variance.


Should we even include these later-occurring data points when we update our function approximation? 


Theoretically, you absolutely should. Suppose you have an environment where almost every reward is equal to $0$, and only after 1000 steps do you actually observe a non-zero reward. If you don't include this, you'll learn nothing! In practice, it can often be a good idea to give them less importance though. This already happens automatically by picking a discount factor $\gamma < 1$.


Is it usually implied that the final data reward in the trajectory is bootstrapped in these cases (i.e., we have some TD(0)-like behavior in this case)?

OR

Are monte carlo updates for policy gradient algorithms even appropriate for non-terminating MDPs due to this issue? 


It would be possible to do some form of bootstrapping at the end yeah, cut off and then have a trained value function predicting what the remainder of the rewards would be. TD($\lambda$) with $\lambda$ close to $1$ would be much closer in behaviour to true MC updates than TD($0$) though. Either way, it would be technically incorrect to still call it Monte-Carlo then, it would no longer be pure Monte-Carlo. So yes, strict Monte-Carlo updates in the purest sense of the term are not really applicable to infinite episodes.
"
Can two planning PDDL actions be taken simultaneously?,"
We are discussing planning algorithms currently, and the question is to describe the steps to check if actions could be taken simultaneously. This is a really open-ended question so I'm not sure where to start. 
","['planning', 'pddl']","
First place to look is how the preconditions/effects of different actions interact.
"
What's the difference in using multiple convolutional layers and no pooling versus using a single convolutional layer and a single max pooling layer?,"
I'm currently working on a college project in which I'm designing a Deep Q-Network that takes images/frames as an input.
I've been searching online to see how other people have designed their convolutional stage and I've seen many different implementations.
Some projects, such as DeepMinds Atari 2600 project, use 3 convolutional layers and no pooling (from what I can see).
However, other projects use fewer convolutional layers and add a pooling layer onto the end.
I understand what both layers do, I was just wondering is there a benefit to how DeepMind did it and not use pooling or should I be using a pooling layer and fewer convolutional layers?
Or have I completely missed out on something? Is Deep Mind actually using pooling after each convolutional layer?
","['convolutional-neural-networks', 'dqn', 'convolution', 'deepmind', 'pooling']",
How can I constraint the actions with dependent coordinates?,"
I am working on a customized RL environment where each action is represented as a tuple $a = (a_1,a_2,\cdots,a_n)$ such that certain condition must be satisfied for entries of $a$ (for instance, $a_1+a_2+\cdots+a_n \leq \text{constant}$). 
I am using the policy gradient method, but I am having some difficulty modeling the underlying probability distribution of actions.  Is there any work done in this direction?
For the constraint $a_1+a_2+\cdots+a_n \leq \text{constant}$, I was thinking about generating $n+1$ uniform random variables $U_1,U_2,\cdots,U_n, U$, and set $a_i = \text{constant}\times U \times \frac{U_i}{\sum_{j=1}^n U_j}$. Problem is that the joint density is a bit messy to calculate, which is needed to get the negative log likelihood. I am curious about how such issue is handled in practice. 
","['reinforcement-learning', 'policy-gradients', 'markov-decision-process']","
At first glance, I thought this was similar to ""continuous-discrete"" action selection (https://arxiv.org/pdf/1810.06394.pdf). However, I think your problem is different.
I am assuming that each $a_i$ is continuous and that the action which interacts with your environment is the entire vector $a = (a_1,a_2,\dotso,a_n)$ and not an individual $a_i$. Then you could treat it like a hierarchical problem. If you want $a_1 + a_2 < 2$ for example, then you could sample $a_1 \sim U(0,2)$ and $a_2 | a_1 \sim U(0, 2-a_1)$ and have $p(a) = p(a_2 | a_1)p(a_1)$. The specifics of how you do this depends more finely on how your problem is set up.
Perhaps you can find similar ideas from the paper linked above. Also, other work in the robitics literature studies structured and hybrid action spaces.
"
How does the regression layer in the localization network of a spatial transformer work?,"
I am trying to understand the spatial transformer network mentioned in this paper https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf. I am clear about the last two stages of the spatial transformer i.e. the grid generator and sampler. However I am unable to understand the localization network which outputs the parameters of the transformation that is applied to the input image. So here are my doubts.

Is the network trained on various affine/projective transforms of the input or only the standard input with a standard pose?
If the answer to question 1 is no, then how does the regression layer correctly regress the values of the transformation applied to the image? In other words how does the regression layer know what transformation parameters are required when it has never seen those inputs before?

Thanks in advance. 
","['neural-networks', 'machine-learning', 'regression', 'transformer']","

Localization network is not trained separately on special transform of input. It's just a part of feed forward network which is trained as a whole, with normal backpropagation.
It's just part of the network which affect the loss function. As in any backpropagation loss function propagate back gradient, which backpropagate through final part of the network, backpropagate through differentiable sampler(that is non-trivial part, which use transfromation produced by localization subnetwork) and after that backpropagate into localization part
Whole approach of spatial transformers could be in doubt now. There were some anecdotal evidences that it was not working on less trivial tasks (private communications, was not working for me either)

"
What is the equation of the separation line for this neuron with identity activation?,"
I have a single neuron with 2 inputs, and identity activation, where f is activation function and u is output:
$u = f(w_1x_1 + w_2x_2 + b) = w_1x_1 + w_2x_2 + b$
My guessing for the separation line equation:
$u = w_1x_1 + w_2x_2 + b$
$\implies x_2 = \dfrac{u - w_1x_1 - b}{w_2}$
$\implies x_2 = (\dfrac{-w_1}{w_2})x_1 + \dfrac{u-b}{w_2}$
And the questions are:
1) Is the separation line equation above correct?
2) And when f is not identity function, is the separation line equation still the same? or different?
","['machine-learning', 'math', 'artificial-neuron', 'activation-functions']",
Is it possible to use deeplearning with spark (with a distributed databases as HDFS or Cassandra)? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed last year.







                        Improve this question
                    



If it is possible, will it be really useful or the model will end up converging very early(with a typical optimum learning rate) ? Any content on this topic will be helpful for me.
","['neural-networks', 'deep-learning', 'tensorflow', 'convergence']",
Is it possible to derive meaning from text by providing multiple ways of saying the same thing to a neural network?,"
Let's say I feed a neural network with multiple string sentences that mean roughly the same thing but are formulated differently. Will the neural network be able to derive patterns of meaning in the same way that it is being done with images. Is this an approach currently used in Natural Language Processing? 
With images of dogs the neural network will get the underlying patterns that define a dog. Could it be the same thing with sentences? 
","['neural-networks', 'natural-language-processing', 'classification', 'text-classification']","
No, it will not derive patterns of meaning, as the network has no understanding of language. What will happen, is that it picks up surface features (usually letter sequences) which are common between sentences with the same (or a similar) meaning.
This approach is often used in chatbots for intent recognition. Sometimes it picks up subtle patterns that humans would not notice, but they are also often not reliable: you get wrong classifications without knowing why.
Having said that, it works fine if you want to distinguish between a limited number of different intents (you don't even need a lot of training examples — 4 to 5 examples are often sufficient if they are well-selected). In this fairly limited scenario yes, otherwise no.
"
Is this model overfitted or not?,"
I am training a neural network and plot model accuracy and model loss.
I am a little confused about overfitting. Is my model overfitted or not? how can I interpret it



EDIT: here is a sample of my input data, I have a binary image classification

","['neural-networks', 'training']","
Overfitting nearly always occurs to some degree when fitting to limited data sets, and neural networks are very prone to it. However neither of your graphs show a major problem with overfitting - that is usually obvious when epoch counts increase, the results on training data continue to improve whilst the results on cross validation get progressively worse. Your validation results do not do that, and appear to remain stable.
It is usually pragmatic to accept that there will be at least some difference between measurements on the training set and cross validation or test sets. The primary goal is usually to get the best measurements in test that you can. With that in mind, you are usually only interested in how much you are overfitting if it implies you could improve performance by using techniques to reduce overfitting e.g. various forms of regularisation.
Without knowing your data set or known good results, it is hard to tell whether the difference you are seeing between test and train in accuracy could be improved. Your accuracy graph shows a train accuracy close to 100% and a validation accuracy close to ~96%. It looks a bit like MNIST results, and if I saw that result on MNIST I would suspect something was wrong, and it might be fixed by looking at regularisation (but it might also be something esle). However, that's only because I know that 99.7% accuracy is possible in test - on other problems I might be very happy with 96% accuracy.
The loss graph is not very useful, since the scale has completely lost any difference there might be between training and validation. You should probably re-scale it to show detail close to 0 loss, and ignore the earlier large loss values.
"
Should I apply image processing techniques to the inputs of convolution networks?,"
After working for some time with feature-based pattern recognition, I am switching to CNN to see if I can get a higher recognition rate.
In my feature-based algorithm, I do some image processing on the picture before extracting the features, such as some convolution filters to reduce noise and segmentation into the foreground and background, and finally identifying and binarization of objects.
Should I do the same image processing before feeding data into my CNN, or is it possible to feed raw data to a CNN and expect that the CNN will adapt automatically without per-image-processing steps? 
","['convolutional-neural-networks', 'image-recognition', 'image-processing']","
The CNN should work without trying to do special feature extraction. As pointed out some pre-processing can aid in enhancing the CNN's classification results. The Keras 
ImageDataGenerator provides optional parameters you can set to provide pre-processing as well as provide data augmentation.
One thing I know that works for sure but can be painful is cropping the images in such a way that the subject of interest occupies a high percentage of the pixels in the resultant cropped image. The cropped image can than be resized as needed. The logic here is simple.
You want your CNN to train on the subject of interest (for example a bird sitting in a tree where the bird is the subject of interest). The part of the image that is not of the bird is essentially just noise making the classifier's job harder. For example say you have a 500 X 500 initial image in which the subject of interest (the bird) only takes up 10% of the pixels (25,000 pixels). Now say as  input to your CNN you reduce the image size to 100 X 100. Now the (pixels that the CNN 'learns' from is down to 1000 pixels.
However lets say you crop the image so that the features of the bird are preserved but the  pixels of the bird in the cropped image take up 50% of the pixels. Now if you resize the cropped image to 100 X 100 , 5000 pixels of relevance are available for the network to learn from. I have done this on several data sets. In particular images of people where the subject of interest is the face. There are many programs that are effective at cropping these images so that  mostly just the face appears in the cropped result. I have trained a deep CNN in one case using uncropped images  and in the other with cropped images. The results are significantly better using the cropped images.
"
"Solving the supervised learning problem of learning $p(y \vert \mathbf{x})$ by using traditional unsupervised technologies to learn $p(\mathbf{x}, y)$","
I am currently studying Deep Learning by Goodfellow, Bengio, and Courville. In chapter 5.1.2 The Performance Measure, $P$, the authors say the following:

Unsupervised learning and supervised learning are not formally defined terms. The lines between them are often blurred. Many machine learning technologies can  be used to perform both tasks. For example, the chain rule of probability states that for a vector $\mathbf{x} \in \mathbb{R}^n$, the joint distribution can be decomposed as
$$p(\mathbf{x}) = \prod_{i = 1}^n p(x_i \vert x_1, \dots, x_{i - 1} ).$$
This decomposition means that we can solve the ostensibly unsupervised problem of modeling $p(\mathbf{x})$ by splitting it into $n$ supervised learning problems. Alternatively, we  can solve the supervised learning problem of learning $p(y \vert \mathbf{x})$ by using traditional unsupervised technologies to learn the joint distribution $p(\mathbf{x}, y)$, then inferring
$$p(y \vert \mathbf{x} ) = \dfrac{p(\mathbf{x}, y)}{\sum_{y'}p(\mathbf{x}, y')}.$$

I found this part vague:

Alternatively, we can solve the supervised learning problem of learning $p(y \vert \mathbf{x})$ by using traditional unsupervised technologies to learn the joint distribution $p(\mathbf{x}, y)$, then inferring
$$p(y \vert \mathbf{x} ) = \dfrac{p(\mathbf{x}, y)}{\sum_{y'}p(\mathbf{x}, y')}.$$

Can someone please elaborate on this, and also explain more clearly the role of $p(y \vert \mathbf{x} ) = \dfrac{p(\mathbf{x}, y)}{\sum_{y'}p(\mathbf{x}, y')}$?
I would greatly appreciate it if people would please take the time to clarify this.
","['unsupervised-learning', 'supervised-learning', 'probability-distribution', 'conditional-probability']",
Have GANs been used to solve regression problems?,"
I've noticed that in the last 2 years GANs have become really popular. I know that initially they have been proposed for image classification but I was curious if any of you are aware of any papers where GANs are used to solve regression problems?
","['machine-learning', 'applications', 'research', 'generative-adversarial-networks', 'regression']","
In reality GANs are not made for image classification, but for data generation, and they have gained popularity on image generation. They are also used for tabular data generation, see for example TGAN, or for time series generation, e.g. Quant GAN. You have even some application for the field of graphs and networking, e.g. NetGAN and GraphGAN.
"
Why is my loss (binary cross entropy) converging on ~0.6? (Task: Natural Language Inference),"
I’m trying to debug my neural network (BERT fine-tuning) trained for natural language inference with binary classification of either entailment or contradiction. I've trained it for 80 epochs and its converging on ~0.68. Why isn't it getting any lower?
Thanks in advance!

Neural Network Architecture:

Training details:

Loss function: Binary cross entropy
Batch size: 8
Optimizer: Adam (learning rate = 0.001)
Framework: Tensorflow 2.0.1
Pooled embeddings used from BERT output.
BERT parameters are not frozen.

Dataset:

10,000 samples
balanced dataset (5k each for entailment and contradiction)
dataset is a subset of data mined from wikipedia.
Claim example: ""'History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.'""
Evidence example: ""The subsequent expansion of the list of principal arts in the 20th century reached to nine : architecture , dance , sculpture , music , painting , poetry -LRB- described broadly as a form of literature with aesthetic purpose or function , which also includes the distinct genres of theatre and narrative -RRB- , film , photography and graphic arts .""

Dataset preprocessing: 

Used [SEP] to separate the two sentences instead of using separate embeddings via 2 BERT layers. (Hence, segment ids are computed as such)
BERT's FullTokenizer for tokenization.
Truncated to a maximum sequence length of 64.

See below for a graph of the training history. (Red = train_loss, Blue = val_loss)

","['deep-learning', 'natural-language-processing', 'tensorflow', 'training', 'bert']","
Are you using BinaryCrossEntropy through tensorflow? If so, check if you are using the logits argument. I am using from_logits=True .It is not similar to the original BinaryCrossEntropy loss.
"
Many of the best probabilistic models represent probability distributions only implicitly,"
I am currently studying Deep Learning by Goodfellow, Bengio, and Courville. In chapter 5.1.2 The Performance Measure, P, the authors say the following:

The choice of performance measure may seem straightforward and objective, but it is often difficult to choose a performance measure that corresponds well to the desired behavior of the system.
In some cases, this is because it is difficult to decide what should be measured. For example, when performing a transcription task, should we measure the accuracy  of the system at transcribing entire sequences, or should we use a more fine-grained performance measure that gives partial credit for getting some elements of the sequence correct? When performing a regression task, should we penalize the  system more if it frequently makes medium-sized mistakes or if it rarely makes very large mistakes? These kinds of design choices depend on the application.
In other cases, we know what quantity we would ideally like to measure, but measuring it is impractical. For example, this arises frequently in the context of density estimation. Many of the best probabilistic models represent probability distributions only implicitly. Computing the actual probability value assigned to a  specific point in space in many such models is intractable. In these cases, one must design an alternative criterion that still corresponds to the design objectives, or design a good approximation to the desired criterion.

It is this part that interests me:

Many of the best probabilistic models represent probability distributions only implicitly.

I don't have the experience to understand what this means (what does it mean to represent distributions ""implicitly""?). I would greatly appreciate it if people would please take the time to elaborate upon this.
","['deep-learning', 'probability-distribution']",
What are the differences between transfer learning and meta learning?,"
What are the differences between meta-learning and transfer learning?
I have read 2 articles on Quora and TowardDataScience. 

Meta learning is a part of machine learning theory in which some
  algorithms are applied on meta data about the case to improve a
  machine learning process. The meta data includes properties about the
  algorithm used, learning task itself etc. Using the meta data, one can
  make a better decision of chosen learning algorithm(s) to solve the
  problem more efficiently.

and

Transfer learning aims at improving the process of learning new tasks
  using the experience gained by solving predecessor problems which are
  somewhat similar. In practice, most of the time, machine learning
  models are designed to accomplish a single task. However, as humans,
  we make use of our past experience for not only repeating the same
  task in the future but learning completely new tasks, too. That is, if
  the new problem that we try to solve is similar to a few of our past
  experiences, it becomes easier for us. Thus, for the purpose of using
  the same learning approach in Machine Learning, transfer learning
  comprises methods to transfer past experience of one or more source
  tasks and makes use of it to boost learning in a related target task.

The comparisons still confuse me as both seem to share a lot of similarities in terms of reusability. Meta-learning is said to be ""model agnostic"", yet it uses metadata (hyperparameters or weights) from previously learned tasks. It goes the same with transfer learning, as it may reuse partially a trained network to solve related tasks. I understand that there is a lot more to discuss, but, broadly speaking, I do not see so much difference between the two.
People also use terms like ""meta-transfer learning"", which makes me think both types of learning have a strong connection with each other.
I also found a similar question, but the answers seem not to agree with each other. For example, some may say that multi-task learning is a sub-category of transfer learning, others may not think so.
","['machine-learning', 'deep-learning', 'comparison', 'transfer-learning', 'meta-learning']","
Meta-learning is more about speeding up and optimizing hyperparameters for networks that are not trained at all, whereas transfer learning uses a net that has already been trained for some task and reusing part or all of that network to train on a new task which is relatively similar. So, although they can both be used from task to task to a certain degree, they are completely different from one another in practice and application, one tries to optimize configurations for a model and the other simply reuses an already optimized model, or part of it at least.
"
CBIR and object detection,"
How does CBIR (content based image recognition) fit into the problem of object detection? Let's say we want to detect 4 types of dogs (Golden Retriever, Cocker Spaniel, Greyhound, and Labrador). We have an ""average"" model trained using YOLOv3.   So it might, for example, have a lot of false positives and false negatives. 
How could we use CBIR to improve the detections from this ""average' YOLOv3 model?
",['object-detection'],
Is the choice of the optimiser relevant when doing object detection?,"
Suppose that we have 4 types of dogs that we want to detect (Golden Retriever, Black Labrador, Cocker Spaniel, and Pit Bull). The training data consists of png images of a data set of dogs along with their annotations. We want to train a model using YOLOv3.
Does the choice of optimizer really matter in terms of training the model? Would the Adam optimizer be better than the Adadelta optimizer? Or would they all basically be the same?
Would some optimizers be better because they allow most of the weights to achieve their ""global"" minima?
","['object-detection', 'object-recognition', 'stochastic-gradient-descent', 'adam', 'adadelta']","
I have experimented with this to a small degree and have not noticed that much of an impact.
To date, Adam appears to give the best results on a variety of image data sets. I have found that ""adjusting"" the learning rate during training is an effective means of improving model performance and has more impact than the selection of the optimizer.
Keras has two callbacks that are useful for this purpose. Documentation is at https://keras.io/callbacks/. The ModelCheckpoint callback enables you to save the full model or just the model weights based on monitoring a metric. Typically, you monitor validation loss and set the parameter save_best_only=True to save the results for the lowest validation loss. The other useful callback is ReduceLROnPlateau, which allows you to adjust the learning rate based on monitoring a metric. Again, the metric usually monitored is the validation loss. If the loss fails to reduce after a user-set number of epochs (parameter patience), the learning rate will be adjusted by a user-set factor (parameter factor). You can think of the training process as traveling down a valley. As you near the bottom of the valley, it becomes more and more narrow. If your learning rate does not adjust to the ""narrowness"" there is no way you will get to the bottom of the valley.
You can also write a custom callback to adjust the learning rate. I have done this and created one which first adjusts the learning rate based on monitoring the training loss until the training accuracy reaches 95%. Then it switches to adjust the learning rate based on monitoring the validation loss. It saves the model weights for the lowest validation loss and loads the model with these weights to make predictions. I have found this approach leads to faster training and higher accuracy.
The fact is you can't tell if your model has converged on a global minimum or a local minimum. This is evidenced by the fact that, unless you take special efforts to inhibit randomization, you can get different results each time you run your model. The loss can be envisioned as a surface in $N$ space, where $N$ is the number of trainable parameters. Lord knows what that surface is like and where your initial parameter weights put you on that surface, plus how other random processes cause you to traverse that surface.
As an example, I ran a model at least 20 times and got resultant losses that were very close to each there. Then I ran it again and got far better results for exactly the same data.
"
Is there a way to ensure that my model is able to recognize an unseen example?,"
My question is more theoretical than practical. Let's say that I am training my cat classifier with a dataset that I feel is pretty representative of cat images in general. But then a new breed of cat is created that is distinct from other cats and it does not exist in my dataset.
My question is: is there a way to ensure that my model is still able to recognize this unseen breed, even though I didn't know it would come into existence when I originally trained my model?
I have been trying to answer this question by intentionally designing my validation and test sets such that they contain examples that are quite distantly related to those that exist in the training set (think of it like intentionally leaving out specific breeds of cats from the training set).
The results are interesting. For example, slight changes to parameters can dramatically change performance on the distantly related test examples, while not changing performance very much for the more closely related examples. I was wondering if anyone has done a deeper analysis of this phenomenon.
","['neural-networks', 'machine-learning', 'overfitting', 'regularization', 'generalization']","
The comments already are giving you some good tips about how to improve what your model recognizes, but I think your question goes above that asking if there's a way to ensure that it will always recognize the cats.
The short answer is ""no"".
The slightly longer answer is ""yes, but cheating"".
Regardless, there are a lot of steps you might take to improve the generalization aspect of your model.

Long answer:
A drama: cat classification in three acts
Act I: Cat as texts
Let's start with an example. Say that your model is trained with these inputs, and learns to correctly recognize them as a cat or not a cat:
cat → yes!
Cat → yes!
ferret → no
cat. → yes!
Cat! → yes!
Three MC's and one DJ → no

Your goal is to train your model so that every new variation, even unseen ones, will correctly be identified.
With a good level of generalization, your model will correctly classify new inputs that it has never seen before:
skunk → no
cat? → yes!
dog → no
CAT → yes

With this scenario, let's say the model now finds this:
kat → ?

Is that a ""cat"" misspelled? Is that short for Katherine? What should the model do?
Act II: But surely this doesn't happen in real life
Leaving the analogy for a bit, will your model that's looking at domestic cats properly accommodate for Savannah Cats, or will it consider them out? (They kind of look like cheetahs.) What about Sphinx cats? (They look like raw chicken to me.) Elf cats? (They look like bats.) This is just an example, but you can probably figure out more.
And the reason behind this problem is that the distinction itself between different classifications (in real life) is not binary, but rather a transition between ""yes, that's a textbook cat"" and ""that's a chair"". Your model will output binary decisions (maybe accompanied by a confidence interval, but even with it, you'll make the call into deciding if it's a cat or not).
Setting specific boundaries will help. You can define that your model will only detect domestic cats, maybe no bigger than a certain size, only of certain colors, etc... This is limiting what the model will correctly recognize as a cat when we (humans) might disagree. For instance, I would still argue that flourescent cats are still cats.
Going back to the simple text analogy, this is similar to deciding that to be detected as a cat, it has to start with a ""c"". So now you've discarded ¡Cat!.
In this way, it's not possible to ensure (notice the word) that your model will detect all of these unknown variations. There will be always some room for error that needs to be accepted, as long as the errors are infrequent or rare enough that they can be accepted as a regular part of the model.
Act III: Concept drift, a cautionary tale
Finally, the problem becomes even harder as we might be dealing with concepts that change over time, outside of the knowledge of the model, and outside of the knowledge of the person that supervised the model learning.
As and the breeds of cat changes, your model will have to accommodate by what we (users of the model) consider a valid definition of cat. Which might change in really unexpected ways and not really ""look"" like a cat. And since your model can only learn from what ""looks"" like a cat, it's always held in a disadvantaged position.
This will happen with almost any machine learning model that is approximating a result, regardless of the technique/algorithm. Approximations include a level of error because reality is usually complex in ways that we either don't know about or that are too computationally expensive.
"
How to pad sequences during training for an encoder decoder model,"
I've got an encoder-decoder model for character level English language spelling correction, it is pretty basic stuff with a two LSTM encoder and another LSTM decoder. 
However, up until now, I have been pre-padding the input sequences, like below:
abc  -> -abc
defg -> defg
ad   -> --ad

And next I have been splitting the data into several groups with the same output length, e.g.
train_data = {'15': [...], '16': [...], ...}

where the key is the length of the output data and I have been training the model once for each length in a loop.
However, there has to be a better way to do this, such as padding after the EOS character etc. But if this is the case, how would I change the loss function so that this padding isn't counted into the loss?
","['neural-networks', 'recurrent-neural-networks', 'long-short-term-memory', 'sequence-modeling', 'language-model']",
Are there any rules of thumb for having some idea of what capacity a neural network needs to have for a given problem?,"
To give an example. Let's just consider the MNIST dataset of handwritten digits. Here are some things which might have an impact on the optimum model capacity:

There are 10 output classes
The inputs are 28x28 grayscale pixels (I think this indirectly affects the model capacity. eg: if the inputs were 5x5 pixels, there wouldn't be much room for varying the way an 8 looks)

So, is there any way of knowing what the model capacity ought to be? Even if it's not exact? Even if it's a qualitative understanding of the type ""if X goes up, then Y goes down""?
Just to accentuate what I mean when I say ""not exact"": I can already tell that a 100 variable model won't solve MNIST, so at least I have a lower bound. I'm also pretty sure that a 1,000,000,000 variable model is way more than needed. Of course, knowing a smaller range than that would be much more useful!
","['neural-networks', 'computational-learning-theory', 'regularization', 'vc-dimension', 'capacity']","
This may sound counter intuitive but one of the biggest rules of thumb for model capacity in deep learning:
IT SHOULD OVERFIT. 
Once you get a model to overfit, its easier to experiment with regularizations, module replacements, etc. But in general, it gives you a good starting ground.
"
Recurrent neural Network for survival analyses: Dealing with forecast data as feature which can exceed the number of days untill a event occurs,"
I am building a Recurrent Neural network (LSTM) for predicting the number of days until a Pollen season starts (when the cumulative of the year exceeds X). One of the features I am including in my model is the weather forecast. 
However, I do not feel confident about the way I defined the model while including this weather forecast; currently, the weather forecast of 7 days is included as one of the predictors, However, when the label (number of days until the season starts) is smaller than the forecast I am training te model on forecast data which is completely irrelevant in determining the start of the season (e.g. if the season starts in 2 days and I am including the forecast of 7 days as predictor I am also training the model on the 5 days after the season already started while these are completely irrelevant).
My feeling is that it is not right when training RNN's for survival analyses. Does anyone know a way to deal with this? Or have an example where someone dealt with a similar issue?
Thanks a lot!
","['recurrent-neural-networks', 'long-short-term-memory', 'forecasting', 'survival']",
Does TD(0) prediction require Robbins-Monro conditions to converge to the value function?,"
Does the learning rate parameter $\alpha$ require the Robbins-Monro conditions below for the TD(0) algorithm to converge to the true value function of a policy?
$$\sum \alpha_t =\infty \quad \text{and}\quad \sum \alpha^{2}_t <\infty$$
","['reinforcement-learning', 'math', 'proofs', 'convergence', 'temporal-difference-methods']","
The paper Convergence of Q-learning: A Simple Proof (by Francisco S. Melo) shows (theorem 1) that Q-learning, a TD(0) algorithm, converges with probability 1 to the optimal Q-function as long as the Robbins-Monro conditions, for all combinations of states and actions, are satisfied. In other words, the Robbins-Monro conditions are sufficient for Q-learning to converge to the optimal Q-function in the case of a finite MDP. The proof of theorem 1 uses another theorem from stochastic approximation (theorem 2).
You are interested in the prediction problem, that is, the problem of predicting the expected return (i.e. a value function) from a fixed policy. However, Q-learning is also a control algorithm, given that it can find the optimal policy from the corresponding learned Q-function.
See also the question Why doesn't Q-learning converge when using function approximation?.
"
Predicting population density from satellite imagery,"
I have very high resolution images from LANDSAT 8 (5 out of 12 bands), which are of various administrative regions of a country. Each image is of variable dimensions, but generally of the order of [1500 X 1200 X 5].
My aim is to predict the population density from urban features visible on the images.
Since the number of images (and hence data points) is small, what is the best implementation strategy to build a model that can predict a value for population density based on these images?
","['prediction', 'regression']",
Function to update weights in back-propagation,"
I am trying to wrap my head around how weights get updated during back propagation. I've been going through a school book and I have the following setup for an ANN with 1 hidden layer, a couple of inputs and a single output. 

The first line gives the error that will be used to update the weights going from the hidden layer to the output layer. $t$ represents the target output, $a$ represents the activation and the formula is using the derivative for the sigmoid function $(a(1-a))$. Then, the weights are updated with the learning rate, multiplied by the error and the activation of the given neuron which uses the weight $w_h$. Then, the next step is moving on to calculate the error with respect to the input going into the hidden layer from the input layer (sigmoid is the activation function on both the hidden and the output layer for this purpose). So we have the total error * derivative of the activation for the hidden layer * the weight for the hidden layer.
I am following this train of thought as it was provided, but my question is — if the activation is changed to $tanh$ for example and the derivative of $tanh$ is $1-f(x)^2$, then would we have the error formula update to $(t-a)*(1-a^2)$ where $a$ represents the activation function so $1-a^2$ is the derivative of $tanh$?
","['backpropagation', 'weights']",
Training an unsupervised convolutional neural network to learn a general representation of a Lua module,"
I am trying to train a CNN in keras to learn a general representation of a Lua module, e.g. requires at the beginning, local variables, local functions, interface (returns) and in between some runnable code (labeled ""other""). So for each module (source code) I generate an AST which I then encode in a json file. The file contains the order of the node in the AST, the text it represents and the type of node it is (require, variable, function, interface, other). It can contain other metrics but so far I have settled on these three, where only the order and type of node will be converted into a vector to serve as input to the CNN. Now I don't have any labels for these modules (I want to treat one module as one input to the CNN), so I have concluded that I need to use unsupervised learning. In keras this should translate to using autoencoders, where I use the encoder part to learn weights for the representation and then connect a fully-connected layer and generate an output. Now before I specify the output, I want to specify the input more closely. In my mind It should be a 3D vector let's say (x,y,z). x represents the number of nodes of an AST that are taken into consideration, y represents the local neighborhood of said node (for now I have settled on 5 nodes) and z should represent the node itself, the order and type of node. So with that, I would want the output of the network to be in the (almost) same dimension. I want x outputs for every node that was taken as input and a number (ideally between 0-1) to specify how ""correct"" the node under consideration is in response to the learned representation. My question as a beginner to neural networks is, how feasible is this and are there any points which are simply impossible to do or are wrongly interpreted on my part?
","['convolutional-neural-networks', 'tensorflow', 'keras', 'unsupervised-learning']",
How to choose the activation function in neuroevolution?,"
I am developing a NEAT flappy bird game, and it doesn't work, the system stays stupid for 300 generations. I chose tanh() for activation, just because it's included in JS.
I can't find a good discussion on the internet of activation functions in the context of neuroevolution, most of what I see is about derivative and other gradient descent issues which I suspect are irrelevant to forward only networks.
If you need a fixed point to answer, I have 8 inputs, one output and the problem is a classification (""jump"", ""don't jump""). But please explain your answer. I currently use tanh() for all the hidden and output nodes, and the output is considered ""jump"" if the output neuron value is >0.85
For some context, the code is here: https://github.com/nraynaud/nraygame and the game here: https://nraynaud.github.io/nraygame/
","['neat', 'activation-functions', 'neuroevolution']","
When it comes to genetic algorithms (neuroevolution), you can pretty much evolve any kind of parameters. You just need to have a fairly complex system where each parameter changes the way the inputs are affecting the outputs.
So, to answer your question, any activation function should work! It would be surprising that the activation function is your problem. But if you want to test some activation functions, here's a list of activation functions. But tanh should work perfectly fine.
As I already mentioned in another answer, the key to have a great neuroevolution algorithm is the fitness function. Your rewards should reflect the goal you're aiming for. You didn't mentioned what fitness function you used, but here's what I suggest: reward a player ONLY if he progresses in the game. For example, your fitness function could simply be the distance traveled by your birds. This could maybe solve your problem...
"
What kind of optimizer is suggested to use for binary classification of similar images?,"
I have spent some time searching Google and wasn't able to find out what kind of optimization algorithm is best for binary classification when images are similar to one another. 
I'd like to read some theoretical proofs (if any) to convince myself that particular optimization has better results over the rest. 
And, similarly, what kind of optimizer is better for binary classification when images are very different from each other?
","['deep-learning', 'classification', 'optimization', 'hyperparameter-optimization', 'optimizers']","
I have consistently found Adam to work very well but to tell you the truth I have not seen all that much difference in performance based on the optimizer. Other factor seem to have much more influence on the final model performance.In particular adjusting the learning rate during training can be very effective. Also saving the weights for the lowest validation loss and loading the model with those weights to make predictions works very well. Keras provides two callbacks that help you achieve this. Documentation is at 
https://keras.io/callbacks/. The ReduceLROnPlateau callback allows you to adjust the learning rate based on monitoring a metric. Typically validation loss is monitored. If the loss fails to reduce after N consecutive epochs(parameter patience) the learning rate is adjusted by a factor(parameter factor). You can think of training as descending into a valley which gets more and more narrow as you approach the bottom. If the learning rate does not adjust to this ""narrowness"" there is no way you will get to the very bottom.
The other callback is ModelCheckpoint. This allows you to save the model( or just the weights) based on monitoring of a metric. Again usually validation loss is monitored and the parameter save_best_only is set to true. This saves the model with the lowest validation loss. That model can than be used to make predictions.
"
How can I perform lossless compression of images so that they can be stored to train a CNN?,"
I have a set of images, which are quite large in size (1000x1000), and as such do not easily fit into memory. I'd like to compress these images, such that little information is missing. I am looking to use a CNN for a reinforcement learning task which involves a lot of very small objects which may disappear when downsampling. What is the best approach to handle this without downscaling/downsampling the image and losing information for CNNs?
","['deep-learning', 'reinforcement-learning', 'convolutional-neural-networks', 'tensorflow', 'numpy']","
Your input image size and memory are not directly related. While using CNN's, there are multiple hyperparameters that effect the video memory(if you are using GPU) or physical memory(if you are using CPU). All the frameworks these days uses a simplified data-loaders, for instance in Tensorflow or PyTorch, you are required to write a data-loader that takes in multiple hyper-parameters that are mentioned below and fit the data into VRAM/RAM, and this is strictly dependent upon you batch size - memory occupied on VRAM has direct relation to the batch size. 
Whatever may be your image size, while you are writing the data-loader you have to mention the transformation parameters to your data-loader, during the training phase the data-loader will automatically load required images into your memory according to the batch size you have mentioned. As you have mentioned about image compression, this is an irrelevant parameter at-least for most of the generic use-cases, the most relevant hyperparameters are 

Scaling 
Cropping 
Random flip 
Normalization of the RGB values
ColorJitter
Padding
RandomAffine

And many more.
PyTorch provides really good transformers in data-loader, please do check https://pytorch.org/docs/stable/torchvision/transforms.html.
For Tensorflow, have a look at https://keras.io/preprocessing/image/.
"
What event would confirm that we have implemented an AGI system?,"
I was listening to a podcast on the topic of AGI and a guest made an argument that if strong music generation were to happen, it would be a sign of ""true"" intelligence in machines because of how much creative capability creating music requires (even for humans).
It got me wondering, what other events/milestones would convince someone, who is more involved in the field than myself, that we might have implemented an AGI (or a ""highly intelligent"" system)?
Of course, the answer to this question depends on the definition of AGI, but you can choose a sensible definition of AGI in order to answer this question.
So, for example, maybe some of these milestones or events could be:

General conversation
Full self-driving car (no human intervention)
Music generation
Something similar to AlphaGo
High-level reading/comprehension

What particular event would convince you that we've reached a high level of intelligence in machines?
It does not have to be any of the events I listed.
","['philosophy', 'agi', 'intelligence-testing']","
It is a difficult question to answer, as — for a start — we still don't really know what 'intelligence' means. It's a bit like Supreme Court Justice Potter Stewart declining to define 'pornography', instead stating that [...]I know it when I see it. AGI will be the same.
There is no single event (almost by definition), as that's not general. OK, we've got machines that can beat the best human players at chess and go, two games that were for centuries seen as an indication of intelligence. But can they order a takeaway pizza? Do they even understand what they are doing? Or, even more fundamental, know what they means in the previous sentence?
In order for a machine to show a non-trivial level of intelligent behaviour, I would expect it to interact with its environment (which is more social intelligence, an aspect that seems to be rather overlooked in much of AI). I would expect it to be aware of what it's doing/saying. If I have a conversation with a chatbot that really understands what it's saying (and can explain why it came to certain conclusions), that would be an indication that we're getting closer to AGI. So Turing wasn't that far off, though nowadays it's more achieved with smoke and mirrors rather than 'real' intelligence.
Understanding a story: being able to finish a partial story in a sensible way, inferring and extrapolating the motives of characters, being able to say why a character acted in a particular way. That for me would be a better sign of AGI than beating someone at chess or solving complex equations. Jokes that are funny; breaking rules in story-telling in a sensible way.
Writing stories: NaNoGenMo is a great idea, and throws up lots of really creative stuff, but how many of the resulting novels would you want to read instead of human-authored books? Once that process has generated a best-seller (based on the quality of the story), then we might be getting closer to AGI.
Composing music: of course you can already generate decent music using ML algorithms. Similar to stories, the hard bit is the intention behind choices. If choices are random (or based on learnt probabilities), that is purely imitation. An AGI should be able to do more than that. Give it a libretto and ask it to compose an opera around it. Do this 100 times, and when more than 70-80 of the resulting operas are actually decent pieces of music that one would want to listen to, then great.
Self-driving cars? That's not really any more intelligent (but a lot sexier!) than to walk around in a crowd without bumping into people and not getting run over by a bus. In my view it's much more a sign of intelligence if you can translate literature into a foreign language and the people reading it actually end up enjoying it (instead of wondering who translated that garbage).
One aspect we need to be aware of is anthropomorphising. Weizenbaum's ELIZA was taken for more than it was, because its users tried to make sense of the conversations they had and built up a mental model of Eliza, which clearly wasn't there on the other side of the screen. I would want to see some real evidence of intentionality of what an AGI was doing, rather than ascribing intelligence to it because it acts in a way that I'm able to interpret.
"
Drone Deployment Platform for Neural Networks,"
Good day everyone,
I would just like to ask if anyone part of a lab or company doing research on aerial robotics has any suggestions of a good platform for deploying computer vision algorithms for aerial robots?
Currently our lab has a set of DJI Matrice drones but are too heavy for our liking. We really wanted to use the Skydio R2 drone for out future research projects but found out later that the SDK does not allow access for implementing our own Deep Learning or Reinforcement Learning networks (only allows the user to code their own preset movements in python). We also took a look at the Parrot AR drones but found that they were discontinued and do not have the computing power that the Skydio has although as an alternative, it does have the capability to stream the video feed to an external computer for online processing instead of on-board.
I suggested that we stick with the DJI Matrice and just use an Nvidia Jetson for deployment but I am still curious to know if anyone knows of other available platforms. Does anyone know of a more compact platform available for research purposes?
Thank you :)
","['deep-learning', 'reinforcement-learning', 'robotics']",
Is the negative of the policy loss function in a simple policy gradient algorithm an estimator of expected returns?,"
Let
$$
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t = 0}^T \nabla_\theta \log \pi_\theta (a_t|s_t) R(\tau) \right]
$$
be the expanded expression for a simple policy gradient, where $\theta$ are the parameters of the policy $\pi$, $J$ denotes the expected return function, $\tau$ is a trajectory of states and actions, $t$ is a timestep index, and $R$ gives the sum of rewards for a trajectory.
Let $\mathcal{D}$ be the set of all trajectories used for training. An estimator of the above policy gradient is given by
$$
\hat{g} = \frac{1}{\mathcal{D}} \sum_{\tau \in \mathcal{D}} \sum_{t = 0}^T \nabla_\theta \log \pi_\theta (a_t|s_t) R(\tau).
$$
A loss function associated with this estimator, given a single trajectory with $T$ timesteps, is given by
$$
L(\tau) = -\sum_{t = 0}^T \log \pi_\theta (a_t|s_t) R(\tau).
$$
Minimizing $L(\tau)$ by SGD or a similar algorithm will result in a working policy gradient implementation.
My question is what is the proper terminology for this loss function? Is it an (unbiased?) estimator for the expected returns $J(\pi_\theta)$ if summed over all trajectories? If someone is able to provide a proof that minimizing $L$ maximizes $J(\pi_\theta)$, or point me to a reference for this, that would be greatly appreciated.
","['reinforcement-learning', 'policy-gradients']",
What is the difference between LSTM and RNN?,"
What is the difference between LSTM and RNN? I know that RNN is a layer used in neural networks, but what exactly is an LSTM? Is it also a layer with the same characteristics?
","['neural-networks', 'comparison', 'recurrent-neural-networks', 'long-short-term-memory', 'recurrent-layers']",
How does adding noise to the action in DDPG help in learning?,"
I can't understand how playing with the action generated by the actor network in DDPG by adding the noise term helps in exploration.
","['deep-learning', 'reinforcement-learning', 'deep-rl', 'ddpg']",
Suggestion on image inpainting algorithm,"
Currently, many algorithms are available for image inpainting. In my application, I have some special restriction on training dataset-

Let's consider the training dataset of human facial images.
Although all human face has the same general structure, they may have subtle differences depending on racial characteristics.
Consider that in the training dataset, we have ten facial images from each race.

Now, in my learning algorithm, can we come up with a two-step method? Wherein the first step, we will learn about the general facial structure more accurately using all training data. In the next step, we will learn those subtle features of each race by only learning ten images associated with that race? It might restore a distorted image more accurately.
Suppose we have a distorted facial image of a person from race 'A,' where the nasal area of that image is lost. Now with the first step, we can learn the nasal structure more accurately by using all of the training data, and in the second step using only the ten images associated with race 'A,' we can fine-tune those generated data. As we have only 10 data with race 'A,' if we use only those small subset data to learn the whole model, then probably we will not be able to capture the all general architecture of the face in the first place.
P.S. I am not from Computer Science/ML background, probably my problem description is a little bit vague. It would be great if someone provides an edit/tag suggestion.
","['generative-model', 'image-generation']",
Are connections genes in a genome ever deleted or just disabled?,"
When a new node is added, the previous connection is disabled and not removed.

Is there any situation in which a connection gene is removed? For example, in the above diagram connection gene with innovation number 2 is not present. It could be because some other genome used that innovation number for a different connection that isn't present in this genome. But are there cases where a connection gene has to be removed?
","['neural-networks', 'deep-learning', 'genetic-algorithms', 'neat', 'neuroevolution']","
The original paper never goes further than disabling the link, but I have seen implementations on github that did have a probability of deleting the link.
python-neat is such an eaxample.
"
OpenAI spinning up convolutional networks with PPO [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am using pytorch version of PPO and I have image input that I need to process with convolutional neural networks, are there any examples on how to set up the network? I know that stable baselines  support this to some extend, but I had better performance with spinning up so I would prefer to keep using these. 
","['reinforcement-learning', 'open-ai']","
I have written a tutorial on using OpenAI Spinning Up in a image-based PyBullet + Gym environment here
In order to be able to use spinup for an image-based environment I had to fork it here and add CNN to PPO's core.py
"
Is there a simple proof of the convergence of TD(0)?,"
Does anybody know a simple proof of the convergence of the TD(0) value function prediction algorithm?

","['reinforcement-learning', 'reference-request', 'proofs', 'convergence', 'temporal-difference-methods']","
As far as I know, there is no very simple proof of the convergence of temporal-difference algorithms. The proofs of convergence of TD algorithms are often based on stochastic approximation theory (given that e.g. Q-learning can be viewed as a stochastic process) and the work by Robbins and Monro (in fact, the Robbins-Monro conditions are usually assumed in the theorems and proofs).
The proofs of convergence of Q-learning (a TD(0) algorithm) and SARSA (another TD(0) algorithm), when the value functions are represented in tabular form (as opposed to being approximated with e.g. a neural network), can be found in different research papers.
For example, the proof of convergence of tabular Q-learning can be found in the paper Convergence of Stochastic Iterative Dynamic Programming Algorithms (1994) by Tommi Jaakkola et al. The proof of convergence of tabular SARSA can be found in the paper Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms (2000) by Satinder Singh et al.
See also How to show temporal difference methods converge to MLE?.
"
Language Learning feedback with AI,"
Is there a program under development that uses AI technology, like Siri, to ""hold hands"" so to speak with a language learner and coach them on accent, colloqiual expressions, or to let them guide the language learning process using an archive of language knowledge? 
Also, could this sort of program be used to learn things in a language one already knows, or in a new language, say for the purposes of travel or to learn about related hyperlinks in an online database?
","['ai-design', 'activation-functions', 'data-preprocessing', 'text-classification', 'knowledge-base']",
What is the best way to detect and recognize traffic signs in a picture?,"
I'm working on a project for my college to recognize traffic signs in pictures. I searched a lot but can't find the best method to do it.
Can someone recommend me a paper, article, or even GitHub link that describes the best way to achieve this? It will be helpful.
","['deep-learning', 'object-recognition', 'object-detection']","
Here are some articles, the first three include code:

Deep Learning for Traffic Signs Recognition, by Moataz Elmasry, April 2, 2018
Traffic Sign Classification with Keras and Deep Learning, by Adrian Rosebrock, November 4, 2019
Traffic Sign Detection using Convolutional Neural Network, by Sanket Doshi, September 1, 2019
AI to manage road infrastructure via Google Street View, June 19, 2019

"
"When stacking LSTM's, should the hidden units increase?","
I'm using Weights and Biases to do some hyperparameter sweeping for a supervised sequence-to-sequence problem I'm working on. One thing I noticed is that the sweeps with a gradually increasing number of hidden units tend to have a lower validation loss:

I'm wondering if this is generally true or just a function of my particular problem?
","['recurrent-neural-networks', 'long-short-term-memory', 'hyperparameter-optimization']",
How is the receptive field of a CNN affected by transposed convolution?,"
When computing receptive field recursively through a CNN, does a transposed convolution affect the receptive field the same way that a convolution does if the kernel and stride is the same?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks']",
Using tensor networks as machine learning models,"
Tensor networks (check this paper for a review) are a numerical method originally introduced in condensed matter physics to model complex quantum systems. Roughly speaking, such systems are described by a very high-dimensional tensor (where the indices take a number of values scaling exponentially with the number of system constituents) and tensor networks provide an efficient representation of the latter as an outer product and contraction of many low-dimensional tensors.
More recently, a specific kind of tensor network (called Matrix Product State in physics) found interesting applications in machine-learning through the so-called Tensor-Train decomposition (I do not know of a precise canonical reference in this context, so I will abstain from citing anything).
Now, over the last few years, several works from the physics community seemed to push for a generalized use of tensor networks in machine learning (see this paper, a second one and a third one and this article from Google AI for context). As a physicist, I am glad to learn that tools initially devised for physics may find interdisciplinary applications. However, at the same time, my critical mind tells me that from the machine learning research community's perspective, these results may not look that intriguing. After all, machine learning is now a very established field and it takes probably more than a suggestion for a new machine learning model and a basic benchmarking on a trivial dataset (as the MNIST one) -which is what the papers essentially do in my humble opinion- to attract any attention in the area. Besides, as I believe to know, there already exists quite a solid body of knowledge on tensor analysis techniques for machine learning (e.g.  tensor decompositions), which may cast doubt on the originality of the contribution.
I would therefore be very curious to have the opinion of machine learning experts on this line of research: is it really an interesting direction to look into, or is it just about surfing on the current machine learning hype with a not-so-serious proposal?
","['neural-networks', 'machine-learning']","
I do not feel comfortable proclaiming that I would be a machine learning expert.
But I want to point out that there is indeed interest in applying tensor networks in machine learning settings.
Let me highlight three particular settings in the following.
They can be used to find the governing equations behind dynamical systems analogous to the SINDy algorithm.
The reasoning is that the dynamics of your system may not be sparse but may indeed have a low-rank structure. (See this paper for a comparison of sparse and low-rank structures and this paper for a discussion about why low-rank matrices so often appear in big data.)
The resulting modification of SINDy is the appropriately named MANDy algorithm.
But if your dynamical law exhibits additional structure (despite being low-rank) then this structure is not used. (Requiring you to collect unnecessary samples.)
This work tackles this issue by allowing you to specify the structure that you expect your law to exhibit.
Another area of interest is the application of tensor networks as a means of compression.
Tensor networks can be used to approximate multivariate functions.
Either directly (see this article or this article for further details) or by representing their high-dimensional coefficient tensor (see e.g. here).
The last paper also highlights where such functions may occur: when solving stochastic or parametric PDEs.
""What does this have to do with machine learning?"" you may rightfully ask.
These PDEs are hard to solve and it is often easier to minimize a residual than to apply a Galerkin method. The results can be shown to be equivalent with high probability. (See this paper for a proof of this statement and an application to some stochastic PDEs or this paper for an application to an optimal control problem.)
Moreover, there is the problem of tensor completion - a generalization of matrix completion to higher dimensions. (This and this are references for algorithms.)
These methods find application in data science where they can be used to decompose or denoise data. (Unfortunately, I don't have a reference with examples but I remember a presentation where these methods where used on EEG signals.)
Additionally, I recently found this interesting symposium covering even more potential applications of tensor networks in machine learning:
https://itsatcuny.org/calendar/quantum-inspired-machine-learning
"
What is the difference between an generalised estimating equation and a recurrent neural network?,"
What is the difference between a generalised estimating equation (GEE) model and a recurrent neural network (RNN) model, in terms of what these two models are doing? Apart from the differences in the structure of these two models, where GEE is an extension of generalised linear model (GLM) and RNN is of a neural network structure, it seems to me that these 2 models are doing the same thing?
","['neural-networks', 'comparison', 'recurrent-neural-networks', 'linear-regression', 'statistical-ai']",
Can the degree and minimum remaining values heuristics be used in conjunction?,"
I am currently studying constraint satisfaction problems and have come across two heuristics for variable selection. The minimum remaining values(MRV) heuristic and the degree heuristic. 
The MRV heuristic tells you to choose a variable that has the least legal assignments, while the degree heuristic tells you to choose a variable that has the biggest effect on the remaining unassigned variables. 
Can these 2 heuristics for variable selection be used in conjunction with each other? Some books say that degree heuristic can be used as first selection of variables. Which heuristic is better followed after that? The MRV or degree heuristic? 

The picture shows the case where degree heuristic is used. If MRV were to be used, then at the 3rd step, the leftmost side of the map would be coloured blue.
","['heuristics', 'graphs', 'constraint-satisfaction-problems']",
"In the policy gradient equation, is $\pi(a_{t} | s_{t}, \theta)$ a distribution or a function?","
I am learning about policy gradient methods from the Deep RL Bootcamp by Peter Abbeel and I am a bit stumbled by the math presented. In the lecture, he derives the gradient logarithm likelihood of a trajectory to be
$$\nabla log P(\tau^{i};\theta) = \Sigma_{t=0}\nabla_{\theta}log\pi(a_{t}|s_t, \theta).$$
Is $\pi(a_{t} | s_{t}, \theta)$ a distribution or a function? Because a derivative can only be taken wrt a function. My understanding is that $\pi(a_{t},s_{t}, \theta)$ is usually represented as a distribution of actions over states, since input of a neural network for policy gradient would be the $s_t$ and output would be $\pi(a_t, s_t)$, using model weights $\theta$.
","['math', 'deep-rl', 'policy-gradients', 'stochastic-policy']",
Why am I getting a difference between training accuracy and accuracy calculated with Keras' predict_classes on a subset of the training data?,"
I'm trying to solve a binary classification problem with AlexNet. I split the original dataset into training and validation datasets using a 70/30 ratio. I have trained my neural network with a dataset of 11200 images, and I obtained a training accuracy of 99% and a validation accuracy was 96%. At the end of the training, I saved my model's weights to a file.
After training, I loaded the saved weights to the same neural network. I chose 738 images out of the 11200 training images, and I tried to predict the class of each of them with my model, and compare them with true labels, then again I calculated the accuracy percentage and it was 74%.
What is the problem here? I guess its accuracy should be about 96% again.
Here's the code that I'm using.
prelist=[]
for i in range(len(x)):
    prediction = model.predict_classes(x[i])
    prelist.append(prediction)
count = 0
for i in range(len(x)):
    if(y[i] == prelist[i]):
        count = count + 1
test_precision = (count/len(x))*100
print (test_precision)

When I use predict_classes on 11200 images that I used to train the neural network and compare its result with true labels and calculated accuracy again its accuracy is 91%.
","['neural-networks', 'training', 'accuracy', 'binary-classification', 'alexnet']","
One problem could be with the selection of the validation set. For your model to work well on data it has not seen as training data is to have a high validation accuracy, but that is not sufficient on its own. The validation set must be large enough and varied enough that its probability distribution is an accurate representation of the probability distribution of all the images. You could have 1000 validation images, but, if they are similar to each other, they would be an inadequate representation of the probability distribution. Therefore, when you run your trained model to make predictions on the test set its performance would be poor.
So the question is: how many validation images did you use and how were they selected (randomly or handpicked)?
Try increasing the number of validation images and use one of the available methods to randomly select images from the training set, remove them from the training set, and use them as validation images.
Keras's flow_from_directory can achieve that or sklearn's train_test_split. I usually have the validation set be selected randomly and have at least 10% as many images as the test set does.
Overtraining is a possibility, but I think unlikely given your validation accuracy is high.
Another thing is how were the test set images selected? Maybe their distribution is skewed. Again, the best thing to do is to select these randomly.
What was your training accuracy? Without a high training accuracy, the validation accuracy may be a meaningless value. Training accuracy should be in the upper 90's for the validation accuracy to be really meaningful.
Finally, is there any possibility your test images were mislabeled? Try switching your test set as the validation set and see what validation accuracy you get.
Here is an example of what I mean. A guy built a CNN to operate on a set of images separated into two classes. One class as ""dogs"", the other class was ""wolves"". He trained the network with great results almost 99.99% training accuracy and 99.6% validation accuracy. When he ran it on the test set, his accuracy was about 50%. Why? Well, it turns out all the images of wolves were taken with snow in the background. None of the images of dogs had snow in the background. So, the neural network figures out if snow must be a wolf, if no snow must be a dog. However, in his test set, he had a mixture of wolves in or not in snow, dogs in or not in snow. Great training and validation results but totally useless performance.
"
What AI applications exist to solve sustainability issues?,"
The Sustainable Development Goals of the United Nations describe a normative framework which states what future development until 2030 should strive for. On a more abstract level a basic definition describes sustainable development as

development that meets the needs of the present without compromising the ability of future generations to meet their own needs.

Alone through the consumption of energy, AI technologies already have a (negative) impact on sustainability questions.
What AI applications already exist, are researched or are at least thinkable from which sustainability would benefit?
","['reference-request', 'social', 'ethics', 'green-ai']","
The paper The role of artificial intelligence in achieving the Sustainable Development Goals (2020, published in Nature) should contain the information you're looking for. 
In the introduction, the authors write

Here we present and discuss implications of how AI can either enable or inhibit the delivery of all 17 goals and 169 targets recognized in the 2030 Agenda for Sustainable Development. Relationships were characterized by the methods reported at the end of this study, which can be summarized as a consensus-based expert elicitation process, informed by previous studies aimed at mapping SDGs interlinkages. A summary of the results is given in Fig. 1 and the Supplementary Data 1 provides a complete list of all the SDGs and targets, together with the detailed results from this work. 

For example, as stated in the supplementary data, goal 1 is

End poverty in all its forms everywhere

and the first target (1.1.) of goal 1 is 

By 2030, eradicate extreme poverty for all people everywhere, currently measured as people living on less than $1.25 a day 

Then the authors suggest that, according to their studies, AI may act as an inhibitor or enabler (i.e. it may be used to fight poverty) for this target.

We identified in the literature studies suggesting that AI may be an inhibitor for this target, due to the potential increase in inequalities which would hinder the achievement of this goal (1). Other references however identify AI as an enabler for this goal, in the context of using satellite data analysis to track areas of poverty and to foster international collaboration (2).

Therefore, techniques for satellite data analysis are one of the AI techniques that may be used to tackle suistainability issues.
"
Why is there more than one way of calculating the accuracy?,"
Some sources consider the true negatives (TN) when computing the accuracy, while some don't.
Source 1:
https://medium.com/greyatom/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b

Source 2:https://www.researchgate.net/profile/Mohammad_Sorower/publication/266888594_A_Literature_Survey_on_Algorithms_for_Multi-label_Learning/links/58d1864392851cf4f8f4b72a/A-Literature-Survey-on-Algorithms-for-Multi-label-Learning.pdf
 
which can be translated as 

which one of these must be considered for my multi-label model.
","['machine-learning', 'classification', 'accuracy', 'metric', 'multi-label-classification']",
Can neural network be trained to solve this problem?,"
I'm working on a problem that given a dataset; where each train example is a binary matrix $X_i$ with dimension $(N_i,D_i)$ (think a training example is a feature matrix) each entry is either 1 or 0.
Also, each training example $X_i$ has a corresponding label $Y_i$ is a correlation matrix dimension $(D_i,D_i)$
My goal is to construct a model that take the input $X_i$ and coutput $\hat{Y}_i$ that matches with $Y_i$.
The major challenges here is that; each training example $X_i$ can have different dimensionality $(N_i,D_i)$ different data points and different number of variables.
I'm wondering if there is any neural network architecture can handle case like this ?
","['neural-networks', 'deep-learning']",
Does a fully convolutional network share the same translation invariance properties we get from networks that use max-pooling?,"
Does a fully convolutional network share the same translation invariance properties we get from networks that use max-pooling?
If not, why do they perform as well as networks which use max-pooling? 
","['deep-learning', 'convolutional-neural-networks', 'pooling', 'fully-convolutional-networks', 'max-pooling']","
Neural networks are not invariant to translations, but equivariant,
Invariance vs Equivariance
Suppose we have input $x$ and the output $y=f(x)$ of some map between spaces $X$ and $Y$. We apply transformation $T$ in the input domain. For general map,output will change in some complicated and unpredictable way. However, for certain class of maps, change of the output becomes very tractable.
Invariance means that output doesn't change after application of the map $T$. Namely:
$$
f(T(x)) = f(x)
$$
For CNN example of the map, invariant to translations, is the GlobalPooling operation.
Equivariance means that symmetry transformation $T$ on the input domain leads to the symmetry transformation $T^{'}$ on the output. Here $T^{'}$ can be the same map $T$, identity map - which reduces to invariance, or some other kind of transformation.
This picture is illustration of translational equivariance.

Equivariance of operations in CNN

Convolutions with stride=1:
$$ f(T(x)) = T f(x)
$$
Output feature map is shifted in same direction and number of steps.
Downsampling operations. Convolutions with stride=1, Pooling (non-global):
$$ f(T_{1/s}(x)) = T_{1/s} f(x)
$$
They are equivariant to the subgroup of translations, which involves translations with integer number of strides.
GlobalPooling :
$$ f(T(x)) = f(x)
$$
These are invariant to arbitrary shifts, this property is useful in classification tasks.

Combination of layers
Stacking multiple equivariant layers you obtain equivariant architecture a whole.
For classification layer it makes sense to put GlobalPooling in the end in order to for NN to output the same probabilities for the shifted image.
For segmentation or detection problem architecture should be equivariant with the same map $T$, in order to translate bounding boxes or segmentation masks by the same amount as the transform on the input.
Non-global downsampling operations reduce equivariance to the subgroup with shifts integer multiples of stride.
"
Why isn't my implementation of DQN using TensorFlow on the FrozenWorld environment working?,"
I am trying to test DQN on FrozenWorld environment in gym using TensorFlow 2.x. The update rule is (off policy)
$$Q(s,a) \leftarrow Q(s,a)+\alpha (r+\gamma~ max_{a'}Q(s',a')-Q(s,a))$$ 
I am using an epsilon greedy policy. 
In this environment, we get a reward only if we succeed. So I explored with 100% until I have 50 successes. Then I saved the data of failures and success in different bins. Then I sampled (with replacement) from these bins and used them to train the Q network. However, no matter how long I train the agent doesn't seem to learn.
The code is available in Colab. I am doing this for a couple of days.
PS: I modified the code for SARSA and Expected SARSA; nothing works.
","['reinforcement-learning', 'tensorflow', 'dqn', 'gym', 'sarsa']","
I see at least 3 issues with your DQN code that need to be fixed:

You should not have separate replay memories for successes/failures. Put all of your experiences in one replay memory and sample from it uniformly.
Your replay memory is extremely small with only 2,000 samples. You need to make it significantly larger; try at least 100,000 up to 1,000,000 samples.
Your batch_target is incorrect. You need to train on returns and not just rewards. In your train function, compute the 1-step return $r + \gamma \cdot max_{a'} Q(s',a')$, remembering to set $max_{a'} Q(s',a') = 0$ if $s'$ is terminal, and then pass it to model.fit() as your prediction target.

"
Why is the loss of one of the outputs of a model with multiple outputs increasing while the others are decreasing?,"
I'm a newbie in neural networks. I'm trying to fit my neural network that has 3 different outputs: 

semantic segmentation, 
box mask and 
box coordinates.

When my model is training, the loss of semantic segmentation and box coordinates are decreasing, but the loss of the box mask is increasing too much. 
My neural network is a CNN and it's based on Chargrid from here. The architecture is this:

For semantic segmentation outputs, it's expected to have 15 classes, for box mask it's expected to have 2 classes (foreground vs background) and for box coordinates it's expected to have 5 classes (1 for each corner of bounding box + 1 for None). 
Loss details
Step 1
Here's the loss/accuracy for each of the three outputs at the end of the first step.

Semantic segmentation (Output 1) - 0.0181/0.958
Box mask (Output 2) - 13.88/0.946
Box coordinates (Output 3) - 0.2174/0.0000000867

Last step
Here's the loss/accuracy at the last step.             

Semantic segmentation (Output 1) - 0.0157/0.963
Box mask (Output 2) - 73.02/0.935
Box coordinates (Output 3) - 0.06/0.82


Is that normal? How can I interpret these results?
I will leave below, the output of the model fit.



","['machine-learning', 'convolutional-neural-networks', 'training', 'overfitting', 'loss']",
How do I optimize the number of filters in a convolution layer?,"
I’m trying to figure out how to write an optimal convolutional neural network with respect to maximizing and minimizing filters in a convolution 2D layer. This is my thinking and I’m not sure if it's correct.
If we have a dataset of 32x32 images, we could start with a Conv2D layer, filter of 3x3 and stride of 1x1. Therefore the maximum times this filter would be able to fit into the 32 x 32 images would be 30 times e.g. newImageX * newImageY
newImageX = (imageX – filterX + 1)  
newImageY = (imageY – filterY + 1)

Am I right in thinking that because there are only newImageX * newImageY patterns in the 32 x 32 image, that the maximum amount of filters should be newImageX * newImageY, and any more would be redundant?
So, the following code is the maximum possible filters given that we have 3x3 filter, 1x1 stride and 32x32 images?
Conv2D((30*30), kernel=(3,3), stride=(1,1), input_shape=(32,32,1))

Is there any reason to go above 30*30 filters, and are there any reasons to go below this number, assuming that kernel, stride and input_shape remain the same?
If you knew you were looking for one specific filter, would you have to use the maximum amount of filters to ensure that the one you were looking for was present, or can you include it another way?
","['machine-learning', 'convolutional-neural-networks', 'convolution']",
"Which generative methods are better for generating graphs, while preserving node and edge labels?","
I started to dig into the topic of graph generation and I have a question - which out of generative methods (autoregressive, variational autoencoders, GANs, any other?) are better for generating graphs while preserving both node and edge labels? (in other words, I want to generate graphs with node and edge labels, similar to what I have in my dataset)
I checked over some papers, but I didn't find exact clues about how to choose the method for graph generation. If anybody is more familiar with the topic, I'll appreciate any link to papers or any advice/recommendations.
","['machine-learning', 'generative-adversarial-networks', 'generative-model', 'geometric-deep-learning']",
"Are POS tagging, Chunking, Disambiguation, etc. subtasks of annotation?","
I wonder about the legitimacy of using the terms ""POS tagging"", ""Chunking"", ""Disambiguation"" and ""Categorization"" to describe an activity that doesn't include writing code and database queries, or interacting with the NLP algorithm and database directly.
More specifically, let's suppose I use the following tools:

an ""Annotator"" for analyzing the input text (e.g. sentences copypasted from online newspapers) and choose and save proper values as regards to ""POS"" of tokens and words, ""Words""(entities and collocations) and ""Chunk"". Tokens are already detected by default. I have to decide which words are entities and/or collocations or not and their typology, though. May the performed tasks be called ""POS tagging"", ""Chunking"" and ""support to categorization""?
A knowledge base, for searching and choosing the proper synsets of the lemmas and assigning them to the words analyzed in the previous Annotation tool. May such a task be called ""Disambiguation""?
A graphical user interface which shows how the NLP analyzes by default the input texts as regards to Lemmas, POS, Chunks, Senses, entities, domains, main concepts, dependency tree, in order to make analyses consistent with it.

If I want to define these activities in a few words, ""Machine Learning annotation"" may be the most correct. 
But what if I want to be more specific? I don't know whether or not the terms ""POS tagging"", ""Chunking"", ""Disambiguation"" and ""Support to categorization"" may be appropriate for they generally come within ""programming contexts"", as far as I know. In other terms, do they involve writing algorithms and programming or are they / may they be referred to the ""less-technical"" activities described above?
","['machine-learning', 'natural-language-processing']","
The procedures you mention don't need to involve writing code. There are now many ready-made tools available which implement various algorithms. 
POS-Tagging, Chunking, and Semantic Tagging/Disambiguation are knowledge-based procedures which can all be seen as classification/clustering tasks: POS-Tagging and Disambiguation are classifications, in that they propose a label (a POS- or semantic tag) that is assigned to a token. Chunking is a clustering algorithm, as it finds coherent groups in sequences of tokens. You could also view it as segmentation, as it splits a sentence into parts.
In principle they don't have anything to do with machine learning; the algorithm you are using (ML or not) is an implementation detail. Early Taggers were mostly rule-based, but with increasing availability of annotated data it became more feasible to use ML algorithms. This, however, has got nothing to do with how you classify the procedures.
In general I would call the activity you describe as 'annotation', as you enrich the source text by adding descriptive categories to tokens and token sequences.
So, no, they do not have to involve programming or implementing algorithms, and you don't need to be technical to perform them. Though some knowledge of linguistic concepts would help.
"
Semi-supervised: Can I predict the label of purposely unlabelled observations?,"
Let's say I have a data set with of length N. A small proportion N2 is labeled. Can I remove some labels and then 'reverse' this action with a trained neural network? I could then use the same process to fill the other (N - N2) rows with labels.
","['neural-networks', 'machine-learning', 'semi-supervised-learning']",
Wouldn't training the model with this data lead to inaccuracies since the testing data would not be normalized in a similar way?,"
I was trying to normalize my input data images for feeding to my convolutional neural network and wanted to use standardize my input data.
I referred to this post, which says that featurewise_center and featurewise_std_normalization scale the images to the range [-1, 1].
Wouldn't training the model with this data lead to inaccuracies since the testing data would not be normalized in a similar way and would only range between [0, 1]? How can I standardize my data while keeping its range between [0, 1]?
","['deep-learning', 'convolutional-neural-networks', 'keras', 'test-datasets', 'standardisation']",
Why does the machine learning algorithm need to learn a set of functions in the case of missing data?,"
I am currently studying the textbook Deep Learning by Goodfellow, Bengio, and Courville. Chapter 5.1 Learning Algorithms says the following:

Classification with missing inputs: Classification becomes more challenging if the computer program is not guaranteed that every measurement in its input vector will always be provided. To solve the classification task, the learning algorithm only has to define a single function mapping from a vector input to a categorical output. When some of the inputs may be missing, rather than providing a single classification function, the learning algorithm must learn a set of functions. Each function corresponds to classifying $\mathbf{x}$ with a different subset of its inputs missing. This kind of situation arises frequently in medical diagnosis, because many kinds of medical tests are expensive or invasive. One way to efficiently define such a large set of functions is to learn a probability distribution over all the relevant variables, then solve the classification task by marginalizing out the missing variables. With $n$ input variables, we can now obtain all $2^n$ different classification functions needed for each possible set of missing inputs, but the computer program needs to learn only a single function describing the joint probability distribution. See Goodfellow et al. (2013b) for an example of a deep probabilistic model applied to such a task in this way. Many of the other tasks described in this section can also be generalized to work with missing inputs; classification with missing inputs is just one example of what machine learning can do. 

I was wondering if people would please help me better understand this explanation. Why is it that, when some of the inputs are missing, rather than providing a single classification function, the learning algorithm must learn a set of functions? And what is meant by ""each function corresponds to classifying $\mathbf{x}$ with a different subset of its inputs missing.""?
I would greatly appreciate it if people would please take the time to clarify this.
","['machine-learning', 'classification', 'learning-algorithms', 'probability-distribution']","
@The Pointer the $2^n$ came from the question: How many function do we need to have if each of the $n$ inputs can be missing?
example: $f_1(\text{missing}, x_2, x_3, \dots, x_n)$ for $x_1$ missing
$f_2(x_1, x_2, \text{missing}, x_4, \text{missing}, \dots, x_n)$ for $x_3$ and $x_5$ missing.
So this problem is a combinatorial one and the event for each $x_i$ is Missing or Not.
Each function corresponds 1-by-1 with a possible set $(x_1, \text{missing}, x_2, \dots, x_n)$. So how many sets can you form $2^n$. Why?
This formula comes back to like tree developing. First variable $x_1$ missing or not (2 possbile events). Now after that, for EACH of these events 2 possbile events for $x_2$ so in total 2 \times 2 (2 for $x_1$ , 2 for $x_2$ for each $x_1$'s event) and etc., $2 \times 2 \times 2 \times \dots$ $n$ times = $2^n$.
"
Can we use a neural network that is trained using Reinforcement Learning for dynamic game level difficulty designing in realtime?,"
I am a newbie to Machine Learning and AI. As per my understanding, with the use of reinforcement learning 
(reward/punishment environment), we can train a neural network to play a game. I would like to know, whether it possible to use this trained model for deciding the difficulty of the next game level dynamically in realtime according to a player's skill level? As an example, please consider a neural network is trained using Reinforcement Learning for playing a mobile game (chess/puzzle, etc.). The game is not consists of a previously designed static set of game levels. After the training, can this model use to detect a particular player's playing style(score, elapsed time) to dynamically decide the difficulty of the next game level and provide customized game levels for each player in realtime?
Thank you very much and any help will be greatly appreciated.
","['reinforcement-learning', 'tensorflow', 'keras', 'game-ai']",
How to get top 5 movies recommendations from Auto-Encoder,"
I have trained a model using Auto-encoder on movielens dataset. Below is how i trained the model.
r = model.fit_generator(
  generator(A, mask),
  validation_data=test_generator(A_copy, mask_copy, A_test_copy, mask_test_copy),
  epochs=epochs,
  steps_per_epoch=A.shape[0] // batch_size + 1,
  validation_steps=A_test.shape[0] // batch_size + 1,
)

It is giving good results but now i am confused how should i get the top 5 recommendation on user input. 
Just wanted to print the result on console. Can anyone help me please?
","['deep-learning', 'autoencoders', 'recommender-system']","
That is not what an auto-encoder is doing. An auto-encoder gives you a compressed representation of the input. It is trained by mapping the input data to itself, with the compressed form in between.
To predict recommendations, you need to train your input data on existing user recommendations.
"
Which neural network can approximate the function $y = x^2 + b$?,"
I am new to ANN. I am trying out several 'simple' algorithms to see what ANN can (or cannot) be used for and how. I played around with Conv2d once and had it recognize images successfully. Now I am looking into trend line analyses. I have succeeded in training a network where it solved for linear equations. Now I am trying to see if it can be trained to solve for $y$ in the formula $y = b + x^2$.
No matter what parameters I change, or the number of dense layers, I get high values for loss and validation loss, and the predictions are incorrect.
Is it possible to solve this equation, and with what network? If it is not possible, why not? I am not looking to solve a practical problem, rather build up understanding and intuition about ANNs.
See the code I tried with below
#region Imports
from __future__ import absolute_import, division, print_function, unicode_literals
import math 
import numpy as np 
import tensorflow as tf
from tensorflow.keras import models, optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Lambda
import tensorflow.keras.backend as K
#endregion

#region Constants
learningRate = 0.01
epochs: int = 1000
batch_size = None
trainingValidationFactor = 0.75
nrOfSamples = 100
activation = None
#endregion

#region Function definitions
def CreateNetwork(inputDimension):
  model = Sequential()
  model.add(Dense(2, input_dim=2, activation=activation))
  model.add(Dense(64, use_bias=True, activation=activation))
  model.add(Dense(32, use_bias=True, activation=activation))
  model.add(Dense(1))
  adam = optimizers.Adam(learning_rate=learningRate)
  # sgd = optimizers.SGD(lr=learningRate, decay=1e-6, momentum=0.9, nesterov=True)
  # adamax = optimizers.Adamax(learning_rate=learningRate)
  model.compile(loss='mse', optimizer=adam)
  return model

def SplitDataForValidation(factor, data, labels):
  upperBoundary = int(len(data) * factor)

  trainingData = data[:upperBoundary]
  trainingLabels = labels[:upperBoundary]

  validationData = data[upperBoundary:]
  validationLabels = labels[upperBoundary:]
  return ((trainingData, trainingLabels), (validationData, validationLabels))

def Train(network, training, validation):
  trainingData, trainingLabels = training
  history = network.fit(
    trainingData
    ,trainingLabels
    ,validation_data=validation
    ,epochs=epochs 
    ,batch_size=batch_size
  )

  return history

def subtractMean(data):
  mean = np.mean(data)
  data -= mean
  return mean

def rescale(data):
  max = np.amax(data)
  factor = 1 / max
  data *= factor
  return factor

def Normalize(data, labels):
  dataScaleFactor = rescale(data)
  dataMean = subtractMean(data)

  labels *= dataScaleFactor
  labelsMean = np.mean(labels)
  labels -= labelsMean

def Randomize(data, labels):
  rng_state = np.random.get_state()
  np.random.shuffle(data)
  np.random.set_state(rng_state)
  np.random.shuffle(labels)

def CreateTestData(nrOfSamples):
  data = np.zeros(shape=(nrOfSamples,2))
  labels = np.zeros(nrOfSamples)

  for i in range(nrOfSamples):
    for j in range(2):
      randomInt = np.random.randint(1, 5)
      data[i, j] = (randomInt * i) + 10
    labels[i] = data[i, 0] + math.pow(data[i, 1], 2)
  
  Randomize(data, labels)
  return (data, labels)
#endregion

allData, allLabels = CreateTestData(nrOfSamples)
Normalize(allData, allLabels)
training, validation = SplitDataForValidation(trainingValidationFactor, allData, allLabels)

inputDimension = np.size(allData, 1)
network = CreateNetwork(inputDimension)

history = Train(network, training, validation)

prediction = network.predict([
  [2, 2], # Should be 2 + 2 * 2 = 6
  [4, 7], # Should be 4 + 7 * 7 = 53
  [23, 56], # Should be 23 + 56 * 56 = 3159
  [128,256] # Should be 128 + 256 * 256 = 65664
])
print(str(prediction))

","['neural-networks', 'keras', 'function-approximation', 'model-request']","
$f(x) = x^2 + b$ is a polynomial (more precisely, a parabola) so it is continuous, thus, a neural network (with at least one hidden layer) should be able to approximate that function (given the universal approximation theorem).
After a very quick look at your code, I noticed you aren't using an activation function for your dense layers (i.e. your activation function is None). Try to use e.g. ReLU.
"
How to detect vanishing gradients?,"
Can vanishing gradients be detected by the change in distribution (or lack thereof) of my convolution's kernel weights throughout the training epochs? And if so how?
For example, if only 25% of my kernel's weights ever change throughout the epochs, does that imply an issue with vanishing gradients?
Here are my histograms and distributions, is it possible to tell whether my model suffers from vanishing gradients from these images? (some middle hidden layers omitted for brevity)




","['deep-learning', 'convolutional-neural-networks', 'deep-neural-networks', 'vanishing-gradient-problem']",
Simple sequential model with LSTM which doesn't converge,"
I'm actually trying to create a sequential neural network in order to translate a ""human"" sentence in a ""machine"" sentence understandable by an algorithm. Like It didn't work, I've try to create a NN that understands whether the input is a unit or not.
Even this NN doesn't work and I don't understand Why. I tried with different optimize/loss/metrics/with Rnn/with LSTM.
So there is one array of unit and one array with lambda words.
I send to the NN :
input -> word in OneHotEncoding where each char is a vector
output -> a vector with a 1 at the relative position of the unit in the array, ex : [0,0,0,1,0]. If It's not a unit, the vector is composed of 0.
I'm actually using LSTM layers and sigmoid activation because I will need it for my ""big"" NN. 
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(13,vocab_size),batch_size=80))
model.add(tf.keras.layers.LSTM(32, return_sequences=True)) 
model.add(tf.keras.layers.LSTM(6, return_sequences=False))
model.add(tf.keras.layers.Dense(6, activation=""sigmoid""))
model.reset_states()
model.summary()
model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])
model.fit(encoded_word, my_targets, batch_size=80, epochs=100, validation_data=(encoded_word, my_targets))



Model: ""sequential_38""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_57 (LSTM)               (80, 13, 32)              6400      
_________________________________________________________________
lstm_58 (LSTM)               (80, 6)                   936       
_________________________________________________________________
dense_32 (Dense)             (80, 6)                   42        
=================================================================
Total params: 7,378
Trainable params: 7,378
Non-trainable params: 0


Epoch 1/100
10000/10000 [==============================] - 6s 619us/sample - loss: 0.7441 - categorical_accuracy: 0.2898 - val_loss: 0.6181 - val_categorical_accuracy: 0.3388
Epoch 2/100
10000/10000 [==============================] - 2s 233us/sample - loss: 0.5768 - categorical_accuracy: 0.3388 - val_loss: 0.5382 - val_categorical_accuracy: 0.3388
Epoch 3/100
10000/10000 [==============================] - 2s 229us/sample - loss: 0.5039 - categorical_accuracy: 0.3979 - val_loss: 0.4640 - val_categorical_accuracy: 0.5084
Epoch 4/100
10000/10000 [==============================] - 2s 229us/sample - loss: 0.4207 - categorical_accuracy: 0.4759 - val_loss: 0.3709 - val_categorical_accuracy: 0.5041

My NN is converging towards 0.5 all the time.
Thank you in advance for your answers !
","['natural-language-processing', 'tensorflow', 'recurrent-neural-networks', 'long-short-term-memory']",
Can a NN be configured to indicate which points of the input influenced its prediction and how?,"
Suppose I want to classify a dataset like the MNIST handwritten dataset, but it has added distractions. For example, here we have a 6 but with extra strokes around it that don't add value.

I suppose a good model would predict a 6, but maybe with less than 100% certainty (or maybe with 100% certainty - I don't know that it matters for the purpose of this question).
Is there any way to get information about which pixels most strongly influenced the decision of the CNN, and which pixels were not so important? So to represent that visually, green means that those pixels were important:

Or conversely, is it possible to highlight pixels which did not contribute to the outcome (or which cast doubt on the outcome thereby reducing the certainty from 100%)

","['convolutional-neural-networks', 'explainable-ai']","
Yes there definitely is, and research into this has actually resulted in some really cool behaviour.
One of the simplest ways is to simply back propagate the gradient all the way back to the input. Areas of the input that affected the final decision will receive larger gradients. Interestingly, this also sort of works as a rudimentary form of semantic segmentation.
Other ways are to change segments of the input and to see how that affects the output (like add 0.1 to a pixel).
You can also determine what each filter in a convolutional layer is looking at using similar techniques.
It's a super interesting field of machine learning, and I highly recommend taking a look at this lecture that is completely free and one of the most interesting ones I've personally seen. it will explain all this much better than I have done.
"
"time-series prediction : loss going down, then stagnates with very high variance","
I am trying to design a model based on LSTM cells to do time-series prediction. The ouput value is an integer in [0,13]. I have noticed that one-hot encoding it and using cross-entropy loss gives better results than MSE loss. 
Here is my problem : no matter how deep I make the network or how many fully connected layers I add I always obtain pretty much the same behavior. Changing the optimizer also doesn't really help.

The loss function quickly decreases then stagnates with a very high variance and never goes down again.
The prediction seems to be offset around the value 9, I really do not understand why since I have one-hot encoded the input and the output.

Here is an example of a the results of a typical training phase, with the total loss :

Do you have any tips/ideas as to how I could improve this or could have gone wrong ? I am a bit of a beginner in ML si I might have missed something. I can also include the code (in PyTorch) if necessary.
","['long-short-term-memory', 'pytorch', 'time-series', 'attention']","
I found the issue, I should have done more unit testing. Upon computing the batch loss before backpropagation, one of the dimension of the ""prediction"" tensor was not corresonding to the ""truth"" tensor. The shape match but the content is not the one that was supposed to be.
This is due to how the NLL loss is implemented in pytorch which I was not aware of ... 
"
Does apprenticeship learning require prospective data?,"
I am thinking of applying apprenticeship learning on retrospective data. From looking at this paper by Ng https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf which talks about apprenticeship learning, it seems to me that at the 5th step of the algorithm, 

Compute (or estimate) $μ^{(i)}$ = $μ(π^{(i)})$, 
where $\mu^{(i)}$ = $E[\sum_{t=0}^{∞}\gamma^{t}$$\phi(s_{t})$ | $\pi^{(i)}]$, $\phi(s_{t})$ is the reward feature vector at state $s_t$. 

From my understanding, a sequence of $s_0, s_1, s_2 ..$ trajectory would have to be generated at this step, following this policy $\pi^{(i)}$. Hence, applying this algorithm on retrospective data would not work?
","['reinforcement-learning', 'rewards', 'apprenticeship-learning']",
Should I grey-scale the coloured frames/channels to build the approximation of the state?,"
I'm doing reinforcement learning, and I have a visual observation that I will use to build an input state for my agent. In the DeepMind's Atari paper, they greyscale the input image before they input it into the CNN to reduce the input space's size, which makes sense to me.
In my environment, I have, for each pixel, 5 possible channels, which are represented in black, white, blue, red, and green. This also makes intuitive sense to me since it's like a bit-encoding.
Any thoughts on what could be better? Greyscaling into 2 shades of grey and black and white also maintains the information, but feels somehow less direct, since my environment's visual space is categorical, which makes more sense in a categorical encoding.
","['reinforcement-learning', 'convolutional-neural-networks', 'dqn', 'deep-rl', 'data-preprocessing']",
How to estimate the error during training in deep reinforcement learning,"
How do I calculate the error during the training phase for deep reinforcement learning models?
Deep reinforcement learning is not supervised learning as far as I know. So how can the model know whether it predicts right or wrong? In literature, I find that the ""actual"" Q-value is calculated, but that sounds like the whole idea behind deep RL is obsolete. How could I even calculate/know the real Q-value if there is not already a world model existing?
","['deep-learning', 'reinforcement-learning', 'deep-rl']","
Yes, reinforcement learning is very different from supervised learning, the policy (what you call a model) does not know if its predicting right or wrong, or if its taking the correct action or not. In RL there is no concept of ""the right action"", everything is evaluated through the reward function.
Also there are no ways to compute the ground truth Q-values, if you had that then there is no need to do RL.
In RL you should not think like in supervised learning, there are no error metrics, everything is evaluated on how much accumulated reward the agent receives over an episode.
"
How to define an action space when an agent can take multiple sub-actions in a step?,"
I'm attempting to design an action space in OpenAI's gym and hitting the following roadblock. I've looked at this post which is closely related but subtly different.
The environment I'm writing needs to allow an agent to make between $1$ and $n$ sub-actions in each step. Leaving it up to the agent to decide how many sub-actions it wants to take. So, something like (sub-action-category, sub-action-id, action) where the agent can specify between $1$ and $n$ such tuples.
It doesn't seem possible to define a Box space without specifying bounds on the shape which is what I need here. I'm trying to avoid defining an action space where each sub-action is explicitly enumerated by the environment like (action) tuple with n entries for each sub-action.
Are there any other spaces I could use to dynamically scale the space?
","['reinforcement-learning', 'ai-design', 'open-ai', 'gym']",
How does policy evaluation work for continuous state space model-free approaches?,"
How does policy evaluation work for continuous state space model-free approaches?
Theoretically, a model-based approach for the discrete state and action space can be computed via dynamic programming and solving the Bellman equation. 
Let's say you use a DQN to find another policy, how does model-free policy evaluation work then? I am thinking of Monte Carlo simulation, but that would require many many episodes. 
","['reinforcement-learning', 'deep-rl', 'monte-carlo-methods', 'model-free-methods', 'policy-evaluation']",
Weird border artifacts when training a CNN,"
I've been trying to use this DeepLabv3+ implementation with my dataset (~1000 annotated images of the same box, out of the same video sequence): https://github.com/srihari-humbarwadi/person_segmentation_tf2.0
But I get border artifacts like this:

Any ideas what could be causing it? Note that if I use bigger batches and train for more epochs, the borders tend to get thinner but never disappear. They also appear randomly around the image.
Any clues what could be causing them and how to solve it?
","['machine-learning', 'convolutional-neural-networks', 'computer-vision', 'datasets']",
How can I reduce combinatorial explosion in an MCTS-like algorithm for program induction?,"
I'd like to develop an MCTS-like (Monte Carlo Tree Search) algorithm for program induction, i.e. learning programs from examples.
My initial plan is for nodes to represent programs and for the search to expand nodes by revising programs.
Many of these expansions revise a single program: randomly resample a subtree of the program, replace a constant with a variable, etc. It looks straightforward to use these with MCTS.
Some expansions, however, generate a program from scratch (e.g. sample a new program). Others use two or more programs to generate a single output program (e.g. crossover in Genetic Programming).
These latter types of moves seem nonstandard for vanilla MCTS.
One idea I've had is to switch from nodes representing programs to nodes representing tuples of programs. The root node would represent the empty tuple $()$, to which expansions could be applied only if they can generate a program from scratch. The first such expansion would produce some program $p$, so the root would now have child $(p)$. The second expansion would produce $p'$, so the root would now also have child $(p')$ as well as the pair $(p, p')$. Even assuming some reasonable restrictions (e.g. moves can use at most 2 programs, pairs cannot have identical elements, element order doesn't matter), the branching factor will grow combinatorially.
What techniques from the MCTS literature (or other literatures) might reduce the impact of this combinatorial explosion?
","['monte-carlo-tree-search', 'combinatorics', 'inductive-programming', 'program-synthesis']","
I think you may get some inspiration from the work on deterministic environments by Javier Segovia et al. See their paper Computing programs for generalized planning using a classical planner (2019).
If you don't have access to Elsevier's papers, I recommend that you check out the first author's profile on dblp.org. From there you will find links to open access versions of the conference paper that led up to the publication above.
"
How to reduce fluctuation of a neural network?,"
I've modeled an AlexNet neural network, with 50 epochs and a batch size of 64. I used a stochastic gradient descent optimizer with a learning rate of 0.01. I attached the train and validation loss and accuracy plots.


How can I reduce the fluctuation of the first epochs?
","['neural-networks', 'deep-learning', 'training', 'objective-functions']",
Using word embedding to extend words for searching POI names,"
I am developing my own mobile app related to digital map. One of the functions is searching POIs (points of interest) in the map according to relevance between user query and POI name.
Besides the POIs whose names contain exact words in the query, the app also needs to return those whose names are semantically related. For example, searching 'flower' should return POI names that contain 'flower' as well as those that contain 'florist'. Likewise, searching 'animal' should return 'animal' as well as 'veterinary'.
That said, I need to extend words in the query semantically. For example, 'flower' has to be extended to ['flower', 'florist']. I have tried to use word embeddings: using the words corresponding to most similar vectors as extensions. Due to the fact I don't have user review data right now and most of the POI names are very short, I used trained word2vec model published by Google. But the results turn out to be not what I expect: most similar words of 'flower' given by word2vec are words like 'roses'and 'orchid', and 'florist' is not even in the top 100 most similar list. Likewise, 'animal' gives 'dog', 'pets', 'cats' etc. Not very useful for my use case.
I think simply using word embedding similarity may not be enough. I may need to build some advanced model based on word embedding. Do you have any suggestions?
","['natural-language-processing', 'word-embedding']","
I think word embeddings are overkill in this particular case.
My suggestion would be to go for a simple dictionary based approach: compose sets of semantically related words, and then use those to expand your query terms. This might take a bit longer to set up, but has several advantages:

simplicity: you can't make many mistakes with this
transparency: you know exactly why a certain term matches, and another one doesn't
accuracy: you have tight control over the whole process; if some term is wrong, you remove it from the set. You cannot do that with embeddings
resources: a dictionary-based approach is far simpler and needs less storage

'Old tech' doesn't sound as sexy as the latest deep learning stuff, but unless you want to have this as a toy project to learn about how to do things with embeddings I would say the latter are the wrong tool for the job. At least you can be sure that it works, and if it doesn't you can easily fix it.
"
When is it time to switch to deep neural networks from simple networks in text classification problems?,"
I did an out of domain detection task (as a binary classification problem) and tried LR and Naive Bayes and BERT but the deep neural network didn't perform better than LR and NB. For the LR I just used BOW and it beats the 12-layer BERT. 
In a lecture, Andrew Ng suggests ""Build First System Quickly, Then Iterate"", but it turns out that sometimes we don't need to iterate the model into a deep neural network and most of the time traditional shallow neural networks are good/competitive enough and much simpler for training. 
As this tweet (and its replies) indicate, together with various papers [1, 2, 3, 4 etc], traditional SVM, LR, and Naive Bayes can beat RNN and some complicated neural networks. 
Then my two questions are: 

When should we switch to complicated neural networks like RNN, CNN, and transformer and etc? How can we see that from the data set or the results (by doing error analysis) of the simple neural networks?       
The aforementioned experiments may be caused by the simple test set, then (how) is it possible for us to design a test set that can fail the traditional models? 

","['natural-language-processing', 'classification', 'text-classification']","

Source: https://blog.easysol.net/building-ai-applications/
When data is too big, complex and nonlinear it's time to try deep learning. It's always good to try to add some layers to see it can eliminate bias and don't lead to high variance. 
Deep learning models can be tweaked(hyperparameters) and regularized(parameters), and it is worth the work.  
"
Specifying resolution for objects with known dimensions using CNN,"
I would like to ask you for advice. I deal with beekeeping but I am also a bit a programmer and an electronics specialist. And this is where my 3 interests come together, actually 4 because recently deep learning has joined this group.
I would like to analyze (I already run some tests) bee behavior using deep learning mechanisms based on images (photos or videos) with bees. These images, of course, can be very different, this applies primarily to the area that can be shown. It can literally be 2x3cm like a macro picture or an area of ​​40x30cm and 3000 bees on it. Trying to create a network analyzing such different areas is a nightmare. And because I am interested in aspects of bees separately, it seems logical to divide the image into these parts, which will contain single bees in their entirety, reduce their resolution to a minimum ensuring recognition of the necessary details and only then analyze each part of the large image separately.
This approach seems even more right to me when I started to read the details, e.g. YOLOv3. Thanks to this, I would avoid a large amount of additional calculations.
And here comes the first stage to be implemented. I need to specify the image resolution, more precisely the number of pixels that on average falls on one bee length. If it is too small, I do nothing but say that the resolution is too small for analysis. If it is suitable (at least 50 pixels for the length of the bee), I algorithmically cut the input image into smaller ones so that they contain individual insects and I subject them to further analysis.
Fortunately, the sizes of bees and bee cells are precisely known to within 0.1mm. And there may be hundreds of such objects (a bee or a hexagonal bee cell) and I can decide how many of them will be at the stage of preparing training data. I have thousands of photos and videos from which I can easily create training data for the network in the form of pairs of objects image bee size expressed in pixels from this image.
Time for question:
At the output I want to get a number saying that one bee in the image is X pixels long, e.g. 43.02 or 212.11
Has anyone of you dealt with a similar case of determining resolution for known objects using a neural network (probably CNN with an output element with rel function) and can share your experience, e.g. a network structure that would be suitable for this purpose?
",['convolutional-neural-networks'],
What is the difference between TensorFlow's callbacks and early stopping?,"
What is the difference between TensorFlow's callbacks and early stopping?
","['neural-networks', 'tensorflow', 'comparison', 'regularization', 'early-stopping']",
Crossing of training and validation loss,"
During training of my models I often encounter the following situation about training (green) and validation (gray) loss:

Initially, the validation loss is significantly lower than the training loss. How is this possible? Does this tell me anything important about my data or model?
One explanation might be, that training and validation data are not properly split, i.e. the validation might primarily contain data, that the model can easily represent. But then why do the curves cross after epoch 30? If this is because of overfitting, then I would expect the validation loss to increase, but so far both losses are (slowly) decreasing.
There is a related question at Data Science SE, but it doesn't give a clear answer.
",['loss'],
AI with conflicting objectives?,"
A recent question on AI and acting recalled me to the idea that in drama, there are not only conflicting motives between agents (characters), but a character may themselves have objectives that are in conflict.
The result of this in performance is typically nuance, but also carries the benefit of combinatorial expansion, which supports greater novelty, and it occurs to me that this would be a factor in affective computing.  
(The actress Eva Green is a good example, where her performances typically involve indicating two or more conflicting emotions at once.)
It occurs to me that this can even arise in the context of a formal game where achieving the most optimal outcome requires managing competing concerns.

Is there literature or examples of AI with internal conflicting objectives?

","['machine-learning', 'reference-request', 'game-theory', 'goal-based-agents', 'affective-computing']","
There are multi-objective optimization problems, where the objective functions may be in conflict with each other, which can potentially have multiple Pareto-optimal solutions. The paper Multi-objective optimization using genetic algorithms: A tutorial (2006) gives a good overview of the multi-objective optimization problem with genetic algorithms, which can be called evolutionary multi-objective optimization (EMO) or multi-objective optimization evolutionary algorithms (MOEAs). 
A common multi-objective genetic algorithm is NSGA (or NSGA-2 and NSGA-3), which stands for Non-dominated Sorting Genetic Algorithm, which is based on the concepts of non-dominated sorting, Pareto front and optimality, niches (sub-populations), and elitism (the best individuals of the current population are carried over to the next generation). 
If you want to play with MOEAs, you may wanna try the Python deap package, which supports, for example, the NSGA-2 algorithm.
"
Can I provide a CNN with hints?,"
Let's say I want to classify a dataset of handwritten digits (CNNs on their own can get 99.7% on the MNIST dataset but let's pretend they can only get 90% for the sake of this question). 
Now, I already have some classical computer vision techniques which might be able to give me a clue. For instance, I can count the intersection points of the pen stroke

1,2,3,5,7 will usually have no intersection points
6,9 will usually have one intersection point each
4,8 will usually have two intersection points each (usually a 4-way crossover yields two intersection points which are close together)

So if I generate some meta-data telling me how many intersection points each sample has, how can I feed that into the CNN training so that it can take advantage of that knowledge?
My best guess is to slot it into the last fully connected layer just before classification.
","['convolutional-neural-networks', 'classification']",
Is traditional machine learning obsolete given that neural networks typically outperform them?,"
I have been coming across visualizations showing that the neural nets tend to perform better as compared to the traditional machine learning algorithms (Linear regression, Log regression, etc.)
Assuming that we have sufficient data to train deep/neural nets, can we ignore the traditional machine learning topics and concentrate more on the neural network architectures?
Given the huge amount of data, are there any instances where traditional algorithms outperform neural nets?
","['neural-networks', 'machine-learning', 'statistical-ai']",
Formal proof that every purely reactive agent has behaviorally equivalent standard agent,"
It kind of makes sense intuitively but I'm not sure about a formal proof. I'll start with briefly listing definitions from Intro to Multiagent systems, Wooldridge, 2002 and then give you my reasoning attempts thus far.
$E$ is a finite set of discrete, instantaneous states, $E=(e, e',...)$. $Ac$ is a repertoire of possible actions (also finite) available to an agent, which transform the environment, $Ac=(\alpha, \alpha', ...)$. A run is a sequence of interleaved environment states and actions, $r=(e_0, \alpha_0, e_1, \alpha_1,..., \alpha_{u-1}, e_u)$, set of all such possible finite sequences (over $E$ and $Ac$) is $R$, $R^E$ is a subset of $R$ containing the runs that end with an env. state.
Purely reactive agent is modeled as: $Ag_{pure}: E\mapsto Ac$, a standard agent is modeled as $Ag_{std}: R^E\mapsto Ac$.
So, if $R^E$ is a sequence of agent's actions and environment states, than it just makes sense that $E\subset R^E$. Hence, $Ag_{std}$ can map to every action to which $Ag_{pure}$ can. And behavioral equivalence with respect to environment $Env$ is defined as $R(Env, Ag_{1}) = R(Env, Ag_{2})$; where $Env=\langle E,e_{0},t \rangle$, $e_{0}$ - initial environment state, $t$ - transformation function (definition irrelevant for now).
Finally, if $Ag_{pure}: E\mapsto Ac$ and $Ag_{std}: R^E\mapsto Ac$, and $E\subset R^E$, we can say that $R(Env,Ag_{pure}) = R(Env, Ag_{std})$ (might be too bold of an assumption). Hence, every purely reactive agent has behaviorally equivalent standard agent. The opposite might not be true, since $E\subset R^E$ means that all elements of $E$ belong to $R^E$, while not all elements $R^E$ belong to $E$.
It's a textbook problem, but I couldn't find an answer key to check my solution. If anyone has formally (and perhaps mathematically) proven this before, can you post your feedback, thoughts, proofs in the comments? For instance, set of mathematical steps to infer $E\subset R^E$ from their definitions: $E=(e_{0}, e_{1},..., e_{u})$ and $R^E$ is ""all agent runs that end with an environment state"" (no formal equation found) is not clear to me.
","['math', 'multi-agent-systems']","
I recommend you to look into the literature about simulation and bisimulation in Automata Theory and its applications to model checking (where you want to make quite regularly proofs of ""behavioural equivalence""). One article that discusses this in the context of a technique for model checking known as ""Abstraction and Abstraction Refinement"" is
Abstraction and Abstraction Refinement
Dennis Dams and Orna Grumberg
In Springer's Handbook of Model Checking, 2018, Chapter 13, pages 385-420

A good (I use it regularly) book that covers behavioural equivalence for a wide variety of automata is 
Verification and Control of Hybrid Systems: A Symbolic Approach
Paulo Tabuada
Springer, 2009

"
"In neural networks, what does the term depth generally mean?","
Is it

number of units in a layer 
number of layers
overall complexity of the network (both 1 and 2)

",['neural-networks'],
How to produce documents like factset blackline?,"
Factset blackline reports essentially can compare two 10-Q SEC filings and show you the difference between the two documents. It highlights added items in green and removed items in red + strikethrough (essentially, it's a document difference, but longer-term I would like to run algorithms on the differences).
I don't care to change colors, but what I would like to do is to produce similar extracts that summarize addition and deletions.
Which AI/ML algorithm could do the same?
","['natural-language-processing', 'recurrent-neural-networks']","
Financial information companies spend a very large amount of effort to do this kind of thing properly, and their models are generally proprietary, so the real answer is not something you'll be able to get on a public site.
Of course, you can achieve a very simple approximation with a literal diff on the texts, and a more sophisticated one by using out-of-the-box NLP or computer vision techniques, but you're probably not going to get close to the performance of factset or other competitors using those tools.
"
How do I poison an SVM with manifold regularization?,"
I'm working on Adversarial Machine Learning, and have read multiple papers on this topic, some of them are mentioned as follows:

Poisoning Attacks on SVMs: https://arxiv.org/pdf/1206.6389.pdf
Adversarial Label Flips on Support Vector Machines

However, I am not able to find any literature on data poisoning for SVMs using Manifold regularization. Is there anyone who has knowledge about that?
","['machine-learning', 'regularization', 'support-vector-machine', 'adversarial-ml', 'causation']","
I think, one could generate a data set, with a random variable in each component of the data vector, add this data to the training data set, and then shuffle the combined data set.
"
Is the minimax algorithm model-based?,"
Trying to get my head around model-free and model-based algorithms in RL. In my research, I've seen the search trees created via the minimax algorithm. I presume these trees can only be created with a model-based agent that knows the full environment/rules of the game (if it's a game)? If not, could you explain to me why?
","['reinforcement-learning', 'comparison', 'minimax', 'model-based-methods', 'model-free-methods']",
How can the V and Q functions take the expectation over a sum where the number of summands is random?,"
Assume the existence of a Markov Decision Process consisting of:

State space $S$
Action space $A$
Transition model $T: S \times A \times S \to [0,1]$
Reward function $R: S \times A \times S \to \mathbb{R}$
Distribution of initial state $p_0: S \to [0,1]$

and a policy $\pi: S \to A$.
The $V$ and $Q$-functions take expectations of the sum of future rewards.
Let's start off by $r_0:= R(x_0,\pi(x_0),x_1)$, where $\pi$ is the current policy while $x_0 \sim p_0$ and $x_1 \sim T(x_0,\pi(x_0),-)$ are random variables. With setting $\mu_i:= T(x_i,\pi(x_i),-),\rho_i:=R(x_i,\pi(x_i),-)$, I obtain
$$E[r_0]= \int_{\mathbb{R}} r d\mu_0^{\rho_0} = \int_S R(x_0,\pi(x_0),-)d\mu_0,$$
where $\mu_i^{\rho_i}:= \mu_i\circ \rho_i^{-1}$ is the pushforward of $\mu_i$ under random variable $\rho_i$. But the above quantity still depends on $x_0$ as both $\mu_o$ and $\rho_0$ depend on $x_0$. Intuitively, I would guess that one has to calculate the integral over every occuring random variable to obtain the overall expectation, i.e. $$E[r_0]= \int_S\int_S R(x_0,\pi(x_0),x_1)d\mu_0(x_1)dp_0(x_0)$$ is that correct ?
Now, the $V$ and $Q$-functions take the expectation over the sum $R_{\tau} = \sum^T_{t=\tau}\gamma^{t-\tau}r_t$, where the instant of termination $T$ itself is a random variable, and, besides that, the agent does not know its distribution, as it is not even included in the MDP model.
How can I take the expectation over a sum where the number of summands is random?
We cannot just calculate $\sum^{E[T]}(\dots)$, because $E[T]$ might not even be an integer.
","['reinforcement-learning', 'math', 'value-functions', 'probability-theory']",
Can neural style transfer work on the image style in this question or is there a better technique?,"
I've been working with this neural style paper https://arxiv.org/pdf/1508.06576v2.pdf
to try and transfer the style from this image to photos of pets. In case you're not familiar with the technique, I'll leave a brief explanation of it at the end.

After taking the time to understand some of the concepts in the paper I've come to my own conclusion that the method won't work. Here's why I think this:

The style is not localised/fine-grained enough. If I take a small piece of this image, I might just get a solid color, or two colors separated by a zig-zag boundary. So the first few convolutional layers won't find any characteristics of interest.
The style depends on long-distance correlations (I made that term up). If you follow some of theses zig-zag interfaces with your eyes you'll see that they traverse up to 1/3 of the characteristic size of the image. So bunches of pixels that take up a lot of space are correlated via the style. And from my intuition of CNNs, you can distill lots of pixels into coarser information (like ""this is a dog"") but you can't really go backwards. I don't think the CNN encodes the required logic to trace out a zig-zag over a long distance.
The color palette is restricted. I'm not sure why, but my intuition tells me the Neural Style Transfer technique won't be able to produce a restricted color pallet like in the style example. I'm guessing again, that the CNN doesn't encode any logic around the number of colors used.

So my question is around whether or not the technique could work, and if not, what's a better technique for this problem.
(optional read) Summary of deep style transfer

Take a pre-trained model like VGG19.
Feed in a photo. For each conv layer you can reconstruct a content representation of what that layer encodes about the photo. You do this by treating the feature maps of that layer as a desired output, and doing gradient descent on a white noise image as the input, where the loss function is the RMS between the original photo and the generated image.
Feed in a painting. For each conv layer you can reconstruct a style representation of what that layer encodes about the painting in a similar way as step 2. This time though, your loss function is the RMS between the gram matrix of all the features maps produced using a white noise input, and the gram matrix of all the feature maps produced using the painting as input. And here, you sum loss over all features maps of prior layers as well, not just the layer you are considering now.
Jointly minimise the loss functions described in 2 and 3 (so you're minimising content loss and style loss together) in order to produce an image with the content of the photo and the style of the painting.

EDIT
I have tried this. Here is an example of my results (left is input, right is output). So it's kind of cool to see some color map reduction happening, and what kind of looks like accentuation of texture, but definitely not getting these illustrated zig zag boundaries that the human mind so readily perceives as fur.

",['convolutional-neural-networks'],
Is there a family tree for reinforcement learning algorithms?,"
Can anyone point me in the direction of a nice graph that depicts the ""family tree"", or hierarchy, of RL algorithms (or models)? For example, it splits the learning into TD and Monte Carlo methods, under which is listed all of the algorithms with their respective umbrella terms. Beneath each algorithm is shown modifications to those algorithms, etc. I'm having difficulty picturing where everything lies within the RL landscape.
","['machine-learning', 'reinforcement-learning']",
Is this neural network architecture appropriate for CIFAR-10? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I have a CNN architecture for CIFAR-10 dataset which is as follows:

Convolutions:           64, 64, pool
Fully Connected Layers: 256, 256, 10
Batch size:             60
Optimizer:              Adam(2e-4)
Loss:                   Categorical Cross-Entropy

When I train this model, training and testing accuracy along with loss has a very jittery behavior and does not converge properly.
Is the defined architecture correct? Should I have a max-pooling layer after every convolution layer?
","['neural-networks', 'convolutional-neural-networks', 'tensorflow']","
To be honest, your model is not very clear. But basically after the convolution, you need to add non-linear layers. Otherwise, there is no point of Neural Networks.
You can add a Relu layer for sure. 
"
How do I generate a feature representation of a saliency map (or mask)?,"
Generally, CNNs are used to extract feature representations of an image. I'm right now dealing with the class of CNN that produces saliency maps, which are generally in the format of a mask. I'm trying to generate a feature representation of that specific Mask. What could be the best way to approach this problem?
","['convolutional-neural-networks', 'image-segmentation']",
How to fairly conduct a model performance with 5-fold cross validation after augmentation?,"
I have, say, a (balanced) data-set with 2k images for binary classification. What I have done is that

randomly divided the data-set into 5 folds;
copy-pasted all 5-fold data-set to have 5 exact copies of data-set (folder_1 to folder_5, all absolutely same data-set)
first fold in folder_1 is saved as test folder and remaining (fold_2, fold_3, fold_4, fold_5) are combined as one train folder
second fold in folder_2 is saved as test folder and remaining (namely, fold_1, fold_3, fold_4, fold_5) are combined as one train folder
third fold in folder_3 is saved as test folder and remaining (namely, fold_1, fold_2, fold_4, fold_5) are combined as one train folder.
similar process has been done on folder_4 and foder_5.

I hope, by now, you got the idea of how I distributed the data-set.
The reason I did so is as follows:
I have augmented the training data (train folder) in each of the folders and used test folders respectively to evaluate (ROC-AUC score). Now I kind of have 5 ROC-AUC scores which I evaluated using test folders. If I get the average value out of those 5 scores.
(Assuming the above cross-validation process is done right) If I were to perform some manual hyperparameter optimizations (like an optimizer, learning rate, batch size, dropout, activation) and perform the above cross-validation with data augmentation and find the best so-called ""mean ROC-AUC"", does it mean I successfully conducted hyper-parameter optimization?
FYI: I have no problem with computing power OR/AND time at all to loop through the hyper-parameters for this type of cross-validation with data augmentation
","['deep-learning', 'datasets', 'hyperparameter-optimization', 'cross-validation']",
Steps to train and re-train a good model,"
I'm still a bit new to deep learning. What I'm still struggling, is what is the best practice in re-training a good model over time? 
I've trained a deep model for my binary classification problem (fire vs non-fire) in Keras. I have 4K fire images and 8K non-fire images (they are video frames). I train with 0.2/0.8 validation/training split. Now I test it on some videos, and I found some false positives. I add those to my negative (non-fire) set, load the best previous model, and retrain for 100 epochs. Among those 100 models, I take the one with lowest val_loss value. But when I test it on the same video, while those false positives are gone, new ones are introduced! This never ends, and Idk if I'm missing something or am doing something wrong.
How should I know which of the resulting models is the best? What is the best practice in training/re-training a good model? How should I evaluate my models?
Here is my simple model architecture if it helps:
def create_model():
  model = Sequential()
  model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(300, 300, 3)))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Dropout(0.2))
  model.add(Flatten())
  model.add(Dense(256, activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(2, activation = 'softmax'))

  return model

#....
if retrain_from_prior_model == False:
    model = create_model()
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
else:
    model = load_model(""checkpoints/model.h5"")

","['machine-learning', 'deep-learning', 'training', 'deep-neural-networks', 'metric']",
Why not more TD(𝜆) in actor-critic algorithms?,"
Is there either an empirical or theoretical reason that actor-critic algorithms with eligibility traces have not been more fully explored? I was hoping to find a paper or implementation or both for continuous tasks (not episodic) in continuous state-action spaces.
This has been the only related question on SE-AI that I have been able to find Why are lambda returns so rarely used in policy gradients?.
Although I appreciated the dialog and found it useful, I was wondering if there was any further detail or reasoning that might help explain the void.
","['reinforcement-learning', 'actor-critic-methods', 'temporal-difference-methods', 'eligibility-traces', 'td-lambda']",
Unable to understand V* at infinite time horizon using Bellman equation for solving MDP,"
I've been following the Berkeley cs188's assignment (I'm not taking the course). Currently, they don't show the solution in the gradescope unless I get it correct. 

My reasoning was
$V^*(a)$ = 10 fixed, because the optimal action is to terminate and receive the reward 10. 
$V^*(b) = 10 \times 0.2 = 2$ using Bellman optimality eqn $V^*(s) = R(s)+ \gamma \ \rm{max}_{a} \sum_{s'} P(s'|s,a) V^*(s')$, where the optimal actionfrom b is to left.
Similarly, I get $V^*(c) = 10 \times (0.2)^2 = 0.4$
For the state $d$, it is optimal to move to the right and exit at $e$ to receive 1, therefore $V^*(d) = 1 \times 0.2 = 0.2$.
And $V^*(e) = 1$ fixed. 
However, there autograder says it's incorrect and doesn't show explanation. Can anyone explain what the right approach or answer is?  
","['markov-decision-process', 'value-iteration']","
Nevermind. I found that above answer is indeed correct, but the gradescope has a bug (it requires the format to be .2 instead of 0.2).
"
Is there a good ratio between the positive and negative rewards in reinforcement learning?,"
Is there an ideal ratio in reinforcement learning between the positive and negative rewards?
Suppose I have the scenario of moving a robot across the river. There are two options, walk across the bridge or walk across the river. If it walks across the river then the robot breaks so the idea is to reinforce the robot to walk across the bridge. What would be the best rewards values? Does this ratio vary between cases?
option1:

Bridge: +10
River: -10

Option2:

Bridge: +10
River: -1

Option3:

Bridge: +1
River: -10

","['reinforcement-learning', 'rewards']","
It usually does not matter, but I'm sure there are situations where it could matter.  In theory, if a reward for good behavior is higher than the rewards for bad behavior, then the neural network will be trained such that the higher rewards are preferred, even if those higher rewards are negative.  For example, if a bad reward is -100, then a relatively good reward could be -50, and the network will then be more likely to choose the action with -50 reward over the action with -100 reward.
"
How should I penalize the model proportionally to the error?,"
I am making an MNIST classifier. I am using categorical cross-entropy as my loss function. I want to make it so that if the correct label is 3, then it will penalize the model less heavily if it classifies a 4 than a 7 because 4 is closer numerically to 3 than 7 is. How do I do this?
","['machine-learning', 'classification', 'objective-functions', 'cross-entropy', 'categorical-crossentropy']",
What is the purpose of the arrow $\leftarrow$ in this formula?,"
What is the purpose of the arrow $\leftarrow$ in the formula below? 
$$V(S_t) \leftarrow V(S_t) + \alpha \left[ G_t - V(S_t) \right]$$
I presume it's not the same as 'equals'.
","['reinforcement-learning', 'notation', 'value-functions']",
Is there an AI that can complete Deezer Spleeter work?,"
I have used Deezer Spleeter but it produces echoes aside the stems, so I wonder if there is already an AI that remove echoes noises.
",['audio-processing'],
Similarity of Images (CBIR) for two different cameras,"
Suppose we have a top down picture of an object (let's say it is a shoe) from an overhead camera. Also suppose we have a database of various objects from a closeup camera. If we feed the top-down picture of the hsoe into a CBIR model, then this picture would obviously have very low similarity with the images in the database. But would the image with the highest similarity score in the database still be a shoe even though the absolute score is small?
",['object-detection'],
Implementing Actor-Critic with Experience Replay for Continuous Action Spaces,"
I have been trying to implement the ACER algorithm for continuous action spaces in reinforcement learning. The paper for the algorithm can be found here:

Sample Efficient Actor-Critic with Experience Replay

I have implemented parts of the algorithm, but I have encountered some roadblocks that I have not been able to figure out. 
The following is the pseudo-code provided in the paper:

Here is what I have implemented so far:
states = tf.convert_to_tensor(trajectory.state)
actions = tf.squeeze(tf.convert_to_tensor(trajectory.action), axis=-1)
rewards = tf.convert_to_tensor(trajectory.reward)
dones = tf.convert_to_tensor(trajectory.done)

explore_means, state_values, action_values = actor_critic(states, actions)

average_means, *_ = brain.average_actor_critic(states)

k = len(trajectory.state)
d = env.action_space.shape[0]

# Policies
explore_policies = k*[None]
behavior_policies = k*[None]
average_policies = k*[None]

# Tracking
explore_actions = np.zeros([k, d])
importance_weights = np.zeros([k, 1])
explore_importance_weights = np.zeros([k, 1])
truncation_parameters = np.zeros([k, 1])

for i in range(k):

    behavior_policy = tfd.MultivariateNormalDiag(
        loc=trajectory.statistics[i],
        scale_diag=tf.ones(d)*POLICY_STD
    ) 

    explore_policy = tfd.MultivariateNormalDiag(
        loc=explore_means[i],
        scale_diag=tf.ones(d)*POLICY_STD
    ) 

    average_policy = tfd.MultivariateNormalDiag(
        loc=average_means[i],
        scale_diag=tf.ones(d)*POLICY_STD
    )

    explore_action = explore_policy.sample()

    importance_weight = explore_policy.prob(actions[i]) / behavior_policy.prob(actions[i])
    explore_importance_weight = explore_policy.prob(explore_action) / behavior_policy.prob(explore_action)

    truncation_parameter = min(1, (importance_weight)**d)


    behavior_policies[i] = behavior_policy
    explore_policies[i] = explore_policy
    average_policies[i] = average_policy
    explore_actions[i] = explore_action
    importance_weights[i] = importance_weight
    explore_importance_weights[i] = explore_importance_weight
    truncation_parameters[i] = truncation_parameter


explore_actions = tf.convert_to_tensor(explore_actions, dtype=tf.float32)
importance_weights = tf.convert_to_tensor(importance_weights, dtype=tf.float32)
explore_importance_weights = tf.convert_to_tensor(explore_importance_weights, dtype=tf.float32)
truncation_parameters = tf.convert_to_tensor(truncation_parameters, dtype=tf.float32)


q_ret = values[-1] if not dones[-1] else tf.zeros(1)
q_opc = tf.identity(q_ret)

for i in reversed(range(k - 1)):

    q_ret = rewards[i] + GAMMA*q_ret
    q_opc = rewards[i] + GAMMA*q_opc


    # Compute quantities needed for trust region updating
    c = TRUNCATION_PARAMETER

    with tf.GradientTape(persistent=True) as tape:

        tape.watch(explore_policies[-2].loc)

        log_prob = explore_policies[-2].log_prob(actions[-2])
        explore_log_prob = explore_policies[-2].log_prob(explore_actions[-2])

        kl_div = tfp.distributions.kl_divergence(average_policies[-2], explore_policies[-2])


    lp_grad = tape.gradient(log_prob, explore_policies[-2].loc)    
    elp_grad = tape.gradient(explore_log_prob, explore_policies[-2].loc) 
    kld_grad = tape.gradient(kl_div, explore_policies[-2].loc) 


    term1 = min(c, importance_weights[-2])*lp_grad*(q_opc - state_values[-2])
    term2 = tf.nn.relu(1 - (c / explore_importance_weights[-2]))*(action_values[-2] - state_values[-2])*elp_grad

    g = term1 + term2

So the goal here was to implement it exactly the way they have it in the paper and then afterwards optimize it for doing batches of trajectories. For now, however, it is sufficient for the purposes of learning. 
My confusion comes from the use of differentials in this algorithm. I don't know what the specific type is for them, such as whether they are using it for the loss value to optimize on or if they are storing the gradients that will be used for updating. Another issue I am having is that it is not clear what they mean by this line:

I don't understand why they are using a partial derivative here if there is clearly more than one parameter in the neural network. Maybe they mean the gradient I am not sure, however.
So what would be helpful is if anybody has some guidance as to what they are getting at in this portion of the paper or if anybody has some advice as to what steps need to be taken in TensorFlow 2.0 to implement this algorithm.
Any help would be greatly appreciated! Thanks!!
","['reinforcement-learning', 'actor-critic-methods', 'experience-replay']",
Can I apply experience on naive actor critic directly? Should it work?,"
Can I apply experience replay on naive actor-critic directly? Should it work?
I have tried that but unfortunately it didn't work.
","['reinforcement-learning', 'actor-critic-methods', 'experience-replay']",
Evaluation a policy learned using Q - learning,"
I have been reading literature on reinforcement learning in healthcare. I am slightly confused between the policy evaluation for both SARSA and Q-learning. 
To my knowledge, I believe that SARSA is used for policy evaluation, to find the Q values of following an already existing policy. This is usually the clinician's policy. 
Q - learning on the other hand seeks to find another policy, different from the clinician's such that the policy learned at different states always maximise the Q - values. This leads to a better treatment policy.
Suppose the Q values are learned from both policies, if the Q values for Q - learning are higher than those of SARSA's, can we say that the policy learned from Q - learning is better than that of the clinician's ?
EDIT
From readings I have found out that computing the state - value function is usually used to compare how good policies are. I believe that new data has to be generated to apply the policy learned from Q - learning and compute the state - value function for following this learnt policy from Q - learning.  
Why can't the Q values learnt from SARSA and Q - learning be used as comparison instead ? Also, for model free approaches (eg. continuous state space), how is policy evaluation usually carried out ?
","['reinforcement-learning', 'q-learning', 'healthcare', 'sarsa']",
What are some of the best methods in detecting facial movement using state-of-the-art machine learning models?,"
I am currently working on implementing a lip reading system in Python using machine learning and image processing. Currently, two initial implementations have provided promising results, albeit not perfect: the LipNet model and the Google Cloud AutoML Vision API. Before I dive head on into these two different systems, I was wondering if anyone on here had been exposed to any other alternatives for lip reading or any other facial detection issue, and their experiences with these models.
Any information will be greatly appreciated.
","['machine-learning', 'image-processing', 'google-cloud']",
Is there a mathematical theory behind why MLP can classify handwritten digits?,"
I'm trying to really understand how multi-layer perceptrons work. I want to prove mathematically that MLP's can classify handwritten digits. The only thing I really have is that each perceptron can operate exactly like a logical operand, which obviously can classify things, and, with backpropagation and linear classification, it's obvious that, if a certain pattern exists, it'll activate the correct gates in order to classify correctly, but that is not a mathematical proof.
","['classification', 'proofs', 'multilayer-perceptrons', 'perceptron']","
The approximation theorem says you can approximate anything. But this is kind of meaningless in so far as you can do KNN and get an arbitrary approximation of your training data also.
Proving CNN correctly extract features is, I don't think possible. Or if it is, something involving VC theory is probably the best you can do.
"
Where can I find good tutorials on user tailored recommendation system for web?,"
I'm currently working on my uni project, but I have no idea where to start for the user tailored recommendation system on web. Where can I find a good guide on it, preferrably on languages like php and javascript. 
",['machine-learning'],
What is the difference between exhaustive nearest neighbor search and k-nearest neighbour search?,"
I have two lists of feature vectors calculated from pre-trained CNN for image retrieval task:
Query: FV_Q and Reference FV_R.
>>> FV_R.shape
(3450, 128)

>>> FV_Q.shape
(3450, 128)

I am a little confused between the concept of exhaustive nearest neighbor search and k-nearest neighbor search. 
In python, I use from sklearn.neighbors import KDTree to extract top k = 5 similar images from the reference database, given the query image!
Can somebody explain if there might be any similarities/differences between these two concepts? 
Am I making a mistake somewhere in my feature vector comparison?
","['convolutional-neural-networks', 'python', 'comparison', 'computer-vision', 'image-recognition']","
The exhaustive nearest neighbor search performs an exhaustive search of the nearest neighbor (i.e. the closest image or matches, depending on your application). Here an exhaustive search means that you will compare your query image with any other image, or, in the case of feature matching, you will compare every feature of the query image with every other feature of the target/reference image.
The $k$-nearest neighbor search finds the $k$ closest images. You typically do that by first building a KD-tree with your query (or target) image, so that to speed up the search for the $k$-nearest neighbors of your query image. 
(Btw, in case you are interested in the concept of feature matching, in OpenCV, the class that you can use to perform an exhaustive search for the matches between two images is the BFMatcher (which stands for Brute Force Matcher), while the class to perform the search with a KD-tree for the k-nearest neighbhours is FlannBasedMatcher).
"
How do we know that the algorithm has converged and ensures the highest possible reward?,"
I started learning about Q table from this blog post Introduction to reinforcement learning and OpenAI Gym, by Justin Francis, which has a line as below -

After so many episodes, the algorithm will converge and determine the optimal action for every state using the Q table, ensuring the highest possible reward. We now consider the environment problem solved.

The Q table was updated by Q-learning formula
Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action])
I ran 100000 episodes of which I got the following -
Episode 99250 Total Reward: 9
Episode 99300 Total Reward: 7
Episode 99350 Total Reward: 6
Episode 99400 Total Reward: 14
Episode 99450 Total Reward: 10
Episode 99500 Total Reward: 10
Episode 99550 Total Reward: 9
Episode 99600 Total Reward: 14
Episode 99650 Total Reward: 5
Episode 99700 Total Reward: 7
Episode 99750 Total Reward: 3
Episode 99800 Total Reward: 5

I don't know what the highest reward is. It does not look like it has converged. Yet, the following graph

shows a trend in convergence but it was plotted for a larger scale.
What should be the sequence of actions to be taken when the game is reset() but the ""learned"" Q table is available? How do we know that and the reward in that case?
","['reinforcement-learning', 'q-learning', 'rewards', 'convergence']","
Great answer by @Brale. I want to point out another thing. The plateau in a graph ---like the one you shared--- indicates convergence, and the converged value may not always be zero. In general, you need to verify the learned behavior by your algorithm. For instance, if you are using Gym's LunarLander environment, you should also double-check what is it that your algorithm has learned after converging. Put another way, your algorithm might converge, but not to the desired behavior.
"
Why does the KL divergence not satisfy the triangle inequality?,"
The KL divergence is defined as
$$D_{KL}=\sum_i p(x_i)log\left(\frac{p(x_i)}{q(x_i)}\right)$$
Why does $D_{KL}$ not satisfy the triangle inequality?
Also, can't you make it satisfy the triangle inequality by taking the absolute value of the information at every point?
","['proofs', 'variational-autoencoder', 'kl-divergence']",
"I need to select the image from a predefined dataset that are the closest to the input, is this possible or do I even need to use ML/AI?","
So as the title states, I have a set of images and I want to process input images and need to select the image that ""looks"" the most like the input image.
I know I've seen something similar where the code could guess who's face was in a picture, I guess I want something like that but for general images.
Sorry if this is a stupid question, but any suggestions or points at resources would be greatly appreciated.
","['classification', 'tensorflow', 'video-classification']","
You will only need to use ML or AI if 
a) the dataset is very big.
b) It is difficult to get the meaning or value of the image.(ex: group of ants, photos of stars in night sky)
And many more.
Below is the way I thought it can be done through a machine(with deeplearning, CNN)
1. Train the model
        arrange the images in dataset into clusters (the number of clusters can 
        be decided based on the criticality of the application, more critical the 
        application more should be the number of clusters) .

Predict about input 
    Then predict the nearest image in the dataset to the provided image.

I recently listened a podcast in which, a problem is approached similarly. The podcast is about youtube predictions and the result is to provide closest video from the youtube videos ( here the dataset).Below is the podcast.
    ( https://open.spotify.com/episode/6PcMtVXR58i7iu8kLH40Wd?si=feyu0LjESoiE479xHkaqSA )   
"
Why is the state-action value function used more than the state value function?,"
In reinforcement learning, the state-action value function seems to be used more than the state value function. Why is it so?
","['reinforcement-learning', 'comparison', 'value-functions']",
"In a neural network, can colors be used for neurons in place of floating points and would there be any benefit in doing so?","
Firstly, some context. I have been reading and watching videos on the subject for around 3 years, but I am still very much a beginner in machine learning and artificial intelligence. That said, I might not know what I'm even talking about here. So bear with me.
If I understand correctly, each node in a neural network (neuron) is represented by some floating point number between 0 and 1, that are arranged in layers and have corresponding weights. Right? While a color has RGB values, CMYK values, and HSV values that are all interrelated to each other.
My question is would there be any benefit to having each node represented by a color instead of a single floating point number?
My thinking is that each neuron could select any of the values (r, g, b, c, m, y, k, h, s, or v) contained within the color in some meaningful way, while the Alpha value could possibly represent the weight associated with that neuron.
Thoughts? Would it not work like that? Could you use it to have multiple congruent networks running on 3 different channels? Again, would there be any benefit to doing this than just using a single number? Or would it over-complicate (or even break) the network? Would it be useless?
Although I've also dabbled in Unity3D (which is how I got the idea in the first place), I'm too much of a beginner to know how to even begin an attempt at testing this myself.
","['neural-networks', 'artificial-neuron']",
Details on body measurements prediction,"
if someone want to do mobile app for body measurements prediction, please what are the necessary things to start with. I need details explanation on this.
",['deep-learning'],
Machine learning frameworks for esoteric languages [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



Is there a machine learning framework/library for any of the esoteric languages, such as the ones listed here ?
","['machine-learning', 'programming-languages', 'resource-request']",
What is the use of concatenate layer in CNN?,"
I am not asking what does concatenate layer does in general in point of mathematical operation. But at feature level, what significance does it provide. Does it helps removing false negatives or does it prevents over-fitting? Do give the reference of papers regarding this topic.
","['neural-networks', 'convolutional-neural-networks', 'training']",
How to encode board before input into the neural net?,"
Currently I'm working on an educational project (implementation of AlphaZero approach to different types of board games).
My biggest concern at the moment is how to encode board before input into the neural network?
For example, how can this be done for Kalah game?
Kalah board has two straight rows of six pits, and two large score-houses.
Each pit contains 6 seeds (so there are 72 seeds) 
","['neural-networks', 'deep-learning', 'reinforcement-learning', 'alphazero']",
How to solve the problem of variable-sized AST as input for a (convolutional) neural network model?,"
In my work I have a given source code for a module. From this module I generate an AST, whose size is dependent on the size of the module (e.g. more source code -> bigger AST). I want to train a neural network model which will learn a general structure of a module and be able to rate (on a scale of 0 to 1) how ""good"" a module is structure wise (if requires are at the beginning, followed by local functions, variables and finally returns). Now I have learnt that Convolutional NNs are quite convenient for this, but the problem I can't seem to solve is that they require a fixed sized input which I can't produce. If I add zero-padding then the outcome will be skewed and the accuracy will suffer. Is there a clear solution to this problem?
","['neural-networks', 'convolutional-neural-networks', 'graphs']","
You discovered already one solution for your problem: Zero-Padding.
There are two other common possibilities:

Using Recurrent NNsThis is often used at text processing, where you feed each word one after another into your model.
Using Recursive NNs (I wont recommend this for your use case)This method is also frequently used in word processing, but is more often applied in the semantic analysis of text. You reduce the text to the essential, until it has reached the desired length. However, information is lost in this process.

"
What is teacher forcing?,"
In the paper Neural Programmer-Interpreters, the authors use the teacher forcing technique, but what exactly is it?
","['neural-networks', 'machine-learning', 'recurrent-neural-networks', 'definitions', 'teacher-forcing']",
What kind of enemy to train a good RL-agents,"
So I want to create an RL-agent for two players-board game. I want to use a simple DQN for the first player (my RL-agent). Then, what kind of algorithm that should I use on the second player (my RL-agent's enemy)? 
I have three options in my mind:

a random agent that act randomly
a rule-based agent that acts by some defined rules
another RL-agent

I have tried the first and second options. When I use a random agent as an enemy, the first player gets a high score and wins easily. But I think it's not really smart as the enemy is a random agent. When I use the second option, the first player got difficulties to train itself, as it can't win any game.
what should I choose and why?
","['reinforcement-learning', 'intelligent-agent']",
How do I make my LSTM model more sensitive to changes in the sequence?,"
I have a many to one LSTM model for multiclass classification. For reference, this is the architecture of the model
    model.add(LSTM(147, input_shape=(1000, 147)))
    model.add(Dense(5, activation='softmax'))
    model.compile(loss='categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])

The model is trained in 5 types of sequences is able to effectively classify each sequence I feed into the model with high accuracy. Now my new objective is to combine these sequences together to form a new sequence. For e.g  I denote the elements from the class '1' with the sequence:
[1,1,1,1,1,1,1] So when I input the above sequence into the LSTM model for prediction, it classifies the sequence as class '1' with accuracy of 0.99
And I denote the elements from class '2' with the sequence:
[2,2,2]
LIkewise for the above sequence, the LSTM model will classify the sequence as class '2' with accuracy of 0.99
Now I combine these sequences together and feed it into the model :
 New sequence : [1,1,1,1,1,1,1,2,2,2]
 However the model does not seem to be sensitive to the presence of the class '2' sequence and still classifies the sequence as class '1' with accuracy of 0.99. 
How do I make the model more ""sensitive"", meaning that I would expect the LSTM model to still maybe predict class '1' but with a drop in accuracy? Or is the LSTM incapable of detecting the inclusion of class '2' sequences? 
Thanks. 
","['python', 'tensorflow', 'long-short-term-memory']",
What models will you suggest to use in Industrial Anomaly Detection and Predictive analysis on live streamed data?,"
I have been working on industrial data, that is fed live, I want to explore a few models which might suit for this the best.
The data are KPI data from the manufacturing Industry.
","['neural-networks', 'models', 'anomaly-detection']",
Can recovering a reward function using IRL lead to better policies compared to reward shaping?,"
I am working on a research project about the different reward functions being used in the RL domain. I have read up on Inverse Reinforcement Learning (IRL) and Reward Shaping (RS). I would like to clarify some doubts that I have with the 2 concepts.
In the case of IRL, the goal is to find a reward function based on the policy that experts take. I have read that recovering the reward function that experts were trying to optimize, and then finding an optimal policy from those expert demonstrations has a possibility of resulting in a better policy (e.g. apprenticeship learning). Why does it lead to a better policy?
","['reinforcement-learning', 'deep-rl', 'rewards', 'reward-shaping', 'inverse-rl']","
Inverse Reinforcement Learning (IRL) is a technique that attempts to recover the reward function that the expert is implicitly maximising based on expert demonstrations. When solving reinforcement learning problems, the agent maximises a reward function specified by the designer, and in the process of reward maximisation, accomplishes some task that it had set out to do. However, reward functions for certain tasks are sometimes difficult to specify by hand. For example the task of driving takes into consideration many different factors such as the distance of the car in front of him, the road conditions and whether or not the person needs to get to his destination quickly. A reward function can be hand specified based on these features. However, when there exists trade offs between these different features, it is difficult to know how to specify the different desiderata of these tradeoffs.
Instead of specifying the trade offs manually, it would be easier to recover a reward function from expert demonstrations using IRL. Such a reward function can lead to better generalisations to unseen states as long as the features of driving do not change.
In the case where reward shaping fails to learn a task (such as driving), it would be better to have someone demonstrate a task and learn a reward function from these demonstrations. Solving the MDP with the learnt reward function will thus yield a policy that should resemble the demonstrated behaviour. The reward function learnt should also generalise to unseen states and the agent acting in unseen states should be able to perform actions that an expert would take when he is placed in the same conditions, assuming that the unseen states come from the same distribution as the training states.
While Reward Shaping might be able to perform the same task as well, IRL might be able to do better, based on some performance metric that will differ from problem to problem.
"
Will AI always depend on models and thus approximations?,"
In section 3 of the paper The Limits of Correctness (1985) Brian Cantwell Smith writes

When you design and build a computer system, you first formulate a model of the problem you want it to solve, and then construct the computer program in its terms.

He then writes

computers have a special dependence on these models: you write an explicit description of the model down inside the computer, in the form of a set
  of rules or what are called representations - essentially linguistic formulae encoding, in the terms of the model, the facts and data
  thought to be relevant to the system's behavior. It is with respect to these representations that computer systems work. In fact, that's really what computers are (and how they differ from other machines): they run by manipulating representations, and representations are always formulated
  in terms of models. This can all be summarized in a slogan: no computation without representation.

And then he says

Models have to ignore things exactly because they view the world at a level of abstraction

He then writes in section 7

The systems that land airplanes are hybrids - combinations of computers and people - exactly because the unforeseeable happens, and because what
  happens is in part the result of human action, requiring human interpretation

As quoted above, computers depend on models, which are abstractions (i.e. they ignore a lot of details), which are written inside the computer. Therefore, the true world cannot really be encoded into an algorithm, but only an abstraction and thus simplification of the world can.
So, will AI always depend on models and thus approximations? Can it get rid of or overcome this limitation?
","['philosophy', 'agi', 'models', 'papers']","
AI is internally limited by model and externally limited by the environment.
Humans are externally limited by the environment but not necessarily internally limited by a computable model (as AI is).
So, humans may possess certain skills (e.g. creativity) that an AI may never possess. I had previously asked a related question Are human brain processes, like creativity, intuition or imagination, computable processes?.
Which research work supports my claims?
Brian Cantwell Smith says that there is no computation without representation (a model).
In the article The Brain Is Not Computable, Miguel Nicolelis, a top neuroscientist at Duke University, also says

The brain is not computable and no engineering can reproduce it
You can't predict whether the stock market will go up or down because you can’t compute it.
You could have all the computer chips ever in the world and you won't create a consciousness.
That's because its most important features are the result of unpredictable, nonlinear interactions among billions of cells

"
LSTM model on different time scales,"
I am a newbie to machine learning. I have an LSTM model that predicts the next output n+1
time 1, params 1, output 1
time 2, params 2, output 2
time 3, params 3, output 3
.
.
time n, params n, , output n
time n+1 --> predicts output n+1
 Here the times are all in minutes, so I can predict the next output in the series which is going to be the next minute. My question is that what if I want to predict the next 5 minutes. One solution was to throw out all the data except in steps of 5 minutes so the next step is automatically would be 5 minutes. This is clearly a waste of all the data that I have gathered. Can you please recommend what I can do about the prediction on different time scales? 
",['long-short-term-memory'],"
What you could do is just try and bypass the rest of the network after the LSTM if it isn't the 5'th minute.
Depending on your framework this can be easy or a painstakingly task compared to the alternative.
The alternative is just running and throwing away the output that isn't the next 5'th minute. While the last may seem inefficient it's rather easy to implement and takes just a bit more execution time. If execution time isn't a problem it's the easiest to get started with, if it doesn't work for your task you can always change it.
"
Pretrained Models for Keyword-Based Text Generation,"
I'm looking for an implementation that allows me to generate text based on a pre-trained model (e.g. GPT-2).
An example would be gpt-2-keyword-generation (click here for demo). As the author notes, there is

[...] no explicit mathematical/theoetical basis behind the keywords
aside from the typical debiasing of the text [...]

Hence my question: Are there more sophisticated ways of keyword-based text generation or at least any other alternatives?
Thank you
","['transformer', 'gpt', 'text-generation']",
Tversky Loss paper implementation: Recall/Precision do not improve as stated,"
I have been trying to implement this paper and I am very much intrigued. I am working on a medical image problem where I have to segment very small specimens on Whole Slide Images (gigapixel resolution). Therefore my dataset is highly unbalanced and I am having a high false positives rate. 
I did my research and found that paper that describes the implementation of Tversky Loss and Focal Tversky Loss. It also describes some modifications to the network architecture which I am postponing for now. 
I implemented the loss (Pytorch) and ran some experiments with several alpha/beta combinations. Well, the results are easy to understand: higher alpha results in higher precision and a lower beta increases the recall and pushes the precision down. Basically, what this loss is doing is balancing my recall and precision, only. That is good, I can solve my False Positives issue but since this is a medical problem, a good recall is mandatory. In the paper, the results show that there is an improvement in the Precision/Recall and I cannot understand how is that possible and how I cannot replicate that. I am just weighing false positives and penalizing them, it does not seem enough to improve the model overall. 
Regards

","['objective-functions', 'image-segmentation']",
Noise Cancellation on live audio stream,"
I want to build an application which takes a live audio from source (mic) and filtering the noise (unwanted sounds like chattering, traffic noises) and fetch into an application for further processing.
I want to apply Machine Learning Framework (TensorFlow, Keras) and Deep Learning neural Networks (i.e RNN) for filtering the noise from the audio. I want to do this in a real time environment. My inference device will be a Nvidia Jetson Device. Please guide me where I can find the related documents and how to proceed with the project.
If there is any solution available in any website please refer the link.
","['machine-learning', 'deep-learning', 'audio-processing']",
Curiosity Driven Learning affect optimal policy,"
I am trying to understand some of the different approaches used to overcome  sparse rewards in a reinforcement learning setting for a research project. Particularly, I have looked at curiosity driven learning, where an agent learns an intrinsic reward function based on the uncertainty of the next state that the agent will end up in as he takes action a in state s. The greater the uncertainty of the next state, the higher the rewards. This will incentive agent's to be more exploratory and it is used particularly in some games where a huge number of steps is needed before the agent reaches the terminal state where is he only then rewarded.
The curiosity driven approach as demonstrated in this paper:
https://pathak22.github.io/noreward-rl/ is able to learn faster than if a 0 rewards were used for each state, action. 
To my knowledge, using different reward functions will affect the optimal policy obtained. Would curiosity driven learning therefore lead to a different policy as compared to whether a 0 reward was used ? Assume that for a 0 immediate reward system, it is able to derive a policy that reaches the goal state. Which of these 2 policies will be more optimal ?
","['reinforcement-learning', 'rewards']",
Is there a reason to use TensorFlow over PyTorch for research purposes?,"
I've been using PyTorch to do research  for a while and it seems to be quite easy to implement new things with. Also, it is easy to learn and I didn't have any problem with following other researchers code so far.
However, I wonder whether TensorFlow has any advantage over PyTorch. The only advantage I know is, it's slightly faster than PyTorch. 
In general, does TensorFlow have any concrete advantages over PyTorch apart from performance, in particular for research purposes?
","['deep-learning', 'tensorflow', 'comparison', 'research', 'pytorch']",
Should I prefer the model with the lowest validation loss or the highest validation accuracy to deploy?,"
I trained a ResNet20 on Cifar10 and obtained the following learning curves.

From the figures, I see at epoch 52, my validation loss is 0.323 (the lowest), and my validation accuracy is 89.7%.
On the other hand, at the end of the training (epoch 120), my validation loss is 0.413 and my validation accuracy is 91.3% (the highest).
Say I'd like to deploy this model on some real-world application. Should I prefer the snapshotted model at epoch 52, the one with lowest validation loss, or the model obtained at the end of training, the one with highest validation accuracy?
","['neural-networks', 'deep-learning', 'applications', 'overfitting', 'early-stopping']","
In highly imbalanced classification problems, the highest accuracy can often be achieved simply by assigning the majority class to all observations. This is why learning algorithms do not maximize classification accuracy but minimize a loss function. 
Fundamentally, loss functions capture how much you ""lose"" when there is a difference between the statistic you want to estimate and the estimate itself. The appropriate loss function is not given by nature, but is provided by you.
With this in mind, it is not quite accurate to say that the highest accuracy is reached after 120 epochs: it is merely the maximum accuracy achieved by the algorithm so far. Unless you run the algorithm for longer, there is no way to say if this is even a local maximum. For example, assigning every observation to the majority class may well achieve a higher accuracy than that achieved at 120 epochs. The only significance of the 120 epochs is therefore that that is how long you ran the algorithm for.
Given these considerations, it makes far more sense to stop at around 50 epochs, when your loss function is minimized.
"
Why is the space-complexity of greedy best-first search is $\mathcal{O}(b^m)$?,"
I am reading through Artificial Intelligence: Modern Approach and it states that the space complexity of the GBFS (tree version) is $\mathcal{O}(b^m)$.
While I am reading, at some points, I found GBFS similar to DFS. It expands the whole branches and goes after one according to the heuristic function. It doesn't expand the rest like BFS. Perceiving this as similar to what depth-first search does, I understand that the worst time complexity is $\mathcal{O}(b^m)$. But I don't understand the space complexity. 
Shouldn't it be the same as DFS, $\mathcal{O}(bm)$, since it only will be expanding $b*m$ nodes during the search in one path?
","['search', 'time-complexity', 'computational-complexity', 'best-first-search', 'space-complexity']","
I was struggling with the same question. This is what I came up with after thinking it through.
With depth-first-search, you backtrack to a node that is a non-expanded child of your parent (or the parent of the parent when your parent has no more non-expanded children (and so on going up the tree)). So the space complexity is limited by your ancestors and the children of these ancestors. Which translates in m*b where m is the max path length (so max number of ancestors) and b is the branching factor (number of children per ancestor).
With greedy search when you backtrack you can jump to any evaluated but unexpanded node, you passed going down on paths earlier. So the algorithm, when backtracking, can make pretty random jumps throughout the tree leaving lots of sibling nodes unexpanded. You will have to remember the value of the evaluation function for all these unexpanded nodes though because possibly they are next up when backtracking occurs. So in a theoretical very worst-case scenario that could mean that almost the whole tree needs to be remembered. Hence O(b^m).
I know there are still gaps in the reasoning but intuitively this makes me understand it best.
"
How does batch size affect model size?,"
I'm suffering from a significant brain fart while trying to get my head around how does batch size affect overall model size e.g for CNNs. Does it serve as an additional dimension for all the weight tensors? 
Considering:

VGG16 model 
batch_size of 16 
image size of 224x224x3
conv_1 being the initial 1x1 convolution with stride 1 and 3:64 channels mapping

The input will be a tensor of [16, 224, 224, 3] shape. Will the output of convolution layer be [16, 224, 224, 64] and therefore - will all the weights have additional 'batch size' dimension and thus - impose a linear increase of model size with respect to the batch size?
","['neural-networks', 'machine-learning', 'training', 'stochastic-gradient-descent']",
Hand-Signs Recognition using Deep Learning Convolutional Neural Networks,"
I am developing a CNN model to recognize 24 hand-signs of American Sign Language. I have 2500 Images/hand-sign. The data split is:
Training = 1250 Images/hand-sign
Validation = 625 Images/hand-sign
Testing = 625 Images/hand-sign
How should I proceed with training the model?:
1. Should I develop a model starting from fewer hand-signs (like 5) and then increase them gradually?
2. Should I start models from scratch or use transfer learning (VGG16 or other)
Applying data augmentation, I did some tests with VGG16 and added a dense classifier at the end and received these accuracies:
Train: 0.87610877
Validation: 0.8867307
Test: 0.96533334
Accuracy and Loss Graph
Test parameters:
NUM_CLASSES = 5
EPOCHS = 50
STEPS_PER_EPOCH = 125
VALIDATION_STEPS = 75
TEST_STEPS = 75
Framework = Keras, Tensorflow
OPTIMIZER = adam
Model:
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(512, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),

    Flatten(),
    Dense(512, activation='relu'),

    Dense(NUM_CLASSES, activation='softmax')
])

If I try images with slightly different background and predict the classes (predict_classes()), I do not get accurate results. Any suggestions on how to make the model robust?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'classification', 'keras']","
I feel your problem might not be with the model itself but with the dataset. If you only have $2500$ images for $24$ labels (hand-signs) that gets you roughly $104$ images per label. This is very little for the models I train (~$80K$ images in the smallest of cases). In my view you got a really decent accuracy at validation and test time for the size of your dataset.
But answering your questions:

Starting from few labels and extending is usually helpful when your model is too deep and suffers from convergence problems. Your model is simple enough not to suffer those problems so I would go learning the $24$ labels at once.
Transfer learning can help a lot at reducing training times. For example, if you start from a VGG classifier that detects hands there is a good chance that the weights of your convolutional layers are already almost configured for your use case.

Generally speaking, the easiest way to increase the accuracy of your model is to use one of these 2 methods:

Increase the dataset: I am not sure if it is possible in your case, maybe you can use image augmentation (rotation, zooming in/out, changes in the color space...).
Increase the depth of your model (provided that you have a big dataset).

If you already fulfilled those items, then you can go and make changes to the model architecture or loss function. Looking at your model and from the top of my head I would try to add Batch-Normalization for the convolutional layers.
Hope this helps a bit :)
"
What is an end-to-end AI project?,"
I often read about the so-called end-to-end AI (or analytics) projects, but I couldn't find a definition of it. What is an end-to-end AI project?  Can someone explain what is meant/expected when someone asks you ""Have you already implemented an end-to-end AI project""?
","['terminology', 'definitions']",
