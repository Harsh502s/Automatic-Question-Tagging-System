Head,Body,Tags,First Answer
Immediate reward received in Atari game using DQN,"
I am trying to understand the different reward functions modelled in a reinforcement learning problem. I want to be able to know how the temporal credit assignment problem, (where the reward is observed only after many sequences of actions, and hence no immediate rewards observed) can be mitigated. 
From reading the DQN paper, I am not able to sieve out how the immediate rewards are being modelled when $Q_{target}(s,a; \theta) = r_s + argmax_aQ(s',a'; \theta)$. What is $r_s $ used in the case where the score has not changed ? Therefore what is the immediate rewards being modelled for temporal credit assignment problems in atari game ? 
If $r_s$ is indeed 0 until score changes, would it affect the accuracy of the DQN ? it seems like the update equation would not be accurate if you do not even know what is the immediate reward if you take that action.
What are some of the current methods used to solve the temporal credit assignment problem ?
Also, I can't seem to find many papers that address the temporal credit assignment problem
","['reinforcement-learning', 'deep-rl', 'rewards']","To answer your questions in order:What is $r_s $ used in the case where the score has not changed ?It is $0$.Therefore what is the immediate rewards being modelled for temporal credit assignment problems in atari game ?Rewards can be re-modelled to aid speed of learning. This is called ""reward shaping"", and is typically done by domain experts who can adjust numbers to reward known good intermediate states and actions.For DQN Atari, this was not done. Instead, the researchers performed a reward normalisation/scaling so that games which used moderate scoring system in single digits could be handled by the same neural network approximator as games that handed out thousands of points at a go.Using sparse rewards is standard practice in reinforcement learning, and the credit assignment problem is solved to some degree by all reinforcement learning methods. Essentially the value functions work as a prediction mechanism theoretically whatever the reward sparsity, so if they are correct, thay can be used to drive policy whether the next reward is 1, 50 or 1000 time steps away. The reward backup updates in everything from Value Iteration, through Monte Carlo Control, SARSA, Q-Learning and Actor-Critic all backup values to states/actions seen in earlier time steps. This value backup is a basic mechanism that addresses credit assignment in principle. The credit assignment problem is then a matter of degree and difficulty of different environments, such that sometimes it is readily solved, and other times it is a major hurdle.In the case of video games, especially older arcade games, it is often not a very hard part of the problem. The games are designed to reward human players by incrementing scores frequently, with very many sub-goals already within the game. In fact this is one of the attractions of video games as toy environments for developing new algorithms.For example the classic space invaders does not simply score +1 for surviving a wave of enemies, but adds points for every player missile that hits. Although the score does not increment on every frame, the reward sparsity is relatively low for games like this, and simple single-step Q learning with experience replay can solve the credit assignment problem readily for that environment (experience replay does help a little with credit assignment). This was what was demonstrated with the original DQN Atari paper, there were no extra allowances made for reward sparsity.If $r_s$ is indeed 0 until score changes, would it affect the accuracy of the DQN ?Not directly, the DQN predicts future expected rewards and can in principle account for delay and sparsity in its estimates. However, if this becomes very sparse you get two problems:Discovering the positive rewards within the environment may take a long time, and may require more advanced techniques, such as reward shaping to encourage searching behaviour (a small negative reward per time step) or ""curiousity""Credit assignment becomes much harder as the possible number of combinations that could of contributed to success can grow exponentially with temporal distance between rewards. Resolving whch ones are important, especially early in a trajectory leading to reward, can take many samples.What are some of the current methods used to solve the temporal credit assignment problem ?As noted above, this is core problem in RL, so there are many approaches in the literature. Some standard approaches are:Background planning as used by DynaQ, or experience replay. This re-evaluates states and actions seen before whilst using latest estimates, and can backup values to where important decisions are made within a trajectory. Prioritised experience replay helps even more by focusing on updates that make the most difference to the current estimates.TD($\lambda$) with eligibility traces. Eligibility traces are intuitively a credit-assignment mechanism, they track state features that were active recently and multiply value updates based on the trace vector. Again, this causes state or state action values to backup to earlier parts of trajectories faster.Reward shaping as discussed above. In some research settings this might be seen as ""cheating"" - for instance when facing a standardised environment test for developing algorithms, adding in domain knowledge to help the agent just demonstrates the core algorithm is weaker than claimed. However, when the challenge presented is to solve an environment with the best agent possible, it is fine to use any knoweldge (or of course not to use RL at all)."
Does this tutorial use normalization the right way?,"
There is this video on pythonprogramming.net that trains a network on the MNIST handwriting dataset. At ~9:15, the author explains that the data should be normalized. 
The normalization is done with
x_train = tf.keras.utils.normalize(x_train, axis=1)
x_test = tf.keras.utils.normalize(x_test, axis=1)

The explanation is that values in a range of 0 ... 1 make it easier for a network to learn. That might make sense, if we consider sigmoid functions, which would otherwise map almost all values to 1.
I could also understand that we want black to be pure black, so we want to adjust any offset in black values. Also, we want white to be pure white and potentially stretch the data to reach the upper limit.
However, I think the kind of normalization applied in this case is incorrect. The image before was:

After the normalization it is

As we can see, some pixels which were black before have become grey now. Columns with few black pixels before result in black pixels. Columns with many black pixels before result in lighter grey pixels.
This can be confirmed by applying the normalization on a different axis:

Now, rows with few black pixels before result in black pixels. Rows with many black pixels result in lighter grey pixels.
Is normalization used the right way in this tutorial? If so, why? If not, would my normalization be correct?
What I expected was a per pixel mapping from e.g. [3 ... 253] (RGB values) to [0.0 ... 1.0]. In Python code, I think this should do:
import numpy as np
import imageio
image = imageio.imread(""sample.png"")
image = (image - np.min(image))/np.ptp(image)

","['python', 'keras', 'batch-normalization', 'handwritten-characters']",
Intellectual property in the age of Industry 4.0,"
I am looking for specific references describing guidance principles around the interplay between IP (intellectual property) and Artificial Intelligence algorithms. For example, Company A has a large dataset and Company B has advanced algorithmic capabilities (assume near-AI). How might Company A protect itself in a joint partnership with company B? Apologies if AI SE is not the place for this - any suggestions for the right site?
","['learning-algorithms', 'legal', 'business']",
Are there names for neural networks with a well-defined layer or neuron characteristics?,"
Are there names for neural networks with a well-defined layer or neuron characteristics? 
For example, a matrix that has the same number of rows and columns is called a square matrix.
Is there an equivalent for classifying different neural network structures. Specifically, I am interested if there is a name for a neural network with x number of layers, but each layer has the same number of neurons?
","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'terminology', 'feedforward-neural-networks']","Neural networks (NNs) are usually classified into feed-forward (i.e. NNs with feedforward connections), recurrent (i.e. NNs with recurrent connections) and convolutional (i.e. NNs that perform a convolution or cross-correlation operation). The term multi-layer NN may also be used to refer to feed-forward neural networks or, in general, neural networks with multiple (hidden) layers. There are also the perceptrons, which do not have hidden layers (i.e. the inputs are directly connected to the outputs). You may still divide neural networks into classifiers (i.e. the outputs and labels are discrete) and regressors (the outputs and labels are numerical). Furthermore, you may classify them into generative models (e.g. the VAE) or discriminative models.By analogy with linear algebra concepts, each layer of a feedforward neural network (FFNN) can be seen as a linear operation followed by an element-wise application of a linear or non-linear activation function.$$
\mathbf{o}^{l} = \sigma \left(\mathbf{a}^{l} \mathbf{W}^{l} + \mathbf{b}^{l}\right)
$$where $\sigma$ is the activation function, $\mathbf{a}^{l}$ the inputs to the layer $l$ and $\mathbf{W}^{l}$ the parameters of the layer $l$ (similar to the parameters or coefficients in linear regression) and $\mathbf{b}^{l}$ is the bias vector (a scalar bias for each neuron) of layer $l$. $\mathbf{o}^{l}$ will then be $\mathbf{a}^{l+1}$ (i.e. the input to the next layer). A recurrent neural network (RNN) performs a slightly more complex operation.$$
\mathbf{o}^{l}_t = \sigma \left(\mathbf{a}^{l}_t \mathbf{W}^{l} + \mathbf{o}^{l}_{t-1} \mathbf{R}^{l} + \mathbf{b}^{l}\right)
$$where, in addition to the matrix $\mathbf{W}^{l}$ associated with the feedforward connections, it also uses another matrix $\mathbf{R}^{l}$ associated with the recurrent connections (i.e. cyclic or loopy connections of the neurons), which is multiplied by the output of the same layer but at the previous time step. $\mathbf{o}^{l}_t$ may actually just be the state of the recurrent layer, which is then used to compute the actual output of the layer, but, for simplicity, you can ignore this. Furthermore, note that there are more complex recurrent architectures, but this is the basic idea.A convolutional neural network (CNN) performs the convolution (or cross-correlation) operation. If you are familiar with signal processing, e.g. kernels, convolution, etc., then you can view a CNN as performing a convolution (or cross-correlation) operation. It's actually possible to view the convolution as a matrix multiplication operation, but the details can easily become cumbersome and tedious to explain in an answer. A CNN may also perform other types of operations (such as downsampling) and it can also be composed of a feedforward part (usually, the last layers of a CNN are feedforward layers), but a CNN is a CNN because it performs the convolution (or cross-correlation).In all cases, the matrices do not necessarily take any particular form (e.g. they are not necessarily square matrices). The dimensionality of the matrices depends on the number of layers and connections in the network, which can vary depending on many factors (e.g. the need for recurrent connections because they are useful for sequence modeling). These matrices (along with the biases) are the learnable parameters of the networks, but the learned values of these matrices highly depend on your data, the way you initialize them before learning, the architecture of the network, the learning algorithm, etc. In the case of the FFNN, if the previous layer $l-1$ has the same number of neurons as the current layer $l$, then $\mathbf{W}^{l}$ is a square matrix. AFAIK, there is no name for a neural network with the same number of neurons for each layer. It could be called a rectangular neural network (but this is a name I've just come up with).To conclude, there are many different neural networks and taxonomies for neural networks, so it is impossible to list or discuss them all in an answer, but, nowadays, the subdivision into feedforward, recurrent and convolution is the most common and general. See e.g. the paper A Taxonomy for Neural Memory Networks if you are interested in a more detailed taxonomy for memory-based neural networks (e.g. recurrent neural networks). "
Why does the discrepancy measure involve a supremum over the hypothesis space?,"
I am referring specifically to the disc defined by Kuznetsov and Mohri in https://arxiv.org/pdf/1803.05814.pdf

This is a kind of worst case path dependent generalization error. But what is the intuitive way of seeing why a worst case is needed? I am probably missing something or reading something incorrectly. 
","['computational-learning-theory', 'generalization']",
LSTM implementation in KERAS [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



I would like to build an LSTM to predict the correct words order given a sentence. My dataset is composed of sentences, where each sentence has a variable number of words (each word is embedded). The dataset then is an array of matrices, where each matrix is an array of embedded words.
Now, I'm looking to implement it with Keras but I'm not sure how to fit the necessary parameters wanted by the LSTM layer in Keras, like timesteps and batch_size. 
Reading on the web, I notice that timesteps is the length of the sequence, so in my case I believe that corresponds to the length of the sentence. But I want to train my LSTM with one sentence at a time, so would the batch_size be 1?
","['keras', 'long-short-term-memory']","As you read, in keras the input dimensions for a LSTM layer are:  (batch_size, timesteps, input_dim). Where:The nice thing about Keras is that you can train with an specific batch size, say batch_size=16, that: helps the model convergence (because it averages the loss of the 16 sentences prediction) and boosts the speed (since the weights are updated only once every 16 sentences).But then you can infer (or do the predictions) with batch_size=1, meaning, one sentence at a time.So, if you want to train, specifically with one sentence at a time: batch_size=1. But if you want to take the advantage of using mini-batch then use batch_size=16 or higher for training and batch_size=1 for inference.BONUS: Keras allows for dynamic batch size change in the models. When a dimension is dynamic in Keras has the value of None, that is why when you do model.summary() you can see: (batch_size, timesteps, input_dim) = (None, 40, 100). This None allows for different batch_size in training and inference"
Binarize ConvNet Feature vector [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



Given a pre-trained CNN model, I extract feature vector of 3450 reference images FV_R as follows:
FV_R = [       [-8.2, -52.2, 9.07, -1.1, -0.08, -9.1, ........, -4.11], 
               [7.8, -3.8, 6.4, -4.27, -2.2, -5.0, ............., 3.6], 
               [-1.2, -0.8, 49.3, 1.73, -1.74, -7.1, ..........., 2.41],
               [-1.2, -.8, 49.3, 0.6, -1.24, -1.04, .........., -2.06],
               .
               .
               .
               [-1.2, -.8, 49.3, 12.77. -2.2, -5.0, .........., -51.1]
       ]

and FV_Q for 1200 query images : 
FV_Q = [       [-0.13, 2.6, -3.7, -0.5, -1.02, -0.6, ........, -0.11], 
               [0.3, -3.8, 6.4, -1.6, -2.2, -5.0, ............., 0.97], 
               [-6.4, -0.08, 8.0, 7.3, -8.07, -5.6, ..........., 0.01],
               [-6.09, -.8, 0.5, -8.9, -0.74, -0.08, .........., -8.9],
               .
               .
               .
               [-1.2, -.8, 49.3, 12.77. -2.2, -5.0, .........., -51.1]
       ]

The size info:
>>> FV_R.shape
(3450, 64896)

Query images:
>>> FV_Q.shape
(1200, 64896)

I would like to binarize the CNN feature vectors (descriptors) and calculate Hamming Distance. I am already aware of this answer to probably use np.count_nonzero(a!=b)(if a.shape == b.shape) but does anyone know a  method to binarize a feature vector with different size?
Cheers,
","['convolutional-neural-networks', 'python', 'feature-selection']",I fixed the problem as follows:
Choosing Data Augmentation smartly for different application,"
I'm trying to understand the role of data augmentation and how it can affect the performance/accuracy of a deep model. My target application is fire detection (on video frames), with almost 15K positive and negative samples, and I was using the following data augmentation techniques. Does using ALL the followings always increase the performance? Or we have to choose them somehow smartly given our target application?
rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,zoom_range=0.2, horizontal_flip=True

When I think a bit more, fire is always straight up, so I think rotation or shift might in fact worsen the results, given that it makes the image sides stretch like this, which is irrelevant to fires in video frames. Same with rotation. So I think maybe I should only keep zoom_range=0.2, horizontal_flip=True and remove the first three. Because I see some false positives when we have a scene transition effect in videos.
Is my argument correct? Should I keep them or remove them?


","['deep-learning', 'training', 'object-detection', 'data-preprocessing', 'accuracy']","In fact, choosing smartly the values of the image augmentation can help the performance of your system. Where I work we developed an object detector for cars. We had the following image augmentation parameters:At first we just combined all the effects (within a certain range of change per effect) randomly in each augmented image. But then we did an study for our use case and we found that our model performed better with: no aspect ratio deformation and no change in color space. After giving it a bit of a thought it really made sense for our use case because it is a detector for a very specific type of car. However, although the improvements accomplished by optimizing the parameters of image augmentation boosted the performance of the model, the improvements were not huge, so in the end it did not really worth the effort. To give some numbers it improved $5\%$ in our custom test metrics, and we were expecting something around $10\%$ so from that moment on we left the specific image augmentation parameters but stopped thinking too much about it.So, in conclussion, does it helps? yes, how much? well, that would depend on the use case. For us, it turned out it did not improve as much as we wanted.Hope this helps :)"
Can GraphRNN be used with very large graphs?,"
In the GraphRNN paper, the authors only implement the algorithm up to a graph size of 2k nodes. Would this still work on much larger graphs (on the order of $10^7$)? Or would the computation just become too substantial?
","['recurrent-neural-networks', 'papers', 'geometric-deep-learning']",
How to automatically detect and correct false information in columnar data?,"
I'm working on data cleaning and I'm stuck. I have a data set with 3 columns: id, age, and weight.
Supposing I have an entry:
id:1 | age:3 (years)  | weight: 150 (kg)

How can we detect that the information is wrong, assuming I have a thousand lines? 
And how can I correct it (using Python)?
Is there any function in Python that I can use or should I use machine learning techniques?
",['data-preprocessing'],
Sign Language to Speech conversion,"
Is there any solution about sign language to speech conversion for mobiles? Can anyone suggest me the flow and tools so that I may implement the solution for mobiles?
","['neural-networks', 'natural-language-processing']",
How to represent a state in a card game environment? (Wizard),"
We are attempting to build an AI that manages to play the cardgame Wizard. So far er have a working network (based on the YOLO object-detection) that is abled to detect which cards are played. When asked it returns the color and rank of the cards on the table. 
But now when starting to build an agent for the actual training I just cant figure out how to represent the states for this game.
In each round, each player gets the amount of cards represented by the round(one card in round one, two in round two and so on). Based on that the players estimate how many tricks they will win in this round. With ending the round the players calculate their points w.r.t their estimation.
So the agent have to estimate its future tricks and have to play depending on that strategy. So how do I encode that into a form that a neural network can work with? 
","['reinforcement-learning', 'q-learning']",
Image-to-Image Regression for GO territory classification,"
I'm trying to implement a neural network that is able to generate an image indicating territory occupation given a board state for GO (a strategy board game). Input images are 19x19x1 grayscale images, with white pixels indicating white pieces, black pixels indicating black pieces, and gray pixels indicating unoccupied areas. Output images are 19x19x1 grayscale images with white pixels indicating white territory, black pixels indicating black territory, and gray areas indicating unassigned territories. A sample input and desired output image is as follows:


The images are quite small, so just to give an overview of trends I noticed: - Pixels surrounded by pixels of opposite colors are 'captured' pieces and therefore part of opponent territory - Two 'eyes' or closed groups of pieces comprising at least two open intersections are invincible or confirmed territory
While I'm not looking for exact specifications of network layers etc., I was hoping I could be given some direction as to what type of network to use, and what it should comprise. Looking at MATLAB documentation, I've found info about semantic segmentation, and autoencoder networks but neither of these seem particularly helpful. I know the question is a little broad, but I just need some direction more than anything. This kind of image recognition problem is a first for me.
","['neural-networks', 'image-recognition', 'go']",
Are there reinforcement learning algorithms not based on Markov decision processes?,"
Are all RL algorithms based on the MDP? If not, could you give examples of some which aren't? I've looked elsewhere, but I haven't seen it explicitly said. 
","['reinforcement-learning', 'markov-decision-process']",
Is a non-linear activation function needed if we perform max-pooling after the convolution layer?,"
Is there any need to use a non-linear activation function (ReLU, LeakyReLU, Sigmoid, etc.) if the result of the convolution layer is passed through the sliding window max function, like max-pooling, which is non-linear itself? What about the average pooling?
","['deep-learning', 'convolutional-neural-networks', 'activation-functions', 'pooling', 'max-pooling']","Let's first recapitulate why the function that calculates the maximum between two or more numbers, $z=\operatorname{max}(x_1, x_2)$, is not a linear function.A linear function is defined as $y=f(x) = ax + b$, so $y$ linearly increases with $x$. Visually, $f$ corresponds to a straight line (or hyperplane, in the case of 2 or more input variables).If $z$ does not correspond to such a straight line (or hyperplane), then it cannot be a linear function (by definition). Let $x_1 = 1$ and let $x_2 \in [0, 2]$. Then $z=\operatorname{max}(x_1, x_2) = x_1$ for all $x_2 \in [0, 1]$. In other words, for the sub-range $x_2 \in [0, 1]$, the maximum between $x_1$ and $x_2$ is a constant function (a horizontal line at $x_1=1$). However, for the sub-range $x_2 \in [1, 2]$, $z$ correspond to $x_2$, that is, $z$ linearly increases with $x_2$. Given that max is not a linear function in a special case, it can't also be a linear function in general.Here's a plot (computed with Wolfram Alpha) of the maximum between two numbers (so it is clearly a function of two variables, hence the plot is 3D).Note that, in this plot, both variables, $x$ and $y$, can linearly increase, as opposed to having one of the variables fixed (which I used only to give you a simple and hopefully intuitive example that the maximum is not a linear function).In the case of convolution networks, although max-pooling is a non-linear operation, it is primarily used to reduce the dimensionality of the input, so that to reduce overfitting and computation. In any case, max-pooling doesn't non-linearly transform the input element-wise.The average function is a linear function because it linearly increases with the inputs. Here's a plot of the average between two numbers, which is clearly a hyperplane.In the case of convolution networks, the average pooling is also used to reduce the dimensionality.To answer your question more directly, the non-linearity is usually applied element-wise, but neither max-pooling nor average pooling can do that (even if you downsample with a $1 \times 1$ window, i.e. you do not downsample at all). Nevertheless, you don't necessarily need a non-linear activation function after the convolution operation (if you use max-pooling), but the performance will be worse than if you use a non-linear activation, as reported in the paper Systematic evaluation of CNN advances on the
ImageNet (figure 2)."
Why can't VAE do sequence to sequence name generation?,"
I'm working on research in this sector where my supervisor wants to do cannonicalization of name data using VAEs, but I don't think it's possible to do, but I don't know explicitly how to show it mathematically. I just know empirically that VAEs don't do good on discrete distributions of latents and observed variables(Because in order to do names you need your latent to be the character at each index and it can be any ASCII char, which can only be represented as a distribution). So the setup I'm using is a VAE with 3 autoencoders, for latents, one for first, middle and last name and all of them sample each character of their respective names from the gumbel-softmax distribution(A form a categorical that is differentiable where the parameters is a categorical dist). From what I've seen in the original paper on the simple problem of MNIST digit image generation, the inference and generative network both did worse as latent dimension increased and as you can imagine the latent dimension of my problem is quite large. That's the only real argument for why this can't work, that I have. The other would have been it's on a discrete distribution, but I solved that by using a gumbel softmax dist instead. 
This current setup isn't working at all, the name generations are total gibberish and it plateaus really early. Are there any mathematical intuitions or reasons that VAEs won't work on a problem like this?
As a note I've also tried semi-supervised VAEs and it didn't do much better. I even tried it for seq2seq of just first names given a first and it super failed as well and I'm talking like not even close to generation of names or the original input.
",['variational-autoencoder'],
"How exactly does self-play work, and how does it relate to MCTS?","
I am working towards using RL to create an AI for a two-player, hidden-information, a turn-based board game. I have just finished David Silver's RL course and Denny Britz's coding exercises, and so am relatively familiar with MC control, SARSA, Q-learning, etc. However, the course was focused on single-player, perfect-information games, and I haven't managed to find any examples similar to the type of game I have, and would like advice on how to proceed.
I am still unsure how self-play works, and how it relates to MCTS. For example, I don't know if this involves using the latest agent to play both sides, or playing an agent against older versions, or training multiple opposing agents simultaneously. Are there good examples (or repositories) for learning self-play and MCTS for two-player games? 
","['reinforcement-learning', 'definitions', 'monte-carlo-tree-search', 'pomdp', 'self-play']",
Can we use GPT-2 to smooth out / correct text?,"
Are we able to use models like GPT-2 to smooth out/correct text? For instance if I have two paragraphs that need some text to make the transition easier to read, could this text be generated? And, could it find inconsistencies between the paragraphs and fix them?
As an example, imagine we're reordering some text so that we can apply the pyramid principle. What I'd like to do is reorder the sentences/paragraphs and still have a coherant story. The following three sentences for instance, start with a statement and then have some facts to support it. What's missing is the story that joins them together, right now they're three independent sentences.

The strawberry is the best fruit based on its flavor profile, its coloring and texture and the nutritional profile.
Strawberries are very rich in antioxidants and plant compounds, which may have benefits for heart health and blood sugar control.
Strawberries have a long history and have been enjoyed since the Roman times.

Feel free to point me at things to read, I have not been able to find anything like this in my searches.
","['neural-networks', 'natural-language-processing', 'gpt']",
How does crossover work in a genetic algorithm?,"
If I had the weights of a certain number of ""parents"" that I wanted to crossbreed, and I used whatever method to pick out the ""best parents"" (I used a roulette wheel option, if that's any relevant), would I be doing this correctly?
For example, suppose I have picked the following two parents.
\begin{align}
P_1 &= [0.5, -0.02, 0.4, 0.1, -0.9] \\
P_2 &= [0.42, 0.55, 0.18, -0.3, 0.12]
\end{align}
When I'm iterating through each index (or gene) of the parents, I am selecting a weight from one parent only. I called this rate the ""cross-rate"", which in my case is $0.2$ (i.e. with $20$% chance, I will switch to choosing the other parents' weight).
So, using our example above, this is what would happen:
\begin{align}
P_1 &=[\mathbf{0.5}, \mathbf{-0.02}, 0.4, 0.1, \mathbf{-0.9}] \\
P_2 &= [0.42, 0.55, \mathbf{0.18}, \mathbf{-0.3}, 0.12]
\end{align}
So the child would be
$$C = [0.5, -0.02, 0.18, -0.3, -0.9]$$
I would choose $0.5$ from $P_1$, but for every time I choose a weight from $P_1$, there's a 20% chance that I actually choose the corresponding gene from $P_2$. But, for the first weight, I end up not landing on that 20% chance. So I move onto the second weight, $-0.02$. This time, we hit the 20% chance, so now we swap over. Our next weight is now from $P_2$, which is $0.18$. And so on, until we hit another 20% chance.
We keep doing this until we hit the end of the indexes ($P_1$ and $P_2$ have the same number of indexes, of course).
Is this the correct way to form a child from 2 parents? Is this the correct ""crossbreeding"" method when it comes to genetic algorithms?
","['genetic-algorithms', 'definitions', 'crossover-operators']",
Is the LSTM component a neuron or a layer?,"
Given the standard illustrative feed-forward neural net model, with the dots as neurons and the lines as neuron-to-neuron connection, what part is the (unfold) LSTM cell (see picture)? Is it a neuron (a dot) or a layer? 
 
","['neural-networks', 'deep-learning', 'long-short-term-memory']",
What are the state-of-the-art meta-reinforcement learning methods?,"
This question can seem a little bit too broad, but I am wondering what are the current state-of-the-art works on meta reinforcement learning. Can you provide me with the current state-of-the-art in this field?
","['reinforcement-learning', 'meta-learning']","One of the most recent papers on meta-RL is meta-Q-learning. This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-reinforcement learning (meta-RL). MQL builds upon three simple ideas.Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory.Using a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies.past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updatesExperiments on standard continuous-control benchmarks suggest that MQL compares favorably with state-of-the-art meta-RL algorithms.I think that other references to other work on meta-RL are present in the experiments part of the MQL paper."
How are newer weight initialization techniques better than zero or random initialization?,"
How do newer weight initialization techniques (He, Xavier, etc) improve results over zero or random initialization of weights in a neural network? Is there any mathematical evidence behind this?
","['neural-networks', 'deep-learning', 'weights', 'weights-initialization']",
How to report the solution path of a search algorithm on a graph?,"
I'm working on a problem where we are given a graph and asked to perform various search algorithms (BFS, DFS, UCS, A*, etc.) and the goal state is to visit all nodes in the graph. After all nodes are visited, we need to print out the ""solution path."" However, I am a bit confused on what ""path"" means in AI. 
For simplicity, let's just consider a graph of 3 nodes: A, B and C with 2 undirected edges (A, B) (A, C). If we perform BFS on this graph starting at node A and traversing alphabetically, we'd visit A, then B, then C.  So, in this case, is the solution path A -> B -> C, i.e. the order in which the nodes are visited? Or is the solution path A -> B -> A -> C? Basically saying that we go from A to B, but to go from B to C, we must go through A again. 
","['search', 'path-planning', 'path-finding']",
"Trained a regression network and getting EXACT same result on validation set, on every epoch","
I trained this network from this github.
The training went well, and returns nice results for new, unseen images.
On training, the loss changed (decreased), thus I must assume the weights changed as well.
On training, I saved a snapshot of the net every epoch.
When trying to run a validation set through each epoch's snapshot, I get the exact same results on every epoch.
How can this be possible? What's causing this?
","['machine-learning', 'deep-learning']","The answer is I was too tired to see I was using the same model for validation every epoch, instead of that epoch's model.Write good code, and this won't happen to you."
Creating a noising model for NLP that models human noising,"
I'm trying to create a noising model that accurately reflects how people would noise name data. I was thinking of randomly switching out characters and creating a probability over which character gets switched in based on keyboard closeness and how similar anatomically another character looks to it. For example, ""l"" has a higher prob of being switched in with ""|"" and ""k"" cause ""k"" is close by on the keyboard and ""|"" looks like ""l"", but that requires a lot of hard coding and reward for that seemed low because that's not the only 2 ways people can noise things. I also had the same idea above except use template matching of every character to every other character but itself and that would give it a similarity score then divide that by the sum over all chars to get the probs. Any other suggestions? My goal is the maximize closeness to actual human noising.
",['natural-language-processing'],
"In this VAE formula, why do $p$ and $q$ have the same parameters?","
In $$\log p_{\theta}(x^1,...,x^N)=D_{KL}(q_{\theta}(z|x^i)||p_{\phi}(z|x^i))+\mathbb{L}(\phi,\theta;x^i),$$ why does $p(x^1,...,x^N)$ and $q(z|x^i)$ have the same parameter $\theta?$
Given that $p$ is just the probability of the observed data and $q$ is the approximation of the posterior, shouldn't they be different distributions and thus their parameters different?
","['variational-autoencoder', 'latent-variable', 'evidence-lower-bound']","I will try to answer your questions directly (but I guess I won't be able to), otherwise, this can become quite confusing, given the inconsistencies that can be found across different sources.In $logp_{\theta}(x^1,...,x^N)=D_{KL}(q_{\theta}(z|x^i)||p_{\phi}(z|x^i))+\mathbb{L}(\phi,\theta;x^i)$ why is $\theta$ and param for $p$ and $q$?In a few words, your equation is wrong because it uses the letters $\phi$ and $\theta$ inconsistently.If you look more carefully at the right-hand side of your equation, you will notice that $q_{\theta}$ has different parameters, i.e. $\theta$, than $p_{\phi}$, which has parameters $\phi$, so $p$ and $q$ have different parameters, and this should be the case, because they are represented by different neural networks in the case of the VAE. However, the left-hand side uses $\theta$ as the parameters of $p$ (while the right-hand side uses $\phi$ to index $p$), so this should already suggest that the equation is not correct (as you correctly thought).In the case of the VAE, $\phi$ usually represents the parameters (or weights) of the encoder neural network (NN), while $\theta$ usually represents the parameters of the decoder NN (or vice-versa, but you should just be consistent, which is often not the case in your equation). In fact, in the VAE paper, in equation 3, the authors use $\phi$ to represent the parameters of the encoder $q$, while $\theta$ is used to denote the parameters of the decoder $p$.So, if you follow the notation in the VAE paper, the ELBO can actually be written something like\begin{align}
\mathcal{L}(\phi,\theta; \mathbf{x}) 
&= 
\mathbb{E}_{\tilde{z} \sim q_{\phi}(\mathbf{z} \mid \mathbf{x})} \left[ \log p_{\theta} (\mathbf{x} \mid \mathbf{z}) \right] - \operatorname{KL} \left(q_{\phi}(\mathbf{z} \mid \mathbf{x}) \| p_{\theta}(\mathbf{z}) \right) \tag{1} \label{1}
\end{align}The ELBO loss $\mathcal{L}(\phi,\theta; \mathbf{x})$ has both parameters (of the encoder and decoder), which will be optimized jointly. Note that I have ignored the indices in the observations $\mathbf{x}$ (for simplicity), while, in the VAE paper, they are present. Furthermore, note that, both in \ref{1} and in the VAE paper, we use bold letters (because these objects are usually vectors), i.e. $\mathbf{x}$ and $\mathbf{z}$, rather than $x$ and $z$ (like in your equation).Note also that, even though $p_{\theta}(\mathbf{z})$ is indexed by $\theta$, in reality, this may be an un-parametrized distribution (e.g. a Gaussian with mean $0$ and variance $1$), i.e. not a family of distributions. The use of the index $\theta$ in $p_{\theta}(\mathbf{z})$ comes from the (implicit) assumption that both $p_{\theta}(\mathbf{z})$ and $p_{\theta} (\mathbf{x} \mid \mathbf{z})$ come from the same family of distributions (e.g. a family of Gaussians). In fact, if you consider the family of all Gaussian distributions, then $p_{\theta}(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ also belongs to that family. But $\theta$ and $\phi$ are also used to denote the parameters (or weights) of the networks, so this becomes understanbly confusing. (To understand equation 10 of the VAE paper, see this answer).Why does $p(x^1,...,x^N)$ and $q(z|x^i)$ have the same parameter $\theta?$This is wrong, in fact. If you look at equation 1 of the VAE paper, they use $\theta$ to denote the parameters of $p(\mathbf{x})$, i.e. $p_{\theta}(\mathbf{x})$, while the parameters of the encoder are $\phi$, i.e. $q_{\phi}(\mathbf{z} \mid \mathbf{x}$).Cause $p$ is just the probability of the observed data and $q$ is the approximation of the posterior so shouldn't they be different distributions and their parameters different?Yes."
Strategy of using intermediate layers of a neural network as features?,"
There is a popular strategy of using a neural network trained on one task to produce features for another related task by ""chopping off"" the top of the network and sewing the bottom onto some other modeling pipeline.
Word2Vec models employ this strategy, for example.
Is there an industry-popular term for this strategy?  Are there any good resources that discuss its use in general terms?
",['neural-networks'],
Which is a better form of regularization: lasso (L1) or ridge (L2)?,"
Given a ridge and a lasso regularizer, which one should be chosen for better performance? 
An intuitive graphical explanation (intersection of the elliptical contours of the loss function with the region of constraints) would be helpful.
","['machine-learning', 'comparison', 'regularization', 'l2-regularization', 'l1-regularization']","The following graph shows the constraint region (green), along with contours for Residual sum of squares (red ellipse). These are iso-lines signifying that points on an ellipse have the same RSS.

Figure: Lasso (left) and Ridge (right) Constraints  [Source: Elements of Statistical Learning]As Ridge regression has a circular constraint ($\beta_1^2 + \beta_2^2 <= d$) with no edges, the intersection will not occur on an axis, signifying that the ridge regression parameters will usually be  non-zero.On the contrary, the Lasso constraint ($|\beta_1| + |\beta_2| <= d$) has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. In 2D, such a scenario would result in one of the parameters to become zero whereas in higher dimensions, more of the parameter estimates may simultaneously reach zero.This is a disadvantage of ridge regression, wherein the least important predictors never get eliminated, resulting in the final model to contain all predictor variables. For Lasso, the L1 penalty forces some parameters to be exactly equal to zero when $\lambda$ is large. This has a dimensionality reduction effect resulting in sparse models.In cases where the number of predictors are small, L2 could be chosen over L1 as it constraints the coefficient norm retaining all predictor variables."
Generation of realistic real-valued sequences using Wasserstein GAN fails,"
My goal is to generate artificial sequences of real-valued data (e.g. time series) with GANs. Starting simple I tried to generate realistic sine-waves using a Wasserstein GAN. But even on this simple task it fails to generate any useful samples.
This is my model:
Generator
model = Sequential()
model.add(LSTM(20, input_shape=(50, 1)))
model.add(Dense(40, activation='linear'))
model.add(Reshape((40, 1)))

Critic
model = Sequential()
model.add(Conv1D(64, kernel_size=5, input_shape=(40, 1), strides=1))
model.add(MaxPooling1D(3, strides=2))
model.add(LeakyReLU(alpha=0.2))
model.add(Conv1D(64, kernel_size=5, strides=1))
model.add(MaxPooling1D(3, strides=2))
model.add(LeakyReLU(alpha=0.2))
model.add(Flatten())
model.add(Dense(1))

Is this model capable of learning such a task or should I use a different model architecture? 
","['keras', 'generative-adversarial-networks', 'time-series', 'sequence-modeling']",
"Understanding the pseudocode of uniform-cost search from the book ""Artificial Intelligence: A Modern Approach""","
On page 84 of Russell & Norvig's book ""Artificial Intelligence: A Modern Approach Book"" (3rd edition), the pseudocode for uniform cost search is given. I provided a screenshot of it here for your convenience.
I am having trouble understanding the highlighted line if child.STATE is not in explored **or** frontier then

Shouldn't that be 
if child.STATE is not in explored **and** frontier then
The way it's written, it seems to suggest that if the child node has already been explored, but not currently in the frontier, then we add it to the frontier, but I don't think that's correct. If it's already been explored, then it means we already found the optimal path for this node previously and should not be processing it again. Is my understanding wrong?
","['search', 'norvig-russell', 'uniform-cost-search', 'pseudocode']","I think this is a problem with missing brackets in pseudocode â€” clearly the state is only added to the frontier if it hasn't been explored already, so it would be:which is equivalent to your interpretation of(according to De Morgan's Laws)Being half-way between a programming language and natural language, this is a case where pseudocode is not quite precise enough."
"While we split data in training and test data, why we have two pairs of each?","
Why do we split the data into two parts, and then split those segments into training and testing data? Why do we have two sets of data for each training and test data?
","['machine-learning', 'training', 'cross-validation']","Are you talking about (X_train,y_train) and (X_test,y_test).
If yes, then X represents the data(features) and y represents the labels of that data. That's why you get a pair when you divide it into training and test data"
Does the bag-of-visual-words method improve the classification accuracy?,"
I'm a beginner in computer vision. I want to know which option among the following two can get better accuracy of image classification.

SIFT features + SVM
Bag-of-visual-words features + SVM

Here's a reference: https://www.mathworks.com/help/vision/ug/image-classification-with-bag-of-visual-words.html.
","['machine-learning', 'computer-vision', 'support-vector-machine', 'image-recognition', 'bag-of-features']",
Neural network seems to just figure out the probability of a specific result,"
I am really new to neural networks, so i was following along with a video series, created by '3blue1brown' on youtube. I created an implementation of the network he explained in c++. I am attempting to train the network to recognize hand written characters, using the MNIST data set. What seems to be happening is, rather than actually learn how to recognize the characters, it is just learning how many of each input there is in the data set and doing it with probability. When testing on a smaller dataset this is more noticeable, for example i was testing on a set with 100, and the numbers that were more frequent would always have a slightly higher activation at the end, and others where very close to 0. Here is my code if it helps:
#include <random>
#include <vector>
#include <iostream>
#include <fstream>
#include <cmath>

double weightMutationRate = 1;
double biasMutationRate = 1;

//Keeps track of the weights, biases, last activation and the derivatives
//of the weights and biases for a single node in a neural network.
struct Node
{
  std::vector<double> weights;
  std::vector<double> derivWeights;
  double activation;
  double derivActivation;
  double bias;
  double derivBias;
};

//Struct to hold the nodes in each layer of a network.
struct Layer
{
  std::vector<struct Node> nodes;
};

//Struct to hold the layers in a network.
struct Network
{
  std::vector<struct Layer> layers;
  double cost;
};

//Stores the inputs and outputs for a single training example.
struct Data
{
  std::vector<double> inputs;
  std::vector<double> answers;
};

//Stores all of the data to be used to train the neural network.
struct DataSet
{
  std::vector<struct Data> data;
};

//Generates a double by creating a uniform distribution between the two arguments.
double RandomDouble(double min, double max)
{
  std::random_device seed;
  std::mt19937 random(seed());
  std::uniform_real_distribution<> dist(min, max);
  return dist(random);
}

//Constructs a network with the node count in each layer defined with 'layers'.
//the first layer will not have any weights and biases and will simply have
//the activation of the input data.
struct Network CreateNetwork(std::vector<int> layers, double minWeight = -1, double maxWeight = 1, double minBias = -1, double maxBias = 1)
{
  //Network to construct.
  struct Network network;
  //Used to store the nodes in the previous layer.
  int prevLayerNodes;
  //Iterates through the layers vector and constructs a neural network with the values in
  //the vector determining how many nodes that are in each of the layers.
  bool isFirstLayer = true;
  for (int layerNodes : layers)
  {
    //Layer to construct.
    struct Layer layer;
    //Creating the nodes for the current layer.
    for (int i = 0; i < layerNodes; i++)
    {
      //Node to construct
      struct Node node;
      //Checks to see if the current layer is not the input layer, which does not have
      //any weights or biases.
      if (!isFirstLayer)
      {
        //Creating weights for the connections between this node
        //and the nodes in the previous layer.
        for (int i = 0; i < prevLayerNodes; i++)
        {
          //Getting a random double for the weight, between the bounds set in the arguments.
          double inputWeight = RandomDouble(minWeight, maxWeight);
          //Adding the inputWeight to the current node.
          node.weights.push_back(inputWeight);
          //Adding a 0 to the deriv weights for the weight just added.
          node.derivWeights.push_back(0.0);
        }
        //Getting a random double for the bias, between the bounds set in the arguments.
        double bias = RandomDouble(minBias, maxBias);
        //Adding the bias to the current node.
        node.bias = bias;
        //Adding the node to the layer.
      }
      layer.nodes.push_back(node);
    }
    //Updating the isFirstLayer variable if the current layer is the input layer.
    if (isFirstLayer)
    {
      isFirstLayer = false;
    }
    //Updating the prevLayerNodes variable for use in the next layer.
    prevLayerNodes = layerNodes;
    //Adding the layer to the network.
    network.layers.push_back(layer);
  }
  //Returning the constructed network.
  return network;
}

//Outputs the network passed to the networkPrint.txt file.
void PrintNetwork(struct Network network)
{
  std::cout << ""Printing network ..."" << std::endl;
  std::ofstream networkPrintFile;
  networkPrintFile.open(""networkPrint.txt"");
  //Iterates through each of the layers in teh network.
  for (int i = 0; i < network.layers.size(); i++)
  {
    std::cout << ""Layer : "" << i << std::endl;
    networkPrintFile << ""Layer "" << i << "":"" << std::endl;
    //Iterates through each of the nodes in the current layer.
    for (int j = 0; j < network.layers[i].nodes.size(); j++)
    {
      networkPrintFile << ""\t"" << ""Node "" << j << "":"" << std::endl;
      //Outputs the node's activation into networkPrintFile.
      double activation = network.layers[i].nodes[j].activation;
      networkPrintFile << ""\t\t"" << ""Activation"" << "": "" << activation << std::endl;
      //Outputs the node's derivActivation into networkPrintFile.
      double derivActivation = network.layers[i].nodes[j].derivActivation;
      networkPrintFile << ""\t\t"" << ""Deriv Activation"" << "": "" << derivActivation << std::endl;
      double bias = network.layers[i].nodes[j].bias;
      double derivBias = network.layers[i].nodes[j].derivBias;
      //Outputs the bias and derivative of the bias.
      networkPrintFile << ""\t\t"" << ""Bias"" << "": "" << bias << std::endl;
      networkPrintFile << ""\t\t"" << ""Deriv Bias"" << "": "" << derivBias << std::endl;
      //Iterates through all of the inputWeights in the current node.
      networkPrintFile << ""\t\t"" << ""Weights"" << "":"" << std::endl;
      for (int k = 0; k < network.layers[i].nodes[j].weights.size(); k++)
      {
        double inputWeight = network.layers[i].nodes[j].weights[k];
        double derivWeight =  network.layers[i].nodes[j].derivWeights[k];
        networkPrintFile << ""\t\t\t"" << ""Weight "" << k << "":"" << std::endl;
        networkPrintFile << ""\t\t\t\t"" << ""Value"" << "":"" << inputWeight << std::endl;
        networkPrintFile << ""\t\t\t\t"" << ""Derivative"" << "":"" << derivWeight << std::endl;
      }
    }
  }
  std::cout << ""Done"" << std::endl;
}

//Takes and input and peforms a mathematical sigmoid
//function on it and returns the value.
//             1
//  Ïƒ(x) = ---------
//          1 + e^x
double Sigmoid(double input)
{
  double expInput = std::exp(-input);
  double denom = expInput + 1;
  double value = 1 / denom;
  return value;
}

//Returns the activation of the node passed in give the previous layer.
double CalculateNode(struct Node &node, struct Layer &prevLayer)
{
  //Keeps a runing total of the weights and activations added up so far.
  double total = 0.0;
  int weightCount = node.weights.size();
  //Iterated through each of the weights, and thus each of the
  //nodes in the previous layer to find the weight * activation.
  for (int i = 0; i < weightCount; i++)
  {
    //Calculated the current weight and activation and
    //adds it to the 'total' variable.
    double weight = node.weights[i];
    double input = prevLayer.nodes[i].activation;
    double value = weight * input;
    total += value;
  }
  //Add the node's bias to the total.
  total += node.bias;
  //Normalises the node's activation value by passing it through
  //a sigmoid function, which bounds it between 0 and 1.
  double normTotal = Sigmoid(total);
  //Returns the caclulated value for this node.
  return normTotal;
}

//Adds the activation values to a layer passed in, given the previous layer.
void CaclulateLayer(struct Layer &layer, struct Layer &prevLayer)
{
  //Iterates through all of the nodes and calculated their activations.
  for (struct Node &node : layer.nodes)
  {
    double activation = CalculateNode(node, prevLayer);
    //Setting the activation to the node.
    node.activation = activation;
  }
}

//Takes in the first layer of the neural network and interates through the
//nodes and sets each input to each node in a loop.
void SetInputs(struct Layer &layer, std::vector<double> inputs)
{
  for (int i = 0; i < layer.nodes.size(); i++)
  {
    //Setting the node's activation to the corrosponding input.
    layer.nodes[i].activation = inputs[i];
  }
}

//Takes in a network and inputs and calculates the value of
//activation for every node for a single input vector.
void CalculateNetwork(struct Network &network, std::vector<double> inputs)
{
  //Setting the activations of the first layer to the inputs vector.
  SetInputs(network.layers[0], inputs);
  //Iterates through all of the layers, apart from the first layer, and
  //calculated the activations of the nodes in that layer.
  for (int i = 1; i < network.layers.size(); i++)
  {
    //Getting the layer to calculate to activations on and the
    //previous layer, which already has it's activations calculated.
    struct Layer currentLayer = network.layers[i];
    struct Layer prevLayer = network.layers[i - 1];
    //Calculating the nodes on the current layer.
    CaclulateLayer(currentLayer, prevLayer);
    //Setting the currentLayer back into the network struct with
    //all of the activations in it now calculated.
    network.layers[i] = currentLayer;
  }
}

//Caclulates the sum of the differences between the outputs and the correct
//values squared.
//
//  Cost = Î£((a-y)^2)
//
double CalculateCost(struct Network &network, std::vector<double> correctOutputs)
{
  //Keeps track of the current sum of the costs.
  double totalCost = 0.0;
  //The layer of the network that holds the calculated values, the
  //last layer in the network.
  struct Layer outputLayer = network.layers[network.layers.size() - 1];
  //Loops through all the node sin the output layer and compared them
  //to their corresponding correctOutput value, calculates the cost
  //and adds it to the running total, totalCoat.
  for (int i = 0; i < outputLayer.nodes.size(); i++)
  {
    struct Node node = outputLayer.nodes[i];
    double calculatedActivation = node.activation;
    double correctActivation = correctOutputs[i];
    double diff = calculatedActivation - correctActivation;
    double modDiff = diff * diff;
    //Adding the cost to the sum of the other costs.
    totalCost += modDiff;
  }
  //Returning the value of the calculated cost.
  return totalCost;
}

//Takes in the output layer of the network and calculates the derivatives of the
//cost function with respect to the activations in each node. this value is then
//stored on the Node struct.
void LastLayerDerivActivations(struct Layer &layer, std::vector<double> correctOutputs)
{
  //Iterating through all the nodes in the layer.
  for (int i = 0; i < layer.nodes.size(); i++)
  {
    //Getting the values of the node output and correct output.
    double activation = layer.nodes[i].activation;
    double correctOutput = correctOutputs[i];
    //Caclulating the partial derivative of the cost function with respect
    //to the current node's activation value.
    double activationDiff = activation - correctOutput;
    double derivActivation = 2 * activationDiff;
    //Setting the activation partial derivative to the layer passed in.
    layer.nodes[i].derivActivation = derivActivation;
  }
}

//Returns the derivative of the sigmoid function.
//   d
//  ---- Ïƒ(x) = Ïƒ(x)(1 - Ïƒ(x))
//   dx
double DerivSigma(double input)
{
  double sigma = Sigmoid(input);
  double value = sigma * (1 - sigma);
  return sigma;
}

//Takes in a node and the layer that the node takes inputs from and adds.
//to the derivWeight and derivBias of the node and adds to each of the
//deriv activations in the previous layer for them to be used in this
//function to calculate their derivatives.
void NodeDeriv(struct Node &node, struct Layer &prevLayer)
{
  //Starting the total at the bias.
  double total = node.bias;
  //Looping through all the weights and biases to find z(x).
  //  z(x) = a w + a w + ... + a w + b
  //          1 1   2 2         n n
  for (int i = 0; i < node.weights.size(); i++)
  {
    double weight = node.weights[i];
    double activation = prevLayer.nodes[i].activation;
    double value = weight * activation;
    //Adding to the running total for z(x).
    total += value;
  }
  //Finding the derivative of the cost function with respect to the
  //z(x) by multiplying the DerivSigma() by the node's derivActivation
  //using the chain rule.
  double derivAZ = DerivSigma(total);
  double derivCZ = derivAZ * node.derivActivation;
  //The derivative of the cost with respect to the bias is the same as
  //the derivative of the cost function with respect to z(x) since
  //d/db z(x) = 1
  node.derivBias += derivCZ;
  //Iterating through all of the nodes and weights to find the derivatives
  //of all of the weights for the node and the activations on the
  //previous layer.
  for (int i = 0; i < node.weights.size(); i++)
  {
    // dc/dw = dc/dz * activation
    double derivCW = derivCZ * prevLayer.nodes[i].activation;
    // dc/da = dc/dz * weight
    double derivCA = derivCZ * node.weights[i];
    //Adding the weights and activations to the node objects.
    node.derivWeights[i] += derivCW;
    prevLayer.nodes[i].derivActivation += derivCA;
  }
  //Resetting the activation derivative.
  node.derivActivation = 0;
}

//Takes in a layer and iterates through all the nodes in order to find
//the derivatives of thw weight and biases in the current layer and the
//activations in the previous layer.
void LayerDeriv(struct Layer &layer, struct Layer &prevLayer)
{
  for (int i = 0; i < layer.nodes.size(); i++)
  {
    NodeDeriv(layer.nodes[i], prevLayer);
  }
}

//Takes in a network and uses backpropogation to find the derivatives of
//all the nodes for a single training example.
void NetworkDeriv(struct Network &network, std::vector<double> expectedOutputs)
{
  //Calculating the derivatives of the activations in the last layer.
  LastLayerDerivActivations(network.layers[network.layers.size() - 1], expectedOutputs);
  //Looping through all the layers to find the derivatives of all of
  //the weights and activations in the network for this training example.
  for (int i = network.layers.size() - 1; i > 0; i--)
  {
    LayerDeriv(network.layers[i], network.layers[i - 1]);
  }
}

//Takes in an input string and char and will return a vector of the string split
//by the char. The char is lost in this conversion.
std::vector<std::string> SplitString(std::string stringToSplit, char delimiter)
{
  //Creating the output vector.
  std::vector<std::string> outputVector;
  //Initialising the lastDelimiter to -1, since the first string should be split as if
  //the char before it was the splitter.
  int lastDelimiterIndex = -1;
  for (int i = 0; i < stringToSplit.size(); i++)
  {
    //Getting the current char.
    char chr = stringToSplit[i];
    //If the current char is the delimiter, create a new substring in the vector.
    if (chr == delimiter)
    {
      //Creating the new substring at the delimiter and adding it to the end
      //of the output vector.
      std::string subString = stringToSplit.substr(lastDelimiterIndex + 1, i - lastDelimiterIndex - 1);
      outputVector.push_back(subString);
      //Setting the last delimiter variable to the current character.
      lastDelimiterIndex = i;
    }
  }
  //Adding the last section of the string to the output vector, since there is no
  //delimiter and will not be added in the for loop.
  std::string subString = stringToSplit.substr(lastDelimiterIndex + 1, stringToSplit.size() - lastDelimiterIndex - 1);
  outputVector.push_back(subString);
  //Returning the split string as a vector of strings.
  return outputVector;
}

//Takes in a vector of strings and converts it to a vector of doubles. normalise argument
//sets what value will be taken to be 1, and other numbers will be a fraction of that.
//Set normalise to 0 to disable normalisation.
std::vector<double> ConvertStringVectorToDoubleVector(std::vector<std::string> input, int normalise = 0)
{
  std::vector<double> convertedVector;
  //Iterating through all the strings int the input vector.
  for (std::string str : input)
  {
    //Converting the string into a double.
    double value = stod(str);
    //Checks to see if normalisation is enabled.
    if (normalise != 0)
    {
      //Normalising the double.
      value /= normalise;
    }
    //Adding the double to the output vector.
    convertedVector.push_back(value);
  }
  //Returning the converted vector.
  return convertedVector;
}

//Takes in a string of data and uses it to create a DataSet object to
//be used in the training of the neural network.
struct DataSet FormatData(std::string dataString)
{
  struct DataSet dataSet;
  //Splitting the input string into the seperate images.
  std::vector<std::string> imageSplit = SplitString(dataString, '|');
  //Looping through all of the images.
  for (int i = 0; i < imageSplit.size(); i++)
  {
    //Getting the current image string.
    std::string imageData = imageSplit[i];
    //Splitting the image between the inputs and expected outputs.
    std::vector<std::string> ioSplit = SplitString(imageData, '/');
    std::string inputs = ioSplit[0];
    std::string outputs = ioSplit[1];
    //converting the input and output strings into string arrays of the values.
    std::vector<std::string> inputVectorString = SplitString(inputs, ',');
    std::vector<std::string> outputVectorString = SplitString(outputs, ',');
    //Converting the string arrays into double arrays and normalising the input doubles.
    std::vector<double> inputVector = ConvertStringVectorToDoubleVector(inputVectorString, 255);
    std::vector<double> outputVector = ConvertStringVectorToDoubleVector(outputVectorString);
    //Creating a new Data object.
    struct Data data;
    data.inputs = inputVector;
    data.answers = outputVector;
    //Adding the object to the dataset.
    dataSet.data.push_back(data);
  }
  //Returning the completed dataset.
  return dataSet;
}

//Takes in a filename and extracts all of the ascii data from the
//file and calls the FormatData function to create a DataSet object.
struct DataSet CreateDataSetFromFile(std::string fileName)
{
  //Opening file.
  std::ifstream dataFile;
  dataFile.open(fileName);
  //Storing fild data in a string.
  std::string data;
  dataFile >> data;
  //Creating DataSet object.
  struct DataSet dataSet = FormatData(data);
  //Returning completed DataSet object.
  return dataSet;
}

//Takes in a network and a Data object and runs the network and adds
//to the derivatives of the network for that one training example.
void NetworkIteration(struct Network &network, struct Data data)
{
  //Extracting the input and output data from the data object.
  std::vector<double> inputs = data.inputs;
  std::vector<double> outputs = data.answers;
  //Caclulating the activations of the network for this data.
  CalculateNetwork(network, inputs);
  //Caclulating the cost for this iteration and adding it to the total.
  double cost = CalculateCost(network, outputs);
  network.cost += cost;
  //Caclulating the derivatives for the network weights and biases
  //for this training example.
  NetworkDeriv(network, outputs);
}

//Takes in a node and caclulates the average of the derivatives over
//the dataset and then multiplies them by a fixes mutation rate and
//applies the derivatives to the node's values.
void GradientDecentNode(struct Node &node, int dataCount)
{
  //Iterating through all of the weights of the node.
  for (int i = 0; i < node.weights.size(); i++)
  {
    double weight = node.weights[i];
    double derivWeight = node.derivWeights[i];
    //Getting the average over all of the training data.
    derivWeight /= dataCount;
    //Applying a constant multiplier to alter the rate at which is mutates.
    derivWeight *= weightMutationRate;
    //Subtracting the derivative from the weight.
    node.weights[i] -= derivWeight;
    //Reseting the weight derivative
    node.derivWeights[i] = 0;
  }
  double bias = node.bias;
  double derivBias = node.derivBias;
  //Applying a constant multiplier to alter the rate at which is mutates.
  derivBias *= biasMutationRate;
  //Subtracting the derivative from the bias.
  node.bias -= derivBias;
  //Resetting the bias derivative.
  node.derivBias = 0;
}

//Takes in a layer and iterated through all of the nodes in the layer
//and applies all of their derivatives to them.
void GradientDecentLayer(struct Layer &layer, int dataCount)
{
  for (struct Node &node : layer.nodes)
  {
    GradientDecentNode(node, dataCount);
  }
}

//Takes in a network and iterated through all of the layers and applies
//all of the derivatives to them.
void GradientDecentNetwork(struct Network &network, int dataCount)
{
  for (int i = 1; i < network.layers.size(); i++)
  {
    GradientDecentLayer(network.layers[i], dataCount);
  }
}

//Iterates through all of the training data in dataSet and calculates the derivatives
//of the weights and biases and then peforms the gradient decent using the derivatives.
void TrainNetworkSingle(struct Network &network, struct DataSet dataSet)
{
  //Iterating through all of the training data.
  for (struct Data data : dataSet.data)
  {
    //Caclulating the network for a single training example.
    NetworkIteration(network, data);
  }
  //Peforming the derivatives.
  GradientDecentNetwork(network, dataSet.data.size());
}

void TrainNetwork(struct Network &network, struct DataSet dataSet, int iterations)
{
  for (int i = 0; true/*i < iterations*/; i++)
  {
    TrainNetworkSingle(network, dataSet);
    std::cout << network.cost << std::endl;
    network.cost = 0;
  }
}

int main()
{
  struct Network network = CreateNetwork({784, 784, 16, 16, 10});
  struct DataSet dataSet = CreateDataSetFromFile(""data.txt"");
  TrainNetwork(network, dataSet, 100);
  PrintNetwork(network);
  return 0;
}
```

","['neural-networks', 'machine-learning', 'probability', 'feedforward-neural-networks']",
Why is the evidence equal to the KL divergence plus the loss?,"
Why is the equation $$\log p_{\theta}(x^1,...,x^N)=D_{KL}(q_{\theta}(z|x^i)||p_{\phi}(z|x^i))+\mathbb{L}(\phi,\theta;x^i)$$ true, where $x^i$ are data points and $z$ are latent variables?
I was reading the original variation autoencoder paper and I don't understand how the marginal is equal to the RHS equation. How does the marginal equal the KL divergence of $p$ with its approximate distribution plus the variational lower bound?
","['objective-functions', 'variational-autoencoder', 'latent-variable', 'kl-divergence', 'evidence-lower-bound']","In variational inference, the original objective is to minimize the Kullback-Leibler divergence between the variational distribution, $q(z \mid x)$, and the posterior, $p(z \mid x) = \frac{p(x, z)}{\int_z p(x, z)}$, given that the posterior can be difficult to directly infer with the Bayes rule, due to the denominator term, which can contain an intractable integral. Therefore, more formally, the optimization objective can be written as\begin{align}
q^*(z \mid x) = \operatorname{argmin}_{q(z \mid x)} D_{\text{KL}}(q(z \mid x) \| p(z \mid x))\tag{1} \label{1}
\end{align}However, solving this optimization problem can be as difficult as the original inference one of computing the posterior $p(z \mid x)$ using the Bayes rule, given that it still involves the possibly intractable term $p(z \mid x)$.If you use the definition of the KL divergence, you can derive the following equation\begin{align}
D_{\text{KL}}(q(z \mid x) \| p(z \mid x)) = \mathbb{E}_{q(z \mid x)} \left[ \log q(z \mid x) \right] - \mathbb{E}_{q(z \mid x)} \left[ \log q(z, x) \right] + \log p(x) \tag{2} \label{2}
\end{align}First, note that the expectations are with respect to the variational distribution, which means that, if you want to approximate these expectations with Monte Carlo estimates, you can do it with respect to the variational distribution, and, given that it is assumed that one can easily sample from the variational distribution (which can e.g. be a Gaussian), this is a nice feature. Second, the KL divergence contains the term $p(x) = \int_z p(x, z)$, the denominator term in the Bayes rule to compute the posterior $p(z \mid x)$, which (as I said) can be intractable. $p(x)$ is often called the evidence.The solution is then to optimize an objective that does not contain this annoying intractable term $p(x)$. The objective that is optimized is the so-called ELBO objective\begin{align}
\text{ELBO}(q) = \mathbb{E}_{q(z \mid x)} \left[ \log q(z, x) \right] - \mathbb{E}_{q(z \mid x)} \left[ \log q(z \mid x) \right]\tag{3} \label{3}
\end{align}The KL divergence \ref{2} and the ELBO objective \ref{3} are similar. In fact, ELBO is an abbreviation for Evidence Lower BOund, because the ELBO is a lower bound on the evidence $p(x)$, i.e. it is a number that is smaller than $p(x)$ or, more formally, $\text{ELBO}(q) \leq \log p(x)$. Therefore, if we maximize $\text{ELBO}(q)$, we also maximize the evidence $p(x)$ of the data (where $x$ is the data in your dataset). So, the objective in variational inference is\begin{align}
q^*(z \mid x) 
&= \operatorname{argmax}_{q(z \mid x)} \operatorname{ELBO}({q}) \\
&= \operatorname{argmax}_{q(z \mid x)}  \mathbb{E}_{q(z \mid x)} \left[ \log q(z, x) \right] - \mathbb{E}_{q(z \mid x)} \left[ \log q(z \mid x) \right]
\tag{4} \label{4}
\end{align}First, note that \ref{4} only contains terms that depend on the variational distribution, so we got rid of intractable terms, which was our goal.Second, note that, as opposed to \ref{1}, we are maximizing (or finding the parameters that maximize the objective).The ELBO objective is actually the negative of \ref{2} plus the logarithm of the evidence term, $\log p(x)$ (and you can easily verify it), that is\begin{align}
\text{ELBO}(q)  = -D_{\text{KL}}(q(z \mid x) \| p(z \mid x)) + \log p(x)
\end{align}which can also be rearranged as\begin{align}
\log p(x) = D_{\text{KL}}(q(z \mid x) \| p(z \mid x)) + \text{ELBO}(q) \tag{5}\label{5}
\end{align}which is your equation (where $\text{ELBO}(q)$ is your $\mathcal{L}$). Therefore, your equation is true by definition, i.e. we define the ELBO such that \ref{5} is true. However, note that we haven't defined the ELBO in the way we dit only for the sake of it, but because it is a lower bound on the log evidence (and this follows from the fact that the KL divergence is never negative)."
"In reinforcement learning, is the value of terminal/goal state always zero?","
Let's assume we are in a $3 \times 3$ grid world with states numbered as $0,1, \dots, 8$. Suppose that the goal state is $8$, the reward of landing in the goal state is $10$, and the reward of just wandering around in the grid world is $0$. Is the state-value of state $8$ always $0$?
","['reinforcement-learning', 'environment', 'value-functions']","In reinforcement learning, is the value of terminal/goal state always zero?Yes, always for episodic problems, the value of a terminal state is always zero, from the definition.The value of a state $v(s)$ is the expected sum (perhaps discounted) of rewards from all future time steps. There are no future time steps when in a terminal state, so this sum must be zero.For the sake of consistent maths notation, you can consider a terminal state to be ""absorbing"", i.e. any transition out of it results in zero reward and returning to the same terminal state. Then you can use the definition of value function to show the same thing:$$v_{\pi}(s) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_{t} = s]$$If $s = s_T$, the terminal state, then all the ""future rewards"" from $k=0$ onwards starting with reward $R_{t+1}$ must be zero. This is consistent with the reward $R_{t}$, i.e. the reward when transitioning to the terminal state, being any value.You can show similar using action value functions, if you accept a ""null"" action in the terminal state."
"Are there optimizers that schedule their learning rate, momentum etc. autonomously?","
I'm aware there are some optimizer such as Adam that adjust the learning rate for each dimension during training. However, afaik, the maximum learning rate they can have is still determined by the user's input. 
So, I wonder if there are optimizers that can increase/decrease their overall learning rate and other parameters (such as momentum or even weight decay) autonomously depending on some metric, e.g., validation loss, running average of gradients etc. ?
","['optimization', 'hyperparameter-optimization', 'adam']",
How to represent and work with the feature matrix for graph convolutional network (GCN) if the number of features for each node is different?,"
I have a question regarding features representation for graph convolutional neural network.
For my case, all nodes have a different number of features, and for now, I don't really understand how should I work with these constraints. I can not just reduce the number of features or add meaningless features in order to make the number of features on each node the same - because it will add to much extra noise to the network.
Are there any ways to solve this problem? How should I construct the feature matrix?
I'll appreciate any help and if you have any links to papers that solve this problem.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'geometric-deep-learning']",
Improve prediction with LSTMs when data have no particular trend (complex),"
I have a deep learning problem, I am working with the CMAPSS dataset, which contains data simulating the degradation of several aircraft engines. The aim is to predict from data collected on a machine in full operation, the remaining useful time at this machine. My problem is the following when the features (sensor data) have a specific trend (either up or down), my model (LSTMs) predicts good results but when the data have no trend, my deep learning model gets a very bad score. I must specify that I work with sequential data. my dataset contains several aircraft engines with data recorded by the sensors. my question is how to process the data that has no particular trend in deep learning.
You will find below some pictures of my dataset where, RUL: is the life of a machine, unit: the machine identity and s1 to s3 is the sensor data.

",['deep-learning'],
What are the current tools and techniques for image segmentation in order of pragmatism?,"
To explain what I mean I'll depict the two extremes and something in the middle.
1) Most pragmatic: If you need to just segment a few images for a design project, forget AI. Go into Adobe Photoshop and hand select the outline of the object you need to extract.
2) Middle ground: If you need to build a reasonably accurate app for human aided segmentation of images, use a pre-trained model on a well known architecture.
3) Least pragmatic: If you need to reach unprecedented levels of accuracy on a large volume of images. Do heavily funded research on new and better methods of image segmentation.
So I'm most interested in painting out the spectrum for that middle ground. That is, how much of the wheel needs to be reinvented versus the complexity of the problem.
For example (and this is what lead me here), I need to segment out dogs from several hundred photos that owners have taken. The dog is probably going to be among the main subjects of the photos. Do I need to reinvent the wheel (design an architecture)? Probably not. Do I even need to change the tyres (train my own model)? I'm guessing not. Do I need to code at all? I'm not sure.
While I'm happy to get answers about my use case, it would be awesome if someone could map out the spectrum on my unfinished rainbow.
","['deep-learning', 'computer-vision', 'image-segmentation']",
Why is exp used in encoder of VAE instead of using the value of standard deviation alone?,"
There's one VAE example here: 
https://towardsdatascience.com/teaching-a-variational-autoencoder-vae-to-draw-mnist-characters-978675c95776.
And the source code of encoder can be found at the following URL: https://gist.github.com/FelixMohr/29e1d5b1f3fd1b6374dfd3b68c2cdbac#file-vae-py.
The author is using $e$ (natural exponential) for calculating values of the embedding vector:
$$z = \mu + \epsilon \times e^{\sigma}$$
where $\mu$ is the mean, $\epsilon$ a small random number and $\sigma$ the standard deviation.
Or in code
z  = mn + tf.multiply(epsilon, tf.exp(sd))

It's not related to the code (practical programming), but why using natural exponential instead of:
$$z = \mu + \epsilon \times \sigma$$
","['machine-learning', 'autoencoders', 'variational-autoencoder', 'notation']","In the source code, the author defines sd bywhich means that $\operatorname{sd}\in \mathbb{R}^n$.
In particular, the support over sd includes negative numbers, which is something we want to avoid.
Since standard deviations are always nonnegative, we can exponentiate to get us in the correct domain.
This is a case where the variable is inappropriately named.
Here, sd is not the standard deviation itself but rather the logarithm of the standard deviation. This allows it to be predicted as the output of a layer in a neural network, so extracting the predicted value of the standard deviation would require exponentiation."
How do I train a multiple-speaker model (speech synthesis) based on Tacotron 2 and espnet?,"
I'm new to Speech Synthesis & Deep Learning. Recently, I got a task as described below:
I have problem in training a multi-speaker model which should be created by Tacotron2. And I was told I can get some ideas from espnet, which is a end-to-end audio tools library. In this way, I found a good dataset called libritts: http://www.openslr.org/60/. And it's also found at espnet: https://github.com/espnet/espnet#tts-results


Here is my initial thoughtï¼š

Download libritts corpus / Read the espnet code: ../espnet/egs/libritts/tts1/run.sh, learning how to train the libritts corpus by pytorch-backend. 


Difficulty: But I cannot get it across that how the author trained a libritts.tacotron2.v1 as I didn't found anything about tacotron2 along those shells related to run.sh. Maybe He didn't make those codes open-source.


Read the tacotron2 code and tune it into a multi-speaker network:


Difficulty: I found the code is really complex.... I just got lost in reading these codes, without a clear understanding about how to tune this model. Cause Tacotron2 was designed with a LJSpeech Dataset(only 1 person).


Training the multi-speaker model with tiny set of dataset (http://www.openslr.org/60/) to save time.



they contains about 110 people's data, which can be enough for my scenario.

In the end:
Coud you please help me about my questions. I've been puzzled by this problem for a long time...
","['deep-learning', 'recurrent-neural-networks', 'audio-processing', 'speech-recognition', 'speech-synthesis']",
How to estimate the capacity of a neural network?,"
Is it possible to estimate the capacity of a neural network model? If so, what are the techniques involved?
","['neural-networks', 'deep-learning', 'computational-learning-theory', 'vc-dimension', 'capacity']",
Should I choose the model with highest validation accuracy or the model with highest mean of training and validation accuracy?,"
I'm training a deep network in Keras on some images for a binary classification (I have around 12K images). Once in a while, I collect some false positives and add them to my training sets and re-train for higher accuracy.
I split my training into 20/80 percent for training/validation sets.
Now, my question is: which resulting model should I use? Always the one with higher validation accuracy, or maybe the higher mean of training and validation accuracy? Which one of the two would you prefer?
Epoch #38: training acc: 0.924, validation acc: 0.944
Epoch #90: training acc: 0.952, validation acc: 0.932

","['deep-learning', 'training', 'accuracy', 'cross-validation', 'early-stopping']","Neither of the above mentioned methods could be a potent indicator of the performance of a model. A simple way to train the model just enough so that it generalizes well on unknown datasets would be to monitor the validation loss. Training should be stopped once the validation loss progressively starts increasing over multiple epochs. Beyond this point, the model learns the statistical noise within the data and starts overfitting.This technique of Early stopping could be implemented in Keras with the help of a callback function:The Loss and Accuracy thresholds can be estimated after a trial run of the model by monitoring the validation/training error graph."
Interpolating image to increase resolution before feeding it to a neural network,"
Interpolation is a common way to make an image fit the right input shape for a neural network. 
But is there any point in using interpolation to make it easier for the network to learn?
I assume interpolation adds no extra information to the input; It only uses existing information to increase the resolution and fill missing values.
However, sometimes I have observed that while I can not see anything with my human eye, using some kind of advanced interpolation technique such as b-spline interpolation makes it crystal clear that the object i am looking for is in the image, especially in the domain of infrared images.
So, is there any benefit for using interpolation rather than feeding a low dimensional image to a neural network?
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'computer-vision', 'interpolation']",
On which data evaluate an object detection model ? (similar or real life data ?),"
I'm training an object detection model (SSD300) to detect and classify body poses in thermal images.
Even I have more than 2k different poses, but the background does not change much (I have only 5 different points of view).
I trained my model on these images (70% for the training and 30% for validation).
Now, I want to evaluate the model on an unbiased dataset.
Should I keep images of my dataset for this purpose or should I use a real life dataset ?
(A good solution would be to have a real life training set, but I don't have)
I tried both, but as expected, I have an mAP=0.9 when evaluated on similar pictures and mAP=0.5 when evaluated on completely different images.
Bonus question: is mAP a relevant metric when I want to show result to a client ? (e.g a client doesn't understand if I tell him ""my model has a mAP=0.7"")
Precision-Recall ? (but I have to choose a pose classification threshold...)
","['deep-learning', 'datasets', 'object-detection']",
When is max pooling exactly applied in convolutional neural networks?,"
When using convolutional networks on images with multiple channels, do we max pool after we sum the feature map from each channel, or do we max pool each feature map separately and then sum? 
What's the intuition behind this, or is there a difference between the two?
","['convolutional-neural-networks', 'convolution']",
Error when using tensorflow HMC to marginalise GPR hyperparameters,"
I originally posted on SO (original post) but was suggested to post here.
I would like to use tensorflow (version 2) to use gaussian process regression to fit some data and I found the google colab example online here 1. I have turned some of this notebook into a minimal example that is below.
Sometimes the code fails with the following error when using MCMC to marginalize the hyperparameters: and I was wondering if anyone has seen this before or knows how to get around this?
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input matrix is not invertible.
     [[{{node mcmc_sample_chain/trace_scan/while/body/_168/smart_for_loop/while/body/_842/dual_averaging_step_size_adaptation___init__/_one_step/transformed_kernel_one_step/mh_one_step/hmc_kernel_one_step/leapfrog_integrate/while/body/_1244/leapfrog_integrate_one_step/maybe_call_fn_and_grads/value_and_gradients/value_and_gradient/gradients/leapfrog_integrate_one_step/maybe_call_fn_and_grads/value_and_gradients/value_and_gradient/PartitionedCall_grad/PartitionedCall/gradients/JointDistributionNamed/log_prob/JointDistributionNamed_log_prob_GaussianProcess/log_prob/JointDistributionNamed_log_prob_GaussianProcess/get_marginal_distribution/Cholesky_grad/MatrixTriangularSolve}}]] [Op:__inference_do_sampling_113645]

Function call stack:
do_sampling

1 https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Process_Regression_In_TFP.ipynb#scrollTo=jw-_1yC50xaM
Note that some of code below is a bit redundant but it should in some sections but it should be able to reproduce the error.
Thanks!
import time

import numpy as np
import tensorflow.compat.v2 as tf
import tensorflow_probability as tfp
tfb = tfp.bijectors
tfd = tfp.distributions
tfk = tfp.math.psd_kernels
tf.enable_v2_behavior()

import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
#%pylab inline
# Configure plot defaults
plt.rcParams['axes.facecolor'] = 'white'
plt.rcParams['grid.color'] = '#666666'
#%config InlineBackend.figure_format = 'png'

def sinusoid(x):
  return np.sin(3 * np.pi * x[..., 0])

def generate_1d_data(num_training_points, observation_noise_variance):
  """"""Generate noisy sinusoidal observations at a random set of points.

  Returns:
     observation_index_points, observations
  """"""
  index_points_ = np.random.uniform(-1., 1., (num_training_points, 1))
  index_points_ = index_points_.astype(np.float64)
  # y = f(x) + noise
  observations_ = (sinusoid(index_points_) +
                   np.random.normal(loc=0,
                                    scale=np.sqrt(observation_noise_variance),
                                    size=(num_training_points)))
  return index_points_, observations_

# Generate training data with a known noise level (we'll later try to recover
# this value from the data).
NUM_TRAINING_POINTS = 100
observation_index_points_, observations_ = generate_1d_data(
    num_training_points=NUM_TRAINING_POINTS,
    observation_noise_variance=.1)

def build_gp(amplitude, length_scale, observation_noise_variance):
  """"""Defines the conditional dist. of GP outputs, given kernel parameters.""""""

  # Create the covariance kernel, which will be shared between the prior (which we
  # use for maximum likelihood training) and the posterior (which we use for
  # posterior predictive sampling)
  kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)

  # Create the GP prior distribution, which we will use to train the model
  # parameters.
  return tfd.GaussianProcess(
      kernel=kernel,
      index_points=observation_index_points_,
      observation_noise_variance=observation_noise_variance)

gp_joint_model = tfd.JointDistributionNamed({
    'amplitude': tfd.LogNormal(loc=0., scale=np.float64(1.)),
    'length_scale': tfd.LogNormal(loc=0., scale=np.float64(1.)),
    'observation_noise_variance': tfd.LogNormal(loc=0., scale=np.float64(1.)),
    'observations': build_gp,
})

x = gp_joint_model.sample()
lp = gp_joint_model.log_prob(x)

print(""sampled {}"".format(x))
print(""log_prob of sample: {}"".format(lp))

# Create the trainable model parameters, which we'll subsequently optimize.
# Note that we constrain them to be strictly positive.

constrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())

amplitude_var = tfp.util.TransformedVariable(
    initial_value=1.,
    bijector=constrain_positive,
    name='amplitude',
    dtype=np.float64)

length_scale_var = tfp.util.TransformedVariable(
    initial_value=1.,
    bijector=constrain_positive,
    name='length_scale',
    dtype=np.float64)

observation_noise_variance_var = tfp.util.TransformedVariable(
    initial_value=1.,
    bijector=constrain_positive,
    name='observation_noise_variance_var',
    dtype=np.float64)

trainable_variables = [v.trainable_variables[0] for v in 
                       [amplitude_var,
                       length_scale_var,
                       observation_noise_variance_var]]
# Use `tf.function` to trace the loss for more efficient evaluation.
@tf.function(autograph=False, experimental_compile=False)
def target_log_prob(amplitude, length_scale, observation_noise_variance):
  return gp_joint_model.log_prob({
      'amplitude': amplitude,
      'length_scale': length_scale,
      'observation_noise_variance': observation_noise_variance,
      'observations': observations_
  })

# Now we optimize the model parameters.
num_iters = 1000
optimizer = tf.optimizers.Adam(learning_rate=.01)

# Store the likelihood values during training, so we can plot the progress
lls_ = np.zeros(num_iters, np.float64)
for i in range(num_iters):
  with tf.GradientTape() as tape:
    loss = -target_log_prob(amplitude_var, length_scale_var,
                            observation_noise_variance_var)
  grads = tape.gradient(loss, trainable_variables)
  optimizer.apply_gradients(zip(grads, trainable_variables))
  lls_[i] = loss

print('Trained parameters:')
print('amplitude: {}'.format(amplitude_var._value().numpy()))
print('length_scale: {}'.format(length_scale_var._value().numpy()))
print('observation_noise_variance: {}'.format(observation_noise_variance_var._value().numpy()))


num_results = 100
num_burnin_steps = 50


sampler = tfp.mcmc.TransformedTransitionKernel(
    tfp.mcmc.HamiltonianMonteCarlo(
        target_log_prob_fn=target_log_prob,
        step_size=tf.cast(0.1, tf.float64),
        num_leapfrog_steps=8),
    bijector=[constrain_positive, constrain_positive, constrain_positive])

adaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(
    inner_kernel=sampler,
    num_adaptation_steps=int(0.8 * num_burnin_steps),
    target_accept_prob=tf.cast(0.75, tf.float64))

initial_state = [tf.cast(x, tf.float64) for x in [1., 1., 1.]]

# Speed up sampling by tracing with `tf.function`.
@tf.function(autograph=False, experimental_compile=False)
def do_sampling():
    return tfp.mcmc.sample_chain(
      kernel=adaptive_sampler,
      current_state=initial_state,
      num_results=num_results,
      num_burnin_steps=num_burnin_steps,
      trace_fn=lambda current_state, kernel_results: kernel_results)

t0 = time.time()
samples, kernel_results = do_sampling()
t1 = time.time()
print(""Inference ran in {:.2f}s."".format(t1-t0))

","['tensorflow', 'gaussian-process']",
Best architecture to learn image rotation?,"
Given an input image and an angle I want the output to be the image rotated at the given angle.
So I want to train a neural network to do this from scratch.
What sort of archetecture do you think would work for this if I want it to be lossless?
I'm thinking of this archetecture:
256x256 image 
--> convolutions to 64x64 image with 4 channels
--> convolutions to 32x32 image with 16 channels and so on
until a 1 pixel image with 256x256 channels.
And then combine this with the input angle, and then a series of deconvolutions back up to 256x256.
Do you think this would work? Could this be trained as a general rotation machine? Or is there a better archetecture?
I would also like to train the same archetecture to do other transforms.
","['convolutional-neural-networks', 'image-generation']",
Acoustic Input Data: Decibel or Pascals,"
In acoustics decibel levels were defined to solve an issue with showing values that are interpretive, understandable, and easy to communicate in contrast to intensity or pressure in Pascals. 
$dB = 10*\log({\frac{p^2}{p_{ref}^2}})$
This log scale helps human understanding of an acoustic signal because human hearing is capable of discerning the difference of about 1 dB ref 20 $uPa$. However, if I have input data for a 2D CNN would it make more sense to have my input data be in Decibels where it is a logarithmic or to have raw pressures? 
Would one or the other benefit my models learning?
","['convolutional-neural-networks', 'data-preprocessing']",
Is there a reason to choose regular momentum over Nesterov momentum for neural networks?,"
I've been reading about Nesterov momentum from here and it seems like a nice improvement over regular momentum with no extra cost whatsoever.
However, is this really the case? Are there instances where regular momentum performs better than Nesterov momentum or is Nesterov momentum performs at least as good as the regular momentum all the time?
","['neural-networks', 'comparison', 'objective-functions', 'gradient-descent', 'hyperparameter-optimization']","The book Deep Learning by Goodfellow, Bengio, and Courville says (Sec 8.3.3, p 292 in my copy) states thatUnfortunately, in the stochastic gradient case, Nesterov momentum does not improve the rate of convergence.I'm not sure why this is, but the theoretical advantage depends on a convex problem, and from this, it sounds like the practical advantage does too - or at least, that it isn't applicable to typical neural network landscapes.Perhaps it can be implemented more efficiently, but it seems to me that you need to do parameter updates twice (in order to calculate the gradient in a place you aren't moving two) and thus Nesterov requires more computation and memory than plain ole momentum."
Specific Neural Network Subtype for Automatic Web Scraping (Hyperlink Identification),"
For a personal project, I'm trying to download files from a specific set of websites using a web scraper. The scraper has to navigate multiple webpages to get to the files I want to download. I'd like to use AI to find each successive link in order to avoid the inflexibility of hardcoded DOM paths. In other words, I want an AI system that replicates the way that a human would click through links to get to a download page.
This seems like a task for a supervised neural network, where the input would be the HTML page and the output would be the href link to the next webpage in the search. But that's about the extent of my AI knowledge. 
Which subtype of neural network would be most effective for this kind of problem?
Note, I have looked at this related question. My problem would probably fall under bullet two.
","['neural-networks', 'supervised-learning']",
Nuances of Turing test requirements,"
My understanding is that there is no singular ""The Turing Test Ruleset"" and competitions don't all do it the exact same way. Still, I'm wondering about some commonly accepted rules and their nuances. My Googling is not producing any specifics about this.
I think most people agree that the purpose of the humans who talk to the judges is to just act normal. In the so-called ""passed Turing test"" instances where the humans tried to fool judges into thinking they were AIs, I would say the tests should be thrown out and I've seen critics in the field agree with that.
But this question is more about how the judge should act.
Let's say we are doing a competition where the threshold is for 40% of judges to call an AI a human after 5 minutes of chat.
During the chat, is the judge supposed to try to trick the potential AI into revealing itself, or is the judge supposed to attempt to act as unbiased and natural as possible, as if it is accepted the potential AI is human, and judge based on a completely natural conversation?
For example, asking ""What is the value of PI out 10 digits?"" or ""What is 123456 times 654321?"" or ""If you saw a bunny and a dime stuck on the road with a truck about to hit them, which one would you pick up?"" would be trying to trick AIs into revealing themselves because you are relying on exploiting the fact that the AI might tell you the correct answer or the inhuman answer.
This is as opposed to simply carrying on a natural and normal conversation, with no biases or expectations. If you came upon someone on the street you would not spend 5 minutes trying to hurriedly ask ridiculous interrogation questions in an effort to prove the other person was an AI.
So is the point of a Turing test generally assumed to be an attempt to flush out AIs or an attempt to judge their natural human conversation without interrogation-like prejudice?
",['turing-test'],
Post-classification after inference,"
I designed a fire detection using Deep Learning based classification approach. In my training dataset, I have both fire and fire smokes are supposed to be detected (all under ""fire""; mostly real fires are detected. Fire smokes are less accurate). 
Now after months, I need to differentiate them in my detection results. It would be difficult to retrain each class separately now. Another option coming into my mind is building a binary classification after the main one, getting the main detections as input and saying which of the two it belongs to. However, I may miss some fire smokes I believe because that's less accurate.
Is there any other approaches? What are pros/cons of various approaches?
","['machine-learning', 'deep-learning', 'classification', 'deep-neural-networks', 'object-detection']",
Mathematical foundations of the ability to learn,"
I am an undergraduate student in applied mathematics with an interest in artificial intelligence. I am currently exploring topics where I could do research. Coming from a mathematical background I am interested in the question: Can we mathematically establish that a certain AI system has the ability to learn a task given some examples of how it should be done? 
I would like to know what research has been done on this topic and also what mathematical tools could be helpful in answering such questions.
","['machine-learning', 'computational-learning-theory', 'math']","Computational learning theory (or just learning theory, abbreviated as CLT, COLT, or LT) is devoted to the mathematical and computational analysis of machine learning algorithms, so it is concerned with the learnability (i.e. generalization, bounds, efficiency, etc.) of certain tasks, given a learner (or a learning algorithm), a hypothesis space, data, etc.CLT can be divided into two subfields:The most famous and studied SLT frameworks might be PAC learning and the VC theory (which extends PAC learning to infinite-dimensional hypothesis spaces).There are many good resources on CLT, some of which can be found in this answer.Here's a related question on this site: What sort of mathematical problems are there in AI that people are working on?."
What is the mean in the variational auto-encoder?,"
Here's a diagram of a variational auto-encoder.

There are 2 nodes before the sample (encoding vector). One is the mean, one is the standard deviation. The mean one is confusing. 
Is it the mean of values or is it the mean deviation?
$$\text{mean} = \dfrac{X_i+..+X_n}{N}$$
$$\text{mean deviation} = \dfrac{[X_i|+..+|X_n|}{N}$$
","['neural-networks', 'machine-learning', 'math', 'autoencoders', 'variational-autoencoder']","Kind of neither, although leaning towards the first definition of the Mean as a simple average of values.It's a distribution parameter of the Gaussian, so it's the expected average of samples as the number of samples approaches infinity.The distrinction is that you could draw three samples, -2, 0 and -1, from a Standard Normal - the mean of samples would be -1, but the distribution mean is still 0.From a network architecture PoV, it's a learnable transformation applied on top of samples from a Standard Normal, so you only need to learn the transformation and get sampling for 'free'."
Recognize carp and give them a unique id,"
For my internship assignment I have to implement a proof of concept for an application that is supposed to scan a picture with a carp on it and identify which carp this is. All of the carps that are going to be scanned are known and they all exist in the database, so no new carps are scanned.
Is this possible? I've been searching a lot about this topic and the only thing I found is customvision.ai, but for using this I need to have at least 15 pictures of the same carp per tag, but the client only has 1 picture per carp.
What are your recommendations or do you think this is not possible?
","['machine-learning', 'object-recognition']","I'm not sure I understood your question entirely so please correct me if i'm wrong. You're having all the carp tagged, so if I give you a picture of any of them you know exactly which one is in the picture right? If that is the case then you're dealing with a classic classification problem. One simple way of solving such problem will be to use CNN on the input image to extract features, and at the end of the network have N neurons, where each neuron matches one carp. Just apply a softmax over all the inputs to have a probability distribution, and select the highest value (or if the values is highest then a threshold). Regarding the number of labeled examples per class, having only one might be a problem. I would suggest you to look into few shot learning, which are trying to solve this exact problem (training decent models with limited training data).  "
Why is Standard Deviation based on L2 Variance and not L1 Variance,"
Standard deviation and variance are in statistics but the formula for variance is somehow related to the L1 and L2.
Mathematically (L2 in machine learning sense),
$$Variance = \dfrac{(X_1-Mean)^2+..+(X_n-Mean)^2}{N}$$
and,
$$Standard\ Deviation= \sqrt(Variance)$$
Why shouldn't it be (L1 in machine learning sense):
$$Variance = \dfrac{|X_1-Mean|+..+|X_n-Mean|}{N}$$
and,
$$Standard\ Deviation= Variance$$
","['machine-learning', 'math', 'statistics']","I've found the answer, the L2 is Standard Deviation, and the L1 is Mean Deviation. Standard deviation describes the variation better and the values are always different on different sets of X while the Mean Deviation gives the same values sometimes.*Footnote: Why square the differences? If we just add up the differences from the mean ... the negatives cancel the positives:standard deviation why a       (4 + 4 âˆ’ 4 âˆ’ 4) / 4  = 0 So that won't work.
  How about we use absolute values?standard deviation why a       (|4| + |4| + |âˆ’4| + |âˆ’4|) / 4  =  (4 + 4 + 4 +
  4) / 4 = 4 That looks good (and is the Mean Deviation), but what about
  this case:standard deviation why b       (|7| + |1| + |âˆ’6| + |âˆ’2|) / 4  =  (7 + 1 + 6 +
  2) / 4 = 4 Oh No! It also gives a value of 4, Even though the differences
  are more spread out.So let us try squaring each difference (and taking the square root at
  the end):standard deviation why a      âˆš( (42 + 42 + 42 + 42) / 4 ) = âˆš( 644) = 4
  standard deviation why b      âˆš( (72 + 12 + 62 + 22) / 4 ) = âˆš( 904) = 4.74...Reference:
https://www.mathsisfun.com/data/standard-deviation.html"
Is Gradient Descent algorithm a part of Calculus of Variations?,"
As in https://en.wikipedia.org/wiki/Calculus_of_variations

The calculus of variations is a field of mathematical analysis that
  uses variations, which are small changes in functions and functionals,
  to find maxima and minima of functionals

The Gradient Descent algorithm is also a method to find minima of a function, it it a part of the Calculus of Variations?
","['machine-learning', 'gradient-descent', 'variational-autoencoder', 'calculus', 'math']",
What is the appropriate RNN structure to do Sentiment Analysis with multiple dependent ratings?,"
Suppose we are doing sentiment analysis for a restaurant. Customers can rate the restaurant by #1: how expensive the restaurant is, #2:how good is the food and #3: how likely they will come again. The ratings are dependent,i.e. the more expensive the restaurant is (higher #1), the less likely they will come back (lower #3), but whey will if the food is good (higher #2).
My questions are: is there a good RNN structure(review as input, #1-#3 as output) that can capture and model the dependency among #1 - #3? 
","['recurrent-neural-networks', 'sentiment-analysis']",
Is the following statement about neural networks overclaimed?,"
Is the following statement about neural networks overclaimed?

Neural networks are iterative methods that minimize a loss function defined on the output layer of neurons.

I wrote this statement in the Introduction section of a conference paper. Reviewer got back to me saying that, ""this statement is over claimed without proper citations"". How is this statement over claimed? Is it not obvious that neural networks are iterative methods that try to minimize a cost function?
","['neural-networks', 'terminology', 'definitions']","I think I would never say that neural networks are iterative methods. I would say that iterative methods (e.g. gradient descent) are used to train neural networks (which can be thought of as linear and non-linear models, but mainly non-linear), which is quite different. Maybe you should or wanted to say that deep learning is an area of study where iterative methods are used to train neural networks.It is possible the reviewer is just telling you that this or similar statements have been used a lot and don't really provide any insight, they are misleading, or simply useless or trivial in the context of the paper or conference. Without further clarifications by the reviewer and about your paper and conference, it is difficult to provide more explanations."
FastDownward PDDL Planner Limitations [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I recently had a look at automated planners and experimented a little bit with FastDownward. As I wanted to start a toy project, I created a PDDL model for the ordinary 3D Rubik's Cube (of course using a planner may not be the most efficient approach).
Although my model may not necessary be ""totally correct"" yet, so far it consists of 24 predicates, 12 actions for the respective moves (each with 8 typed parameters, 4 ""edge subcubes"" and 4 ""corner subcubes""). For each of the movable subcubes I have a domain object whose position is basically determined by the respective predicate; overall, at first glance this seemed to me as a model of quite moderate size.
It was indeed not a very complex task to come up with this model and although it currently does not consider orientations of subcubes yet, I simply wanted to give it a try with an instance where only a single move (so application of one action) has to be conducted â€” I assumed that such a plangraph should level off quite soon, so basically after the first layer where a goal state can be reached.
However, as I started the planner, it soon run out of memory and started to page.
Previously, I only read something on PDDL and the respective sections of Russell & Norvig. I took a closer look at the planner itself and found that it transform the PDDL description into some intermediate description. 
I tried to only execute the transformation and after cutting off a third of my Rubik's Cube it â€” at least â€” terminated. I investigated the transformed model file and found that the planner/solver actually instantiates (flattens) the actions with its parameters.
Since the Rubik's Cube has a quite unrestricted domain and each action has apparently 8 parameters (4 corners, 4 edges), this inherently results in a huge number of flattened actions. Even more, although I added precondition constraints to ensure distinctness of the parameters (so the very same subcube cannot be both edgecube $e_1$ and $e_2$), the flattened version still contains these invalid actions.
I have the following questions:

are state-of-the-art planners even suitable for such a problem or are they designed for problems where flattening the actions is of great advantage (e.g. few parameters, moderate number of objects per class, etc.). IMHO this would be a major limitation for their applicability in contrast to e.g. CP solvers. 
can anyone recommend another planner that is more appropriate and that maybe does not perform the model transformation, which seems to be quite expensive for my PDDL spec (e.g. from this list, https://ipc2018-classical.bitbucket.io, i have chosen FastDownward since it seemed to be among the best...)
does PDDL or even FastDownward allow to specify that the parameters have to be distinct (i just found this: https://www.ida.liu.se/~TDDC17/info/labs/planning/writing.shtml â€” search for distinct â€” but this is more than vague), cause this may already lead to a significant reduction of flattened actions.
I'd also be happy for any other recommendations or remarks

","['search', 'planning', 'strips', 'pddl']","You have stumbled upon a common drawback of the vast majority of modern planning technology. The ""flattening"" you refer to is actually called ""grounding"" in the community.
Indeed, grounding is the first step of almost every planner out there.
Planners that don't do this grounding phase are typically referred to as ""lifted planners"", but their availability is far more limited (i.e., you'd likely need to reach out to an author of a paper to get access to their code).That said, many of the planners, Fast Downward included, will try to be smart about the grounding phase. This includes simple things such as making sure that typing for the parameters (and the objects that instantiate them) is adhered to, but goes further with certain checks on reachability.
The process isn't flawless, but it usually does a good job when the true number of ground actions is relatively small. If your model really does have an astronomical number of ground actions, then you're kind of out of luck here.To your questions specifically:They aren't assumed to be distinct. There are situations when you wouldn't want them to be, and so it is up to the designer to add the preconditions that ensure they are distinct (note that this is one excellent way to equip the grounder with extra information about what it should generate).Generally, having actions with many parameters is a recipe for problems that have too many ground actions. Either manually remodeling things yourself, or applying a technique like operator splitting is the approach most often taken.Finally, if you'd like to have a more directed discussion, feel free to join us over on the slack channel. There may be more eyes on the questions you pose there."
What is the name of this neural network architecture with layers that are also connected to non-neighbouring layers?,"
Consider a feedforward neural network. Suppose you have a layer of inputs, which is feedforward to a hidden layer, and feedforward both the input and hidden layers to an output layer.  Is there a name for this architecture? A layer feeds forward around the layer after it?
","['machine-learning', 'terminology', 'feedforward-neural-networks', 'residual-networks']","This could be called a residual neural network (ResNet), which is a neural network with skip connections, that is, connections that skip layers. Here's a screenshot of a figure from the paper Deep Residual Learning for Image Recognition (2015), an important paper that shows the usefulness of these architectures."
Why does the error of my LSTM not decrease after 10 epochs?,"
Despite the problem being very simple, I was wondering why an LSTM network was not able to converge to a decent solution.
import numpy as np
import keras

X_train = np.random.rand(1000)
y_train = X_train
X_train = X_train.reshape((len(X_train), 1, 1))

model= keras.models.Sequential()
model.add(keras.layers.wrappers.Bidirectional(keras.layers.LSTM(1, dropout=0., recurrent_dropout=0.)))
model.add(keras.layers.Dense(1))

optimzer = keras.optimizers.SGD(lr=1e-1)

model.build(input_shape=(None, 1, 1))
model.compile(loss=keras.losses.mean_squared_error, optimizer=optimzer, metrics=['mae'])
history = model.fit(X_train, y_train, batch_size=16, epochs=100)

After 10 epochs, the algorithm seems to have reached its optimal solution (around 1e-4 RMSE), and is not able to improve further the results.
A simple Flatten + Dense network with similar parameters is however able to achieve 1e-13 RMSE.
I'm surprised the LSTM cell does not simply let the value through, is there something I'm missing with my parameters? Is LSTM only good for classification problems?
","['long-short-term-memory', 'convergence']","I think there are some problems with your approach.Firstly, looking at the Keras documentation, LSTM expects an input of shape (batch_size, timesteps, input_dim). You're passing an input of shape (1000, 1, 1), which means that you're having ""sequences"" of 1 timestep.RNNs have been proposed to capture temporal dependencies, but it's impossible to capture such dependencies when the length of each series is 1, and the numbers are randomly generated. If you want to create a more realistic scenario, I would suggest you generate a sine wave, since it has a smooth periodic oscillation. Afterward, increase the timesteps from 1, and you can test on the following timestamps (to make predictions).For the second part, if you think about a normal RNN (I will explain for a simple RNN but you can imagine a similar flow for LSTM) and a Dense layer when applied to 1 timestamp, there are not so many many differences. The dense layer will have $Y=f(XW + b)$, where $X$ is the input, $W$ is the weight matrix, $b$ is the bias and $f$ is the activation function. Whereas RNN will have $Y=f(XW_1 + W_2h_0 + b)$, since is the first timestamp $h_0$ is $0$, so we can reduce it to $Y=f(XW_1 +b)$, which is identical with the Dense layer. I suspect that the results differences are caused by the activation functions, by default Dense layer has no activation function, and LSTM has tanh and sigmoid."
Convert input dataset given in hex addresses to int,"
I have created an LSTM Neural Network which take as input the following format in an .csv file
sinewave
0.841470985
0.873736397
0.90255357
0.927808777
0.949402346
0.967249058
0.98127848
0.991435244

How can I write some code so it can take as input hex addresses and convert them to int ? 
eg the following .xlsx file containing 400.000 samples
0xbfb22b18
0xbfb22b14
0xbfb22b10
0xbfb22b0c
0xbfb22b18
0xbfb22b14
0xbfb22b10
0xbfb22b0c
0xbfb22b18
0xbfb22b14
0xbfb22b10
0xbfb22b0c

","['neural-networks', 'convolutional-neural-networks', 'recurrent-neural-networks', 'prediction', 'time-series']",Add train_data 1st
Formulation of a Markov Decision Process Problem,"
Given a list of $N$ questions. If question $i$ is answered correctly (given probability $p_i$), we receive reward $R_i$; if not the quiz terminates. Find the optimal order of questions to maximize expected reward. (Hint: Optimal policy has an ""index form"".)
I am fairly new to Reinforcement Learning and Markov Decision Problems (MDP). I am aware that the goal of the problem is to maximize the expected reward but I am not sure how exactly to formulate this into an MDP.
This is the approach I thought of:
1) Assume only 2 questions. Then the state space is $S\in \{1,2\}$. 
2) Compute the expected total reward $J = E(R)$ for both cases, when we start with question $1$ and question $2$ and then find the maximum of the two.
3) If we start with $1$, then $$J(S_0 = 1) = p_1(1-p_{2})R_1 + (R_1 + R_2)p_1p_2$$
4) Similarly, if we start with $2$, $$J(S_0 = 2) = p_2(1-p_{1})R_2 + (R_1 + R_2)p_1p_2$$.
To determine the maximum reward of the two, the required condition for $1$ to be the optimal starting question is $$R_1p_1 - R_2p_2 + p_1p_2(R_2 - R_1) \gt 0$$ If the above expression is negative, then we should start with $2$.
I would like to know if the approach is correct and how to proceed further. I am also not sure how to define the action space in this case. Can a dynamic programming approach be used here to find the optimal policy?
","['reinforcement-learning', 'markov-decision-process', 'probability', 'probability-distribution']",
How does Monte Carlo have high variance?,"
I was going through David Silver's lecture on reinforcement learning (lecture 4). At 51:22 he says that Monte Carlo (MC) methods have high variance and zero bias. I understand the zero bias part. It is because it is using the true value of value function for estimation. However, I don't understand the high variance part. Can someone enlighten me?
","['reinforcement-learning', 'monte-carlo-methods', 'temporal-difference-methods', 'bias-variance-tradeoff']",
Possible model to use to find pixel locations of objects,"
I want to make a model that outputs the centre pixel of objects appearing in an image.
My current method involves using a CNN with L2 loss to output an image of equivalent size to the input where each pixel has a value of 1 if it is the center of an object and 0 otherwise. Each input image has roughly ~80 objects.
The problem with this is the CNN learns the easiest way to reduce the error, which is having the entire output be 0, because for 97% of cases that's correct. As such, error decreases but it learns nothing.
What is another potential method for training a network to do something similar? I also tried adding dropout, which made the output a lot more noisy and it seemed to learn ok, but eventually ended up in the same state as before with the entire output being 0, never really seeming to learn how to output the locations of objects.
","['convolutional-neural-networks', 'object-detection', 'regression']",
"What does the figure ""Blackjack Value Function..."" from Sutton represent?","
I came across this graph in David Silver's youtube lecture and Sutton's book on reinforcement learning. 
Can anyone help me understand the graph?
From the graph, for 10000 episodes what i  see is that when we don't have a usable ace we always lose the game except if the sum is 20 or 21. But in case if we have a usable ace there are chances to win when our sum is below 20. I don't know how this is possible.

","['reinforcement-learning', 'monte-carlo-methods', 'sutton-barto']","The left hand graphs are showing you the estimated value function from using Monte Carlo evaluation, after 10,000 episodes. They give a sense of what your value table will look like before convergence.In the case of upper ""usable ace"" chart, the estimates are still showing a lot of inaccuracy due to variance in the data. This is for two main reasons:The probability of getting a usable ace at the start is a fraction of all (around 15%), so the number of samples used to build the chart is lower.There are more variations in play when there is a usable ace, due to the extra flexibility allowed by it, so the end result also varies more, requiring more samples to converge.In addition, if you are looking at the bottom edge of the chart, this represents the player starting with two aces. If you pick on one of the high points (dealer shows 4), then that also reduces the probability of seeing that particular state. So you are looking at a sample size of typically 4-5, but maybe in this case just one sample or maybe two, which the player then happened to go on to win, even though the odds made it unlikely. There is always some chance of winning and dealer showing 4 is a bad start for the dealer, who has a good chance of going bust provided the player does not. If it hadn't happened this time for the ""two aces + dealer showing 4"" state, it may have happened for ""two aces + dealer showing 5"" state. That's due to  the nature of random sampling - if you have hundreds of states to sample, then a few of them are going to behave like outliers purely by chance until you have taken enough samples.In short, 10,000 randomly sampled games are nowhere near enough to reduce error bounds on the value estimates to reasonable numbers for special cases such as starting with two aces. However, you can see in the 10,000 samples charts the beginnings of convergence, especially elsewhere in the chart.From the graph, for 10000 episodes what i see is that when we don't have a usable ace we always lose the game except if the sum is 20 or 21.Actually you don't see that, the expected result is not quite -1.0, but a little higher. So that means there is still a chance to win. Under this player policy, the worst chance is no usable ace and score 19, because the policy will be to ""hit"" and need an Ace or 2 card just to stay in the game. Even then , the value is not quite as low as -1.0, but more like -0.9"
Are PAC learnability and the No Free Lunch theorem contradictory?,"
I am reading the Understanding Machine Learning book by Shalev-Shwartz and Ben-David and based on the definitions of PAC learnability and No Free Lunch Theorem, and my understanding of them it seems like they contradict themselves. I know this is not the case and I am wrong, but I just don't know what I am missing here.
So, a hypothesis class is (agnostic) PAC learnable if there exists a learner A and a function $m_{H}$ s.t. for every $\epsilon,\delta \in (0,1)$ and for every distribution $D$ over $X \times Y$, if $m \geq m_{H}$ the learner can return a hypothesis $h$, with a probability of at least $1 - \delta$
$$ L_{D}(h) \leq min_{h'\in H} L_{D}(h') + \epsilon $$
But, in layman's terms, the NFL theorem states that for prediction tasks, for every learner there exists a distribution on which the learner fails.  
There needs to exists a learner that is successful (defined above) for every distribution $D$ over $X \times Y$ for a hypothesis to be PAC learnable, but according to NFL there exists a distribution for which the learner will fail, aren't these theorems contradicting themselves?
What am I missing or misinterpreting here?
","['machine-learning', 'comparison', 'computational-learning-theory', 'pac-learning', 'no-free-lunch-theorems']","There is no contradiction. 
First, agnostic PAC learnable doesn't mean that the there is a good hypothesis in the hypothesis class; it just means that there is an algorithm that can probably approximately do as well as the best hypothesis in the hypothesis class.Also, these NFL theorems have specific mathematical statements, and hypothesis classes for which they apply are often not the same as the hypothesis class for which PAC-learnability holds. For example, in Understanding Machine Learning  by Shalev-Shwartz and Ben-David, a hypothesis class is agnostic PAC learnable if and only if has finite VC dimension (Theorem 6.7). Here, the algorithm is ERM.
On the other hand, the application of the specific version of NFL that this book uses has Corollary 5.2, that the hypothesis class of all classifiers is not PAC learnable, and note that this hypothesis class has infinite VC dimension, so the Fundamental Theorem of PAC learning does not apply.The main takeaway is that in order to learn, we need some sort of inductive bias (prior information). This can be seen in the form of measuring the complexity of the hypothesis class or using other tools in learning theory. "
What's the best architecture for time series prediction with a long dataset?,"
I have to build a neural network without any architecture limitations which have to predict the next value of a time series.
The dataset is composed of 400.000 values, which are given in hex format. For example
0xbfb22b14
0xbfb22b10
0xbfb22b0c
0xbfb22b18
0xbfb22b14

I think LSTM is suitable for this problem, but I am worried about the length of the input. Would it be a good idea to use CNN?
def structure(step,n_features):
    # define model
    model = Sequential()
    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(step, n_features)))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

What about this one ?
""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
    ""save_dir"": ""saved_models"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 999,
                ""input_timesteps"": 998,
                ""input_dim"": 1,
                ""return_seq"": true
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.05
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 100,
                ""return_seq"": false
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.05
            },
            {
                ""type"": ""dense"",
                ""neurons"": 1,
                ""activation"": ""linear""
            }

","['neural-networks', 'deep-learning', 'prediction', 'time-series']","Yes, LSTM are ideal for this. For even stronger representational capacity, make your LSTM's multi-layered.
Using 1-dimensional convolutions in a CNN is a common way to exctract information from time series too, so there's no harm in trying. Typically, you'll test many models out and take the one that has best validation performance."
Is it a good idea to first train a spiking neural network and then convert it to a conventional neural network?,"
In many papers about artificial spiking neural networks (SNNs), the performance of them is not up to par with traditional ANNs. I have read how some people have converted ANNs to SNNs using various techniques.
There has been work done on using unsupervised learning in SNN to recognise MNIST digits through spike-timing-dependent plasticity (for example, the paper Unsupervised learning of digit recognition using spike-timing-dependent plasticity, by Diehl and Cook, 2015). This form of learning is not possible in traditional ANNs due to their synchronous nature.
I was wondering would it be a good idea to first train an SNN in an unsupervised manner to learn some of the structure of the data. Then convert to a traditional ANN to take advantage of their superior performance with some more training. I can see this being useful for training a network on a sparsely labelled dataset.
I am quite a novice in this area, so I was looking for feedback on any immediate barriers as to why this would not work or if it is even worth doing.
","['neural-networks', 'biology', 'spiking-neural-networks']",
How can I train a RL agent to play board games successfully without human play?,"
How would you go about training an RL Tic Tac Toe (well, any board game, really) application to learn how to play successfully (win the game), without a human having to play against the RL? 
Obviously, it would take a lot longer to train the AI if I have to sit and play ""real"" games against it. Is there a way for me to automate the training? 
I guess creating ""a human player"" to train the AI who just selects random positions on the board won't help the AI to learn properly, as it won't be up against something that's not using a strategy to try to beat it.
","['reinforcement-learning', 'training', 'game-ai']","The approach will vary depending on some features of the game:How many players (two for tic tac toe and many classic games).Whether it is a ""perfect information"" game (yes for chess and tic tac toe), or whether there is significant hidden information (true for most forms of poker)Whether the game is zero sum. Any game with simple win/loss/draw results can be considered zero sum.Branching complexity of the game. Tic tac toe has relatively low branching factor, at worst 9, and reduces by one each turn. The game of Go has branching factoraround 250.Action complexity of the game. A card game like Magic the Gathering has huge complexity even if the number of possible actions at each turn is quite small.The above traits and others make a large impact on the details of which algorithms and approaches will work best (or work at all). As that is very broad, I will not go into that, and I would suggest you ask separate questions about specific games if you take this further and try to implement some self-playing learning agents.It is possible to outline a general approach that would work for many games:You need to have a programmable API for running the game, which allows for code representing a player (called an ""agent"") to receive current state of the game, to pass in a chosen action, and to return results of that action. The results should include whether any of the players has won or lost, and to update internal state of the game ready for next player.There are several kinds of learning algorithms that are suitable for controlling agents and learning through experience. One popular choice would be Reinforcement Learning (RL), but also Genetic Algorithms (GA) can be used for example.Key here is how different types of agent solve the issues of self-play:GA does this very naturally. You create a population of agents, and have them play each other, selecting winners to create the next generation. The main issue with GA approaches is how well they scale with complexity - in general not as well as RL.With RL you can have the current agent play best agent(s) you have created so far - which is most general approach. Or for some games you may be able to have it play both sides at once using the same predictive model - this works because predicting moves by the opposition is a significant part of game play.Without going into lines of code, what you need for self-play is:The game API for automation and scoringOne or more agents that can use the APIA system that takes the results of games between agents and feeds them back so that learning can occur:In a GA, with tournament selection, this could simply be saving a reference - the id of the winner - into a list, until the list has grown large enough to be the parents for the next generation. Other selection methods are also possible, but the general approach is the same - the games are played to help populate the next generation of agents.In RL, typically each game state is saved to memory along with next resulting state or the result win/draw/lose (converted into a reward value e.g. +1/0/-1). This memory is used to calculate and update estimates of future results from any given state (or in some variants used directly to decide best action to take from any given state). Over time, this link between current and next states provides a way for the agent to learn better early game moves that eventually lead to a win for each player.An important ""trick"" in RL is to figure out a way for the model and update rules to reflect opposing goals of players. This is not a consideration in agents that solve optimal control in single agent environments. You either need to make the prediction algorithm predict future reward as seen from perspective of the current agent, or use a global scoring system and re-define one of the agents as trying to minimise the total reward - this latter approach works nicely for two player zero sum games, as both players can then direcly share the same estimator, just using it to different ends.A lot of repetition in a loop. GA's unit of repetition is usually a generation - one complete assessment of all existing agents (although again there are variants). RL's unit of repetition is often smaller, individual episodes, and the learning part of the loop can be called on every single turn if desired. The basic iteration in all cases is:A purely reactive learning agent can do well in simple games, but typically you also want the agent to look ahead to predict future results more directly. You can combine the outputs from a learning model like RL with a forward-planning approach to get the best of both worlds.Forward search methods include minimax/negamax and Monte Carlo Tree Search. These can be combined with learning agents in multiple ways - for instance as well as in planning stages using the RL model, they can also be used to help train a RL model (this is how it is used in Alpha Zero)."
How can I perform lane detection with reinforcement learning?,"
I'm quite new to reinforcement learning and my project will consist of detecting lanes with RL. 
I'm using q-learning and I'm having a hard time thinking how my q table should look like, I mean - what could represent a state. My main idea is to feed the machine with a frame that contains a road picture, which the edge detection function is being applied to (and by thus getting lots of lines that exits in the frame). And train the machine which lines are the correct lane line. I already have a deterministic function that already recognizes the lanes and it will be the function that will teach the machine. I already organized some lane parameters such as (lane length, lane cords, lane color (white or yellow have a better probability to be a lane), lane diameter and the lane incline). 
Now, my only issue is how should I construct the Q-table. Basically, what could represent a state and which lanes or decisions I should reward.
","['reinforcement-learning', 'applications']",
What is the difference between graph semi-supervised learning and normal semi-supervised learning?,"
Whenever I look for papers involving semi-supervised learning, I always find some that talk about graph semi-supervised learning (e.g. A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning). 
What is the difference between graph semi-supervised learning and normal semi-supervised learning?
","['machine-learning', 'comparison', 'geometric-deep-learning', 'graphs', 'semi-supervised-learning']",
RealNVP gives wrong probabilities,"
I am trying to use RealNVP with some data I have (the input size is a 1D vector of size 22). Here is the link to the RealNVP paper and here is a nice, short explanation of it (the paper is pretty long). My code is mainly based on this code from GitHub and below are the main piece that I am using (with slight adjustments). The problem is that the loss is getting negative, which in the definition of my code means that the log-probability of the my data is positive, which in turn means that the probabilities are bigger than 1. This is impossible mathematically, and I see no way how this can happen, from a mathematical point of view. I also couldn't find a mistake in my code. Can someone help me with this? Is there a mistake there? Am I missing something with my understanding of normalizing flows? Thank you! 
class NormalizingFlowModel(nn.Module):

    def __init__(self, prior, flows):
        super().__init__()
        self.prior = prior
        self.flows = nn.ModuleList(flows)

    def forward(self, x):
        m, _ = x.shape
        log_det = torch.zeros(m).cuda()
        for flow in self.flows:
            x, ld = flow.forward(x)
            log_det += ld
        z, prior_logprob = x, self.prior.log_prob(x)
        return z, prior_logprob, log_det

    def inverse(self, z):
        m, _ = z.shape
        log_det = torch.zeros(m).cuda()
        for flow in self.flows[::-1]:
            z, ld = flow.inverse(z)
            log_det += ld
        x = z
        return x, log_det

    def sample(self, n_samples):
        z = self.prior.sample((n_samples,))
        x, _ = self.inverse(z)
        return x


class FCNN_for_NVP(nn.Module):
    """"""
    Simple fully connected neural network to be used for Real NVP
    """"""
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(in_dim, 32),
            nn.Tanh(),
            nn.Linear(32, 32),
            nn.Tanh(),
            nn.Linear(32, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 32),
            nn.Tanh(),
            nn.Linear(32, 32),
            nn.Tanh(),
            nn.Linear(32, out_dim),
        )

    def forward(self, x):
        return self.network(x)


class RealNVP(nn.Module):
    """"""
    Non-volume preserving flow.

    [Dinh et. al. 2017]
    """"""
    def __init__(self, dim, base_network=FCNN_for_NVP):
        super().__init__()
        self.dim = dim
        self.t1 = base_network(dim // 2, dim // 2)
        self.s1 = base_network(dim // 2, dim // 2)
        self.t2 = base_network(dim // 2, dim // 2)
        self.s2 = base_network(dim // 2, dim // 2)

    def forward(self, x):
        lower, upper = x[:,:self.dim // 2], x[:,self.dim // 2:]      
        t1_transformed = self.t1(lower)
        s1_transformed = self.s1(lower)
        upper = t1_transformed + upper * torch.exp(s1_transformed)
        t2_transformed = self.t2(upper)
        s2_transformed = self.s2(upper)
        lower = t2_transformed + lower * torch.exp(s2_transformed)
        z = torch.cat([lower, upper], dim=1)
        log_det = torch.sum(s1_transformed, dim=1) + torch.sum(s2_transformed, dim=1)
        return z, log_det

    def inverse(self, z):
        lower, upper = z[:,:self.dim // 2], z[:,self.dim // 2:]
        t2_transformed = self.t2(upper)
        s2_transformed = self.s2(upper)
        lower = (lower - t2_transformed) * torch.exp(-s2_transformed)
        t1_transformed = self.t1(lower)
        s1_transformed = self.s1(lower)
        upper = (upper - t1_transformed) * torch.exp(-s1_transformed)
        x = torch.cat([lower, upper], dim=1)
        log_det = torch.sum(-s1_transformed, dim=1) + torch.sum(-s2_transformed, dim=1)
        return x, log_det

flow = RealNVP(dim=data.size(1))
flows = [flow for _ in range(1)]
prior = MultivariateNormal(torch.zeros(data.size(1)).cuda(), torch.eye(data.size(1)).cuda())
model = NormalizingFlowModel(prior, flows)
model = model.cuda()

for i in range(10):
    for j, dtt in enumerate(my_dataloader_bkg_only):
        optimizer.zero_grad()
        x = dtt[0].float()
        z, prior_logprob, log_det = model(x)
        logprob = prior_logprob + log_det
        loss = -torch.mean(prior_logprob + log_det)
        loss.backward()
        optimizer.step()
    if i % 1 == 0:
        print(""Saved"")
        best_loss = logprob.mean().data.cpu().numpy()
        print(logprob.mean().data.cpu().numpy(), prior_logprob.mean().data.cpu().numpy(),
                  log_det.mean().data.cpu().numpy())

","['objective-functions', 'pytorch']",
Are model-free and off-policy algorithms the same?,"
In respect of RL, is model-free and off-policy the same thing, just different terminology? If not, what are the differences? I've read that the policy can be thought of as 'the brain', or decision making part, of machine learning application, where it stores its learnings and refers to it when a new action is required in the new state.
","['reinforcement-learning', 'comparison', 'terminology', 'off-policy-methods', 'model-free-methods']","In respect of RL, is model-free and off-policy the same thing, just different terminology? No, they are entirely different terms, with the only thing they have in common is that they are both ways in which an RL agent can vary. An agent is generally either working off-policy or on-policy, and is generally either model-based or model-free. These things can otherwise appear in all four combinations.If not, what are the differences?A model-based learning agent uses knowledge of the environment dynamics in order to make predictions of expected outcomes. A model-free learning agent does not use such knowledge. The model here might be provided explicitly by the developer - that could be code for physics to predict a mechanical system, or it might be the rules of a board game that the agent is allowed to know and query to predict outcomes of actions before taking them. Models can also be learned statistically from experience, although that is harder to make effective.An on-policy agent learns statistically about how it is currently acting, and assuming a control problem, then uses that knowledge to change how it should act in future. An off-policy agent can learn statistically from other observed behaviours (including its own past behaviour, or random and exploratory behaviour) and use that knowledge to understand how a different target behaviour would perform.Off-policy learning is a strict generalisation of on-policy learning and includes on-policy as a special case. However, off-policy learning is also often harder to perform since observations typically contain less relevant data.I've read that the policy can be thought of as 'the brain', or decision making part, of machine learning application, where it stores its learnings and refers to it when a new action is required in the new state.That's basically correct when considering how an agent learns how to behave in an environment.You are assigning a bit too much to the word policy here. A policy is strictly only the mapping from a state to an action (or probability distribution over actions), and often written $\pi(a|s)$, i.e. the probability of taking action $a$ given the agent is in state $s$. The ""brain"" part might include how the agent learns that policy. That could include storing past experience or some summary of past experience in e.g. a neural network. However, outside of machine learning context, a really simple function containing and if/then statement would also be a policy, if the input to the function was a state of the environment, and the output was an action or probabilities of taking a range of actions. Behaving completely randomly is also a policy, but outside of very specific environments (e.g. Rock/Paper/Scissors) it is usually not the optimal thing to do."
What effect does increasing the actions in RL have?,"
Consider a 2D snake game, where the snake has to eat food to become longer. It must avoid hitting walls and biting into her tail.
Such a game could have a different amount of actions:

3 actions: go straight, turn left, turn right (relative to crawling direction)
4 actions: north, east, south, west (absolute direction on the 2D map)
7 actions: a combination of option A and option B (leaves the preferred choice to the player)

While the game in principle is always the same, I would like to understand the impact of the amount of actions on the training of a neuronal network. One obvious thing is the number of output nodes of the neuronal network.
In case A (3 actions), the neuronal network cannot perform an incorrect action. Any of the 3 choices are valid moves.
In case B (4 actions), the net IMHO needs to learn that going into opposite direction does not have the desired effect and the snake continues moving into the old direction.
In case C (7 actions), the net needs to learn both, 1 action is always illegal and the 3 relative actions somehow map to the 3 absolute actions.
How can I consider the learning curve in these situations? Does option B need 25% more training than option A to achieve the same results (same fitness) (similar: option C needs 125% more training time)?
Is giving a negative reward for an impossible move considered cheating, because I do code the rules of the game into the reward logic?
","['machine-learning', 'reinforcement-learning', 'training']",
How can I generate unique random patterns (similar to the ones in Nutella jars)?,"
How can I generate unique patterns, as they did for these Nutella jars? See, for example, the video Algorithm designs seven million different jars of Nutella.

","['neural-networks', 'ai-design', 'python', 'image-processing', 'image-generation']",
How is the depth of the filters of convolutional layers determined? [duplicate],"







This question already has answers here:
                                
                            




How is the depth of a convolutional layer determined?

                                (3 answers)
                            

Closed 1 year ago.



I am a bit confused about the depth of the convolutional filters in a CNN.
At layer 1, there are usually about 40 3x3x3 filters. Each of these filters outputs a 2d array, so the total output of the first layer is 40 2d arrays.
Does the next convolutional filter have a depth of 40? So, would the filter dimensions be 3x3x40?
","['convolutional-neural-networks', 'filters', 'convolutional-layers', 'convolution-arithmetic', 'feature-maps']","Does the next convolutional filter have a depth of 40? So, would the filter dimensions be 3x3x40?Yes. The depth of the next layer $l$ (which corresponds to the number of feature maps) will be 40. If you apply $8$ kernels with a $3\times 3$ window to $l$, then the number of features maps (or the depth) of layer $l+1$ will be $8$. Each of these $8$ kernels has an actual shape of $3 \times 3 \times 40$. Bear in mind that the details of the implementations may change across different libraries.The following simple TensorFlow (version 2.1) and Keras programoutputs the followingwhere conv2d has the output shape (None, 26, 26, 40) because there are 40 filters, each of which will have a $3\times 3 \times 40$ shape.The documentation of the first argument (i.e. filters) of the Conv2D saysfilters â€“ Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).and the documentation of the kernel_size parameter stateskernel_size â€“ An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.It doesn't actually say anything about the depth of the kernels, but this is implied from the depth of the layers.Note that the first layer has $(40*(3*3*1))+40 = 400$ parameters. Where do these numbers come from? Note also that the second Conv2D layer has $(8*(3*3*40))+8 = 2888$ parameters. Try to set the parameter use_bias of the first Conv2D layer to False and see the number of parameters again.Finally, note that this reasoning applies to 2d convolutions. In the case of 3d convolutions, the depth of the kernels could be different than the depth of the input. Check this answer for more details about 3d convolutions."
How can a new metric applied for humans causing danger on railtracks?,"
I am writing myself and was thinking about, what kind of metric can be applied to measure the ""dangerousness"" of a human being on a railtrack? For example detecting if a human is running on the rails?
Maybe predicting the human movement towards rails, the distance to the vehicle (ego perspective) or something else?
","['computer-vision', 'human-computer-interaction']",
"In machine learning, how can we overcome the restrictive nature of conjunctive space?","
In machine learning, problem space can be represented through concept space, instance space version space and hypothesis space. These problem spaces used the conjunctive space and are very restrictive one and also in the above-mentioned representations of problem spaces, it is not sure that the true concept lies within conjunctive space.
So, let's say, if we have a bigger search space and want to overcome the restrictive nature of conjunctive space, then how can we represent our problem space? Secondly, in a given scenario which algorithm is used for our problem space to represent the learning problem?
","['machine-learning', 'algorithm', 'genetic-algorithms', 'problem-solving', 'intelligence']",
Why are the terms classification and prediction used as synonyms in the context of deep learning?,"
Why are the terms classification and prediction used as synonyms especially when it comes to deep learning? For example, a CNN predicts the handwritten digit.
To me, a prediction is telling the next step in a sequence, whereas classification is to put labels on (a finite set of) data.
","['deep-learning', 'classification', 'comparison', 'terminology', 'prediction']","Many people confuse and misuse the two terms, classification and prediction (or classify and predict).  This is because in many cases classification techniques are being used for prediction purposes which creates part of the confusion to others who then use the term â€˜predictionâ€™ (or â€˜predictâ€™) inappropriately.Your understanding of the definitions of classification and prediction is mostly correct and you are absolutely correct that there are many people using the terms synonymously, sometimes correctly but I believe mostly erroneously.  There are many good articles elaborating on the two and I have added some links and excerpts at the end of this answer.  What these articles don't cover is that many forecasting (i.e. prediction) researchers and practitioners will use conventional classifiers to predict the future state of a time series or data sequence.  More advanced researchers and practitioners will use time-recurrent models, which learn temporal patterns.  These are still called classifiers but the for purpose of prediction.There are more papers written on this use of classifiers, conventional and time-recurrent type classifiers, for time series than the use of regressor models!  This adds to the confusion in the data science and machine learning community in the usage of the terms â€˜classifyâ€™ and â€˜predict'.  Galit Shmueli sums it up best in his paper, â€œTo Explain or to Predict?â€, where he states: â€œConflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge.â€There is also the opposite problem where people will confuse regression models with classification.  See the first article below.Classification vs. Prediction, by Professor Frank HarrellExcerpt:
By not thinking probabilistically, machine learning advocates frequently utilize classifiers instead of using risk prediction models. The situation has gotten acute: many machine learning experts actually label logistic regression as a classification method (it is not). It is important to think about what classification really implies. Classification is in effect a decision. Optimum decisions require making full use of available data, developing predictions, and applying a loss/utility/cost function to make a decision that, for example, minimizes expected loss or maximizes expected utility. Different end users have different utility functions. In risk assessment this leads to their having different risk thresholds for action. Classification assumes that every user has the same utility function and that the utility function implied by the classification system is that utility function.To Explain or to Predict?, by Galit ShmueliAbstract. Statistical modeling is a powerful tool for developing and testing
theories by way of causal explanation, prediction, and description. In many
disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are
inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing
scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the
many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction
between explanatory and predictive modeling, to discuss its sources, and to
reveal the practical implications of the distinction to each step in the modeling process.What is the difference between classification and prediction?, from KDnuggetsIf one does a decision tree analysis, what is the result? A classification? A prediction?Gregory Piatetsky-Shapiro answers:
The decision tree is a classification model, applied to existing data. If you apply it to new data, for which the class is unknown, you also get a prediction of the class.The assumption is that the new data comes from the similar distribution as the data you used to build your decision tree. In many cases this is a correct assumption and that is why you can use the decision tree for building a predictive model.When Classification and Prediction are not the same?Gregory Piatetsky-Shapiro answers:
It is a matter of definition. If you are trying to classify existing data, e.g. group patients based on their known medical data and treatment outcome, I would call it a classification. If you use a classification model to predict the treatment outcome for a new patient, it would be a prediction.gabrielac adds
In the book ""Data Mining Concepts and Techniques"", Han and Kamber's view is that predicting class labels is classification, and predicting values (e.g. using regression techniques) is prediction.Other people prefer to use ""estimation"" for predicting continuous values."
How to avoid rapid actuator movements in favor of smooth movements in a continuous space and action space problem?,"
I'm working on a continuous state / continuous action controller. It shall control a certain roll angle of an aircraft by issuing the correct aileron commands (in $[-1, 1]$).
To this end, I use a neural network and the DDPG algorithm, which shows promising results after about 20 minutes of training.
I stripped down the presented state to the model to only the roll angle and the angular velocity, so that the neural network is not overwhelmed by state inputs.
So it's a 2 input / 1 output model to perform the control task.
In test runs, it looks mostly good, but sometimes, the controller starts thrashing, i.e. it outputs flittering commands, like in a very fast bangbang-controlm which causes a rapid movement of the elevator.

Even though this behavior kind of maintains the desired target value, this behavior is absolutely undesirable. Instead, it should keep the output smooth. So far, I was not able to detect any special disturbance that starts this behavior. Yet it comes out of the blue.
Does anybody have an idea or a hint (maybe a paper reference) on how to incorporate some element (maybe reward shaping during the training) to avoid such behavior? How to avoid rapid actuator movements in favor of smooth movements?
I tried to include the last action in the presented state and add a punishment component in my reward, but this did not really help. So obviously, I do something wrong.
","['reinforcement-learning', 'rewards', 'ddpg', 'reward-shaping', 'reward-design']","After some research on the subject, I found a possible solution to my problem of high frequency oscillations in continuous control using DDPG:I added a reward component based on the actuator movement, i. e. the delta of actions from one step to the next.Excessive action changes are punished now and this could mitigate the tendency to oscillate. The solution is nnot really perfect, but it works for the moment.This finding is detailed out in the ""Reward Engineering"" section of my master's thesis. Please have a look into https://github.com/opt12/Markov-Pilot/tree/master/thesis I'll be glad to get feedback on it. And I'll be glad to hear better solutions than adding a delta-punishment.Regards, Felix"
Why is my model accuracy high in train-test split but actually worse than chance in validation set?,"
I have trained a XGboost model to predict survival for the Kaggle Titanic ML competition.
As with all Kaggle competitions there is a train dataset with the target variable included and a test dataset without the target variable which is used by Kaggle to compute the final accuracy score that determines your leaderboard ranking.
My problem:
I have build a fairly simple ensemble classifier (based on XGboost) and evaluated it via standard train-test-splits of the train data. The accuracy I get from this validation is ~80% which is good but not amazing by public leaderboard standards (excluding the 100% cheaters).
The results and all the KPIs I looked at of this standard model do not indicate severe overfitting, etc. to me.
However when I submit my predictions for the test set my public score is ~35% which is way below even a random chance model. It is sooo bad I even improved my score by simply reversing all predictions from the model.
Why is my model so much worse on the test?
I know that Kaggle computes their scores a bit differently than I do locally, additionally there is probably some differences between the datasets. Most who join the competition notices at least some difference between their local test scores and the public scores.
However my difference is really drastic and indeed reversing the predictions improves my score. This does not make sense to me because reversing the predictions on my local validations leads to garbage predictions, so this is not a simple problem of generally reversed predictions.
So can you help me understand how those two issues happen at the same time:

Drastic difference between local accuracy and public score
Reversing actually leads to the better public score.

Here is my notebook for the code (please ignore the errors, they are simply because the code does not work on kaggle kernels only locally):
https://www.kaggle.com/fnguyen/titanicrising-test
","['machine-learning', 'prediction']","Looking at your code, one set of data transformations were applied to the train data and a different set of transformations were applied to the test data. Different data transformations could account for different evaluation metric performance. It is best practices to put all data transformations in a function so they can be applied to all data in a similar way. Since you are using scikit-learn, sklearn.compose.ColumnTransformer is designed for this purpose. Example code for the Titanic dataset is here."
Is a neural network able to optimize itself for speed?,"
I am experimenting with OpenAI Gym and reinforcement learning. As far as I understood, the environment is waiting for the agent to make a decision, so it's a sequential operation like this:
decision = agent.decide(state)
state, reward, done = environment.act(decision)
agent.train(state, reward)

Doing it in this sequential way, the Markov property is fulfilled: the new state is a result of the old state and the action. However, a lot of games will not wait for the player to make a decision. The game will continue to run and perhaps, the action comes too late.
Has it been observed or is it even possible that a neuronal network adjusts its weights so that the PC computes the result faster and thus makes the ""better"" decision? E.g. one AI beats another because it is faster.
Before posting an answer like ""there are always the same amount of calculations, so it's impossible"", please be aware that there is caching (1st level cache versus RAM), branch prediction and maybe other stuff.
","['neural-networks', 'reinforcement-learning', 'open-ai', 'markov-property']",
Applying Artificial neural network into kaggle's house prices data set gave bad predicted values,"
I am trying to solve the kaggle's house prices using neural network. I've already made it with ensembling several models (XGBoost, GradientBooster and Ridge) and I've got a great score ranking me between the top 25%.
I imagined that by adding a new model to the ensembled models like ANN would increase prediction accuracy, so I did the following:
import keras

model = keras.models.Sequential()

model.add(keras.layers.Dense(235, activation='relu', input_shape=(235,)))
model.add(keras.layers.Dense(235, activation='relu'))
model.add(keras.layers.Dense(235, activation='relu'))
model.add(keras.layers.Dense(235, activation='relu'))
model.add(keras.layers.Dense(235, activation='relu'))
model.add(keras.layers.Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()
model.fit(dataset_ann, y, epochs=100, callbacks=[keras.callbacks.EarlyStopping(patience=3)])
y_pred = model.predict(X_test_ann)

I choosed 235 neurons for each layer, as the training set has 235 features.
For model ensembling:
y_p = (0.1*model.predict(X_test_ann)+0.2*gbr.predict(testset)+0.3*xgb.predict(testset)+0.1*regressor.predict(testset)+0.1*elastic.predict(testset)+0.2*ridge.predict(testset))

The shape of y_p is (1459, 1459) instead of (1459, ) where columns are all having the same values, so taking y_p[0] would be more than enough.
I submitted the result into kaggle and went from top 25% into bottom 60%.
Is it because the number of hidden layers with its input? Or because there is few data to train (1460 rows of train set) and the neural network needs more than that? Or is it because of the number of neurons in each layer?
I tried with epoch = 30, 100, 1000 and got nearly the same bad ranking. 
","['neural-networks', 'python', 'artificial-neuron']",
"Small size datasets for object detection, segmentation and localization [closed]","







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I am looking for a small size dataset on which I can implement object detection, object segmentation and object localization.
Can anyone suggest me a dataset less than 5GB? Or do I need to know anything before implementing these algorithms?
","['datasets', 'object-recognition', 'object-detection', 'resource-request']","There are various dataset available such as Pascal VOC dataset: You can perform all your task with these. 

size of the dataset is as  follows
ADE20K Semantic Segmentation Dataset: you can perform only segmentation here
COCO dataset: This is rich dataset but a size larger then 5 GB so you can try downloading using google colab in your drive and then make a zip file of data as less than 5 GB
You can download all these datasets using Gluoncv easily.here link."
Labeling for multilabel image classification,"
with a friend of mine, we got in an argument over how to label images for multi-label. 
Note:
Groups of a species and the species of catfish is important to recognize.
The labels are:

'I': an individual fish of any type except catfish
'R': A group of same species
'K': Catfish

First conflict:
For an image containing a bank of fish, our conflicting opinions are:

I and R
R

Second conflict:
If in an image there's actually a bank of catfish, our conflicting opinions are:

K and R
K

Third conflict:
If in an image there's actually a group of same species of fish and other individual fish from different species, our conflicting opinions are:

R
I and R 

Summary:

I think one very important difference of opinion here, is if in multi-label, we should give two labels to one object in image(say group and fish) or if an object should have only one label.
Should the overwhelming presence of an object, overshadow the presence of another (group of fish of same species and another individual fish) 

What do you think? 
",['datasets'],
Why do ResNets avoid the vanishing gradient problem?,"
I read that, if we use the sigmoid or hyperbolic tangent activation functions in deep neural networks, we can have some problems with the vanishing of the gradient, and this is visible by the shapes of the derivative of these functions. ReLU solves this problem thanks to its derivative, even if there may be some dead units. ResNet uses ReLU as activation function, but looking online what I understood is that ResNet solves the vanishing of the gradient thanks to its identity map, and I do not totally agree with that. So what's the purpose of the identity connections in ResNet? Are they used for solving the vanishing of the gradient? And ReLU really solves the vanishing of the gradient in deep neural networks?
","['deep-learning', 'activation-functions', 'exploding-gradient-problem', 'residual-networks', 'vanishing-gradient-problem']","Before proceeding, it's important to note that ResNets, as pointed out here, were not introduced to specifically solve the VGP, but to improve learning in general. In fact, the authors of ResNet, in the original paper, noticed that neural networks without residual connections don't learn as well as ResNets, although they are using batch normalization, which, in theory, ensures that gradients should not vanish (section 4.1). So, in this answer, I'm just giving a potential explanation of why ResNets may also mitigate (or prevent to some extent) the VGP, but the cited research papers below also confirm that ResNets prevent the VGP. Given that I didn't fully read all the papers mentioned in this answer, the information in this answer may not be fully accurate.The skip connections allow information to skip layers, so, in the forward pass, information from layer $l$ can directly be fed into layer $l+t$ (i.e. the activations of layer $l$ are added to the activations of layer $l+t$), for $t \geq 2$, and, during the forward pass, the gradients can also flow unchanged from layer $l+t$ to layer $l$.How exactly could this prevent the vanishing gradient problem (VGP)? The VGP occurs when the elements of the gradient (the partial derivatives with respect to the parameters of the NN) become exponentially small, so that the update of the parameters with the gradient becomes almost insignificant (i.e. if you add a very small number $0 < \epsilon \ll 1$ to another number $d$, $d+\epsilon$ is almost the same as $d$) and, consequently, the NN learns very slowly or not at all (considering also numerical errors). Given that these partial derivatives are computed with the chain rule, this can easily occur, because you keep on multiplying small (finite-precision) numbers (please, have a look at how the chain rule works, if you're not familiar with it). For example, $\frac{1}{5}\frac{1}{5} = \frac{1}{25}$ and then $\frac{1}{5}\frac{1}{25} = \frac{1}{125}$, and so on. The deeper the NN, the more likely the VGP can occur. This should be quite intuitive if you are familiar with the chain rule and the back-propagation algorithm (i.e. the chain rule). By allowing information to skip layers, layer $l+t$ receives information from both layer $l+t-1$ and layer $l$ (unchanged, i.e. you do not perform multiplications). For example, to compute the activation of layer $l+t-1$, you perform the usual linear combination followed by the application of the non-linear activation function (e.g. ReLU). In this linear combination, you perform multiplications between numbers that could already be quite small, so the results of these multiplications are even smaller numbers. If you use saturating activation functions (e.g. tanh), this problem can even be aggravated. If the activation of layer $l+t$ are even smaller than the activations of layer $l+t-1$, the addition of the information from layer $l$ will make these activations bigger, thus, to some extent, they will prevent these activations from becoming exponentially small. A similar thing can be said for the back-propagation of the gradient.Therefore, skip connections can mitigate the VGP, and so they can be used to train deeper NNs.These explanations are roughly consistent with the findings reported in the paper Residual Networks Behave Like Ensembles of Relatively Shallow Networks, which statesOur results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.In the paper Norm-Preservation: Why Residual Networks Can Become Extremely Deep?, the authors also discuss another desirable effect of skip connections.We show theoretically and empirically that each residual block in ResNets is increasingly norm-preserving, as the network becomes deeper"
How to tell if two hotel reviews addressing the same thing,"
I am playing with a large dataset of hotel reviews, which contains both positive and negative reviews (the reviews are labeled). I want to use this dataset to perform textual style transfer - given a positive review, output a negative review which address the same thing. For example, if the positive review mentioned how spacious the rooms are, I want the output to be a review that complains about the small and claustrophobic rooms.
However, I don't have positive review-negative review pairs for the training. I was thinking that maybe I could create those pairs myself, but I'm not sure what is the best way to do that. Simple heuristics like jaccard index and such didn't give the desired results. 
","['natural-language-processing', 'text-generation']",
Why does the BatchNormalization layer produce different outputs during training and inference?,"
I modified resnet50 architecture to get a regression network. I just add batchnorm1d and ReLU layers just before the fully connected layer. During the training, the output of batchnorm1d layer is nearly equal to 3 and this gives good results for training. However, during inference, output of batchnorm1d layer is about 30 so this leads to too low accuracy for test results. In other words, outputs of batchnorm1d layers give very different normalized output during training and inference.
What is the reason for this situation, and how can I solve it? I am using PyTorch.
","['convolutional-neural-networks', 'pytorch', 'batch-normalization', 'inference']",
How to handle set-like size agnostic input format,"
Let's set up some hypothetical simplified scenario: Each instance $i$ of my imaginary dataset $D=\{i_{1}, \ldots, i_{MAX}\}$ has different number $k_{i}$ of $n$-dimensional vectors as input into my neural network. Each of them will be transformed with $m \times n$ matrix $M$ (so, matrices with same parameters) and acted point-wise with some non-linearity $\sigma_{1}$.
Now there are 2 possibilities I want to consider separately:

Case: I want to average all those outputs, thus depending of total number of vectors $k_{i}$.
Case: I want to choose 2 subsets (no necessarily mutually disjoint) and average vectors from those subsets.

For the later layers (in both cases) I'd use some the ""standard"" neural networks architectures and loss functions. Notice, I'll probably go with more complex connections than averaging, this is only for the purpose of this technical question.
My questions are:
 1. Is there a ""simple"" way (meaning: high-level library like
    tensorflow) which allows me to create custom layer with shared
    weights which is also size-agnostic (not size of vectors, but number
    of vectors) Is it possible to parallelise computation like this?
I have some knowledge of tf, but I still didn't use all obscure low-level details and don't know all capabilities of $2.0$ version. Also, I'd know to do this stuff in numpy with handcrafted back-propagation implementation. I'd like to do it in more high level way which will handle backpropagation and parallelisation for me.
I'd like to avoid having tensor of vectors which will be padded, but I'm not sure is it possible. My real problem is more complex but has same characteristics: I need to use same weights on differently connected layers, and I would handle normalization of the input to the following layers to the same scale manually (i.e. with ""normal"" averaging or weighted attention-like averaging,...). Not always I'd connect all vectors from first layers to all from second. It needs to be flexible for each instance of data.
So, in short: I want to reuse the same weight matrices but connect them differently for each instance of dataset. I want to do it in framework which already has automatic differentiation/backpropagation and parallelisation. Is it possible?
Does eager execution in tf help in solving problems like this? I guess nothing could be done with prebuilt computation graphs in 1.x versions.
","['neural-networks', 'tensorflow']",
"Transpose convolution in TiF-GAN: How does ""same"" padding works?","
This question should be quite generic but I faced the problem in the case of the TiF-GAN generator so I am going to use it as an example. (Link to paper) 
If you check the penultimate page in the paper you can find the architecture design of the generator. 
The generator has a dense layer and then a reshape layer converting the hidden layer feature map to a dimensionality of 8x4x512 (given that $s = 64$)
Then what follows is a transpose convolution operation with a kernel size of 12x3 with $512$ filters and a stride of $2$ in all dimensions. The output of this layer should be then 16x8x512. 
After fiddling with some coding I found out that the authors also used the setting padding=same in their tensor flow code.
So, my question is: How and what do you pad when you perform such a transpose convolution to get those output dimensions?
Without any padding I would assume that you should get an output of 26x9x1534 assuming that each output dimension is equal to dim = kernel_dim + strides * (input_dim - 1)
","['machine-learning', 'convolutional-neural-networks', 'padding', 'transpose-convolution']",
Did people analyze dynamics of very simple LSTMs?,"
I wonder if researchers tried to understand how LSTMs work by analyzing the dynamics of simple LSTM (e.g. with 2 units)? For example, how the hidden state evolves depending on the properties of weight matrices.
It seems like a very natural thing to try (especially because it is easy to draw hidden states with 2 units on a 2D plane). However, I haven't found any papers that would play with such a toy example.
Are there such papers at all?
Or with such a simple example, it is impossible to get some understanding because of its over-simplicity? (which I doubt because even logistic maps generate very complicated behavior)
","['deep-learning', 'reference-request', 'long-short-term-memory']",
Text detection on English and Chinese language,"
https://arxiv.org/abs/1910.07954
In this paper, we have a convolutional character neural network where we have object detection by taking a character as a basic unit. First, we do character detection and recognition and then we go for text detection.
 
Here (Page number 5 under the subheading Iterative character detection) it is written that a model trained on English and Chinese texts will generalize well in regards to text detection.
But how English and Chinese texts are good for generalization in text detection.
If you have any queries regarding the paper you can ask me in the comment section
Thanks in advance! 
","['convolutional-neural-networks', 'object-detection', 'text-classification']",
Can we train the model to detect real users with only positive labels?,"
We have hundreds of thousands of customers records, and we need to take the benefits of our data to train a model that will recognize fake entries or unrealistic ones for our platform, where customers are asked to enter their names, phone number and zip code.
So, our attributes are name, phone number, zip code and IP address to train the model with. We have only data associated with real users. Can we train a model provided with only positive labels (as we do not have a negative dataset to train the model with)?
","['machine-learning', 'classification', 'features']",
How should I design this LSTM network to perform stock prediction?,"
I'm trying to develop a stock predictor. 
I'm using LSTM but I am unsure about the structure of the Neural Network. For example, I'm assuming that the Neural Network is a many-to-one since we have many inputs (i.e Open, Close etc) and one output (stock price).
By misunderstanding is coming with how to construct the nodes. For example, what input goes into the ""Cell"" (or node)? I.e does say 60 timestep mean 60 days of 'Open Price' are fed into the Neural Network at t and then 60 days of 'Close' into t + 1 until we use all input to produce an output?
If someone could explain the process of how LSTM are used with stock predictions that would be appreciated. 
","['neural-networks', 'deep-learning', 'ai-design', 'long-short-term-memory']",
What is the difference between using a backbone architecture and transfer learning?,"
I'm super new to deep learning and computer vision, so this question may sound dumb.
In this link (https://github.com/GeorgeSeif/Semantic-Segmentation-Suite), there are pre-trained models (e.g., ResNet101) called front-end models. And they are used for feature extraction. I found these models are called backbone models/architectures generally. And the link says some of the main models (e.g. DeepLabV3 or PSPNet) rely on pre-trained ResNet.
Also, transfer learning is to take a model trained on a large dataset and transfer its knowledge to a smaller dataset, right?

Do the main models that rely on pre-trained ResNet do transfer learning basically (like from ResNet to the main model)?      
If I use a pre-trained network, like ResNet101, as the backbone architecture of the main model(like U-Net or SegNet) for image segmentation, is it considered as transfer learning?

","['deep-learning', 'convolutional-neural-networks', 'comparison', 'image-segmentation', 'transfer-learning']",
Why is it possible to eliminate this branch with alpha-beta pruning?,"
Can someone explain to me why it is possible to eliminate the rest of the middle branch in this image for alpha-beta pruning? I am confused because it seems the only information you know is that Helen would pick at least a 2 at the top (considering if we iterate from left to right in DFS), and Stavros would absolutely not pick anything above 7. This leaves 5 possible numbers that the rest of the branch could take on that Helen could potentially end up picking, but can't because we've eliminated those possibilities via pruning.

","['search', 'minimax', 'alpha-beta-pruning']",
Is there a way to use RNN (in tensorflow) to do something like a batch Kalman with the weight dynamics specified in the loss?,"
Or would you simply do this as a time series of models. 
Basically I think you can think of time series of weights as the hidden states and the dynamics driving the weight time series as the RNN weights. I'm not sure if the data gradients are avoiding look ahead in this context though.
I am basically thinking of smoothing (data assimilation) formulation of the filtering problem. Usually smoothing has look-ahead bias but with stop_gradients (and a large graph) it should be possible to do ""batch"" filtering.
","['tensorflow', 'recurrent-neural-networks']",
Can a neural network learn to predict a number given a binarized image of a rectangle?,"
Let's assume that we have a regression problem. Our input is just binarized image that contains a single rectangle and we want to predict just a float number.  Actually, this floating-point number depends on rectangle angle, rectangle size and rectangle location. Is this problem can be solved by a neural network?
I think, it can not be solved by a neural network, because rectangle angle, size and location are latent variables and without learning these latent variable, above problem can not be solved. What do you think?
","['deep-learning', 'convolutional-neural-networks', 'representation-learning']",
Why does reinforcement learning using a non-linear function approximator diverge when using strongly correlated data as input?,"
While reading the DQN paper, I found that randomly selecting and learning samples reduced divergence in RL using a non-linear function approximator (e.g a neural network).
So, why does Reinforcement Learning using a non-linear function approximator diverge when using strongly correlated data as input?
","['reinforcement-learning', 'dqn', 'deep-rl', 'convergence', 'function-approximation']",
Is it feasible to use GAN for high-quality image synthesis other than human faces?,"
The famous Nvidia paper Progressive Growing of GANs for Improved Quality, Stability, and Variation, the GAN can generate hyperrealistic human faces. But, in the very same paper, images of other categories are rather disappointing and there hasn't seemed to be any improvements since then. Why is it the case? Is it because they didn't have enough training data for other categories? Or is it due to some fundamental limitation of GAN?
I have come across a paper talking about the limitations of GAN: Seeing What a GAN Cannot Generate.
Anybody using GAN for image synthesis other than human faces? Any success stories?
","['generative-adversarial-networks', 'generative-model', 'papers', 'image-generation']",
How I can predict the next number in a sequence with a neural network?,"
I've been dabbling with machine learning and neural networks (namely, resnet50) for a few months now, mostly doing image recognition. I am currently trying to make a program that, given a string of numbers as input, can predict the next number in this sequence. For example, the input could be 1, 2, 3, 4 and the output should be 5.
I read something that said this could be done with a multilayer perceptron neural net, but that didn't elaborate much. 
Any ideas, or links to tutorials/code?
","['neural-networks', 'machine-learning', 'prediction', 'multilayer-perceptrons']",
"Why do CNN's sometimes make highly confident mistakes, and how can one combat this problem?","
I trained a simple CNN on the MNIST database of handwritten digits to 99% accuracy. I'm feeding in a bunch of handwritten digits, and non-digits from a document.
I want the CNN to report errors, so I set a threshold of 90% certainty below which my algorithm assumes that what it's looking at is not a digit. 
My problem is that the CNN is 100% certain of many incorrect guesses. In the example below, the CNN reports 100% certainty that it's a 0. How do I make it report failure?

My thoughts on this:
Maybe the CNN is not really 100% certain that this is a zero. Maybe it just thinks that it can't be anything else, and it's being forced to choose (because of normalisation on the output vector). Is there any way I can get insight into what the CNN ""thought"" before I forced it to choose?
PS: I'm using Keras on Tensorflow with Python.
Edit
Because someone asked. Here is the context of my problem:
This came from me applying a heuristic algorithm for segmentation of sequences of connected digits. In the image above, the left part is actually a 4, and the right is the curve bit of a 2 without the base. The algorithm is supposed to step through segment cuts, and when it finds a confident match, remove that cut and continue moving along the sequence. It works really well for some cases, but of course it's totally reliant on being able to tell if what it's looking at is not a good match for a digit. Here's an example of where it kind of did okay.

My next best option is to do inference on all permutations and maximise combined score. That's more expensive.
","['convolutional-neural-networks', 'tensorflow', 'keras', 'bayesian-deep-learning', 'uncertainty-quantification']","The concept you are looking for is called epistemic uncertainty, also known as model uncertainty. You want the model to produce meaningful calibrated probabilities that quantify the real confidence of the model.This is generally not possible with simple neural networks as they simply do not have this property, for this you need a Bayesian Neural Network (BNN). This kind of network learns a distribution of weights instead of scalar or point-wise weights, which then allow to encode model uncertainty, as then the distribution of the output is calibrated and has the properties you want.This problem is also called out of distribution (OOD) detection, and again it can be done with BNNs, but unfortunately training a full BNN is untractable, so we use approximations.As a reference, one of these approximations is Deep Ensembles, which train several instances of a model in the same dataset and then average the softmax probabilities, and has good out of distribution detection properties. Check the paper here, in particular section 3.5 which shows results for OOD based on entropy of the ensemble probabilities."
"Param count in last layer high, how can I decrease?","
Not sure where to put this... I am trying to create a convolutional architecture for a DQN in keras, and I want to know why my param count is so high for my last layer compared to the rest of the network. I've tried slowly decreasing the dimensions of the layers above it, but it performs quite poorly. I want to know if there's anything I can do to decrease the param count of that last layer, besides the above.
Code:
#Import statements.
import random
import numpy as np
import tensorflow as tf
import tensorflow.keras.layers as L
from collections import deque
import layers as mL
import tensorflow.keras.optimizers as O
import optimizers as mO
import tensorflow.keras.backend as K


#Conv function.
def conv(x, units, kernel, stride, noise=False, padding='valid'):
    y = L.Conv2D(units, kernel, stride, activation=mish, padding=padding)(x)
    if noise:
        y = mL.PGaussian()(y)
    return y

#Network
        x_input = L.Input(shape=self.state)
        x_goal = L.Input(shape=self.state)
        x = L.Concatenate(-1)([x_input, x_goal])
        x_list = []
        for i in range(2):
            x = conv(x, 4, (7,7), 1)
        for i in range(2):
            x = conv(x, 8, (5,5), 2)
        for i in range(10):
            x = conv(x, 6, (3,3), 1, noise=True)
        x = L.Conv2D(1, (3,3), 1)(x)
        x_shape = K.int_shape(x)
        x = L.Reshape((x_shape[1], x_shape[2]))(x)
        x = L.Flatten()(x)
        crit = L.Dense(1, trainable=False)(x)
        critic = tf.keras.models.Model([x_input, x_goal], crit)
        act1 = L.Dense(self.action, trainable=False)(x)
        act2 = L.Dense(self.action2, trainable=False)(x)
        act1 = L.Softmax()(act1)
        act2 = L.Softmax()(act2)
        actor = tf.keras.models.Model([x_input, x_goal], [act1, act2])
        actor.compile(loss=mish_loss, optimizer='adam')
        actor.summary()


actor.summary():
________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 300, 300, 3) 0                                            
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 300, 300, 3) 0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 300, 300, 6)  0           input_2[0][0]                    
                                                                 input_3[0][0]                    
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 294, 294, 4)  1180        concatenate[0][0]                
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 288, 288, 4)  788         conv2d_52[0][0]                  
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 142, 142, 8)  808         conv2d_53[0][0]                  
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 69, 69, 8)    1608        conv2d_54[0][0]                  
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 67, 67, 6)    438         conv2d_55[0][0]                  
__________________________________________________________________________________________________
p_gaussian (PGaussian)          (None, 67, 67, 6)    1           conv2d_56[0][0]                  
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 65, 65, 6)    330         p_gaussian[0][0]                 
__________________________________________________________________________________________________
p_gaussian_1 (PGaussian)        (None, 65, 65, 6)    1           conv2d_57[0][0]                  
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 63, 63, 6)    330         p_gaussian_1[0][0]               
__________________________________________________________________________________________________
p_gaussian_2 (PGaussian)        (None, 63, 63, 6)    1           conv2d_58[0][0]                  
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 61, 61, 6)    330         p_gaussian_2[0][0]               
__________________________________________________________________________________________________
p_gaussian_3 (PGaussian)        (None, 61, 61, 6)    1           conv2d_59[0][0]                  
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 59, 59, 6)    330         p_gaussian_3[0][0]               
__________________________________________________________________________________________________
p_gaussian_4 (PGaussian)        (None, 59, 59, 6)    1           conv2d_60[0][0]                  
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 57, 57, 6)    330         p_gaussian_4[0][0]               
__________________________________________________________________________________________________
p_gaussian_5 (PGaussian)        (None, 57, 57, 6)    1           conv2d_61[0][0]                  
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 55, 55, 6)    330         p_gaussian_5[0][0]               
__________________________________________________________________________________________________
p_gaussian_6 (PGaussian)        (None, 55, 55, 6)    1           conv2d_62[0][0]                  
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 53, 53, 6)    330         p_gaussian_6[0][0]               
__________________________________________________________________________________________________
p_gaussian_7 (PGaussian)        (None, 53, 53, 6)    1           conv2d_63[0][0]                  
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 51, 51, 6)    330         p_gaussian_7[0][0]               
__________________________________________________________________________________________________
p_gaussian_8 (PGaussian)        (None, 51, 51, 6)    1           conv2d_64[0][0]                  
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 49, 49, 6)    330         p_gaussian_8[0][0]               
__________________________________________________________________________________________________
p_gaussian_9 (PGaussian)        (None, 49, 49, 6)    1           conv2d_65[0][0]                  
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 47, 47, 1)    55          p_gaussian_9[0][0]               
__________________________________________________________________________________________________
reshape (Reshape)               (None, 47, 47)       0           conv2d_66[0][0]                  
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2209)         0           reshape[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2000)         4420000     flatten[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 200)          442000      flatten[0][0]                    
__________________________________________________________________________________________________
softmax (Softmax)               (None, 2000)         0           dense_1[0][0]                    
__________________________________________________________________________________________________
softmax_1 (Softmax)             (None, 200)          0           dense_2[0][0]                    
==================================================================================================
Total params: 4,869,857
Trainable params: 7,857
Non-trainable params: 4,862,000
__________________________________________________________________________________________________

","['convolutional-neural-networks', 'weights']","If I understood correctly you want to decrease the parameters count on the last layer (dense_2 layer right?). It would be nice to know why you want to decrease the number of parameters in the last layers... But I'll proceed with what I see.Firstly, the dense layers (or fully connected in literature) have a deterministic number of parameters (or weights) to learn according to the size of the input and output tensor. The relation is the following:$N_{params} = Y_{output} \cdot (X_{input} +1) = 200 \cdot (2209 +1) = 442000$Where:So if you want to decrease the number of parameters you can:So, in short: if you are not willing to change your the input or output tensor to the Dense layers, then no, you can not decrease the number of parameters.BONUS: In case you missed it, I have noticed you have set your dense layers to trainable=False. So in principle you should not care about decreasing number of parameters (which in most cases is motivated to reduce training time) since they are already not being trained. You can check that in the Keras summary output:Where the non-trainable parameters are $4862000 = 4420000 + 442000 $, which are the number of parameters of your 2 dense layers."
How many layers exists in my neural network?,"
I have a neural network model defined as below. How many layers exist there? Not sure which ones to count when we are asked about the number.
def create_model():
    channels = 3
    model = Sequential()
    model.add(Conv2D(32, kernel_size = (5, 5), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, channels)))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(BatchNormalization())
    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(BatchNormalization())
    model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(BatchNormalization())
    model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(BatchNormalization())

    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(2, activation = 'softmax'))

    return model

","['deep-learning', 'convolutional-neural-networks', 'python', 'training', 'deep-neural-networks']",
Best AI Approach for 2D to-down space shooter,"
I am building a 2d top-down space game, which involves several objects, such as asteroids, drones, spaceships, space litter and power-ups.
It follows the rules of space gravity, with speed and acceleration.
The idea is that a player controls his own spaceship, can fire bullets, and is allowed to spawn 3 support drones that will attack the enemy. Its goal is to deal the most damage to the opponent spaceship in 3 minutes. 
It involves certain dynamics, such as ""Destroying an asteroid spawns a power-up"", ""touching an asteroid deals damage to your spaceship"" etc.
What would be the best approach to define an AI agent for it?
I was thinking of Reinforcement learning, but maybe the game is too complex and I wouldn't have the computational power for it?
","['ai-design', 'game-ai']",
How can I formulate a fuzzy inference system to approximate the tangent function?,"
Consider the function $f(x)=\tan(2x)$. How can I determine a fuzzy system that approximates $f(x)$? How to choose membership functions and how to determine fuzzy rules? Any help would be appreciated. 
",['fuzzy-logic'],
Predicting probabilities of events using neural networks,"
I've got a few thousands of sequences like
1.23, 2.15. 3.19, 4.30, 5.24, 6.22

where the numbers denote times on which an event happened (there's just a single kind of events). The events are sort of periodical and the period is known to be exactly one, however, the  exact times varies. Sometimes, events are missing and there are other irregularities, but let's ignore them for now.
I'd like to train an neural network for predicting the probability that there'll be a next even in a given time interval. The problem is that I have no probabilities for the training.
All I have are the above sequences. If I had four sequences like
1.23, 2.15. 3.19, 4.30, 5.24, 6.05
1.23, 2.15. 3.19, 4.30, 5.24, 6.83
1.23, 2.15. 3.19, 4.30, 5.24, 6.27
1.23, 2.15. 3.19, 4.30, 5.24, 6.22
1.23, 2.15. 3.19, 4.30, 5.24, 6.17

then I could say that the probability of an event in the interval [6.10, 6.30] is 60% and use this value for learning. However, all my sequences are different. I could try to group them somehow so that I can define something like a probability, but this sounds way more complicated than what I'm trying to achieve.
Instead, I could try to use the sequence
1.23, 2.15. 3.19, 4.30, 5.24, 6.22

to learn that after the prefix 1.23, 2.15. 3.19, 4.30, 5.24, there will be an event in the interval [6.10, 6.30] for sure (value to learn equal to one); if there was 6.05 instead of 6.22, the value to learn would be zero. A learned network would produce the average value (let's say 0.60).
However, the error would never converge to zero, so there'd be no quality criterion and probably a big chance of overtraining leading to non-sense results.
Is there a way to handle this?
","['neural-networks', 'prediction', 'probability']",
What are advantages of using meta-heuristic algorithms on optimization problems?,"
What are the advantages and disadvantages of using meta-heuristic algorithms on optimization problems? Simply, why do we use meta-heuristic algorithms, like PSO, over traditional mathematical techniques, such as linear, non-linear and dynamic programming?
I actually have a good understanding of meta-heuristic algorithms and I know how they work. For example, one advantage of this kind of algorithms is that they can find an optimal solution in a reasonable time.
However, my lack of knowledge about other methods and techniques brought this question to my mind.
","['comparison', 'optimization', 'applications', 'meta-heuristics']",
How to understand the average l2 loss?,"
In the snippet below, the highlighted part is the average norm, but since $1/|p_i|$ is outside the summation, it is very confusing to understand.

is $|p_i|$ l2-norm(as per wolfram) or l1-norm or absolute value as per wiki.
Should the $i$ inside summation be considered for $1/|p_i|$, which is outside the summation?


","['machine-learning', 'math', 'objective-functions', 'notation']","I agree that this notation is unclear. I would interpret it as follows: Given that the expression is supposed to denote the average norm $|p_i|$ is likely the cardinality of the set $\{p_i\}$. In that case the expression would just be the sum over all norms divided by the number of norms, resulting in the average norm. The authors likely use this notation because they didn't want to introduce the number $n$ of $p_{i < n}$. $|\{p_i\}|$ would be clearer, but maybe uglier to typeset. "
What make a CNN suitable for image classification or semantic segmentation? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I've just started with CNN and there is something that I haven't understood yet:
How do you ""ask"" a network: ""classify me these images"" or ""do semantic segmentation""?
I think it must be something on the architecture, or in the loss function, or whatever that makes the network classify its input or do semantic segmentation.
I suppose its output will be different on classification and on semantic segmentation.
Maybe the question could be rewritten to:
What do I have to do to use a CNN for classification or for semantic segmentation?
","['convolutional-neural-networks', 'classification', 'image-recognition', 'image-segmentation']","Disclaimer: This question is very broad, my answer is admittedly partial and is intended to just give an idea of what's out there and how to find out more.How do you ""say"" a network: ""classify me these images"" or ""do semantic segmentation""?You're mixing two very different problems there. Although there are SO many variations of problems people are applying CNNs to, for this example we can focus on the ""classification of something in the image"" subset ad identify 4 key tasks:So, how do we build networks capable of solving one problem or the other?
We build architectures towards one specific problem, exploiting reusable parts where possible. For example, typically classification and object detection are based on a deep ""backbone"" that extracts highly complex features from the image, that finally are used by a classifier layer interprets to make a prediction (for image classification) or a box prediction head to predict where objects lie in the image (very big simplification, look up object detection architectures and how they work for the proper description!). What do I have to do to use a CNN for classification or for semantic segmentation?In principle you can't just take a network built for classification and just ""ask"" it to do semantic segmentation (think of it as trying to use a screwdriver as scissors... it just was not built for that!). You need changes in the architecture, which necessarily imply new training, at the very least for the new parts that were added."
Predicting a day's data,"
I have a dataset containing timestamp and temperature. For each day, I have 1440 values viz., I have data for every minute of that day(60minutes * 24hrs = 1440). 
The Dataset looks like this:

As an initial step, I gathered day1 data to predict day2 data. I have tried AR, ARIMA, SARIMAX models but I didn't find any positive results. I think this is multivariate since the time and the temperature values changes with respect to date. I need guidance to choose the ML model that will suit for my dataset and it should be able to predict next day/ next month
","['prediction', 'data-science', 'regression', 'time-series']",
Simulating successful trajectories in Montezuma's Revenge turns out to be unsuccessful,"
I have written code in OpenAI's gym to simulate a random playing in Montezuma's Revenge where the agent randomly samples actions from the action space and tries to play the game. A success for me is defined as the case when the agent is atleast able to successfully retrieve the key (Gets a reward of 100). And such cases I dump in a pickle file. I got 44 successful cases when I kept it to run for a day or so. Here is the code I use to generate the training set :
import numpy
import gym
import cma
import random
import time
import pickle as pkl

env   = gym.make('MontezumaRevenge-ram-v0')

observation = env.reset()

#print(observation)
#print(env.action_space.sample())

obs_dict    = []
action_dict = []
success_ctr = 0

for i in range(0, 1000000):
    print('Reward for episode',i+1)
    done = False
    rew = 0
    action_list = []
    obs_list    = []
    while not done:
        action = env.action_space.sample()
        observation, reward, done, _ = env.step(action)
        action_list.append(action)
        obs_list.append(observation)

        rew += reward
        env.render()
        time.sleep(0.01)
        if done:
            env.reset()
            if rew > 0:
                success_ctr += 1
                print(action_list)
                action_dict.append(action_list)
                obs_dict.append(obs_list)
                pkl.dump(obs_dict, open(""obslist.pkl"", ""wb""))
                pkl.dump(action_dict, open(""action.pkl"", ""wb""))

    print(rew)
    time.sleep(1)

try:
    print(obs_dict.shape)
except:
    pass

print(""Took key:"", success_ctr)

I loaded the successful cases from my generated pickle file, and simulated the agent's playing using those exact same cases. But, the agent never receives a reward of 100. I dont understand why this is happening. A little search online suggested it could be because of noise in the game. So, I gave a sleep time, before running each episode. Still, doesn't work. Can someone please explain why is this happening? And suggest a way I could go about generating the training set?
","['reinforcement-learning', 'open-ai', 'rewards', 'gym']",
Neural nets not learning mnist dataset,"
I tried training a 2 hidden layer network using the mnist dataset, but I am not getting any results. I have tried tuning the learning rate(tried 0.1 and 0.0001) and the number of epochs(tried 10 and 50). I even changed the size of hidden layer from 10 to 250. First i had initialized the weights between 0 and 1 and was getting the same classification for all test samples but added (-) sign to 50% of them(chose the figure of 50% by myself) and that problem was solved. Now I cant figure out why it is not working.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() 

def to_array(img):
    img = np.asarray(img)
    return img

'''def standardize(gray):
    st_gray = (gray-np.amin(gray))/(np.amax(gray)-np.amin(gray))
    return st_gray
'''
def activ_func(x):
    for i in range(x.shape[0]):
        '''x[i][0]=(1-np.e**(-2*x[i][0]))/(1+np.e**(-2*x[i][0]))'''
        x[i][0] = 1/(1+np.e**(-x[i][0]))
    return x

def deriv_activ_func(x):
    for i in range(x.shape[0]):
        '''x[i][0] = 1-math.pow(x[i][0],2)'''
        x[i][0] = (x[i][0])*(1-x[i][0])
    return x

def cost(out_layer, label, ind):
    cost = (out_layer[ind]-label)**2
    return cost

def update(x, grad, r):
    for i in range(x.shape[0]):
        x[i][0] = x[i][0]+r*grad[i][0]
    return x

path = ""mnist/mnist_train.csv""
gray = pd.read_csv(path)
labels = gray['label']
gray = gray.drop(['label'], axis=1)
gray = to_array(gray)
labels = to_array(labels)
st_gray = np.empty(shape=(gray.shape[1],1))

def rand_sign(w):
    n = np.random.randint(2,size=w.shape[0]*w.shape[1]).reshape(w.shape[0],w.shape[1])
    for i in range(w.shape[0]):
        for j in range(w.shape[1]):
            if(n[i][j]==1):
                w[i][j]=(-1)*w[i][j]
    return w

def initialize():
    in_layer = np.empty(shape=(st_gray.shape[0],1))
    out_layer = np.unique(labels).reshape(-1,1)
    w1 = rand_sign(np.random.rand(250,in_layer.shape[0]))
    b1 = rand_sign(np.random.rand(250,1))
    l1 = np.empty(shape=(250,1))
    w2 = rand_sign(np.random.rand(250,l1.shape[0]))
    b2 = rand_sign(np.random.rand(250,1))
    l2 = np.empty(shape=(250,1))
    w3 = rand_sign(np.random.rand(out_layer.shape[0],l2.shape[0]))
    b3 = rand_sign(np.random.rand(out_layer.shape[0],1))
    l3 = np.empty_like(out_layer)
    return l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,out_layer

def feed_forward(l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,i):
    st_gray = scaler.fit_transform(gray[i][:].reshape(-1,1))
    in_layer = st_gray
    l1 = np.dot(w1,in_layer)+b1
    l1 = activ_func(l1)
    l2 = np.dot(w2,l1)+b2
    l2 = activ_func(l2)
    l3 = np.dot(w3,l2)+b3
    l3 = activ_func(l3)
    return l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer

def one_hot(out_layer,label):
    for j in range(out_layer.shape[0]):
        if(out_layer[j][0]==label):
            out_layer[j][0] = 1
        else:
            out_layer[j][0] = 0
    return out_layer 

def back_prop(l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,out_layer,lr):
    error = out_layer-l3
    grad = np.dot(error*deriv_activ_func(l3),l2.T)
    w3 = update(w3, grad, lr)
    grad = error*deriv_activ_func(l3)
    b3 = update(b3, grad, lr)
    grad = np.dot(w3.T,error*deriv_activ_func(l3))
    error = grad
    grad = error*deriv_activ_func(l2)
    w2 = update(w2, grad, lr)
    grad = error*deriv_activ_func(l2)
    b2 = update(b2, grad, lr)
    grad = np.dot(w2.T,error*deriv_activ_func(l2))
    error = grad
    grad = error*deriv_activ_func(l1)
    w1 = update(w1, grad, lr)
    grad = error*deriv_activ_func(l1)
    b1 = update(b1, grad, lr)
    return l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer

def predict(l3):
    out = np.amax(l3)
    count = 0
    for j in range(l3.shape[0]):
        count=count+1
        if(l3[j]==out):
            break
    return count

def trainer():
    l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,out_layer = initialize()
    for epochs in range(50):
        for i in range(gray.shape[0]):
            out_layer = np.unique(labels).reshape(-1,1)
            l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer = feed_forward(l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,i)
            out_layer = one_hot(out_layer,labels[i])
            l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer = back_prop(l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,out_layer,0.0001)
        print(""End of epoch :"",epochs+1) 
    return l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,out_layer

l1,l2,l3,w1,w2,w3,b1,b2,b3,in_layer,out_layer = trainer()

path = ""mnist/mnist_train.csv""
gray = pd.read_csv(path)
labels = gray['label']
gray = gray.drop(['label'], axis=1)
gray = to_array(gray)
labels = to_array(labels)
st_gray = np.empty(shape=(gray.shape[1],1))

for i in range(10):
    st_gray = scaler.fit_transform(gray[i][:].reshape(-1,1))
    in_layer = st_gray
    l1 = np.dot(w1,in_layer)+b1
    l1 = activ_func(l1)
    l2 = np.dot(w2,l1)+b2
    l2 = activ_func(l2)
    l3 = np.dot(w3,l2)+b3
    l3 = activ_func(l3)
    count = predict(l3)
    print(""Expected: "",labels[i],"" Predicted: "",count)


","['neural-networks', 'classification']",
How powerful is OpenAI's Gym and Universe in board games area? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I'm a big fan of computer board games and would like to make Python chess/go/shogi/mancala programs. Having heard of reinforcement learning, I decided to look at OpenAI Gym.
But first of all, I would like to know, is it possible using OpenAI Gym/Universe to create a chess bot that will be nearly as strong as Stockfish and create a go bot that will play as good as AlphaGo?
Is it worth learning OpenAI?
","['reinforcement-learning', 'open-ai', 'chess', 'gym', 'go']","OpenAI's Gym is a standardised API, useful for reinforcement learning, applied to a range of interesting environments many of which you can then access for free with little effort. It is very simple to use, and IMO worth learning if you want to practice RL using Python to any depth at all. You could use it to ensure you have good understanding of basic algorithms such as Q learning, independently of and before you look at using RL in a board game context.There are limitations for Gym and Universe when dealing with multiple agents. The API is not really designed with that in mind. For instance, there is no simple way to add two agents to an environment, you would have to write a new environment and attach an opposing agent inside of it. This is still possible, and not necessarily a terrible idea (it depends on the training setup you want to investigate).If you want to look into classic two-player games, and write bots like AlphaGo and Stockfish, then I would point out that:Game-playing bots often make extensive use of planning that can interrogate potential future game states. OpenAI's Gym doesn't prevent you doing that, but it doesn't help in any way.Algorithms for AlphaGo are public, with many nice tutorials. It would be quicker to follow one of these and develop your own bot training code in most cases, than to try and adapt an OpenAI solution for single agent play.Probably the biggest time-saver you could find for any game is a rules engine that implements the board, pieces and game rules for you. If Gym already has a game environment for the game you want your bot to play, it might be worth checking the Gym code to see what it is integrating, then try to use the same library yourself, but not the Gym environment directly.Many decent game-playing algorithms don't use RL at all. You can frame most of them as search (finding best moves) plus heuristics (rating moves or positions), and can usually make independent choices for algorithms that perform each sub-task. You can apply RL so that a bot learns game heuristics, then use a more traditional search e.g. negamax in order to make decisions during play. Or you can use any analysis of the game you like in order to generate heuristics. Very simple games usch as tic-tac-toe (noughts and crosses in UK) can just have heuristic of +1 if X has won, -1 if O has won and 0 otherwise, and still be quickly solved with a minimax search for perfect play.DeepMind's AlphaGo uses a variant of MCTS for the search algorithm, which can be considered a RL technique, but the lines are a bit blurry around definitions there - it is safer to say that AlphaGo incorporates MCTS as the chosen search technique for both self-play training and active play against any other opponent."
"What is the difference between the notations $\|x\|_1, \|x\|_2$ and $|x|$?","
What is the difference between the notations $\|x\|_1, \|x\|_2$ and $|x|$? I think $|x|$ is the magnitude of $x$.
","['machine-learning', 'math', 'notation']","$\|x\| = |x|$ denotes the absolute value norm, which is a special case of the $L_1$ norm defined on the 1-D vector spaces formed by real or complex numbers.$\|\textbf{x}\|_1 = \sum_{i=1}^n|x_i|$ denotes the Taxicab / Manhattan norm, relating to how a Taxi would drive along a rectangular grid of roads to reach a point $(x, y)$ from $(0,0)$.$\|\textbf{x}\|_2 = \sqrt{x_1^2 + \dots + x_n^2}$ denotes the Euclidean norm on an N-D Euclidean space, which is a result of the Pythagorean theorem (the shortest distance between two points)."
How to exploit translational symmetry for extrapolation in video generation using machine learning,"
I'll try to rephrase my problem in the context of video processing. Imagine that initial frame of video has some translational symmetry. The frame evolves according to an update rule. 
I generate a time series for how an edge, say right up edge, of the frame evolves. I generate another time series for how a larger edge, including the smaller right up edge, evolves. Since there is translational symmetry, I should be able to find how the smaller edge is related to the larger edge. The final goal is to use the obtained correlation to extrapolate to larger edges. I want to find the correlation between these two multivariate time series using machine learning (ML) methods. 
I want to know
1 - which one of ML methods can be used in general for this task?
2 - if I use neural networks, the input and output shapes would be (values at time steps, number of variables). For the input it makes sense, but how can I define the output layer (for example, for LSTM in tensorflow)?
","['machine-learning', 'recurrent-neural-networks', 'time-series', 'forecasting']",
How can supervised learning be viewed as a conditional probability of the labels given the inputs?,"
In the literature and textbooks, one often sees supervised learning expressed as a conditional probability, e.g.,
$$\rho(\vec{y}|\vec{x},\vec{\theta})$$
where $\vec{\theta}$ denotes a learned set of network parameters, $\vec{x}$ is an arbitrary input, and $\vec{y}$ is an arbitrary output. If we assume we have already learned $\vec{\theta}$, then, in words, $\rho(\vec{y}|\vec{x},\vec{\theta})$ is the probability that the network will output an arbitrary $\vec{y}$ given an arbitrary input $\vec{x}$.
I am having a hard time reconciling how, after learning $\vec{\theta}$, there is still a probabilistic aspect to it. Post training, a network is, in general, a deterministic function, not a probability. For any specific input $\vec{x}$, a trained network will always produce the same output.
Any insight would be appreciated.
","['machine-learning', 'probability', 'supervised-learning', 'statistical-ai', 'bayesian-deep-learning']","This formulation/interpretation can indeed be confusing (or even misleading), as the output of a neural network is usually deterministic (i.e. given the same input $x$, the output is always the same, so there is no sampling), and there isn't really a probability distribution that models any uncertainty associated with the parameters of the network or the input.People often use this notation to indicate that, in the case of classification, there is a categorical distribution over the labels given the inputs, but this can be misleading, as the softmax (the function often used to model this categorical distribution) only squashes its inputs and doesn't really model any uncertainty associated with the input or the parameter of the neural network, although the elements of the resulting vector add up to 1. In other words, in traditional deep learning, only a point estimate for each parameter of the network is learned and no uncertainty is properly modeled.Nevertheless, certain supervised learning problems have a formal probabilistic interpretation. For example, the minimization of the mean squared error function is equivalent to the maximization of a log probability, assuming your probability distribution is a Gaussian with a mean equal to the output of your model. In this probabilistic interpretation, you typically attempt to learn a probability (e.g. of the labels in the training dataset) and not a probability distribution. Watch Lecture 9.5 â€” The Bayesian interpretation of weight decay (Neural Networks for Machine Learning) by G. Hinton or read the paper Bayesian Learning via Stochastic Dynamics or Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo Method by R. Neal for more details.Moreover, there are Bayesian neural networks (BNNs), which actually maintain a probability distribution over each parameter of the neural network that models the uncertainty associated with the value of this parameter. During the forward pass of this BNN, the specific parameters are actually sampled from the corresponding probability distributions. The actual learnable parameters of a BNN are the parameters of these distributions. For example, if you decide to have a Gaussian distribution over each parameter of the neural network, then you will learn the mean and variance of these Gaussians."
Is the temperature equal to epsilon in Reinforcement Learning?,"
This is a piece of code from my homework. 
# action policy: implements epsilon greedy and softmax
def select_action(self, state, epsilon):
    qval = self.qtable[state]
    prob = []
    if (self.softmax):
        # use Softmax distribution
        prob = sp.softmax(qval / epsilon)
        #print(prob)
    else:
        # assign equal value to all actions
        prob = np.ones(self.actions) * epsilon / (self.actions -1)
        # the best action is taken with probability 1 - epsilon
        prob[np.argmax(qval)] = 1 - epsilon
    return np.random.choice(range(0, self.actions), p = prob)

This is a method in order to select the best action according to the two polices i think. My question is, why in the softmax computation there is the epsilon parameter used as temperature. Is really the same thing? Are they different? I think they should be two different variables. Should the temperature be a fixed value over time? Because when i use the epsilon-greedy policy my epsilon decrease over time.
",['reinforcement-learning'],"Your are correct that epsilon in epsilon-greedy and temperature parameter in the ""softmax distribution"" are different parameters, although they serve a similar purpose. The original author of the code has taken a small liberty with variable names in the select_action method in order to use just one simple name as a positional argument.Should the temperature be a fixed value over time? Not necessarily, if your goal is to converge on an optimal policy you will want to decrease temperature. A slow decay factor applied after each update or episode, as you might use for epsilon (e.g. 0.999 or other value close to 1), can also work for temperature decay. A very high temperature is roughly equivalent to epsilon of 1. As temperature becomes lower, differences in action value estimates become major differences in action selection probabilities, with a sometimes desirable effect of picking ""promising"" action choices more often, plus with very low probabilities of picking the actions with the worst estimates. Using a more sophisticated action selection such as the temperature based on in the example code can speed learning in RL. However, this particular approach is only good in some cases - it is a bit fiddly to tune, and can simply not work at all. The tricky part of using a temperature parameter is choosing good starting point, as well as the decay rate and end values (you have to do the last for epsilon-decay as well). The problem is that the impact of using this distribution depends on the actual differences between action choices. You need a temperature value on roughly the same scale as the Q value differences. This is difficult to figure out in advance. In addition, if the Q value differences are more pronounced in some states than in others, you risk either having next to no exploration or having too much in some parts of the problem."
TensorFlow fit() and GradientTape - number of epochs are different,"
if I define the architecture of a neural network using only dense fully connected layers and train them such that there are two models which are trained using model.fit() and GradientTape. Both the methods of training use the same model architecture.
The randomly initialized weights are shared between the two models and all other parameters such as optimizer, loss function and metrics are also the same.
Dimensions of training and testing sets are:
X_train = (960, 4), y_train = (960,), X_test = (412, 4) & y_test = (412,)
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow_model_optimization as tfmot
from tensorflow_model_optimization.sparsity import keras as sparsity


def create_nn():
    """"""
    Function to create a
    Neural Network
    """"""
    model = Sequential()                                                    

    model.add(
        Dense(
            units = 4, activation = 'relu',
            kernel_initializer = tf.keras.initializers.GlorotNormal(),
            input_shape = (4,)
        )
    )

    model.add(
        Dense(
            units = 3, activation = 'relu',
            kernel_initializer = tf.keras.initializers.GlorotNormal()
        )
    )

    model.add(
        Dense(
            units = 1, activation = 'sigmoid'
        )
    )

    """"""
    # Compile the defined NN model above-
    model.compile(
        loss = 'binary_crossentropy',  # loss = 'categorical_crossentropy'
        optimizer = tf.keras.optimizers.Adam(lr = 0.001),
        metrics=['accuracy']
    )
    """"""

    return model


# Instantiate a model- model = create_nn()

# Save weights for fair comparison- model.save_weights(""Random_Weights.h5"", overwrite=True)


# Create datasets to be used for GradientTape-
# Use tf.data to batch and shuffle the dataset train_ds = tf.data.Dataset.from_tensor_slices(
    (X_train, y_train)).shuffle(100).batch(32)

test_ds = tf.data.Dataset.from_tensor_slices(
    (X_test, y_test)).shuffle(100).batch(32)

# Define early stopping- callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', patience=3,
    min_delta = 0.001, mode = 'min' )

# Train defined model- history_orig = model.fit(
    x = X_train, y = y_train,
    batch_size = 32, epochs = 500,
    validation_data = (X_test, y_test),
    callbacks = [callback],
    verbose = 1 )


# Instantiate a model- model_gt = create_nn()

# Restore random weights as used by the previous model for fair comparison- model_gt.load_weights(""Random_Weights.h5"")


# Choose an optimizer and loss function for training- loss_fn = tf.keras.losses.BinaryCrossentropy() optimizer = tf.keras.optimizers.Adam(lr = 0.001)

# Select metrics to measure the error & accuracy of model.
# These metrics accumulate the values over epochs and then
# print the overall result- train_loss = tf.keras.metrics.Mean(name = 'train_loss') train_accuracy = tf.keras.metrics.BinaryAccuracy(name = 'train_accuracy')

test_loss = tf.keras.metrics.Mean(name = 'test_loss') test_accuracy = tf.keras.metrics.BinaryAccuracy(name = 'train_accuracy')


# Use tf.GradientTape to train the model-

@tf.function def train_step(data, labels):
    """"""
    Function to perform one step of Gradient
    Descent optimization
    """"""

    with tf.GradientTape() as tape:
        predictions = model_gt(data)
        loss = loss_fn(labels, predictions)

    gradients = tape.gradient(loss, model_gt.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model_gt.trainable_variables))

    train_loss(loss)
    train_accuracy(labels, predictions)


@tf.function def test_step(data, labels):
    """"""
    Function to test model performance
    on testing dataset
    """"""

    predictions = model_gt(data)
    t_loss = loss_fn(labels, predictions)

    test_loss(t_loss)
    test_accuracy(labels, predictions)


EPOCHS = 100

# User input- minimum_delta = 0.001 patience = 3

patience_val = np.zeros(patience)


# Dictionary to hold scalar metrics- history = {}

history['accuracy'] = np.zeros(EPOCHS) history['val_accuracy'] = np.zeros(EPOCHS) history['loss'] = np.zeros(EPOCHS) history['val_loss'] = np.zeros(EPOCHS)

for epoch in range(EPOCHS):
    # Reset the metrics at the start of the next epoch
    train_loss.reset_states()
    train_accuracy.reset_states()
    test_loss.reset_states()
    test_accuracy.reset_states()

    for x, y in train_ds:
        train_step(x, y)

    for x_t, y_t in test_ds:
        test_step(x_t, y_t)

    template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'

    history['accuracy'][epoch] = train_accuracy.result()
    history['loss'][epoch] = train_loss.result()
    history['val_loss'][epoch] = test_loss.result()
    history['val_accuracy'][epoch] = test_accuracy.result()

    print(template.format(epoch + 1, 
                          train_loss.result(), train_accuracy.result()*100,
                          test_loss.result(), test_accuracy.result()*100))

    if epoch > 2:
        # Computes absolute differences between 3 consecutive loss values-
        differences = np.abs(np.diff(history['val_loss'][epoch - 3:epoch], n = 1))

        # Checks whether the absolute differences is greater than 'minimum_delta'-
        check =  differences > minimum_delta

        # print('differences: {0}'.format(differences))

        # Count unique element with it's counts-
        # elem, count = np.unique(check, return_counts=True)
        # print('\nelem = {0}, count = {1}'.format(elem, count))

        if np.all(check == False):
        # if elem.all() == False and count == 2:
            print(""\n\nEarlyStopping Evoked! Stopping training\n\n"")
            break

In ""model.fit()"" method, it takes around 82 epochs, while GradientTape method takes 52 epochs.
Why is there this discrepancy in the number of epochs?
Thanks!
","['neural-networks', 'tensorflow']",
How to process data in a data stream for a LSTM,"
How can a data stream for a RNN (LSTM) be handled, when the stream contains data sets belonging to different prediction classes?
Training phase:
I have trained a LSTM to predict a class out of a sequence of Letters. For the training phase I used a fixed data array where the beginning an the ending of a sequence belonged to a class. Of course there is a little noise but the whole data set was labled with a class. E.g:
Seq.    is  Class
ABC     is  One
CBA     is  Two
ABD     is  Three

The network predicts well when it sees a static data array.
Problem in Prediction Phase:
During prediction the LSTM will receive a data stream where there is a sequence off arrays but there is no delimiter. The data set can not be distinguished or separated. I am not sure how it would perform when I have a data stream for different classes like 
    ABCABCCBAABD. 
I guess in speech recognition one must face similiar problems.
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory']",
How can we combine different deep learning models?,"
I know that ensembles can be made by combining sklearn models with a VotingClassifier, but is it possible to combine different deep learning models? Will I have to make something similar to Voting Classifiers?
","['deep-learning', 'convolutional-neural-networks', 'ensemble-learning']",
"Can I solve this assignment problem with RL or AI planning, and if yes how?","
I have a list of positive nonzero integers ",,
"What is the difference between the concepts ""known environment"" and ""deterministic environment""?","
According to the book ""Artificial Intelligence: A Modern Approach"", ""In a known environment, the outcomes (or outcome probabilities if the environment is stochastic) for all actions are given."", and in a deterministic environment, ""the next state of the environment is completely determined by the current state and the action executed by the agent..."". 
What's the difference between the two terms? Don't they mean the same thing?
","['reinforcement-learning', 'comparison', 'terminology', 'norvig-russell']",
Autoencoder network for feature selection not converging,"
I am training an undercomplete autoencoder network for feature selection. I am using one hidden layer in the encoder and decoder networks each. The ELU activation function is used for each layer. For optimization, I am using the ADAM optimizer. Moreover, to improve convergence, I have also introduced learning rate decay. The model shows good convergence initially, but later starts to generate losses (12 digits) in the same range of values for several epochs, and is not converging. How to solve this issue?
","['autoencoders', 'learning-rate']","The trick was to normalize the input dataset values with the respective mean and standard deviation in each column. This reduced the loss drastically, and my network is training more efficiently now. Moreover, normalizing the data also helps you calculate the weights associated with each input node more easily, especially when trying to find out variable importance."
What is generally the best way to combine tabular image metadata with image data in a convolutional neural network?,"
I have 26 features from tabular data (clinical variables from patients like age gender etc) that I want to add to my cnn which is using xray images from patients. I am using the inception network. Right now I am just concatenating these features to the final fully connected layer just before the softmax activation. 
My concern though is that since this final layer also contains 2048 image features that these 26 features will not contribute much in the classification. 
Empirically, these 26 features should contribute more than the 2048 image features. A random forest trained on only 26 features did better than the cnn using only the images. I'd like to get a model that does better than either of them separately so I thought I should add these metadata features to the cnn.
Are my concerns warranted? What is the best approach?
",['convolutional-neural-networks'],
What's the best solution to find distance of an object to camera?,"
I have an object with known size and I want to know that's the distance from the camera and camera angle. Is there any way to do this? I have a single source (camera).
","['computer-vision', 'image-processing']",
"What is ""Computational Linguistics""?","
It's not clear to me whether or not someone whose work aims to improve an NLP system may be called a ""Computational Linguist"" even when she/he doesn't modify the algorithm directly by coding.
Let's consider the following activities:
Annotation for Machine Learning: 
analysis of Morphology, Syntax, POS tagging
Annotation, analysis, and annotation of entities (NER) and collocations; supporting content categorization; chunking; word sense disambiguation.
Recording of technical issues of the annotation tool to improve its reliability.
Recording of linguistic and logical particular rules adopted by the research team who develops the NLP algorithm to improve consistency between annotation and criteria previously adopted to train the NLP.
May be these activities considered ""Computational Linguistics""? If not, which is their professional category and how should they be included in the resume in a word which synthesizes them?
","['machine-learning', 'natural-language-processing', 'terminology']","(Disclosure: I am a researcher and lecturer in Computational Linguistics)It is true that annotation and debugging work with existing tools without modification can be considered Computational Linguistics.And yet, most Computational Linguists program on a daily basis, since they actively develop tools. Just to give you some context, at major Computational Linguistics conferences such as ACL or EMNLP (the biggest ones), most authors did the coding themselves.To say that coding is an unimportant side aspect of being a Computational Linguist, as claimed in another answer, is a slight misrepresentation."
What are examples of approaches to dimensionality reduction of feature vectors?,"
Given a pre-trained CNN model, I extract feature vector of images in reference and query dataset with several thousands of elements.
I would like to apply some augmentation techniques to reduce the feature vector dimension to speed up cosine similarity/euclidean distance matrix calculation.
I have already come up with the following two methods in my literature review:

Principal Component Analysis (PCA) + Whitening
Locality Search Hashing (LSH)

Are there more approaches to perform dimensionality reduction of feature vectors? If so, what are the pros/cons of each perhaps?
","['convolutional-neural-networks', 'representation-learning', 'principal-component-analysis', 'dimensionality-reduction']","Dimensionality reduction could be achieved by using an Autoencoder Network, which learns a representation (or Encoding) for the input data. While training, the reduction side (Encoder) reduces the data to a lower-dimension and a reconstructing side (Decoder) tries to reconstruct the original input from the intermediate reduced encoding. You could assign the encoder layer output ($L_i$) to a desired dimension (lower than that of the input). Once trained, $L_i$ could be used as a alternative representation of your input data in a lower feature-space, and can be used for further computations."
"What does ""episodic training"" mean?","
I'm reading the book Hands-On Meta Learning with Python, and in Prototypical networks said:

So, we use episodic trainingâ€”for each episode, we randomly sample a
few data points from each class in our dataset and we call that a
support set and train the network using only the support set, instead
of the whole dataset.

I think, but I'm not sure, I have understood what ""episodic training"" is, but what is the meaning of ""episodic"" or ""episode"" here?
I'm sorry, I'm not English and I can't guess what it is meaning searching in a dictionary. I know what an episode is, but I don't know what an episode, in this context of training, means.
","['training', 'terminology', 'meta-learning']",
How to track performance of your model during experimenting? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






During weeks and months of your work, many things may change, for example :

You may modify the loss function
Your training or validation datasets may change
You modify data augmentation

Which tools or processes do you use to track modifications you have made and how did they affected the model ?
","['neural-networks', 'machine-learning', 'ai-design']","Maybe you are looking for a combination of a version control system (like git and Github) and a tool like comet.ml. In the past, I used comet.ml to keep track of different experiments performed with different hyper-parameters or different versions of the code. There are other alternatives to comet.ml, such as sacred, but they may also have different features and may not be as visually pleasing as comet.ml or even free. Personally, I liked comet.ml (even though, at the time, it still lacked some features). In any case, a VCS, like git, is widely used in software development (not just in AI projects) to keep track of different versions of the code, etc. You may also be interested in continuous integration (e.g. Travis CI) and code review (e.g. codacy) tools."
How does the update rule for the one-step actor-critic method work?,"
Can you please elucidate the math behind the update rule for the critic? I've seen in other places that just a squared distance of  $R + \hat{v}(S', w) - \hat{v}(S,w)$ is used, but Sutton suggests an update rule (and the math behind) that is beyond my understanding?

Also, why do we need $I$?
","['reinforcement-learning', 'math', 'actor-critic-methods']",
State of the art in voice recognition,"
In the media there's lot of talk about face recognition, mainly with respect to identifying faces (= assigning to persons). Less attention is paid to the recognition of facially expressed emotions but there's a lot of research done into this direction, too. Even less attention is paid to the recognition of facially expressed emotions of a single person (which could be much more detailed) - even though this would be a very interesting topic.
What holds for faces does similarily hold for voices. With the help of artifical intelligence voices can be identified (= assigned to persons) and emotions as expressed by voice can be recognized - on a general and on an individual's level.
My general question goes into another direction: As huge progress has been made in visual scene analysis (""what is seen in this scene?"") there has probably been some progress made in auditory scene analysis: ""What is heard in this scene?""
My specific question is: Are there test cases and results where some AI software was given some auditory data with a lot of ""voices"" and could tell how many voices there were?
As a rather easy specific test case consider some Gregorian chant sung in perfect unison. (See also here.)
","['computer-vision', 'emotional-intelligence', 'facial-recognition', 'voice-recognition', 'audio-processing']",
Q-learning problem wrong policy,"
I am coding out a simple 4x4 grid game whereby the agent starts at a particular state and his aim is to reach the terminal state. The agent is supposed to avoid traps along the way and reach the end goal with high reward. The below picture illustrates the environment. 

The code that I am running is shown below:
# 4x4 Grid
import random


gamma = 1
grid = [[-0.1 for i in range(4)] for j in range(4)]
episodes = 500000
epsilon = 1 # start greedy
decay = 0.999
min_epsilon = 0.1
alpha = 0.65
# set terminal states
grid[1][0] = -1
grid[2][2] = -1
grid[0][3] = 1

# Set up Q tables
# 0: up, 1: down, 2: left, 3: right
# Q = {(0,0): {0: z, 1: x, 2: c, 3: v}, ... }}
Q = {}

# 4 rows
for row in range(4):
    # 4 columns
    for column in range(4):
        Q[(row,column)] = {}
        # 4 actions
        for k in range(4):
            Q[(row,column)][k] = 0


def isTerminal(state):
    if state == (1,0) or state == (2,2) or state == (0,3):
        return True
    return False

def get_next_state_reward(state, action):

    row = state[0]
    col = state[1]
    #print(row, col)
    if action == 0: # up
        # out of grid
        if (row - 1) < 0:
            return (state, grid[row][col])

    if action == 1: # down

        # out of grid
        if (row + 1) > len(grid) - 1:
            return (state, grid[row][col])

    if action == 2: # left

        if (col - 1 < 0):
            return (state, grid[row][col])

    if action == 3: # right

        if (col + 1 > len(grid[row]) - 1):
            return (state, grid[row][col])

    if action == 0:

        row -= 1
        return ((row,col), grid[row][col])

    if action == 1:

        row += 1
        return ((row,col), grid[row][col])

    if action == 2:

        col -= 1
        return ((row,col), grid[row][col])

    if action == 3:

        col += 1
        return ((row,col), grid[row][col])

state_visit = {}
for row in range(4):
    # 4 columns
    for column in range(4):
        state_visit[(row,column)] = 0

for episode in range(episodes):

    # let agent start at start state
    state = (3,0)

    while not isTerminal(state):

        r = random.uniform(0,1)

        if r < epsilon:
            action = random.randint(0,3)
        else:
            action = max(Q[state], key=lambda key: Q[state][key])


        next_state, reward = get_next_state_reward(state, action)

        TD_error = reward + gamma * max(Q[next_state]) - Q[state][action]

        Q[state][action] = Q[state][action] + alpha * TD_error

        state = next_state

        state_visit[next_state] += 1
        epsilon = max(min_epsilon, epsilon*decay)

        #input()


policy = {}
# get optimal policies for each state
for states in Q:
    policy[states] = max(Q[states], key=lambda key: Q[states][key])

When I finish running the algorithm however, I am unable to achieve the optimal policy no matter how many tweaks I do to the number of episodes, or epsilon decay, or the alpha value. 
Particularly, the Q values that I attain for state (2,0), (0,1) and (0,0) have Q values that are equal values for three directions except for the last direction which brings the agent to the terminal state. 
For example, these are the Q-values that I get for state (0,0), (0,1) and (2,0) respectively.
(0,0): {0: 2.0, 1: 2.9, 2: 2.9, 3: 2.9}
(0,1): {0: 2.9, 1: 2.0, 2: 2.9, 3: 2.9}
(2,0): {0: 2.9, 1: 2.9, 2: 2.9, 3: 2.9}
I am not sure why the Q-values for the 3 directions should be the same because each extra step that the agent takes incurs a negative reward. 
Would anyone be able to help ? Thank you so much !
","['reinforcement-learning', 'q-learning']","You have a simple mistake in your TD Error function:You have made Q[next_state] a Python dict, so this will take the maximum key which is 3 for all your Q table entries. This is why you end up with values very close to 3 at the end, which is impossible for your problem, the maximum return will be 1.0 when stepping from adjacent grid points to the positive terminal state.The correct code is:Alternatively, since your actions are all numeric, you could use a list instead of a dict in your Q table."
What is the expected value of an IOU in this case?,"
I have a detection problem. An object with a probability of 0.5 is in a box with coordinates ((0,0), (2, 2)) and with a probability of 0.5 a box with coordinates ((2,0), (4,2)). 
What is the maximum expected value of an intersection over union (IOU) with an object that a constant detection algorithm that produces one box can reach? I can't understand how I should find the expected value here. P.S. IOU is 0 cos intersection is empty.
","['neural-networks', 'machine-learning', 'object-detection']",
OCR - Text recognition from Image,"
I plan to develop OCR application using tensorflow to get the value from the image. Text in the image may handwritting or text printed.

From the image, my ocr appplication will able to get the value of below:
1. ChequeDate
2. Payee Name
3. Legal Amount
4. Courtesy Amount
How the OCR application can get the value i want as highlighted as red color? is it need to crop it as small size of the picture for the OCR?
","['machine-learning', 'optical-character-recognition']",
Do we need an explicit policy to sample $A'$ in order to compute the target in SARSA or Q-learning?,"
I would much appreciate if you could point me in the right direction regarding this question about targets for SARSA and Q-learning (notation: $S$ is the current state, $A$ is the current action, $R$ is the reward, $S'$ is the next state and $A'$ is the action chosen from that next state).
Do we need an explicit policy for the Q-learning target to sample $A'$ from? And for SARSA?
I guess this is true for Q-learning since we need to get max Q-value which determines which action $A'$ we'll use for the update. For SARSA, we update the $Q(S, A)$ depending on which action was actually taken (no need for max). Please correct me if I'm wrong.
","['reinforcement-learning', 'comparison', 'q-learning', 'sarsa']","Q-learning uses an exploratory policy, derived from the current estimate of the $Q$ function, such as the $\epsilon$-greedy policy, to select the action $a$ from the current state $s$. After having taken this action $a$ from $s$, the reward $r$ and the next state $s'$ are observed. At this point, to update the estimate of the $Q$ function, you use a target that assumes that the greedy action is taken from the next state $s'$. The greedy action is selected by the $\operatorname{max}$ operator, which can thus be thought of as an implicit policy (but this terminology isn't common, AFAIK), so, in this context, the greedy action is the action associated with the highest $Q$ value for the state $s'$.In SARSA, no $\operatorname{max}$ operator is used, and you derive a policy (e.g. the $\epsilon$-greedy policy) from the current estimate of the $Q$ function to select both $a$ (from $s$) and $a'$ (from $s'$).To conclude, in all cases, the policies are implicit, in the sense that they are derived from the estimate of the $Q$ function, but this isn't a common terminology. See also this answer, where I describe more in detail the differences between Q-learning and SARSA, and I also show the pseudocode of both algorithms, which you should read (multiple times) in order to fully understand their differences."
Do L2 regularization and input normalization depend on sigmoid activation functions?,"
Following the online courses with Andrew Ng, he talks about L2 regularization (a.k.a. weight decay) and input normalization.  Now, the argument is that L2 regularization make the weights smaller, which makes the sigmoid activation functions (and thus the whole network) ""more"" linear.  
Question 1: can this rather handwavey explanation be formalized?  Can we define ""more linear"" in a mathematical sense, and demonstrate that smaller weights in fact achieve this?
Question 2: in contrast to sigmoid, ReLU activations have a single point where it is nonlinear - i.e. the breaking point at x=0.  No scaling of the input changes the shape (i.e. derivative) of this function, the only effect is reducing the magnitude of positive outputs.  Does the argument still hold?  Why?
Input normalization is given as a good practice, but it seems to me that the network should just compensate for varying magnitude between components of the input by scaling the weights appropriately.  The only exception I can think of is again under L2 regularization, which would penalize large weights (assoiciated with small inputs).  
Question 3: Is this correct, and is input scaling thus mostly important with L2 normalization, or is there some reason why the network would fail to adjust the weights without scaling?
","['regularization', 'sigmoid']",
Finding unique faces in a video,"
I am trying to find unique (distinct) faces in multiple videos files. What is the best way to do that?
","['object-detection', 'facial-recognition', 'clustering']",
Getting started with creating a general AI based on textual and then image based data?,"
I have a pool of knowledge that I want to mine for information and allow an AI to deduce likely conclusions from this information.
My goal is to give the AI a set of textual data that is rated on a scale of 0 to 100 ranging from false (0) to unequivocally true (100).  Based on ongoing learning I want to be able to ask it about it's data and to make relational conclusions as to not simply whether things are true or false, but to extrapolate likelihoods, conclusions and so forth... or to simply tell me it can't understand something which would then trigger me to give it more information and to train it with additional material - even if it's my own limited answers.
Ultimately I'll deal with image data as well, but that's a bit down the road.
I'm new to the area of neural nets and deep learning and so I'm hoping someone could point me in the right direction in the way of terminology to search for / research as well as where perhaps I should start.
I wouldn't mind working in C predominantly if possible, but other languages (especially Ruby) is fine.
The field is moving so fast and there's so much research now that seems to trump information available from just a couple years ago now and so I'm hoping to jump into information that takes advantage of more general learning algorithms so that this can be as robust possible while taking care of current trends.
Where do I go from here?
",['deep-learning'],
"In deep learning, is it possible to use discontinuous activation functions?","
In deep learning, is it possible to use discontinuous activation functions (e.g. one with jump discontinuity)?
(My guess: for example, ReLU is non-differentiable at a single point, but it still has a well-defined derivative. If an activation function has a jump discontinuity, then its derivative is supposed to have a delta function at that point. However, the backpropagation process is incapable of considering that delta function, so the convex optimization process will have some problem?)
","['deep-learning', 'backpropagation', 'optimization', 'activation-functions', 'relu']",
How is the DQN able to generalise the learning to unseen states with such a loss function?,"
I am trying to understand how deep Q learning (DQN) works. To my current understanding, each $Q(s, a)$ functions is estimated to be a function of a feature vector of its state $\phi$(s) and the weight of the network $\theta$.
The loss function to minimise is $||\delta_{t+1}||^2$ where $\delta_{t+1}$ is shown below. The loss function is from the website talking about function approximation. Even though it is not explicitly deep Q learning, the loss function to minimise is similar.
$$\delta_{\mathrm{t}+1}=\mathrm{R}_{\mathrm{t}+1}+\max _{\mathrm{a}\in\mathrm{A}} \boldsymbol{\theta}^{\top} \Phi\left(\mathrm{s}_{t+1}, \mathrm{a}\right)-\boldsymbol{\theta}^{\top} \Phi\left(\mathrm{s}_{\mathrm{t}}, \mathrm{a}\right)$$
Source: https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566.
Intuitively, I am not able to understand why the loss function is defined as such. Once the network converges to a $\theta$ using gradient descent, does that mean that the $Q_{max}(s,a)$ is found?
In essence, I am not able to grasp intuitively how the neural network is able to generalise the learning to unseen states.
The algorithm I am looking at to help me understand the deep Q networks is below.

Source: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf
","['reinforcement-learning', 'deep-rl', 'dqn', 'objective-functions', 'generalization']",
What class of problem is this?,"
If I have a lot of input output pairs as training data
<float Xi, float Yi>
and I have a parametrized approximation function (I know the function algorithm, but not the values of the many many parameters it contains) which shall approximate the process by which the original data pairs were generated. The function takes two input values:
// c is a precomputed classifier for x and can have values from 0 to 255, so there can be up to 256 different classes
y = f(float x, int c)

the hidden parameters of the function are some big lookup tables (a lot of free parameters, but still much fewer than the amount of data points in the training data)
Now I want to fit all the hidden parameters that f contains AND compute for each Xi a ci, such that for the fitted function the error over all i of Yi - f(Xi, ci) is minimized
So, using some algorithm I want to fit the parameters of f and also classify the inputs Xi so that f(Xi, ci) aproximates Yi
How is this kind of problem called and what kind of algorithm is used to solve it?
I assume it's possible to initialize all hidden parameters as well as all ci with random values and then somehow use back propagation of the error to iteratively find  parameters and ci such that the function works well.
What I don't know is whether this is a well known class of problem and I just don't know the name of it, so I'm asking for pointers.
Or maybe in other words:
I have a function that has a certain layout (for performance reasons) which I want to use to approximate and interpolate my training data, I want to tune the parameters of this function such that it approximates the original data well. since the data points fall into some 'categories', I want to pre-classify the data-points to make it easier for the function to do its job. What kind of algorithm do I use to find the function's parameters and to pre-classify the input?
","['classification', 'autoencoders']",
What is the intuition behind TD($\lambda$)?,"
I'd like to better understand temporal-difference learning. In particular, I'm wondering if it is prudent to think about TD($\lambda$) as a type of ""truncated"" Monte Carlo learning?
","['reinforcement-learning', 'comparison', 'monte-carlo-methods', 'temporal-difference-methods', 'td-lambda']","TD($\lambda$) can be thought of as a combination of TD and MC learning, so as to avoid to choose one method or the other and to take advantage of both approaches.More precisely, TD($\lambda$) is temporal-difference learning with a $\lambda$-return, which is defined as an average of all $n$-step returns, for all $n$, where an $n$-step return is the target used to update the estimate of the value function that contains $n$ future rewards (plus an estimate of the value function of the state $n$ steps in the future). For example, TD(0) (e.g. Q-learning is usually presented as a TD(0) method) uses a $1$-step return, that is, it uses one future reward (plus an estimate of the value of the next state) to compute the target. The letter $\lambda$ actually refers to a parameter used in this context to weigh the combination of TD and MC methods. There are actually two different perspectives of TD($\lambda$), the forward view and the backward view (eligibility traces).The blog post Reinforcement Learning: Eligibility Traces and TD(lambda) gives a quite intuitive overview of TD($\lambda$), and, for more details, read the related chapter of the book Reinforcement Learning: An Introduction."
What is the difference between the $\epsilon$-greedy and softmax policies?,"
Could someone explain to me which is the key difference between the $\epsilon$-greedy policy and the softmax policy? In particular, in the contest of SARSA and Q-Learning algorithms. I understood the main difference between these two algorithms, but I didn't understand all the combinations between algorithm and policy

SARSA + $\epsilon$-greedy
SARSA + Softmax
Q-Learning + $\epsilon$-greedy
Q-Learning + Softmax

","['reinforcement-learning', 'q-learning', 'sarsa', 'epsilon-greedy-policy', 'softmax-policy']",
Can the importance sampling estimator have a non-stationary behaviour policy even if the target policy is stationary?,"
The inverse propensity score (IPS) estimator, which is used for off-policy evaluation in a contextual bandit problem, is well explained in the paper Doubly Robust Policy Evaluation and Optimization.
The old policy $\mu$, or the behavior policy, is okay to be non-stationary in the IPS estimator even if the new policy $\nu$, or the target policy, should be stationary.
Is this true for the importance sampling (IS) estimator, which seems to be a variant of IPS, for off-policy evaluation in a reinforcement learning problem?
IS estimator is explained in this paper Doubly Robust Off-policy Value Evaluation for Reinforcement Learning.
The target policy should be stationary, but can the old policy be non-stationary in the IS estimator?
","['reinforcement-learning', 'comparison', 'policies', 'off-policy-methods', 'importance-sampling']",
EEG and Accelerometer Neural Network,"
I have frequency EEG data from fall and non-fall events and I am trying to incorporate it with accelerometer data that was collected at the same time. One approach is, of course, to use two separate algorithms and find the threshold for each. Then comparing the threshold of each. In other words, if the accelerometer algorithm predicts a fall (fall detected = 1) and the EEG algorithm detects a fall, based on the power spectrum (fall detected = 1), then the system outputs a ""1"" that a fall was truly detected. This approach uses the idea of a simple AND gate between the two algorithms.
I would like to know how to correctly process the data so that I can feed both types of data into a CNN. Any advice is really appreciated, even a lead to some literature, articles or information would be great.
","['neural-networks', 'datasets', 'data-preprocessing', 'computational-linguistics']",
What are evolutionary algorithms for topology and weights evolving of ANN (TWEANN) other than NEAT?,"
I wonder, if there are other than NEAT approaches to evolving architectures and weights of artificial neural networks?
To be more specific: I am looking for projects/frameworks/libraries that use evolutionary/genetic algorithms to simultanousely evolve both topology and train weights of ANNs other than NEAT approach. By 'other' I mean similar to NEAT but not based entirely on NEAT. I hope to find different approaches to the same problem.
","['neural-networks', 'training', 'genetic-algorithms', 'evolutionary-algorithms', 'neuroevolution']",The Wikipedia article on neuroevolution contains a list of neuroevolution techniques (e.g. NEAT). I will list below the examples that evolve both the parameters and the topology of the neural network.
Which deep learning models are suitable for image-to-image mapping?,"
I am working on a problem in which I need to train a neural network to map one or more input images to one or more output images (1 channel for image). Below I report some examples of input&output. In this case I report 1 input and 1 output image, but may need to pass to more inputs and outputs, maybe by encoding this in channels. However, the images are all of this kind, maybe rotated, tralated or changed a bit in shape. (fyi, they are fields defined by fluid dynamics simulations)
I was thinking about CNN, but the standard architecture used for image classification (convolutional layers + fully connected layers) seems not to be the best choice. Instead, I tried using the U-net architecture, composed of compression+decompression convolutional layers. This works quite fine, but maybe there is some other architecture that could be more suited to my problem.
Any suggestion would be appreciated!


","['convolutional-neural-networks', 'computer-vision']",
Scoring feature vector with Support Vector Machine,"
I am reading the R-CNN paper by Ross Girshick1 et al. (link) and I fail to understand how they do the inference. This is described in the section 2.2.Test-time Detection in the paper. I quote:

At test time, we run selective search on the test image to extract around 2000 region proposals (we use selective searchâ€™s â€œfast modeâ€ in all experiments). We warp each proposal and forward propagate it through the CNN in order to read off features from the desired layer. Then, for each class, we score each extracted feature vector using the SVM trained for that class.

I do not understand how a Support Vector Machine (SVM) can score a feature vector since SVM does not tell you class probability, it only tells you if an object belongs to a class or not. How is this possible?
It seems that detection flow is: get image, run it through CNN and get feature vector, score this feature vector and run Non-Maximal Suppresion (NMS). But for running NMS we need the feature vector scored, and again, SVM do not score predictions, right?
Actually, when represented in the same paper, the SVM does not provide a score as you can see in the next image (taken from the same paper).

So, how this makes sense?
","['convolutional-neural-networks', 'computer-vision', 'papers', 'support-vector-machine']",
Lego minifigure facial recognition: where to start? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I'm interested in starting a project that will identify the face of a Lego mini figure from a digital photo. My goal is to eventually map the expression of a person's face to the Lego mini figure.
I don't have any experience working with image recognition technology (my technical experience is mainly in web technology), and I am looking for recommended platforms or resources that I could get started with.
Most helpful would be recommendations for image recognition technologies (Python would be great!) that I could start to experiment with.
NOTE: I'm aware of SparkAR as a library designed to for Instagram camera effects specifically, and even though I'm not interested in Instagram, I wonder if there are comparable libraries/studios/products for working with image recognition development.
","['machine-learning', 'facial-recognition']","Since you are mainly interested in starter libraries and packages to read up on here are some pointers:Image recognition as well as general spatial classification mainly consists of two major tasks.Translating images to data that can be fed into a ML model of any kind.Building a model that is able to use this data to complete tasks like classification (which lego figure is this?), spatial clustering (which pixels form the ""face""), etc.For the first task you will find a lot of python libraries that suit the task but `` PIL/Pillow``` should be the main library for this. So start by reading some documentation and transferring the images you do have (this includes standardizing and cleaning the images to improve the results of step 2).For the second task the actual model you will use depends on your task but generally speaking some form of neural net is a good place to start and if you prefer python then you should look into tensorflow and keras (easy to use interface to tensorflow).Where to startStart with the classical MNIST numbers recognition case and if you have grokked that, you will now where to go from there or at least be able to ask a more specific question here.References"
Why is this Monte Carlo approach scalable for a growing number of states variables and action variables?,"
I am reading a research paper on the formulation of MDP problems to ICU treatment decision making: Treatment Recommendation in Critical Care: A Scalable and Interpretable Approach in Partially Observable Health States. The paper applies a Monte Carlo approach to approximate the value function. Below is a screenshot of the excerpt that I came across. 

The last sentence of the excerpt reads ""The approach is scalable for growing number of states variables and action variables"". 
What does it mean when the author says that the Monte Carlo approach is scalable for a growing number of states variables and action variables? Wouldn't the amount of data needed to approximate the value function increase with the higher dimensionality of states? Or does the Monte Carlo approach scale better in time complexity as compared to traditional Q-learning methods?
","['reinforcement-learning', 'markov-decision-process', 'papers', 'monte-carlo-methods']",
Why does the bias need to be a vector in a neural network?,"
I am learning to use tensorflow.js. I am also using the tfvis library to print information about the neural net to the web browser. When I create a create a dense neural net with a layer with 5 neurons and another layer with 2 neurons, each layer has a bias vector of length 5 and 2 respectively. I checked the docs (https://js.tensorflow.org/api/0.6.1/#layers.dense), and it says that there is indeed a bias vector for each dense layer. Isn't a vector redundant? Doesn't each layer only need a single number for the bias? See the code below:
//Create tensorflow neural net
this.model = tf.sequential();

this.model.add(tf.layers.dense({units: 5, inputShape: [1]}))
this.model.add(tf.layers.dense({units: 2}))

const surface = { name: 'Layer Summary', tab: 'Model Inspection'};
tfvis.show.layer(surface, this.model.getLayer(undefined, 0))

","['neural-networks', 'deep-learning', 'tensorflow', 'javascript']","In a simple feed-forward network, each artificial neuron has a separate bias value. This allows for greater flexibility for the output layer function than if each neuron had to use a single whole-layer bias. Although not an absolute requirement, without this arrangement it may become very hard to approximate some functions. Moving from a bias vector to a single scalar bias value per layer will most of the time reduce the effectiveness of a neural network through lost flexibility in how it fits to the target function.Once you have $N$ output neurons in a layer leading to needing $N$ values for bias, then it is fairly straightforward to model this collection of bias values as a vector.Often you will see a neural network layer function written in this form or similar:$$\mathbf{y} = f(\mathbf{W}\mathbf{x} + \mathbf{b})$$Where $f()$ is the activation function (applied element-wise), $\mathbf{W}$ the weights matrix for the layer and $\mathbf{b}$ is the bias. When written in this form, it is easy to see that $\mathbf{y}$, $\mathbf{W}\mathbf{x}$ and $\mathbf{b}$ must all be vectors of the same size.This layer design has become so standard that it is possible to forget that other designs and implementations are possible for neural network parameters, and can sometimes be useful. Frameworks like TensorFlow also make it easier to take the standard approach, which is why you need a vector for bias on the example you are using. Whilst you are learning, and probably 99% of the time after that, it will be best to go with what the framework is doing here. "
Searching for powerfull AI modules to improve teefÂ gloves,"
I have seen theÂ teefÂ glove ofNavidÂ AzodiÂ andÂ Thomas Pryor, like thisÂ :

and alsoÂ seen this post whichÂ has been saidÂ about this kind of work problem :

Their six-page letter, whichÂ PaddenÂ passed along to the dean, points
  out how theÂ SignAloudÂ glovesâ€”and all the sign-language translation
  gloves inventedÂ soÂ farâ€”misconstrue the nature of ASL (and other sign
  languages) by focusing on what the hands do. Key parts of the grammar
  of ASL include â€œraised or lowered eyebrows, a shift in the orientation
  of the signerâ€™s torso, orÂ a movement ofÂ the mouth,â€ reads the letter.
  â€œEven perfectly functioning gloves would not have access to facial
  expressions.â€ ASL consists of thousands of signs presented in
  Â sophisticatedÂ ways that have, so far, confounded reliable machine
  recognition. One challenge for machines isÂ the complexity ofÂ ASL and
  other sign languages. Signs donâ€™t appear like clearly delineated beads
  on a string; they bleed into one another in a process that linguists
  call â€œcoarticulationâ€ (where, for instance, a hand shape in one sign
  anticipates the shape orÂ locationÂ of the following sign; this happens
  in words in spoken languages, too, where sounds can take on
  characteristics of adjacent ones). Another problem is the lack of
  large data sets of people signing thatÂ can beÂ used to train
  machine-learning algorithms.

SoÂ iÂ like to know what proper AI module do you know for doing theÂ navidÂ works better by adding for first step the geometrical position analyzing into it,Â iÂ like to use popular AI blocks likeÂ TensorfliwÂ for doing this kind of analyzing by fast online and the modules which updated at the time byÂ largeÂ community users.
Update:
I think, some Virtual realityÂ analyzerÂ for theÂ positionÂ analyzingÂ must beÂ existing, so whichÂ oneÂ if popular and free to contributing withÂ largeÂ community?
thanksÂ for your attention.
",['real-time'],
How is clustering used in the unsupervised training of a neural network?,"
How is clustering used in the unsupervised training of a neural network? Can you provide an example?
","['neural-networks', 'machine-learning', 'unsupervised-learning', 'clustering']",
In which cases is the categorical cross-entropy better than the mean squared error?,"
In my code, I usually use the mean squared error (MSE), but the TensorFlow tutorials always use the categorical cross-entropy (CCE). Is the CCE loss function better than MSE? Or is it better only in certain cases?
","['machine-learning', 'comparison', 'objective-functions', 'mean-squared-error', 'categorical-crossentropy']","As a rule of thumb, mean squared error (MSE) is more appropriate for regression problems, that is, problems where the output is a numerical value (i.e. a floating-point number or, in general, a real number).  However, in principle, you can use the MSE for classification problems too (even though that may not be a good idea). MSE can be preceded by the sigmoid function, which outputs a number $p \in [0, 1]$, which can be interpreted as the probability of the input belonging to one of the classes, so the probability of the input belonging to the other class is $1 - p$.Similarly, cross-entropy (CE) is mainly used for classification problems, that is, problems where the output can belong to one of a discrete set of classes. The CE loss function is usually separately implemented for binary and multi-class classification problems. In the first case, it is called the binary cross-entropy (BCE), and, in the second case, it is called categorical cross-entropy (CCE). The CE requires its inputs to be distributions, so the CCE is usually preceded by a softmax function (so that the resulting vector represents a probability distribution), while the BCE is usually preceded by a sigmoid. See also Why is mean squared error the cross-entropy between the empirical distribution and a Gaussian model? for more details about the relationship between the MSE and the cross-entropy. In case you use TensorFlow (TF) or Keras, see also How to choose cross-entropy loss in TensorFlow?, which gives you some guidelines for how to choose the appropriate TF implementation of the cross-entropy function for your (classification) problem. See also Should I use a categorical cross-entropy or binary cross-entropy loss for binary predictions? and Does the cross-entropy cost make sense in the context of regression?."
Can Bert be used to extract embedding for large categorical features?,"
I've lot of training data points (i.e in millions) and I've around few features but the issue with that is all the features are categorical data with 1 million+ categories in each.
So, I couldn't use one hot encoding because it's not efficient so I went with the other option which is embedding of fixed length. I've just used neural nets to compute embedding.
My question is can we use advanced NLP models like bert to extract embeddings for categorical data from my corpus? Is it possible? I've only asked it because I've only heard that bert is good for sentence embeddings.
Thank you.
","['neural-networks', 'machine-learning', 'deep-learning', 'natural-language-processing', 'bert']",
How do I determine the best neural network architecture for a problem with 3 inputs and 12 outputs?,"
This post continues the topic in the following post: 
 Is it possible to train a neural network with 3 inputs and 12 outputs?.
I conducted several experiments in MATLAB and selected those neural networks that best approximate the data.
Here is a list of them:

Cascade-forward backpropagation
Elman backpropagation
Generalized regression
Radial basis (exact fit)

I did not notice a fundamental difference in quality, except for Elman's backpropagation, which had a higher error than the rest.
How to justify the choice of the structure of the neural network in this case?
","['recurrent-neural-networks', 'feedforward-neural-networks', 'neural-architecture-search', 'thought-vectors']",
What are the differences between TensorFlow and PyTorch? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






What are the differences between TensorFlow and PyTorch, both in terms of performance and functionality?
","['comparison', 'tensorflow', 'pytorch']","TensorFlow was developed by Google and is based on Theano (Python library), while Facebook developed PyTorch using the Torch library. Both frames are useful and have a great community behind them. Both provide machine learning libraries to accomplish various tasks and do the job. TensorFlow is a powerful and deep learning tool with active visualization and debugging capabilities. TensorFlow also offers serialization benefits since the entire graphic is saved as a protocol buffer. It also has support for mobile platforms and offers a production-ready implementation. PyTorch, on the other hand, is still gaining momentum and attracting Python developers, since it is more Python friendly. In summary, TensorFlow is used to speed things up and create AI-related products, while research-oriented developers prefer PyTorch."
Can we get the inverse of the function that a neural network represents?,"
I was wondering if it's possible to get the inverse of a neural network. If we view a NN as a function, can we obtain its inverse?
I tried to build a simple MNIST architecture, with the input of (784,) and output of (10,), train it to reach good accuracy, and then inverse the predicted value to try and get back the input - but the results were nowhere near what I started with. (I used the pseudo-inverse for the W matrix.)
My NN is basically the following function: 
$$
f(x) = \theta(xW + b), \;\;\;\;\; \theta(z) = \frac{1}{1+e^{-z}}
$$
I.e. 
def rev_sigmoid(y):
    return np.log(y/(1-y))

def rev_linear(z, W, b):
    return (z - b) @ np.linalg.pinv(W)

y = model.predict(x_train[0:1])
z = rev_sigmoid(y)
x = rev_linear(z, W, b)
x = x.reshape(28, 28)
plt.imshow(x)


^ This should have been a 5:

Is there a reason why it failed? And is it ever possible to get inverse of NN's? 
EDIT: it is also worth noting that doing the opposite does yield good results. I.e. starting with the y's (a 1-hot encoding of the digits) and using it to predict the image (an array of 784 bytes) using the same architecture: input (10,) and output (784,) with a sigmoid. This is not exactly equivalent, to an inverse as here you first do the linear transformation and then the non-linear. While in an inverse you would first do (well, undo) the non-linear, and then do (undo) the linear. I.e. the claim that the 784x10 matrix is collapsing too much information seems a bit odd to me, as there does exist a 10x784 matrix that can reproduce enough of that information. 

","['neural-networks', 'deep-learning', 'python', 'math']",
How is back-propagation useful in neural networks?,"
I am reading about backpropagation and I wonder why I have to backpropagate. 
For example, I would update the network by randomly choosing a weight to change, $w$. I would have $X$ and $y$. Then, I would choose $dw$, a random number from $-0.1$ to $0.1$, for example. Then, I would do two predictions of the neural network and get their losses with the original neural network and one with $w$ changed by $dw$ to get the respective losses $L_{\text{original}}$ and $L_{\text{updated}}$. $L_{\text{updated}} - L_{\text{original}}$ is $dL$. I would update $w$ by $\gamma \frac{d L}{dw}$, where $\gamma$ is the learning rate and $L$ is the loss.
This does not need a gradient backpropagation throughout the system, and must have somehow a disadvantage because no one uses it. What is this disadvantage?
","['neural-networks', 'deep-learning', 'backpropagation']","The method you propose is already known, its basically a numerical approximation to the gradient. It is not used to train neural networks because its well... an approximation. You still need to do two forward passes to get an approximation, which introduces noise and might make the training process fail.Using backpropagation to compute the gradient is an exact solution, so why would you use an approximation if the exact computation is equally efficient?Numeric approximations of the gradient only make sense if exact computation is not possible."
Understanding the nature of psychological defense system by artificial intelligence,"
Do you think psychological defense system, for example, repression,Â regression,Â reaction, formation,Â isolation,Â undoing,Â projection,Â introjection,Â sublimation, etc., could be created by artificial intelligence systems? Can AI also be used to better understand the psychological defense system?
If yes, what tools do we need? Maybe supervised learning algorithms, such as PSO or ANN, are better suited for these levels?
That doesn't seem to be that easy, and I think it needs to have a more general understanding of these algorithms. So I asked here.
On the other hand, what do you think is the appropriate workspace or workspace available for this job?
For example, I think the interactions between robots that use different levels of the defense system by our selection as a game theory round play, could be good and suited to the community which is equal to the psychological connection of this part of us and as a testing environment.
But the robots themselves are dealing with a real problem that makes it even more difficult, som according to this approach, by selecting a more nonlinear problem to solve we can count the using the levels of these defense mechanisms (for example, in simple problem 30% 1th level, 20% 2th level and ...).
",['game-theory'],
Can we just switch off a malicious artificial intelligence?,"
Let us assume we have a general AI that can improve itself and is at least as intelligent as humans.
It has wide access to technical systems including the internet, and it can communicate with humans.
The AI could become malicious. 
Can we just switch off a rouge AI?
","['philosophy', 'agi']",
Is it possible to train a neural network with 3 inputs and 12 outputs?,"
The selection of experimental data includes a set of vectors of different dimensions. The input is a 3-dimensional vector, and the output is a 12-dimensional vector. The sample size is 120 pairs of input 3-dimensional and output 12-dimensional vectors. 
Is it possible to train such a neural network (in MATLAB)? Which structure of the neural network is best suited for this?
","['neural-networks', 'matlab', 'neural-architecture-search', 'thought-vectors']","There is nothing stopping you, you can setup Dense Neural Networks to have any size inputs or outputs (simple proof is to imagine a single layer NN with no activation is just a linear transform and given input dim $n$ and output dim $m$, it's just a matrix of $n$ x $m$, trivially this works though with any number of hidden layers)The better question is should you?. In all honesty, it depends on the data that you have, but, usually, with only 120 examples you'll either overfit completely or do relatively well if the true solution is a very simple function, but, in general, in the common situations where that isn't the case I find myself more likely or not using Bayesian approaches, so I can actually consider confidence (with little data, this is really nice)"
RNN models displays upper limit on predictions,"
I have trained a RNN, GRU, and LSTM on the same dataset, and looking at their respective predictions I have observed, that they all display an upper limit on the value they can predict. I have attached a graph for each of the models, which shows the upper limit quite clearly. Each dot is a prediction, and the orange graph is simply there to illustrate the ground truth (i.e. ground truth on both axis). 



My dataset is split in 60% for training, 20% for test, and 20% for validation and then each of the splits are shuffled. The split/shuffle is the same for all three models, so each model uses the exact same split/shuffle of data for its predictions too. The models are quite simple (2 layers, nothing fancy going on). I have used grid search to find the most optimal hyperparameters for each model. Each model is fed 20 consecutive inputs (a vector of features, e.g. coordinates, waiting time, etc) and produces a single number as output which is the expected remaining waiting time. 
I know this setup strongly favours LSTM and GRU over RNN, and the accuracy of the predictions definitively shows this too.
However, my question is why do each model display an upper limit on its predictions? And why does it seem like such a hard limit?
I cannot wrap my head around what the cause of this is, and so I am not able to determine whether it has anything to do with the models used, how they are trained, or if it is related to the data. Any and all help is very much appreciated!

Hyperparameters for the models are:
RNN: 128 units pr layer, batch size of 512, tanh activation function
GRU: 256 units pr layer, batch size of 512, sigmoid activation function
LSTM: 256 units pr layer, batch size of 256, sigmoid activation function
All models have 2 layers with a dropout in between (with probability rate 0.2), use a learning rate of $10^{-5}$, and are trained over 200 epochs with early stopping with a patience of 10. All models use SGD with a momentum of 0.8 , no nesterov and 0.0 decay. Everything is implemented using Tensorflow 2.0 and Python 3.7. I am happy to share the code used for each model if relevant. 

EDIT 1
I should point out the graphs are made up of 463.597 individual data points, most of which are placed very near the orange line of each graph. In fact, for each of the three models, of the 463.597 data points, the number of data points within 30 seconds of the orange line is:
RNN: 327.206 data points
LSTM: 346.601 data points
GRU: 336.399 data points
In other words, the upper limit on predictions shown on each graph consists of quite a small number of samples compared to the rest of the graph.
EDIT 2
In response to Sammy's comment I have added a graph showing the distribution of all predictions in 30 second intervals. The y-axis represents the base 10 logarithm of the number of samples which fall into a given 30 second interval (the x-axis). The first interval ([0;29]) consists of approximately 140.000 predicted values, out of the roughly 460.000 total number of predicted values. 

","['recurrent-neural-networks', 'long-short-term-memory', 'prediction', 'gated-recurrent-unit']",
How does the generator in GAN's work?,"
After reading a lot of articles (for instance, this one - https://developers.google.com/machine-learning/gan/generator), I've been wondering: how does the generator in GAN's work?
What is the input to the generator? What is the meaning behind ""input noise""?
As I've read, the only input that the generator receives is a random noise, which is weird.
If I would like to create a similar picture of $x$, and put as an input a matrix of random numbers (noise) - it would take A LOT of training until I would get some sort of picture $x^*$, that is similar to the source picture $x$.
The algorithm should receive some type of reference or a basic dataset (for instance, the set of $x$'s) in order to start the generation of the fake image $x^*$.
","['deep-learning', 'generative-adversarial-networks']","In the basic implementation of GANs, the Generator only takes in a vector of random variables. This might seem strange, but after training, the generator can transform this input noise into an image resembling those of the training set.It is trained along with its counterpart the Discriminator, whose goal is to distinguish real images (i.e. the dataset's images) from fake ones (i.e. images produced by the Generator). The Generator's goal in training is to fool the Discriminator into thinking that its images are real.In the beginning, where they are both untrained, they are both ""terrible"" at their respective tasks. The Generator can't produce anything resembling an image, but the Discriminator can't distinguish real from fake. As training progresses, the Discriminator starts identifying ways to distinguish the real images from the fake ones (i.e. patterns that appear in real images, but not in fake ones). The Generator, however, in its attempt to fool the Discriminator, starts producing those same patterns in its own images. After a while of both models becoming better at their respective tasks, we reach a point where the Generator can produce realistic images and the Discriminator is very good at distinguishing between real or fake.Edit as suggested from comment:A vector of random values is used as an input, so that the Generator can learn to generate unique outputs. In itself the Generator is deterministic, meaning that it has no internal sources of randomness. If we give it the same input vector twice, it will produce the same output both times. Thus, we feed the Generator with random values, so that it can learn to produce different outputs, depending on those values."
Regional specialization in neural networks (especially for language processing)?,"
What is the status of the research on regional specialization of the artificial neural networks? Biology knows that such specialization exists in the brain and it is very important for the functioning of the brain. My thinking is that specialization can solve the transfer learning/catastrophic forgetting by creating centers of sophisticated skills if such sophistication is necessary. Actually - there is no much alternative to specialization? If specialization exists then there can be small decision centers that routes the request to the actual part and such decision centers can be efficient. But if specialization does not exist then routing happens in the total soup/pudding of neuronal see and such all-to-all routing should be very inefficient. Of course there should be some mixing of specialization vs pudding, because there is always mixing between rationality and emotions, between execution and exploration, but nevertheless - specialization should happen, at least partially.
The problem is - that I can not find any focused article about such specialization and I can not find how such specialization can be trained? There are research on hierarchical reinforcement learning - but that is about imposing the external fixed structure on the set of neural networks, but it is not how the nature works - nature implements such hiearchy within neural network and not by imposing rigorous, symbolic structures.
Are there some notions, terms, keywords, research trends, important articles (and researchers) devoted to such specialization (including the machine learning of such specialization).
Of course, my topic is very large, but the actual work on this is small ir nonexistant and that is why it is focused.
There is work on convolution neural networks but maybe there is another approach for language processing where the parts can be specialized in - parsing, understanding, anaphora resolution, translation, etc? And is the convolution the kind of specialization I am seeking? 
Maybe the notion of attention somehow is connected with my question. But usually the attention is connected with the single neurons and not with the regions? Maybe there is notions about hierarchy of attentions - one level of attention values refers to high level tasks/skills, but another level of attention values refers to subskills, etc.
","['neural-networks', 'reference-request', 'artificial-neuron', 'neural-architecture-search']",
Solving a planning if finding the goal state is part of the problem,"
I having trouble finding some starting points for solving an occupancy  problem which seems like a good candidate for ai.
Assume the following situation:
In a company I have n cars and m employees. Not every employee can drive any car (f. e. a special driving license is required). A car can only be used by one employee at a specific point in time.
There is a plan which states which employee must be somewhere within some time (therefor they must use a car, so the car is blocked for that amount of time).
The goal is to find a near optimal occupancy of the cars according to that plan.
This problem is easy to specify, but I'm stumped as to which methods to implement.
As it can be represented by a graph I think the right way to solve such a problem is using searching techniques, but a problem here is that I don't know the goal state (and there is no efficient way to compute it - thats the task I want the ai to do...). Neither finding the goal state is in fact part of the problem.
So my question is: What ai techniques could be used to solve such a problem ?
Edit: Some clarification:
Assmume we have two sets - one of the employees (E) and one of cars (C). |C| < |E| is most likely true.
Each car has an assigned priority which corresponds to the costs of using it (for example using a Ferrari costs more than using a Dacia, therefore a Dacia has a higher priority (ex. 1) compred to the Ferrari (ex. 10))).
Assume further that having employees which are not using a car at a specific time slice are a bad thing - they cost an individual penalty (you want the employeed to be at the customer and sell things etc.).
The goal is to find the occupation of employees and cars which has a low total cost. 
One Example: If you assign an employee to a car at a specific time slice  it may turn out that another employee gets no car within that time slice.
This can be either because 

a car is free, but he has no license for it   
because a car is free, but the costs of using this car would be higher than having the employee staing at the head quater 
because no car is free anymore

Of cause it could be better in terms of costs to change the occupation and give that employee which got no car in this solution a car and therefore having another employee getting no car or not using all cars or ...
Note: There is no need to find an exact optimal solution (=lowest total cost of all possible occupations), as this would require checking out all possible occupations of the exponential solution space.
Insetad finding a more or less good approximation of a near-optimal low total cost is sufficent.
","['algorithm', 'optimization', 'problem-solving', 'graph-theory']",
Is greedy layer-wise pretraining obsolete?,"
I was looking into the use of a greedy layer-wise pretraining to initialize the weights of my network.
Just for the sake of clarity: I'm referring to the use of gradually deeper and deeper autoencoders to teach the network gradually more abstract representations of the input one layer at the time.
However, reading HERE, I read:

Nevertheless, it is likely better performance may be achieved using modern methods such as better activation functions, weight initialization, variants of gradient descent, and regularization methods.

and

Today, we now know that greedy layer-wise pretraining is not required to train fully connected deep architectures, but the unsupervised pretraining approach was the first method to succeed.

My question is then: if I'm building a network already using ""modern"" techniques, such as ReLU activations, batch normalization, adam optimizers, etc, is the good-ol' greedy layer-wise pretraining useless? Or can it still provide an edge in the initialization of the network?
",['neural-networks'],"It depends. It could give you a boost or it could not.Intuitively I would expect it to actually hurt performance if the network is initialized correctly (I think the optimizer is less of a bottleneck because they will have the same effect in both approaches). Ideal World: We optimize the network as a whole to gain better course grained features over the sequential layers of the encoderReality years ago: Deep nets have trouble in information propagation either forward or backwards (ex: vanishing/exploding activation and vanishing/exploding gradient).  Reality today: Depending on the domain, there are various publications discussing how to circumvent this issue (ex: Residual networks, He initialization, fixup initialization, batchnorms, different activation functions, etc...)I hope this  helped give some form of intuition of the matter."
How does non-max suppression work when one or multiple bounding boxes are predicted for the same object?,"
My understanding of how non-max suppression works is that it suppresses all overlapping boxes that have a Jaccard overlap smaller than a threshold (e.g. 0.5). The boxes to be considered are on a confident score (may be 0.2 or something). So, if there are boxes that have a score over 0.2 (e.g. the score is 0.3 and overlap is 0.4) the boxes won't be suppressed.
In this way, one object will be predicted by many boxes, one high score box, and many low confident score boxes, but I found that the model predicts only one box for one object. Can someone enlighten me?
I currently viewing the ssd from  https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection
Here is the code.
#Finding Jaccap Overlap and sorting scotes
class_scores, sort_ind = class_scores.sort(dim=0, descending=True)
class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)
overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  
suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  

for box in range(class_decoded_locs.size(0)):
# If this box is already marked for suppression
    if suppress[box] == 1:
    continue
    suppress = torch.max(suppress, overlap[box] > max_overlap)
    suppress[box] = 0

","['computer-vision', 'object-detection', 'non-max-suppression']",
How can I show that the VC dimension of the set of all closed balls in $\mathbb{R}^n$ is at most $n+3$?,"
How can I show that the VC dimension of the set of all closed balls in $\mathbb{R}^n$ is at most $n+3$?
For this problem, I only try the case $n=2$ for 1. When $n=2$, consider 4 points $A,B,C,D$ and if one point is inside the triangle formed by the other three, then we cannot find a circle that only excludes this point. If $ABCD$ is convex assume WLOG that $\angle ABC + \angle ADC \geq 180$ then use some geometric argument to show that a circle cannot include $A,C$ and exclude $B,D$.
For the general case Iâ€™m thinking of finding $n+1$ points so that a ball should be quite â€˜largeâ€˜ to include them, and that this ball can not exclude the other 2 points.  However, in high-dimensional case I do not know how to use maths language to describe what is â€˜largeâ€™.
Can anyone give some ideas to this question please?
","['proofs', 'computational-learning-theory', 'vc-dimension']",
Why I got the same action when testing the A2C?,"
I'm working on an advantage actor-critic (A2C) reinforcement learning model, but when I test the model after I trained for 3500 episodes, I start to get almost the same action for all testing episodes. While if I trained the system for less than 850 episodes, I got different actions. The value of state is always different, and around 850 episodes, the loss becomes zero.
Here is the Actor and critic Network
        with g.as_default():
            #==============================actor==============================#
            actorstate = tf.placeholder(dtype=tf.float32, shape=n_input, name='state')
            actoraction = tf.placeholder(dtype=tf.int32, name='action')
            actortarget = tf.placeholder(dtype=tf.float32, name='target')

            hidden_layer1 = tf.layers.dense(inputs=tf.expand_dims(actorstate, 0), units=500, activation=tf.nn.relu, kernel_initializer=tf.zeros_initializer())
            hidden_layer2 = tf.layers.dense(inputs=hidden_layer1, units=250, activation=tf.nn.relu, kernel_initializer=tf.zeros_initializer())
            hidden_layer3 = tf.layers.dense(inputs=hidden_layer2, units=120, activation=tf.nn.relu, kernel_initializer=tf.zeros_initializer())
            output_layer = tf.layers.dense(inputs=hidden_layer3, units=n_output, kernel_initializer=tf.zeros_initializer())
            action_probs = tf.squeeze(tf.nn.softmax(output_layer))
            picked_action_prob = tf.gather(action_probs, actoraction)

            actorloss = -tf.log(picked_action_prob) * actortarget
            # actorloss = tf.reduce_mean(tf.losses.huber_loss(picked_action_prob, actortarget, delta=1.0), name='actorloss')

            actoroptimizer1 = tf.train.AdamOptimizer(learning_rate=var.learning_rate)

            if var.opt == 2:
                actoroptimizer1 = tf.train.RMSPropOptimizer(learning_rate=var.learning_rate, momentum=0.95,
                                                            epsilon=0.01)
            elif var.opt == 0:
                actoroptimizer1 = tf.train.GradientDescentOptimizer(learning_rate=var.learning_rate)

            actortrain_op = actoroptimizer1.minimize(actorloss)

            init = tf.global_variables_initializer()
            saver = tf.train.Saver(max_to_keep=var.n)

        p = tf.Graph()
        with p.as_default():
            #==============================critic==============================#
            criticstate = tf.placeholder(dtype=tf.float32, shape=n_input, name='state')
            critictarget = tf.placeholder(dtype=tf.float32, name='target')

            hidden_layer4 = tf.layers.dense(inputs=tf.expand_dims(criticstate, 0), units=500, activation=tf.nn.relu, kernel_initializer=tf.zeros_initializer())
            hidden_layer5 = tf.layers.dense(inputs=hidden_layer4, units=250, activation=tf.nn.relu, kernel_initializer=tf.zeros_initializer())
            hidden_layer6 = tf.layers.dense(inputs=hidden_layer5, units=120, activation=tf.nn.relu, kernel_initializer=tf.zeros_initializer())
            output_layer2 = tf.layers.dense(inputs=hidden_layer6, units=1, kernel_initializer=tf.zeros_initializer())
            value_estimate = tf.squeeze(output_layer2)

            criticloss= tf.reduce_mean(tf.losses.huber_loss(output_layer2, critictarget,delta = 0.5), name='criticloss')
            optimizer2 = tf.train.AdamOptimizer(learning_rate=var.learning_rateMADDPG_c)
            if var.opt == 2:
                optimizer2 = tf.train.RMSPropOptimizer(learning_rate=var.learning_rate_c, momentum=0.95,
                                                            epsilon=0.01)
            elif var.opt == 0:
                optimizer2 = tf.train.GradientDescentOptimizer(learning_rate=var.learning_rateMADDPG_c)

            update_step2 = optimizer2.minimize(criticloss)

            init2 = tf.global_variables_initializer()
            saver2 = tf.train.Saver(max_to_keep=var.n)

 

This is the choice of action.
def take_action(self, state):
                """"""Take the action""""""
                action_probs = self.actor.predict(state)
                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)
                return action

This is the actor.predict function.
def predict(self, s):
        return self._sess.run(self._action_probs, {self._state: s})

Any Idea what causing this?
Update
Change the learning rate, state, and the reward solve the problem where I reduce the size of the state and also added switching cost to the reward.
","['reinforcement-learning', 'python', 'actor-critic-methods', 'advantage-actor-critic', 'a3c']","Disclaimer: Without the full code, we can only speculate. I encourage you to post the full code on Google Colab or something like this.
In the meanwhile, here is my point of view:Looks like your model has found some ""master action"" that always leads to zero loss, no matter what the state is. So it's not necessarily bad, it's just unexpected according to your point of view.An example for that would be pausing the game - so you never loose.You might not like it, but in de model's point of view, it's absolutely nailing it!So how to convince the actor not to pause the game?Not by changing the model, or tuning hyper-parameters, but by reformulating the problem. In this example, instead of just penalizing the model for failing, you should reward if for winning, so pausing is no longer the best option.It might not be a problem in the Machine Learning model, but in your environment and reward models. As we don't have access to that, it's hard to provide an answer.You are the CartPole-v0 environment:A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.Source: https://gym.openai.com/envs/CartPole-v0/It is a solvable problem. Probably your model has just learned how to solve it after a few hundreds generations. (The link shows a table with ""Episodes before solve"" for each algorithm, showing numbers consistent to yours)."
A problem about the relation between 1-oracle and 2-oracle PAC model,"
This problem is about two-oracle variant of the PAC model. Assume that positive and negative examples are now drawn from two separate distributions $\mathcal{D}_{+}$ and $\mathcal{D}_{-} .$ For an accuracy $(1-\epsilon),$ the learning algorithm must find a hypothesis $h$ such that:
$$
\underset{x \sim \mathcal{D}_{+}}{\mathbb{P}}[h(x)=0] \leq \epsilon \text { and } \underset{x \sim \mathcal{D}_{-}}{\mathbb{P}}[h(x)=1] \leq \epsilon$$
Thus, the hypothesis must have a small error on both distributions. Let $\mathcal{C}$ be any concept class and $\mathcal{H}$ be any hypothesis space. Let $h_{0}$ and $h_{1}$ represent the identically 0 and identically 1 functions, respectively. Prove that $\mathcal{C}$ is efficiently PAC-learnable using $\mathcal{H}$ in the standard (one-oracle) PAC model if and only if it is efficiently PAC-learnable using $\mathcal{H} \cup\left\{h_{0}, h_{1}\right\}$ in this two-oracle PAC model.
However, I wonder if the problem is correct. In the official solution, when showing that 2-oracle implies 1-oracle, the author returns $h_0$ and $h_1$ when the distribution is too biased towards positive or negative examples. However, in the problem, it is required that only in 2-oracle case we can return $h_0$ and $h_1$. Therefore, in this too-biased case, it seems that there may not exist a 'good' hypothesis at all.
Is this problem wrong? Or I make some mistake somewhere?
","['machine-learning', 'proofs', 'computational-learning-theory', 'pac-learning']",
"How to Prove This Inequality, Related to Generalization Error (Not Using Rademacher Complexity)?","
This is an inequality on page 36 of the Foundations of Machine Learning by Mohri, but the author only states it without proof.
$$
\mathbb{P}\left[\left|R(h)-\widehat{R}_{S}(h)\right|>\epsilon\right] \leq 4 \Pi_{\mathcal{H}}(2 m) \exp \left(-\frac{m \epsilon^{2}}{8}\right)
$$
Here the growth function $\Pi_{\mathcal{F}}: \mathbb{N} \rightarrow \mathbb{N}$ for a hypothesis set $\mathcal{H}$ is defined by:
$$
\forall m \in \mathbb{N}, \Pi_{\mathcal{F}}(m)=\max _{\left\{x_{1}, \ldots, x_{m}\right\} \subseteq X}\left|\left\{\left(h\left(x_{1}\right), \ldots, h\left(x_{m}\right)\right): h \in \mathcal{H}\right\}\right|
$$
Given a hypothesis h $\in \mathcal{H},$ a target concept $c \in \mathcal{C}$ and an underlying distribution $\mathcal{D},$ the generalization error or risk of $h$ is defined by
$$
R(h)=\underset{x \sim D}{\mathbb{P}}[h(x) \neq c(x)]=\underset{x \sim D}{\mathbb{E}}\left[1_{h(x) \neq c(x)}\right]
$$
where $1_{\omega}$ is the indicator function of the event $\omega$.
And the empirical error or empirical risk of $h$ is defined
$$
\widehat{R}_{S}(h)=\frac{1}{m} \sum_{i=1}^{m} 1_{h\left(x_{i}\right) \neq c\left(x_{i}\right)}
$$
In the book, the author proves another inequality that differs from this one by only a constant using Rademacher complexity, but he says that the stated inequality can be proved without using Rademacher complexity. Does anyone know how to prove it?
","['machine-learning', 'proofs', 'probability', 'computational-learning-theory', 'generalization']",
Reinforcement learning CNN input weakness,"
I'm trying to train a network to navigate a 48x48 2D grid, and switch pixels from on to off or off to on. The agent receives a small reward if correct, and small punishment if incorrect pixel plotted.
I thought, like the Deepmind ""Playing Atari with Deep Reinforcement Learning"" Paper, I could just use only the pixel input, fed through 2 convolutional layers, to solve this task. The output of this is fed into 512 fully connected layer.
Unfortunately, it barely trains. 
When instead using additional vectors as input containing information about nearby pixels' state around the agent, the agent learns the task quite well (yet often orients the wrong awkwardly). 
Each step, the agent moves up down left right, and plot pixel or not.
The agent is visualized in the environemtn as a red square with white center dot. (also tried single red pixel). On-pixels within the red square are colored purple.
Is there something I can try to make the agent learn visual input better?
The orange line is the training with only visual observations, the grey one contained vector observations about the immediate neighboring pixel state as well. 

","['reinforcement-learning', 'convolutional-neural-networks']",
"Why Monte Carlo epsilon-soft approach cannot compute $\max Q(s,a)$?","
I am new to Reinforcement learning and am currently reading up on the estimation of Q $\pi(s, a)$ values using MC epsilon-soft approach and chanced upon this algorithm. The link to the algorithm is found from this website. 
https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/
def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):

if not policy:
    policy = create_random_policy(env)
# Create an empty dictionary to store state action values
Q = create_state_action_dictionary(env, policy)

# Empty dictionary for storing rewards for each state-action pair
returns = {} # 3.

for _ in range(episodes): # Looping through episodes
    G = 0 # Store cumulative reward in G (initialized at 0)
    episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively

    # for loop through reversed indices of episode array.
    # The logic behind it being reversed is that the eventual reward would be at the end.
    # So we have to go back from the last timestep to the first one propagating result from the future.

    # episodes = [[s1,a1,r1], [s2,a2,r2], ... [Sn, an, Rn]]
    for i in reversed(range(0, len(episode))):
        s_t, a_t, r_t = episode[i]
        state_action = (s_t, a_t)
        G += r_t # Increment total reward by reward on current timestep

        # if state - action pair not found in the preceeding episodes,
        # then this is the only time the state appears in this episode.

        if not state_action in [(x[0], x[1]) for x in episode[0:i]]: #
            # if returns dict contains a state action pair from prev episodes,
            # append the curr reward to this dict
            if returns.get(state_action):
                returns[state_action].append(G)
            else:
                # create new dictionary entry with reward
                returns[state_action] = [G]

            # returns is a dictionary that maps (s,a) : [G1,G2, ...]
            # Once reward is found for this state in current episode,
            # average the reward.
            Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes

            # Finding the action with maximum value.



            Q_list = list(map(lambda x: x[1], Q[s_t].items()))
            indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]
            max_Q = random.choice(indices)

            A_star = max_Q # 14.

            # Update action probability for s_t in policy
            for a in policy[s_t].items():
                if a[0] == A_star:
                    policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))
                else:
                    policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))

return policy

This algorithm computes the $Q(s, a)$ for all state action value pairs that the policy follows. If $\pi$ is a random policy, and after running through this algorithm, and for each state take the $\max Q(s,a)$ for all possible actions, why would that not be equal to $Q_{\pi^*}(s, a)$ (optimal Q function)? 
From this website, they claim to have been able to find the optimal policy when running through this algorithm.
I have read up a bit on Q-learning and the update equation is different from MC epsilon-soft. However, I can't seem to understand clearly how these 2 approaches are different. 
","['reinforcement-learning', 'q-learning', 'monte-carlo-methods']","If $\pi$ is a random policy, and after running through this algorithm, and for each state take the $\max Q(s,a)$ for all possible actions, why would that not be equal to $Q_{\pi^*}(s, a)$ (optimal Q function)? Assuming that the estimates for $Q_{\pi}(s,a)$ have converged to close to correct values from many samples, then a policy based on $\pi'(s) = \text{argmax}_a Q_{\pi}(s,a)$ is not guaranteed to be an optimal policy unless the policy $\pi$ being measured is already the optimal policy.This is because the action value $Q_{\pi}(s,a)$ gives the expected future reward from taking action $a$ in state $s$, and from that point on following the policy $\pi$. The function does not, by itself, adapt to the idea that you might change other action choices as well. It is a measure of immediate differences between action choices at any given time step. Therefore if there are any long-term dependencies where your action choice at $t$ would be different if only you could guarantee a certain choice at $t+1$ or later, this cannot be resolved by simply taking the maximum $Q_{\pi}(s,a)$ when $\pi$ was a simple random policy.However, if you do decide to change the policy such that you always follow actions $\pi'(s) = \text{argmax}_a Q_{\pi}(s,a)$ for all states, then you can say this: For each state, $V_{\pi'}(s) \ge V_{\pi}(s)$. I.e.  $\pi'(s)$ is no worse than, and may be a strict improvement over $\pi(s)$. Better than that, $\pi'(s)$ will be a strict improvment over $\pi(s)$ if $Q_{\pi}(s,a)$ is accurate and $\pi(s)$ is not already the optimal policy $\pi^*(s)$. This is the basis for the Policy Improvement Theorem which shows that if you repeat the process of measuring $Q_{\pi^k}$ and then creating a new policy $\pi^{k+1}(s) = \text{argmax}_a Q_{\pi^k}(s,a)$ that you will eventually find the optimal policy. You only have to repeat your idea many times to eventually find $\pi^*$The Dynamic Programming technique Policy Iteration does this exactly. All other value-based Reinforcement Learning methods are variations of this idea and rely at least in part on this proof."
Why everyone is using CNN for image segmentation?,"
I'm a newbie in artificial intelligence.
I have started to research how to do image segmentation and all the papers that I have found are about CNN. Most of them use the same network, U-net, but with little variations: with more or fewer layers, different parameter values, etc.; but with not very different results.
It seems that CNNs are in fashion and everyone uses them. Or there are other reasons that I don't know.
If everyone is getting not very different results, why are they using the same approach instead of trying different ones?
","['convolutional-neural-networks', 'applications', 'image-segmentation', 'u-net']",CNN is used since it is effectively an optimized use case for dealing with image data. CNN effectively automatically extracts features from images. Other techniques are more likely to not take full advantage of the data. CNN is able to make full use of the data by also including information from adjacent pixels and downsample through layers.  
Why is this deep Q agent constantly learning just one action?,"
I'm trying to implement deep q learning in the OpenAI's gym ""Taxi-v3"" environment. But my agent only learns to do one action in every state. What am I doing wrong? Here is the Github repository with the code.
","['deep-learning', 'reinforcement-learning', 'open-ai', 'pytorch', 'gym']","I thought about my input-layer. I had the 500 states one hot encoded. So 499 of every input node would be 0. And 0 is very bad in an neural network. I tried the same code with the ""CardPole-v0"" and it worked. So think about your input guys "
What activation functions are better for what problems?,"
Iâ€™ve been reading about neural network architectures. In certain cases, people say that the sigmoid ""more accurately reflects real-life"" and, in other cases, functions like hard limits reflect ""the brain neural networks more accurately"".  
What activation functions are better for what problems?
","['neural-networks', 'deep-learning', 'activation-functions']",
Convert a PAC-learning algorithm into another one which requires no knowledge of the parameter,"
This is part of the exercise 2.13 in the book Foundations of Machine Learning (page 28). You can refer to chapter 2 for the notations.

Consider a family of concept classes $\left\{\mathcal{C}_{s}\right\}_{s}$ where $\mathcal{C}_{s}$ is the set of concepts in $\mathcal{C}$ with size at most $s.$ Suppose we have a PAC-learning algorithm $\mathcal{A}$ that can be used for learning any concept class $\mathcal{C}_{s}$ when $s$ is given. Can we convert $\mathcal{A}$ into a PAC-learning algorithm $\mathcal{B}$ that does not require the knowledge of $s ?$ This is the main objective of this problem.
To do this, we first introduce a method for testing a hypothesis $h,$ with high probability. Fix $\epsilon>0, \delta>0,$ and $i \geq 1$ and define the sample size $n$ by $n=\frac{32}{\epsilon}\left[i \log 2+\log \frac{2}{\delta}\right].$ Suppose we draw an i.i.d. sample $S$ of size $n$ according to some unknown distribution $\mathcal{D}.$ We will say that a hypothesis $h$ is accepted if it makes at most $3 / 4 \epsilon$ errors on $S$ and that it is rejected otherwise. Thus, $h$ is accepted iff $\widehat{R}(h) \leq 3 / 4 \epsilon$
(a) Assume that $R(h) \geq \epsilon .$ Use the (multiplicative) Chernoff bound to show that in that case $\mathbb{P}_{S \sim D^{n}}[h \text { is accepted}] \leq \frac{\delta}{2^{i+1}}$
(b) Assume that $R(h) \leq \epsilon / 2 .$ Use the (multiplicative) Chernoff bounds to show that in that case $\mathbb{P}_{S \sim \mathcal{D}^{n}}[h \text { is rejected }] \leq \frac{\delta}{2^{i+1}}$
(c) Algorithm $\mathcal{B}$ is defined as follows: we start with $i=1$ and, at each round $i \geq 1,$ we guess the parameter size $s$ to be $\widetilde{s}=\left\lfloor 2^{(i-1) / \log \frac{2}{\delta}}\right\rfloor .$ We draw a sample $S$ of size $n$ (which depends on $i$ ) to test the hypothesis $h_{i}$ returned by $\mathcal{A}$ when it is trained with a sample of size $S_{\mathcal{A}}(\epsilon / 2,1 / 2, \widetilde{s}),$ that is the sample complexity of $\mathcal{A}$ for a required precision $\epsilon / 2,$ confidence $1 / 2,$ and size $\tilde{s}$ (we ignore the size of the representation of each example here). If $h_{i}$ is accepted, the algorithm stops and returns $h_{i},$ otherwise it proceeds to the next iteration. Show that if at iteration $i,$ the estimate $\widetilde{s}$ is larger than or equal to $s,$ then $\mathbb{P}\left[h_{i} \text { is accepted}\right] \geq 3 / 8$

Question (a) and (b) are easy to prove, but I have trouble with the question (c). More specifically, I don't know how to use the condition that $\widetilde{s} \geq s$. Can anyone help?
","['machine-learning', 'proofs', 'probability', 'computational-learning-theory', 'pac-learning']",
Why we multiply probabilities with support to obtain Q-values in Distributional C51 algorithm?,"
In 'Deep Reinforcement Learning Hands-On' book and chapter about Distributional C51 algorithm I'm reading, that to obtain Q-values from the distribution I need to calculate the weighted sum of the normalized distribution and atom's values.
Why I have to multiply that distribution with support? How does it work and what happening there?
","['reinforcement-learning', 'probability-distribution']",
How can I compare EEG data with accelerometer data in 1 algorithm?,"
I have frequency EEG data from fall and non-fall events and I am trying to incorporate it with accelerometer data that was collected at the same time.  One approach is, of course, to use two separate algorithms and find the threshold for each.  Then comparing the threshold of each.  In other words, if the accelerometer algorithm predicts a fall (fall detected = 1) and the EEG algorithm detects a fall, based on the power spectrum (fall detected = 1), then the system outputs a ""1"" that a fall was truly detected.  This approach uses the idea of a simple AND gate between the two algorithms.  
I would like to know how to correctly process the data so that I can feed both types of data into one algorithm, perhaps a CNN. Any advice is really appreciated, even a lead to some literature, articles or information would be great.  
","['neural-networks', 'deep-learning', 'comparison', 'datasets', 'computational-linguistics']",
How does the weight update formula for logistic regression work?,"
I am trying to use Logistic Regression to make a spam filter, but I am having trouble understanding the weight update part. I have processed my email dataset, and I have an attribute vector of the top n words that are most likely to be contained within a spam.
From my understanding, during training, I will have to implement an optimization formula after each training example in order to update the weights.
$$
w_l \leftarrow w_l + \eta \cdot \sum_{i=1}^m [ y^{(i)} - P(c_+ \mid \vec{x}^{(i)} )] \cdot x_l^{(i)}
$$
How does a formula such as this work? How can it be implemented in Python?
","['machine-learning', 'logistic-regression', 'text-classification']",
How many ways are there to perform image segmentation?,"
I'm new in Artificial Intelligence and I want to do image segmentation.
Searching I have found these ways

Digital image processing (I have read it in this book: Digital Image Processing, 4th edition)
Convolutional neural networks

Is there something else that I can use?
","['convolutional-neural-networks', 'reference-request', 'image-segmentation', 'algorithm-request', 'model-request']","Apart from the multitudes of traditional image segmentation techniques (Watershed, Clustering or Variational methods), newer Segmentation schemes using Deep Learning are actively being used, which provide better results and are better for real-time applications, owing to minimum computation overheads involved.The following blog provides a detailed review of recent advancements in this field: Review of Deep Learning Algorithms for Image Semantic SegmentationFor the traditional methods, this Wikipedia article provides a nice summary:
Image Segmentation"
What is the difference between batches in deep Q learning and supervised learning?,"
How is the batch loss calculated in both DQNs and simple classifiers? From what I understood, in a classifier, a common method is that you sample a mini-batch, calculate the loss for every example, calculate the average loss over the whole batch, and adjust the weights w.r.t the average loss?
(Please correct me if I'm wrong)
But is this the same in DQNs? So, you sample a batch from your memory, say 64 transitions. Do I iterate through each transition and adjust the weights ""on the fly"", or do I calculate the average loss of the batch and THEN in a big step adjust the weights w.r.t the average batch loss?
","['reinforcement-learning', 'deep-learning', 'backpropagation', 'objective-functions', 'batch-learning']","From what I understood in a classifier a common method is that you sample a mini-batch, calculate the loss for every example, calculate the average loss over the whole batch and adjust the weights w.r.t the average loss? (Please correct me if I'm wrong)You are wrong.The weights are adjusted w.r.t. the average gradient, and this must be calculated using individual loss function results. The average loss (or cost function when considering the whole dataset) is a useful metric for current performance, and it is the measure being minimised. But you cannot calculate meaningful gradients against the average loss directly.But is this the same in DQNs?The batch process is not as you described, but an experience replay minibatch in RL and a sampled minibatch in supervised learning can be very similar. The main difference in RL is that your prediction targets must be recalculated as part of the sampling process (using $G_{t:t+1} = R_{t+1} + \gamma \text{max}_{a'}\hat{q}(S_{t+1},a', \theta)$ to calculate the TD target, assuming you are using single step Q learning), whilst in most supervised learning the target values are fixed for each example.In theory you could use repeated single item stochastic gradient descent in DQN, it doesn't break any theory, and it would work. However, it will usually be more efficient to use a standard minibatch update, combining all gradients into one average gradient for the minibatch and making a single update step.If you are using a high level library for your neural network model in DQN, you usually don't need to worry about this detail. You can use the .fit function or whatever the library provides. In that case the only difference between a supervised learning update and an experience replay DQN update is what you get from the sampling. In supervised learning you get a set of $(\mathbf{x}_i, \mathbf{y}_i)$ examples directly by sampling a minibatch. In RL you get $(\mathbf{s}_i, \mathbf{a}_i, r, \mathbf{s'}_i, done)$ and must construct the $(\mathbf{x}_i, \mathbf{y}_i)$ minibatch from these before passing to your .fit function"
"When training a CNN, what are the hyperparameters to tune first?","
I am training a convolutional neural network for object detection. Apart from the learning rate, what are the other hyperparameters that I should tune? And in what order of importance? Besides, I read that doing a grid search for hyperparameters is not the best way to go about training and that random search is better in this case. Is random search really that good?
","['convolutional-neural-networks', 'training', 'computer-vision', 'optimization', 'hyperparameter-optimization']",
Why does PyTorch use a different formula for the cross-entropy?,"
In my understanding, the formula to calculate the cross-entropy is
$$
H(p,q) = - \sum p_i \log(q_i)
$$
But in PyTorch nn.CrossEntropyLoss is calculated using this formula:
$$
loss = -\log\left( \frac{\exp(x[class])}{\sum_j \exp(x_j)} \right)
$$
that I think it only addresses the $\log(q_i)$ part in the first formula. 
Why does PyTorch use a different formula for the cross-entropy?
","['neural-networks', 'deep-learning', 'objective-functions', 'pytorch', 'cross-entropy']","When you one-hot-encode your labels with $p_i \in \{0,1\}$ you get $p_i = 0$ iff $i$ is not correct and, equivalently, $p_i =1$ iff $i$ is correct. Hence, $p_i \log(q_i) = 0 \log(q_i) = 0 $ for all classes except the ""truth"" and $p_i \log(q_i) = 1 \log(q_i) = \log(q_i) $ for the correct prediction. Therefore, your loss reduces to:
$$
H(p,q) = - \sum p_i \log(q_i) = - \log(q_{truth})
$$"
Are there clever (fitness-based) crossover operators for binary chromosomes?,"
While studying genetic algorithms, I've come across different crossover operations used for binary chromosomes, such as the 1-point crossover, the uniform crossover, etc. These methods usually don't use any ""intelligence"".
I found methods like the fitness-based crossover and Boltzmann crossover, which use fitness value so that the child will be created from better parents with a better probability.
Is there any other similar method that uses fitness or any other way for an intelligent crossover for binary chromosomes?
","['genetic-algorithms', 'evolutionary-algorithms', 'crossover-operators', 'chromosomes']",
What's the function that SGD takes to calculate the gradient?,"
I'm struggling to fully understand the stochastic gradient descent algorithm. 
I know that gradient descent allows you to find the local minimum of a function. What I don't know is what exactly that function IS.
More specifically, the algorithm should work by initializing the network with random weights. Then, if I'm not mistaken, it forward-propagates $n$ times (where $n$ is the mini-batch size). At this point, I've no idea about what function should I search for, with over hundreds of neurons each having hundreds of parameters.
","['neural-networks', 'deep-learning', 'backpropagation', 'objective-functions', 'gradient-descent']",
Training dataset for convolutional neural network classification - will images captured on the ground be useful for training aerial imagery?,"
I am an agronomy graduate student looking to classify crops from weeds using convolutional neural networks (CNNs).
The basic idea that I am wanting to get into involves separating crops from weeds from aerial imagery (either captured by drones or piloted aircraft). The idea of the project that I am proposing involves spending some time driving around to different fields and capturing many images of both crops and weeds. These images will then be used to train a CNN that will classify aerial imagery on the location of crops and weeds. After classifying the imagery, a herbicide application map will be generated for site-specific weed control. This involves the integration of CNN classification and GIS technology.
My question is this: If you have an orthomosaic image generated from a drone, will images captured from a digital camera on the ground be effective for training a CNN that will classify high-resolution aerial imagery? 
Being new to CNNs, I just didn't know if I had to use aerial imagery to train a CNN to classify aerial imagery, or if a digital camera will work just fine.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'image-recognition', 'training']",
How can a DQN backpropagate its loss?,"
I'm currently trying to take the next step in deep learning. I managed so far to write my own basic feed-forward network in python without any frameworks (just numpy and pandas), so I think I understood the math and intuition behind backpropagation. Now, I'm stuck with deep q-learning. I've tried to get an agent to learn in various environments. But somehow nothing works out. So there has to be something I'm getting wrong. And it seems that I do not understand the critical part right at least that's what I'm thinking. 
The screenshot is from this video.
What I'm trying to draw here is my understanding of the very basic process of a simple DQN. Assuming this is right, how is the loss backpropagated? 
Since only the selected $Q(s, a)$ values (5 and 7) are further processed in the loss function, how is the impact from the other neurons calculated so their weights can be adjusted to better predict the real q-values?
","['deep-learning', 'reinforcement-learning', 'q-learning', 'backpropagation', 'dqn']","It is quite common in DQN to instead of having the neural network represent function $f(s,a) = \hat{q}(s,a,\theta)$ directly, it actually represents $f(s)= [\hat{q}(s,1,\theta), \hat{q}(s,2,\theta), \hat{q}(s,3,\theta) . . . \hat{q}(s,N_a,\theta)]$ where $N_a$ is the maximum action, and the input the current state. That is what is going on here. It is usually done for a performance gain, since calculating all values at once is faster than individually.However, in a Q learning update, you cannot adjust this vector of output values for  actions that you did not take. You can do one of two things:Figure out the gradient due to the one item with a TD error, and propagate that backwards. This involves inserting a known gradient into the normal training update step in a specific place and working from there. This works best if you are implementing your own backpropagation with low-level tools, otherwise it can be a bit fiddly figuring out how to do it in a framework like Keras.Force the gradients of all other items to be zero by setting the target outputs to be whatever the learning network is currently generating.If you are using something like Keras, the second approach is the way to go. A concrete example where you have two networks n_learn and n_target that output arrays of Q values might be like this:For each sample (s, a, r, next_s, done) in your minibatch*Fit the minibatch n_learn.fit(train_X, train_Y)* It is possible to vectorise these calculations for efficiency. I show it as a for loop as it is simpler to describe that way"
How can I make the kernels non-learnable and set them manually?,"
I'm a newbie in Convolutional Neural Networks. I have found out that kernels in convolutional layers are usually learned while training.
Suppose I have a kernel that is very good to extract the features that I want to extract. In that case, I don't want the kernels to be learnable. So, how can I make the kernels non-learnable and set them manually?
Maybe, in that case, I have to use something different from a CNN.
","['convolutional-neural-networks', 'keras', 'image-processing', 'convolution', 'filters']","In most modern neural network frameworks, the update rules for training can be selectively applied to some parameters and not others. How to do that is dependent on the framework. Some will have the concept of ""freezing"" a layer, preventing parameters in it being updated. Keras does this for example. Others will do the opposite and expect you to provide a list of trainable parameters - these typically come with helpers that will list all parameters in a neural network, so you would need to add some kind of filter after collecting that data to exclude your pre-trained layer. PyTorch does this (although the linked example is slightly more complex in that it stops calculating gradients too).If your framework of choice does not allow you to select and isolate layers in the training process, then you still have a couple of options:You could store a copy of layer parameters that you want to keep and force your learning network to re-load these parameters after each mini-batch. This does depend on you having a method that can selectively set parameters.If your pre-trained layers are the first ones, immediately next to the input, then instead of including them in your learning network model, you can pre-process all your training data with just the fixed layers (build a model using only those layers), save the output and use that as an alternative input for the learning layers (build a second model with only the learning layers). Later, once training is complete, you can build a combined neural network out of the fixed layers and the learning layers."
Why are traditional ML models still used over deep neural networks?,"
I'm still on my first steps in the Data Science field. I played with some DL frameworks, like TensorFlow (pure) and Keras (on top) before, and know a little bit of some ""classic machine learning"" algorithms like decision trees, k-nearest neighbors, etc.
For example, image classification problems can be solved with deep learning, but some people also use the SVM.
Why are traditional ML models still used over neural networks, if neural networks seem to be superior to traditional ML models? Keras is rather simple to use, so why don't people just use deep neural networks with Keras? What are the pros and cons of each approach (considering the same problem)?
","['machine-learning', 'deep-learning', 'keras', 'comparison']","Why are still traditional machine learning (ML) models used over neural networks if neural networks seem to be superior to traditional ML models?Of course, the model that achieves state-of-the-art performance depends on the problem, available datasets, etc., so a comprehensive comparison between traditional ML models and deep neural networks is not appropriate for this website, because it requires a lot of time and space. However, there are certain disadvantages of deep neural networks compared to traditional machine learning models, such as k-nearest neighbors, linear regression, logistic regression, naive Bayes, Gaussian processes, support vector machines, hidden Markov models and decision trees.Often, traditional ML models are conceptually simpler (for example, k-NN or linear regression are much simpler than deep neural networks, such as LSTMs).Personally, I've noticed that traditional ML models can be used more easily compared to deep neural networks, given the existence of libraries, like scikit-learn, which really have a simple and intuitive API (even though you apparently do not agree with this).Deep neural networks usually require more data than traditional ML models in order not to overfit. Empirically, I've once observed that certain traditional ML models can achieve comparable performance to deep neural networks in the case of small training datasets.Even though there's already a new and promising area of study called Bayesian deep learning, most deep neural networks do not really provide any uncertainty guarantees, they only provide you a point estimate. This is a big limitation, because, in areas like healthcare, uncertainty measures are required. In those cases, Gaussian processes may be more appropriate."
NoisyNet DQN with default parameters not exploring,"
I implemented a DQN algorithm that plays OpenAIs Cartpole environment. The NN architecture consists of 3 normal linear layers that encode the state, and one noisy linear layer, that predicts the Q value based on the encoded state.
My NoisyLinear layers looks like this:
class NoisyLinear(nn.Module):
  def __init__(self, in_features, out_features):
    super(NoisyLinear, self).__init__()
    self.in_features = in_features
    self.out_features = out_features
    self.sigma_zero = 0.5
    self.weight_mu = torch.empty(out_features, in_features)
    self.weight_sigma = torch.empty(out_features, in_features)
    self.weight_epsilon = torch.empty(out_features, in_features, requires_grad=False)
    self.bias_mu = torch.empty(out_features)
    self.bias_sigma = torch.empty(out_features)
    self.bias_epsilon = torch.empty(out_features, requires_grad=False)
    self.reset_parameters()
    self.reset_noise()

  def reset_parameters(self):
    mu_range = 1 / math.sqrt(self.in_features)
    self.weight_mu.data.uniform_(-mu_range, mu_range)
    self.weight_sigma.data.fill_(self.sigma_zero / math.sqrt(self.in_features))
    self.bias_mu.data.uniform_(-mu_range, mu_range)
    self.bias_sigma.data.fill_(self.sigma_zero / math.sqrt(self.out_features))

  def _scale_noise(self, size):
    x = torch.randn(size)
    return x.sign().mul_(x.abs().sqrt_())

  def reset_noise(self):
    epsilon_in = self._scale_noise(self.in_features)
    epsilon_out = self._scale_noise(self.out_features)
    self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))
    self.bias_epsilon.copy_(epsilon_out)

  def forward(self, input):
    return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon, self.bias_mu + self.bias_sigma * self.bias_epsilon)

However, with the default hyperparameters from the (sigma_0 = 0.5), the agent does not explore at all, and even if I crank it up to sigma_0 = 5, it works way worse than epsilon greedy.
(When I use noisy nets I don't use epsilon greedy).
","['reinforcement-learning', 'dqn']",
What are some ways to quickly evaluate the potential of a given NN architecture?,"
Main question
Is there some way we can leverage general knowledge of how certain hyperparameters affect performance, to very rapidly get some sort of estimate for how good a given architecture could be?
Elaboration
I'm working on a handwritten character recognition problem using CNNs. I want to try out a few different architectures (mostly at random) to iterate towards something which might work. The problem is that one run takes a really long time.
So what's a way to quickly verify if a given architecture is promising? And let me elaborate on what I've tried:

Just try it once. Yeah but maybe I chose some bad hyperparameter combination and actually that architecture was going to be the ground breaker.
Do Bayesian optimisation. That's still really slow. From examples and trials, I've seen that it takes quite some time for convergence. And besides, I'm not trying to optimise yet, I just want to check if there's any potential.

","['neural-networks', 'convolutional-neural-networks', 'hyperparameter-optimization']",
How can I match numbers with expressions?,"
Let's say I have the number 123.45 and the expression one hundred twenty-three and forty-five cents. 
Can I develop AI to identify these two values as a match? If I can, how should I do that?
",['machine-learning'],
Is it normal to see oscillations in tested hyperparameters during bayesian optimisation?,"
I've been trying out bayesian hyperparameter optimisation (with TPE) on a simple CNN applied to the MNIST handwritten digit dataset. I noticed that over iterations of the optimisation loop, the tested parameters appear to oscillate slowly.
Here's the learning rate:

Here's the momentum:

I won't add a graph, but the batch size is also sampled from one of 32, 64, or 128. Also note that I did this with a fixed 10 epochs in each trial.
I understand that we'd expect the trialled parameters to converge gradually towards the optimal, but why the longer term movement of the average?
For context here is the score (1 - accuracy) over iterations

And also for context, here's the architecture of the CNN.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 9, 9, 64)          36928     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 100)               102500    
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1010      
=================================================================

Optimization done with mini-batch gradient descent on the cross entropy.
","['convolutional-neural-networks', 'hyperparameter-optimization', 'bayesian-optimization']",
Suggestion for finding the stable regions in spiral galaxy data?,"
I am working with a data set that consists of the actual pitch angle (given as PA(Y)) and the pitch angle at each radii (listed from 1 to 217). In the image below, you can only see radii 1 through 16. The Mode(Y) in the image below is not of relevance at the moment.

There are regions that range between certain radii in which the pitch angle measurement does not change (in the image, you'll notice this happens for all the radii values, but they do change after a certain radius that's cut off in the image). These are known as stable regions. My goal is to capture all the ranges in the data in which the pitch angle measurement does not change, and create a program that returns those values. 
Is there a machine learning method in which this is possible, or is this just a non-machine learning problem? I have tried creating plots and have considered creating a CNN that can identify these flat regions, but I feel like this is overkill. My PIs want to use a machine learning method and they have proposed neural networks, hence why I tried the CNN, but I just am not sure if that is possible? 
I should add, usually stable regions radii ranges are unknown, so the goal is to try to see if certain radii ranges usually can predict where a stable region are located.
Moreover, I've thought of using a classifier to determine whether a region is flat or not. I am just very confused as to how to approach this. Are there any similar examples to the problem I'm currently working on that someone can point me to? 
","['machine-learning', 'convolutional-neural-networks', 'classification', 'datasets', 'data-preprocessing']",
Why don't the neural networks inside LSTM cells contain hidden layers?,"
I watched a video explaining how LSTM cells have very rudimentary feed-forward neural networks, basically a 2 layer input-output with no hidden layers.
Why don't LSTM cells have more complex neural networks before each gate, i.e. containing 1 hidden layer?
I would think that if you want more advanced gating decisions, that you would use at least 1 hidden layer to have more complex processing.
","['deep-learning', 'long-short-term-memory']",
What is the formula for the momentum and Adam optimisers?,"
In the gradient descent algorithm, the formula to update the weight $w$, which has $g$ as the partial gradient of the loss function with respect to it, is:
$$w\ -= r \times g$$
where $r$ is the learning rate.
What should be the formula for momentum optimizer and Adam (adaptive momentum?) optimizer? Something should be added to the right side of the formula above?
","['machine-learning', 'gradient-descent', 'momentum', 'adam', 'optimizers']","I'm going to use slightly different notation, $\leftarrow$ for an assignment, $\alpha$ for learning rate, $\nabla_w J$ in place of $g$* and implied multiplication as these are slightly more common. Also, using bold letters to represent vectors. In that notation, the update rule for basic gradient descent would be written as:$$\mathbf{w} \leftarrow \mathbf{w} - \alpha \nabla_w J$$This cannot be extended to momentum and Adam update rules whilst keeping it as a single line and modifying the right hand side. That is because these variations of gradient descent maintain running statistics of previous gradient values, which have their own separate update rules. When implemented on a computer, these become additional terms, mainly vectors the same size as the weight vector being updated. These variables also require initialisation before use.Momentum maintains a ""velocity"" term which essentially tracks a recency-weighted average of gradients. However, the classic form of momentum given here does not normalise the resulting vector, and you often have to adjust the learning rate down when using it. Momentum has a parameter $\beta$ which should be between 0 and 1, and typically is set at $0.9$ or higher.Intitialisation$$\mathbf{m} \leftarrow \mathbf{0}$$Update rules$$\mathbf{m} \leftarrow \beta \mathbf{m} + \nabla_w J$$
$$\mathbf{w} \leftarrow \mathbf{w} - \alpha \mathbf{m}$$There are some variants of these update rules in practice. An important one is Nesterov momentum.The Adam optimiser maintains a momentum term, plus a scaling term, and also corrects these terms for initial bias. Adam has three parameters $\beta_m$ for momentum (typically 0.99), $\beta_v$ for scaling (typically 0.999) and $\epsilon$ to avoid divide by zero and numerical stability issues (typically $10^{-6}$). Initialisation$$\rho_m \leftarrow 1$$
$$\rho_v \leftarrow 1$$
$$\mathbf{m} \leftarrow \mathbf{0}$$
$$\mathbf{v} \leftarrow \mathbf{0}$$Update rules$$\rho_m \leftarrow \beta_m \rho_m$$
$$\rho_v \leftarrow \beta_v \rho_v$$
$$\mathbf{m} \leftarrow \beta_m \mathbf{m} + (1-\beta_m) \nabla_w J$$
$$\mathbf{v} \leftarrow \beta_v \mathbf{v} + (1-\beta_v) (\nabla_w J \odot \nabla_w J)$$
$$\mathbf{w} \leftarrow \mathbf{w}- \alpha(\frac{\mathbf{m}}{\sqrt{\mathbf{v}}+\epsilon} \frac{\sqrt{1-\rho_v}}{1-\rho_m})$$The symbol $\odot$ stands for element-wise multiplication. Here that essentially means to square each term of the gradient to calculate terms in $\mathbf{v}$. The square root and division of $\mathbf{m}$ by $\sqrt{\mathbf{v}}+\epsilon$ in the last update step are also handled element-wise.The variant I show here has an ""optimisation"" to the bias correction so that you don't need to calculate high powers of either of the parameters. You may see variants that don't have $\rho_m$ and $\rho_v$ (or equivalents), but instead use $\beta_{m}^t$ or $\beta_{v}^t$ directly, which is exactly what $\rho_m$ and $\rho_v$ represent.* $\nabla_w J$ is the gradient of $J$ with respect to $\mathbf{w}$. By writing it this way, it also describes the goal of the update explicitly within the notation i.e. to minimise a function that is parameterised by $\mathbf{w}$."
Train AI with shapes + drop shadows to remove background colors,"
for a screen printing app, I'd like to remove background colors from images.
There is still a white border around text from anti-aliasing.
Dropshadows also break it.
So, I was thinking I could train an AI by creating images with shapes and text, with and without backgrounds.
The AI input would get a version with a background and the ""goal"" would be the version without the background.
How do I go about doing this? ( total AI noob )
================
Non-AI solution
If anyone is interested... I have made a non AI solution which takes all colors within a tolerance of the background, then looks at the 4x4 neighbors.. from each neighbor(which is a candidate for converting into semi-transparent), it looks at the 3x3 neighbors around the candidate for the furthest color from the removal color ( which typically grabs the solid pixels ), and then converts the current pixel to an alpha version by copying the rgb values, and converting alpha to 255 * (1 - dist_removal_to_current / dist_removal_to_furthest) or something like that.  
I should write an article or something... it was an interesting algorithm to write.  linkedin me Dan Schumann in wisconsin
","['image-processing', 'image-generation']",
How much knowledge of the world is learnt through words?,"
We know a lot of common sense about the world. Things like ""to buy something you need money"".
I wonder how much of this common sense comes about through actual someone explicitly telling you the instructions ""You need money to buy things"". Which we store in our brains as a sort of rule. As opposed to just intutively understanding things and picking it up. 
I am imagining children playing at shop-keeping and saying things like ""I give you this and you give me that"". And other children not quite understanding the concept of buying things until being told by a teacher.
If so, giving a computer a list of common sense rules likes these is no different to teaching a child. So I am wondering why this area of AI research (semantic webs etc.) has been frowned upon in the last decade in favour of trying to learn everything through experience like deep neural networks?
","['natural-language-processing', 'semantics']",
Generating 5 numbers with 1 input before loss function,"
I am trying make an ANN model that takes a constant m (will be changed later but now it is just a constant, let's say 0) as an input and generate 5 non-integer numbers (a1,a2..a5) after some layers like relu, linear,relu ... and then these 5 numbers enter to the loss function layer along with an additional 5 numbers (b1,b2..b5) given by hand directly into the same loss function. In the loss function, S=a1xb1+...+a5xb5 will be calculated and the model should use mean square error with this S and S0, which is given by hand again to tune the 5 numbers generated after the NN layers.
For a dummy like me, this looks like a totally different model than the examples online. I'd really appreciate any guidance here like the model I should use, examples etc. I can't even know where to start even though I believe I understand the generic NN examples that one can find online.
Thanks
",['neural-networks'],
"Given the precision and recall of this model, what can I say about it?","
The following table shows the precision and recall values I obtained for three object detection models.

The goal is to find the best object detection model for that particular data set.
I evaluate the first two models as the following.

Model 1 has a high recall and precision values. High precision relates to a low false-positive rate, and high recall relates to a low false-negative rate. High scores for both show that the model is returning accurate results.

Model 2 has high precision but low recall. This means it returns very few results, but most of its identified objects are correct.


How can I evaluate the third one?
","['object-detection', 'accuracy', 'precision', 'recall']","The second model has the same precision, but worse recall, than model 1. Therefore we would rather have model 1 than model 2.The third model has worse recall than model 1, and worse precision than model 1, therefore we would rather have model 1 than model 3.Thus, model 1 is the best object detection model."
Why isn't there a model playing FPS like CoD or Battlefield already existing?,"
Assuming we had an unlimited time to train a model and a very powerful machine to use our model in real-time (hello quantum computer), I'd like to know why no one could achieve to build an AI able to play a FPS, using ONLY pixels shown on the screen.
Disclaimer: I am not tackling this problem and neither am I planning on doing such a thing, this is pure speculation and curiosity.

I read this great article: playing FPS Games with Deep Reinforcement Learning (2017) (Guillaume Lample, Devendra Singh Chaplot) where they achieve a 4/1 kills/death ratio on Doom against bots. But this is 3 years old now.
Here is a picture of their model:

But they made 2 assumptions that we, humans, do not make when we are playing for the first time to a new game like Call of Duty or Battlefield:

Game feature augmentation. To train a part of their model they used the game engine to know if there is (or not) an enemy in the frame they are processing. We obviously can't do this with CoD or Battlefield, and we, as human, just ""learn"" to recognize an enemy without these informations.
Changing textures of several maps while training to generalize better the model (see 5.2 of the paper linked previously). To summarize, they trained the model with 10 maps changing texture of some elements to make the model generalize better. Then they tested it on 3 unknown maps. In real world (ie in the scenario where we base our training/testing exclusively on the pixels of the screen), we can't train a model with different textures on the same map. And humans are able to play a deathmatch on an unknow map without re-learning everything (detect enemies, move, hide, reloading...). We just need to construct a 3D model of the map in our head to play our best.


Their agent ""divides the problem into two phases: navigation (exploring the map to collect items and find enemies) and action (fighting enemies when they are observed), and uses separate networks for each phase of the game"".
Would it be wise to use more than 2 models? Let's say:

1 CNN to detect enemies
1 model to deal with space features (position/navigation of the agent and the enemies)
1 model to choose the actions given all data previous models have found?

We could train them independently, at the same time.
I think we'd get better result by processing manually some features (using computer vision techniques) like the minimap to get know positions of enemies and number of ammo to feed as input of the last model (action decider).
But there are other problem we'd get: there is a delay between the frame we choose to pull the trigger, the time the bullet hit the enemy and the time the ""reward"" appears on the screen (ex: ""100 points, kill [nameOfLateEnemy]"" appears after the 3th bullet, and if there is ping because we are playing online it may appear 100ms after). How to use reinforcement learning when we don't know exactly which action was the one getting the reward? (we can move the agent while changing the lok directon while pulling the trigger at the same time. It's the combination of these actions that is making the agent kill an enemy).
If the 2 assumptions they made were easy to get rid of, they would be discarded already.
However detecting enemies is basically a simple CNN, and making the navigation network generalize certainly have solutions I can't think of right now but some researchers should have in this 3-year gap between the paper and today.
So why isn't there a model playing CoD or Battlefield better than humans? What am I missing?
","['machine-learning', 'deep-learning', 'game-ai', 'models']",
"If I trained a model to perform semantic segmentation on images with only one object, would it also work on images with multiple objects?","
I'm working on semantic segmentation tasks in the medical space using the U-Net. Let's say that I train a U-Net model on medical images with the goal of segmenting out, say, ligaments, from a medical image. If I train that model on images that contain just a single labelled ligament, it will be able to segment out single ligaments pretty well, I assume. If I present it with an image with multiple ligaments, should it also be able to segment the multiple ligaments well too?
Based on my understanding, semantic segmentation is just pixel-wise classification. As a result, shouldn't the number of the objects in the image not be relevant since it's only looking at individual pixels? So, as long as a pixel matches that of a ligament, it should be able to segment it equally right?
Or am I misunderstanding some piece?
Basically, if I train a U-Net on images with just single ligaments, will it also be able to segment images with multiple ligaments equally as well based on my logic above?
","['computer-vision', 'image-segmentation', 'u-net', 'instance-segmentation', 'semantic-segmentation']",
Why is the average reward plot for my reinforcement learning agent different than the usual plots?,"
I'm building an RL agent using SARSA and Q-Learning for testing its capabilities. 
The environment is a 10x10 grid, where it gets a reward of 1 if he reaches the goal while he takes -1 every time he takes a step out of the grid. So, it can freely move out and every time it takes a step outside of the grid it gets -1. 
After tuning the main parameters 

alpha_val: 0.25 
discount: 0.99 
episode_length: 50 
eps_val: 0.5 

I get the following plot for 10000 episodes (The plot is sampled every 100 episodes): 

But when I look at the plots online I see usually plots like this one:

Since I'm new at RL, I'm asking some comments about my outcome or any type of suggestion if anyone of you think that I'm doing something wrong.
","['machine-learning', 'reinforcement-learning']",
Is Sanskrit still relevant for NLP/AI?,"
I came across a news article from 2018 where the president of India was saying that Sanskrit is the best language for ML/AI. I have no idea regarding his qualification on either AI or Sanskrit to say this but this idea has been floated earlier in the context of NLP. Specifically, Rick Briggs had said so in 1985.
I know elementary Sanskrit and I know NLP. I can understand the point that as a strongly declined language Sanskrit is less ambiguous than say English as the position of words in a sentence are not important. Add to it the fact that words are also declined (that's not the technical term for verbs and I am not sure what is) so that verb gender and number help identify which entity they refer to.
However, that point was valid in 1985. My question is that post the Deep Learning revolution of the last couple of years is it still relevant? Especially given the fact that humans still have to first learn Sanskrit in case NLP is done in Sanskrit.
Of course, as can be guessed from the tone of the question, I am of the opinion that Sanskrit is not relevant for AI now but I wanted to know if someone who works in AI thinks otherwise and if so what is their reason so think so.
",['natural-language-processing'],
How to Layer based Feature extraction?,"
I have read that in deep networks you can engineer each layer for a particular purpose with regards to feature learning. I'm wondering how that is actually done and how it is trained?
In addition doesn't this conflict with the idea of deep-networks having ""automatic"" feature extraction? 
For example consider this: 
Lets say you want to detect stop signs. How would you teach a deep network to do this in a layer-wise fashion? 
People write about one layer of a Deep Network does edge detection, but how? 
","['neural-networks', 'machine-learning', 'deep-learning']",
YOLOv3 Synthetic Data Training,"
Suppose we want to train a model to detect various objects.   Let's say we have training data of those objects in various backgrounds along with their bounding boxes. Basically these objects have been three dimensionally created and the bounding boxes have been drawn on them. Then these have been ""synthetically inserted"" into various blank backgrounds. 
Why would a model trained only on this data do better than a model that has this data along with ""real"" data of these objects with their bounding boxes manually drawn? 
","['convolutional-neural-networks', 'object-recognition', 'object-detection']",
Why am I getting the logarithm of the probability bigger than zero when using Neural Spline Flows?,"
I am using a normalizing flow (Neural Spline Flows) to approximate a probability. After some training, the average loss is around 0.5 (so the logarithm of the probability = -0.5). However, when I am trying it on some new test data, I am getting some values of the logarithm of the probability bigger than zero, which would mean that the probability for that element is bigger than one (which doesn't make sense). 
Does anyone know what could cause this? Isn't the flow supposed to keep all the probabilities below 1 automatically?
","['probability', 'probability-distribution']",
How to deal with random weights initialization in hyperparameters tuning?,"
In the step of tuning my neural networks I often encounter a problem that every time I train the exact same network, it gives me different final error due to random initialization of the weights. Sometimes the differences are small and negligible, sometimes they are significant, depending on the data and architecture.
My problem arises when I want to tune some parameters like number of layers or neurons, because I don't know if the change in final error was caused by recent changes in network's architecture or it is simply effect of the aforementioned randomness. 
My question is how to deal with this issue?
","['neural-networks', 'training', 'randomness']",
How to correctly label images for multi-label classification?,"
I have images that contain lots of elements. Some I know, some I don't. I want to know if it's ok to only label those I do know. Let's take this image for example. I would label the green stuff and the worm but leave the rest unlabeled.
Is that ok?
Another question I would also like to ask is how concise I should be in labeling. For instance, You can see in the picture a bit of blue behind the green plant. So should I label that bit and say water or leave it unlabeled?
EDIT:
I also want to ask if it's ok to label only the things I'm interested in even if they take up to  30% of the picture? Won't the neural network be confused by all the details in the picture that it perceives and that I label as A for example, even if A is just a part of it?
Another question would be, let's say I have labels A, B and C. I have an image in which I'm a bit confused if a certain object is of label B or A or even a totally different class other than (A,B,C). What should I do in this instance?
I'm having a hard time with the dataset. It would take an expert to label this correctly. But I want to do things as cleanly as possible, so all the effort doesn't go to waste. I would really appreciate your help. Thank you guys. 

","['classification', 'datasets']",
What's the mathematical relationship between number of trainable parameters and size of training set?,"
Let's say that I have a pre-trained model where the training set used to pretrain the model is very different from my training set. 
Let's say I unfreeze layers that have X trainable parameters. What size should the training set be with/without data augmentation for multi-class/multi-label image classification with Y number of labels?
","['deep-learning', 'classification']",
"In AlphaZero, which policy is saved in the dataset, and how is the move chosen?","
I've been doing some research on the principles behind AlphaZero.
Especially this cheat sheet (1) and this implementation (2) (in Connect 4) were very useful.
Yet, I still have two important questions:

How is the policy network updated? In (2), board positions are saved in a dataset of tuples (state, policy, value). The value is derived from the result of the self-played game. However, I'm not sure which policy is saved: the number of times that each move has been played, the prior probabilities for each move (I guess not), or something else?

The cheat sheet says that (for competitive play) the move is chosen with the greatest N (=most visited). Wouldn't it be more logical to choose the move with the highest probability calculated by the policy head?


","['reinforcement-learning', 'alphazero', 'self-play']",
What's the difference between semi-supervised VAEs and conditional VAEs?,"
Can someone explain the difference? I'm assuming the difference is just that the neural nets representing the encoder and decoder are trained in a semi-supervised manner in semi-supervised VAE, which in conditional the approximation to the posterior and the posterior's distributions are conditioned on some labels? So, I'm guessing that semi-supervised VAE affects the loss evaluation, whereas, in conditional VAEs, the inference network is conditioned on another label as well?
","['neural-networks', 'deep-learning', 'comparison', 'variational-autoencoder']",
Regularization of non-linear parameters?,"
I was wondering whether it is possible to regularize (L1 or L2) non-linear parameters in a general regression model. Say, I have the following non-linear least squares cost function, where $p$ is a $3d$ vector of fitting parameters:
$cost(p_i) = ( y(x) - sin^{p_1}(x) + p{_2}e^{(p_3*x)} )^2$
In the above cost function, $p_1$ and $p_3$ are non-linear parameters. How should I go about regularizing them? If they were linear, I can just sum them up together with the linear parameters (absolute values or squares) and add as a penalty to the cost function, right? However, I'm not sure if I'm allowed to do so for non-linear parameters.
Has anyone considered this problem?
","['regularization', 'non-linear-regression']",
What is the mathematical definition of an activation function? [duplicate],"







This question already has answers here:
                                
                            




Which functions can be activation functions?

                                (1 answer)
                            


What kind of functions can be used as activation functions? [duplicate]

                                (2 answers)
                            

Closed 2 years ago.



What is the mathematical definition of an activation function to be used in a neural network?
So far I did not find a precise one, summarizing which criterions (e.g. monotonicity, differentiability, etc.) are required.  Any recommendations for literature about this or â€“ even better â€“ the definition itself?
In particular, one major point which is unclear for me is differentiability. In lots of articles, this is required for the activation function, but then, out of nowhere, ReLU (which is not differentiable) is used. I totally understand why we need to be able to take derivatives of it and I also understand why we can use ReLU in practice anyway, but how does one formalize this?
","['neural-networks', 'deep-learning', 'math', 'definitions', 'activation-functions']",
how to convert one structured data to another without specifying structure,"
I have lots of text documents structured as
{
{
    Item1=[
            {a1=1,
             a2=2,
             a3=3},
            {a1=11,
             a2=22,
             a3=33},
            {a1=41,
             a2=52,
             a3=63},
            {a1=19,
             a2=29,
             a3=39}
    ],
    Item2=[
            {a4=1,
             a5=2,
             a6=3},
            {a4=11,
             a5=22,
             a6=33},
            {a4=41,
             a5=52,
             a6=63},
            {a4=19,
             a5=29,
             a6=39}
    ],
}
}

Now this can be formatted into two csv's as

and

I can write regex parser for this but is there a way by which a neural network or deep learning model can be trained for this, which can create these csvs?
The above example has been indented for better visuals, the raw text looks something like
{{Item1=[{a1=1,a2=2,a3=3},{a1=11,a2=22,{a1=41,a2=52,a3=63},{a1=19,a2=29,a3=39}],Item2=[{a4=1,a5=2,a6=3},{a4=11,a5=22,a6=33},{a4=41,a5=52,a6=63},{a4=19,a5=29,a6=39}]}}

","['neural-networks', 'deep-learning', 'pattern-recognition', 'structured-data', 'data-mining']",
What is a cascaded convolutional neural network?,"
For a project I am doing, I found the paper Face Alignment in Full Pose Range: A 3D Total Solution.
It is using a cascaded convolutional neural network, but I wasn't able to find the original paper explaining what that is.
In layman's terms and intuitively, how does a cascaded CNN work? What does it solve? 
","['deep-learning', 'convolutional-neural-networks', 'computer-vision', 'definitions', 'papers']","The paper you are citing is the paper that introduced the cascaded convolution neural network. In fact, in this paper, the authors sayTo realize 3DDFA, we propose to combine two achievements in recent years, namely, Cascaded Regression and the Convolutional Neural Network (CNN). This combination requires the introduction of a new input feature which fulfills the ""cascade manner"" and ""convolution manner"" simultaneously (see Sec. 3.2) and a new cost function which can model the priority of 3DMM parameters (see Sec. 3.4)where 3DDFA stands for 3D Dense Face Alignment, the framework proposed in this paper for face alignment, in which a dense 3D Morphable Model (3DMM) is fitted to the image via cascaded CNNs (the regressor), where the term dense refers to the number of points of the face that will be modeled. See figure 1 of this paper, which should provide some intuition behind the purpose of this framework.In section 3 (page 3), they also sayIn this section, we introduce how to combine Cascaded Regression and CNNs to realize 3DDFA. By applying a CNN as the regressor in Eqn. 1, Cascaded CNN can be formulated as:\begin{align}
\mathbf{p}^{k+1} = \mathbf{p}^{k} + \text{Net}^{k}
(\text{Fea}(\mathbf{I}, \mathbf{p}^k)) \tag{1}\label{1}
\end{align}where The expression cascaded CNN apparently refers to the fact that equation \ref{1} is used iteratively, so there will be multiple CNNs, one for each iteration $k$. In fact, in the paper, they sayUnlike existing CNN methods that apply different network structures for different fitting stages, 3DDFA employs a unified network structure across the cascade. In general, at iteration $k$ ($k = 0, 1, \dots, K$), given an initial parameter $\mathbf{p}^k$, we construct PNCC and PAF with $\mathbf{p}^k$ and train a two-stream CNN $\text{Net}^k$ to conduct fitting. The output features from two streams are merged to predict the parameter update $\Delta \mathbf{p}^k$$$
\Delta \mathbf{p}^k = \text{Net}^k(\text{PAF}(\mathbf{p}^k, \mathbf{I}), \text{PNCC}(\mathbf{p}^k, \mathbf{I}))
$$Afterwards, a better intermediate parameter $\mathbf{p}^{k+1} = \mathbf{p}^k + \Delta \mathbf{p}^k$ becomes the input of the next network $\text{Net}^k$ which has the same structure but different weights with $\text{Net}^k$.In figure 2 of the paper (page 4), the structure of this two-stream CNN, $\text{Net}^k$, at iteration $k$, is shown."
Training an RNN to answer simple quesitons,"
I would like to train an RNN to follow the sentences:
""Would you like some cheese""? with ""Yes, I would like some cheese.""
So whenever the template ""Would you like some ____?"" appears then RNN produces the sequence above. And it should even work on sentences which are new like ""Would you like some blumf?""
I have thought of various ways of doing this. Such as, as well as having 26 outputs for letters of the alphabet have about 20 more for ""repeat the character that is 14 characters to the left"" and so on.
Has this been done before or is there a better way?
","['natural-language-processing', 'recurrent-neural-networks', 'long-short-term-memory']",
Has anyone attempted to take a bunch of similar neural networks to extract general formulae about the focus area? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 3 years ago.







                        Improve this question
                    



When a neural network learns something from a data set, we are left with a bunch of weights which represent some approximation of knowledge about the world.  Although different data sets or even different runs of the same NN might yield completely different sets of weights, the resulting equations must be mathematically similar (linear combinations, rotations, etc.).  Since we usually build NNs to model a particular concrete task (identify cats, pedestrians, tumors, etc.), it seems that we are generally satisfied to let the network continue to act as a black box.
Now, I understand that there is a push for ""understandability"" of NNs, other ML techniques, etc.  But this is not quite what I'm getting at.  It seems to me that given a bunch of data points recording the behavior of charged particles, one could effectively recover Maxwell's laws using a sufficiently advanced NN.  Perhaps that requires NNs which are much more sophisticated than what we have today.  But it illustrates the thing I am interested in: NNs could, in my mind, be teaching us general truths about the world if we took the time to analyze and simplify the formulae that they give us1.
For instance, there must be hundreds, if not thousands of NNs which have been trained on visual recognition tasks that end up learning many of the same sub-skills, to put it a bit anthropomorphically.  I recently read about gauge CNNs, but this goes the other way: we start with what we know and then bake it into the network.
Has anyone attempted to go the opposite way?  Either:

Take a bunch of similar NNs and analyze what they have in common to extract general formulae about the focus area2
Carefully inspect the structure of a well-trained NN to directly extract the ""Maxwell's equations"" which might be hiding in them?

1 Imagine if we built a NN to learn Newtonian mechanics just to compute a simple ballistic trajectory.  It could surely be done, but would also be a massive waste of resources.  We have nice, neat equations for ballistic motion, given to us by the ""original neural networks"", so we use those.
2 E.g., surely the set of all visual NNs have collectively discovered near-optimal algorithms for edge/line/orientation detection, etc.).  This could perhaps be done with the assistance of a meta-ML algorithm (like, clustering over NN weight matrices?).
","['neural-networks', 'deep-learning', 'explainable-ai']",
Optimal RL function approximation for TicTacToe game,"
I modeled the TicTacToe game as a RL problem - with an environment and an agent.
At first I made an ""Exact"" agent - using the SARSA algorithm, I saved every unique state, and chose the best (available) action given that state. I made 2 agents learn by competing against each other.
The agents learned fast - it took only 30k games for them to reach a tie stand-off. And the agent clearly knew how to play the game.
I then tried to use function approximation instead of saving the exact state. My function was a FF-NN. My 1st (working) architecture was 9 (inputs) x 36 x 36 x 9 (actions). I used the semi-gradient 1-step SARSA. The agents took much longer time to learn. After about 50k games they were still less good than the exact agent. I then made a stand off between the Exact and the NN agent - the exact agent won 1721 games from 10k, the rest were tied. Which is not bad. 
I then tried reducing the number of units in the hidden layers to 12, but didn't get good results (even after playing for 500k+ games total, tweaking stuff). I also tried playing with convolution architectures, again - not getting anywhere. 
I am wondering if there's some optimal function approximation solution that can get as-good of results as the exact agent. TicTacToe doesn't seem like such a hard problem for me. 
Conceptually I think there should be much less complexity involved in solving it then can be expressed in a 9x36x36x9 network. Am I wrong, and it's just an illusion of simplicity? Or are there better ways? Maybe modeling the problem differently? 
","['reinforcement-learning', 'deep-rl', 'sarsa']","I think you can break this problem down into two parts to try and find the solution.Take the tabular function you have learned in the exact agent, and treat it as training data for the neural network model, using the same loss function and other hyperparameters as you intend to use when the NN is being used online inside the RL inner loops.You can answer two related questions with this:Does the loss reduce down to a low value after a suitable number of epochs? If so, then the NN has capacity to learn and can learn fast enough. If not, you need to look at the hyperparameters of the NN.Does the trained NN play well against the exact agent? Ideally it plays the same, but it is possible that even though the loss is low, one or two key values in the function are compromised, meaning it still loses. I am not entirely sure what you would do in this case, but either try changing the hyperparameters to increase the capacity of the NN, or try augmenting the data so that there are more examples of the ""difficult"" action values to learn, to see if the issue is something that can be solved in learning.Probably you will find your NN architecture is good, or only requires minor changes to become useful. The more likely issues are in next section.It is quite hard to get this right. Bootstrapping value based methods can easily become unstable if converted naively from tabular to function approximation approaches. Some variants are moderately stable - most stable would probably be Monte Carlo approaches.If you don't want to use Monte Carlo control, then the answer here would be to take ideas from DQN used originally to play Atari games:Don't learn online. Store transitions in an experience replay table - store $(s, a, r, s', done)$ tuples where $done$ is true if $s'$ is a terminal state - and sample a minibatch from it on every step. Reconstruct the bootstrap estimates of value functions to train from each time you sample, don't store and re-use the estimate from the time the action was taken.Optionally use two value estimators - the current learning one, used to select plays and which is updated on each step, and a ""target"" one used to calculate TD targets. Update the target network by cloning the learning network every N steps (e.g. every 100 games).To avoid figuring out hyperparameters for SARSA epsilon decay, I suggest use one-step Q learning. Also one issue you may be facing with SARSA combined epsilon decay is ""catastrophic forgetting"" where the agent gets good, starts to train itself only on play examples by good players, and forgets the values of states that it has not seen in the training data for many time steps. With Q learning you can avoid that by having a relatively high minimum epsilon e.g. 0.1. In fact with TicTacToe learning through self-play, you can get away with $\epsilon = 1$ and Q learning should still work - i.e. it can learn optimal play by observing random self-play. This should apply equally tabular and function approximation agents. It doesn't scale to more complex games where random play would take too long to discover optimal strategies."
Can training a model on a dataset composed by real images and drawings hurt the training process of a real-world application model?,"
I'm training a multi-label classifier that's supposed to be tested on underwater images. I'm wondering if feeding the model drawings of a certain class plus real images can affect the results badly. Was there a study on this? Or are there any past experiences anyone could share to help?
","['deep-learning', 'classification']",
Is there a mathematical formula that describes the learning curve in neural networks?,"
In training a neural network, you often see the curve showing how fast the neural network is getting better. It usually grows very fast then slows down to almost horizontal.
Is there a mathematical formula that matches these curves?

Some similar curves are:
$$y=1-e^{-x}$$
$$y=\frac{x}{1+x}$$
$$y=\tanh(x)$$
$$y=1+x-\sqrt{1+x^2}$$
Is there a theoretical reason for this shape?
","['neural-networks', 'machine-learning', 'math', 'learning-curve']",
Recognizing Set CARDs,"
Set is a card game and is Nicely described here. 
Each set-card has 4 properties: 

The number(1,2 or 3)
the color (Red, Green or Purple)
Fill (Full, Stripes, None)
Form (Wave, Oval or Diamond)


converts to 2 Purple Waves No fill (code: 2PWN)
 and 
convert to codes 1RON and 3GDN
For every combination there is one card so in total there are 3^4 = 81 cards. The goal is to identify 3 cards (set) out of collection of 12 displayed randomly chosen set cards where all properties occur 0,1 or 3 times.
As a hobby project I want to create an android app which can -with the camera- capture the 12 (less or more) set cards and indicate the sets present in the collection of 12. I'm looking for ways to leverage image recognition as efficient as possible.
I've been thinking of taking multiple pictures of all the individual cards, label them and feeding them to a trainer (firebase ML KIT AutoML Vision Edge) But I have the feeling that this a bit of brute force and takes a lot of time and effort photographing and labeling. I could also take pictures of multiple set cards and provide the different codes as labels.
What would be the best (most efficient) approach to have a model for labelling all cards?
",['image-recognition'],
Extract product information from email receipt HTML [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am trying to extract product information from email receipts HTML.  Most services I have found focus on OCR from paper receipts or PDFs.  I would imagine that extraction of product information would be easier from structured HTML.  What type of AI approach would be used to support this?
","['neural-networks', 'natural-language-processing', 'feature-extraction']",
Is there a way to parallelize GloVe cooccur function?,"
I would like to create a GloVe word embedding on a very large corpus (trillions of words). However, creating the co-occurence matrix with the GloVe cooccur script is projected to take weeks. Is there any way to parallelize the process of creating a co-occurence matrix, either using GloVe or another resource that is out there? 
","['word-embedding', 'glove']",
Effect of batch size and number of GPUs on model accuracy,"
I have a data set that was split using a fixed random seed and I am going to use 80% of the data for training and the rest for validation.
Here are my GPU and batch size configurations

use 64 batch size with one GTX 1080Ti
use 128 batch size with two GTX 1080Ti
use 256 batch size with four GTX 1080Ti

All other hyper-parameters such as lr, opt, loss, etc., are fixed. Notice the linearity between the batch size and the number of GPUs.
Will I get the same accuracy for those three experiments? Why and why not?
","['deep-learning', 'keras', 'accuracy', 'gpu', 'batch-size']","This should make a difference, but how big is the difference heavily depends on your task. However, generally speaking, a smaller batch size will have a lower speed if counted in sample/minutes, but have a higher speed in batch/minutes. If the batch size is too small, the batch/minute will be very low and therefore decreasing training speed severely. However, a batch size too small (for example 1) will make the model hard to generalize and also slower to converge.This slide (source) is a great demonstration of how batch size affects training.As you can see from the diagram, when you have a small batch size, the route to convergence will be ragged and not direct. This is because the model may train on an outlier and have its performance decrease before fitting again. Of course, this is an edge case and you would never train a model with 1 batch size.On the other hand, with a batch size too large, your model will take too long per iteration. With at least a decent batch size (like 16+) the number of iterations needed to train the model is similar, so a larger batch size is not going to help a lot. The performance is not going to vary a lot.In your case, the accuracy will make a difference but only minimally. Whilst writing this answer, I have run a few tests on batch size effect on performance and time, and here are the results. (Results to be added for 1 batch size)You can test out yourself in this Google Colab.As you can see, the accuracy increases while the batch size decreases. This is because a higher batch size means it will be trained on fewer iterations. 2x batch size = half the iterations, so this is expected. The time required has risen exponentially, but the batch size of 32 or below doesn't seem to make a large difference in the time taken. The accuracy seems to be normal as half the iterations are trained with double the batch size.In your case, I would actually recommend you stick with 64 batch size even for 4 GPU. In the case of multiple GPUs, the rule of thumb will be using at least 16 (or so) batch size per GPU, given that, if you are using 4 or 8 batch size, the GPU cannot be completely utilized to train the model.For multiple GPU, there might be a slight difference due to precision error. Please, see here.The batch size doesn't matter to performance too much, as long as you set a reasonable batch size (16+) and keep the iterations not epochs the same. However, training time will be affected. For multi-GPU, you should use the minimum batch size for each GPU that will utilize 100% of the GPU to train. 16 per GPU is quite good."
How can I do hyperparameter optimization for a CNN-LSTM neural network?,"
I have built a CNN-LSTM neural network with 2 inputs and 2 outputs in Keras. I trained the network with model.fit_generator() (and not model.fit()), to load just parts of the training data when needed, because the training data is too large to load at once.  
After the training the model was not working. So I checked training data (before and after augmentation). The training data are correct. So I thought the reason why the model does not work must be that I have not found the optimal hyperparameters yet. 
But how can I do hyperparameter optimization on a network with multiple inputs and outputs and trained with model.fit_generator()? All I can find online is hyperparameter optimization of networks with a single input and single output and trained with model.fit(). 
","['deep-learning', 'convolutional-neural-networks', 'long-short-term-memory', 'hyperparameter-optimization', 'hyper-parameters']",
Autoencoder produces repeated artifacts after convergence,"
As experiment, I have tried using an autoencoder to encode height data from the alps, however the decoded image is very pixellated after training for several hours as show in the image below. This repeating patter is larger than the final kernel size, so I would think it would possible to remove these repeating patterns from the image to some extent.
The image is (1, 512, 512) and is sampled down to (16, 32, 32). This is done with pytorch. Here is the relevant sample of the code in which the exact layers are shown.
        self.encoder = nn.Sequential(
                # Input is (N, 1, 512, 512)
                nn.Conv2d(1, 16, 3, padding=1), # Shape (N, 16, 512, 512)
                nn.Tanh(),
                nn.MaxPool2d(2, stride=2), # Shape (N, 16, 256, 256)
                nn.Conv2d(16, 32, 3, padding=1), # Shape (N, 32, 256, 256)
                nn.Tanh(),
                nn.MaxPool2d(2, stride=2), # Shape (N, 32, 128, 128)
                nn.Conv2d(32, 32, 3, padding=1), # Shape (N, 32, 128, 128)
                nn.Tanh(),
                nn.MaxPool2d(2, stride=2), # Shape (N, 32, 64, 64)
                nn.Conv2d(32, 16, 3, padding=1), # Shape (N, 16, 64, 64)
                nn.Tanh(),
                nn.MaxPool2d(2, stride=2) # Shape (N, 16, 32, 32)
            )
        self.decoder = nn.Sequential(
                # Transpose convolution operator
                nn.ConvTranspose2d(16, 32, 4, stride=2, padding=1), # Shape (N, 32, 64, 64)
                nn.Tanh(),
                nn.ConvTranspose2d(32, 32, 4, stride=2, padding=1), # Shape (N, 32, 128, 128)
                nn.Tanh(),
                nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1), # Shape (N, 32, 256 256)
                nn.Tanh(),
                nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1), # Shape (N, 32, 512, 512)  
                nn.ReLU()
            )


Relevant image: left side original, right side result from autoencoder
So could these pixellated effects in the above image be resolved?
","['convolutional-neural-networks', 'autoencoders', 'image-processing']",
How can I extract the reason of the legal compensation from a court report?,"
I'm working on a project (court-related). At a certain point, I have to extract the reason of the legal compensation. For instance, let's take these sentences (from a court report)

Order mister X to pay EUR 5000 for compensation for unpaid wages 

and

To cover damages, mister X must pay EUR 4000 to mister Y 

I want to make an algorithm that is able from this sentence to extract the motive of legal compensation. For the first sentence 

Order mister X to pay EUR 5000 for compensation for unpaid wages 

the algorithm's output must be ""compensation for unpaid wages"" or ""compensation unpaid wages "". 
For the second sentence, the algorithm's output must be ""cover damages"". Output can be a string or a list of string, it doesn't matter.
As I'm not an NLP expert (but I have already worked on a project on sentiment analysis, so I know some stuff about NLP), and there are so many articles, I don't know where to start.
I'm working on French texts, but I can get away with working on English texts.
","['neural-networks', 'deep-learning', 'natural-language-processing']",
How exactly does MICE imputation combine multiple datasets into one?,"
I'm trying to understand Multiple Imputation with Chained Equation (MICE) imputation process (a statistical method for imputing missing data). I have read some articles and I have understood how the imputation happens, but I didn't get the ""pooling"" step.
After analyzing the resulting datasets with Rubin's rules, how to pool these datasets? How to get only one dataset?
In the end, do I combine all these datasets? If yes, how? Or do I compare every dataset's estimators with Rubin's estimators and choose one dataset?
","['machine-learning', 'datasets']",
What is the difference between Sutton's and Levine's REINFORCE algorithm?,"
I followed the videos/slides of Berkley RL course, but now I am a bit confused when implementing it. Please see the picture below. 

In particular, what does $i$ represent in the REINFORCE algorithm? If $\tau^i$ is the trajectory for the whole episode $i$, then why don't we average across the episodes $\frac{1}{N}$, which approximates the gradient of the objective function? Instead, it is a sum over the $i$. So, do we update the gradients per episode or have batches of episodes to update it? 
When I compare the algorithm to Sutton's book as shown below, I see that there we update the gradients per episode. 

But wouldn't it then contradict the derivation on the Levine's slide that the gradient of the objective function $J$ is the expectation (therefore sampling) of the gradients of the logs? 
Secondly, why do we have a cumulative sum of the returns over $T$ in Sutton's version but do not do it in Levine's (instead, all returns are summed together)   
","['reinforcement-learning', 'comparison', 'policy-gradients', 'reinforce']","About the first question, you are right. The $i$ denotes a sample trajectory corresponding to a whole episode. However, Sutton's version is exactly the same one as Levine's if you choose $N=1$. About the second question, the Policy Gradient theorem only tells you what is the gradient up to a constant, so basically any constant is irrelevant. Now, even if you do know the constant, you are going to multiply the gradients by an arbitrary learning rate $\alpha$. So, you can think that the factor $\frac{1}{N}$ is actually already considered ""inside"" $\alpha$.  "
How can I develop a genetic algorithm with a constraint on the sum of alleles?,"
I'm working on a genetic algorithm with a constraint on the sum of the alleles, e.g. if we use regular binary coding and a chromosome is 5-bits long I'd like to constrain it so that the sum of the bits has to be 3 or less (011100 is valid but 011110 is not). Moreover, the fitness function is such that invalid chromosomes cannot be evaluated.
Any ideas on how this problem could be approached? 
I've started looking into the direction of messy GAs (since those can be over-specified) but I'm not sure if there's anything there.
","['genetic-algorithms', 'genetic-programming', 'constraint-satisfaction-problems']",
Does summing up word vectors destroy their meaning?,"
For example, I have a paragraph that I want to classify in a binary manner. But because the inputs have to have a fixed length, I need to ensure that every paragraph is represented by a uniform quantity.
One thing I've done is taken every word in the paragraph, vectorized it using GloVe word2vec, and then summed up all of the vectors to create a ""paragraph"" vector, which I've then fed in as an input for my model. In doing so, have I destroyed any meaning the words might have possessed?
Considering these two sentences would have the same vector:

My dog bit Dave


Dave bit my dog

How do I get around this? Am I approaching this wrong?
What other way can I train my model? If I take every word and feed that into my model, how do I know how many words I should take? How do I input these words? In the form of a 2D array, where each word vector is a column?
I want to be able to train a model that can classify text accurately.
Surprisingly, I'm getting a high (>90%) for a relatively simple model like RandomForestClassifier just by using this summing up method. Any insights?
","['deep-learning', 'natural-language-processing', 'word-embedding', 'text-classification', 'binary-classification']",
Is this a problem well suited for machine learning?,"
The light field of a certain scene is the set of all light rays that travel through the volume of that scene at a specific point in time. A light field camera, for example, captures and stores a subset of the light field of a scene.
I've got an unstructured subsampling of such a scene (a few billion rays, each having a direction and light intensity information).
What I wish to do now is to create an approximation of the original scene that created this light field, with the approximation consisting of 3 arbitrarily positioned (alpha-)textured 2D planes (in 3D space), where each point on the surface radiates light towards uniformly in all directions based on the pixel color at this position.
So, I guess, this is like finding regions in the volume where similarly 'colored' rays intersect, such that the planes maximize the number of intersections they can cover.
So, the available data is the few billions of rays, the desired output is the parameters(position, normal and size) of the three planes plus one RGBA texture for each.
I'm asking here about experiences and opinions: Is this problem rather well-suited for a machine learning approach or rather not?
Edit:
A classical algorithm I could think of to solve this would be to voxelize the volume and use pathtracing to add a color sample for each ray to all cells along its way, then give each cell some value based on how similar all its contained samples are and then search for planar surfaces that intersect as many high rated cells as possible.
But maybe machine learning is better suited for such a problem?
","['machine-learning', 'algorithm-request']",
"Given enough graphical data, could you train an AI to plot a polynomial graph based on the input conditions?","
Good day everyone.
I am curious if it is possible for an AI to plot a time-series graph based on a single input.
Using free fall impact as an example.
Assuming we drop a ball from height 100m and record the force it receives relative to time. 
We will get a graph that looks something like below.

Now we drop the ball from a height of 120m, record the forces, and we get another graph in addition to our original.

What I am wondering is:
If we have a large set of data on 60m to 140m (20m interval) height drops, would we be able to generate a regression model that plots the responses when given an arbitrary drop height? (i.e plot force response when dropped from 105m)
Thank you all very much for your time and attention.
",['machine-learning'],"Yes this is possible, using any machine learning approach that supports regression. You have two main approaches:Input $h$ the height of the drop, multiple outputs, one per time offset that you want to plot. Each individual output calculates the predicted force at a specific offset time.Inputs $h$ the height of the drop and $t$ a time offset, one output. The single output calculates the predicted force due to given height and at given time.The main thing to bear in mind is that statistical learning techniques typically do not generate physics-like models. Test inputs close to training examples should generate reasonable graphs that interpolate between those from training data. Test inputs far away from the training examples (e.g. you train on data of drops from 60m to 140m, but use an input of 10m or 200m) will likely generate wildly incorrect outputs. The main exception to this is if your ML model includes some good guesses at the underlying physics model, in which case it is possible that a regression algorithm will tune the parameters of that model plus filter out terms that should not be part of the model, resulting in a system that extrapolates much better. That is very unlikely happen by chance, it requires up-front design."
What is the neuron-level math behind backpropagation for a neural network?,"
I am quite new in the AI field. I am trying to create a neural network, in a language (Dart) where I couldn't find examples or premade libraries or tutorials. I've tried looking online for a strictly ""vanilla"" python implementation (without third-party libraries), but I couldn't find any. 
I've found a single layer implementation, but it's done only with matrices and it's quite cryptic for a beginner.
I've understood the idea between the feed forwarding, a neuron calculates the sum of its inputs, adds a bias and activates it. 
But I couldn't find anything a neuron-level explanation of the math behind backpropagation. (By neuron-level I think of the math down to the single neuron as a sequence of operations instead of multiple neurons treated as matrices). 
What is the math behind it? Are there any resources to learn it that are suitable as a beginner?
","['backpropagation', 'math', 'resource-request']","Backpropagation is actually a lot easier than it is made out to be - if you have a basic understanding of calculus and the chain rule, and the single multi-variable calculus rule that to combine 2 gradient vectors, you simply add them.This is hands down the best walk through of back prop I've found on the internet. If you are still confused after that, feel free to ask me any further questions. Here is also a quick forward and backward pass example I made for a simple CNN (only a few layers though, and the gradient only goes back to channel 1 of filter 1)"
Creating an AI than can learn to give instructions,"
So we think a computer is dumb because it can only follow instructions. Therefor I am trying to create an AI that can give instructions.
The idea is this: Create a geometric scene (A) then make a change in scene such as turning a square red or moving a circle right one unit. This becomes the new scene B. Then the computer compares the scenes A and B and it's goal is to give the shortest possible instruction that will change scene A to scene B. Examples might be:
""Turn the green square red"".

or
""Move the yellow square down"".

Or when we get more advanced we might have:
""Move the green square below the leftmost purple square down.""

Equally, this task could be seen as finding a description of the change. e.g. ""The green square has turned red"".
The way it would work is that there'd be a simplified English parser, and the computer would generate a number of phrases and check whether these achieved the desired result.
I would probably give it some prior knowledge of things like colours, shape-names, and so on. Or it could learn these by example.
Eventally I would hope it to generate more complicated loop type expressions such as ""Move the square left until it reaches the purple circle."" And these would essentially be little algorithms the AI has generated in words.
I've got some ideas how to do this. But do you know any similar projects that I could look at? If you were implementing this, how would you go about it?
[In other words we have an English parser that is interpreted to change a scene A into a scene B. But we want the AI to learn, given scenes A and B, how to generate instructions.]
","['natural-language-processing', 'generative-model', 'geometric-deep-learning']",
What does the notation sup dist mean in distributional RL?,"
I'm trying to understand distributional RL, based on this article. In one of the equations, there is a symbol $\operatorname{sup dist}$. 
\begin{align}
\operatorname{sup dist}_{s, a} (R(s, a) + \gamma Z(s', a^*), Z(s, a)) \\
s' \sim p(\cdot \mid s, a)
\end{align}
What does $\operatorname{sup dist}$ mean? 
","['reinforcement-learning', 'math', 'notation']","It doesn't seem that it is a ""proper"" symbol. I guess that $\sup$ simply refers to the supremum, that is, you want to select actions that maximize the quantity that comes to the right of $\sup$, while $\text{dist}$ is simply a proxy for any possible distance between distributions. For example, you can replace $\text{dist}$ with the Kullback-Leibler divergence or with the mutual information."
Rendering images and voxelizing the images,"
I am using the shapenet dataset. From this dataset, I have 3d models in .obj format. I rendered the images of these 3d models using pyrender library which gives me an image like this :

Now I am using raycasting to voxelize this image. The voxel model I get is something like below :

I am not able to understand why I am getting the white or light brown colored artifacts in the boundary of the object.
The reason I could come up with was maybe the pixels at the boundary of the object contain two colors, so when I traverse the image as numpy array, I get an average of these two colors which gives me these artifacts. But I am not sure if this is the correct reason.
If anyone has any idea about what could be the reason, please let me know 
","['computer-vision', 'image-processing']",
How to formalize learning in terms of information theory?,"
Consider the following game on a MNIST dataset:

There are 60000 images. 
You can pick any 1000 images and train your Neural Network without access to the rest of images. 
Your final result is prediction accuracy on all dataset.

How to formalize this process in terms of information theory? I know that information theory works with distributions, but maybe you can provide some hints how to think in terms of datasets instead of distributions.

What is the information size of all datasets. My first idea was
that each image is iid from uniform distribution and information
content = -log2(1/60000). But common sense and empirical results
(training neural network) show that there are similar images and
very different images holding a lot more information. For example if
you train NN only on good looking images of 1 you will get bad
results on unusual 1s.
How to formalize that the right strategy is to choose as much as possible different 1000 images. I was thinking to take image by image with the highest entropy relative to the images you already have. How to define distance function.
How to show that all dataset contains N bits of information, training dataset contain M bits of information and there is a way to choose K images < 60000 that hold >99.9% of information.

","['deep-learning', 'convolutional-neural-networks', 'cross-entropy']",
Is the Q value updated at every episode?,"
I trying to understand the Bellman equation for updating the Q table values. The concept of initially updating the value is clear to me. What is unclear is the subsequent updates to the value. Is the value replaced with each episode?  It doesn't seem like this would learn from the past.  Maybe average the value from the previous episode with the existing value?
Not specifically from the book. I'm using the equation
$$V(s) = \max_a(R(s, a) + \gamma V(s')),$$
where $\gamma$ is the learning rate. $0.9$ will encourage exploration, and $0.99$ will encourage exploitation.  I'm working with a simple $3 \times 4$ matrix from YouTube
","['reinforcement-learning', 'q-learning']","I think you are a bit confused about what is the update function and the target.The equation you have there, and what is done in the video is the estimation of the true value of a certain state. In Temporal-Difference algorithms this is called the TD-Target.The reason for your confusion might be that in the video he starts from the end state and goes backwards using that formula to get the final value of each state. But that is not how you update the values, that is where you want to get to at the end of iterating through the states.The update formula may have several forms depending on the algorithm. For TD(0), which is  a simple 1-step look ahead off-policy where what is being evaluated is the state (as in your case), the update function is:$$
V(s) = (1 - \alpha) * V(s) + \alpha * (R(s,a) + \gamma V(s')),
$$
where alpha is the learning rate. What alpha will do is balance how much of your current estimate you want to change. You keep $1 - \alpha$ of the original value and add $\alpha$ times the td-target, which uses the reward for the current state plus the discounted estimate of the value of the next state. Normal values for alpha can be 0.1 to 0.3, for example.The estimate will slowly converge into the real value of the state which is given by your equation:
$$
V(s) = \max_a(R(s, a) + \gamma V(s')).
$$Also, the $\gamma$ is actually the discount associated with future states, as it is said in the video you referenced. It basically says how much importance you give to future states rewards. If $\gamma = 0$, then you only care about the reward in your current state to evaluate it (this is not what is used). On the other extreme if $\gamma = 1$ you will give as much value for a reward received in a state 5 steps ahead as you will to the current state. If you use some intermediate value you will give some importance to future rewards, but not as much as for the present one. The decay on the reward received on a state $n steps$ in the future is given by $\gamma^n$.Another thing that I would correct is that the exploration - exploitation balance is not in any way related to $\gamma$. It is normally balanced by some policy, for example $\epsilon - greedy$. This one for example says that a certain % of the actions you take are random, which in turn makes you explore less valued states."
How are small scale features represented in an Inverse Graphics Network (autoencoder)?,"

This post refers to Fig. 1 of a paper by Microsoft on their Deep Convolutional Inverse Graphics Network: 
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/kwkt_nips2015.pdf
Having read the paper, I understand in general terms how the network functions. However, one detail has been bothering me: how does the network decoder (or ""Renderer"") generate small scale features in the correct location as defined by the graphics code? For example, when training the dataset on faces, one might train a single parameter in the graphics code to control the (x,y) location of a small freckle. Since this feature is small, it will be ""rendered"" by the last convolutional layer where the associated kernels are small. What I don't understand is how the information of the location of the freckle (in the graphics code) propagates through to the last layer, when there are many larger-scale unpooling + convolutional layers in-between.
Thanks for the help!
","['convolutional-neural-networks', 'autoencoders', 'features']",
Can we calculate mean recall and precision,"
I'm evaluating the accuracy in detecting objects for my image data set using three deep learning algorithms. I have selected a sample of 30 images. To measure the accuracy, I manually count the number of objects in each image and then calculate recall and precision values for three algorithms. Following is a sample:

Finally to select the best model for my data set, can I calculate the mean Recall and mean Accuracy? For Example:

","['deep-learning', 'accuracy']","For the precision metric for example you have:$$
 Precision = \frac{TP}{TP+FP},
$$
with TP = True Positive and FP = False Positive.Imagine you have the following values:
Image 1: $TP = 2, FP = 3$
Image 2: $TP = 1, FP = 4$
Image 3: $TP = 3, FP = 0$ The precision scores as you calculated will be:
Image 1: $2/5$
Image 2: $1/5$
Image 3: $1$
Your average will be: $0.533$On the other hand if you sum them all up and then calculate the precision value you get:$P = \frac{6}{6+7} = 0.462$This proves that averaging the precision scores is not the same as calculating the total precision in one go.Since what you want is to know how precise your algorithm is, independently of the precision for each image, you should sum all the TP and FP and only then calculate the precision for each model. This way you will not have a biased average. The average would give the same weight to an image with a larger number of objects as to another image which had fewer objects."
Why does my model overfit on pseudo-random numbers training data?,"
I am trying to predict pseudo-random numbers using the past numbers with a multiplayer perceptron. The error while training is very low. However, as soon as I test it with a test set, the model overfits and returns very bad results. The correlation coefficient and error metrics are both not performing well.
What would be some of the ways to solve this issue?
For example, if I train it with 5000 rows of data and test it with 1000, I get:
Correlation coefficient                  0.0742
Mean absolute error                      0.742 
Root mean squared error                  0.9407
Relative absolute error                146.2462 %
Root relative squared error            160.1116 %
Total Number of Instances             1000     

As mentioned, I can train it with as many training samples as I want and still have the model overfits. If anyone is interested, I can provide/generate some data and post it online.
","['neural-networks', 'training', 'overfitting', 'multilayer-perceptrons']","Simply said, predicting pseudo random number is just not possible for now. Pseudo random numbers generated now have a high enough ""randomness"" so that it cannot be predicted. Pseudo random numbers is the basis of modern cryptography which is widely used in the world wide web and more. It may be possible in the future through faster computers and stronger AI, but for now it is not possible. If you train a model to fit on pseudo random numbers, the model will just overfit and thus creating a scenario as shown in the question. The training loss will be very low while test loss will be extremely high. The model will just ""remember"" the training data instead of generalising to all pseudo random numbers, thus the high test loss.Also, as a side note, loss is not represented by %, instead it is just a raw numeric value.See this stack exchange answer for details."
How to back propagate for implementation of Sequence-to-Sequence with Multi Decoders,"
I am proposing a modified version of Sequence-to-Sequence model with dual decoders. The problem that I am trying to solve is Neural Machine Translation into two languages at once. This is the simplified illustration for the model.
                            /--> Decoder 1 -> Language Output 1
Language Input -> Encoder -|
                            \--> Decoder 2 -> Language Output 2

What I understand about back propagation is that we are adjusting the weights of the network to enhance the signal of the targeted output. However, it is not clear to me on how to back propagate in this network because I am not able to find similar implementations online yet. I am thinking of doing the back propagation twice after each training batch, like this:
$$ Decoder\ 1 \rightarrow Encoder $$
$$ Decoder\ 2 \rightarrow Encoder $$
But I am not sure whether the effect of back propagation from Decoder 2 will affect the accuracy of prediction by Decoder 1. Is this true?
In addition, is this structure feasible? If so, how do I properly back propagate in the network?
","['recurrent-neural-networks', 'machine-translation']",
Deduce properties of the loss functions from the training loss curves,"
I have two convex, smooth loss functions to minimise. During the training (a very simple model) using batch SGD (with tuned optimal learning rate for each loss function), I observe that the (log) loss curve of the loss 2 converges much faster and is much more smooth than that of the loss 2, as shown in the figure.
What can I say more about the properties of the two loss functions, for example in terms of smoothness, convexity, etc?

","['training', 'objective-functions', 'loss']",
How can a system recognize if two strings have the same or similar meaning?,"
How can a system recognize if two strings have the same or similar meaning?
For example, consider the following two strings

Wikipedia provides good information.
Wikipedia is a good source of information.

What methods are available to do this?
","['neural-networks', 'deep-learning', 'text-classification']",
Calculation of FPS on object detection task,"
How to calculate mean speed in FPS for an object detection model like YOLOv3 or YOLOv3-Tiny? Different object detection models are often presented on charts like this:

I am using the DarkNet framework in my project and I want to create similar charts for my own models based on YOLOv3. Is there some easy way to get mean FPS speed for my model with the ""test video""?
","['convolutional-neural-networks', 'object-detection', 'yolo']","@Clement HuiThanks for your answer, I ask AlexeyAB from Darknet the same question and he add now flag for Darknet for this type of model speed measurments:
https://github.com/AlexeyAB/darknet/issues/4627I added -benchmark flag for detector demo, now you can use command 2652263./darknet detector demo obj.data yolo.cfg yolo.weights test.mp4 -benchmarkBut for very fast models the bottleneck will be in the Video Capturing from file/camera, >or in Video Showing (you can disable showing by using -dont_show flag).I think that it is the best solution, you only need the newest version of Darknet (from AlexeyAB)."
Speaker Identification / Recognition for less size audio files,"
I am working on speaker identification problem using GMM (Gaussian Mixture Model). I have to just identify one user present in the given audio, so for second class noise or silent audio may use or not just like in image classification for an object we create a non-object class. 
I have used a silent class is always showing the user is present ( which is not).
If any other model can give better accuracy fulfil the condition that only 30 sec of audio of a particular user is available and given test audio may has long size.
","['generative-model', 'data-science', 'speech-recognition', 'state-of-the-art']",
Which algorithm to use to solve this optimization problem?,"

I have items called 'Resources' from 1 to 7.
I have to use them in different actions identified from 1 to 10.
I can do a maximum of 4 actions each time. This is called 'Operation'.
The use of a resource has a cost of 1 per each 'Operation' even if it is used 4 times.
The following table indicates the resources needed to do the related actions:


|        |            Resources             |
|--------|----------------------------------|
| Action |  1 |  2 |  3 |  4 |  5 |  6 |  7 |
|--------|----------------------------------|
|     1  |  1 |  0 |  1 |  1 |  0 |  0 |  0 |
|     2  |  1 |  1 |  0 |  0 |  1 |  0 |  0 |
|     3  |  1 |  0 |  1 |  0 |  0 |  1 |  0 |
|     4  |  0 |  1 |  0 |  0 |  0 |  0 |  0 |
|     5  |  1 |  0 |  1 |  1 |  0 |  1 |  0 |
|     6  |  1 |  1 |  1 |  0 |  0 |  0 |  0 |
|     7  |  0 |  1 |  0 |  0 |  0 |  0 |  0 |
|     8  |  1 |  0 |  1 |  0 |  1 |  0 |  0 |
|     9  |  0 |  1 |  0 |  1 |  0 |  0 |  0 |
|    10  |  1 |  1 |  1 |  0 |  0 |  0 |  1 |


The objective is to group all the 'Actions' in 'Operations' that minimize the total cost. For example, a group composed by actions {3, 7, 9} needs the resources {1, 2, 3, 4, 6} and therefore has a cost of 5, but a group composed by actions {4, 7, 9} needs the resources {2, 4} and therefore has a cost of 2. 
It is needed to get done all the actions the most economically.
Which algorithm can solve this problem?
",['algorithm'],"I have finally abandoned the idea of â€‹â€‹doing it with an exact method and I have passed to the heuristic. I have mixed multi-boot, local search and certain random movements. Apparently this is called Greedy Randomized Adaptative Search Procedures (GRASP)Hypothesis: the best solution is reached filling the 'operations' with the maximum of 'Actions' per 'Operation'. Other combinations are more expensivecreate a random solution 
[[1 2 3 4] [5 6 7 8] [9 10]]calculate the cost of each 'Operation' 
[[6] [6] [5]] == 17Study the posibility of minimize the lower 'Operation' permuting some 'Action' with the other 'Operations'.  
Actions 4 & 7 permute with 9 & 10 
[[1 2 3 9] [5 6 10 8] [4 7]]Calculate the new cost 
[[6] [7] [1]] == 14Repeat steps 2 to 4 until it is not possible to minimize the lower, then the second lower, then the third etcYou will find soon a Local Minimum. If no more moves are allowed but the Local Minimum is not the Global Minimum, permute two random 'actions' between two random 'operations' and repeat the steps 2Âº to 5Âº.With this method it is possible to find very fast a solution adjusted to the minimum global cost with a complexity of O(nÂ·log(n)), where n is the amount of 'actions'I've tested it with a random sample of 60 'actions' and 26 'resources' grouped in 'operations' of 6 'actions'. It toke arround 5 minutes to get a really good solution arround 40% better that the initial one (*), and 30 minutes after it only got better solution in about 1%(*) the initial solution was not actually a random solution. Instead of that I used 'Ant Colony' algorithm with 15 ants to get a better initial solution and reduce the amount of iterations, but that is another history"
Rarely predict minority class imbalanced datasets,"
I have a dataset in which class A has 99.8%, class B 0.1% and class C 0.1%. If I train my model on this dataset, it predicts always class A. If I do oversampling, it predicts the classes evenly. I want my model to predict class A around 98% of the time, class B 1% and class C 1%. How can I do that?
","['neural-networks', 'machine-learning', 'deep-learning']",
Why machine learning instead of simple sorting and grouping?,"
I have a hard time formulating this question(I'm not knowledgeable enough I think), so I'll give an example first and then the question:
You have a table of data, let's say the occupancy of a building during the course of the day; each row has columns like ""people_inside_currently"", ""apartment_id"", ""hour_of_day"", ""month"", ""year"", ""name_of_day""(monday-sunday), ""amount_of_kids"", ""average_income"" etc.
You might preprocess two columns into a column ""percent_occupied_during_whole_day"" or something like that, and you want to group the data points in accordance with this as the main focus.
What I'm wondering is: why use machine learning(particularly unsupervised clustering) for this? Why not just put it into an SQL database table(for example), calculate two columns into that new one, sort by descending order, and then split it into ""top 25%, next 25%, next 25%, last 25%"" and output this as ""categories of data""? This is simpler, isn't it? I don't see the value of, for instance, making a Principle Component Analysis on it, reducing columns to some ""unifying columns"" which you don't know what to call anymore, and looking at the output of that, when you can get so much clearer results by just simply sorting and dividing the rows like this? I don't see the use of unsupervised clustering, I've googled a bunch of terms, but only found tutorials and definitions, applications(which seemed unnecessarily complex for such simple work), but no explanation of this.
",['machine-learning'],
Can we use imitation learning for on-policy algorithms?,"
Imitation learning uses experiences of an (expert) agent to train another agent, in my understanding. If I want to use an on-policy algorithm, for example, Proximal Policy Optimization, because of it's on-policy nature we cannot use the experiences generated by another policy directly. Importance Sampling can be used to overcome this limitation, however, it is known to be highly unstable. How can imitation learning be used for such on-policy algorithms avoiding the stability issues?
","['reinforcement-learning', 'proximal-policy-optimization', 'importance-sampling', 'on-policy-methods', 'imitation-learning']",
What is the best way to smoothen out a loss curve plot?,"
I am currently using a loss averaged over the last 100 iterations, but this leads to artifacts like the loss going down even when the current iteration has an average loss, because the loss 100 iterations ago was a large outlier.
I thought about using different interval lengths, but I wonder if an average over the last few iterations really is the right way to plot the loss.
Are there common alternatives? Maybe using decaying weights in the average? What are the best practices for visualizing the loss?
","['machine-learning', 'objective-functions', 'plotting']",
"Are CNN, LSTM, GRU and transformer AGI or computational intelligence tools?","
Will CNN, LSTM, GRU and transformer be better classified as Computational Intelligence (CI) tools or Artificial General Intelligence (AGI) tools? The term CI arose back when some codes like neural networks, GA, PSO were considered doing magical stuff. These days CI tools do not appear very magical. Researchers want codes to exude AGI. Do the current state of art Deep Learning codes fall in the AGI category?
","['deep-learning', 'terminology', 'agi']","CNNs, LSTMs, GRUs and transformers are or use artificial neural networks. The expression computational intelligence (CI) is often used interchangeably with artificial intelligence (AI). CI can also refer to a subfield or superfield of AI where biology is often an inspiration. See What is Computational Intelligence and what could it become? by WÅ‚odzisÅ‚aw Duch.RNNs are Turing complete and CNNs have been shown to be universal function approximators (they can approximate any continuous function to an arbitrary accuracy given a sufficiently deep architecture), but that doesn't mean we will be able to create AGI with them, unless you believe that AGI is just a bunch of algorithms, but, IMHO, that alone doesn't produce AGI. See also the computational theory of mind. To conclude, CNNs, LSTMs, GRUs and transformers are deep learning tools (so they could also be considered CI tools, given some definitions of CI), which might be useful for the development of AGI."
Is there any classifier that works best in general for NLP based projects?,"
I've written a program to analyse a given piece of text from a website and make conclusary classifications as to its validity. The code basically vectorizes the description (taken from the HTML of a given webpage in real time) and takes in a few inputs from that as features to make its decisions. There are some more features like the domain of the website and some keywords I've explicitly counted.
The highest accuracy I've been able to achieve is with a RandomForestClassifier, (>90%). I'm not sure what I can do to make this accuracy better except incorporating a more sophisticated model. I tried using an MLP but for no set of hyperparameters does it seem to exceed the previous accuracy. I have around 2000 datapoints available for training.
Is there any classifier that works best for such projects? Does anyone have any suggestions as to how I can bring about improvements? (If anything needs to be elaborated, I'll do so.)
Any suggestions on how I can improve on this project in general? Should I include the text on a webpage as well? How should I do so? I tried going through a few sites, but the next doesn't seem to be contained in any specific element whereas the description is easy to obtain from the HTML. Any help?
What else can I take as features? If anyone could suggest any creative ideas, I'd really appreciate it.
","['machine-learning', 'deep-learning', 'natural-language-processing', 'classification', 'text-classification']",
"Object Detection Algorithm that detects four corners of arbitrary quadrilateral, not just perpendicular rectangular","
Is there some established Object Detection algorithm that is able to detect the four corners of an arbitrary quadrilateral  (x0,y0,x1,y1,x2,y2,x3,y3) as opposed to the more typical perpendicular rectangular (x,y,w,h) ?
",['object-detection'],
What are some approaches to estimate the transition and observation probabilities in POMDP?,"
What are some common approaches to estimate the transition or observation probabilities, when the probabilities are not exactly known? 
When realizing a POMDP model, the state model needs additional information in terms of transition and observation probabilities. Often these probabilities are not known and an equal distribution is also not given. How can we proceed? 
","['reinforcement-learning', 'probability', 'pomdp']",
Can dropout layers not influence LSTM training?,"
I am working on a project that requires time-series prediction (regression) and I use LSTM network with first 1D conv layer in Keras/TF-gpu as follows:
model = Sequential()
model.add(Conv1D(filters=60, activation='relu', input_shape=(x_train.shape[1], len(features_used)), kernel_size=5, padding='causal', strides=1))
model.add(CuDNNLSTM(units=128, return_sequences=True))
model.add(CuDNNLSTM(units=128))
model.add(Dense(units=1))

As an effect my model is clearly overfitting:

So I decided to add dropout layers, first I added layers with 0.1, 0.3 and finally 0.5 rate:
model = Sequential()
model.add(Dropout(0.5))
model.add(Conv1D(filters=60, activation='relu', input_shape=(x_train.shape[1], len(features_used)), kernel_size=5, padding='causal', strides=1))
model.add(Dropout(0.5))
model.add(CuDNNLSTM(units=128, return_sequences=True))
model.add(Dropout(0.5))
model.add(CuDNNLSTM(units=128))
model.add(Dense(units=1))

However I think that it has no effect on the network learning process, even though 0.5 is quite large dropout rate:

Is this possible that dropout has little/no effect on a training process of LSTM  or maybe I do something wrong here?
[EDIT] Adding plots of my TS, general and zoomed in view.


I also want to add that the time of training increases just a bit (i.e. from 1540 to 1620 seconds) when I add the dropout layers.
","['neural-networks', 'keras', 'long-short-term-memory', 'dropout', 'regularization']",
Determine Frequency from Noisy Signal With Neural Networks (With Adeline Model),"
I'm trying to determine the frequency from a signal with NN. I'm using the Adeline model for my project and I'm taking a few samples in each 0.1-volt step in a true signal and a noisy one. 
First question: am I wrong?
Second question: my network works fine until the frequency of my sample for the test is equal to the frequency of my sample for the training. Otherwise, my network doesn't work and gives me the wrong answer. 
What do I need to do for this model?
for solving this problem I must use nonlinear steps like logarithmic steps. but How to use logarithmic steps in MatLab?  
Edition: I understand my problem is not Overfitting! I found that my samples step are linear and my samples are nonlinear so this is wrong
for solving this problem I must use nonlinear steps like logarithmic steps. but How to use logarithmic steps in MatLab?  
","['neural-networks', 'signal-processing']",
Is it acceptable to use various training sets for the individual models when using a majority vote classifier?,"
So I am trying to use a majority vote classifier combining different models  and I was wondering if it is acceptable to use different training sets for the individual models (including different features) if these sets all come from one larger dataset?
Thanks
","['machine-learning', 'classification']",
Is it expected that adding an additional hidden layer to my 3-layer ANN reduces accuracy significantly?,"
I've been using several resources to implement my own artificial neural network package in C++.
Among some of the resources I've been using are 
https://www.anotsorandomwalk.com/backpropagation-example-with-numbers-step-by-step/
https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/
https://cs.stanford.edu/people/karpathy/convnetjs/intro.html, 
as well as several others.
My code manages to replicate the results in the first two resources exactly. However, these are fairly simple networks in terms of depth. Hence the following (detailed) question:
For my implementation, I've been working with the MNIST Database of handwritten digits (http://yann.lecun.com/exdb/mnist/).
Using the ANN package I wrote, I have created a simple ANN with 784 input neurons, one hidden layer with 16 neurons, as well as an output layer with ten neurons. I have implemented ReLU on the hidden layer and the ouput layer, as well as a softmax on the output layer to get probabilities.The weights and biases are each individiually initialized to random values in the range [-1,1]
So the network is 784x16x10.
My backpropagation incorporates weight gradient and bias gradient logic.
With this configuration, I repeatedly get about a 90% hit rate with a total average cost of ~0.07 on the MNIST training set comprising 60,000 digits, and a slightly higher hit rate of ~92.5% on the test set comprising 10,000 digits.
For my first implementation of an ANN, I am pretty happy with that. However, my next thought was: 
""If I add another hidden layer, I should get even better results...?"".
So I created another artificial network with the same configuration, except for the addition of another hidden layer of 16 neurons, which I also run through a reLU. So this network is 784x16x16x10.
On this ANN, I get significantly worse results. The hit rate on the training set repeatedly comes out at ~45% with a total average error of ~0.35, and on the test set I also only get about 45%.
This leads me to either one or both of the following conclusions:
A) My implementation of the ANN in C++ is somehow faulty. If so, my bet would be it is somewhere in the backpropagation, as I am not 100% certain my weight gradient and bias gradient calculation is correct for any layers before the last hidden layer.
B) This is an expected effect. Something about adding another layer makes the ANN not suitable for this (digit classification) kind of problem.
Of course, A, B, or A and B could be true.
Could someone with more experience than me give me some input, especially on whether B) is true or not?
If B) is not true, then I know I have to look at my code again.
","['image-recognition', 'hidden-layers', 'multilayer-perceptrons', 'c++']","You probably got the back propagation wrong. I have done a test on the accuracy on adding an extra layer and the accuracy went up from 94% to 96% for me. See this for details:https://colab.research.google.com/drive/17kAJ2KJ36grG9sz-KW10fZCQW9i2Tf2cTo run the notebook click Open in playground and run the code. There is a commented line which add 1 extra layer. The syntax should be easy to understand even though it is in python. For back propagation, you can try to see this python implementation of multi layer perceptron backpropagation.https://github.com/enggen/Deep-Learning-Coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynbA network will not usually decrease it's accuracy by almost a half in normal scenario when you add an extra layer, though it is possible to have the network decrease accuracy when you add an extra layer due to overfitting. Though if this happen the performance drop won't be that dramatic.Hope I can help you."
What can model everything?,"
I've been thinking about what ""mathematical model"" can be used to model every possible thing (including itself).
Examples: a simple neuron network models a function but doesn't model an algorithm. A list of instructions models an algorithm but doesn't model relations between elements...
You might be thinking ""maybe there is nothing that can model everything"" but in reality ""language"" does model everything including itself. The issue is that it's not an organized model and it's not clear how to create it from scratch (e.g. if you will send it to aliens that don't have any common knowledge to start with).
So what is some possible formalization of a mathematical model that models every possible thought that can be communicated?
Edit 1:
The structure formalization I'm looking for has to have a few necessary properties:

Hierarchical: the representation of ideas should rely on other ideas. (E.g. an programming function is a set of programming functions, the concept ""bottle of water"" is sum of two concepts ""water"" and a ""bottle""...)
Uniqueness of elements: When an idea uses in its definition another idea, it must refer to one specific idea, not recreate it each time. For example, when you think of a digit ""9"" and the digit ""8"", you notice that both have a small circle at the top, you don't recreate a new concept ""circle"" every time, instead, you use a fixed concept ""circle"" for everything. By contrast, a neural network might recreate the same branch for different inputs. So two representations of concepts must be different iff they have a difference.)

",['ai-design'],
Interpretation of feature selection based on the model,"
The description of feature selection based on a random forest uses trees without pruning.
Do I need to use tree pruning?
The thing is, if I don't cut the trees, the forest will retrain.
Below in the picture is the importance of features based on 500 trees without pruning.

With a depth of 3.

I always use the last four signs 27, 28, 29, 30.
And I try to add to them signs from 0 to 26 by means of cycles, going through possible combinations.
Empirically, I assume that the trait number 0, 26 is significant.
But, on both pictures it is not visible. Although the quality of classification with the addition of 0, 26 has improved.
","['machine-learning', 'classification', 'python', 'scikit-learn']",random forest's feature importances are not reliable and you should probably avoid them. Instead you can use permutation_importance: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py
How to use AI for language recognition?,"
Given an audio track, I'm trying to find a way to recognize the audio language. Only within a small set (e.g. English vs Spanish). Is there a simple solution to detect the language in a speech?
","['machine-learning', 'natural-language-processing', 'audio-processing', 'speech-recognition']",
Is NLP likely to be sufficiently solved in the next few years?,"
The reason I am asking this question is because I am about to start a PhD in NLP. So I am wondering if there would be as much job opportunities in research in industry as oppose to in academia in the future (~ 5 to 10 years) or would it be mostly a matter of using a library off the shelf. I have done some research and it seems NLP is AI-complete, which means it's probably a problem that will be ""solved"" only when AGI is solved , but still I would appreciate any input.
",['natural-language-processing'],
What role do distractors play in natural language processing?,"
Iâ€™m doing research on natural language processing (NLP). Iâ€™d like to put together my own model. However, I'm running into a concept I am not familiar with, namely, distractors. A google search does not reveal much.
I've been reading this article specifically: https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313
In the section under ""Multi-Tasks Losses"" it reads:

Next-sentence prediction: we pass the hidden-state of the last token
  (the end-of-sequence token) through a linear layer to get a score and
  apply a cross-entropy loss to classify correctly a gold answer among
  distractors.

I understand how transformers and coss-entropy works, however I'm not sure what a distractor or a ""gold answer"" is for that matter. 
In this context, what does the author mean by distractor?
","['natural-language-processing', 'terminology']",
How to use LSTM to generate a paragraph,"
A LSTM model can be trained to generate text sequences by feeding the first word. After feeding the first word, the model will generate a sequence of words (a sentence). Feed the first word to get the second word, feed the first word + the second word to get the third word, and so on.
However, about the next sentence, what should be the next first word? The thing is to generate a paragraph of multiple sentences.
","['deep-learning', 'long-short-term-memory', 'sequence-modeling', 'text-classification', 'text-generation']",Take the sentence that was generated by your LSTM and feed it back into the LSTM as input. Then the LSTM will generate the next sentence. So the LSTM is using it's previous output as it's input. That's what makes it recursive. The intial word is just your base case. Also you should consider using GPT2 by open AI to do this. It's pretty impressive. https://openai.com/blog/better-language-models/
Why is a softmax used rather than dividing each activation by the sum?,"
Just wondering why a softmax is typically used in practice on outputs of most neural nets rather than just summing the activations and dividing each activation by the sum. I know it's roughly the same thing but what is the mathematical reasoning behind a softmax over just a normal summation? Is it better in some way?
","['neural-networks', 'activation-functions']",
What is the reasoning behind the number of filters in the convolution layer?,"
Let's assume an extreme case in which the kernel of the convolution layer takes only values 0 or 1. To capture all possible patterns in input of $C$ number of channels, we need $2^{C*K_H*K_W}$ filters, where $(K_H, K_W)$ is the shape of a kernel. So to process a standard RGB image with 3 input channels with 3x3 kernel, we need our layer to output $2^{27}$ channels. Do I correctly conclude that according to this, the standard layers of 64 to 1024 filters are only able to catch a small part of (perhaps) useful patterns?
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']",
"When using neural networks, should I bin the continuous variables and apply some transformation before performing variable selection and modeling?","
I come from a background of scorecard development using logistic regression. Steps involved there are:

binning of continuous variables into intervals (eg age can be binned into 10-15 years, 15-20 years, etc)

weight of evidence transformation

coarse classing of bins to ensure event rate has a monotonic relationship with the variable


Variable selection is made from the coarse classed transformed variables.
I was wondering if I should follow the same steps for ANN models. That is, should I bin the continuous variables and apply some transformation before performing variable selection and modeling.
","['neural-networks', 'classification', 'data-preprocessing', 'feature-selection']",
Why can neural networks generalize at all?,"
Neural networks are incredibly good at learning functions. We know by the universal approximation theorem that, theoretically, they can take the form of almost any function - and in practice, they seem particularly apt at learning the right parameters. However, something we often have to combat when training neural networks is overtfitting - reproducing the training data and not generalizing to a validation set. The solution to overfitting is usually to simply add more data, with the rationalization that at a certain point the neural network pretty much has no choice but to learn the correct function.
But this never made much sense to me. There is no reason, in terms of loss, that a neural network should prefer a function that generalizes well (i.e. the function you are looking for) over a function that does incredibly well on the training data and fails miserably everywhere else. In fact, there is usually a loss advantage to overfitting. Equally, there is an infinite number of functions that fit the training data and have no success on anything but. 
So why is it that neural networks almost always (especially for simpler data) stumble upon the function we want, as opposed to one of the infinite other options? Why is it that neural networks are good at generalizing, when there is no incentive for them to?
","['neural-networks', 'machine-learning', 'overfitting', 'computational-learning-theory', 'generalization']",
How do AI that play games of incomplete information decide their opening strategy?,"
This question was inspired by watching AlphaStar play Starcraft 2, but I'm also interested in the concept in general.
How does the AI decide what build order to start with?  In Starcraft, and many other games, the player must decide what strategy or class of strategies to follow as soon as the game begins.  To use a Starcraft-specific example, one must decide to 6-pool Zerg Rush before any scouting information has been gathered.  Delaying the rush to wait for info means the opponent will be stronger when the rush arrives; the opponent may even discover the rush and prepare a dedicated counter.
This is not limited to deciding between a risky early all-or-nothing attack.  Some long-term strategies also preclude others.  Terran players must decide early on how heavily they will invest in mech units.  They can focus on biological units like marines, or vehicular units like siege tanks and hellions.  Going equally into both, however, often means a weaker army overall, because you must spend resources on the overhead costs of both tech trees.  You must upgrade your vehicle weapons as well as your infantry weapons for instance, meaning less resources can be spent on more units.  Suffice to say, Terran players usually must decide very early on what they will focus on.
How can AI make these kinds of choices given incomplete and often uncertain information?
","['gaming', 'incomplete-information']",
Which CNN hyper-parameters are most sensitive to centered versus off centered data?,"
Which hyper-parameters of a convolutional neural network are likely to be the most sensitive to depending on whether the training (and test and inference) data involves only accurately centered images versus off-centered images.
More convolutional layers, wider convolution kernels, more dense layers, wider dense layers, more or less pooling, or ???
e.g. If I can preprocess the data to include only accurately centered images, which hyper-parameters should I experiment with changing to create a smaller CNN model (for a power and memory constrained inference engine)?  Or conversely, if I have a minimized model trained on centered data, which hyper-parameters would I most likely need to increase to get similar loss and accuracy on uncentered (shifted in XY) data?
","['convolutional-neural-networks', 'training', 'hyper-parameters']",
How difficult is this sound classification?,"
I want a microphone to pick up sounds around me (let's say beyond a 3 foot radius), but ignore sounds made at my desk, such as the rustling of paper, clicking a mouse and typing, my hands brushing up on the table, putting a pen down, etc. 
How hard would it be for AI to be able to distinguish these sounds from surrounding sounds, such as someone knocking on my door or a random loud sound from further away? How would you implement this? Is it possible that a pre-trained model could accomplish this, and work reliably for most people at their desk? I don't have any experience in AI.
","['classification', 'training', 'implementation']",
"What is the intuition behind the TD(0) equation with average reward, and how is it derived?","
In chapter 10 of Sutton and Barto's book (2nd edition) is given the equation for TD(0) error with average reward (equation 10.10):
$$\delta_t = R_{t+1} - \bar{R} + \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_{t}, \mathbf{w})$$
What is the intuition behind this equation? And how exactly is it derived? 
Also, in chapter 13, section 6, is given the Actor-Critic algorithm, which uses the TD error. How can you use 1 error to update 3 distinct things - like the average reward, value function estimator (critic), and the policy function estimator (actor)? 
Average Reward update rule: $\bar{R} \leftarrow \bar{R} + \alpha^{\bar{R}}\delta$
Critic weight update rule: $\mathbf{w} \leftarrow \mathbf{w} + \alpha^{\mathbf{w}}\delta\nabla \hat{v}(s,\mathbf{w})$
Actor weight update rule: $\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha^{\mathbf{\theta}}\delta\nabla ln \pi(A|S,\mathbf{\theta})$
","['reinforcement-learning', 'actor-critic-methods', 'temporal-difference-methods']","This is simply from definition of return in average reward setting (look at equation $10.9$). The ""standard"" TD error is defined as
\begin{equation}
TD_{\text{error}} = R_{t+1} + V(S_{t+1}) - V(S_t)
\end{equation}
In average reward setting, average reward $r(\pi)$ is subtracted from reward at $t$, $R_t$, so TD error in this case is
\begin{equation}
TD_{\text{error}} = R_{t+1} - \bar R_{t+1} + V(S_{t+1}) - V(S_t)
\end{equation}
where $\bar R_{t+1}$ is estimate of $r(\pi)$.  You can use $\delta_t$ in all 3 updates because neither of these updates depend on each other. For example if you update $\mathbf w$ you don't use that then to update $\mathbf \theta$, or if you update $\bar R$ you don't use updated version to update $\mathbf w$ or $\mathbf \theta$ so you're not introducing additional bias. In each separate update you also don't have $\delta_t$ present multiple times so that you require multiple sampling per timestep to get the unbiased update.  Additionally this is semi-gradient algorithm, it uses bootstrapped estimate $V_{t+1}$ but it doesn't calculate full derivative with respect to it aswell, only with respect to $V_t$ so the algorithm is biased by default, but works well enough in practice for linear case."
Why do we average gradients and not loss in distributed training?,"
I'm running some distributed trainings in Tensorflow with Horovod. It runs training separately on multiple workers, each of which uses the same weights and does forward pass on unique data. Computed gradients are averaged within the communicator (worker group) before applying them in weight updates. I'm wondering - why not average the loss function across the workers? What's the difference (and the potential benefits) of averaging gradients?
","['tensorflow', 'distributed-computing']",
How to create a fully connected(matrix) layer with vector input,"
I am trying to replace last fully connected layer of size 4096/2048 with a matrix of size 100x300 with previous fc layer output of 2048.

I've tried 

2D convolution - to map from 2048 --> 100x300 (Which is not realizable)
Intermediate projections :
    2048 --> 100
    [100x1] X [1x300] --> [100x300] (possible but complicated)

I am looking for a simple and effective solution with least linear transformations. 

",['convolutional-neural-networks'],"You can use tf.reshape() method (tensorflow doc) to reshape (2048) dimensional tensor to (100,300). Here's one way to do this: If you're not using TensorFlow but using Numpy, here's an implementation:Note: You might want to follow up this layer with tf.nn.conv2d layers to ""densify"" the sparse matrix/values obtained from the above step."
Is there an AI technology that can predict human behaviour?,"
Is there an AI technology out there or being developed that can predict human behaviour, given that we as humans are irrational decision-makers?
I'm looking at this from an economic standpoint - the issue with current economic models is that they assume that humans are perfectly rational, but obviously this isn't the case. Could AI develop better models and therefore produce better models of recessions?
","['machine-learning', 'reference-request', 'prediction']",
Sample size for the evaluation of Deep Learning Models,"
I'm evaluating the performance and accuracy in detecting objects for my data set using three deep learning algorithms. In total there are 24,085 images. I measure the performance in terms of time taken to detect the objects. To measure the accuracy, I manually count the number of objects in each image and then calculate recall and precision values for three algorithms.
However, since I'm manually counting to get actual object count, I selected only 30 images. Will that sample be enough to make a conclusion that algorithm 1 is better than others in terms performance and accuracy?
","['deep-learning', 'performance', 'accuracy']",
"What is the $\ell_{2, 1}$ norm?","
I'm reading this paper and it says:

In this paper, we present a multi-class embedded feature selection method called as sparse optimal scoring with adjustment (SOSA), which is capable of addressing the data heterogeneity issue. We propose to perform feature selection on the adjusted data obtained by estimating and removing the unknown data heterogeneity from original data. Our feature selection is formulated as a sparse optimal scoring problem by imposing $\ell_{2, 1}$-norm regularization on the coefficient matrix which hence can be solved effectively by proximal gradient algorithm. This allows our method can well handle the multi-class feature selection and classification simultaneously for heterogenous data

What is the $\ell_{2, 1}$ norm regularization? Is it L1 regularization or L2 regularization?
","['machine-learning', 'feature-selection', 'regularization']","$\ell_{2,1}$ is a matrix norm, as stated in this paper.
For a certain matrix $A \in \mathbb{R}^{r\times c}$,
we have
$$\|A\|_{2,1} = \sum_{i=1}^r \sqrt{\sum_{j=1}^c A_{ij}^2}$$
You first apply $\ell_2$ norm along the columns to obtain a vector with r dimensions. Then, you apply $l_1$ norm to that vector to obtain a real number. You can generalize this notation to every norm $\ell_{p,q}$."
Hyperparameter optimisation over entire range or shorter range of training episodes in Deep Reinforcement Learning,"
I am optimising hyperparameters for my deep reinforcement learning project (using PPO2, DQN and A2C) and was wondering:
Should I find the optimum hyperparameters to get maximum reward from training over my entire range of training (e.g. 50 million steps) or can I optimise over less time (e.g. 1 million steps)?
What is the conventional approach and why?
","['reinforcement-learning', 'deep-rl', 'hyperparameter-optimization']",
Do all neurons in a layer have the same activation function?,"
I'm new to machine learning (so excuse my nomenclature), and not being a python developer, I decided to jump in at the deep (no pun intended) end writing my own framework in C++.
In my current design, I have given each neuron/cell the possibility to have a different activation function. Is this a plausible design for a neural network? A lot of the examples I see use the same activation function for all neurons in a given layer.
Is there a model which may require this, or should all neurons in a layer use the same activation function? Would I be correct in using different activation functions for different layers in the same model, or would all layers have the same activation function within a model?
","['neural-networks', 'activation-functions', 'hidden-layers', 'network-design']",From here:Using other activation functions donâ€™t provide significant improvement in performance and tweaking them doesnâ€™t provide any big improvement. So as per simplicity we use same activation function for most of the case in Deep Neural Networks.
How is the loss value calculated in order to compute the gradient?,"
The gradient descent step is the following
\begin{align}
\mathbf{W}_i = \mathbf{W}_{i-1} - \alpha * \nabla L(\mathbf{W}_{i-1})
\end{align}
were $L(\mathbf{W}_{i-1})$ is the loss value, $\alpha$ the learning rate and $\nabla L(\mathbf{W}_{i-1})$ the gradient of the loss.
So, how do we get to the $L(\mathbf{W}_{i-1})$ to calculate the gradient of $L(\mathbf{W}_{i-1})$? As an example, we can initialize the set of $\mathbf{W}$ to 0.5. How can you explain it to me?
","['deep-learning', 'gradient-descent']","In your case, $L$ is the loss (or cost) function, which can be, for example, the mean squared error (MSE) or the cross-entropy, depending on the problem you want to solve. Given one training example $(\mathbf{x}_i, y_i) \in D$, where $\mathbf{x}_i \in \mathbb{R}^d$ is the input (for example, an image) and $y_i \in \mathbb{R}$ can either be a label (aka class) or a numerical value, and $D$ is your training dataset, then the MSE is defined as follows$$L(\mathbf{W}) = \frac{1}{2} \left(f(\mathbf{x}_i) - y_i \right)^2,$$where $f(\mathbf{x}_i) \in \mathbb{R}$ is the output of the neural network $f$ given the input $\mathbf{x}_i$.If you have a mini-batch of $M$ training examples $\{(\mathbf{x}_i, y_i) \}_{i=1}^M$, then the loss will be an average of the MSE for each training example. For more info, have a look at this answer https://ai.stackexchange.com/a/11675/2444. The https://ai.stackexchange.com/a/8985/2444 may also be useful.See the article Loss and Loss Functions for Training Deep Learning Neural Networks for more info regarding different losses used in deep learning and how to choose the appropriate loss for your problem."
How to understand my CNN's training results?,"
I created a multi-label classification CNN to classify chest X-ray images into zero or more possible lung diseases. I've been doing some configuration tests on it and analyzing its results and I'm having a hard time understanding some things about it.
First of all, these are the graphs that I got for different configurations:
Results of CNN with different configurations
Note 1: I've only changed the dataset size and the number of color channels in each configuration
Note 2: In case you're wondering why I tested the network with both 1 and 3 color channels, it's because the images are technically grayscale, but I am using the AlexNet architecture, which was made to take as input 224 x 224 images with 3 channels, so I wanted to see if the network somehow performed better with 3 channels instead of just the one
These are the things about it I don't understand:

Why does the sensitivity and specificity of the network vary so much between different epochs?
Is it normal for the validation loss of the network barely ever change as the number of epochs increase?
Looking at the results I got, it looks like 2 epochs is where there tends to be the best results. Does that make sense? I've heard of people training their networks with dozens of epochs sometimes.
Why is it that, many times, when the sensitivity of the network increases between epochs, the specificity tends to decrease, and vice-versa?

Sorry if some of these questions are dumb, I'm still a newbie at this. Also, my total dataset is drastically larger than what I present in these results (~110,000 images). I just haven't done tests with more images due to the time the network takes to train.
Network Architecture:

Base Architecture: AlexNet
Loss Function: Sigmoid Cross-Entropy Loss
Optimizer: Adam Optimization Algorithm with learning rate of 0.001

EDIT: I forgot to mention that the number of diseases to predict is 15, and that the network sees 0's much more than 1's due to the imbalance of classes. I've considered changing the loss function to a weighted version of sigmoid cross-entropy because of that, but I'm not sure if that would help the network much.
","['neural-networks', 'convolutional-neural-networks', 'classification', 'objective-functions']",
Is it possible to combine multiple SVMs that were trained on sublayers of a CNN into one combined SVM?,"
I have created a CNN for use on the MNIST dataset for now (so I have 10 classes). I have trained SVMs on the sublayers of this trained CNN and wish to combine them into a combined SVM as to give a combined score.
So far, I trained two individual SVMs at two of the sublayers of my neural network. 
What is the best method I can go about combining the two SVMs and what are the different options available to me? Is it simply a case of taking the maximum/average of each SVM prediction for a class and using that as the score for the combined SVM class prediction?
Thanks
","['neural-networks', 'convolutional-neural-networks', 'support-vector-machine']",
Are PAC learning and VC dimension relevant to machine learning in practice?,"
Are PAC learning and VC dimension relevant to machine learning in practice? If yes, what is their practical value?
To my understanding, there are two hits against these theories. The first is that the results all are conditioned on knowing the appropriate models to use, for example, the degree of complexity. The second is that the bounds are very bad, where a deep learning network would take an astronomical amount of data to reach said bounds.
","['machine-learning', 'computational-learning-theory', 'pac-learning', 'vc-dimension', 'vc-theory']",
What is the term for an RNN that is a completely connected directed graph?,"
There seems to be a severe problem with the taxonomy of neural network topologies.  What I'd like to know is the term I should use to search for the most general topology:  completely connected directed cyclic graph (henceforth CCDCGRNN).  This is because all other topologies degenerate by constraint from CCDCGRNN.  This includes topologies that are often confused with CCDCGRNN such as Elfman and Jordan networks* and more legitimately-so than, say LSTMs.
I know there are  claims such as this question at stats.stackexchange.com (including cites) that unqualified ""RNN"" refers to CCDCGRNN but this is not true if one looks a little deeper. Examples include not only the Wikipedia article on ""RNN"" (who trusts WP anyway, right?), but a ""mostly complete"" catalog of neural network topologies.  
There must have been, at some point in the ancient past, research into the methods by which one can, in a principled manner, degenerate the CCDCGRNs or at least why it isn't worth studying in its own right. 
*RNNs containing feed-through time delays are a degenerate case of CCDCGRNNs where a time delay of N out of a node is accomplished by allocating N neurons constrained to have only one input with weight of 1 (and a linear transfer function with slope 1).
","['recurrent-neural-networks', 'terminology']",
Categorizing text into dynamic amount of categories,"
I'm looking for a supervized system/approach, that could learn how to categorize incoming texts/documents, where new categories can be added over time and the training set will be small. The trained model should not be static and should be able to evolve with adding new categories or evaluating new documents.
For each document it should first give it's suggestion that can be then corrected.
",['categorical-data'],
Reasoning behind $Zero$ validation accuracy in the following ResNet50 model for classification,"
I have written this code to classify Cats and dogs using Resnet50. Actually while studying I came to the conclusion that Transfer learning gives very good accuracy for deep learning models, but I ended getting a far worse result and I didn't understand the cause for it. Any description with reasoning would be very helpful. The dataset contains 2000 images of cats and dogs as training and 1000 images as the validation set.
The following summarises my model
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer, Flatten, GlobalAveragePooling2D
num_classes = 2
IMG_SIZE = 224
IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)
my_new_model=tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=IMG_SHAPE, pooling='avg', classes=2)
my_new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import preprocess_input
train_datagen = ImageDataGenerator(
 preprocessing_function=preprocess_input,
 rotation_range=40,
 width_shift_range=0.2,
 height_shift_range=0.2,
 shear_range=0.2,
 zoom_range=0.2,
 horizontal_flip=True,)

# Note that the validation data should not be augmented!
test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

train_generator = train_datagen.flow_from_directory(
     train_dir,  # This is the source directory for training images
     target_size=(224,224),  # All images will be resized to 224x224
     batch_size=20,
     class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
     validation_dir,
     target_size=(224, 224),
     class_mode='binary')

my_new_model.fit_generator(
     train_generator,
     epochs = 8,
     steps_per_epoch=100,
     validation_data=validation_generator)

For this I get the training logs as,
Train for 100 steps, validate for 32 steps
Epoch 1/8
100/100 - 49s - loss: 7889.4051 - accuracy: 0.0000e+00 - val_loss: 7834.5318 - val_accuracy: 0.0000e+00
Epoch 2/8
100/100 - 35s - loss: 7809.7583 - accuracy: 0.0000e+00 - val_loss: 7775.1556 - val_accuracy: 0.0000e+00
Epoch 3/8
100/100 - 35s - loss: 7808.4858 - accuracy: 0.0000e+00 - val_loss: 7765.3964 - val_accuracy: 0.0000e+00
Epoch 4/8
100/100 - 35s - loss: 7808.0520 - accuracy: 0.0000e+00 - val_loss: 7764.0735 - val_accuracy: 0.0000e+00
Epoch 5/8
100/100 - 35s - loss: 7807.7891 - accuracy: 0.0000e+00 - val_loss: 7762.4891 - val_accuracy: 0.0000e+00
Epoch 6/8
100/100 - 35s - loss: 7807.6872 - accuracy: 0.0000e+00 - val_loss: 7762.1766 - val_accuracy: 0.0000e+00
Epoch 7/8
100/100 - 35s - loss: 7807.6633 - accuracy: 0.0000e+00 - val_loss: 7761.9766 - val_accuracy: 0.0000e+00
Epoch 8/8
100/100 - 35s - loss: 7807.6514 - accuracy: 0.0000e+00 - val_loss: 7761.9346 - val_accuracy: 0.0000e+00
<tensorflow.python.keras.callbacks.History at 0x7f5adff722b0>

If I change the class_mode='categorical' it's giving error as
Incompatible shapes: [20,2] vs. [20,2048].
","['machine-learning', 'deep-learning', 'tensorflow', 'keras', 'transfer-learning']",
Is running more epochs really a direct cause of overfitting?,"
I've seen some comments in online articles/tutorials or Stack Overflow questions which suggest that increasing the number of epochs can result in overfitting. But my intuition tells me that there should be no direct relationship at all between the number of epochs and overfitting. So I'm looking for an answer which explains if I'm right or wrong (or whatever's in between).
Here's my reasoning though. To overfit, you need to have enough free parameters (I think this is called ""capacity"" in neural networks) in your model to generate a function that can replicate the sample data points. If you don't have enough free parameters, you'll never overfit. You might just underfit.
So really, if you don't have too many free parameters, you could run infinite epochs and never overfit. If you have too many free parameters, then yes, the more epochs you have the more likely it is that you get to a place where you're overfitting. But that's just because running more epochs revealed the root cause: too many free parameters. The real loss function doesn't care about how many epochs you run. It existed the moment you defined your model structure before you ever even tried to do gradient descent on it.
In fact, I'd venture as far as to say: assuming you have the computational resources and time, you should always aim to run as many epochs as possible because that will tell you whether your model is prone to overfitting. Your best model will be the one that provides great training and validation accuracy, no matter how many epochs you run it for.
EDIT
While reading more into this, I realise I forgot to take into account that you can arbitrarily vary the sample size as well. Given a fixed model, a smaller sample size is more prone to overfitting. And then that kind of makes me doubt my intuition above. Still happy to get an answer though!
","['neural-networks', 'gradient-descent', 'overfitting', 'capacity']",
Which AGI systems have already been implemented and tested?,"
I wish to compile a (somewhat) comprehensive list of AGI systems that have actually been created and tested (to whatever degrees of success) instead of those that simply advertise they are going to 'do' something about it or have patented theoretical concepts. 
For the purposes of this question, we can use the following definition of AGI:

Artificial general intelligence (AGI) is the intelligence of a machine that can understand or learn any intellectual task that a human being can

","['reference-request', 'agi', 'research', 'state-of-the-art']",
Is my fine-tuned model learning anything at all?,"
I am practicing with Resnet50 fine-tuning for a binary classification task. Here is my code snippet.
base_model = ResNet50(weights='imagenet', include_top=False)
x = base_model.output
x = keras.layers.GlobalAveragePooling2D(name='avg_pool')(x)
x = Dropout(0.8)(x)
model_prediction = keras.layers.Dense(1, activation='sigmoid', name='predictions')(x)
model = keras.models.Model(inputs=base_model.input, outputs=model_prediction)
opt = SGD(lr = 0.01, momentum = 0.9, nesterov = False)
 
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  #
   
train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=False)
  
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
        './project_01/train',
        target_size=(input_size, input_size),  
        batch_size=batch_size,
        class_mode='binary')    

validation_generator = test_datagen.flow_from_directory(
        './project_01/val',
        target_size=(input_size, input_size),
        batch_size=batch_size,
        class_mode='binary')

hist = model.fit_generator(
        train_generator,
        steps_per_epoch= 1523 // batch_size, # 759 + 764 NON = 1523
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps= 269 // batch_size)  # 134 + 135NON = 269

I plotted a figure of the model after training for 50 epochs:

You may have noticed that train_acc and val_acc have highly fluctuated, and train_acc merely reaches 52%, which means that network isn't learning, let alone over-fitting the data.
As for the losses, I haven't got any insights.
Before training starts, network outputs:
Found 1523 images belonging to 2 classes.
Found 269 images belonging to 2 classes.

Is my fine-tuned model learning anything at all?
I'd appreciate if someone can guide me to solve this issue.
","['convolutional-neural-networks', 'training', 'keras', 'transfer-learning', 'fine-tuning']",
Is the number of neurons in each capsule in a capsule neural network hardcoded?,"
The capsule neural networks have been formally introduced in the paper Dynamic Routing Between Capsules.
Much ado has been made about how the capsules output a vector (magnitude = probability that an entity is present, orientation space = the instantiated parameters), which can then allow it to maintain more information than a max-pooled operation which outputs only a scalar. 
Within these vector representations, the dimensions of this space turn out to be the parameters by which a written digit could vary: scale, stroke thickness, skew, width, etc. There are 16 neurons in each capsule to represent 16 dimensions. 
It is unclear to me, from reading the paper, if these parameters emerged through training, or if they were hand-coded a priori. If these parameters were not hand-coded, why do such ""clean"" dimensions emerge? Why don't mixed-selective neurons emerge within the 16?
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'capsule-neural-network']",
Is word embedding a form of feature extraction?,"
Feature extraction is a concept concerning the translation of raw data into the inputs that a particular machine learning algorithm requires. These derived features from the raw data that are actually relevant to tackle the underlying problem. On the other hand, word embeddings are basically distributed representations of text in an n-dimensional space.
As far as I understand, word embedding is a somehow feature extraction technique. Am I wrong ? I had an argument with a friend who believes the two topics are totally separate. Is he right? What are the similarities and dissimilarities between word embedding and feature extraction?
","['comparison', 'word-embedding', 'feature-extraction']","Though word-embedding is primarily a language modeling tool, it also acts as a feature extraction method because it helps transform raw data (characters in text documents) to a meaningful alignment of word vectors in the embedding space that the model can work with more effectively (than other traditional methods such as TF-IDF, Bag of Words, etc, on a large corpus). Word Embedding techniques help extract information from the pattern and occurence of words and goes further than other traditional token representation methods to decode/identify the meaning/context of the words, thereby providing more relevant and important features to the model to tackle the underlying problem.However, from another standpoint, word-embedding models were not developed aiming to solve a particular feature extraction problem, but rather, to generalize and model the language used in a corpus to gain a semantic understanding of the words and the relationships between them. Such that, all the various corpus-specific tasks can then employ the same ""library"" of information which was collectively & exhaustively learnt by the embedding model. Meaning, the word embedding model learns a language model that is task-agnostic for all tasks on that corpus unlike feature extraction methods which are specifically task-oriented.Hence, the similarity is - word-embeddings can effectively aid in feature extraction; the dissimilarity is - they're not primarily meant to extract features more than they are for modeling a language which might be an ""overkill"" for a particular feature extraction task on a dataset."
How to describe an keras Model in a scientific report,"
how would you describe a machine learning model in a scientific report? It should be detailed but I just listed the hyperparameters... Have you got more important properties?
","['machine-learning', 'keras', 'academia']","Some other details you could mention are:  total number of model parameters (e.g. 1.2M or 0.15M) & depth of the network (e.g. 38-layered network)family/style of the network architecture (e.g. encoder-decoder arch., LSTM)For more info on the best kinds of details to be included in the report, refer to ""Methodology""/ ""Training""/ ""Implementation""/ ""Proposed Architecture"" sections of the deep learning research papers in your relevant area."
"What is a ""batch"" in batch normalization?","
I'm working on an example of CNN with the MNIST hand-written numbers dataset. Currently I've got convolution -> pool -> dense -> dense, and for the optimiser I'm using Mini-Batch Gradient Descent with a batch size of 32.
Now this concept of batch normalization is being introduced. We are supposed to take a ""batch"" after or before a layer, and normalize it by subtracting its mean, and dividing by its standard deviation.
So what is a ""batch""? If I feed a sample into a 32 kernel conv layer, I get 32 feature maps. 

Is each feature map a ""batch""? 
Are the 32 feature maps the ""batch""? 

Or, if I'm doing Mini-Batch Gradient Descent with a batch size of 64,

Are 64 sets of 32 feature maps the ""batch""? So in other words, the batch from Mini-Batch Gradient Descent, is the same as the ""batch"" from batch-optimization?

Or is a ""batch"" something else that I've missed?
","['neural-networks', 'batch-normalization']","The ""batch"" is same as in mini-batch gradient descent. The mean in batch-norm here would be the average of each feature map in your batch (in your case either 32 or 64 depending on which you use)generally batch is used quite consistently in ML right now, where it refers to the inputs you send in together for forward/backward pass."
Ambulance dataset needed [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



Could I get a dataset that can classify ambulances? 
I have searched everywhere, but, couldn't seem to get hold of a set of annotated images for ambulances.
",['datasets'],
How to calculate the precision and recall given the predictions and targets in this case?,"
I'm using three pre-trained deep learning models to detect vehicles and count from an image data set. The vehicles belong to one of these classes ['car', 'truck', 'motorcycle', 'bus']. So, for a sample I have manually counted number of vehicles in each image. Also, I employed the three deep learning models and obtained the vehicle counts. For example:
    Actual        | model 1 count| model 2 count  | model 3 count 
------------------------------------------------------------------
    4 cars, 1 bus | 2 cars       | 2 cars, 1 truck| 4 cars
    2 cars        | 0            | 1 truck        | 1 car, 1 bus

In this case, how can I measure accuracy scores such as precision and recall?
","['deep-learning', 'convolutional-neural-networks', 'object-detection', 'precision', 'recall']","Precision is the number of true positives over the number of predicted positives(PP), and recall is the number of true positives(TP) over the number of actual positives(AP) you get. I used the initials just to make it easier ahead.A true positive is when you predict a car in a place and there is a car in that place.A predicted positive is every car you predict, being right or wrong does not matter.A actual positive is every car that actually is in the picture.You should calculate these values separately for each category, and then sum over the examples you sampled, if I am not mistaken.So for the CAR category you have (assuming the predictions do match with the target, i.e., you are not predicting a truck as a car for example) :So in total precision  is 2/2 = 1 and recall is 2/6 = 0.3(3).You can then do the same for the other categories, and for the other models. This way you can check if a model is predicting one category better than the other. For example, model 1 can be better at finding cars in a picture whilst model 3 can be better at finding buses.The important part is that you know if the objects the model predicted actually correspond to what is in the picture. A very unlikely example would be a picture with 1 car and 1 truck where the algorithm recognizes the car as a truck and the truck as a car. From the info that is in the table I cannot be sure if the 2 cars you predict are the actual cars in the picture, or in other words, if they are actually True Positives or are actually False Positives. "
Can an image recognition model used for human pose estimation?,"
I am currently writing my thesis about human pose estimation and wanted to use Google's inception network, modify it for my needs and use transfer learning to detect human key joints. I wanted to ask if that could be done in that way?
Assuming I am having n-keypoints, generating the n-feature maps, use transfer learning and cut off the final classification layers and replace it by a FCN which guesses the key joints. I am asking myself if this might be possible.
However, these feature maps should output heatmaps with the highest probability as well. Is this assumption valid? 
","['classification', 'computer-vision', 'object-detection']",
Improving graph decoder network,"
I have been using a network to generate graphs. The architecture that I have been using is the following:

In this figure, $D_1$ is the signal generator and $D_2$ is the graph topology generator, which is a square, symmetric matrix which indicates which node is connected to which. In this network, $l$ shows linear layers, $a$ shows activation functions. Here we are using leaky relu activation function. 
The problem that I am experiencing is that after training the network, my output is only a chain of nodes, meaning that only subdiagonal and superdiagonal elements have non-zero values and it is very rare to have other forms of graph. I was wondering if anyone has a suggestion for improving the output. Note that my training data is diverse and has every kind of graphs.
","['generative-model', 'graphs']",
How to make DNN learn multiplication/division?,"
A single neuron with 2 weights and identity activation can learn addition/subtraction as the 2 weights will converge to 1 and 1 (addition), or 1 and -1 (subtraction).
However, for multiplication and division, it's not that easy. Can a single neuron learn multiplication or division? If not, how many layers of DNN can learn these?
","['neural-networks', 'machine-learning', 'deep-learning', 'feedforward-neural-networks']","In reallity any continous function on a compact can be approximated by a neural network having one hidden layer with a finite number of neurones (This is the Universal Approximation Theorem). Thus you only need one hidden layer to approximate the multiplication on a compact, note that you need to apply a non linear activation on the hidden layer to do this. "
How many parameters are being optimised over in a simple CNN?,"
Okay so here's my CNN (simple example from a tutorial) along with some arithmetic to get the total number of free parameters.
We've got a dataset of 28*28 grayscale image (MNIST).

First layer is a 2D convolution using 32 3x3 kernels. Dimensionality of the output is 26x26x32 (kernel stride length was 1 and we have 32 feature maps of 26x26). Running parameter count: 288
Second layer is 2x2 MaxPool with a 2x2. Dimensionality of the output is 13x13x32 but then we flatten so we got a vector of length 5408. No extra parameters here.
Third layer is Dense. A 5408x100 matrix. Dimensionality of the output is 100. Running Parameter count: 540988
Fourth layer is Dense also. A 100x10 matrix. Dimensionality of the output is 10. Running Parameter count: 541988

Then we're supposed to do stochastic gradient descent on a 541988 parameter space!
That feels like a ridiculously big number to me. And this is meant to be the hello world problem of CNNs. Am I missing something fundamental in my understanding of how this is meant to work? Or maybe the number is correct but it's not actually a big deal for a computer to crunch?
In case it helps. Here is how the model was built in Keras:
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape=(28,28,1)))
    model.add(MaxPooling2D((2,2)))
    model.add(Flatten())
    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    opt = SGD(lr=0.01, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metric=['accuracy'])
    return model

","['convolutional-neural-networks', 'python', 'keras', 'gradient-descent']","Neural networks can have a lot of different structures. CNNs can have a number of parameters that ranges from a few thousands to several millions. In general you aim to increase the number of filters and reduce the first 2 dimensions, as you go deeper in the network.So if you had Conv -> pool -> Conv -> pool -> ... , you could do for example first conv with kernel size = 5 and 8 filters and the second conv with kernel size = 5 and 16 filters. And both pools being (2,2).. But this is just an example.In your network you start with a 28*28 image, and you use 32 3*3 filters. so number of parameters is (3*3 + 1) * 32 = 320.In the dense layer you have as input a 13*13*32 and use a 100 FC layer. so n_parameters is (13*13*32 + 1)*100 which is 540900.Then you get (100+1) * 10 FC which is 1010 more.total = 320 + 540900 + 1000 which is 542230, as expected.The +1 that shows up in every layer is the Bias neuron. Basically you add a bias neuron per output in a connection between 2 layers. In the FC1000 to the FC10 it is easy to understand, you have a bias per output neuron. In the Convolutional layers you have a bias term per each of the filters applied, so for each filter you have the filters weights plus 1 for the bias.Apart from that you also had a small math mistake when adding the 288 at the beginning, you added 188. So your missing 242 parameters were: 32 from the conv layer, 100 from the first dense, 10 from the second dense and 100 from the sum. 100+100+32+10 = 242."
Why is my variational auto-encoder generating random noise?,"
This is my first variational autoencoder. Background info: I am using the MNIST digits dataset. The model is created and trained in PyTorch. The model is able to get a reasonably low loss, but the images that it generates are just random noise. Here are my script and the images that were generated by the model: https://github.com/jweir136/PyTorch-Variational-Autoencoder-Latent-Space-Visualization.
Any advice or answers on how to solve this problem are greatly appreciated.
","['autoencoders', 'pytorch', 'variational-autoencoder']",
Should I add some noise when the dataset is small?,"
I want to create a small dataset (about 10 classes and 20-30 images each), should I add some noise (wrong label sample) in the training, validation and test datasets, and why?
",['datasets'],
Find the nearest object in a image which is captured from camera?,"
Objective :  To find the nearest object (closer distance object)  in the single camera image. But Image Contains multiple objects shown below: 

I searched in the net and found this formula to calculate the distance of object from camera 
F = (P x  D) / W Further Detail click on the Link
Is there any other better approach to find the nearest object in a image?
Thanks in Advance!!!
","['machine-learning', 'computer-vision', 'object-detection', 'image-processing', 'image-segmentation']",
Why information gain with entropy as impurity function can't be used as a splitting method for Decision Tree Regression?,"
In Decision Tree Regression, we can use 'Reduction in Variance' or MSE (Mean Squared Errors) as splitting methods. There are methods like Gini Index, Information Gain, Chi-Square for splitting on classification trees. Now, I read somewhere that we cannot use Information gain (with impurity function as entropy) as a splitting method for regression trees. Why is it so, and what other methods are there which we can and cannot use, and why?
EDITS:
Please suggest me a reference to understand maths behind it. 
The references I used are : 
https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/ 
https://www.python-course.eu/Regression_Trees.php
https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054
In the first article, it is mentioned that:

Gini Index, Chi-Square and Information gain (impurity function as entropy) algorithms are used for Classification trees while Reduction in Variance is used for Regression Trees.

In the second article, it is mentioned that:

Since our target feature is continuously scaled, the IGs of the categorically scaled descriptive features are no longer appropriate splitting criteria.
As stated above, the task during growing a Regression Tree is in principle the same as during the creation of Classification Trees. Though, since the IG turned out to be no longer an appropriate splitting criteria (neither is the Gini Index) due to the continuous character of the target feature we must have a new splitting criteria.
  Therefore we use the variance which we will introduce now.

In the third article, it is mentioned that:

""Entropy as a measure of impurity is a useful criterion for classification. To use a decision tree for regression, however, we need an impurity metric that is suitable for continuous variables, so we define the impurity measure using the weighted mean squared error (MSE) of the children nodes instead""

Thank You!
","['machine-learning', 'decision-trees', 'metric']",
N-tuple based tic tac toe diverges in temporal difference learning,"
I have n-tuple based tic tac toe. I already have perfect minimax player and perfectly trained table-based player. My n-tuple network consists of 8 different rows of 3 of the board as triplets having possible empty, X or O, and one bit defining who's move is now, so totally 2 * 3^3 = 54 states in tuple. I train and update weights with the idea of the pseudo code from ""Learning to Play Othello with N-Tuple Systems"" by Simon Lucas:
public void inGameUpdate(double[] prev, double[] next) {
  double op = tanh(net.forward(prev));
  double tg = tanh(net.forward(next));
  double delta = alpha * (tg - op) * (1 - op * op);
  net.updateWeights(prev, delta);
}

public void terminalUpdate(double[] prev, double tg) {
  double op = tanh(net.forward(prev));
  double delta = alpha * (tg - op) * (1 - op * op);
  net.updateWeights(prev, delta);
}

And the score is the sum of weights of those rows of 3. The temporal difference training generally works for n-tuple based tic tac toe and after several thousands games it mostly plays perfectly. But after a while it diverges from the perfection and oscillates between perfect and near perfect. I realized it was in situations like this:
OXO
X-O
-XX

I suspect because row that prevent opponent from winning has big value. And having two of such rows seems to be better than losing later.
I know I can have perfect player basing on this particular n-tuple network. I could just stop training after I reach perfection, but in bigger games I can't do that. I fiddled with different alpha in ranges 0.1-0.0001 and e-greedy epsilon 1%-50%, or adaptive. Increasing epsilon to about 50% somewhat mitigates this effect, but this value is mostly very big to use in other games.
Here are couple questions:

Does this effect have a name in the machine learning world? It values preventing opponent from winning. But if opponent has more opportunities to win, its value will be bigger, so that it will exceed the (negative of) losing value.
Aside from probably using different n-tuple networks and tweaking hyper-parameters, what can I do to mitigate or eliminate this effect?
In bigger games, this learning and n-tuple system give fairly good result, but I see big oscillations after certain points. I.e. in breakthrough game against 1-ply minimax, after it reaches about 60% winrate (testing 10000 games after training every 10000 games against itself), its winrate goes slitghly up, but in testing its winrate oscillates between 45-65%. Can this effect be caused by the problem I mentioned in 1.?

","['q-learning', 'temporal-difference-methods']",
YOLO 9000 about Better Stronger,"
In this paper, YOLO has three features compared to YOLO v1. This question is about Better and Faster.
In the Better section, there are many techniques such as Batch Norm, Anchor Box and so on. In the Faster section, there is a darknet only.
Darknet has 19 Conv Layer but it doesn't use Layer Norm or Passthrough Layer. So, I think that Darknet doesn't use Better section techniques.
Is the Better Section model different from Faster Section Model?
In my understanding, there are three models named YOLO v2. First is Better YOLO v2, second is Faster YOLO v2, third Strong YOLO v2. Is this right?
","['deep-learning', 'object-recognition', 'papers', 'yolo']",
How does the memory mechanism (reading and writing) work in a neural Turing machine?,"
In neural Turing machine (NTM), reading memory is represented as 
\begin{align} 
r_t \leftarrow \sum\limits_i^R w_t(i) \mathcal{M}_t(i) \tag{2}
\end{align}
and writing to memory is represented as
Step1: Erase
\begin{align} 
\mathcal{M}_t^{erased}(i) \leftarrow \mathcal{M}_{t-1}(i)[\mathbf{1} - w_t(i) e_t ] \tag{3}
\end{align}
Step2: Add
\begin{align} 
\mathcal{M}_t(i) \leftarrow \mathcal{M}_t^{erased}(i) + w_t(i) a_t \tag{4}
\end{align}
In the reading mechanism, if we take this example values and applied to the above formula, instead of a vector, we get a scalar of value 2.
M_t =[[1,0,1,0],
      [0,1,0,0],
      [1,1,1,0]]

w_t = [1,1,1]

The same thing happens in writing as well; here we take the dot product of two vectors,  $w_t(i) e_t$,  with a scalar value as output. According to paper, unless $w_t$ or $e_t$ are zeros, it will erase all values in the memory matrix.
My own idea about NTM memory was that it uses the weights to find the indices or rows inside the memory matrix corresponding to a certain task.
How does the memory in NTM work?
How a memory for a particular task is stored, that is, is it stored row-wise or it's stored in the whole matrix?
","['neural-networks', 'deep-learning', 'math', 'memory', 'neural-turing-machine']",
How to classify human actions?,"
I'm quite new to machine learning (I followed the Coursera course of Andrew Ng and now starting deeplearning.ai courses).
I want to classify human actions real-time like:

Left-arm bended
Arm above shoulder
...

I first did some research for pre-trained models, but I didn't find any.
Because I'm still quite new, I want to have advice about how I should solve this.

I thought maybe I need to create for every action enough pictures and from there on I can do image classification. 
Or I use PoseNet from TensorFlow so that I have the pose estimation points. And from there on I create videos of a couple of seconds with every pose I want to track and I save the estimation points. From there on, I use a classification algorithm (neural network) to classify those points. 

What is the most efficient option or are they both bad and is there a better way to do this?
","['neural-networks', 'classification', 'image-recognition', 'video-classification']",
How to handle a high dimensional video (large number of frames per video) data for training a video classification network,"
I have a video dataset as follows. 
Dataset size: 1k videos
Frames per video: 4k (average) and 8k (maximum)
Labels: Each video has one label. 
So the size of my input will be (N, 8000, 64, 64, 3) 
64 is height and width of video. I use keras. I am not really sure how to do an end-to-end training with this kind of dataset. I was thinking of dividing each input in blocks of frames (N, 80, 100, 64, 64, 3) for training. But still it wont work for an end-to-end network training.
I am not in favor of  dropping the frames. That might be my last choice. 
Any help will be appreciated. Thanks in advance. 
","['deep-learning', 'keras', 'dimensionality', 'video-classification']",
What are some new deep learning models for learning latent representation of data?,"
I know that autoencoders are one type of deep neural networks that can learn the latent representation of data. I guess there should be several other models like autoencoders. 
What are some new deep learning models for learning latent representation of data?
","['neural-networks', 'deep-learning', 'autoencoders', 'latent-variable']","Here's a link to my answer on CV Stack Exchange, where I have mentioned about latent spaces and some deep learning models that learn these representations: https://stats.stackexchange.com/questions/442352/what-is-a-latent-space/442360#442360In short, deep learning models for Domain Adaptation, Computer Vision, Natural Language Processing, Recommendation Systems, Music/Speech/Audio processing, Adversarial models, etc., all learn some form of latent representation of data.In fact, any place we're learning a function to map input and output spaces of a dataset, the model essentially learns a latent representation of data irrespective of whether the model is based on deep neural networks or a stochastic method or any other."
How can I train a neural network to find the hyper-parameters with which the data was generated?,"
I have 10000 tuples of numbers (x1, x2, y) generated from the equation: y = np.cos(0.583 * x1) + np.exp(0.112 * x2). I want to use a neural network, trained with gradient descent, in PyTorch, to find the 2 parameters, i.e. 0.583 and 0.112
Here is my code:
class NN_test(nn.Module):
    def __init__(self):
        super().__init__()
        self.a = torch.nn.Parameter(torch.tensor(0.7))
        self.b = torch.nn.Parameter(torch.tensor(0.02))

    def forward(self, x):
        y = torch.cos(self.a*x[:,0])+torch.exp(self.b*x[:,1])
        return y

model = NN_test().cuda()

lrs = 1e-4
optimizer = optim.SGD(model.parameters(), lr = lrs)
loss = nn.MSELoss()

epochs = 30
for epoch in range(epochs):
    model.train()
    for i, dtt in enumerate(my_dataloader):
        optimizer.zero_grad()

        inp = dtt[0].float().cuda()
        output = dtt[1].float().cuda()

        ls = loss(model(inp),output)

        ls.backward()
        optimizer.step()
    if epoch%1==0:
        print(""Epoch: "" + str(epoch), ""Loss Training: "" + str(ls.data.cpu().numpy()))

where x contains the 2 numbers x1 and x2. In theory, it should work easily, but the loss doesn't go down. What am I doing wrong? 
","['gradient-descent', 'pytorch']",
How to deal with nonstationary rewards in asymmetric self-play reinforcement learning?,"
Suppose we're training two agents to play an asymmetric game from scratch using self play (like Zerg vs. Protoss in Starcraft). During training one of the agents can become stronger (discover a good broad strategy for example) and start winning most of the time, which causes big portion of the state values (or Q(s,a) values) become very high for this agent and low for another, just because the first is generally stronger and receives most of the rewards. Some training time later the other one finds a weakness in the first's play (in many states too) and starts dominating and the reward stream shift the other way.
The problem is, we have to retrain function approximator (deep neural net) to wildly different value/Q states, this slows and destabilizes learning. For each of the agents this is similar to highly nonstationary environment (the opponent), that can be harsh or easy at times. 
What do people usually do in such a case? I think what is needed is some kind of slowly changing baseline (similar to advantage in A2C), but applied to the reward values themselves.
","['reinforcement-learning', 'deep-rl', 'self-play']",
How to explain peak in training history of a convolutional neural network?,"
I am training a simple convolutional neural network to recognize two types of 1024-point frequency spectra (FFT). This is the model I'm using:
cnn = Sequential()
cnn.add(Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), input_shape=(nInput,1)))
cnn.add(Conv1D(filters=64, kernel_size=3, activation=LeakyReLU()))
cnn.add(MaxPooling1D(pool_size=2))
cnn.add(Flatten())
cnn.add(Dense(nFinalDense, activation=LeakyReLU()))
cnn.add(Dense(nOutput, activation='sigmoid'))

However I get the following accuracy and loss during training:

Why do I get the large peak in both plots? How can it be explained? Is there a problem with the data I'm using (I mention that I obtain a similar peak when training an autoencoder for denoising using the same data)?
","['convolutional-neural-networks', 'training', 'accuracy', 'loss']","I found that the peak was caused by the data I am using. Specifically, the MinMaxScaler changed the data shape and I resolved the issue by simply dividing to the max value."
Flattened vector observation or convolutional neural network input?,"
This is more of a general question of how to model/preprocess 'visual' state-observations to an Agent in Reinforcement Learning that I'll illustrate with an example.
Say you have a reinforcement learning problem where the agent has to draw pixels in an n * n 2D state-matrix of 0's and 1's. Say n = 100. The agent can move one step (up, down, left, right) and on its location can additionally switch 0's into 1's or the other way around.
Each step, it needs to take action so that the state-matrix resembles an n * n target-matrix (that has a certain shape). It is rewarded accordingly each step.
The agent will know its location from an x and y position that are given in addition to the state- and target-matrix each step.
Now I'm curious to the question what the best way is to represent the state to the agent. Using a visual 'prior', or not. Here's two ways:

Based on that you want to give only the essential information to the agent: The agent is presented with a matrix (with target subtracted from state), that will be flattened into one array of n^2. Additionally it'll know its current location as an additional (x, y) vector observation.
Based on that (1) would be more difficult to solve for a human, because you'll have to learn from a flattened array how different points are connected (think about how hard a flattened game of chess would be), you can also use a convolutional neural network to encode the current scene. In this case the agent will be e.g. a red dot. Given that it's such a visual task, it seems to me that using this would give the agent a better model of how the environment works, since the spatial relations are kept intact. Also it feels that keeping the 2D shape intact with a CNN would mean that it'd form better representations that generalize to other shapes, but I can't really say why.

On the other hand one could say that it's arrogant to assume that our 'human' spatial way of interpreting visual information is the best way for this case. Maybe there's a mathematical solution?
Any ideas?
","['machine-learning', 'reinforcement-learning', 'convolutional-neural-networks']",
How to understand the concept of self-supervised learning in AI?,"
I am new to self-supervised learning and it all seems a little magical at the moment.
The only way I can get an intuitive understanding is to assume that, for real-world problems, features are still embedded at a per-object level.
For example, to detect cats in unseen images, my self-supervised network would still have to be composed exclusively of cats.
So, if I had 100 images of cats and 100 images of dogs, then I thought self-supervised approaches would learn the features of the images. For example, if an image is rotated 90 degrees, it learns what was in the image that was rotated 90 degrees. However, if I wanted to classify just cats using this representation, then I wouldn't be able to do so without separating out what makes a cat a cat and a dog a dog.
Is my assumption correct?
","['self-supervised-learning', 'representation-learning']",
How does a batch normalization layer work?,"
I understood that we normalize to input features in order to bring them on the same scale so that weights won't be learned in arbitrary fashion and training would be faster. 
Then I studied about batch-normalization and observed that we can do the normalization for outputs of the hidden layers in following way:
Step 1: normalize the output of the hidden layer in order to have zero mean and unit variance a.k.a. standard normal (i.e. subtract by mean and divide by std dev of that minibatch).
Step 2: rescale this normalized vector to a new vector with new distribution having $\beta$ mean and $\gamma$ standard deviation, where both $\beta$ and $\gamma$ are trainable.
I did not understand the purpose of the second step. Why can't we just do the first step, make the vector standard normal, and then move forward? Why do we need to rescale the input of each hidden neuron to an arbitrary distribution which is learned (through beta and gamma parameters)?
","['neural-networks', 'deep-learning', 'batch-normalization']",
Interesting examples of discrete stochastic games,"
SGs are a generalization of MDPs to multiple agents. Like this previous question on MDPs, are there any interesting examples of zero-sum, discrete SGsâ€”preferably with small state and action spaces? I'm hoping to use such examples as benchmarks, but couldn't find much in the literature. One example I can think of is a pursuit-evasion game on a graph.
","['game-theory', 'environment', 'markov-decision-process', 'benchmarks']","Some of the domains in the International Probabilistic Planning Competition, such as the Wildlife Preserve benchmark, fit quite well the constraints you have given. Note that the problems are modeled with a high-level declarative language, RDDL. This means that you can define problems as big or as small as your heart desires with relative ease, since you can parametrize state description in terms of functions describing properties of an arbitrary number of objects.There's also a quite useful project that allows to instance OpenAI gym environments from the declarative description of the environment, states and actions."
How can I split the data into training and validation sets such that entries with a certain value are kept together?,"
I have the following kind of data frame. These are just example:
A 1 Normal
A 2 Normal
A 3 Stress
B 1 Normal
B 2 Stress
B 3 Stress
C 1 Normal
C 2 Normal
C 3 Normal

I want to do 5-fold cross-validation and splitting the data using 
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)

data = (ImageList.from_folder(PATH)
        .split_by_rand_pct(valid_pct=0.2)
        .label_from_folder()
        .transform(get_transforms(do_flip=True, flip_vert= True,max_zoom=1.1, max_rotate=10, max_lighting=0.5),size=224)
        .databunch()
        .normalize() )

It works great. It splits the data randomly which is expected. Though I want to keep the data points together in the training or validation, having the same value in column 1. So, all the A's would be in either the training or validation dataset, all the B's would be in the training or validation dataset, and so on.
More info on my data:
I have cell assay images which are labelled in three classes. Now, these images are big in size, so I split one image into 16 small, non overlapping tiles, to bring down the size to 224( optimal enough to feed into CNN). All these tiles have the same label as the original image. These tiles are the final input to the CNN. TO perform cross-validation, I need to keep the tiles of same image into one fold and set.
","['python', 'cross-validation', 'scikit-learn']",
Reinforcement learning possible with big action space?,"
Iâ€™m experimenting with reinforcement learning for a 2D pixel plotting task, and am running into an issue that (I think) has to do with the big action space. It goes like this:
The Agent gets two vector inputs each step.
Each describes an (n x n) 2d matrix composed of zeros and ones. 
One is the (n x n) target matrix, containing a certain shape of zeros
The other is an (n x n) state matrix, containing another shape
Every step, I want my agent to pick an (x, y) coordinate:
x (picks one of n)
y (picks one of n)
This will turn a zero into one, or one into zero.
every step, if correct, I give a small reward, and itâ€™ll get punished when incorrect.
Iâ€™m training the agent (a network with 3 layers with 256 hidden units) with PPO, and curiosity in the loss, and for a 12 x 12 matrix it works quite well, not 100% but okay. (see image). Note that the agent doesn't get enough steps here to fully delete the initial shape when the target shape is empty, that's why it doesn't fully make it. Takes about 800K steps to converge though.
 
But the agent starts struggling in local minima when I increase beyond 32 x 32.
This one is at 32 x 32:

Is this even scalable to bigger matrices even? I was hoping to go 3D eventually, by reaching 100x100x100 .
I do realize that i have a huge input and action space when working with such a grid. 
Is something like that even possible with an RL paradigm? Iâ€™ve tried increasing the network size, and decreasing learning rate, but Iâ€™m not satisfied. Any ideas or alternative approaches to plot pixels like this?
Any input is very much appreciated!
Thanks!
","['machine-learning', 'reinforcement-learning']",
How can I convert the probability score between 0 to 1 to another format?,"
I have trained a multi-class CNN model using fastai. The model splits out probabilites for each of the three classes, which, of course, sum up to 1. The class with highest probability becomes the predicted class. 
Is there any way I can convert them into 0 to 1 scale, where near to 0 value would mean class 1, near to 0.5 would mean class 2 and near to 1 would mean class 3?
","['convolutional-neural-networks', 'probability']","You could maybe do something like this, it's a bit hackish
\begin{equation}
y = C_1\cdot 1 + C_2 \cdot 0.5 + C_3 \cdot 0
\end{equation}
$y$ represents the output and its bounded $\in [0, 1]$. $C_i$ is probability for class $i$. This way when $C_1 \approx 1, C_2 \approx 0, C_3 \approx 0$ you have
\begin{equation}
 y \approx 1\cdot 1 + 0.5 \cdot 0 + 0 \cdot 0 \approx 1
\end{equation}
when $C_1 \approx 0, C_2 \approx 1, C_3 \approx 0 $ you have
\begin{equation}
 y \approx 1\cdot 0 + 0.5 \cdot 1 + 0 \cdot 0 \approx 0.5
\end{equation}
and when $C_1 \approx 0, C_2 \approx 0, C_3 \approx 1 $ you have
\begin{equation}
 y \approx 1\cdot 0 + 0.5 \cdot 0 + 0 \cdot 1 \approx 0
\end{equation}"
How to perform binary classification when one class is more predominant than the other?,"
Assuming we have big $m \times n$ input dataset, with $m \times 1$ output vector. It's a classification problem with only two possible values: either $1$ or $0$.
Now, the problem is that almost all elements of the output vector are $0$s with a very few $1$s (i.e. it's a sparse vector), such that if the neural network would ""learn"" to give always 0 as output, this would produce high accuracy, while I'm also interested in learning when the 1s occurs.
I thought one possible approach could be to write a custom loss function giving more weight to the 1s, but I'm not completely sure if this would be a good solution.
What kind of strategy can be applied to detect such outliers?
","['machine-learning', 'datasets', 'objective-functions', 'binary-classification', 'imbalanced-datasets']",
"What is a working configuration of a neuronal network (number of layers, lerning rate and so on) for a specific dataset?","
I try to solve some easy functions with a neuronal network (aforge-lib):
This is how I generate the dataset:
const int GesamtAnzahl = 200;
float[,] tempData = new float[GesamtAnzahl, 2];
float minX = float.MaxValue;
float maxX = float.MinValue;

Random rnd = new Random();
var granzen = new List<int>() 
{
    rnd.Next(1, GesamtAnzahl-1),
    rnd.Next(1, GesamtAnzahl-1),
    rnd.Next(1, GesamtAnzahl-1),
    rnd.Next(1, GesamtAnzahl-1),
};
granzen.Sort();

for (int i = 0; i < GesamtAnzahl; i++)
{

    var x = i;
    var y = -1;
    if ((i > granzen[0] && i < granzen[1]) ||
        (i > granzen[2] && i < granzen[3]))
    {
        y = 1;
    }
    tempData[i, 0] = x;
    tempData[i, 1] = y;
}

So this is quite easy: The output is 1 if the input is between the 2 lower random generated ""borders"" or between the 2 higher numbers. Otherwise the output is 1.
The input values are standardices to fit between -1 and 1. So 0 is -1 and 200 is 1.
As a network I used a BackPropagationLearning with a BipolarSigmoidFunction and several configurations like:
Learning Rate: 0,1
Momentum: 0
Sigmoids alpha value: 2
Hidden Layer 1: 4 neurons
Hidden Layer 2: 2 neurons


Learning Rate: 0,1
Momentum: 0
Sigmoids alpha value: 2
Hidden Layer 1: 4 neurons
Hidden Layer 2: 2 neurons
Hidden Layer 3: 2 neurons


Learning Rate: 0,2
Momentum: 0
Sigmoids alpha value: 2
Hidden Layer 1: 4 neurons
Hidden Layer 2: 2 neurons
Hidden Layer 3: 2 neurons

and so on. None of them worked. As described here: https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e it should be enough to have 2 hidden layers. The first one with 4 neurons and the second one with 2. 
The configurations which worked best were:
Learning Rate: 0,01
Momentum: 0
Sigmoids alpha value: 2
Hidden Layer 1: 4 neurons
Hidden Layer 2: 4 neurons
Hidden Layer 3: 4 neurons

Learning Rate: 0,02
Momentum: 0
Sigmoids alpha value: 2
Hidden Layer 1: 4 neurons
Hidden Layer 2: 2 neurons

This solves the problem about 50 % of the times.
As this is a quite simple problem I wonder if I am doing something wrong. I think there has to be a configuration which has better results. 
What is the best configuration for this problem and why?
Additionally I tried:

Having more data does not help. I created 5000 a dataset of 5000 points ( GesamtAnzahl = 5000). Then the networks have a even worse sucess rate.
I tried to add an extra constant input (always 1) to the dataset but this also lowered the sucess rate

",['neural-networks'],
How does the forget layer of an LSTM work?,"
Can someone explain the mathematical intuition behind the forget layer of an LSTM? 
So as far as I understand it, the cell state is essentially long term memory embedding (correct me if I'm wrong), but I'm also assuming it's a matrix. Then the forget vector is calculated by concatenating the previous hidden state and the current input and adding the bias to it, then putting that through a sigmoid function that outputs a vector then that gets multiplied by the cell state matrix.
How does a concatenation of the hidden state of the previous input and the current input with the bias help with what to forget?
Why is the previous hidden state, current input and the bias put into a sigmoid function? Is there some special characteristic of a sigmoid that creates a vector of important embeddings?
I'd really like to understand the theory behind calculating the cell states and hidden states. Most people just tell me to treat it like a black box, but I think that, in order to have a successful application of LSTMs to a problem, I need to know what's going on under the hood. If anyone has any resources that are good for learning the theory behind why cell state and hidden state calculation extract key features in short and long term memory I'd love to read it.
","['neural-networks', 'long-short-term-memory', 'math']","TL;DR Here is a beautiful explanation with diagrams: source.To address:the cell state is essentially long term memory embedding (correct me if I'm wrong)The embedding can be long or short term and it is a vector.To answer:Why is the previous hidden state, current input and the bias put into a sigmoid function? Is there some special characteristic of a sigmoid that creates a vector of important embeddings?Excerpt from source:The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means â€œlet nothing through,â€ while a value of one means â€œlet everything through!â€Formally the forget vector is:$$f_t = \sigma(W_f\cdot[h_{t-1},x_t],+b_f)$$So we see that it is actually a linear operation followed by a non-linear operation, which restricts the values to be between 0 and 1. This is followed by an element-wise operation on the previous cell state. That is we ""gate""/""filter"" the previous cell state doing:$$C_{t-1}\odot f_t,$$where $\odot$ is element-wise multiplication.Thus we see that the forget gate really is acting like a gate: It either lets values pass through or it pushes them toward zero. So, it's purpose is to assign what values to forget/remember.To answer:How does a concatenation of the hidden state of the previous input and the current input with the bias help with what to forget?This concatenation takes into account the ""rolling"" hidden state and the current input. That is, the linear operation results in a new vector which can be seen as a ""consideration embedding"" of the current input and a ""summary"" of past inputs and cell states. The sigmoid then converts this ""consideration embedding"" into the forget vector.In summary, the forget gate has the primary purpose of forming a vector of values between zero and one. This vector results by considering the current input and the previous hidden state. The vector is used to forget/remember parts of the previous cell state via element-wise multiplication. So, no, it is not a black box, it is a highly considerate non-linear operation through time.Additional Intuition - More MathematicalRecall that multiplying a matrix on the right by a column vector produces a vector, which is a linear combination of the columns of the matrix. So each column of $W_f$ can be seen as a column of ""toggles"" that push up to 1 or pull down to 0 (once we apply sigmoid). That is, the columns of $W_f$ form a sort of ""basis"" for a ""toggle space.""When we concatenate to form $[h_{t-1},x_t]$ and then multiply we see that the term $W_f\cdot[h_{t-1},x_t]$ gives consideration to the hidden state and the input. That is, both $h_{t-1}$ and $x_t$ have there hand in toggling the gate.The LSTM has learned from the data how to make this toggling ""meaningful."" Below is a simple pictographic example.That is, $W_f\cdot[h_{t-1},x_t]$ will be a vector in the column space of $W_f$ (ie the ""toggle space""). Thus, the concatenated vector will ""point"" to the best ""toggle"" in the column (""toggle"") space of $W_f$. This ""toggle"" is then transformed into a ""gate"" in the ""gate space.""So intuitively, the range of the forget component is a sort of ""gate space"" where the domain is all vectors of form $[h_{t-1},x_t]$. Formally:$$f_t:[h_{t-1},x_t]\mapsto g_t$$That is to say that the forget gate learns the best mapping from the domain of concatenated hidden/input's to the range of possible gates.Important NoteThe LSTM is only one of many types of forget-gate mappings. The main takeaway is that the LSTM works empirically for many applications.That is a forget gate is an instance of a general ""gate mapping"":$$f_t:v_t \mapsto g_t,$$where $v_t$ could be any number of vectors resulting from any number of concatenations.ConclusionSome data from the past is irrelevant to the current time step. We need a way to correctly ""forget"" irrelevant information. One way to ""forget"" is to use a forget gate mapping (like the one in LSTMs). Then we need to optimize the parameters of that forget gate. One implementation is used in the common LSTM architecture. Finally, the concatenation of the previous hidden state provides contextual information to the current input which is very helpful with forgetting.More Mathematical IntuitionHere is another view of what is happening in the LSTM from a dynamic point of view: video."
Is there a way of automatically drawing bounding boxes around interested objects?,"
Given thousands of images, where some of the images contain target objects and others do not, is there an easy way of drawing bounding boxes on these target objects rather than relying on manual annotation? Wouldn't drawing 4 orientations of an object and their respective bounding boxes and randomly inserting them into the images be a viable option?
It becomes painful to manually annotate thousands of images by yourself.
","['training', 'image-recognition', 'datasets']",
What is the difference between linear and non-linear regression?,"
In machine learning, I understand that linear regression assumes that parameters or weights in equation should be linear. For Example:
$$y = w_1x_1 + w_2x_2$$
is a linear equation where $x_1$ and $x_2$ are feature variables and $w_1$ and $w_2$ are parameters.
Also
$$y = w_1(x_1)^2 + w_2(x_2)^2$$
is also linear as parameters $w_1$ and $w_2$ are linear with respect to $y$.
Now, I read some articles stating that in the equation like
$$y = \log(w_1)x_1 + \log(w_2)x_2$$
can also be made linear by considering other variables $v_1$ and $v_2$ as:
\begin{align}
v_1 &= \log(w_1)\\
v_2 &= \log(w_2)
\end{align}
Thus,
$$y = v_1x_1 + v_2x_2$$
So, in this sense, any non-linear equation can be made linear, then what is non-linear regression here? I think I am missing something important here. I am a beginner in the field of Machine Learning. Can somebody help me?
","['machine-learning', 'comparison', 'linear-regression', 'non-linear-regression']","The difference is simply that non-linear regression learns parameters that in some way control the non-linearity - e.g. any weight or bias that is applied before a non-linear function. For instance:$$y = (w_1 x_1 + w_2 x_2)^2 + w_3$$ With such a function to learn, you cannot separate out transformed values of $w_1$ and $w_2$ and turn this into a linear function of just $x_1$ and $x_2$.   What you are describing as non-linearities in your examples are instead all applied by the machine learning engineer to create new candidate features for linear regression. This is not usually described as non-linear regression, but feature transformation or feature engineering.There is also a kind of middle ground where a central linear algorithm e.g. linear regression, is trained on many variations of the original features, by automated generation and filtering of transformed features. The most general variants of this approach are not hugely popular because they suffer from same risks of overfitting as non-linear models whilst not offering much in the way of improved performance. However, if you narrow down the types of feature and transformation combinations based on some knowledge of how you expect the target function to behave, it leads to many useful variants of linear regression - e.g. regression on fourier transforms, radial basis functions etc."
What is the input for the prior model of VQ-VAE?,"
I'm trying to implement the VQ-VAE model. In there, a continuous variable $x$ is encoded in an array $z$ of discrete latent variables $z_i$ that are mapped each to an embedding vector $e_i$. These vectors can be used to generate an $\hat{x}$ that approximates $x$. 
In order to obtain a reasonable generative model $p_\theta(x)=\int p_\theta(x|z)p(z)$, one needs to learn the prior distribution of the code $z$. However, it is not clear in this paper, or its second version, what should be the input of the network that learns the prior. Is it $z=[z_i]$ or $e=[e_i]$? The paper seems to indicate that it is $z$, but if that's the case, I don't understand how I should encode $z$ properly. For example, a sample of $z$ might be an $n\times n$ matrix with discrete values between $0$ and $511$. It is not reasonable to me to use a one-hot encoding, nor to simply use the discrete numbers as if they were continuous, given that there is no defined order for them. On the other hand, using $e$ doesn't have this problem since it represents a matrix with continuous entries, but then the required network would be much bigger.
So, what should be the input for the prior model? $z$ or $e$? If it is $z$, how should I represent it? If it is $e$, how should I implement the network?
","['machine-learning', 'generative-model', 'variational-autoencoder']",
Why does the denoising autoencoder always returns the same output?,"
I am trying to implement a denoising autoencoder (DAE) to remove noise from 1024-point FFT spectra. I am using two types of spectra: (1) that contain a distinctive high amplitude spectral peak and (2) that contain only noise peaks.
If I understood correctly, I can train the DAE using the corruputed spectra (spectra+noise) and afterwards I can use it the remove noise from new datasets. The problem is that when testing the DAE, it returns the type (1) spectrum mentioned above, regardless of the input. The same case when I apply predict on the training data. This is the code I am using (Python/Tensorflow):
def BuildModel(nInput):
    input_dim = Input(shape = (nInput, ))

    # Encoder Layers
    encoded1 = Dense(896, activation = 'relu')(input_dim)
    encoded2 = Dense(768, activation = 'relu')(encoded1)
    encoded3 = Dense(640, activation = 'relu')(encoded2)
    encoded4 = Dense(512, activation = 'relu')(encoded3)
    encoded5 = Dense(384, activation = 'relu')(encoded4)
    encoded6 = Dense(256, activation = 'relu')(encoded5)
    encoded7 = Dense(encoding_dim, activation = 'relu')(encoded6)

    # Decoder Layers
    decoded1 = Dense(256, activation = 'relu')(encoded7)
    decoded2 = Dense(384, activation = 'relu')(decoded1)
    decoded3 = Dense(512, activation = 'relu')(decoded2)
    decoded4 = Dense(640, activation = 'relu')(decoded3)
    decoded5 = Dense(768, activation = 'relu')(decoded4)
    decoded6 = Dense(896, activation = 'relu')(decoded5)
    decoded7 = Dense(nInput, activation = 'sigmoid')(decoded6)

    # Combine Encoder and Deocoder layers
    autoencoder = Model(inputs = input_dim, outputs = decoded7)

    autoencoder.summary()
    # Compile the Model
    autoencoder.compile(optimizer=OPTIMIZER, loss='binary_crossentropy')
    #autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())

    return autoencoder

X_train, X_test, y_train, y_test = train_test_split(spectra.iloc[:,0:spectra.shape[1]-1], spectra['Class'], test_size=testDatasetSize, stratify=spectra.Class, random_state=seedValue)

X_train, y_train = shuffle(X_train, y_train, random_state=seedValue)
X_test, y_test = shuffle(X_test, y_test, random_state=seedValue)

X_unseen = X_train.to_numpy()[0:1000,:] # Data not used for training, only for testing
y_unseen = y_train.to_numpy()[0:1000]
X_train = X_train.iloc[1000:]
y_train = y_train.iloc[1000:]

# Scaling
maxVal = max(X_train)
X_train = (X_train/maxVal).to_numpy()
X_test = (X_test/maxVal).to_numpy()
X_unseen = (X_unseen/maxVal)#.to_numpy()

# Corrupted data
noise_factor = 0.01
X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_unseen_noisy = X_unseen + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_unseen.shape)

ae = BuildModel(X_train.shape[1])
PrintConsoleLine('Creating model finished')
print('')

history = ae.fit(X_train_noisy, X_train, epochs=NB_EPOCH, batch_size=BATCH_SIZE, validation_data=[X_test_noisy, X_test])
save_model(ae, modelFile, overwrite=True)

# Test
X = X_unseen
X_noisy = X_unseen_noisy
X_denoised = ae.predict(X_noisy) # X_train gives the same result (spectra type (1)) !?!
N = len(X_denoised[0,:])
index = 6
PlotDataSimple(3, np.linspace(0,N-1,N), X[index,:], 'Frequency domain', 'Index', 'Amplitude', None)
PlotDataSimple(4, np.linspace(0,N-1,N), X_noisy[index,:], 'Frequency domain', 'Index', 'Amplitude', None)
PlotDataSimple(5, np.linspace(0,N-1,N), X_denoised[index, :], 'Frequency domain', 'Index', 'Amplitude', None)

Dataset shape: (17000, 65, 65, 1) (files, samples X axis, samples Y axis, class)
Train on 12600 samples, validate on 3400 samples
Epoch 1/3
12600/12600 [==============================] - 26s 2ms/sample - loss: 0.6813 - val_loss: 0.4913
Epoch 2/3
12600/12600 [==============================] - 14s 1ms/sample - loss: 0.1621 - val_loss: 0.0578
Epoch 3/3
12600/12600 [==============================] - 16s 1ms/sample - loss: 0.0230 - val_loss: 0.0169

The results I am getting (Column 1 - Initial signal, Column 2 - Corrupted signal, Column 3 - Denoised signal):

So why does the DAE output the same spectra regardless of the inputs? Am I misunderstanding the DAE principle or is there a problem in my implementation?
","['machine-learning', 'tensorflow', 'python', 'autoencoders']","You are using Dense layers, try 1d convolution instead.
Have you tried a different activation function such as softmax and instead of Binary cross entropy try MSE loss? Are all your inputs between 0 and 1? Also, I think your noise amplitude is too much in 2nd and 3rd case as compared to the actual signal. Can you try training on different types of spectra separately and check the result, if the DAE is learning anything. Also, try trining for more epochs. I am not sure and would have commented this but I can't due to low rep, but I am interested in the solution."
Why are the current means and the old ones the same in this implementation of Elastic Weight Consolidation?,"
I'm trying to re-implement Elastic Weight Consolidation (EWC) as outlined in this paper. As a reference, I am also using this Github repository (another implementation).
My model/idea is pretty straightforward. Train the network to do the bit operation AND (e.g 1 && 0 = 0), then using EWC, train it to use OR (e.g 1 || 0 = 1). I've got three inputs: bit1, bit2 and operation (0 stands for AND and 1 for OR) and one output neuron - the output of the operation. For example, if I have 0 1 0 the ground truth should be 0.
The problem, however, comes when calculating the EWC loss.
def penalty(self, model: nn.Module):
    loss = 0
    for n, p in model.named_parameters():
        _loss = self._precision_matrices[n] * (p - self._means[n]) ** 2
        loss += _loss.sum()
    return loss

I've got two problems:

The current means (p) and the old ones (self._means[n]) are always the same, resulting in multiplication by 0, which completely negates EWC.
As I have just one output neuron the calculation of the fisher's matrix is a bit different than the repo. The one I have written seems to be wrong. Any ideas?

I initialise the self._means[n] and self._precision_matrices (fisher's matrix) in the init method of the EWC model:
class EWC(object):
def __init__(self, model: nn.Module, dataset: list, device='cpu'):

    self.model = model
    self.dataset = dataset
    self.device = device

    self._means = {}
    self._precision_matrices = self._diag_fisher()

    for n, p in self.model.named_parameters():
        self._means[n] = p.data.clone()

def _diag_fisher(self):
    precision_matrices = {}

    # Set it to zero
    for n, p in self.model.named_parameters():
        params = p.clone().data.zero_()
        precision_matrices[n] = params

    self.model.eval()

    for input in self.dataset:
        input = input.to(self.device)

        self.model.zero_grad()

        output = self.model(input)
        label = torch.sigmoid(output).round()
        loss = F.binary_cross_entropy_with_logits(output, label)
        # loss = F.nll_loss(F.log_softmax(output, dim=1), label)
        loss.backward()

        for n, p in self.model.named_parameters():
            precision_matrices[n].data += p.grad.data ** 2 / len(self.dataset)

    precision_matrices = {n: p for n, p in precision_matrices.items()}
    return precision_matrices

And this is the actual training:
# Train the model EWC
for epoch in tqdm(range(EPOCS)):

    # Get the loss
    ls = ewc_train(model, opt, loss_func, dataloader[task], EWC(model, old_tasks), importance, device)

def ewc_train(model: nn.Module, opt: torch.optim, loss_func:torch.nn, data_loader: torch.utils.data.DataLoader, ewc: EWC, importance: float, device):
    epoch_loss = 0

    for i, (inputs, labels) in enumerate(data_loader):
        inputs = inputs.to(device).long()
        labels = labels.to(device).float()

        opt.zero_grad()

        output = model(inputs)
        loss = loss_func(output.view(-1), labels) + importance * ewc.penalty(model)
        loss.backward()
        opt.step()

        epoch_loss += loss.item()

    return loss

Note: the loss function that I am using is nn.BCEWithLogitsLoss() and optimisation is: SGD(params=model.parameters(), lr=0.001).
","['neural-networks', 'pytorch', 'implementation', 'incremental-learning', 'elastic-weight-consolidation']",
What is the most suitable AI technique to use for path planning?,"
I am making a firetruck using Arduino Uno with flame sensors and ultrasonic sensors to detect how to move and where to go. As this is a project for my university, I am asked to implement AI in it for path planning.
I am not sure whether to use something like A* technique or ID3 decision tree or if there is something better than both to implement path planning for my robot. Any suggestions?
","['robotics', 'decision-trees', 'path-planning']",
Are current AI models sufficient to achieve Artificial General Intelligence?,"
I read an interesting essay about how far we are from AGI. There were quite a few solid points that made me re-visit the foundation of AI today. A few interesting concepts arose:

imagine that you require a program with a more ambitious functionality: to address some outstanding problem in theoretical physics â€” say the nature of Dark Matter â€” with a new explanation that is plausible and rigorous enough to meet the criteria for publication in an academic journal.
Such a program would presumably be an AGI (and then some). But how would you specify its task to computer programmers? Never mind that itâ€™s more complicated than temperature conversion: thereâ€™s a much more fundamental difficulty. Suppose you were somehow to give them a list, as with the temperature-conversion program, of explanations of Dark Matter that would be acceptable outputs of the program. If the program did output one of those explanations later, that would not constitute meeting your requirement to generate new explanations. For none of those explanations would be new: you would already have created them yourself in order to write the specification. So, in this case, and actually in all other cases of programming genuine AGI, only an algorithm with the right functionality would suffice. But writing that algorithm (without first making new discoveries in physics and hiding them in the program) is exactly what you wanted the programmers to do!

The concept of creativity seems like the initial thing to address when approaching a true AGI. The same type of creativity that humans have to ask the initial question or generate new radical ideas to long-lasting questions like dark matter.
Is there current research being done on this?
I've seen work with generating art and music, but it seems like a different approach.

In the classic â€˜brain in a vatâ€™ thought experiment, the brain, when temporarily disconnected from its input and output channels, is thinking, feeling, creating explanations â€” it has all the cognitive attributes of an AGI. So the relevant attributes of an AGI program do not consist only of the relationships between its inputs and outputs.

This is an interesting concept behind why reinforcement learning is not the answer. Without input from the environment, the agent has nothing to improve upon. However, with the actual brain, if you had no input or output, it is still in a state of ""thinking"".
","['machine-learning', 'reinforcement-learning', 'agi', 'artificial-creativity']","Task SpecificationIt's been proposed that novelty search may circumvent this problem.  See: Abandoning Objectives: Evolution Through
the Search for Novelty Alone.  In this model, the agent has no goal or objective, but just messes around with the data to see what results.  (This could be regarded as finding/forming patterns.  Here's a recent popular article on the subject: Computers Evolve a New Path Toward Human Intelligence).A form of procedural generation may also be useful, specifically the capability of creating novel models/environments and processes/algorithms to analyze them.  (See: AI-GAs: AI-generating algorithms).In terms of programmers communicating a task to the AGI, that's a natural language problem if the task relates to mundane human activity or art and craft, and a math problem if the subject is physics.  (In the former case, humans are describing the problem in natural language, in the latter, presumably feeding all of the data that suggests dark matter into the algorithm.  Natural language is challenging for computers, but math, along with logic, is one of their two core functions.)  Re: dark matter, it may be a matter of asking the algorithm to find patterns in the data, and build models based on the data. The patterns and models would be the output, which humans could then consider. The output would be mathematical.  (Converting that mathematical output into metaphors, as in common on science programs like Nova and Cosmos, would be another goal of AGI.) Brain in a BoxThere needs to be stimulus/input to initiate ""thought"" process/computation.  In brain in a box, the brain is providing it's own internal stimulus.  I'd argue that an RL algorithm engaged in self-play is not dependent on external stimulus, but internally generated inputs, so that the process of model-based reinforcement learning is often a brain in a box, considering a subject or problem.    "
"Why can't LSTMs keep track of the ""important parts"" of a sequence?","
I keep reading about how LSTMs can't remember the ""important parts"" of a sequence which is why attention-based mechanisms are required. I was trying to use LSTMs to find people's name format. 
For example, ""Millie Bobby Brown"" can be seen as first_name middle_name last_name format, which I'll denote as 0, but then there's ""Brown, Millie Bobby"" which is last_name, first_name middle_name, which I'll denote as 1. 
The LSTM seems to be overfitting to one classification of format. I suspect it's because it's not paying special attention to the comma which is a key feature of what format it could be. I'm trying to understand why an LSTM won't work for a case like this. It makes sense to me because LSTMs are better at identifying sequence to sequence generation and things such as summarization and sentiment analysis usually require attention. I suspect another reason why the LSTM is not able to infer the format is that the comma can be placed in different indexes of the sequence, so it could be losing its importance in the hidden state the longer the sequence is (not sure if that makes sense). Anyone else has any theories? I'm trying to convince my fellow researchers that a pure LSTM won't be sufficient for this problem.
","['neural-networks', 'long-short-term-memory', 'attention']","The problem is not that RNN flavours such as LSTMs are incapable of keeping track of the ""important"" parts of the input. They also do not have much trouble recognizing commas in different places.To prove this point, I recommend reading Andrej Karpathy's excelllent write-up about the behaviour of individual RNN ""neurons"".Addressing specifically this comment in your question:I suspect another reason why the LSTM is not able to infer the format is that the comma can be placed in different indexes of the sequence, so it could be losing its importance in the hidden state the longer the sequence is (not sure if that makes sense).If commas are relevant to solve the task at hand, LSTMs can learn to remember its position or related information. This information is not necessarily diluted by repeated application of reccurrence with long sequences: networks can learn to propagate and promote crucial information from one recurrent state to the next.Input sequences have arbitrary length, which means that LSTMs need to compress information about seen sequence elementsRather, the input sequence has an arbitrary length, while the LSTM state vectors have a fixed size. State vectors are the only way for an LSTM to ""keep track of important parts"". This means that those fixed-size vectors are a bottleneck and there is an information-theoretic upper bound on the amount of information about ""important parts"" that can be kept by an LSTM.LSTMs potentially take multiple decisions. For each decision, something else in the input sequence is most importantFor tasks such as summarization that you mention in the question, an LSTM makes a series of predictions (predicting the tokens of the summary one token at a time). For each prediction, different things in the input sequence might be important. Put another way, for each decision, another view of the input may be most helpful.This is a key motivation for using attention networks. Each time an LSTM is making a prediction, an attention network can provide a dynamic, optimally helpful view of the input sequence."
Training and inference for highly-context-sensitive information,"
What is the best way to train / do inference when the context matters highly as to what the inferred result should be?  
For example in the image below all people are standing upright, but because of the perspective of the camera, their location highly affects their skeletal pose.  If the 2D inferred skeleton of the person on the right were located where the middle person is in pixel space, it should not be considered upright even though it should be considered upright where it is now.
I assume the location would be fed in during both training and inference somehow, but I don't know the names of the techniques that should be used and are there any best practices when doing this type of scenario?

","['deep-learning', 'training', 'features', 'inference']",
How can I write out the Real-TIme Recurrent Learning Gradient equations for a network?,"
This question is about Real-Time Recurrent Learning Gradient on a Recurrent neural network .
How can I write out the RTRL equations for a network ?
Before present an example give let's introduce some notation :
Notation

So the network for which we want to write the RTRL equations is the following :
Network

A similar question can be found here at page 561 for another network .
","['neural-networks', 'recurrent-neural-networks', 'learning-algorithms']",
Are neurons in layer $l$ only affected by neurons in the previous layer?,"
Are artificial neurons in layer $l$ only affected by those in layer $l-1$ (providing inputs) or are they also affected by neurons in layer $l$ (and maybe by neurons in other layers)?
","['neural-networks', 'machine-learning', 'artificial-neuron']","It depends on the architecture of the neural network. However, in general, no, neurons at layer $l$ are not only affected by neurons at layer $l-1$.In the case of a multi-layer perceptron (or feed-forward neural network), only neurons at layer $l-1$ directly affect the neurons at layer $l$. However, neurons at layers $l-i$, for $i=2, \dots, l$, also indirectly affect the neurons at layer $l$.In the case of recurrent neural networks, the output of neuron $j$ at level $l$ can also affect the same neuron but at a different time step.In the case of residual networks, the output of a neuron at a layer $l-i$, for $i=2, \dots, l$, can directly affect the neurons at layer $l$. These non-neighboring connections are called skip connections because they skip layers.There are probably other combinations of connections between neurons at different layers or the same layer. "
Why does the adversarial search minimax algorithm use Depth-First Search (DFS) instead of Breadth-First Search (BFS)?,"
I understand that the actual algorithm calls for using Depth-First Search, but is there a functionality reason for using it over another search algorithm like Breadth-First Search?
","['search', 'minimax', 'breadth-first-search', 'depth-first-search', 'adversarial-search']","The primary reason is that Breadth-First Search requires much more memory (and this probably also makes it a little bit slower in practice, due to time required to allocate memory, jumping around in memory rather than working with what's still in the CPU's caches, etc.). Breadth-First Search needs memory to remember ""where it was"" in all the different branches, whereas Depth-First Search completes an entire path first before recursing back -- which doesn't really require any memory other than the stack trace. This is assuming we're using a recursive implementation for DFS -- which we normally do in the case of minimax.You can clearly see this if you look at pseudocode for the two approaches (ignoring the minimax details here, just presenting pseudocode for straightforward searches):You see that the BFS requires a queue object that explicitly stores a bunch of stuff in memory, whereas DFS doesn't.There's more to the story once you get to extensions of Minimax, like Alpha-Beta pruning and Iterative Deepening... but since the question is just about Minimax, I'll leave it at that for now."
What is exactly the role of commutative property in a Constraint Satisfaction Problem?,"
I have been looking into the backtracking search for CSPs, and understand that if we just plainly do a typical depth-limited search we have a vast tree with leaves size $n!d^n$ where $n$ is the number of variables and $d$ the domain size. It can also be easily understood that there exists instead only $d^n$ complete assignments. So the reason for the the tree being so large is attributed to the fact that we are ignoring the commutative way of variable assignments in CSP. Can anyone please explain, as to how exactly this commutative property affects?
",['constraint-satisfaction-problems'],
"Why the error rates in table3 and table4 are differenct in the paper ""deep residual learning for image recognition""","
Why are the error rates in table 3 and table 4 are different in the paper Deep Residual Learning for Image Recognition (2015).  
They are both error rates on the validation sets by single model. 

Why there are different rates for the same architecture? 

",['deep-learning'],
"How would an AI learn the concept of the words ""repeat twice""?","
In a hypothetical conversation:
Person A - ""Repeat the word 'cat' twice"".
Person B - ""cat cat"".

I'm thinking about how a human or AI can learn the concept of ""repeat twice"". In reinforcment learning it would require that after the first sentence the AI would go through every random sentence until it got it right and hence got a reward. 
Another way might be the AI or human overhearing the conversation. Then on hearing a repetition or a word it may trigger some neurons in the brain related to detecting repetition. Thus by pavlovian learning associate the word ""repeat"" or ""twice"" with these neurons. When given the stimulus of the word ""repeat"" these neurons may get triggered making the brain do some repetition algorithm. (This is my favorite theory).
I suppose a third way might be as follows:
Person A - ""Hello! Hello!""
Person B - ""Stop repeating yourself"".

It might learn to associate repeating with the word ""repeating"" in this way. 
I think either way the brain must have some neurons dedicated to detecting repetitions and possibly inacting them. (I don't think any standard RNN has this capability).
What do you think is the most likely way?
","['machine-learning', 'natural-language-processing', 'learning-algorithms', 'human-computer-interaction']",
Is there a probabilistic version of minimax?,"
How would a probabilistic version of minimax work?
For example, we may choose a move that could result in a very bad outcome, but that outcome might just be extremely unlikely so we might think it would be worth the risk.
","['minimax', 'expectiminimax']","Yes, there is at least one probabilistic version of minimax, which is called expectiminimax. In expectiminimax, in addition to min and max nodes, there are also chance nodes, which perform a weighted sum of the successors, so the probabilities associated with chance nodes must be known. Given that expectiminimax assumes the existence of random events (represented by the chance nodes), the decisions are thus based on expected values. Section 5.5 of the book Artificial Intelligence: A Modern Approach provides a description of the expectiminimax algorithm, which was introduced by Donald Michie in Game-playing and game-learning automata (1966). The paper Optimal strategy in games with chance nodes (2007) also gives a decent description of the expectiminimax algorithm."
What is the difference between tracking and mapping (TAM) and localization and mapping (LAM)?,"
In the paper Visual SLAM algorithms: a survey from 2010 to 2016 by Takafumi Taketomi, Hideaki Uchiyama and Sei Ikeda it is mentioned

It should be noted that tracking and mapping (TAM) is used instead of using localization and mapping. TAM was first used in Parallel Tracking and Mapping (PTAM) [15] because localization and mapping are not simultaneously performed in a traditional way. Tracking is performed in every frame with one thread whereas mapping is performed at a certain timing with another thread. After PTAM was proposed, most of vSLAM algorithms follows the framework of TAM. Therefore, TAM is used in this paper.

I do not quite follow the difference between localization and mapping versus tracking and mapping.

What is the difference?

What are some advantages of TAM?

Why is SLAM not called STAM?


","['computer-vision', 'comparison', 'terminology', 'papers', 'slam']",
How can I prove that all the a-cuts of any fuzzy set A defined on $R^n$ are convex?,"
How can I prove that all the a-cuts of any fuzzy set A defined on $R^n$
are convex if and only if
$$\mu_A(\lambda r + (1-\lambda)s) \geq min \{\mu_A(r), \mu_A(s)\}$$
such that $r, s \in R^n$, $\lambda \in [0, 1]$ ?
That's a fuzzy question on my assignment. Any idea on how to start with?
",['fuzzy-logic'],"A fuzzy set A in $R^n$ is said to be a convex fuzzy set if its
  $\alpha$-cuts $A_\alpha$ are (crisp) convex sets for all $A \in (0,1]$
  .Let A be a convex fuzzy set if and only if for all $r, s \in$ $R^n$, $\lambda \in [0, 1]$ .Let $\alpha=\mu_A\leq\mu_B$Then\begin{equation}
    r\in A_{\alpha}, s\in A_{\alpha}
\end{equation}and also\begin{equation}
    \lambda r + (1-\lambda)s \geq \alpha = min \{\mu_A(r), \mu_A(s)\}
\end{equation}Conversely, if the membership funciton $\mu_A$ of the fuzzy set A satisfies the inequality of Theorem 13.1 Convex fuzzy set, then taking $\alpha=\mu_A(r), A_\alpha$ may be regarded as set of all points $s$ for which $\mu_A(s)\geq\alpha=\mu_A(r)$. Therefore for all $r,s \in A_\alpha$,\begin{equation}
    \mu_A(\lambda r + (1-\lambda)s) \geq min \{\mu_A(r), \mu_A(s)\} = \mu_A(r)=\alpha
\end{equation}which inplies that $\lambda r + (1-\lambda)s \in A_\alpha$. Hence $A_\alpha$ is a convex set for every $\alpha \in [0,1]$"
Can't figure out what's going wrong with my dataset construction for multivariate regression,"
TL;DR: I can't figure out why my neural network wont give me a sensible output. I assume it's something to do with how I'm presenting the input data to it but I have no idea how to fix it.
Background:
I am using matched pairs of speech samples to generate a model which morphs one persons voice into another. There are some standard pre-processing steps which have been done and can be reversed in order to generate a new speech file.
With these I am attempting to generate a very simple neural network that translates the input vector into the output one and then reconstructs a waveform.
I understand what I'm trying to do mathematically but that's not helping me make keras/tensorflow actually do it.
Inputs:
As inputs to my model I have vectors containing Fourier Transform values from the input speech sample matched with their counterpart target vectors.
These vectors contain the FT values from each 25ms fragment of utterance are in the form $[r_1, i_1, ..., r_n, i_n]$ where $r$ is the real part of the number and $i$ is the imaginary one.
I am constructing these pairs into a dataset reshaping each input vector as I do so:
def create_dataset(filepaths):
    """"""
    :param filepaths: array containing the locations of the relevant files
    :return: a tensorflow dataset constructed from the source data
    """"""
    examples = []
    labels = []

    for item in filepaths:
        try:
            source = np.load(Path(item[0]))
            target = np.load(Path(item[1]))

            # load mapping
            with open(Path(item[2]), 'r') as f:
                l = [int(s) for s in list(f.read()) if s.isdigit()]
                it = iter(l)
                mapping = zip(it, it)

            for entry in mapping:
                x, y = entry
                ex, lab = source[x], target[y]
                ex_ph, lab_ph = np.empty(1102), np.empty(1102)

                # split the values into their real and imaginary parts and append to the appropriate array
                for i in range(0, 1102, 2):
                    idx = int(i / 2)

                    ex_ph[i] = ex[idx].real
                    ex_ph[i+1] = ex[idx].imag
                    lab_ph[i] = lab[idx].real
                    lab_ph[i+1] = lab[idx].imag

                examples.append(ex_ph.reshape(1,1102))

                # I'm not reshaping the labels based on a theory that doing so was messing with my loss function
                labels.append(lab_ph)

        except FileNotFoundError as e:
            print(e)

    return tf.data.Dataset.from_tensor_slices((examples, labels))

This is then being passed to the neural network:
def train(training_set, validation_set, test_set, filename):
    model = tf.keras.Sequential([tf.keras.layers.Input(shape=(1102,)),
                                 tf.keras.layers.Dense(551, activation='relu'),
                                 tf.keras.layers.Dense(1102)])

    model.compile(loss=""mean_squared_error"", optimizer=""sgd"")

    model.fit(training_set, epochs=1, validation_data=validation_set)

    model.evaluate(test_set)
    model.save(f'../data/models/{filename}.h5')
    print(model.summary())

and I get out... crackling. Every time, no matter how much data I throw at it. I assume I'm doing something obviously and horribly wrong with the way I'm setting this up.
","['tensorflow', 'python', 'keras', 'speech-synthesis']",
"What do the terms ""front-end"" and ""back-end"" refer to in this article?","
I found the terms front-end and back-end in the article (or blog post) How to Develop a CNN for MNIST Handwritten Digit Classification. What do they mean here? Are these terms standard in this context?
","['convolutional-neural-networks', 'terminology']","I do not think these are formally defined.The distinction is just to facilitate discussion of the NN architecture: e.g., you may have a few convolutional layers with pooling as a front-end, and a different architecture as a back-end (in a text-book architecture, just a fully-connected layer. But to get wild, maybe LSTM? To really get wild, BERT?).In the end (no pun intended), computers do not care if a layer is seen by humans as a front-end or a back-end."
How does reinforcement learning with video data work?,"
My goal is to train an agent to play MarioKart on the Nintendo DS. My first approach (in theory) was to setup an emulator on my pc and let the agent play for ages. But then a colleague suggested to train the agent first on pre recorded humanly played video data, to achieve some sort of base level. And then for further perfection let the agent play for its own with the emulator.
But I have no clue how training with video data works. E.g. I wonder how to calculate a loss since there is no reward. Or am I getting the intuition wrong?
I would appreciate it if someone could explain this technique to me.
",['reinforcement-learning'],"In reinforcement learning, to learn off policy control, you need data on the states, actions and rewards at each time step. If, in addition to a recorded video, you had a recording of controller inputs, and could add reward data by hand, then you could use a standard reinforcement learning method, e.g. DQN. Simply run the DQN training loop as normal, but skip the parts where it acts in the environment, and only train on batches of recorded experience.With only video data, your options are limited. However, it might still be useful, because a significant part of the challenge is a machine vision task. For a DQN agent, it will need to convert frames from the video (e.g. last 4 frames) into a prediction of the different rewards that it could get depending on which controller buttons are pressed. If you can teach a separate neural network to perform a vision task on relevant video data, it may may help. You could use the learned weights from the first layers of this network as the starting point for your Q values network, and it will likely speed up a DQN figuring out the relationship to its predictions. This sort of task switch following learning is called transfer learning, and is often used in computer vision tasks.A possibly useful starting task if you have a video, but no controller or reward data, would be to predict the next frame(s) of the video, given say four starting frames (you need more than one so that the neural network can use velocity information). It should be possible to generate the training data using opencv or ffmpeg from your recordings."
Maximum Single Ply Branching Factor for Legal Checkers Boards,"
I am writing a checkers move generation function in C that will be extended to Python. It is much easier to handle the possible boards in a fixed size array to pass back to Python.
Basically, I build out possible boards and add them to this array:
uint32_t boards[x][3];


Therefore, the optimal value for x should be the maximum single ply branching factor out of all possible legal board states.
I am not sure if I am being very clear, so here is an example:
For tic-tac-toe this value would be 9, as the first move has the greatest number of possible directly resulting board states, out of all of the legal boards.
Has this value been calculated for checkers? Has a program like Chinook derived a reasonably close number?
Thank you for your help! 
","['branching-factors', 'checkers']",
Finding the optimal combination of inputs which return maximal output,"
I am currently working on a problem and now got stuck to implement one of it's steps. This is a simple attempt to explain what I am currently facing, which is something that I am aiming to implement in my regression simulation in python.
Let's say that I fit a non-linear model to my data. Now, I want to find the combination of inputs within a specified range that returns the the highest outcome. When I am using a quadratic function or only a few inputs, this task is quite simple. However, the problem comes when trying to apply the same logic for more complex models. Supposing that I have 9 variables as inputs, I will have to test all possible combinations and that would be computationally unfeasible by doing it with meshgrid if you want to cover a range with a several intervals in between.
So, here it comes my question, is there such a way to avoid having to go through this computationally costly process in order to achieve the combinations of inputs defined in a given range that return the optimal output?
","['machine-learning', 'regression']",
What effect does a negative output of a neuron have on neighbouring neurons?,"
Artificial neural networks are composed of multiple neurons that are connected to each other. When the output of an artificial neuron is zero, it does not have any effect on neighboring neurons. When the output is positive, the neuron has an effect on neighboring neurons. 
What does it mean when the output of a neuron is negative (which can e.g. occur when the activation function of a neuron is the hyperbolic tangent)? What effect would this output have on neighboring neurons? 
Do biological neural networks also have this property?
","['neural-networks', 'artificial-neuron', 'biology']","In the case of artificial neural networks, your question can be (partially) answered by looking at the definition of the operation that an artificial neuron performs. An artificial neuron is usually defined as a linear combination of its inputs, followed by the application of a non-linear activation function (e.g. the hyperbolic tangent or ReLU). More formally, a neuron $i$ in layer $l$ performs the following operation\begin{align}
o_i^l = \sigma \left(\sum_{j=1}^N w_j o_j^{l-1} \right) \tag{1}\label{1},
\end{align}where $o_j^{l-1}$ is the output from neuron $j$ in layer $l-1$ (the previous layer), $w_j$ the corresponding weight, $\sigma$ an activation function and $N$ the number of neurons from layer $l-1$ connected to neuron $i$ in layer $l$.Let's assume that $\sigma$ is the ReLU, which is defined as follows$$
\sigma(x)=\max(0, x)
$$which means that all negative numbers become $0$ and all non-negative numbers become themselves.In equation \ref{1}, if $w_j$ and $o_j^{l-1}$ have the same sign, then the product $w_j o_j^{l-1}$ is non-negative (positive or zero), else it is negative (or zero). Therefore, the sign of the output of neuron $j$ in layer $l-1$ alone does not fully determine the effect on $o_i^l$, but the sign of the $w_j$ is also required. Let's suppose that the product $w_j o_j^{l-1}$ is negative, then, of course, this will negatively contribute to the sum in equation \ref{1}. In any case, even if the sum $\sum_{j=1}^N w_j o_j^{l-1}$ is negative, if $\sigma$ is the ReLU, no matter the magnitude of the negative number, $o_i^l$ will always be zero. However, if the activation function is hyperbolic tangent, the magnitude of a negative $\sum_{j=1}^N w_j o_j^{l-1}$ affects the magnitude of $o_i^l$. More precisely, the more negative the sum is, the closest $o_i^l$ is to $-1$. To conclude, in general, the effect of the sign of an output of an artificial neuron on neighboring neurons depends on the activation function and the learned weights, which depend on the error the neural network is making (assuming the neural network is trained with gradient descent combined with back-propagation), which in turn depends on the training dataset, the loss function, the architecture of the neural network, etc.Biological neurons and synapses are more complex than artificial ones. Nevertheless, biological synapses are usually classified as either excitatory or inhibitory, so they can have an excitatory or inhibitory effect on connected neurons."
How to perform Interpretability analysis toward a simple reinforcement learning network,"
We are currently using a RL network with the following simple structure to train a model which helps to solve a transformation task:
Environment (a binary file) + reward ---> LSTM (embedding)  --> FC layer --> FC layer --> FC layer --> decision (to select and apply a kind of transformation toward the environment from a pool of transformations)
The model will receive a simple reward and also take the input to make the decision. And we have a condition to stop each episode.
So the current workflow, although it is simple, it seems to have learned something and with multiple episode of training, we can observe the accumulated reward for each episode increases. So right now, what we are thinking is to interpret the model, well, a fancy term.
So basically we are thinking to let the model tell us from which component of the Environment (input file), the model somewhat makes the decision to select a transformation to apply. And I have learned a bunch of interpretability articles, which basically use an activation map (e.g., link) to highlight certain components from the input. 
However, the problem is that, we don't have any sort of CNN layer in our simple RL model. In that sense, the aforementioned method cannot apply, right? I also learned a number of techniques from this book, but still, I don't see any specific techniques applicable for RL models.
So here is my question, in terms of our simple RL model, how can we do certain ""interpretability"" analysis and therefore have a better idea on which part of the ""Environment"" leads to each step of decision? Thank you very much.
",['reinforcement-learning'],
How can we design the mutation and crossover operations when the order of the genes in the chromosomes matters?,"
Consider an optimization problem that involves a set of tasks $T = \{1,2,3,4,5\}$, where the goal is to find a certain order of these tasks.
I would like to solve this problem with a genetic algorithm, where each chromosome $C = [i, j, k, l, m]$ corresponds to a specific order of these five tasks, so each gene in $C$ corresponds to a task in $T$.
So, for example, $C = [1,3,5,4,2]$ and $C' = [1,5,4,2,3]$ would be two chromosomes that correspond to two different orders of the tasks.
In this case, how could we design the mutation and cross-over operations so that these constraints are maintained during evolution?
The genetic algorithm should produce the three best chromosomes or order of tasks.
","['genetic-algorithms', 'crossover-operators', 'mutation-operators', 'constrained-optimization', 'chromosomes']",
"Could you share with me the tree size, search time and search depth of your implementation of Gomoku with minimax and alpha-beta prunning?","
Currently, I am working on a Gomoku AI implementation with minimax + alpha-beta pruning.
I'm targeting these two rules from 'acceptable implementation' in terms of search time and search depth :

Search time (over 0.5 seconds is ""bad"", less 0.5 seconds is ok)
Search depth (less than 10 search depth levels is ""bad"", over 10 search depth levels is ok)

The minimax algorithm generates, by recursive function calls, a tree of nodes, each node represented by a function call with a specific game state.
Increasing the depth search increases the number of nodes in the tree, and therefore search time.
There is a compromise between search time and search depth.
Alpha-beta pruning tends to help this compromise by pruning useless nodes search and reducing tree size. The pruning is directly related to the evaluation/heuristic function. Bad implementation of heuristic may lead to bad efficiency of alpha-beta pruning.

If you are working on or have done a Gomoku AI, sharing your stats of tree size, search depth and time search from your implementation at some game steps, and explain how you reach it may help to investigate.

The implementation at this time does not fit the 'is not acceptable' for me, having search time over 1sec for a search depth of 4 at first step ... on IntelCore i7 3.60GHz CPU ...
Here are the properties of the actual implementation:

Board of size 19x19
Implements search window of size 5x5 around stones to reduce search nodes
Implements heuristic computation at each node on the played stone instead of computation on all board size on leaf nodes number.
Implements alpha-beta pruning
No multi thread


Here are the current stats it is reaching for search depth of 4 at the first step:

Timing minimax algorithm: 1.706175 seconds
Number of nodes in that compose the tree: 2850

. . . . . . . . . . . . . . . . . . . 00
. . . . . . . . . . . . . . . . . . . 01
. . . . . . . . . . . . . . . . . . . 02
. . . . . . . . . . . . . . . . . . . 03
. . . . . . . . . . . . . . . . . . . 04
. . . . . . . . . . . . . . . . . . . 05
. . . . . . . . . . . . . . . . . . . 06
. . . . . . . . . . . . . . . . . . . 07
. . . . . . . . . . . . . . . . . . . 08
. . . . . . . . . . . . . . . . . . . 09
. . . . . . . . . . . . . . . . . . . 10
. . . . . . . . . . . . . . . . . . . 11
. . . . . . . . . . . . . . . . . . . 12
. . . . . . . . . . . . . . . . . . . 13
. . . . . . . . . . . . . . . . . . . 14
o x . . . . . . . . . . . . . . . . . 15
. . . . . . . . . . . . . . . . . . . 16
. . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . 18
A B C D E F G H I J K L M N O P Q R S 
Player: o - AI: x

Bad stats might be lead to bad heuristics, causing inefficient pruning. Waiting for other stats/replies to validate this hypothesis may help.
Edit 1
Coming back from a new search campaign on this question.

The implementation was facing a 19*19 loop index at each heuristic computation ... Removed this by heuristic computation at a specific index (not the entire board)

The implementation was facing a 19*19 loop index to check win state ... Removed this by checking only around played index any alignment at each step.

The implementation was facing a 19*19 loop index to check where it can play (even with the windows) ...
Removed by propagating indexes array of valid indexes through the recursion updated at each step.
The array is a dichotomic array (with $O(n)$ insertion, $O(\log n)$ search and $O(1)$ deletion by index)

The implementation was lacking a Zobrist hash table, a very nice idea from the below answer. It is now implemented with unit tests to prove that implementation is working. An array sorted by hash is updated at each new node, with the hash-node association. The array is a dichotomic array (with $O(n)$ insertion, $O(\log n)$ search and $O(1)$ deletion by index)

The implementation is at each step trying each index in a random way (not computation order or evaluation score order).


The before edit example is not great because it is playing on a sideboard and the allowed indexes window is half max size.
Here are the newly obtained performances :

with Zobrist table off and seed at 42 for search depth of 4 at the first step

Timing minimax algorithm: 0.083288 seconds
Number of nodes that compose the tree: 6078



. . . . . . . . . . . . . . . . . . . 00
. . . . . . . . . . . . . . . . . . . 01
. . . . . . . . . . . . . . . . . . . 02
. . . . . . . . . . . . . . . . . . . 03
. . . . . . . . . . . . . . . . . . . 04
. . . . . . . . . . . . . . . . . . . 05
. . . . . . . . . . . . . . . . . . . 06
. . . . . . . . . . . . . . . . . . . 07
. . . . . . . . . . . . . . . . . . . 08
. . . . . . . . . . . . . . . . . . . 09
. . . . . . . . . . . . . . . . . . . 10
. . . . . . . . . x . . . . . . . . . 11
. . . . . . . . o . . . . . . . . . . 12
. . . . . . . . . . . . . . . . . . . 13
. . . . . . . . . . . . . . . . . . . 14
. . . . . . . . . . . . . . . . . . . 15
. . . . . . . . . . . . . . . . . . . 16
. . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . 18
A B C D E F G H I J K L M N O P Q R S
Player: o - AI: x


with Zobrist table on and seed at 42 for search depth of 4 at the first step

Timing minmax_algorithm: 0.434098 seconds
Number of nodes that compose the tree: 9320



. . . . . . . . . . . . . . . . . . . 00
. . . . . . . . . . . . . . . . . . . 01
. . . . . . . . . . . . . . . . . . . 02
. . . . . . . . . . . . . . . . . . . 03
. . . . . . . . . . . . . . . . . . . 04
. . . . . . . . . . . . . . . . . . . 05
. . . . . . . . . . . . . . . . . . . 06
. . . . . . . . . . . . . . . . . . . 07
. . . . . . . . . . . . . . . . . . . 08
. . . . . . . . . . . . . . . . . . . 09
. . . . . . x . . . . . . . . . . . . 10
. . . . . . . . . . . . . . . . . . . 11
. . . . . . . . o . . . . . . . . . . 12
. . . . . . . . . . . . . . . . . . . 13
. . . . . . . . . . . . . . . . . . . 14
. . . . . . . . . . . . . . . . . . . 15
. . . . . . . . . . . . . . . . . . . 16
. . . . . . . . . . . . . . . . . . . 17
. . . . . . . . . . . . . . . . . . . 18
A B C D E F G H I J K L M N O P Q R S
Player: o - AI: x

Actually, it is ok for search depth 4, but not for more than 6. The node number is becoming exponential (over 20 000) ...
Found here great implementation in the same language/techno than can go to 10 depth in less than 1sec, without Zobrist or smart trick, and followed the logic.
The issue must be somewhere else, causing exponential growth of node - inefficient pruning.
","['game-ai', 'minimax', 'alpha-beta-pruning']","Intuitively I kind of doubt expecting a search depth of 10 in half a second is reasonable, especially for the initial game state where there's a rather large branching factor and no immediately-winning moves that help to prune some branches quickly.I've never implemented any Alpha-Beta agents for Gomoku specifically, but I can provide some numbers for our Alpha-Beta implementation in the Ludii General Game System. Note that this is a general game system that implements a wide variety of games in a single game description language. Due to its generality, it's unlikely that any single game runs as efficiently as it would in a highly-optimised game-specific implementation. Therefore, you should consider these numbers to be lower bounds on what you can achieve in a Gomoku-only game-specific implementation.Note that it's very important to take into account the computational cost of your heuristic evaluation function. We're using a somewhat expensive heuristic which computes all potential lines of a length of 5 (because this is appears in the end rules in Gomoku) through all pieces that have been placed so far, as described on pages 82-84 of Automatic generation and evaluation of recombination games (but with a simpler scoring rule than the union of probabilities as described there).CPU:
- Intel Core i5-6500 CPU @ 3.20GHzGame implementation details:Alpha-Beta implementation details:"
Reinforcement Learning on quantum circuit,"
I am trying to teach an agent to make any random 1-qubit state reach uniform superposition. So basically, the full circuit will be State -> measurement -> new_state (|0> if 0, |1> if 1) -> Hadamard gate. It just needs to perform 2 actions. That's all. So it's more of an RL problem rather than QC.
I am using reinforcement learning to train the model but it doesn't seem to learn anything. The reward keeps on decreasing and even after 3 million episodes, the agent doesn't seem to converge anywhere. This is how I am training:
def get_exploration_rate(self, time_step):
        return self.epsilon_min + (self.epsilon - self.epsilon_min)*\
               math.exp(1.*time_step*self.epsilon_decay)

def act(self, data,t): #state
        rate = self.get_exploration_rate(t)
        if random.random() < rate:
            options = self.model.predict(data) #state
            options = np.squeeze(options)
            action =  random.randrange(self.action_size)
        else:
            options = self.model.predict(data) #state
            options = np.squeeze(options)
            action = options.argmax()
        return action, options, rate

def train(self):

        batch_size = 200
        t = 0                   #increment
        states, prob_actions, dlogps, drs, proj_data, reward_data =[], [], [], [], [], []
        tr_x, tr_y = [],[]
        avg_reward = []
        reward_sum = 0
        ep_number = 0
        prev_state = None
        first_step = True
        new_state = self.value
        data_inp = self.data

        while ep_number<3000000:
            prev_data = data_inp
            prev_state = new_state
            states.append(new_state)
            action, probs, rate = self.act(data_inp,t)
            prob_actions.append(probs)
            y = np.zeros([self.action_size])
            y[action] = 1
            new_state = eval(command[action])
            proj = projection(new_state, self.final_state)
            data_inp = [proj,action]
            data_inp = np.reshape(data_inp,(1,1,len(data_inp)))
            tr_x.append(data_inp)
            if(t==0):
                rw = reward(proj,0)
                drs.append(rw)
                reward_sum+=rw

            elif(t<4):
                rw = reward(new_state, self.final_state)
                drs.append(rw)
                print(""present reward: "", rw)
                reward_sum+=rw
            elif(t==4):
                if not np.allclose(new_state, self.final_state):
                    rw = -1
                    drs.append(rw)
                    reward_sum+=rw
                else:
                    rw = 1
                    drs.append(rw)
                    reward_sum+=rw

            print(""reward till now: "",reward_sum)
            dlogps.append(np.array(y).astype('float32') * probs)
            print(""dlogps before time step: "", len(dlogps))
            print(""time step: "",t)
            del(probs, action)
            t+=1
            if(t==5 or np.allclose(new_state,self.final_state)):                         #### Done State
                ep_number+=1
                ep_x = np.vstack(tr_x) #states
                ep_dlogp = np.vstack(dlogps)
                ep_reward = np.vstack(drs)
                disc_rw = discounted_reward(ep_reward,self.gamma)
                disc_rw = disc_rw.astype('float32')
                disc_rw -= np.mean(disc_rw)
                disc_rw /= np.std(disc_rw)

                tr_y_len = len(ep_dlogp)
                ep_dlogp*=disc_rw
                if ep_number % batch_size == 0:
                  input_tr_y = prob_actions - self.learning_rate * ep_dlogp
                    input_tr_y = np.reshape(input_tr_y, (tr_y_len,1,6))

                    self.model.train_on_batch(ep_x, input_tr_y)
                    tr_x, dlogps, drs, states, prob_actions, reward_data = [],[],[],[],[],[]
                env = Environment()
                new_state = env.reset()
                proj = projection(state, self.final_state)
                data_inp = [proj,5]
                data_inp = np.reshape(data_inp,(1,1,len(data_inp)))
                print(""State after resetting: "", new_state)
                t=0

I have tried various things like changing the inputs, reward function, even added exploration rate. I have assigned max time step as 5 even though it should complete in just 2. 
What am I doing wrong? Any suggestions?
","['neural-networks', 'reinforcement-learning', 'python', 'policy-gradients']",
What is the difference between principal component analysis and singular value decomposition in image processing?,"
What is the difference between principal component analysis and singular value decomposition in image processing? Which one performs better, and why?
","['computer-vision', 'comparison', 'image-processing', 'principal-component-analysis', 'singular-value-decomposition']",
"Why don't people always use TensorFlow Lite, if it doesn't decrease the accuracy of the models?","
I have been exploring edge computation for AI, and I came across multiple libraries or frameworks, which can help to convert the model into a lite format, which is suitable for edge devices.

TensorFlow Lite will help us to convert the TensorFlow model into TensorFlow lite.
OpenVino will optimise the model for edge devices.

Questions

If we have a library to optimise the model for edge devices (e.g. TensorFlow Lite), after conversion, could it make the accuracy decrease?

If not, then why do people prefer don't always use e.g. TensorFlow Lite?


","['deep-learning', 'tensorflow', 'accuracy', 'efficiency']","I have explored the AI for edge devices. My findings for tflite model.Quantization dramatically reduces both the memory requirement and
  computational cost of using neural networks.Answer in brief:"
How are batch statistics computed in Recurrent Batch Normalization?,"
I'm implementing recurrent BN per this paper in Keras, but looking at it and those citing it, a detail remains unclear to me: how are batch statistics computed? Authors omit explicit clarification, but state (pg. 3) (emphasis mine):

At training time, the statistics E[h] and Var[h] are estimated by the sample mean and sample variance of the current minibatch

Yet another paper (pg. 3) using and citing it describes:

We subscript BN by time (BN_t) to indicate that each time step tracks its own mean and variance. In practice, we track these statistics as they change over the course of training using an exponential moving average (EMA)

My question's thus two-fold:

Are minibatch statistics computed per immediate minibatch, or as an EMA? 
How are the inference parameters, shared across all timesteps, gamma and beta computed? Is the computation in (1) simply averaged across all timesteps? (e.g. average EMA_t for all t)


Existing implementations: in Keras and TF below, but are all outdated, and am unsure regarding correctness

Keras, TF-A, and TF-B
All above agree that during training, immediate minibatch statistics are used, and that beta and gamma are updated as an EMA of these minibatches
Problem: the bn operation (in A, and presumably B & C) is applied on a single timestep slice, to be passed to the K.rnn control flow for re-iteration. Hence, EMA is computed w.r.t. minibatches and timesteps - which I find questionable:


EMA is used in place of a simple average when population statistics are dynamic (e.g. minibatch-to-minibatch), whereas we have access to all timesteps in a minibatch prior having to update gamma and beta
EMA is a worse but at times necessary alternative to a simple average, but per above, we can use latter - so why don't we? Timestep statistics can be cached, averaged at the end, then discarded - holds also for stateful=True


","['tensorflow', 'keras', 'recurrent-neural-networks', 'long-short-term-memory', 'batch-normalization']",
Implementing Logic Inference with Deep Learning,"
Short question
How can I implement Logic Inference with Deep Learning?
Long question
Based on Symbolic Logic, chaining multiple predicates (a short example is Syllogism) is a method of implementing Logic Inference. This programmatic way is suitable for many cases, however, if combined with NLP, too much programmatic way is involved.
If someone implements that Logic Inference with not programmatic way but machine learning, what model should be adopted, and what label data should be input to the model?
","['deep-learning', 'logic']",
Optimizer effects on neural network with two outputs,"
I'm confused about the following issue. Let assume that we have a neural network that takes one input and two outputs. I try to visualize my model like as follows:
        / --- First stream    --- > output_1
Input --
        \ ---- Second stream  ---> output_2

I used sgd with momentum. Is there any difference between using one optimizer for both streams and using two optimizers for each stream? In other words, if i use one optimizer, can one stream optimization process affect another stream? If it can, How can it be possible?
","['neural-networks', 'convolutional-neural-networks', 'optimization']",
Reinforcement learning without trajectories [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



Does it make sense to use Reinforcement Learning methods in an environment that does not have trajectories?
I have a lot of states and actions in my environment. However, there are no trajectories.
If the agent takes action $a$ in the state $s$, it reaches $s'$. In $s'$, it takes action $a'$ and reaches state $s''$. However, if it takes the reverse order of actions $a'$ and then $a$, it would reach the same state $s''$.
How reinforcement learning methods handle with this?
",['reinforcement-learning'],
Are there any strategies that would help me visualize the 'behavior space' and make a novelty function?,"
In â€œAbandoning Objectives: Evolution through the Search for Novelty Aloneâ€, it is explained how the novelty search is a function that is domain specific, depending on the differing behaviors that can potentially emerge.
The primary test is a deceptive maze and it seems like they define novelty as a function that is dependent on each actor's ending position as a distance from other actors' ending position.
I am wanting to try implementing this on some tasks. Some simple AI tasks such as playing pong, or recreating MarI/O, or sticking them in an arena as an actor who can move, turn, and shoot (with other actors in the arena with them).
I have a really hard time thinking of how to model the behavior functions for these kinds of instances without making it into an objective.
For pong, I imagine I could determine novelty by the AI's point score, but isn't this basically making the score an objective since it can only go up? For MarI/O, I've seen some implementations that look at the list of unique grid locations that Mario visited in what order, but I didn't come up with that myself.
For the arena example, my first impulse is to have a score based on how long the actor survived and how many other actors the AI eliminated; but again, this can only go up and seems to me like it is defining an objective.
Are there any strategies or ways to think about the problems that would help me better visualize the 'behavior space' and make a better novelty function?
","['genetic-algorithms', 'evolutionary-algorithms', 'novelty-search']","I think that the best approach is to ""switch point of view"" from the general, objective-oriented, Genetic Algorithm's behaviours.Usually GAs rely on individualism: the best survives. To do this you have to define what 'best' means and this is done through a fitness function that measures something objective, independent from the individual (i.e. a score, classical example).   If you want to measure novelty, collectivism becomes more important, as the whole population (present and past) must be considered. You have to think about measuring how different the individual X did from 'the rest' (i.e. location never reached before).Another interesting point is that you should think in a more multidimensional way: a score is uni-dimensional, everyone wants to get higher, that is easy. A location in space is more difficult to achieve and that is why novelty becomes interesting.  To sum up: stay multidimensional, always treat the population as a multi-organism rather than focusing on the single individual. I hope this helps :)"
What are the properties of a model that is well suited for for high performance real-time inference,"
What are general best practices or considerations in designing a model that is optimized for real-time inference?
","['ai-design', 'models', 'performance', 'gpu', 'real-time']",
Can sequence-to-sequence models be used to convert source code from one programming language to another?,"
Sequence-to-sequence models have achieved good performance in natural language translation. Could these models also be applied to convert source code written in one programming language to source code written in another language? Could they also be used to convert JSON to XML? Has this been done?
There are plenty of models that generate source code (which looks like real source code) using RNNs, although the generated source code doesn't make logical sense. I haven't been able to find any models or examples that take valid existing code and convert it into different valid code.
","['deep-learning', 'sequence-modeling', 'machine-translation']",
Which AI methods are most appropriate for login face recognition?,"
I want to make a face authentication application. I need to approve the face during the login based on whether the registered face and the login face match. 
Which are the possible appropriate AI methods or technologies for this task?
","['machine-learning', 'computer-vision', 'reference-request', 'facial-recognition']",
Imposing contraints on sequence of image classifications,"
Are there example implementations of networks that apply constraints across sequences of image classifications where class labels are ordinal numbers?  For example, to cause the output of a CNN to monotonically increase across frames, where the number may increase either more or less steeply but only in one direction across the entire sequence, or as another example, to smoothly vary rather than jumping precipitously from frame to frame. In my first example, the output can jump quickly from one frame  to the next, as long as only in one direction, whereas in my second example, they can either increase or decrease as long as not too ""fast"" from one frame to the next as if being passed through a low pass boxcar filter. The first is a monotonicity constraint and the second is a smoothness constraint, but in both examples, the key is for adjacent frames to have an effect on the conclusions for a given frame. 
Thank you,
Andy
","['convolutional-neural-networks', 'recurrent-neural-networks', 'optimization', 'convergence']",
Giving an AI a purpose to talk,"
I am trying to teach my AI to talk. The problem is I'm struggling to find a good scenario in which it needs to.
Some ideas I had were:
""Describe a geometric scene"" - Then together with a parser we could see how close the generated instructions came to the official geometric language.
""Give another AI instructions of where to find some food"" e.g. ""Go straight on passed the box then turn left until you get to the tree. Look under the rock.""
Another one might be ""Find out more information about a scene by asking questions of another AI in order to navigate a scene blindfolded"". This is quite an extreme example!
I need it to talk in formal English sentences (not some kind of made up secret langauge.) 
Basically instead of just interpreting a language and following instructions, I want my AI to generate instructions.
So the things I want to teach it are the following:

Ability to ask questions + ability to use the information gathered
Ability to give instructions

Do you know of any projects like this?
","['natural-language-processing', 'algorithm', 'generative-model']",
Are there any approaches other than deep learning to deal with unexpected questions in a question answering system?,"
I'm working on a question answering bot as my graduation project. The main concept is having a text file with many sentences, and building a question answering bot which answers a user's question based on the text file in hand.
Until now, I used tf-idf and cosine similarity and the results are somewhat satisfactory. The main problem is, if the user was to ask a question that doesn't have a word that is in the text file, my bot can't deduce what to bring back as an answer. For example, if I have a sentence in my text file that says ""I have a headache because my heart rate is low"", if the user was to ask ""Why do you have a headache?"", my bot chooses the correct sentence, but if he asked ""What's wrong with you?"" my bot doesn't know what to do.
All I've seen on the web until now are deep learning methods and neural networks, such as LSTM and such. I was wondering if there are any pure NLP approaches to go with my requirements.
","['deep-learning', 'natural-language-processing', 'chat-bots', 'question-answering']",
"If you have a very distorted image, would affine transformations applied to images make object detection algorithms make more mistakes?","
If you have a very distorted video/image, would affine transformations of the images make object detection algorithms make more mistakes compared to a normal camera?
","['convolutional-neural-networks', 'computer-vision', 'object-detection', 'affine-transformations']",
Train a competitive layer on nonnormalized vectors using LVQ technique,"
How can we train a competitive layer on non-normalized vectors using LVQ technique ?
an example is given below from Neural Network Design (2nd Edition) book
The net input expression for LVQ networks calculates the distance between the input
and each weight vector directly, instead of using the inner product. The result is that the
LVQ network does not require normalized input vectors. This technique can also be
used to allow a competitive layer to classify non-normalized vectors. Such a network is
shown in figure below.

Use this technique to train a two-neuron competitive layer on the (non-normalized)
vectors below, using a learning rate $\alpha=0.5$
$p_1=\begin{bmatrix}
1 \\
1
\end{bmatrix}, p_2=\begin{bmatrix}
-1 \\
2
\end{bmatrix}, p_3=\begin{bmatrix}
-2 \\
-2
\end{bmatrix}$
Present the vectors in the following order : $p_1, p_2, p_3, p_2, p_3, p_1$
Initial weights : $W_1=\begin{bmatrix}
0 \\
1
\end{bmatrix}, W_2=\begin{bmatrix}
1 \\
o
\end{bmatrix}$
","['neural-networks', 'machine-learning', 'convolutional-neural-networks']","We have 2 classes , 1 subclass for each class\begin{equation}
    W^2=\begin{vmatrix}
1 & 0\\ 
0 & 1\\
\end{vmatrix}
\end{equation}$p_1$:\begin{equation}
    \alpha^1=compet(n^1)=compet\begin{vmatrix}
||w_1-p_1||\\ 
||w_2-p_1||\\
\end{vmatrix} = compet\begin{vmatrix}
||\begin{vmatrix}
0 & 1\\ 
\end{vmatrix}^T-\begin{vmatrix}
1 & 1\\ 
\end{vmatrix}^T||\\ 
||\begin{vmatrix}
1 & 0\\ 
\end{vmatrix}^T-\begin{vmatrix}
1 & 1\\ 
\end{vmatrix}^T||\\
\end{vmatrix}=
compet(\begin{vmatrix}
1\\ 
1\\
\end{vmatrix}) = \begin{vmatrix}
1\\ 
0\\
\end{vmatrix})
\end{equation}\begin{equation}
   \alpha^2=W^2\cdot\alpha^1= \begin{vmatrix}
1 & 0\\ 
0 & 1\\
\end{vmatrix}\begin{vmatrix}
1\\ 
0\\
\end{vmatrix}=\begin{vmatrix}
1\\ 
0\\
\end{vmatrix}
\end{equation}\begin{equation}
    W_1(1) = W_1(0) + \alpha\cdot(p_1-W_1(0))=\begin{vmatrix}
0\\ 
1\\
\end{vmatrix}+0.5\cdot(\begin{vmatrix}
1\\ 
1\\
\end{vmatrix}-\begin{vmatrix}
0\\ 
1\\
\end{vmatrix})=\begin{vmatrix}
0\\ 
1\\
\end{vmatrix}+\begin{vmatrix}
0.5\\ 
0\\
\end{vmatrix}=\begin{vmatrix}
0.5\\ 
1\\
\end{vmatrix}
\end{equation}$p_2$ :\begin{equation}
    \alpha^1=compet(n^1)=compet\begin{vmatrix}
||w_1-p_2||\\ 
||w_2-p_2||\\
\end{vmatrix} = compet\begin{vmatrix}
||\begin{vmatrix}
0.5 & 1\\ 
\end{vmatrix}^T-\begin{vmatrix}
1 & 2\\ 
\end{vmatrix}^T||\\ 
||\begin{vmatrix}
1 & 0\\ 
\end{vmatrix}^T-\begin{vmatrix}
1 & 2\\ 
\end{vmatrix}^T||\\
\end{vmatrix}=
compet(\begin{vmatrix}
1.8027756377\\ 
2.8284271247\\
\end{vmatrix}) = \begin{vmatrix}
0\\ 
1\\
\end{vmatrix})
\end{equation}\begin{equation}
   \alpha^2=W^2\cdot\alpha^1= \begin{vmatrix}
1 & 0\\ 
0 & 1\\
\end{vmatrix}\begin{vmatrix}
0\\ 
1\\
\end{vmatrix}=\begin{vmatrix}
0\\ 
1\\
\end{vmatrix}
\end{equation}wrong class\begin{equation}
    W_2(1) = W_2(0) - \alpha\cdot(p_2-W_2(0))=\begin{vmatrix}
1\\ 
0\\
\end{vmatrix}-0.5\cdot(\begin{vmatrix}
-2\\ 
2\\
\end{vmatrix}=\begin{vmatrix}
2\\ 
-1\\
\end{vmatrix}
\end{equation}$p_3$ :\begin{equation}
    \alpha^1= compet\begin{vmatrix}
||\begin{vmatrix}
0.5 & 1\\ 
\end{vmatrix}^T-\begin{vmatrix}
-2 & 2\\ 
\end{vmatrix}^T||\\ 
||\begin{vmatrix}
2 & -1\ 
\end{vmatrix}^T-\begin{vmatrix}
-2 & 2\\ 
\end{vmatrix}^T||\\
\end{vmatrix}=
compet(\begin{vmatrix}
70\\ 
5\\
\end{vmatrix}) = \begin{vmatrix}
1\\ 
0\\
\end{vmatrix})
\end{equation}\begin{equation}
   \alpha^2=W^2\cdot\alpha^1\begin{vmatrix}
1\\ 
0\\
\end{vmatrix}
\end{equation}wrong class\begin{equation}
    W_1(2) = W_1(1) - \alpha\cdot(p_3-W_1(1))=\begin{vmatrix}
0.5\\ 
0\\
\end{vmatrix}-0.5\cdot(\begin{vmatrix}
-2.5\\ 
1\\
\end{vmatrix}=\begin{vmatrix}
1.75\\ 
0.5\\
\end{vmatrix}
\end{equation}$p_2$ :\begin{equation}
    \alpha^1=compet(n^1)= compet\begin{vmatrix}
||\begin{vmatrix}
1.75 & 0.5\\ 
\end{vmatrix}^T-\begin{vmatrix}
-1 & 2\\ 
\end{vmatrix}^T||\\ 
||\begin{vmatrix}
2 & -1\ 
\end{vmatrix}^T-\begin{vmatrix}
-1 & 2\\ 
\end{vmatrix}^T||\\
\end{vmatrix}=
compet(\begin{vmatrix}
3.13\\ 
4.24\\
\end{vmatrix}) = \begin{vmatrix}
1\\ 
0\\
\end{vmatrix})
\end{equation}\begin{equation}
   \alpha^2=W^2\cdot\alpha^1\begin{vmatrix}
1\\ 
0\\
\end{vmatrix}
\end{equation}\begin{equation}
    W_1(3) = W_1(2) - \alpha\cdot(p_2-W_1(2))=\begin{vmatrix}
1.75\\ 
0.5\\
\end{vmatrix}+0.5\cdot(\begin{vmatrix}
-2.75\\ 
1.5\\
\end{vmatrix}=\begin{vmatrix}
0.375\\ 
1.25\\
\end{vmatrix}
\end{equation}$p_3$ :\begin{equation}
    \alpha^1=compet(n^1)= compet\begin{vmatrix}
||\begin{vmatrix}
0.375 & 1.25\\ 
\end{vmatrix}^T-\begin{vmatrix}
-2 & 2\\ 
\end{vmatrix}^T||\\ 
||\begin{vmatrix}
2 & -1\ 
\end{vmatrix}^T-\begin{vmatrix}
-2 & 2\\ 
\end{vmatrix}^T||\\
\end{vmatrix}=
compet(\begin{vmatrix}
2.95\\ 
5\\
\end{vmatrix}) = \begin{vmatrix}
1\\ 
0\\
\end{vmatrix})
\end{equation}\begin{equation}
   \alpha^2=W^2\cdot\alpha^1\begin{vmatrix}
1\\ 
0\\
\end{vmatrix}
\end{equation}wrong class\begin{equation}
    W_1(4) = W_1(1) - \alpha\cdot(p_3-W_1(3))=\begin{vmatrix}
0.375\\ 
1.25\\
\end{vmatrix}-0.5\cdot(\begin{vmatrix}
-2.375\\ 
0.75\\
\end{vmatrix}=\begin{vmatrix}
1.5625\ 
0.875\\
\end{vmatrix}
\end{equation}p_1 :\begin{equation}
    \alpha^1=compet(n^1)= compet\begin{vmatrix}
||\begin{vmatrix}
1.5625 & 0.875\\ 
\end{vmatrix}^T-\begin{vmatrix}
1 & 1\\ 
\end{vmatrix}^T||\\ 
||\begin{vmatrix}
2 & -1\ 
\end{vmatrix}^T-\begin{vmatrix}
1 & 1\\ 
\end{vmatrix}^T||\\
\end{vmatrix}=
compet(\begin{vmatrix}
0.57\\ 
2.23\\
\end{vmatrix}) = \begin{vmatrix}
1\\ 
0\\
\end{vmatrix})
\end{equation}\begin{equation}
   \alpha^2=W^2\cdot\alpha^1\begin{vmatrix}
1\\ 
0\\
\end{vmatrix}
\end{equation}\begin{equation}
    W_1(5) = W_1(4) - \alpha\cdot(p_1-W_1(4))=\begin{vmatrix}
1.5625\\ 
0.875\\
\end{vmatrix}+(\begin{vmatrix}
-0.28125\\ 
0.0625\\
\end{vmatrix}=\begin{vmatrix}
1.28125\ 
0.9375\\
\end{vmatrix}
\end{equation}"
Good model and training algorithm to store texture data for fast gpu inference,"
Now, the following may sound silly, but I want to do it for my better understanding of performance and implementation of GPU inference for a set of deep learning problems.
What I want to do is to replace a surface texture for a 3d model by a NN that stores that texture data in some way and allows to infere the rgb color of an arbitrary texel from its UV coordinates. So basically it should offer the same functionality as the texture itself.
A regular texture lookup takes a UV coordinate and returns the (possibly filtered) RGB color at these texture coordinates.
So, I want to train a network that takes two floats in [0,1] range as input and outputs three floats of rgb color.
I further want to then train that network to store my 4096x4096 texture. So the training data I have available are 4096*4096=16777216 of <float2, float3> pairs
Finally I want to evaluate the trained network in my (OpenGL 4 or directX11) pixel shader, feeding it for every rendered pixel the interpolated UV coordinates at this pixel and retrieving the RGB value from it.
It's clear that this will

have lower fidelity than just using the texture directly
use likely more memory than just using the texture directly
be slower than using the texture directly

and as such may be silly to do, but I'd still like to try to do this somewhat optimally, especially in terms of inference performance (I'd like to be able to run it at interactive framerates at 1080p resolutions).
Can someone point me to a class of networks or articles or describe a model and training algorithm that would be well suited for this task (especially in terms of implementing inference for the pixel shader)?
",['gpu'],
What is the difference between Inductive Learning and Connectionist Learning?,"
According to what we know about inductive and connectionist learning, what is the difference between them ?
For those who do not know about :
Inductive Learning, like what we have in decision tree and make a decision based on amount of samples
Connectionist Learning, like what we have in artificial neural network
","['neural-networks', 'decision-trees']","All of the statistical learning is about inductive learning.Inductive learning is about identifying patterns from examples. It is more related to statistics. Connectionist learning is more about finding a common pattern and predicting as well as self-learning(learning from the experience of prediction).Connectionist learning is where learning occurs by modifying connection strengths based on experience. This is not the case with inductive learning. In inductive learning, we are not modifying things based on experience. Inductive learning just finds common patterns, not self-learning based on experience.Learning requires both practice and rewardsIn inductive learning, we learn the model from raw data (so-called training set), and in the deductive learning, the model is applied to predict the behaviour of new data. Connectionist Learning is a group of inductive learning and deductive learning."
Can a neural network be used to predict a sequence of integers based on dataset of previously produced random numbers?,"
What i really want to do, is to predict an integer sequence of (5 numbers with values from 1 to 50) for example based on a big dataset of other 5 numbers sequences with same values range created by the same random number generator. I suppose there is a way to train based on the dataset and the program will find a pattern or based on the most common numbers predict the next number sequence. The more numbers will predict in the sequence correctly the better of course. Any help, directions and preferably python code would be greatly appreciated. 
I recently read the following  can-a-neural-network-be-used-to-predict-the-next-pseudo-random-number and i am new to the AI field. The proposed code while it creates a sequence of 25 numbers it ends showing 20 numbers i do not understand why. It seems they try to do something similar if i understand correctly
I tried The code here can-a-neural-network-be-used-to-predict-the-next-pseudo-random-number
It shows always the same numbers no matter how many epochs and or iterations i do is that normal?
Is the last code close to what i want to accomplish?
Thanks in advance.
","['neural-networks', 'machine-learning', 'deep-learning', 'prediction', 'randomness']",
Is it useful to train a CycleGAN in a supervised setting?,"
I am very interested in the application of CycleGANs. I understand the concept of unpaired data and it makes sense to me.
But now a question comes to my mind: what if I have enough paired image data, is then a CycleGAN an over-engineering, if I use it in a ""supervised"" setting (input matches with the label - but still a CycleGAN)? For what kind of application could it be useful? Would it be more useful to process it using a ""normal"" supervised setting?
So, basically, my question is whether it is useful to train a CycleGAN in a supervised setting?
","['deep-learning', 'applications', 'generative-adversarial-networks']",
"How to define the ""Pre-Processing"" in machine learning?","
Is every process (such as data acquisition, splitting the data for validation, data cleaning, or feature engineering) that is done on the data before we train the model always called the pre-processing part? Or are there some processes that are not included?
","['machine-learning', 'definitions', 'data-preprocessing', 'data-science', 'data-mining']","Data preprocessing consists of all those techniques used to generate the final datasets (with an appropriate size, structure, and format) for the machine learning algorithms or models. Data acquisition should not be part of data preprocessing, but the step preceding it, which gathers the raw data (which may e.g. be noisy).The book Data Preprocessing in Data Mining (2014), by Salvador GarcÃ­a et al., which provides a good overview of the data preprocessing techniques and their connection with data mining and machine learning algorithms and models, defines data preprocessing as follows.Data preprocessing includes data preparation, compounded by integration,
  cleaning, normalization and transformation of data; and data reduction tasks; such as feature selection, instance selection, discretization, etc. The result expected after a reliable chaining of data preprocessing tasks is a final dataset, which can be considered correct and useful for further data mining algorithms.From page 10 onwards, there is a description and categorization of the main data preprocessing techniques. I will just list them, so refer to the book for a definition and explanation of each of these techniques.Here are two screenshots (from the cited book) that illustrate some of the data preparationand data reduction techniques."
How can I predict the nutrients in dishes given the ingredients used to prepare them?,"
I want to know which algorithm will work most efficiently for calculating nutrients present in a food dish if I am giving the ingredients used in the food. Basically, let us assume that I want to make a health status for a person A based on the intake of food and based on it create a diet for him.
","['machine-learning', 'ai-design', 'applications', 'prediction']",
What's the value of making the RL agent's output stochastic opposed to deterministic?,"
I have a question about a reinforcement learning problem. 
I'm training an agent to add or delete pixels in a [12 x 12] 2D space (going to be 3D in the future).
Its action space consists of two discrete outputs: x[0-12] and y[0-12].
What would be the value of instead outputting a (continuous) probabilistic output representation, like the [12 x 12] space with each pixel as a probability, and sampling from it. E.g. a softmax function applied to 144 (12*12) output nodes.
",,
How can we compute the gradient of max pooling with overlapping regions?,"
While studying backpropagation in CNNs, I can't understand how can we compute the gradient of max pooling with overlapping regions.
That's also a question from this quiz and can be also found on this book.
","['convolutional-neural-networks', 'backpropagation', 'gradient', 'pooling', 'max-pooling']","When gradients in a neural network can follow multiple paths to same parameter, the different gradient values from the sources can often be added together, because the operations in the forward direction are also sums and $\frac{d}{dx}(y+z) = \frac{dy}{dx} + \frac{dz}{dx}$. That is the case already with gradients of kernels (which are sums over the image area), and is equally the case for overlapping aggregation, including maximums, minimums or averages.So in the 1d case, if you have a max pool over the input params $[a_0, a_1, a_2, a_3, a_4]$ a max function $m_0 = max(a_0, a_1, a_2)$, $m_1 = max(a_2, a_3, a_4)$ which overlap at $a_2$, and gradients $\nabla_{\mathbf{m}} J = [\frac{\partial J}{\partial m_0}, \frac{\partial J}{\partial m_1}]$, then you would allocate those gradients to vector $\mathbf{a}$ according to which items in each group was the max of that group, adding them when they overlapped.Examples:If $\mathbf{a} = [3,0,1,2,0]$ and  $\nabla_{\mathbf{m}}J = [0.7, 0.9]$, then $\nabla_{\mathbf{a}}J = [0.7, 0, 0, 0.9, 0]$If $\mathbf{a} = [3,0,4,2,0]$ and  $\nabla_{\mathbf{m}}J = [0.7, 0.9]$, then $\nabla_{\mathbf{a}}J = [0, 0, 1.6, 0, 0]$"
Ghost camera or video overlays for example in sports,"
Secondary camera, ghost overlay, video merge... I do not know if what I mean has a more specific name.
I wonder if this is a thing. This could be insightful for example in racing sports where participants race one after another e.g. alpine skiing, downhill mountain bike, showjumping etc. E.g. comparing the current starter to the leader.
Given the camera position is fixed and only the camera angle and zoom is varying to focus on the current starter, the tasks to be able to overlay videos would be to:

match the timing, i.e. both videos start when the timer starts
align and overlay the videos according to specific marker points. Keypoint detection and tracking.
get the opacity right so that both videos are visible

My question is if there is any research on this. If so, what keywords do I need to search for?

Edit:
My seach led me to SIFT (Scale Invariant Feature Transform) and SURF (Speeded-Up Robust Features). Feature matching should be possible with kNN or brute force. A lot can be done with OpenCV.
",['computer-vision'],
How to draw bounding boxes for gender classification?,"
I wonder what is the better way of drawing rectangles on images for gender classification. My task is to create a classifier (CNN based) to detect gender from pictures of entire bodies (not just faces). When I started labeling pictures I noticed that I am not sure whether I should draw it around an entire person like example 1 (including hands and legs and some background space between them) or just the inner part like example 2 (where there is almost no background), in order to achieve better results?
Example 1

Example 2

","['convolutional-neural-networks', 'classification', 'computer-vision']",
How to use BERT as a multi-purpose conversational AI?,"
I'm looking to make an NLP model that can achieve a dual purpose. One purpose is that it can hold interesting conversations (conversational AI), and another being that it can do intent classification and even accomplish the classified task. 
To accomplish this, would I need to use multimodal machine learning, where you combine the signal from two models into one? Or can it be done with a single model?
In my internet searches, I found BERT, developed by Google engineers (although apparently not a Google product), which is an NLP model trained in an unsupervised fashion on 3.3 billion words or more and seems very capable. 
How can I leverage BERT to make my own conversational AI that can also carry out tasks? Is it as simple as copying the weights from BERT to your own model?
Any guidance is appreciated.
","['natural-language-processing', 'classification', 'bert', 'language-model']",
How to compute the number of weights of a CNN?,"
How can we theoretically compute the number of weights considering a convolutional neural network that is used to classify images into two classes:

INPUT: 100x100 gray-scale images.
LAYER 1: Convolutional layer with 60 7x7 convolutional filters (stride=1, valid
padding).
LAYER 2: Convolutional layer with 100 5x5 convolutional filters (stride=1, valid
padding).
LAYER 3: A max pooling layer that down-samples Layer 2 by a factor of 4 (e.g., from 500x500 to 250x250)
LAYER 4: Dense layer with 250 units
LAYER 5: Dense layer with 200 units
LAYER 6: Single output unit

Assume the existence of biases in each layer. Moreover, the pooling layer has a weight (similar to AlexNet)
How many weights does this network have?
Here would be the corresponding model in Keras, but note that I am asking for how to calculate this with a formula, not in Keras.
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Conv2D, MaxPooling2D

model = Sequential()
model.add(Conv2D(60, (7, 7), input_shape = (100, 100, 1), padding=""same"", activation=""relu"")) # Layer 1
model.add(Conv2D(100, (5, 5), padding=""same"", activation=""relu"")) # Layer 2
model.add(MaxPooling2D(pool_size=(2, 2))) # Layer 3
model.add(Dense(250)) # Layer 4
model.add(Dense(200)) # Layer 5

model.summary()

","['neural-networks', 'convolutional-neural-networks', 'keras', 'filters', 'convolution-arithmetic']","Calculating the number of parameters in a CNN is very straightforward.A CNN is composed of different filters, which are essentially 3d tensors. CNN weights are shared, meaning they are used multiple times and reused in different locations. Each layer has $n$ tensors, each with dimension $w \times h \times c$, where $w$ = width, $h$ = height, $c$ = channels (the input channel size). Therefore, the number of parameters of a convolutional layer is $w * h * c * n$. There is also a bias for each output channel, so the number of biases is $n$. At the end the parameter number is calculated with: $n * w * h * c + n$. See more about this in hear: ArticleThe pooling layer does not have weights, it only has hyperparameters. You may have confused the two. There are hyperparameters for the stride, the factor and etc. These are predefined and not trainable.For Keras, you can use the solution here."
How can we prove that an autoassociator network will continue to perform if we zero the diagonal elements of a weight matrix?,"
How can we prove that an auto-associator network will continue to perform if we zero the diagonal elements of a weight matrix that has been determined by the Hebb rule? In other words, suppose that the weight matrix is determined from $W = PP^T- QI$, where $Q$ is the number of prototype vectors. 
I have been given a hint: show that the prototype vectors continue to be eigenvectors of the new weight matrix.
This is a question from Neural Network Design (2nd Edition) book by
Martin T. Hagan, Howard B. Demuth, Mark H. Beale, Orlando De Jesus .
Resource : E7.5 p 224-225
","['neural-networks', 'machine-learning', 'learning-algorithms', 'hebbian-learning']","As we have an autoassociative network prototype vectors are both input and output vectors. So , we have that :
\begin{equation}
    T = P
\end{equation}\begin{equation}
    W = PP^T - QI = TP^T - QI = \sum_{q=1}^Q p_q p_q^T - QI
\end{equation}Applying a prototype vector as input :\begin{equation}
    \alpha = W \cdot p_k = \sum_{q=1}^Q p_k p_q p_q^T - QIp_k
\end{equation}Because they are orthogonal we have :
\begin{equation}
    \alpha = p_k(p_k^T\cdot p_k) - Q \cdot I \cdot p_k = p_k (p_k^T\cdot p_k - Q \cdot I ) = p_k (\textbf{R - Q $\cdot$ I})
\end{equation}where $R-Q\cdot I$ is the length of vectorsSo since ,\begin{equation}
    W \cdot p_k= (R - Q \cdot I) \cdot p _k
\end{equation}
prototype vectors
continue to be eigenvectors of the new weight matrix .It is often the case that for auto-associative nets, the diagonal weights (those which connect an input component to the corresponding output component) are set to 0. There are papers that say this helps learning. Setting these weights to zero may improve the net's
ability to generalize or may increase the biological
plausibility of the net. In addition, it is necessary if we use iterations (iterative nets) or the delta rule is used"
Recognition of lines in a chalkboard,"
I'm trying to develop a real-time application that, from the sequence of chalkboard images captured by a webcam, recognizes the lines being draw on it. 
It must be able of recognize the lines from the chalkboard background, filter the presence in the image of the teacher, and translate these lines to some representation, something as a list of basic events like ""start of line at xxx,xxx"", ""continue line at xxx,xxx"", ...
After several days looking for references and bibliography, none is found. The most similar are the character recognition applications, in particular when they have a stroke recognition stage.
Any hint ?
Input will be a sequence as this one,  this one or this one (just without the presence of the students). I've expect the teacher not hidding his hand. We could imagine a start with an empty chalkboard.
Thanks.
Note: I am looking for more than an answer which says only something similar to ""you can use a deep learning training it with two classes"", without details or references. 
","['object-detection', 'image-processing']",
How to train image segmentation task with only one class?,"
Is there a neural network that has architecture optimizations for segmenting only one class (object and background)? I have tried U-net but it is not providing good enough results.
I am wondering if this can be due to the fact that my dataset has different image resolutions/aspect ratios. 
","['neural-networks', 'convolutional-neural-networks', 'classification', 'image-segmentation', 'u-net']",
Why cannot an AI agent adjust the reward function directly?,"
In standard Reinforcement Learning the reward function is specified by an AI designer and is external to the AI agent. The agent attempts to find a behaviour that collects higher cumulative discounted reward. In Evolutionary Reinforcement Learning the reward function is specified by
the agentâ€™s genetic code and evolved in simulated Darwinian evolution over multiple generations.
Here too the AI agent cannot directly adjust the reward function and instead adjusts its behaviour
towards collecting higher rewards. Why do both approaches prevent the AI agent from changing its reward function at will? What happens if we do allow the AI agent to do so?
","['reinforcement-learning', 'rewards']",
Anyone familiar with Bilateral Recommendation System? And suggest any related papers?,"
I'm working on Bilateral Recommendation System. But not able to find much related papers. Could anyone suggest any papers relative? 
Thanks
","['deep-learning', 'recommender-system']",
How can I avoid displaying the velocity in the updated tracklets?,"
In target tracking, the dimensions of objects change, especially if they are detected using a LIDAR sensor. Also, the static objects in consecutive frames are not 100% static, their position changes a little bit, due to the point cloud segmentation algorithms (which is somehow expected). 
After I associate a tracklet (which maintains the objects previous dimension) and a measurement (that has changed its dimension in the current frame) and perform a Kalman update, a small velocity is induced in the new updated tracklet, even if my object is static (I am considering the reference point of the object and tracking its center).
Is there any solution for not inducing and displaying such a velocity in the updated tracklets?
","['computer-vision', 'autonomous-vehicles', 'robotics']",
Why is reinforcement learning not the answer to AGI?,"
I previously asked a question about How can an AI freely make decisions?. I got a great answer about how current algorithms lack agency.
The first thing I thought of was reinforcement learning, since the entire concept is oriented around an agent getting rewarded for performing a correct action in an environment. It seems to me that reinforcement learning is the path to AGI.
I'm also thinking: what if an agent was proactive instead of reactive? That would seem like a logical first step towards AGI. What if an agent could figure out what questions to ask based on their environment? For example, it experiences an apple falling from a tree and asks ""What made the Apple fall?"". But it's similar to us not knowing what questions to ask about say the universe.
","['reinforcement-learning', 'philosophy', 'agi', 'chinese-room-argument', 'artificial-curiosity']","Some AI researchers do think RL is a path to AGI, and your intuition about how an agent would need to be proactive in selecting actions to learn about is exactly the area these researchers are now focused on.Much of the work in this area is focused on the idea of curiosity, and since 2014 this idea has gained a lot of traction in the research community.So, maybe RL can lead to AGI. We don't know for sure yet.However, many of the classic arguments against AGI aren't addressed by the RL approach. For instance, if like Searle, you think computers just don't have the right kind of hardware to do thinking, then running an RL algorithm on that hardware isn't going to yield AGI, just ever increasingly robust narrow AI. Ultimately Searle's arguments get into issues of metaphysics, so it isn't clear that there exists any argument that would convince someone like Searle that a particular computer-based technique is AGI-capable.There are also other arguments. For example, the cognitivist school of thought thinks that statistical learning approaches to AI, and, in particular, the black-box approaches of statistically-driven RL, are unlikely to lead to general intelligence because they do not engage in the kind of systematic reasoning process that proponents of cognitivism assume is necessary for general intelligence. Some more extreme proponents of this school might say that a logical planning algorithm like STRIPS is innately more intelligent than any approach based on deep learning, because it involves sound logical deduction rather than mere statistical calculation. In particular, STRIPS can correctly generalize to any new domain, as long as it is fed the correct sense data, while an RL approach will need to learn how to act there.So, while there are definitely reasons to be optimistic about RL as a direction for achieving AGI, it's definitely not yet settled."
When exactly is a model considered over-parameterized?,"
When exactly is a model considered over-parameterized?
There are some recent researches in Deep Learning about the role of over-parameterization toward generalization, so it would be nice if I can know what exactly can be considered as such.
A hand-wavy definition is: over-parameterized model is often used to described when you have a model bigger than necessary to fit your data.
In some papers (for example, in A Convergence Theory for Deep Learning
via Over-Parameterization), over-parameterization is described as:

they have much more parameters than the number of training samples
meaning that the number of neurons is polynomially large comparing to the input size
the network width is sufficiently large: polynomial in $L$, the number of layers, and in $n$, the number of samples

Shouldn't this definition depend on the type of input data as well?
For example, I fit:

1M-parameters model on 10M samples of 2 binary features, then it should not be over-parameterized, or

1M-parameters model on 0.1M samples of 512x512 images, then is over-parameterized, or

the model in the paper Exploring the Limits of
Weakly Supervised Pretraining ""IG-940M-1.5k ResNeXt-101 32Ã—48d"" with 829M parameters, trained on 1B Instagram images, is not over-parameterized


","['deep-learning', 'definitions', 'convergence', 'generalization']",
How to detect patterns in salary distribution if we are suspecting malicious distribution based on employee's region?,"
We are having suspects in salary distribution in our organisation due to employee's region. The data we have is as the following:
Name
Region
Work Position (4 main positions)
Salary
Gender

What technique should we use in Machine learning to check and detect malicious salary distribution? By using clustering?
","['machine-learning', 'pattern-recognition', 'clustering']",
Would AI be appropriate for converting unstructured text into an XML?,"
I need to understand whether it is better to use AI algorithms (ML, DL, etc.) instead of the classic parser (based onto grammars with regular expression and automaton) for the following task: structuring in XML an unstructured plain text. 
The text is a legal document, so the structure is well defined and a classic parser could do a good job.
In the case AI could be a viable way, what would be an appropriate approach for the task?
","['machine-learning', 'deep-learning']","The question confuses ML (including DL) with AI. AI is a bigger field than ML and includes rule-based systemsYou probably need to extract entities (spans of text) from the unstructured text and embed them into an XML. ML (and DL) are good when the problem is fuzzy (you need very many rules to solve the problem) so it could be a valid option if you have a variety of document structures that each needs its own set of rules. You would need enough data to train your models in this case. Otherwise if you have limited document structures, very limited data (maybe none) and 100% accuracy is expected then going with rules is the obvious choice. "
What is the correct input shape for my LSTM network?,"
My professor gave us a workshop where we have to do classification of a dataset of ECG signals between healthy and unhealthy types using LSTM. Each signal consists of 1,285 time steps.
What my prof did was to cut up each signal into segments 24 time steps long, advancing 6 steps for the next segment. In other words, for the following signal
0, 1, 2, ... 1283, 1284, 1285

It will be cut up into the following segments
0, 1, 2, ... 21, 22, 23
6, 7, 8, ... 27, 28, 29
12, 13, 14, ... 33, 34, 35
...

These segments are the input to the LSTM model for each signal to be classified.
Using the code that my prof used to cut the signal into segments, and feeding that into Tensorflow-Keras InputLayer, it tells me that the output shape is (None, 211, 24).
However, I am told by a classmate that the correct implementation for Tensorflow-Keras LSTM should be (None, 24, 211). He tried clarifying with the prof but it seems the prof doesn't really understand what my classmate's trying to say. I tried to Google but examples I can find online are either of the two cases below:

The format of an input signal should be (None, # timesteps, # features). However, in my problem the signal only has one feature that is chopped up into segments. In this example I found online, there's no mention of segments.
The only example of signals being cut into segments I can find is when each segment is a single input. That is not the case for me. My input is a single signal, cut up into 211 segments which collectively make up a single input.

Which is the correct output shape for my LSTM model? My prof used his method and achieved 96% classification accuracy, and our assignment is to surpass that rate, but when I tested using what my classmate said is the correct shape, my LSTM model with the exact same architecture, hyperparameters etc. gave me a flat 74.04% accuracy from the 1st epoch all the way to the end without ever changing. So which is wrong?
","['neural-networks', 'tensorflow', 'keras', 'recurrent-neural-networks', 'long-short-term-memory']",
Is the training of multi-version of the same system at the same time affecting the results?,"
I'm using DQN to train multi-version of the same system and there is a small difference when I run them both separately. However, my result suddenly dropped in both versions if I run them both at the same time.  I tried again but I got the same results with slightly different.  Would it be affecting my results if I run multi-version of my system at the same time?
Is there any explanation for that and How can I get accurate results when  I train multi-version of the same system at the same time? 
","['dqn', 'deep-rl']",
CSP Formulation of an algebraic problem,"

Is anyone able to explain how to do this? I'm not looking for the complete answer, I would settle for a ""how to for dummies"" explanation of how this is supposed to be solved. 
I understand constraints, but in the first example it would seem to me that the second half of the first partial assignment $x_2=-1$ is a violation of the constraint 2, that says $x_1 > x_2$... when down below it says both $x_1$, and $x_2$, are $-1$
",['linear-algebra'],
How can I detect diagram region and extract (crop) it from a research paper [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 3 years ago.







                        Improve this question
                    



How can I detect diagram region and extract(crop) it from a research paper

","['machine-learning', 'deep-learning', 'object-detection', 'optical-character-recognition']",I will consider that you need to extract(crop) the digram from the pdf research paper. You can use PyPDF2 or PyMuPDF to extract the images from the PDF file and then you can apply machine learning to do recognition and classification of the images. There are different types of machine learning solutions for image classification and you can start with Convolutional Neural Network and you can start here.For more information for Recognition and classification of figures  see these  and to extract of figures from scholarly documents read this  
Most suitable model for video classification with a fixed camera,"
Consider a fixed camera that records a given area. Three things can happen in this area: 

No action
People performing action A
People performing action B

I want to train a model to detect when action B happens. A human observer could typically recognize action B even with a single frame, but it would be much easier with a short video (a few seconds at low FPS).
What are the most suitable models for this task? I read this paper where different types of fusion are performed in order to feed different frames to a CNN. Are there better alternatives?
","['convolutional-neural-networks', 'video-classification']",
Can a character-level Seq2Seq setup learn to perfectly reconstruct structured data like name strings?,"
If not perfect, how well can they do? For example, if I give the Seq2Seq setup a name it did not see in the training process, can it output the same name without error?
Example
name = ""Will Smith""
output = DecoderRNN(EncoderRNN(name))
can_this_be_true = name == output

","['natural-language-processing', 'recurrent-neural-networks', 'sequence-modeling']",
How are data assimilation and machine learning different?,"
This might seem like a really silly question, however I have not been able to find any answers to it on the internet. 
From my rough understanding of data assimilation, it combines data with numerical models by having weights on the adjustment initial condition parameters? That sounds really similar to what machine learning/neural network does.
What are the distinct differences?
",['machine-learning'],
"Are most things generally discovered because they work empirically and later justified mathematically, or vice-versa?","
In the original GloVe paper, the authors discuss group theory when coming up with the equation (4). Is it possible that the authors came up with this model, figured out it was good, and then later found out various group theory justifications that justified it? Or was it discovered sequentially as it is described in the paper?
More generally: In AI research, are most things discovered because they work empirically and later justified mathematically, or is it the other way around?
","['research', 'papers', 'glove']",
Why does the ELBO come to a steady state and the latent space shrinks?,"
I'm trying to train a VAE using a graph dataset. However, my latent space shrinks epoch by epoch. Meanwhile, my ELBO plot comes to a steady state after a few epochs.
I tried to play around with parameters and I realized, by increasing the batch size or training data, this happens faster, and ELBO comes to a steady state even faster.
Is this a common problem, with a general solution?
With these signs, which part of the algorithm is more possible to cause the issue? Is it an issue from computing loss function? Does it look like the decoder is not trained well? Or it is more likely for the encoder not to have detected features that are informative enough?

Edit:
I figured out that the problem is probably caused by the loss function. My loss function is a combination of the KL term and reconstruction loss. In the github page for graph auto-encoders, it is suggested that the loss function should include normalization factors according to the number of nodes in the graph. I haven't figured it out exactly, but by adding a factor of 100 to my reconstruction loss and a factor of 0.5 to my KL loss, the algorithm is working fine. I would appreciate it if someone can expand on how this exactly is supposed to be set up.
","['variational-autoencoder', 'graphs', 'evidence-lower-bound']",
Can a deep neural network be trained to classify an integer N1 as being divisible by another integer N2?,"
So Iâ€™ve been working on my own little dynamic architecture for a deep neural network (any number of hidden layers with any number of nodes in every layer) and got it solving the XOR problem efficiently. I moved on to trying to see if I could train my network on how to classify a number as being divisible by another number or not while experimenting with different network structures and have noticed some odd things. I know this is a weird thing to try and train a neural network to do but I just thought it might be easy because I can simply generate the training data set and test data set programmatically.
From what Iâ€™ve tested, it seems that my network is only really good at identifying whether or not a number is divisible by a number who is a power of 2. If you test divisibility by a power of two, it converges on a very good solution very quickly. And it generalizes well on numbers outside of the training set - which I guess it kind of makes sense, as Iâ€™m inputting the numbers into the network in binary representation, so all the network has to learn is that a number n is only divisible by 2^m if the last m digits in the binary input vector are 0 (i.e. fire the output neuron if the last m neurons on the input layer don't fire, else don't). When checking divisibility by non-powers of two, however, there does not seem to be as much of a ""positional"" (maybe that's the word, maybe not) relationship between the input bits and whether or not the number is divisible.  I thought though, that if I threw more neurons and layers at the problem that it might be able to solve classifying divisibility by other numbers â€“ but that is not the case. The network seems to converge on not-so-optimal local minima on the cost function (for which I am using mean-squared-error) when dividing by numbers that are not powers of 2. Iâ€™ve tried different learning rates as well to no avail. 
Do you have any idea what would cause something like this or how to go about trying to fix it? Or are plain deep neural networks maybe just not good at solving these types of problems?
Note: I should also add that I've tried using different activation functions for different layers (like having leaky-relu activation for your first hidden layer, then sigmoid activation for your output layer, etc.) which has also not seem to have made a difference
Here is my code if you feel so inclined as to look at it: https://github.com/bigstronkcodeman/Deep-Neural-Network/blob/master/Neural.py
(beware: it was all written from scratch by me in the quest to learn so some parts (namely the back-propagation) are not very pretty - I am really new to this whole neural network thing)
","['neural-networks', 'machine-learning', 'classification']","There is a recent development in research that was looking into effectiveness of neural networks on arithmetic. Interestingly, feed-forward neural networks (MLPs) with various activation functions as well as LSTMs (RNNs which are Turing-complete) are not able to model simple arithmetic operations (e.g. addition/multiplication), they designed a new logic unit which can solve all of the simple arithmetic problems.See: Neural Arithmetic Logic UnitsMore recently, DL can solve symbolic maths: Deep Learning for Symbolic Mathematics"
Relationship between model complexity (depth) and dataset size,"
I'm new to deep learning. I was wondering what's the relationship between a deep model complexity (e.g. total number of parameters, or depth) and the dataset size? 
Assuming I want to do a binary classification with 10K data for a problem like fire detection. How should I know what complexity I should go for?
","['machine-learning', 'deep-learning', 'classification', 'training', 'deep-neural-networks']",
Regression using neural network,"
I'd like to ask for any kind of assistance regarding the following problem:
I was given the following training data: 100 numbers, each one is a parameter, they together define a number X(also given).This is one instance,I have 20 000 instances for training.Next, I have 5000 lines given, each containing the 100 numbers as parameters.My task is to predict the number X for these 5000 instances.
I am stuck because I only know of the sigmoid activation function so far, and I assume it's not suitable for cases like this where the output values aren't either 0 or 1.
So my question is this : What's a good choice for an activation function and how does one go about implementing a neural network for a problem such as this?
","['neural-networks', 'deep-learning', 'backpropagation', 'regression']",
What makes GAN or VAE better at image generation than NN that directly maps inputs to images,"
Say a simple neural network's input is a collection of tags (encoded in some way), and the output is an image that corresponds to those tags. Say this network consists of some dense layers and some reverse (transpose) convolution layers.
What is the disadvantage of this network, that directs people to invent fairly complicated things like GANs or VAEs?
","['convolutional-neural-networks', 'generative-adversarial-networks', 'image-generation', 'variational-autoencoder']","The only disadvantage and difference between these generative models and the method you describe, is the input. You describe inputting tags, where as for a GAN, or VAE, the generation segment of the model takes in some representation of a probability distribution. For a GAN, it's mostly random noise, and for a VAE it is some latent space (see nbros answer).Your described method prevents the network from properly learning fluid generation. If you have a discrete input, the network will attempt to perform a sort of classification on the input, rather then generation, and so when you try to generate a new image, you will most likely get the image equivalent of gibberish.In fact, that's why a standard auto-encoder (not variational) doesn't work very well for generation. You would think that you could feed in your own custom input into the latent space:But if you tried this, you would end up telling the network to try and generate something from a latent space it can't interpret.Hence where the ""variational"" part of the VAE comes in. This helps the network learn to generate from a continuous distribution, so no matter what input you use, the network will be able to interpret it and give an appropriate output.As for a GAN, it is simply fed random noise at each training step, so it too generates based on a continuous distribution.If you were to try and train your method of generation, I would predict that you would find an average of all images that share similar tags (say you have the tags of cat, dog and brown haired, if you input ""dog=1, cat=0, brown haired=1"" you would get an average of all brown haired dogs), but if you tried to input a combination of tags the network has not seen, as it has not learnt from a continuous distribution, the resultant image would not be anything like what you'd expect from those tags."
Why is dropout favoured compared to reducing the number of units in hidden layers?,"
Why is dropout favored compared to reducing the number of units in hidden layers for the convolutional networks?
If a large set of units leads to overfitting and dropping out ""averages"" the response units, why not just suppress units?
I have read different questions and answers on the dropout topic including these interesting ones, What is the ""dropout"" technique? and this other Should I remove the units of a neural network or increase dropout?, but did not get the proper answer to my question.
By the way, it is weird that this publication A Simple Way to Prevent Neural Networks from Overfitting (2014), Nitish Srivastava et al., is cited as being the first on the subject. I have just read one that is from 2012:
Improving neural networks by preventing co-adaptation of feature detectors.
","['neural-networks', 'machine-learning', 'deep-learning', 'dropout', 'regularization']","Dropout only ignores a portion of units during a single training batch update.  Each training batch will use a different combination of units which gives them the best chance of that portion of them working together to generalize. Note the weights for each unit are kept and will be updated during the next batch in which that unit is selected. During inference, yes, all units are used (with a factor applied to activation...the same factor that defines the fraction used)...this becomes essentially an ensemble of all the different combinations of units that were used.Contrasted with fewer units, the fewer units approach will only learn what those fewer units can be optimized for.  Think of dropout as an ensemble of layers of fewer units.(with the exception that there are partial weight sharing between the layers)"
Using U-NET for image semantic segmentation,"
I'm getting literally crazy trying to understand how U-NET works. Maybe it is very easy, but I'm stuck (and I have a terrible headache). So, I need your help.
I'm going to segment MRI to find white matter hyperintensities. I have a dataset with MRI brain images, and another dataset with the WMH. For each one of the brain images, I have one black image with white dots on it in the WMH dataset. These white dots represent where is a WMH on its corresponding brain image.
This is an image from the MRI brain images:

And this is the corresponding WMH image from the WMH dataset:

How can I use the other images in network validation?
I suppose there will be a loss function and this network is supervised learning.
","['convolutional-neural-networks', 'objective-functions', 'architecture', 'image-segmentation', 'u-net']",
How can I recognise the name of a molecule given an image of its structure?,"
I want to recognize the name of the chemical structure from the image of the chemical structure. For example, in the image below, it is a benzene structure, and I want to recognize that it is benzene from the image (I should be able to recognize all these structures as benzene).
How can I recognize the name of a molecule given an image of its structure?

","['machine-learning', 'object-recognition', 'optical-character-recognition']",
Why do current models use multiple normalization layers?,"
In most current models, the normalization layer is applied after each convolution layer. Many models use the block $\text{convolution} \rightarrow \text{batch normalization} \rightarrow \text{ReLU}$ repeatedly. But why do we need multiple batch normalization layers? If we have a convolution layer that receives a normalized input, shouldn't it spit out a normalized output? Isn't it enough to place normalization layers only at the beginning of the model?
","['deep-learning', 'convolutional-neural-networks', 'batch-normalization']",
Which neural network should I use to transform the pixels of a video overtime?,"
I want to train a network with video data and have it transform pixel values overtime on an input video. This is for an art project and does not need to be super elaborate, but the videos I want to render out of this might be big in resolution and frame count. 
Which neural network would be appropriate for this task? I think I'm looking for a convolutional network (but I'm not too sure of that either). Which framework could easily allow me to do this? 
Now, I'm no proper programmer, but self-learned on the go. I know some Javascript, but rather would like to learn more Python. Ideally, the easier and simpler the better though: I would be perfectly happy with something like ""Uber Ludwig"" (except maybe that it's from Uber).
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'resource-request']",
How does a transformer leverage the GPU to be trained faster than RNNs?,"
How does a transformer leverage the GPU to be trained faster than RNNs?
I understand the parameter space of the transformer might be significantly larger than that of the RNN. But why does the transformer structure can leverage multiple GPUs, and why does that accelerate its training?
","['natural-language-processing', 'training', 'transformer', 'attention', 'gpu']",
Should I consider mean or sampled value for action selection in ppo algorithm?,"
When considering the policy network in PPO algorithm, we need to fit a Gaussian distribution to the neural network output (for a continuous action space problem). When I use this network to obtain action, should I sample from the fitted distribution or directly consider the mean output from the policy network? Also, what should be the standard deviation of the fitted distribution?
","['reinforcement-learning', 'policy-gradients', 'proximal-policy-optimization']",
What are some common heuristics that might be innate?,"
Here's a question I might ask an AI to solve:
""Colour the states of the USA using just 4 colours"".

Now, a common heuristic a human might use is to start at one state and ""work their way out"". Or start at an edge state. Now this seems to work best rather than just colouring states in a random order like a computer might do. And it means a human is often better than a computer because a computer might just start colouring random states and get into trouble very quickly.
(Also I wonder if this is a learned heuristic or would a child develop this on his/her own?)
Now the question is, whether this heuristic is an innate optimisation strategy, or just laziness on the part of the human. i.e. colouring things close together takes less effort. Either way it leads to a good strategy.
But I wonder if there are any other examples of heuristics that humans inately use without realising it, that lead to good strategies?
One heuristic that computers often don't know is 
""If you're trying to play a game don't keep turn around and go the other way for no reason."" 

Again, a human would not do this, but again this could be laziness on the part of a human. It takes more effort to turn around than keep going in one direction to explore it.
","['human-like', 'heuristics']",
"Is there an algorithm for ""contextual recognition"" with probabilities?","
Example 1
An object is composed of 3 sub-objects. 

Object 1: 90% looks like an eye 10% looks like a wheel
Object 2: 50% looks like an eye 50% looks like a wheel
Object 3: 90% looks like a mouth 10% looks like a roof

OK. So now we want to determine what the whole-object is. Using this evidence maybe we find:

Combined Object: 90% looks like a face 10% looks like an upside-down car.

But now, given this, we go back an reclassify the sub-objects.

Given whole object is a car object 2 is 99% an eye.

I'm looking for an algorithm that sort of goes back and forth between the context and the subobjects to classify both an object and it's parts. 
(This is related to the rabbit-duck illusion, where such an algorithm once decided something is a rabbit, it classifies it's parts as rabbit-parts).
In other words the algorithm needs to calculate conditional probabilities P(A|B) but the B depends on what all the A's are! So it's a feed back loop.
Example 2
There is a word ""funny"". The sub-letters are classified (wrongly) as F U M Y. It guesses that the word is FUNNY and then goes back and tries to reclassify the letters. Using this reclassification it is even more certain the word is FUNNY. And much more certain the letters in the middle are NN and not M.
(Perhaps later the word is used in a sentence ""The fumes from this fire are really fumy"". And then using this new evidence it has to go back and reclassify the word again and now it thinks the word is FUMY with 80% probability).
I have an idea that I would write all the condition probabilities in a matrix $M$ then gives starting probabilities $S_0$. Then iterate like this: $S_{n+1}=M S_n$ until hopefully it converges on something.
","['classification', 'algorithm', 'probability']",
How can an AI freely make decisions?,"
Suppose a deep neural network is created using Keras or Tensorflow. Usually, when you want to make a prediction, the user would invoke model.predict. However, how would the actual AI system proactively invoke their own actions (i.e. without the need for me to call model.predict)?
","['reinforcement-learning', 'deep-learning', 'philosophy', 'agi', 'intelligent-agent']","Neural networks, deep learning and other supervised learning algorithms do not ""take actions"" by themselves, they lack agency.However, it is relatively easy to give a machine agency, as far as taking actions is concerned. That is achieved by connecting inputs to some meaningful data source in the environment (such as a camera, or the internet), and connecting outputs to something that can act in that environment (such as a motor, or the API to manage an internet browser). In essence this is no different from any other automation that you might write to script useful behaviour. If you could write a series of tests, if/then statements or mathematical statements that made useful decisions for any machine set up this way, then in theory a neural network or similar machine learning algorithm could learn to approximate, or even improve upon the same kind of function.If your neural network has already been trained on example inputs and the correct actions to take to achieve some goal given those inputs, then that is all that is required.However, training a network to the point where it could achieve this in an unconstrained environment (""letting it loose on the internet"") is a tough challenge.There are ways to train neural networks (and learning functions in general) so that they learn useful mappings between observations and actions that progress towards achieving goals. You can use genetic algorithms or other search techniques for instance, and the NEAT approach can be successful training controllers for agents in simple environments. Reinforcement learning is another popular method that can also scale up to quite challenging control environments. It can cope with complex game environments such as Defense of the Ancients, Starcraft, Go. The purpose of demonstrating AI prowess on these complex games is in part showing progress towards a longer-term goal of optimal behaviour in the even more complex and open-ended real world. State of the art agents are still quite a long way from general intelligent behaviour, but the problem of using neural networks in a system that learns how to act as an agent has much research and many examples available online."
What would be the implications of mistakenly adding bias after the activation function?,"
I was looking at the source code for a personal project neural network implementation, and the bias for each node was mistakenly applied after the activation function. The output of each node was therefore $\sigma\big(\sum_{i=1}^n w_i x_i\big)+b$ instead of $\sigma\big(\sum_{i=1}^n w_i x_i + b\big)$. Assuming standard back-propagation algorithms are used for training, what (if any) impact would this have?
","['neural-networks', 'machine-learning', 'training', 'backpropagation', 'activation-functions']",
Are there tabular datasets where deep neural networks outperform traditional methods?,"
Are there (complex) tabular datasets where deep neural networks (e.g. more than 3 layers) outperform traditional methods such as XGBoost by a large margin?
I'd prefer tabular datasets rather than image datasets, since most image dataset are either too simple that even XGBoost can perform well (e.g. MNIST), or too difficult for XGBoost that its performance is too low (e.g. almost any dataset that is more complex than CIFAR10; please correct me if I'm wrong).
","['neural-networks', 'deep-learning', 'datasets', 'gradient-boosting', 'boosting']",
Is CNN capable of extracting the descriptive statistics features,"
I was trying to build a CNN model. I used time series data of daily temperature to predict if there is risk of an event, say bacteria growth. I calculated the descriptive statistics of the time series, ie. mean, variance, skewness, kurtosis etc for each observation and added them to input data. 
My question: 
Is CNN capable of extracting the effect of the descriptive statistics the label, meaning that adding these descriptive statistics features manually does not make a difference?
(I will still try this later, but like to hear what you think about it). Thanks
","['neural-networks', 'machine-learning', 'convolutional-neural-networks', 'keras']","CNNs learn convolutional filters that get trained on finding local, recurring patterns in some kind of image/volume data. 1D convolution is actually a thing, but I think what would be more suitable for your case is using Recurrent Neural Nets. They are specifically designed for working on time series-es of heterogeneous data.Update:I would like to reconsider the answer I gave earlier.
First of all, in case of dealing with time series data where it is uncertain over which time span a given event to be detected lasts, I'd generally consider using Recurrent Neural Networks (RNNs) rather than CNNs, since RNNs maintain so-called hidden states that can carry potentially useful information over time, i.e. past inputs may influence present outputs. So, either if you know that the event to be detected spans many time steps (which might not even be consecutive time steps) or if you are uncertain about how long exactly the time span of an event to be detected is, then I'd suggest going for RNNs. If you decide to use RNNs, adding extra statistics to your data might make the learning task easier for the network, but is not strictly required since you would expect an RNN to learn relevant time-series statistics on its own.However, there is a case where using CNNs might actually suffice and yield good results as well. Consider the case of the figure shown below, where the white tiles are time series inputs and the blueish area is the area that a CNN filter/kernel (or multiple ones) is working on right now.As you can see, there is a time series of data points and some statistics derived from that time series that have been appended to the input data stream. So, in the figure, the augmented time series input consists of the measurements themselves as well as mean, variance, and skewness values derived from the time series data (over consecutive time steps). Under the assumption that the type of event you are interested in is bounded, in terms of time steps, to the width of the CNN filter(s)/kernel(s) that you slide over your input data, then a simple CNN might suffice to detect the events you are interested in. In other words, if the width of your filter spans over at least as many time steps as an event of interest lasts, then your CNN will be able to learn to detect the event (given proper training). In that case, adding further statistics to your input data might indeed help or even be necessary to detect certain events, since it implicitly widens the time window that your CNN filter can observe at any given time step, since a mean (or other statistics) may also considers past data that the filter itself (or multiple filters...) might not span over at a given time step where you want to detect an event."
Deciding std. deviation for policy network output?,"
When I try to fit a Normal Distribution to the output of a policy network, for a continuous action space problem, what should be its standard deviation? mean for the distribution will directly be the output of the policy network.
","['machine-learning', 'reinforcement-learning', 'policy-gradients', 'probability-distribution', 'proximal-policy-optimization']",
Does a varying ANN model accuracy mean underfitting or overfitting?,"
Background:
This is for a simulated robot with four legs, walking on a flat terrain. The ANN (an MLP) is given inputs as the robot's body angle, positions and angle of each leg with respect to the body and two points of contact with the terrain, on each leg (if there's no contact, the value is zero). The outputs are the four motor rates for each leg.
I'm using Keras with the CNTK backend to train the network. The network has 30 input nodes (ReLU), one hidden layer with 64 nodes (ReLU) and an output layer with 4 nodes (Sigmoid). Optimizer is Adam.  
The training data has 2459 datapoints. Running model.validate with parameters testDataPercentage = 0.25, epochs = 50 and batchSize = 10 gave me:   loss: 2.9509 - accuracy: 0.3283 - val_loss: 2.8592 - val_accuracy: 0.3213.   
But running model.evaluate multiple times gave me:  
['loss', 'accuracy'] [3.10, 0.50]
['loss', 'accuracy'] [3.04, 0.23]
['loss', 'accuracy'] [3.01, 0.11]
['loss', 'accuracy'] [3.45, 0.02]
['loss', 'accuracy'] [3.17, 0.40]
['loss', 'accuracy'] [3.03, 0.27]
['loss', 'accuracy'] [3.012, 0.46]

Loss doesn't decrease much over 50 epochs. It reduces from maybe 3 to 2.8. That's it.
Question:
I don't understand why the accuracy varies so much for each run.
If I add a hidden layer or even add a dropout of 0.2, the results are similar: loss: 2.9253 - accuracy: 0.2978 - val_loss: 2.9350 - val_accuracy: 0.3148. 
Reducing the number of hidden nodes to 15 gives the same results: loss: 2.9253 - accuracy: 0.2978 - val_loss: 2.9350 - val_accuracy: 0.3148. Hidden layers with 64 nodes gives the same results. Training data with just 500 data points also gives the same results. Using sigmoid instead of ReLU gives slightly worse results.
I've been through many tutorials and guides on how to debug or check why the neural network is not working, but they don't teach properly, what these values mean and how to adjust the network.
Does the loss not decreasing, mean the network is not learning?
Does the fact that increasing or decreasing the layers or the number of training data mean that the network is not learning?
","['keras', 'artificial-neuron', 'multilayer-perceptrons']",
Need to analyze input CSV files and determine whether input file is good or bad w.r.t it's data,"
We have a scenario where we need to implement an Artificial Intelligence solution which will evaluate the input data file of my Azure Data Factory pipeline and let us know whether the file is good or bad with respect to it's data. 
For example, I have 10 files several rows which are good input files and 2 files with several rows which are bad input files.
Each file either it is good/bad has 26 columns. The above two files are bad because of below reasons. 

One file has all empty values for one column which is not expected.
Another file has, the value 'TRUE for all rows for a specific column which was also not he general scenario. (some % of TRUE's and some % records with FALSE will appear in good files)

Like this, there may be several scenarios where the input file may be treated as bad file. 
We want to implement an Artificial Intelligence solution which should analyze all the input files and identify the hidden patterns of the data in file and detect abnormal scenarios like above and should eventually mark the file as bad file. 
Please suggest for the approach or what components in Azure can help to achieve this kind of file sanity check.
Thanks.
",['machine-learning'],
What is the difference between artificial intelligence and artificial neural networks?,"
I have made several neural networks by using Brain.Js and TensorFlow.js. 
What is the difference between artificial intelligence and artificial neural networks?
","['neural-networks', 'comparison']","From wikipedia:Artificial neural networks (ANN) or connectionist systems are computing systems that are inspired by, but not identical to, biological neural networks that constitute animal brains. Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with task-specific rules.Artifical Intelligence on the other hand refers to the broad term of intelligence demonstrated by machinesThis obviously doesn't clear much up, so the next logical question is: ""What is intelligence?"" This, however, is one of the most debated questions in computer science and many other fields, so there isn't a straight answer for this. The most you can do is decide yourself what you think intelligence refers to, because as far as we know, there is no agreed upon way of quantifying intelligence, and so the definition of such will remain ambiguous."
Should I use deep learning to solve my task?,"
I need to predict the performance (CPI cycles-per-instruction) of 90 machines for the next hour (or day). Each machine has a thousand records (e.g. CPU and memory usage). 
Currently, I am using a neural network with one hidden layer for this task. I have 9 inputs (features), 23 neurons in the hidden layer, and one output. I am using the Levenberg-Marquardt algorithm. Examples of the inputs (or features) are the CPU and memory capacity and usage, and the machine_id. and Output is performance. I have 90 machines. Currently, I get an MSE of $0.1$ and an R of $0.80$.
My dataset consist of 30 days. I trained my network for the first 29 days, and I use day 30 to test. 
I have been advised to use deep learning to have more flexibility and improve the MSE and R results. Could deep learning be helpful in this case? If yes, which deep learning model could I use to improve the results?
","['neural-networks', 'deep-learning', 'deep-neural-networks']",
"DQN, how to choose the reward fucntion?","
I built a simple AI system that tries to solve the 8 puzzle using DQN.
The problem is, if the agent gets only a reward greater than zero when winning, the training will take a long time, so I made a smooth reward function instead: $R=(n/9)^3$, where $n$ is the number of pieces that are in the right position.
The training became quicker but the AI chose to match 7 pieces out of 9 to get a reward of $(7/9)^3/(1-\gamma) = 0.47/(1-\gamma) = 4.7$, for $\gamma=0.9$, choosing to win and getting reward of 1 doesn't make sense to the AI, lowering $\gamma$ will result in the AI to choose instant reward instead of long-term reward, so that will not be very helpful; lowering rewards of non-winning stats will make the training very slow.
So, how do I choose a good reward function?
","['machine-learning', 'dqn']",
Should an RL agent directly observe the reward?,"
I am training an A2C reinforcement learning agent in a dense reward environment (where rewards are known and explicit at every timestep).
Is it redundant to include the previous reward in the current observation space?
The reward is implicitly observed by the agent when collecting experiences and updating the network parameters. But could it also be useful for the agent to explicitly observe the reward of its previous action?
","['reinforcement-learning', 'rewards']",
Reinforcement learning for a 2D game involving two players,"
I'd like to create an AI for a 2D game involving two players fighting against each other. The map look something like this (The map is a NxN array somehow randomly generated): 

Basically the players can look for objects such as weapons located on platforms, shoot at each other to cause damages etc. The output actions are therefore limited to a few such as going up, left, right, down, shooting angle, shooting boolean...
I'm wondering if Reinforcement Learning using a neural network is a good approach to the problem. If so, how should I proceed for the learning phase? Should I force the AI to compete with a weaker version of itself at each iteration? Would it be computationally reasonable to train on a 4Gb GPU?
Thanks in advance for your advice ! 
","['reinforcement-learning', 'ai-design', 'training', 'q-learning', 'genetic-algorithms']",
Does each filter in each convolution layer create a new image? [duplicate],"







This question already has answers here:
                                
                            




In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?

                                (12 answers)
                            


How is the depth of a convolutional layer determined?

                                (3 answers)
                            

Closed 1 year ago.



Say I have a CNN with this structure:

input = 1 image (say, 30x30 RGB pixels)
first convolution layer = 10 5x5 convolution filters
second convolution layer = 5 3x3 convolution filters
one dense layer with 1 output

So a graph of the network will look like this:

Am I correct in thinking that the first convolution layer will create 10 new images, i.e. each filter creates a new intermediary 30x30 image (or 26x26 if I crop the border pixels that cannot be fully convoluted).
Then the second convolution layer, is that supposed to apply the 5 filters on all of the 10 images from the previous layer? So that would result in a total of 50 images after the second convolution layer.
And then finally the last FC layer will take all data from these 50 images and somehow combine it into one output value (e.g. the probability that the original input image was a cat).
Or am I mistaken in how convolution layers are supposed to operate? 
Also, how to deal with channels, in this case RGB? Can I consider this entire operation to be separate for all red, green and blue data? I.e. for one full RGB image, I essentially run the entire network three times, once for each color channel? Which would mean I'm also getting 3 output values.
","['convolutional-neural-networks', 'image-processing', 'convolution', 'hidden-layers', 'convolution-arithmetic']","You are partially correct.  On CNNs the output shape per layer is defined by the amount of filters used, and the application of the filters (dilation, stride, padding, etc.).  In your example, your input is  30 x 30 x 3.  Assuming stride of 1, no padding, and no dilation on the filter, you will get a spatial shape equal to your input, that is 30 x 30.  Regarding the depth if you have 10 filters (of shape 5 x 5 x 3) you will end up with a 30 x 30 x 10 output at your first layer.  Similarly, on the second layer with 5 filters (of shape 3 x 3 x 10, note the depth to work on the previous layer) you have 30 x 30 x 5 output. The FC layer has the same amount of weights as the input (that is 4500 weights) to create a linear combination of them.Note that the CNNs operate differently from the traditional signal processing convolution.  In the former, the convolution operation performs a dot product with the filter and the input to output a single value (and even add bias if you want to).  While the latter outputs the same amount of channels.The CNNs borrow the idea of a shifting kernel and a kernel response.  But they do not apply a convolution operation per se.  The CNN is not operating on each channel separately.  It is merging the responses of the three channels and mixing them further.  The deeper you get the more mix you get over your previous results.  The output of your FC is just one value.  If you want more, you need to add more FC neurons to get more linear combinations of your inputs."
What is the difference between game theory and machine learning?,"
What is the difference between game theory and machine learning? 
I had gone through the papers Deep Learning for Predicting
Human Strategic Behavior, by Jason Hartford et al., and When Machine Learning Meets AI and Game
Theory, by Anurag Agrawal et al., but I am not able to understand.
","['machine-learning', 'deep-learning', 'comparison', 'game-theory']",
DQN card game - how to represent the actions?,"
I want to train a DQN card game named witches. It consists of 60 Cards (1-14 of Yellow, Blue, Green, Red Cards + 4 Wizzards). The color of the first layed card has to be respected by the other players (if they have this card in hand). The one who has the card with the highest number gets the played cards. Each collected red card gives you a -1 point. 
With respect to this answer I setup the inputs / state of the NN as binary (meaning I have 180 bool values (60 card x is currently on the table, 60 card x is in the ai hand, 60 card x is already been played)
How to design the outputs / actions?

If the ai is the first player of a round it can play any card
If the ai is not first player it has to respect the first played card (or play a wizzard)

This means there is actually a list of available options. I then sort this list and have 60 Output bools which I set to 1 if this option is possible. Among these options the ai should then decide what the correct option is? Is this the correct procedure?
Inconsistent / Varying Action Space
This is what we have to deal here with. As explained in here I think a DQN as well as Policy Gradient Methods is not the correct architecture to choose for solving such multi-agent card games. What architecture would you choose?
General procedure?
Assume I have 4 players, so do I have to get the old state before the ai is the next player and the new state is directly after this round is finished?
my_game = game([""Tim"", ""Bob"", ""Lena"", ""Anja""])
while True:
    #1. Play unti AI has to move:
    my_game.play_round_until_ai()

    #2. Get old state:
    state_old = agent.get_state(my_game)

    #3. Get the action the AI should perform
    action_ = agent.get_action(state_old, my_game)

    #4. perform new Action and get new state
    #reward rates how good the current action was
    #score is the actual score of this game!
    reward, done, score = my_game.finishRound(action_)

    # 5: Calculate new state
    state_new = agent.get_state(my_game)

    #6. train short memory base on the new action and state
    agent.train_short_memory(state_old, action_, reward, state_new, done)

    #7. store the new data into a long term memory
    agent.remember(state_old, action_, reward, state_new, done)

    if done == True:
        # One game is over, train on the memory and plot the result.
        sc = my_game.reset()

My code so far is available here: https://github.com/CesMak/witches_ai 
",['dqn'],
Is the following the correct implementation of the Q learning algorithm for a neural network?,"
I just wanted to confirm that my understanding of Q learning was correct (with respect to a neural network).
The network, Q, is initialised randomly.
for n ""episodes"":
    The state, s1, is initialised randomly
    while s1 != terminal state:
        s1 is fed into Q to get action vector q1
        action a1 is chosen based off the max of q1 (or randomly)
        state s2 is found by progressing s1 based on a1

        s2 is fed into Q to get action vector q2

        The expected output for Q at q1, y, is found by:
        {If s2 is terminal, it is the reward at s2
        {Otherwise: reward at s2 + gamm*max(q2)
        (The ""otherwise"" doesn't match bellmans equation as Î±=1)

        Do gradient step where error = (y - max(q1))^2, only the max of q1 gets any gradient
        s1 = s2

This does not directly follow equations found by searching Q-learning as I find them rather ambiguous.
I am also not taking into account storing network states (in this case, the network is called Q) for proper learning to avoid catastrophic forgetting, as I'm more concerned on getting the specifics right before good practice.
","['neural-networks', 'q-learning']","Your pseudocode is fairly close to something that could be implemented. There are a couple of omissions, where it is not clear whether your implementation would do the right thing. There are a few non-general assumptions that are worth noting - they don't prevent you implementing for a specific purpose, but do prevent you writing the most generic DQN that you could. There is one mistake that prevents it working at all.Your mistake is here:Do gradient step where error = (y - max(q1))^2, only the max of q1 gets any gradientThe loss function and gradient must be applied to the state/action pair s1, a1, i.e. the action that was taken. This is an important distinction for when the agent takes exploratory (non-greedy) actions, as it must if it is going to learn about their values.A corrected statement might read: Do gradient step where error = (y - Q(s1,a1))^2, only the output for a1 gets any gradient.Regarding the rest of the pseudocode I could make the following observations:The state, s1, is initialised randomlyThis is fine if it is possible. However, it is more usual in Q learning to initialise the state to whatever the normal start of the episode would be. That might be from some randomised starting rules, but that is not quite the same as 
picking a random state from all possible states.action a1 is chosen based off the max of q1 (or randomly)It is not quite clear what you mean by ""or randomly"". For Q learning - and most RL - exploration is important. So this is not a design time choice for your algorithm, but a runtime choice that must itself be made so that all actions can be tried. Typically you would use an $\epsilon$-greedy policy here that took the argmax action by default, but with probability $\epsilon$ took a random sample from actions with same chance of each possible action.state s2 is found by progressing s1 based on a1This is one of a few places where you have not quite captured the nature of a time step in a general MDP. You appear to have made the assumption that rewards are directly associated with states (with language like ""reward at s2""). Whilst some MDPs are like that, it is not true in general.A better, more general statement might be:s2, reward, done are found by taking a step from s1, using a1 as the actionTypically the environment (or a wrapper for it that establishes state representations and rewards) will provide a step method that returns the tuple $s_{t+1}, r_{t+1}, (s_{t+1} = s_T)$, although you could also hold these values in the environment model for the current time step and return them as required.This statement:(The ""otherwise"" doesn't match bellmans equation as Î±=1)I don't know which Bellman equation you are referring to, but they don't include a learning rate, $\alpha$ in any version I have seen. Perhaps you are referring to a tabular Q learning update? In any case, it doesn't seem relevant, as whatever learning rate you are using is now inside the gradient step, and does not need to be part of the pseuodcode.Finally, regarding this:I am also not taking into account storing network states (in this case, the network is called Q) for proper learning to avoid catastrophic forgetting, as I'm more concerned on getting the specifics right before good practice.DQN will very likely not work at all, for most environments, unless you implement experience replay i.e. training the neural network on a random sample of stored s1,a1,r,s2,done taken from a table that you build up on each time step. Usually you cannot simply use the latest data from the current step. This is not just ""good practice"", but an almost-always necessary stability improvement."
What is the most accurate pretrained sentiment analysis model by 2019?,"
I've been using OpenAI's 2017 Sentiment Neuron implementation (https://github.com/openai/generating-reviews-discovering-sentiment) for a while, because it was easy to set up and was the most accurate on benchmarks. What is the most accurate alternative now that I should use?
","['natural-language-processing', 'sentiment-analysis', 'text-classification']",
How should I weight the factors that affect the choice of an action in a strategy board game with multiple actions?,"
I have written an AI that plays a strategy board game. There are lots of different types of moves (e.g. attack, defend, help ally colony, etc.).
I calculate the best moves to do depending on a variety of factors, such as the value of nearby enemy colonies, the number of armies the colony currently has, etc (each of these has separate weightings). I'm trying to find the optimal weighting for each of the different factors.
Currently, I decide the best configuration of parameters in a King of the Hill style tournament. I choose random values between a suitable range for each of the different parameters and then play two of these AI against each other 20 times. I have a total of 100 AI that play against the king, and then take the final king as the best AI.
The problem is that this is quite slow and I feel it's very inefficient, as a lot of the AI don't play well at all (probably due to the randomness of parameter values). 
I'm wondering if there's a more efficient way to determine the optimal value of parameters?
","['game-ai', 'optimization']","You could use a genetic algorithm to optimise the parameter settings. Here you don't choose random parameters all the time, but only at the beginning. Each AI (which is a vector of parameter settings) plays each other one for a ranking (you can probably reduce the number of total games by using a ladder-style ranking where only neighbours play against each other). Also, you don't need 100 to start with, as you might come across better combinations than those present initially throughout the processing.Then you discard the worst AIs (ie parameter vectors), and recombine the best ones, adding some random mutations into the mix. In theory this should converge faster, as you preserve good parameter settings (depending on how interdependent they are) and remove bad ones from the 'pool'.You can also have a higher rate of mutations initially, which slowly goes down as you progress through the generations."
How to solve Snake Game with a Hamiltonian graph algorithm?,"
I wonder if there is a way to solve snake game using Hamiltonian algorithm?
if there is a way

how to apply it?
what data structure will be used with algorithm?
time complexity and space complexity?
is this algorithm an optimal solution or there is a better way?

","['game-ai', 'graph-theory', 'path-finding']",
Can a trained object detection model deal with variations of the input?,"
Suppose an object detection algorithm is good at detecting objects and people when an object and person is close to a camera and upright. If the person walks farther away from the camera and is ""upside-down"" from the perspective of the camera (e.g. a fisheye camera), should the algorithm still be good at detecting people and objects in this position? 
","['machine-learning', 'convolutional-neural-networks', 'object-detection', 'adversarial-ml']",
Sequence-to-Sequence models without specifying the start and end of sentences,"
Is there a seq-to-seq model which does not require to know the start and end of a sentence? I need to model a system which gets a long sequence of words and creates a long sequence of tokens as long as the input. For example it takes a sequence of 1000 words and creates a sequence of 1000 tokens, each token corresponds to an input word. 
","['deep-learning', 'sequence-modeling']",
Need examples for the following definitions,"
I am currently reading the paper ""Similarity of Narratives"" by Loizos Michael (link below) and I am having a hard time figuring out the definitions listed (p.107 - p.109).
Could someone please give me a practical example for each of the definitions?
Article: http://narrative.csail.mit.edu/cmn12/proceedings.pdf 
","['machine-learning', 'natural-language-processing']",
Building an AI that generates text by itself,"
Now I know this might break some StackExchange rules and I am definitely open for taking the thread down if it does!
I am trying to build an AI that can write it's own book and I have no idea where to start or what are the appropriate algorithms and approaches to go with. 
How should I start and what do I exactly need for such a project? 
","['neural-networks', 'machine-learning', 'deep-learning', 'artificial-neuron']","There have been many methods proposed for text generating, but recurrent network dominates natural language processing with a key component: the perception of time.Many networks have been tried for text generation, with notable examples such as Markov chain. However RNN have been proven to work the best and is dominating the field of language modelling (text generation).A neural network that generates text is commonly called a language model. It is trained on large amount of text with labels being the next token. The text generation process uses several random token as the starting phrase and then the network predicts the rest. However the network does not just predicts the most probable word, instead it randomly chooses one of the few most probable token, hence the generating part.RNN have a perception of time built into the architecture of teh network. LSTM, a popular RNN variant used, is composed of ""memory units"" that ""remembers"" past text, thus the ""time"" part. RNN process input according to the sequence of time, so the network can naturally understand time, thus the superior performance compared to other networks.A language model consists of the encoder and the decoder. The encoder compresses word one-hot representation to a smaller sized vector representation. The smaller sized representation is then passed through the decoder, which maps the encoding to the words one hot vectors again.Language modelling is an actively researched field in the AI community, and recently the model GPT-2 have achieved a breakthrough in language modelling accuracy, producing almost human like text with a special component added, the attention layer. Attention basically maps the ""memory states"" of the encoder and feed it as input to the decoder. The data teh model is trained on is also very large, with over 20GB of web scraped data from sites like Reddit.One limit of language modelling is the size of generated text. As GPU don't have unlimited memory, language model usually limits the input token size to a specific number, padding or trimming to this number. The number is usually 500-1000, which includes a paragraph or two, but not an entire book. You can only generate short paragraphs and essay with language modelling. For long text it is much harder.GPT-2 open AI blog: https://openai.com/blog/better-language-models/
GPT-2 online interactive site for text generation: https://talktotransformer.com/
How to train and fine tune GPT-2 in python: https://minimaxir.com/2019/09/howto-gpt2/Hope I can help you"
"What is ""temporal depth""?","
I need some explanation about the following paragraph (page 3) from the paper A Novel Approach for Robust Multi Human Action Detection and Recognition based on 3-Dimentional Convolutional Neural Networks. 

We introduce a 3D convolution neural network with the following notations: $I(x, y, d)$ as an input video with a size of $x y$ and $d$ the temporal depth

What is ""temporal depth""? Is it the number of frames?
","['neural-networks', 'computer-vision', 'terminology', 'papers', 'action-recognition']",
Does using the softmax function in Q learning not defeat the purpose of Q learning?,"
It is my understanding that, in Q-learning, you are trying to mimic the optimal $Q$ function $Q*$, where $Q*$ is a measure of the predicted reward received from taking action $a$ at state $s$ so that the reward is maximised.
I understand for this to be properly calculated, you must explore all possible game states, and as that is obviously intractable, a neural network is used to approximate this function.
In a normal case, the network is updated based on the MSE of the actual reward received and the networks predicted reward. So a simple network that is meant to chose a direction to move would receive a positive gradient for all state predictions for the entire game and do a normal backprop step from there.
However, to me, it makes intuitive sense to have the final layer of the network be a softmax function for some games. This is because in a lot of cases (like Go for example), only one ""move"" can be chosen per game state, and as such, only one neuron should be active. It also seems to me that would work well with the gradient update, and the network would learn appropriately.
But the big problem here is, this is no longer Q learning. The network no longer predicts the reward for each possible move, it now predicts which move is likely to give the greatest reward.
Am I wrong in my assumptions about Q learning? Is the softmax function used in Q learning at all?
","['reinforcement-learning', 'q-learning']","However, to me, it makes intuitive sense to have the final layer of the network be a softmax function for some games. This is because in a lot of cases (like Go for example), only one ""move"" can be chosen per game state, and as such, only one neuron should be active.You are describing a network that approximates as policy function, $\pi(a|s)$, for a discrete set of actions.It also seems to me that would work well with the gradient update, and the network would learn appropriately.Yes there are ways to do this, based on the Policy Gradient Theorem. If you read it you will probably discover this is more complex to understand than you first thought, the problem being that the agent is never directly told what the ""best"" action is in order to simply learn in a supervised manner. Instead, it has to be inferred from rewards observed whilst acting. This is a bit harder to figure out than the Q learning update rules which are just sampling from the Bellman optimality equation.You can split Reinforcement Learning methods broadly into value-based methods and policy gradient methods. Q learning is a value-based method, whilst REINFORCE is a basic policy gradient method. It is also common to use a value based method within a policy gradient method in order to help estimate likely future return used to drive the polcy gradient updates - this combination is called Actor-Critic where the actor learns a policy function $\pi(a|s)$ and the critic learns a value function e.g. $V(s)$.But the big problem here is, this is no longer Q learning. The network no longer predicts the reward for each possible move, it now predicts which move is likely to give the greatest reward.This is true, but it is not a big problem. The main issue is that policy gradient methods are more complex than value based methods. They may or may not be more effective, it depends on the environment you are tryng to create an optimal agent for.Is the softmax function used in Q learning at all?I cannot think of any non-contrived environment in which this function would be useful for an action value approximation.However, it is possible to use a variant of softmax to create a behaviour policy for Q learning. This uses a temperature hyperparameter $T$ to weight the Q values, and provide a probability of selecting an action, as follows$$\pi(a_i|s) = \frac{e^{Q(s,a_i)/T}}{\sum_j e^{Q(s,a_j)/T}}$$when $T$ is high all the probabilities of actions will be similar, when it is low even a small difference in $Q(s,a_i)$ will make a big difference to probability of selecting action $a_i$. This is quite a nice distribution for exploring whilst avoiding previously bad decisions. It will tend to focus the agent on exploring differences between similarly high rated actions. The main issue with it is that it introduces hyperparameters for deciding starting $T$, ending $T$ and how to move between them."
FasterRCNN's RPN network training,"
I would like to know if my understanding of RPN training is correct, and if never training the RPN on some specific anchor box is bad (i.e if the anchor never sees good nor bad examples).
To make my point clear, assume we have two functions. $f_{\theta_1}$ which represents the backbone that outputs a feature map of size $n$ (assume flattened) for an image of size $m$ (WLOG assume the image is flattened)
$$
f_{\theta_1}: \mathbb{R}^m \to \mathbb{R}^n
$$
and $f_{\theta_2}$ that represents the 'objectness' of each anchor box. We can suppose that $f_{\theta_2}$ and $f_{\theta_1}$ are convolutional neural networks, where $\theta_1, \theta_2$ are the networks' parameters. For simplicity, assume the RPN does not output bounding boxes correction, and only outputs the probabilities that an anchor box is an object or not.
$$ f_{\theta_1}: \mathbb{R}^n \to \mathbb{R}^{k \cdot n}$$
We can assume $k=1$, which is the number of boxes per anchor.
If my understanding is correct, we select $p$ good proposals $G_p$, and $p$ bad proposals $B_p$ for training the RPN, which are indices of good and bad predictions. In other terms, if $x$ is an image (assume flattened), then $f_{\theta_2}(f_{\theta_1}(x)) = y$, next we only back-propagate the loss for coordinates $B_p$ in $y$ and $G_p$ in y. For instance, if $p=1$, and $G_p = \{i\}$ and $B_p = \{j\}$ and $ 1\leq i \neq j \leq n$ then we only compute the loss of the RPN for coordinates $i$ and $j$ in $y$. 
My questions are:
1- Is my understanding correct? and if not, how do we perform training?
2- Assuming my understanding is right or partially right about the last step, What happens if we never train component $y_0$ from the RPN's output for example? (i.e we never back propagate the  loss through some components for $y$) woudn't this be a problem (i.e hurt performance or network training does not go well at all in some cases?)
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'object-recognition', 'object-detection']",
"How can I classify instances into two categories and then into sub-categories, when the number of features is high?","
I'm working with a problem where I have a lot of variables for different cases of different users. Depending on the values of the different variables of a concrete user in a concrete case, the algorithm must classify that user in that case as:

Positive
Negative

But if the user is classified as positive, it must be classified as:

Positive normal
Positive high
Positive extra-high

If a case is positive, depending on the values of a part of the parameters, we know that the probability to be, for example, positive normal is bigger or lower.
To sum up, I see the problem as a spam detector with different spam types.
May this work if I apply an algorithm like:

Random Forest
Decision Tree

Or maybe I can include the negative case as a new group and then implement a K-means algorithm? Maybe this would help to find new groups of parameters that will say that the concrete case forms part of a group for sure.
Which one will fit best with a lot of parameters?
","['machine-learning', 'classification', 'decision-trees', 'random-forests', 'k-means']",
Applying Machine Learning to 2D Laser Scanner Data,"
We are using 2D Laser Scanner to scan various objects of different geometric shapes for e.g. cylinder, spiked, cylinder with notch, cylinder with curved edges e.t.c. The dataset contains points in the format [x, y] with the dimension of 1 complete scan being 160x2. The goal is to use these scan points to classify the various shapes.
I have used a multilayer NN with sigmoid as the final layer and Adadelta optimizer for this problem but the accuracy reaches only upto 70%. 
Can anyone recommend a proper model that can be used for Laser Scanner Data Classification?

          MODEL
def baseline_model():
    model = Sequential()
    model.add(Dense(2048, input_dim=160, activation='relu'))
    model.add(Dropout(0.1))
    model.add(Dense(1024, activation='relu'))
    model.add(Dropout(0.1))
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.4))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(6, activation='softmax'))
    Adam = optimizers.Adam(lr=0.001)
    Adadelta =  optimizers.Adadelta(lr = 1)
    model.compile(loss='categorical_crossentropy', optimizer=Adadelta,   metrics=['accuracy'])

","['neural-networks', 'keras', 'multilayer-perceptrons']",
"In unsupervised learning, what is meant by ""finding the probability of an image""?","
The specific problem I'm having is with a Fully Visible Belief Network. It is an explicit density model (though I don't know what quantifies something being such) that uses the chain rule to decompose the likelihood of an image x into a product of 1-d distributions.

What is meant by ""the likelihood of an image x""? With respect to what? I assume it refers to how common this image would be in the data set it is selected from? Like if you had 1000 images, 800 of which were white and 200 of which were black, the model should ideally output 0.2 for any black image inputted? Obliviously with more complicated clustering like dogs vs cats it'd be a bit different, but that's my intuition. Is that correct?
Also as a side question, that equation looks very wrong. If you have an image of $1048\times720$ pixels, and say every pixel evaluates to have a probability of 0.9, you'd expect the final probability of the image to be 0.9 or 90%. But according to that equation, it's $0.9^{720*1048}$, which is stupidly small, essentially 0. What's going on here?
",['unsupervised-learning'],"When you say likelihood, you are invoking several other concepts like events, sample, parameters, model, probability density function (PDF), etc (it would be helpful if you learn more about these concepts). In essence, a likelihood function $l(x|\theta)$ is a PDF that quantifies how likely is that event $x$ happens out of a set of possible events, given the parameters $\theta$ that define your model.  In the specific case of images, the set of possible events is usually one of two options: 1) all the available images in a dataset, or 2) all the existing images. Usually you want to model the likelihood in the option 2), but only having access to a sample of all the possible images. In either case, the likelihood is just the probability that you select one image out of all the possible ones. If you consider only images of $1048\times 720$ pixels, the possible amount of images is $(256\times3)^{1048\times 720}$, where I am assuming that each pixel consists of 3 colors and each color can take 256 values. Since the amount of possible images is so so big, it is very common that the probability of selecting a specific one is very very small. This is a reason why you usually work with log-likelihood (the logarithm of the likelihood) instead of directly using likelihoods. For example, if all your images were equally probable, the likelihood would be in the order of $10^{-{10^7}}$, while the log-likelihood would be around $-10^7$.  To solve your paradox with the probability of images and pixels, consider that instead of pixels you have coins and instead of images you have sequences of coins. Let's say that you have a fair coin, so the probability of tails ($T$) after tossing the coin is 0.5. If you toss a second coin, the probability of having $T$ again is naturally 0.5 as well, but what is the probability that both results where $T$? It is the product (0.25) since the events are independent. Similarly, the probability of the other three sequences $TH$, $HT$ and $HH$ is just 0.25. You can see that since the probability needs to be shared between 4 sequences equally probable, they are less probable with respect to the probabilities of the sequences of length 1. If you toss the coin 3 times, then the probability of all these coins being tails is just $0.5^{3}$. Again, there are now 8 possible sequences, all sharing the same amount of probability. You can see what's going on. Since the amount of possible options becomes large, the probability of each possible sequence of coins becomes small. Clearly, you would never toss a coin 10 times and expect to obtain all $T$, right? Well, exactly the same happens in the case of the pictures."
Finding the right model,"
Let us say that i have two ball throwing machines which has some algorithm running in the back-end for releasing the balls. One machine shows it throws 5 balls in 1 sec. Other shows the exact distribution of how many balls were thrown in each 0.2 secs (say the distribution is: 2,1,0,1,1) but the sum is 5 balls/sec for this machine too. Can i use this data and some other independent parameters like speed, direction etc as inputs and predict the similar distribution for lower accuracy machine.?
Re-framing my question: 
I am searching for an apt supervised model for the following use case: 
If I have a  sum (say 10) and it can be distributed in a predefined number of bins (say 5) in a number of ways for instance: 
1. 1, 2, 5, 0, 2
2. 0, 0, 3, 7, 0
etc.
The distribution bins always have whole numbers and the sum is also a whole number. The distribution depends on a number of factors and patterns which can be learned in the volumetric data. Hence, if I am able to load more than one sum (say n sums) and output the corresponding n*5 distributions it will be better for precise prediction (as per my intuitions). I tried using some networks but they are not doing much good. 
","['neural-networks', 'machine-learning']",
Optimisation of dependence of efficiency of CNN on training data,"
I got a large dataset of images (dimensions of 16 x 16, 250k samples) and corresponding spherical coordinates (distributed uniformly in each coordinate). On these, I trained a convolutional regression network to directly yield the coordinates for a provided image. The network itself is rather simple and consists of multiple convolutional layers where the last of them is flattened and followed by some dense layers to get the desired output. Since the input image size is rather small, pooling layers are obsolete I think (doesn't make much difference if used).
If I now train on all of the data I will get reasonable results in the end. But, if I filter them before training, i.e. only use coordinates which are limited by a certain radius, the network will increase it's performance quite a bit, but will only work well if my input image corresponds to the parameter space used during training. 
So my question is, if the network isn't deep enough or has the wrong architecture to perform on the complete dataset with high confidence or if this is expected behaviour. One naive approach would be to train the network for different coordinate ranges and to store the weights for each of them. Then, you could train a classifier to decide in which range you are and use the previously determined weights for the network accordingly. But this seems strange to me, as a single network should somehow be able to achieve the same without this weird architecture, I think.
I would be pleased if someone has an idea how I could optimise the performance of my network to yield the best results over the whole coordinate space.
","['deep-learning', 'convolutional-neural-networks', 'regression']",
How to detect any native language when written in Latin characters?,"
Assume somebody knows only to write in Latin characters. If they write words of any other language (example: Hindi, French, Latin)  using the Latin alphabet, how can I detect that language?
Example: If they write Hindi language word using the Latin alphabet) 


kya kar raha hai


     >> the output is Hindi language

","['machine-learning', 'deep-learning', 'natural-language-processing']",
"To deal with infinite loops, should I do a deeper search of the best moves with the same value, in alpha-beta pruning?","
I have implemented minimax with alpha-beta pruning to play checkers. As my value heuristic, I am using only the summation of material value on the board regardless of the position.
My main issue lays in actually finishing games. A search with depth 14 draws against depth 3, since the algorithm becomes stuck in a loop of moving kings back and forth in a corner. The depth 14 player has a significant material advantage with four kings and a piece against a single king, however, it moves only one piece.
I have randomly selected a move from the list of equally valued moves and this leads to more interesting games (thus preventing the loop). However, whichever player used this random tactic ended up far worse off.
I am not quite sure how to solve this problem. Should I do a deeper search of the best moves with the same value? Or is the heuristic at fault? If so, what changes would you suggest?
So far I have tried a simple genetically generated algorithm that optimizes a linear scoring function (that accounts for the position). However as the algorithm optimized, it led to only draws and the same king loop.
Any suggestions for how to stop this king loop are very welcome!
","['game-ai', 'search', 'minimax', 'alpha-beta-pruning', 'checkers']",
How to perform regression with multiple numeric (positive and negative) inputs and one numeric output?,"
I have a dataset with different types of numerical values (both negative and positive numerical values) for the inputs (for example, -40, -35, 1, 25, 39, etc., that is, multiple inputs) and single output numerical value (either negative or positive).
I have tried to use linear regression, but I haven't been so successful and I think one of the reasons is negative values.
What is the best way to deal with this scenario? What model should I use?
I am using Keras for my AI model.
","['machine-learning', 'ai-design', 'tensorflow', 'keras', 'regression']",
Eligibility vector for softmax policy with policy gradients,"
There is this nice result for policy gradients that the gradient of some performance measure, $\nabla v_{\pi_{\theta}}(s_0)$ (here, in the episodic case for the starting state $s_0$ and policy $\pi$, parametrised by some weights $\theta$) is equal to the expectation gradient of the logarithm of the policy, i.e.
$$\nabla v_{\pi_{\theta}}(s_0)=\mathbb{E}\Big{[}\sum_{t=0}^{T-1}\nabla_\theta\log(\pi_{\theta}(a_t|s_t)]\cdot G_t\Big{]},$$
where $G_t$ is the discounted future reward from state $s_t$ onward and $s_T$ the final state of some trajectory $(s_0, a_0, s_1, a_1, ..., s_{T-1}, a_{T-1}, s_T)$.
Now, when using a softmax policy, $\nabla_\theta\log(\pi_{\theta}(a_t|s_t)$ can be written as
$$\nabla_\theta\log(\pi_{\theta}(a_t|s_t))=\phi(s_t,a_t)-\mathbb{E}[\phi(s_t,\cdot)],$$
where $\phi(s,a)$ is some input vector of a state-action tuple.
However: what exactly is this vector? A typical input with policy gradients (for example in a neural network) is a feature vector for the state and the output a vector with dimensions equal to the number of actions, e.g. $(14, 15, 11, 17)^T$ for four possible actions. The softmax-function now scales these outputs, which results in the logits $(.042, .114, .002, .842)^T$ in this example.
What I would usually do in neural networks is take some input vector, for example something that describes if there are borders in a grid world, e.g. $\phi(s)=(1, 0, 0, 1)^T$, and multiply that with my weight matrix $\theta$ (and add biases b), i.e. $\theta\phi(s)+b$. So, continuing above example, $1\cdot \theta_{1,1} + 0\cdot \theta_{1,2} + 0\cdot \theta_{1,3} + 1\cdot \theta_{1,4} = 14$ and $1\cdot \theta_{2,1} + 0\cdot \theta_{2,2} + 0\cdot \theta_{2,3} + 1\cdot \theta_{2,4} = 15$.
But what is $\phi(s,a)$ here? And how would I compute $\nabla_\theta\log(\pi_{\theta}(a|s))=\phi(s,a)-\mathbb{E}[\phi(s,\cdot)]$?
","['machine-learning', 'gradient-descent', 'policy-gradients', 'softmax-policy']","Calculation of gradient
\begin{align}
\nabla_{\theta} \log(\pi_{\theta}(a|s)) &= \phi(s,a) - \mathbb E[\phi (s, \cdot)]\\
&= \phi(s,a) - \sum_{a'} \pi(a'|s) \phi(s,a')
\end{align}
is only valid for linear function approximation with action preferences of form
\begin{equation}
h(s, a, \theta) = \theta^T \phi(s,a)
\end{equation}
and softmax policy
\begin{equation}
\pi(a|s) = \frac{e^{h(s,a,\theta)}}{\sum_{a'} e^{h(s,a',\theta)}}
\end{equation}
The gradient would be calculated as it is written. For example, if your current state is $s = (1, 1)$ and in that state you have actions $a_0 = 0$ and $a_1 = 1$ and probabilities for those actions are $\pi(a_0|s) = 0.7$, $\pi(a_1|s) = 0.3$ then gradient for action $a_0$ would be
\begin{equation}
\nabla_{\theta} \log(\pi_{\theta}(a_0|s)) = (1, 1, 0)^T - (0.7 \cdot (1, 1, 0)^T + 0.3\cdot (1,1,1)^T) = (0, 0, -0.3)^T
\end{equation}
Feature vector $\phi$ can be basically anything you want. For example you could stack state feature and action (like I did in small example), you could use polynomials, radial basis functions, tile coding, etc.  If you're using multilayered neural network you would have to propagate gradients through all layers, usually done with backpropagation algorithm. Easiest way is to use automatic differentiation software (e.g. Tensorflow) which can do that for you so you don't have to write your implementation. All you have to do is define your objective function that you want to optimize
\begin{equation}
J_\theta = \sum_t \log(\pi(a_t|s_t, \theta)) G_t
\end{equation}
and software will automatically calculate gradient $\nabla J_{\theta}$ and update weights."
structure of neural network for classification problems with large amounts of null classifications,"
I am building a Convolution neural network to predict certain categories based on images (the location of a pointer on a surface) . However in many cases there will be no pointer in the view or something that is not the pointer. Initially I was just going to train it with outputs of the different classifications including the null classification. However given that the null classification is far more common than the others (perhaps 1000 times more likely) would it be better to have a separate null classifier and then if this outputted non null then the second classifier would be used.
Any suggestions?
","['convolutional-neural-networks', 'classification']",
Correlating two models to predict the output of one that corresponds to an output of the other,"
I am currently working on a problem and now got stuck to implement one of it's steps. This is a simple attempt to explain what I am currently facing, which is something that I am aiming to implement in my python simulation.
The idea is that I will some input parameters into my simulation, however one simulation is not able to perfectly capture all the dynamics involved in a real scenario. Hence, what I am aiming to do is to feed some inputs of the real scenario into my simulate and perform the simulation for all cases in which I have real data. So I will have the same amount of data for technically the same situation for both real and simulated scenario.
With my simulated data I can find out the optimal parameters(for the simulation), so the idea now is to correlate my simulated model with the real data, and then, with this correlation, find out what would be the equivalent of the optimal simulated parameters into the the optimal real parameters. Here not really precise diagram that might help on the visualization of the problem:

I have already seen a lot of machine learning being utilized to fit to a set of data, but haven't really seen anything that could help me on this task that I currently have in hand, as ""fitting models"". So here comes the questions, how to correlate the models and utilized it to extract the optimal parameters.
Hope that I managed to be succinct and precise albeit the length of the text. I would really appreciate your help on this one!
","['machine-learning', 'models', 'model-based-methods']",
How can I find what does an specific neuron do in neural network?,"
How can I know what each neuron does in NN?
Consider the Playground from Tensorflow, there are some hidden layers with some neurons in each. Each of them shows a line(horizontal or vertical or ...). Where these shapes come from. I think they are understandable for nn not a person!
","['neural-networks', 'tensorflow']","In TensorFlow Playground, the horizontal line show where each class is separated for each neuron. What happens when you take any intermediate neuron to make the decision? You can see the answer by the line provided by that neuron. And this decision is a result of the weighted sum from the decisions of the previous neurons (up to activation).Take the middle-top neuron in the link you share, which is an almost horizontal line - slightly tilted to the right. This neuron classifies everything above it as a blue, and everything below it as an orange. Hover over the neuron to see a larger picture on the output. You can also see how this is actually calculated by hovering over the line coming from the neurons in the previous layer to the neuron you are looking at. For the case of the same neuron (center-top), the weight coming from the first input ($x_1$) is 0.091, while from the second one ($x_2$), it is 0.49. The neuron ends up being almost horizontal because the contribution from the horizontal input ($x_2$) is so much larger compared to the vertical one ($x_1$). Of course you need to take into account the nonlinearity coming from the activation function but the idea presented above is the essence of it. The example uses tanh activation, which behaves very linear in its intermediate region so one can ignore this issue to some extend for this particular case.  Edit: It appears that the values for the weights change at every browser session, so the neuron I describe might look a little different to you. To get the same configuration, simply click on the colored lines between neurons to edit them and use the values above for the connections. "
Are there OpenAI Gym continuing environments (other than inverted pendulum) and baselines?,"
I would like to use OpenAI Gym to solve a continuing environment, that is, a problem with a single, never-ending episode (please note I don't mean a continuous environment with continuous state and actions).
The only continuing environment I found in their repository was the classic inverted pendulum problem, and I found no baseline methods (algorithms) that don't require episodic environments.
So I have two questions:

are there any continuing environments other than the inverted pendulum one?
is there an OpenAI Gym baseline method that I can use to solve the inverted pendulum problem as well as other continuing environments?

","['reinforcement-learning', 'open-ai', 'gym']",
Are there ensemble methods for regression?,"
I have heard of ensemble methods, such as XGBoost, for binary or categorical machine learning models. However, does this exist for regression? If so, how are the weights for each model in the process of predictions determined? 
I am looking to do this manually, as I was planning on training two different models using separate frameworks (YoloV3 aka Darknet and Tensorflow for bounding box regression). Is there a way I can establish a weight for each model in the overall prediction for these boxes?
Or is this a bad idea?
","['neural-networks', 'deep-learning', 'tensorflow', 'computer-vision', 'ensemble-learning']",
Do backgroundSubtractor functions in opencv only detect moving objects?,"
There are some backgroundsubtractor functions in opencv like backgroundsubtractormog2 , backgroundsubtractorGMG and ... . It seems that these functions only detect moving objects in a video. 
But I understand from the concept of these functions, that they do some clustering in an image.
Do these functions only detect moving objects? Why? Or am I wrong?
Any help will be appreciated
","['computer-vision', 'object-detection', 'image-processing']",
How can I recognise possibly overlapping line segments in 2D?,"
I am given a 2-dimensional picture (black & white, white background) and it is assumed that there are some 'sticks' (basically 'thick lines' with different widths and lengths) that are (mostly) overlapping with one another.
I want to somehow recognize these sticks (where they are and how big they are).
Is there any approach you would recommend or, even better, anything that already exists? I am working with MatLab, but a general (theoretical) approach would be also fine! I am open to machine-learning, but I'd prefer classical algorithms here.
","['pattern-recognition', 'matlab']",
What is the correct dimension of mu/logvar and z in the VAE?,"
I'm having a problem to understand the needed dimensions of a VAE, especially for mu, logvar and z layer.
Let's say I have an input of 512x512, 1 color channel (CT images), batch size 32.  Then my encoder/decoder looks like the following:
self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 32x512x512
            nn.ReLU(True),
            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 32x256x256
            nn.ReLU(True),
            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 32x128x128
            nn.ReLU(True),
            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 32x64x64
            nn.ReLU(True),
            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),  # 32x32x32
            nn.ReLU(True))

self.decoder = nn.Sequential(
            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid())

What is the correct dimension of mu/logvar and z? latent_dim = 1000, filter_depth=32.
I'm not sure if the input of the linear layer mu/logvar is right or not.
mu = nn.Linear(self.filter_depth * 32 * 32, self.latent_dim)
logvar = nn.Linear(self.filter_depth * 32 * 32, self.latent_dim)
z = nn.Linear(self.latent_dim, self.filter_depth * 32 * 32)

","['implementation', 'variational-autoencoder', 'hyper-parameters']",
Is it possible to use AI for detecting the volume of a cup,"
I was just wondering if it's possible to use Machine Learning to train a model from a dataset of images of cups with a given volume in the image and then use object detection to detect other cups and assume the volume of the cup,
Basically the end goal is to detect the volume of a cup using object detection with a phone's camera,
I would highly appreciate it if someone can point me to the right direction.
","['machine-learning', 'datasets', 'object-recognition', 'object-detection']",
Can I shuffle image channel data as a form of data augmentation?,"
If I want to augment my dataset, is shuffling or permuting the channels (RGB) of an image a sensible augmentation for training a CNN? IIRC, the way convolutions work is that a kernel operates over parts of the image but maintains the order of the kernels.
For example, the kernel has $k \times k$ weights for each channel and the resulting output is the multiplication of the weights and the pixel values of the image and is finally averaged to form a new pixel in the next feature map.
In this case, if we shuffle the channels of the image (GBR, BGR, RBG, GRB, etc.), a CNN that is only trained on the ordering RGB would do poorly on such images. Therefore, is it not sensible to shuffle the channels of the image as a form of data augmentation? Or will this have a regularizing effect on the CNN model?
","['convolutional-neural-networks', 'image-processing', 'convolution']","As a rule of thumb for image data augmentation, look at the augmented images:Can you correctly classify or measure your target label from the augmented images?Could something similar to the augmented images appear in the environment where you want to run inferences on previously unseen inputs?For your suggested augmentation of shuffling the channels, it may pass the first test. However, the second test shows that you are probably taking a step too far.will this have a regularizing effect on the CNN model?Yes, but it might not be that useful to have strong cross-channel regularisation. If there is important information for your task in the separate colour channels, then shuffling the channels makes it harder for the neural network to use that (it is not impossible, the CNN can still learn filters that will trigger most strongly on features that tend to appear in red channel and not blue in your problem for instance).If there is not important information for your task in the colour information, then you may find it simpler and easier to turn your images into single channel greyscale instead, and use that throughout. Although that is not completely the same, for many image types it will achieve a similar effect (and possible boost to accuracy) for a fraction of the effort."
"In Faster R-CNN, how can I get the predicted bounding box given the neural network's output?","
The RPN loss in Faster RCNN paper is
$$
L({p_i}, {t_i}) = \frac{1}{N_{cls}} \sum_{i} L_{cls}(p_i,p_i^*) + \lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^*)
$$
For regression problems, we have the following parametrization
$$t_x=\frac{x - x_a}{w_a}, \\ t_y=\frac{yâˆ’y_a}{h_a}, \\ t_w= \log \left( \frac{w}{w_a} \right),\\ t_h= \log \left(\frac{h}{h_a} \right)$$
and the ground-truth labels are
$$t_x^*=\frac{x^* - x_a}{w_a},\\ t_y^*=\frac{y^*âˆ’y_a}{h_a}, \\ t_w^*= \log \left( \frac{w^*}{w_a} \right), \\ t_h^*= \log \left(\frac{h^*}{h_a} \right)$$
where

$x$ and $y$ are the two coordinates of the center, $w$ the width, and $h$ the height of the predicted box.

$x$ and $y$ are the two coordinates of the center, $w$ the width, and $h$ the height of the anchor box.

$L_{reg}(t_i, t_i^*) = R(t_i âˆ’ t_i^*)$, where $R$ is a robust loss function (smooth $L_1$)


These equations are unclear to me, so here are my two questions.

How can I get the predicted bounding box given the neural network's output?

What exactly is the smooth $L_1$ here? How is it defined?


","['convolutional-neural-networks', 'papers', 'object-detection', 'r-cnn', 'faster-r-cnn']",
Is logistic regression used for unconstrained or constrained optimisation problems?,"
Is logistic regression used for unconstrained or constrained optimization problems, and why?
","['machine-learning', 'optimization', 'logistic-regression']",
How should I handle different input sizes in graph convolution networks?,"
I'm a student beginning to study deep learning, and would like to practice with a simple project using a Graph Convolutional Network.
However, I'm not quite sure how to handle different input sizes of graphs for the GCN. How would I do this?
Is zero-padding the only way to solve this problem? While zero-padding is applicable to CNNs, I'm not sure if it is for a GCN.
","['convolutional-neural-networks', 'geometric-deep-learning']",
Designing a reinforcement learning AI for a game of connect 4 [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I've made a connect 4 game in javascript, and I want to design an AI for it. I made a post the other day about what output would be needed, and I think I could use images of the board and a CNN. I did some research into Reinforcement learning, and I think that's what I need to do. I don't have much experience with ML in general, much less RL with Q-learning, but that is what I'd like to do. 
Now, I don't really know how to start out with such a big project. I have a few questions first:

What do I do with my input? I'm thinking I give the AI 7 options for moves to make, one for each row of the board. How do I implement a way that it can ""look"" at the board? Can I just import an image of the current board state?
How do I make a reward table? How should I do the points system for the Q-learning? I'm thinking something like: If it drops a chip it gets a point, if it lines up 2 chips in a row it gets 5 points, if it gets 3 in a row it gets 30, and if it gets 4 in a row it gets 100. Would that be an effective way to do this? How do I implement this?
Is there a library I can use to do any of the work where I make an algorithm and board states and reward tables? Or do I have to hard code any of it?
I've done some research, are there any links or tutorials you think I should read or follow along with? Any other general advice or help for me?

I greatly appreciate anyone who answers one or all of these questions! Thank you so much!
","['deep-learning', 'reinforcement-learning', 'ai-design', 'game-ai']",
How can I train a neural network to detect subliminal messages?,"
Is there a way to train a neural network to detect subliminal messages? Where can I find the dataset on which to train the neural network?
If I have to create the dataset, how would I go about it?
United Nations has defined subliminal messages as perceiving messages without being aware of them, it is unconscious perception, or perception without awareness. Like you may be aware of a message but cannot consciously perceive that message in the form of text, etc. 
There are two many types of subliminal messages, one which can be made through visual means, another which can be made through audio. 
In visual means, I'm referring to these types:

Messages which are flashed for very short while on the screen. 
Messages whose opacity is changed to blend with the background.
Messages whose colors are varied slightly to blend with the background. 

Example of 3rd type of subliminal messages: if there is a background of red, on this can be shown a message made up of slight variation of red, as a conscious mind can't distinguish between such close shades of red, people will take the entire thing to be red block, but a subconscious mind notice the slight variation in color, register the message, because humans can see millions of colors. 
",['neural-networks'],
What is the relationship between the training accuracy and validation accuracy?,"
During model training, I noticed various behaviour in between training and validation accuracy. I understand that 'The training set is used to train the model, while the validation set is only used to evaluate the model's performance...', but I'd like to know if there is any relationship between training and validation accuracy and, if yes,

what exactly is happening when training and validation accuracy change during training and;

what do different behaviours imply


For instance, some believe there is overfitting problem if training > validation accuracy. What happens if one is greater than the other alternately, which is the case below?
Here is the code
inputs_1 = keras.Input(shape=(10081,1))

layer1 = Conv1D(64,14)(inputs_1)
layer2 = layers.MaxPool1D(5)(layer1)
layer3 = Conv1D(64, 14)(layer2)
layer4 = layers.GlobalMaxPooling1D()(layer3)

inputs_2 = keras.Input(shape=(104,))             
layer5 = layers.concatenate([layer4, inputs_2])
layer6 = Dense(128, activation='relu')(layer5)
layer7 = Dense(2, activation='softmax')(layer6)


model_2 = keras.models.Model(inputs = [inputs_1, inputs_2], output = [layer7])
model_2.summary()


X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:10185], df[['Result_cat','Result_cat1']].values, test_size=0.2) 
X_train = X_train.to_numpy()
X_train = X_train.reshape([X_train.shape[0], X_train.shape[1], 1]) 
X_train_1 = X_train[:,0:10081,:]
X_train_2 = X_train[:,10081:10185,:].reshape(736,104)   


X_test = X_test.to_numpy()
X_test = X_test.reshape([X_test.shape[0], X_test.shape[1], 1]) 
X_test_1 = X_test[:,0:10081,:]
X_test_2 = X_test[:,10081:10185,:].reshape(185,104)    

adam = keras.optimizers.Adam(lr = 0.0005)
model_2.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['acc'])

history = model_2.fit([X_train_1,X_train_2], y_train, epochs = 120, batch_size = 256, validation_split = 0.2, callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)])

model summary
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  from ipykernel import kernelapp as app
Model: ""model_3""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 10081, 1)     0                                            
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 10068, 64)    960         input_5[0][0]                    
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 2013, 64)     0           conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 2000, 64)     57408       max_pooling1d_3[0][0]            
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 64)           0           conv1d_6[0][0]                   
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 104)          0                                            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 168)          0           global_max_pooling1d_3[0][0]     
                                                                 input_6[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 128)          21632       concatenate_3[0][0]              
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 2)            258         dense_5[0][0]                    
==================================================================================================
Total params: 80,258
Trainable params: 80,258
Non-trainable params: 0

and the training process
__________________________________________________________________________________________________
Train on 588 samples, validate on 148 samples
Epoch 1/120
588/588 [==============================] - 16s 26ms/step - loss: 5.6355 - acc: 0.4932 - val_loss: 4.1086 - val_acc: 0.6216
Epoch 2/120
588/588 [==============================] - 15s 25ms/step - loss: 4.5977 - acc: 0.5748 - val_loss: 3.8252 - val_acc: 0.4459
Epoch 3/120
588/588 [==============================] - 15s 25ms/step - loss: 4.3815 - acc: 0.4575 - val_loss: 2.4087 - val_acc: 0.6622
Epoch 4/120
588/588 [==============================] - 15s 25ms/step - loss: 3.7480 - acc: 0.6003 - val_loss: 2.0060 - val_acc: 0.6892
Epoch 5/120
588/588 [==============================] - 15s 25ms/step - loss: 3.3019 - acc: 0.5408 - val_loss: 2.3176 - val_acc: 0.5676
Epoch 6/120
588/588 [==============================] - 15s 25ms/step - loss: 3.1739 - acc: 0.5663 - val_loss: 2.2607 - val_acc: 0.6892
Epoch 7/120
588/588 [==============================] - 15s 25ms/step - loss: 3.2322 - acc: 0.6207 - val_loss: 1.8898 - val_acc: 0.7230
Epoch 8/120
588/588 [==============================] - 15s 25ms/step - loss: 2.9777 - acc: 0.6020 - val_loss: 1.8401 - val_acc: 0.7500
Epoch 9/120
588/588 [==============================] - 15s 25ms/step - loss: 2.8982 - acc: 0.6429 - val_loss: 1.8517 - val_acc: 0.7365
Epoch 10/120
588/588 [==============================] - 15s 25ms/step - loss: 2.8342 - acc: 0.6344 - val_loss: 1.7941 - val_acc: 0.7095
Epoch 11/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7426 - acc: 0.6327 - val_loss: 1.8495 - val_acc: 0.7162
Epoch 12/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7340 - acc: 0.6531 - val_loss: 1.7652 - val_acc: 0.7162
Epoch 13/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6680 - acc: 0.6616 - val_loss: 1.8097 - val_acc: 0.7365
Epoch 14/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6922 - acc: 0.6786 - val_loss: 1.7143 - val_acc: 0.7500
Epoch 15/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6161 - acc: 0.6786 - val_loss: 1.6960 - val_acc: 0.7568
Epoch 16/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6054 - acc: 0.6905 - val_loss: 1.6779 - val_acc: 0.7297
Epoch 17/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6072 - acc: 0.6684 - val_loss: 1.6750 - val_acc: 0.7703
Epoch 18/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5907 - acc: 0.6871 - val_loss: 1.6774 - val_acc: 0.7432
Epoch 19/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5854 - acc: 0.6718 - val_loss: 1.6609 - val_acc: 0.7770
Epoch 20/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5621 - acc: 0.6905 - val_loss: 1.6709 - val_acc: 0.7365
Epoch 21/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5515 - acc: 0.6854 - val_loss: 1.6904 - val_acc: 0.7703
Epoch 22/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5749 - acc: 0.6837 - val_loss: 1.6862 - val_acc: 0.7297
Epoch 23/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6212 - acc: 0.6514 - val_loss: 1.7215 - val_acc: 0.7568
Epoch 24/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6532 - acc: 0.6633 - val_loss: 1.7105 - val_acc: 0.7230
Epoch 25/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7300 - acc: 0.6344 - val_loss: 1.6870 - val_acc: 0.7432
Epoch 26/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7355 - acc: 0.6650 - val_loss: 1.6733 - val_acc: 0.7703
Epoch 27/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6336 - acc: 0.6650 - val_loss: 1.6572 - val_acc: 0.7297
Epoch 28/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6018 - acc: 0.6803 - val_loss: 1.7292 - val_acc: 0.7635
Epoch 29/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5448 - acc: 0.7143 - val_loss: 1.8065 - val_acc: 0.7095
Epoch 30/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5724 - acc: 0.6820 - val_loss: 1.8029 - val_acc: 0.7297
Epoch 31/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6622 - acc: 0.6650 - val_loss: 1.6594 - val_acc: 0.7568
Epoch 32/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6211 - acc: 0.6582 - val_loss: 1.6375 - val_acc: 0.7770
Epoch 33/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5911 - acc: 0.6854 - val_loss: 1.6964 - val_acc: 0.7500
Epoch 34/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5050 - acc: 0.7262 - val_loss: 1.8496 - val_acc: 0.6892
Epoch 35/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6012 - acc: 0.6752 - val_loss: 1.7443 - val_acc: 0.7432
Epoch 36/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5688 - acc: 0.6871 - val_loss: 1.6220 - val_acc: 0.7568
Epoch 37/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4843 - acc: 0.7279 - val_loss: 1.6166 - val_acc: 0.7905
Epoch 38/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4707 - acc: 0.7449 - val_loss: 1.6496 - val_acc: 0.7905
Epoch 39/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4683 - acc: 0.7109 - val_loss: 1.6641 - val_acc: 0.7432
Epoch 40/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4671 - acc: 0.7279 - val_loss: 1.6553 - val_acc: 0.7703
Epoch 41/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4479 - acc: 0.7347 - val_loss: 1.6302 - val_acc: 0.7973
Epoch 42/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4355 - acc: 0.7551 - val_loss: 1.6241 - val_acc: 0.7973
Epoch 43/120
588/588 [==============================] - 14s 25ms/step - loss: 2.4286 - acc: 0.7568 - val_loss: 1.6249 - val_acc: 0.7973
Epoch 44/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4250 - acc: 0.7585 - val_loss: 1.6248 - val_acc: 0.7770
Epoch 45/120
588/588 [==============================] - 14s 25ms/step - loss: 2.4198 - acc: 0.7517 - val_loss: 1.6212 - val_acc: 0.7703
Epoch 46/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4246 - acc: 0.7568 - val_loss: 1.6129 - val_acc: 0.7838
Epoch 47/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4237 - acc: 0.7517 - val_loss: 1.6166 - val_acc: 0.7973
Epoch 48/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4287 - acc: 0.7432 - val_loss: 1.6309 - val_acc: 0.8041
Epoch 49/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4179 - acc: 0.7381 - val_loss: 1.6271 - val_acc: 0.7838
Epoch 50/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4164 - acc: 0.7381 - val_loss: 1.6258 - val_acc: 0.7973
Epoch 51/120
588/588 [==============================] - 14s 24ms/step - loss: 2.1996 - acc: 0.7398 - val_loss: 1.3612 - val_acc: 0.7973
Epoch 52/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1387 - acc: 0.8265 - val_loss: 1.4811 - val_acc: 0.7973
Epoch 53/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1607 - acc: 0.8078 - val_loss: 1.5060 - val_acc: 0.7838
Epoch 54/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1783 - acc: 0.8129 - val_loss: 1.4878 - val_acc: 0.8176
Epoch 55/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1745 - acc: 0.8197 - val_loss: 1.4762 - val_acc: 0.8108
Epoch 56/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1764 - acc: 0.8129 - val_loss: 1.4631 - val_acc: 0.7905
Epoch 57/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1637 - acc: 0.8078 - val_loss: 1.4615 - val_acc: 0.7770
Epoch 58/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1563 - acc: 0.8112 - val_loss: 1.4487 - val_acc: 0.7703
Epoch 59/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1396 - acc: 0.8146 - val_loss: 1.4362 - val_acc: 0.7905
Epoch 60/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1240 - acc: 0.8316 - val_loss: 1.4333 - val_acc: 0.8041
Epoch 61/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1173 - acc: 0.8333 - val_loss: 1.4369 - val_acc: 0.8041
Epoch 62/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1228 - acc: 0.8384 - val_loss: 1.4393 - val_acc: 0.8041
Epoch 63/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1113 - acc: 0.8316 - val_loss: 1.4380 - val_acc: 0.8041
Epoch 64/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1102 - acc: 0.8452 - val_loss: 1.4217 - val_acc: 0.8041
Epoch 65/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0961 - acc: 0.8469 - val_loss: 1.4129 - val_acc: 0.7973
Epoch 66/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0903 - acc: 0.8537 - val_loss: 1.4019 - val_acc: 0.8041
Epoch 67/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0890 - acc: 0.8503 - val_loss: 1.3850 - val_acc: 0.8176
Epoch 68/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0878 - acc: 0.8520 - val_loss: 1.4035 - val_acc: 0.7635
Epoch 69/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0984 - acc: 0.8469 - val_loss: 1.4060 - val_acc: 0.8041
Epoch 70/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0893 - acc: 0.8418 - val_loss: 1.3981 - val_acc: 0.7973
Epoch 71/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0876 - acc: 0.8605 - val_loss: 1.3951 - val_acc: 0.8041__________________________________________________________________________________________________
Train on 588 samples, validate on 148 samples
Epoch 1/120
588/588 [==============================] - 16s 26ms/step - loss: 5.6355 - acc: 0.4932 - val_loss: 4.1086 - val_acc: 0.6216
Epoch 2/120
588/588 [==============================] - 15s 25ms/step - loss: 4.5977 - acc: 0.5748 - val_loss: 3.8252 - val_acc: 0.4459
Epoch 3/120
588/588 [==============================] - 15s 25ms/step - loss: 4.3815 - acc: 0.4575 - val_loss: 2.4087 - val_acc: 0.6622
Epoch 4/120
588/588 [==============================] - 15s 25ms/step - loss: 3.7480 - acc: 0.6003 - val_loss: 2.0060 - val_acc: 0.6892
Epoch 5/120
588/588 [==============================] - 15s 25ms/step - loss: 3.3019 - acc: 0.5408 - val_loss: 2.3176 - val_acc: 0.5676
Epoch 6/120
588/588 [==============================] - 15s 25ms/step - loss: 3.1739 - acc: 0.5663 - val_loss: 2.2607 - val_acc: 0.6892
Epoch 7/120
588/588 [==============================] - 15s 25ms/step - loss: 3.2322 - acc: 0.6207 - val_loss: 1.8898 - val_acc: 0.7230
Epoch 8/120
588/588 [==============================] - 15s 25ms/step - loss: 2.9777 - acc: 0.6020 - val_loss: 1.8401 - val_acc: 0.7500
Epoch 9/120
588/588 [==============================] - 15s 25ms/step - loss: 2.8982 - acc: 0.6429 - val_loss: 1.8517 - val_acc: 0.7365
Epoch 10/120
588/588 [==============================] - 15s 25ms/step - loss: 2.8342 - acc: 0.6344 - val_loss: 1.7941 - val_acc: 0.7095
Epoch 11/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7426 - acc: 0.6327 - val_loss: 1.8495 - val_acc: 0.7162
Epoch 12/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7340 - acc: 0.6531 - val_loss: 1.7652 - val_acc: 0.7162
Epoch 13/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6680 - acc: 0.6616 - val_loss: 1.8097 - val_acc: 0.7365
Epoch 14/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6922 - acc: 0.6786 - val_loss: 1.7143 - val_acc: 0.7500
Epoch 15/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6161 - acc: 0.6786 - val_loss: 1.6960 - val_acc: 0.7568
Epoch 16/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6054 - acc: 0.6905 - val_loss: 1.6779 - val_acc: 0.7297
Epoch 17/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6072 - acc: 0.6684 - val_loss: 1.6750 - val_acc: 0.7703
Epoch 18/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5907 - acc: 0.6871 - val_loss: 1.6774 - val_acc: 0.7432
Epoch 19/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5854 - acc: 0.6718 - val_loss: 1.6609 - val_acc: 0.7770
Epoch 20/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5621 - acc: 0.6905 - val_loss: 1.6709 - val_acc: 0.7365
Epoch 21/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5515 - acc: 0.6854 - val_loss: 1.6904 - val_acc: 0.7703
Epoch 22/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5749 - acc: 0.6837 - val_loss: 1.6862 - val_acc: 0.7297
Epoch 23/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6212 - acc: 0.6514 - val_loss: 1.7215 - val_acc: 0.7568
Epoch 24/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6532 - acc: 0.6633 - val_loss: 1.7105 - val_acc: 0.7230
Epoch 25/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7300 - acc: 0.6344 - val_loss: 1.6870 - val_acc: 0.7432
Epoch 26/120
588/588 [==============================] - 15s 25ms/step - loss: 2.7355 - acc: 0.6650 - val_loss: 1.6733 - val_acc: 0.7703
Epoch 27/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6336 - acc: 0.6650 - val_loss: 1.6572 - val_acc: 0.7297
Epoch 28/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6018 - acc: 0.6803 - val_loss: 1.7292 - val_acc: 0.7635
Epoch 29/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5448 - acc: 0.7143 - val_loss: 1.8065 - val_acc: 0.7095
Epoch 30/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5724 - acc: 0.6820 - val_loss: 1.8029 - val_acc: 0.7297
Epoch 31/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6622 - acc: 0.6650 - val_loss: 1.6594 - val_acc: 0.7568
Epoch 32/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6211 - acc: 0.6582 - val_loss: 1.6375 - val_acc: 0.7770
Epoch 33/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5911 - acc: 0.6854 - val_loss: 1.6964 - val_acc: 0.7500
Epoch 34/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5050 - acc: 0.7262 - val_loss: 1.8496 - val_acc: 0.6892
Epoch 35/120
588/588 [==============================] - 15s 25ms/step - loss: 2.6012 - acc: 0.6752 - val_loss: 1.7443 - val_acc: 0.7432
Epoch 36/120
588/588 [==============================] - 15s 25ms/step - loss: 2.5688 - acc: 0.6871 - val_loss: 1.6220 - val_acc: 0.7568
Epoch 37/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4843 - acc: 0.7279 - val_loss: 1.6166 - val_acc: 0.7905
Epoch 38/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4707 - acc: 0.7449 - val_loss: 1.6496 - val_acc: 0.7905
Epoch 39/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4683 - acc: 0.7109 - val_loss: 1.6641 - val_acc: 0.7432
Epoch 40/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4671 - acc: 0.7279 - val_loss: 1.6553 - val_acc: 0.7703
Epoch 41/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4479 - acc: 0.7347 - val_loss: 1.6302 - val_acc: 0.7973
Epoch 42/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4355 - acc: 0.7551 - val_loss: 1.6241 - val_acc: 0.7973
Epoch 43/120
588/588 [==============================] - 14s 25ms/step - loss: 2.4286 - acc: 0.7568 - val_loss: 1.6249 - val_acc: 0.7973
Epoch 44/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4250 - acc: 0.7585 - val_loss: 1.6248 - val_acc: 0.7770
Epoch 45/120
588/588 [==============================] - 14s 25ms/step - loss: 2.4198 - acc: 0.7517 - val_loss: 1.6212 - val_acc: 0.7703
Epoch 46/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4246 - acc: 0.7568 - val_loss: 1.6129 - val_acc: 0.7838
Epoch 47/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4237 - acc: 0.7517 - val_loss: 1.6166 - val_acc: 0.7973
Epoch 48/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4287 - acc: 0.7432 - val_loss: 1.6309 - val_acc: 0.8041
Epoch 49/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4179 - acc: 0.7381 - val_loss: 1.6271 - val_acc: 0.7838
Epoch 50/120
588/588 [==============================] - 15s 25ms/step - loss: 2.4164 - acc: 0.7381 - val_loss: 1.6258 - val_acc: 0.7973
Epoch 51/120
588/588 [==============================] - 14s 24ms/step - loss: 2.1996 - acc: 0.7398 - val_loss: 1.3612 - val_acc: 0.7973
Epoch 52/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1387 - acc: 0.8265 - val_loss: 1.4811 - val_acc: 0.7973
Epoch 53/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1607 - acc: 0.8078 - val_loss: 1.5060 - val_acc: 0.7838
Epoch 54/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1783 - acc: 0.8129 - val_loss: 1.4878 - val_acc: 0.8176
Epoch 55/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1745 - acc: 0.8197 - val_loss: 1.4762 - val_acc: 0.8108
Epoch 56/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1764 - acc: 0.8129 - val_loss: 1.4631 - val_acc: 0.7905
Epoch 57/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1637 - acc: 0.8078 - val_loss: 1.4615 - val_acc: 0.7770
Epoch 58/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1563 - acc: 0.8112 - val_loss: 1.4487 - val_acc: 0.7703
Epoch 59/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1396 - acc: 0.8146 - val_loss: 1.4362 - val_acc: 0.7905
Epoch 60/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1240 - acc: 0.8316 - val_loss: 1.4333 - val_acc: 0.8041
Epoch 61/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1173 - acc: 0.8333 - val_loss: 1.4369 - val_acc: 0.8041
Epoch 62/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1228 - acc: 0.8384 - val_loss: 1.4393 - val_acc: 0.8041
Epoch 63/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1113 - acc: 0.8316 - val_loss: 1.4380 - val_acc: 0.8041
Epoch 64/120
588/588 [==============================] - 15s 25ms/step - loss: 1.1102 - acc: 0.8452 - val_loss: 1.4217 - val_acc: 0.8041
Epoch 65/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0961 - acc: 0.8469 - val_loss: 1.4129 - val_acc: 0.7973
Epoch 66/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0903 - acc: 0.8537 - val_loss: 1.4019 - val_acc: 0.8041
Epoch 67/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0890 - acc: 0.8503 - val_loss: 1.3850 - val_acc: 0.8176
Epoch 68/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0878 - acc: 0.8520 - val_loss: 1.4035 - val_acc: 0.7635
Epoch 69/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0984 - acc: 0.8469 - val_loss: 1.4060 - val_acc: 0.8041
Epoch 70/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0893 - acc: 0.8418 - val_loss: 1.3981 - val_acc: 0.7973
Epoch 71/120
588/588 [==============================] - 15s 25ms/step - loss: 1.0876 - acc: 0.8605 - val_loss: 1.3951 - val_acc: 0.8041

Notice how at first acc is lower than val_acc and later is greater than val_acc. Can someone please shed some light what could be happening here? Thank you
","['neural-networks', 'machine-learning', 'training', 'accuracy', 'cross-validation']","very interesting questions:1. what exactly is happening when training and validation accuracy change during training2. what do different behaviours implyThe acc and val_acc normally differ from each other due to different split sizes.Overfitting only occurs when the graph fashion or tendency changes and val_acc starts to drop and accuracy keeping increasing. This means that your model can not do any better with the validation dataset (non previously seen images).I work with loss and val_loss which are highly correlated with accuracy. Normally the loss is the inverse, so interpret the comments above in the inverse sense (sorry about the confusion but I'm taking this example from my current experiments) I hope it helps:There are 2 experiments, orange and grey.In both experiments, val_loss is always slightly higher than loss (because of my current validation split which it happens to be also 0.2, but normally is 0.01 and val_loss is even higher).On both experiments the loss trend is linearly decreasing, this is because gradient descent works and the loss functions is well defined and it converges.Orange experiment is overfitting from epoch 20 onwards because, the val_loss won't drop any more and, on the contrary, it start increasing.Grey experiment is just right, both loss and val_loss are still decreasing, and although the val_loss might be greater than loss it is not overfitting because it is still decreasing. So that is why it is still training :)Complex concepts here, I hope I was able to explain myself clearly! Cheers"
Does adding a constant to all rewards change the set of optimal policies in episodic tasks?,"
I'm taking a Coursera course on Reinforcement learning. There was a question there that wasn't addressed in the learning material: Does adding a constant to all rewards change the set of optimal policies in episodic tasks?
The answer is Yes - Adding a constant to the reward signal can make longer episodes more or less advantageous (depending on whether the constant is positive or negative).
Can anyone explain why is this so? And why it doesn't change in the case of continuous (non episodic) tasks? I don't see why adding a constant matters - as an optimal policy would still want to get the maximum reward...
Can anyone give an example of this?
",['reinforcement-learning'],"Generally we can write for $R_c$ the total reward with added constant $c$ of a policy as
$$
R_c = \sum_{i=0}^K (r_i + c) \gamma^i = \sum_{i=0}^K r_i \gamma^i + \sum_{i=0}^K c \gamma^i
$$
So if we have two policies with the same total reward (without added constant)
$$
\sum_{i=0}^{K_1} r_i^1 \gamma^i = \sum_{i=0}^{K_2} r_i^2 \gamma^i
$$
but with different lengths $K_1 \neq K_2$ the total reward with added constant will be different, because the second term in $R_c$ ( $\sum_{i=0}^K c \gamma^i$ ) will be different.As an example: Consider two optimal policies, both generating the same cumulative reward of 10, but the first policy visits 4 states, before it reaches a terminal state, while the second visits only two states. The rewards can be written as:
$$
10 + 0 + 0 + 0 = 10
$$
and 
$$
0 + 10 = 10
$$
But when we add 100 to every reward:
$$
110 + 100 + 100 + 100 = 410
$$
and 
$$
100 + 110 = 210
$$
Thus, now the first one is better.In the continious case, the episodes always have length $K = \infty$. Therefore, they always have the same length, and adding a constant doesnt change anything, because the second term in $R_c$ stays the same."
Can learned feature vectors be considered a good encryption?,"
Considering I have some neural network that, using supervised learning, transforms a string into a learned feature vector where ""close"" strings will result into more close vectors.
I know that since a NN is no one-way-function there is a way to retrieve the input data from my output if I have the entire network at hand (if I know biases, weights, etc.)
My question is if the network is not known, is there a way for me (using ie. some probabilistic distributions) to make assumptions or even reconstruct the input data?
","['neural-networks', 'machine-learning']",
Why is the stationary distribution independent of the initial state in the proof of the policy gradient theorem?,"
I was going through the proof of the policy gradient theorem here: https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#svpg 
In the section ""Proof of Policy Gradient Theorem"" in the block of equations just under the sentence ""The nice rewriting above allows us to exclude the derivative of Q-value function..."" they set
$$
\eta (s) = \sum^\infty_{k=0} \rho^\pi(s_0 \rightarrow s, k)
$$
and
$$
\sum_s \eta (s) = const
$$
Thus, they basically assume, that the stationary distribution is not dependent on the initial state. But how can we justify this? If the MDP is described by a block diagonal transition matrix, in my mind this should not hold.
","['reinforcement-learning', 'policy-gradients', 'proofs', 'policy-gradient-theorem']","I think your doubt is completely reasonable. Probably there is an additional assumption that they (both Lilian Weng and Rich Sutton (pag.269)) do not make explicit in the proof and that is that your MDP is not only stationary, but also ergodic. A particular property of those systems is that the probability of eventually reaching a state $s$ from a starting point $s_0$ is 1. In such a case it is clear that $\eta(s)$ exists and is independent of any $s_0$ chosen. Clearly, an MDP with block-diagonal transition matrix does not satisfy such an assumption since the starting point completely restricts those states you can reach in an infinite time.What I do not understand is why Rich Sutton does mention ergodicity as a necessary condition in the case of a ""continuing task"", as opposed to ""episodic tasks"" (pag.275). For me, their proof requires this condition in both cases.As an additional note, I also think that Lilian Weng does not really explain why we should buy that from the initial reasonable definition $J(\theta)=\sum_\mathcal S d^{\pi_\theta}(s)V^{\pi_\theta}(s)$ we should accept the much simpler one $J(\theta)=V^{\pi_\theta}(s_0)$. I guess the only reason is that the gradient of the initial expression does require to know the gradient of $d^{\pi_\theta}(s)$ and so you would be accepting the approximation:$$\nabla_\theta J(\theta)=\nabla_\theta\left(\sum_\mathcal S d^{\pi_\theta}(s)V^{\pi_\theta}(s)\right)\approx\sum_\mathcal S d^{\pi_\theta}(s)\nabla_\theta V^{\pi_\theta}(s),$$where the last term is just $\nabla_\theta V^{\pi_\theta}(s_0)$ under the ergodicity assumption. "
What ML algorithms would you suggest in fraud detection?,"
There are a lot of ML algorithms suggested for fraud detection. Now, I have not been able to find a general overview for all of them. My goal is to create this overview. What algorithms would you suggest and why? 
","['machine-learning', 'algorithm']",
How are the inputs passed to the neural network during training for the XOR classification task?,"
Let's suppose we have to train a neural network for the XOR classification task.
Are the inputs $(00, 01, 10, 11)$ inserted in a sequential way? For example, we first insert the 00 and change the weights, then the 01 and again slightly change them, etc. Or is there another way it can be implemented?
","['neural-networks', 'machine-learning', 'classification', 'training']","Hello and welcome to the community. There are multiple ways you can train a neural network: stochastic, mini-batch and batch. What you explained is the stochastic mode, where you input one training example 01 for example, calculate the gradients and update the networks weights before the next training example is fed. You could also select multiple such examples (a mini-batch) and update the weights only after you computed all the outputs (for this particular mini-batch). Finally you can use a batch size which is equal with the total number of examples in your dataset so you will update the weights only after you have all the outputs for all samples. Each of those methods have their own strengths and weaknesses, depending on your dataset you might prefer one over the others."
MCTS: How to choose the final action from the root,"
When the time allotted to Monte Carlo tree search runs out, what action should be chosen from the root?

The original UCT paper (2006) says bestAction in their algorithm.

Monte-Carlo Tree Search: A New Framework for Game AI (2008) says



The game action finally executed by the program in the actual game, is the one corresponding to the child which was explored the most.


A Survey of Monte Carlo Tree Search Methods (2012) says


The basic algorithm involves iteratively building a search tree until some predefined computational budget â€“ typically a time, memory or iteration constraint â€“ is reached, at which point the search is halted and the best-performing root action returned.
[...] The result of the overall search a(BESTCHILD(v0)) is the action a that leads to the best child of the root node v0, where the exact definition of â€œbestâ€ is defined by the implementation.
[...] As soon as the search is interrupted or the computation budget is reached, the search terminates and an action a of the root nodet0is selected by some mechanism. Schadd [188] describes four criteria for selecting the winning action, based on the work of Chaslot et al [60]:

Max child: Select the root child with the highest reward.

Robust child: Select the most visited root child.

Max-Robust child: Select the root child with both the highest visit count and the highest reward. If none exist, then continue searching until an acceptable visit count is achieved [70].

Secure child: Select the child which maximises a lower confidence bound.


[...] Once some computational budget has been reached, the algorithm terminates and returns the best move found,corresponding to the child of the root with the highest visit count.
The return value of the overall search in this case is a(BESTCHILD(v0,0)) which will give the action a that leads to the child with the highest reward, since the exploration parameter c is set to 0 for this final call on the root node v0. The algorithm could instead return the action that leads to the most visited child; these two options will usually â€“ but not always! â€“ describe the same action. This potential discrepancy is addressed in the Go program ERICA by continuing the search if the most visited root action is not also the one with the highest reward. This improved ERICAâ€™s winning rate against GNU GO from 47% to 55% [107].

But their algorithm 2 uses the same criterion as the internal-node selection policy:
$$\operatorname{argmax}_{v'} \frac{Q(v')}{N(v')} + c \sqrt{\frac{2 \ln N(v)}{N(v')}}$$
which is neither the max child nor the robust child! This situation is quite confusing, and I'm wondering which approach is nowadays considered most successful/appropriate.
","['algorithm', 'monte-carlo-tree-search', 'monte-carlo-methods', 'planning', 'tree-search']","By far the most commonly used strategy is to select the child with the highest number of visits. This is as described in the 2008 paper you linked. It's also what's referred to as the ""robust child"" in the 2012 paper you linked.In algorithm 2 of the 2012 paper, they actually use the highest average reward, which corresponds to ""Max child"". It looks like they're using the UCB1 policy, but they actually use a value of $0$ for the exploration parameter $c$, which makes the entire square root term drop out. This is also explained in the text at the end of your quote. But usually, a robust child / max visit count performs better.Progressive Strategies for Monte-Carlo Tree Search is a different paper from 2008, in which these strategies are experimented with a bit. Usually, they perform similarly, but a robust child tends to be the best if there is any difference at all."
How can I minimise the false positives?,"
I have 50,000 samples. Of these 23,000 belong to the desired class $A$. I can sacrifice the number of instances that are classified as belonging to the desired class $A$. It will be enough for me to get 7000 instances in the desired class $A$, provided that most of these instances classified as belonging to $A$ really belong to the desired class $A$. How can I do this?
The following is the confusion matrix in the case the instances are perfectly classified.
[[23000   0]
 [  0 27000]]

But it is unlikely to obtain this confusion matrix, so I'm quite satisfied with the following confusion matrix.
[[7000   16000]
 [  500 26500]]

I am currently using the sklearn library. I mainly use algorithms based on decision trees, as they are quite fast in the calculation.
","['machine-learning', 'classification', 'python']","I think you're looking for the minimization of false positives, that is, the instances that are classified as belonging to the desired class (the positive part of false positives) but that do not actually belong to that class (the false part of false positives). In practice, given your constraints, you may want to maximize the precision, while maintaining a good recall.In this answer to the question How can the model be tuned to improve precision, when precision is much more important than recall?, the user suggests performing a grid search (using sklearn.grid_search.GridSearchCV(clf, param_grid, scoring=""precision"")) to find the parameters of the model that maximize the precision. See also the question Classifier with adjustable precision vs recall."
What are the current big challenges in natural language processing and understanding?,"
I'm doing a paper for a class on the topic of big problems that are still prevalent in AI, specifically in the area of natural language processing and understanding. From what I understand, the areas:

Text classification
Entity recognition 
Translation
POS tagging

are for the most part solved or perform at a high level currently, but areas such as:

Text summarization
Conversational systems
Contextual systems (relying on the previous context that will impact current prediction)

are still relatively unsolved or are a big area of research (although this could very well change soon with the releases of big transformer models from what I've read).
For people who have experience in the field, what are areas that are still big challenges in NLP and NLU? Why are these areas (doesn't have to be ones I've listed) so tough to figure out?
","['natural-language-processing', 'natural-language-understanding']",
Prerequisites for Andrew Ng's Machine Learning Course [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 3 years ago.







                        Improve this question
                    



I am planning to enroll for Andrew Ng's Machine Learning course https://www.coursera.org/learn/machine-learning. I've no background in math. Is it OK if I start the course and learn math as and when required?
",['machine-learning'],"This course is focused on machine learning using MATLAB, which is not practical nowadays as it is a programming language used specifically for computing, and cannot display GUI or communicate through the network. The language is powerful but limited in some ways. Nowadays most people use python for machine learning, as it is versatile and can connect to other backend like C++, java, JavaScript easily. The language is also a general language, and unlike MATLAB can do many things not limited to computing.If you really want to join this course, I would recommend first learning MATLAB language and also learn basic calculus like derivatives. This would greatly help on your learning of the course.However if you want to here serious about machine learning, I would encourage you to enroll in the Deep Learning specialization also by Andrew Ng on Coursera.
https://www.coursera.org/specializations/deep-learning
This course uses python as the programming language and teaches more modern approaches to deep learning like recurrent neural networks, convolutional neural networks and more. It also talks more about application of neural network. There is also theory, but it also talks about application of a specific algorithm and how it works. Hope I can help you."
"Building 'evaluation' neural networks for go, reversi, checkers etc, how to train?","
I'm trying to build neural networks for games like Go, Reversi, Othello, Checkers, or even tic-tac-toe, not by calculating a move, but by making them evaluate a position. 
The input is any board situation. The output is a score or estimate for the probability to win, or how favorable a given position is. Where 1 = guaranteed to win and 0 = guaranteed to lose.
In any given turn I can then loop over all possible moves for the current player, evaluate the resulting game situations, and pick the one with the highest score. 
Hopefully, by letting this neural network play a trillion games vs itself, it can develop a sensible scoring function resulting in strong play. 
Question: How do I train such a network ?
In every game, I can keep evaluating and making moves back and forth until one of the AI players wins. In that case the last game situation (right before the winning move) for the winning player should have a target value of 1, and the opposite situation (for the losing player) has a target value of 0. 
Note that I don't intend to make the evaluation network two-sided. I encode the game situation as always ""my own"" pieces vs ""the opponent"", and then evaluate a score from my own (i.e. the current player's) side or perspective. Then after I pick a move, I flip sides so to speak, so the opponent pieces now become my own and vice versa, and then evaluate scores again (now from the other side's perspective) for the next counter move.
So the input to such a network does explicitly encode black and white pieces, or naughts and crosses (in case of tic-tac-toe) but just my pieces vs their pieces. And then evaluates how favorable the given game situation is for me, always assuming it's my turn.
I can obviously assign a desired score or truth value for the last move in the game (1 for win, 0 for loss) but how do I backpropagate that towards earlier situations in a played game?
Should I somehow distribute the 1 or 0 result back a few steps, with a decaying adjustment factor or learning rate? In a game with 40 turns, it might make sense to consider the last few situations as good or bad (being close to winning or losing) but I guess that shouldn't reflect all the way back to the first few moves in the game. 
Or am I completely mistaken with this approach and is this not how it's supposed to be done?
","['neural-networks', 'reinforcement-learning', 'game-ai', 'software-evaluation', 'alphazero']","The evaluation of the last steps in the game can be made with the 1 and 0 as you said. For all the other steps, the evaluation should be the evaluation of the best next step with a small decay. "
Is weight pruning applied to all layers or only to dense layers in CNNs?,"
I was reading about weight pruning in convolutional neural networks. Is it applied for all the layers including convolutional layers or only it is done for dense layers? 
",['convolutional-neural-networks'],
What form of output would be needed to train a model on a connect 4 AI?,"
I've had a big interest in machine learning for a while, and I've followed along a few tutorials, but have never made my own project. After losing many games of connect 4 with my friends, I decided to try to make a replica of that, then create a neural network and AI to play against me (Or at least something where I can enter the current board scenario, and it will output which row is the best move). This may be an ambitious first project, but I'm willing to put in the work and research to create something I'm proud of. I created the game using p5.js, and though it may be simple, I'm really happy with how it turned out, as it's one of my first more interesting and unique projects in computer science. Now I don't know a ton about ML, so bear with me. I would like the use pytorch, but I'm open to tensorflow/keras as well. 
Here are a few of my questions:

What output do I need to train? My game currently doesn't have a win condition. Would an array or matrix filled with a 0 where there isn't a chip, a 1 where a red chip is, and a 2 for a yellow? Ie

[0,0,0,0
 1,0,0,0
 1,0,0,0
 1,0,2,0
 1,2,2,2]

and enter a 1 somewhere to signify this as a win for player 1? Could an AI recognize this 4 in a row pattern as what needs to be done to win? 

What is the best way to simulate a lot of games to get my training data? I'm imagining using an RNG to drop chips randomly, export the data output to a file and then enter whether it was a win for p1, p2, or a tie?
Any other general words of wisdom or links to read?

Thanks so much for reading this and any help you can offer?
","['reinforcement-learning', 'training']",
Does the neural network calculate different relations between inputs automatically?,"
Suppose you want to predict the price of some stock. Let's say you use the following features.
OpenPrice  
HighPrice
LowPrice
ClosePrice

Is it useful to create new features like the following ones?
BodySize = ClosePrice - OpenPrice  

or the size of the tail    
TailUp = HighPrice - Max(OpenPrice, ClosePrice)  

Or we don't need to do that because we are adding noise and the neural network is going to calculate those values inside?    
The case of the body size maybe is a bit different from the tail, because for the tail we need to use a non-linear function (the max operation). So maybe is it important to add the input when it is not a linear relationship between the other inputs not if it's linear?     
Another example. Consider a box, with height $X$, width $Y$ and length $Z$.
And suppose the real important input is the volume, will the neural network discover that the correlation is $X * Y * Z$? Or we need to put the volume as input too?    
Sorry if it's a dumb question but I'm trying to understand what is doing internally the neural network with the inputs, if it's finding (somehow) all the mathematically possible relations between all the inputs or we need to specify the relations between the inputs that we consider important (heuristically) for the problem to solve?
",['neural-networks'],"On paper, one expects a complex enough network to determine any complicated function of a limited number of inputs, given a large enough dataset. But in practice, there is no limit to the possible difficulty of the function to be learnt, and the datasets can be relatively small on occasion. In such cases - or arguably in general - it is definitely a good idea to define some combination of the inputs depending on some heuristics as you suggested. If you think some combination of inputs is an important variable by itself, you definitely should include it in your inputs. We can visualize this situation in TensorFlow playground. Consider the circular pattern dataset on top left corner with some noise. You can use the default setting: $x_1$ and $x_2$ as inputs with 2 hidden layers with 4 and 2 neurons respectively. It should learn the pattern in less than 100 epochs. But if you reduce the number of neurons in the second layer to 2, it is not going to get as good as before. So, you are making the model more complicated to get the correct answer.You can experiment and see that one needs at least one 3 neuron layer to get the correct classification from just $x_1$ and $x_2$. Now, if we examine the dataset, we see the circles so we know that instead of $x_1$ and $x_2$, we can try $x_1^2$ and $x_2^2$. This will learn perfectly without any hidden layers as the function is linear in these parameters. The lesson to be learnt here is that, our prior knowledge of the circle ($x_1^2 + x_2^2 = r^2$)  and familiarity with the data helped us in getting a good result with a simpler model (smaller number of neurons), by using derived inputs.Take the spiral data at the lower right corner for a more challenging problem. For this one, if you do not use any derived features, it is not likely to give you the correct result, even with several hidden layers. Keep in mind that every extra neuron is a potential source of overfitting, on top of being a computational burden.Of course the problem here is overly simplified but I expect the situation to be more or less the same for any complicated problem. In practice, we do not have infinite datasets or infinite compute times and the model complexity is always a restriction, so if you have any reason to think some relation between your inputs is relevant for your final result, you definitely should include it by hand at the beginning. "
The membership function of Consequents (Outputs) in Fuzzy classifier,"
The
problem in Iris data is to classify three species of iris (setosa, versicolor and virginica) by
four-dimensional attribute vectors consisting of 

sepal length (x1)
sepal width (x2)
petal length (x3)
petal width (x4)

Every attribute of the fuzzy classifier is assigned with three linguistic
terms (fuzzy sets): short, medium and long. With normalized attribute values the
membership functions of these fuzzy sets for all the four attributes are depicted in the
figure below:

Now, consider the situation that a set of Rules are given - some examples are as below:

R1: If (x3=short OR medium) AND (x4=short) Then iris setosa
R2: If (x2=short OR medium) AND (x3=long) AND (x4=long) Then iris virginica

Now, I want to make a fuzzy classifier on Iris dataset. The problem here is, I need to use a membership function for Consequents in Rules(i.e, The 3 classes) so as to be able to compute aggregation of rules and difuzzification. 

What is the proper pre-defined domain and membership function for three classes? 
All the Consequents (Outputs)  in fuzzy classifier need to acquire membership functions?

","['machine-learning', 'classification', 'fuzzy-logic']",
How to reduce variance of the model loss during training?,"
I know that stochastic gradient descent always gives different results. What are the best practices to reduce this variance today?
I tried to predict simple function with two different approaches and every time I train them I see very different results.
Input data:
def plot(model_out):
  fig, ax = plt.subplots()
  ax.grid(True, which='both')
  ax.axhline(y=0, color='k', linewidth=1)
  ax.axvline(x=0, color='k', linewidth=1)

  ax.plot(x_line, y_line, c='g', linewidth=1)
  ax.scatter(inputs, targets, c='b', s=8)
  ax.scatter(inputs, model_out, c='r', s=8)

a = 5.0; b = 3.0; x_left, x_right = -16., 16.
NUM_EXAMPLES = 200
noise   = tf.random.normal((NUM_EXAMPLES,1))

inputs  = tf.random.uniform((NUM_EXAMPLES,1), x_left, x_right)
targets = a * tf.sin(inputs) + b + noise
x_line  = tf.linspace(x_left, x_right, 500)
y_line  = a * tf.sin(x_line) + b

Keras training:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(50, activation='relu', input_shape=(1,)))
model.add(tf.keras.layers.Dense(50, activation='relu'))
model.add(tf.keras.layers.Dense(1))

model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.01))
model.fit(inputs, targets, batch_size=200, epochs=2000, verbose=0)

print(model.evaluate(inputs, targets, verbose=0))
plot(model.predict(inputs))


Manual training:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(50, activation='relu', input_shape=(1,)))
model.add(tf.keras.layers.Dense(50, activation='relu'))
model.add(tf.keras.layers.Dense(1))

optimizer = tf.keras.optimizers.Adam(0.01)

@tf.function
def train_step(inpt, targ):
  with tf.GradientTape() as g:
    model_out = model(inpt)
    model_loss = tf.reduce_mean(tf.square(tf.math.subtract(targ, model_out)))

  gradients = g.gradient(model_loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  return model_loss

train_ds = tf.data.Dataset.from_tensor_slices((inputs, targets))
train_ds = train_ds.repeat(2000).batch(200)

def train(train_ds):
  for inpt, targ in train_ds:
    model_loss = train_step(inpt, targ)
  tf.print(model_loss)

train(train_ds)
plot(tf.squeeze(model(inputs)))


","['python', 'tensorflow', 'keras', 'gradient-descent']",
Which AI conference presentation on predicting terrorist movement inside buildings caused protest in the audience and media?,"
I remember reading an online news article a while ago on AI ethics that described a conference presentation on the subject of a military AI system that predicted terrorist movement inside buildings and used drones to shoot them when they exited the building. The presentation caused protests and sharp criticism in the audience due to the dehumanizing nature of the AI usage presented. 
The article was in mainstream media, but I am unable to find it with Google search. 
Which AI conference presentation was it and are any articles on it available?
",['ethics'],
What would be a good internal language for an AI?,"
For an AI to represent the world, it would be good if it could translate human sentences into something more precise.
We know, for example, that mathematics can be built up from set theory. So representing statement in language of set theory might be useful.
For example

All grass is green

is something like:

$\forall x \in grass: isGreen(x)$

But then I learned that set theory is built up from something more basic. And that theorem provers use a special form of higher-order logic of types. Then there is propositional logic.
Basically what the AI would need would be some way representing statements, some axioms, and ways to manipulate the statements.
Thus what would be a good language to use as an internal language for an AI?
","['knowledge-representation', 'programming-languages', 'language-model']",
Which algorithm and architecture to use for 1:1 matrix transformation of an 8X8 dimension?,"
I would like to map the simplest 8X8 matrices, one to one, but am not sure which AI algorithm would give the best performance. I am thinking about the DeepLearning4j, however, I don't know which neural architecture to use.
I would like to make a simple and a very ""stupid"" bot for playing the chess. The result I am hoping to obtain is a system which can learn the chess rules rather than make intelligent moves (although it would be great if it could do that as well).  I understand that chess bots are nothing new, however, I am not interested in making a chess bot but a system that can simply learn by giving nothing else than an 8X8 matrix as an input and 8X8 matrix as an output. Chess is irrelevant, and it can be any other game that can be represented with values within an 8X8 matrix.
I am aware of the newest image mappers that transform horses to zebraz, but I need something more precise and 1 to 1 learning.
The amount of data I can get for learning is also not an issue (I can generate as much as I want).
","['machine-learning', 'deep-learning', 'regression', 'rule-acquisition']",
How can I train YOLO with the COCO dataset?,"
I am trying to implement the original YOLO architecture for object detection, but I am using the COCO dataset. However, I am a bit confused about the image sizes of COCO. The original YOLO was trained on the VOC dataset and it is designed to take 448x448 size images. Since I am using COCO, I thought of cropping down the images to that size. But that would mean I would have to change the annotations file and it might make the process of object detection a bit harder because some of the objects might not be visible.
I am pretty new to this, so I am not sure if this is the right way or what are some other things that I can do. Any help will be appreciated.
","['convolutional-neural-networks', 'object-detection', 'pytorch', 'yolo', 'coco-dataset']",
What is the time complexity of the forward pass algorithm of a feedforward neural network?,"
How do I determine the time complexity of the forward pass algorithm of a feedforward neural network? How many multiplications are done to generate the output?
","['neural-networks', 'feedforward-neural-networks', 'time-complexity', 'complexity-theory', 'forward-pass']","Let's suppose that we have an MLP with $15$ inputs, $20$ hidden neurons and $2$ output neurons. The operations performed are only in the hidden and output neurons, given that the input neurons only represent the inputs (so they do not perform any operation).Each hidden neuron performs a linear combination of its inputs followed by the application of a non-linear (or activation) function. So, each hidden neuron $j$ performs the following operation\begin{align}
o_j = \sigma \left(\sum_{i}^{15} w_{ij}x_i \right),\tag{1}\label{1}
\end{align}where $i$ is the input coming from the input neuron $i$, $w_{ij}$ is the weight of the connection from the input neuron $i$ to the hidden neuron $j$, and $o_j$ is used to denote the output of neuron $j$. There are $20$ hidden neurons and, for each of them, according to equation $\ref{1}$, we perform $15$ multiplications (ignoring any multiplications that might be associated with the activation function), so $15*20 = 300$ multiplications are performed at the (only) hidden layer. In general, if there are $n$ inputs and $m$ hidden neurons, then $n*m$ multiplications will be performed in the first hidden layer.Now, each neuron $j$ at the next layer (in this case, the output layer), also performs a linear combination followed by the application of an activation function\begin{align}
o_j = \tau \left(\sum_{i}^{20} w_{ij}x_i \right),\tag{2}\label{2}
\end{align}where $\tau$ is another activation function which might or not be equal to $\sigma$, but we ignore all multiplications that might involve the application of the activation functions (we just want to count the ones in the linear combinations). Of course, in this case, $x_i$ corresponds to the activation of neuron $i$ (of the hidden layer).Similarly to the previous reasoning, there are $2$ output neurons and, to compute the output of each of them, $20$ multiplications are performed (in the linear combination), so there are a total of $2*20 = 40$ multiplications at the output layer. So, an MLP with $15$ inputs, $20$ hidden neurons and $2$ output neurons will perform $15*20 + 20*2 = 340$ multiplications (excluding activation functions). Of course, in this case, the number of multiplication depends not only on the number of neurons but also on the input size.In general, an MLP with $n$ inputs, $M$ hidden layers, where the $i$th hidden layer contains $m_i$ hidden neurons, and $k$ output neurons will perform the following number of multiplications (excluding activation functions)\begin{align}
nm_{1} + m_{1}m_{2} + m_{2}m_{3} + \dots + m_{M-1}m_{M} + m_{M}k = nm_{1} + m_{M}k + \sum_{i=1}^{M-1} m_{i}m_{i+1} 
\end{align}which, in big-O notation, can be written as\begin{align}
\Theta\left(nm_{1} + m_{M}k + \sum_{i=1}^{M-1} m_{i}m_{i+1} \right)
\end{align}where $\Theta(\cdot)$ is used (as opposed to $\mathcal{O}(\cdot)$) because this is a strict bound. If you have just one hidden layer, the number of multiplications becomes \begin{align}
\Theta\left(nm_{1} + m_{1}k \right)
\end{align}Of course, at each layer, the number of multiplications can be computed independently of the multiplications of the other layers (you can think of each layer as a perceptron), hence we sum (and not e.g. multiply) the multiplications of each layer when computing the total number of multiplications of the whole MLP.In general, when analyzing the time complexity of an algorithm, we do it with respect to the size of the input. However, in this case, the time complexity (more precisely, the number of multiplications involved in the linear combinations) also depends on the number of layers and the size of each layer. The time complexity of a forward pass of a trained MLP thus is architecture-dependent (which is a similar concept to an output-sensitive algorithm).You can easily include other operations (sums, etc.) in this reasoning to calculate the actual time complexity of a trained MLP."
Interpretability of feature weights from Gaussian process classifier,"
Suppose I trained a Gaussian process classifier with a linear kernel (using GPML toolbox) and got some feature weights for each input feature. 
My question is then:
Does it/When does it make sense to interpret the weights to indicate the real-life importance of each feature or interpret at group level the average over the weights of a group of features? 
","['machine-learning', 'feature-selection', 'weights', 'gaussian-process']",
Does the substituted variable/constant have to appear in the unified term?,"
I'm checking out how to manually apply resolution on a first order predicate logic knowledge base and I'm confused about what is allowed or not in the algorithm.
Let's say that we have the following two clauses (where $A$ and $B$ are constants):
$\neg P(A, B) \vee H(A)$
$\neg L(x_1) \vee P(x_1, y_1)$
If I try to unify these two clauses by making the substitutions $\{x_1/A, y_1/B\}$ do I get $\neg L(A) \vee H(A)$ ? Is it allowed to substitute $y_1$ by $B$ even if $B$ doesn't appear in the unified clause?
Then we have the other way around where:
$\neg P(A, y_1) \vee H(A)$
$\neg L(x_1) \vee P(x_1, B)$
Can I do $\{x_1/A, B/y_1\}$ for $\neg L(A) \vee H(A)$ ?
What about the case where:
$\neg P(A, z_1) \vee H(A)$
$\neg L(x_1) \vee P(x_1, y_1)$
Can I substitute $\{x_1/A, y_1/z_1\}$ and get $\neg L(A) \vee H(A)$ ?
Finally there is also the cases where we have something like this:
$\neg P(x_2, y_2) \vee H(z_1)$
$\neg L(x_1) \vee P(x_1, y_1)$
Can we do $\{x_1/x_2, y_1/y_2\}$ to get $\neg L(x_3) \vee H(z_2)$ ?
I'm really confused about when unification succeeds once we have two clauses with a literal of the same kind (negation in one of them and not in the other one) that are a candidates for unification. 
",['logic'],
How did MuZero learn the rules of chess?,"
Google says that their new AI program MuZero learnt the rules of chess and some other board games without being told so. How is this even possible?
https://towardsdatascience.com/deepmind-unveils-muzero-a-new-agent-that-mastered-chess-shogi-atari-and-go-without-knowing-the-d755dc80ff08
","['chess', 'google']",
Could AI kill the joy of competitive sports and games?,"
Lee Sedol, former world champion, and legendary Go player today announced his retirement with the quote ""Even if I become the No. 1, there is an entity that cannot be defeated"".
Is it possible that AIs could kill the joy of competitive games(Go, chess, Dota 2, etc.) or (thinking more futuristic with humanoid AIs) in sports?
What happens if AIs gets better than us at painting and making music. Will we still appreciate it in the same way we do now? 
","['philosophy', 'agi', 'go']","Unlikely! Chess has been ""solved"" by AI much longer than GO (chess engines even before AI are way too strong for human players) and still people are playing and competing.Simply put competition and sports live from the human element. Humans competing against each other will still create the same joy for most people regardless of the fact that all involved players might lose against a computer. Some select individuals on the highest level might be put off by the new reality but it won't be the end of competition. No human is faster than a car and yet we still celebrate running competitions.Indeed I think long-term we will gain entertainment by watching different AIs and models compete against each other in chess or Go."
How GoogleNet actually deal with reducing overfitting?,"
Today I was going through a tutorial of Andrew Ng about Inception network. He said that GoogLeNet's hidden layers are also good in prediction and it had somehow a regularization effect, so it reduces overfitting. I also search on this topic and tried to figure out by reading the GoogLeNet paper. But I am not satisfied. 
Can anyone give me any mathematical intuition or reasoning about this in detail?
","['neural-networks', 'convolutional-neural-networks', 'papers', 'overfitting']",
How can I detect moving objects in a video by OpenCV without using deep learning techniques?,"
I want to detect moving objects in a surveillance video without using machine learning tools (like neural networks).
Is there a simple way in the OpenCV library? What is an efficient solution for this purpose?
","['computer-vision', 'object-detection', 'image-processing']",
"Can CNNs be applied to non-image data, given that the convolution and pooling operations are mainly applied to imagery?","
When using CNNs for non-image (times series) data prediction, what are some constraints or things to look out for as compared to image data?
To be more precise, I notice there are different types of layers in a CNN model, as described below, which seem to be particularly designed for image data.
A convolutional layer that extracts features from a source image. Convolution helps with blurring, sharpening, edge detection, noise reduction, or other operations that can help the machine to learn specific characteristics of an image.
A pooling layer that reduces the image dimensionality without losing important features or patterns.
A fully connected layer also known as the dense layer, in which the results of the convolutional layers are fed through one or more neural layers to generate a prediction.
Are these operations also applicable to non-image data (for example, times series)?
","['convolutional-neural-networks', 'time-series', 'convolution', 'pooling']","Usually, you need to ensure that your convolutions are causal, meaning that there is no information leakage from the future into the past. You could start by looking at this paper, which compares Temporal Convolutional Networks (TCN) with vanilla RNNs models."
What is the effect of picking action deterministicly at inference with Policy Gradient Methods?,"
In policy gradient methods such as A3C/PPO, the output from the network is probabilities for each of the actions. At training time, the action to take is sampled from the probability distribution.
When evaluating the policy in an environment, what would be the effect of always picking the action that has the highest probability instead of sampling from the probability distribution?
","['reinforcement-learning', 'policy-gradients']","When evaluating the policy in an environment, what would be the effect of always picking the action that has the highest probability instead of sampling from the probability distribution?Depends what you mean by ""evaluating the policy"". Unlike in value-based methods, such as Q learning, the policy in gradient methods is not implied by anything else, it is described directly by the probability density function that is being optimised. Taking the maximum probability item will technically change the policy (unless you are already using deterministic policy gradient), and you would be evaluating a different but related policy to that found by your policy gradient. However, in a standard MDP environment, and after at least some training, this should be a reasonable process that would give some indication of how well the agent is performing.In some cases, the nature of the environment means the agent is relying on a stochastic policy. In some partially-observable scenarios it may be better to decide randomly - a simple example is a corridor that needs to be traversed, but where the state features don't give enough information to determine the true direction. A deterministic policy will not be able to traverse the corridor in both directions, but a stochastic policy will get through it both ways, eventually. Another example is in adversarial situations where another agent can learn your agent's policy (the classic version of that being Scissor/Paper/Stone where two ideal opposed agents would learn probability $\frac{1}{3}$ for each action according to Nash equilibrium) If you don't think you have these special cases, then it should be OK to derive a deterministic policy from your policy gradient agent, and assess that. That's not quite the same as assessing the ""learned policy"", but is quite sensible to do once you think the agent has converged anyway, since it may still be selecting non-optimal actions at some low probability, and you could get closer to optimal behaviour by removing that."
How are weights updated in a genetic algorithm with neural network?,"
Suppose an AI is to play the game flappy bird. And the fitness function is how long the bird has traveled before the game ends.
Would we have multiple neural networks initialized at the beginning with random weights (as in each bird has its own network) and then we determine the neural networks that have lasted the longest for the game and then we perform a selection of weights from the ""better"" neural networks followed by mutation? Those will then be used as the new weights of a brand new neural network (ie the offspring from two ""better"" neural networks?)? 
If that is the case, does that mean there is no backpropagation because there isn't a cost function?  
","['neural-networks', 'genetic-algorithms', 'neat', 'neuroevolution']",
How can I formulate the k-knights problem as a constraint satisfaction problem?,"
There are three things in every constraint satisfaction problem (CSP):

Variables
Domain
Constraints

In the given scenario, I know how to identify the constraints, but I don't know how to identify the variables and the domain.
The given scenario is:

You are given a $n \times n$ board, where $n \geq 3$. On this board, you have to put $k$ knights where $k < n^2$, such that no knight is attacking the other knight. The knights are expected to be placed on different squares on the board. A knight can move two squares vertically and one square horizontally or two squares horizontally and one square vertically. The knights attack each other if one of them can reach the other in a single move. For example, on a $3 \times 3$ board, we can place $k=5$ knights.

So, the input is $m = 3, n = 3, k = 5$. There are two solutions.
Solution 1
K A K   
A K A    
K A K

Solution 2
A K A
K K K
A K A

","['problem-solving', 'constraint-satisfaction-problems']","There are many possible ways to encode this problem, and some will be more advantageous than othersAn encoding that seems like a reasonable starting point to me is:That should be it. Note the use of $\forall_{i<j}$ in the constraints is to reduce the total number of constraints by half, exploiting the symmetry that knight $i$ can attack knight $j$ iff knight $j$ can attack knight $i$. You could also use $\forall_{i\neq j}$, but it would increase your constraint count to no gain."
Can Grad CAM feature maps be used for Training?,"
I am trying to recreate the architecture of the following paper: https://arxiv.org/pdf/1807.03058.pdf
Can someone help me in explaining how are the feature maps coming out of the output of GradCam used in the following conv layers?
","['convolutional-neural-networks', 'computer-vision', 'gradient-descent', 'architecture', 'pytorch']",
Where should we place layer normalization in a transformer model?,"
In Attention Is All You Need paper:

That is, the output of each sub-layer is $LayerNorm(x+Sublayer(x))$, where $Sublayer(x)$ is the function implemented by the sub-layer itself. We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.

which makes the final formula $LayerNorm(x+Dropout(Sublayer(x)))$. However, in https://github.com/tensorflow/models/blob/0effd158ae1e6403c6048410f79b779bdf344d7d/official/transformer/model/transformer.py#L278-L288, I see
def __call__(self, x, *args, **kwargs):
  # Preprocessing: apply layer normalization
  y = self.layer_norm(x)

  # Get layer output
  y = self.layer(y, *args, **kwargs)

  # Postprocessing: apply dropout and residual connection
  if self.train:
    y = tf.nn.dropout(y, 1 - self.postprocess_dropout)
  return x + y

which ends up as $x+Dropout(Sublayer(LayerNorm(x)))$. Plus there are extra LayerNorms as final layers in both encoder and decoder stacks.
In a quick test, the performance of this model seems to be better than if I change back to the paper's order of operations. My question is: why? And could it be predicted in advance?
I note that Generating Long Sequences with Sparse Transformers uses the $x+Dropout(Sublayer(LayerNorm(x)))$ order, but doesn't discuss it, unlike the other changes it makes to Transformer.
","['neural-networks', 'deep-learning', 'papers', 'transformer', 'layer-normalization']",
Where does entropy enter in Soft Actor-Critic?,"
I am currently trying to understand SAC (Soft Actor-Critic), and I am thinking of it as a basic actor-critic with the entropy included. However, I expected the entropy to appear in the Q-function. From SpinningUp-SAC, it looks like the entropy is entering through the value-function, so I'm thinking it enters by the $\log \pi_{\phi}(a_t \mid s_t)$ in the value function?
I'm a little stuck on understanding SAC, can anyone confirm/explain this to me?
Also, side-note question: is being a soft agent equivalent to including entropy in one of the object functions? 
","['reinforcement-learning', 'actor-critic-methods', 'soft-actor-critic']","In the answer I'll be using notation similar to the one from the SAC paper.
If we look at the standard objective function for policy gradient methods we have
\begin{align}
J_\pi &= V_\pi(s_t)\\
&= \mathbb E_{a_t \sim \pi(a|s_t)}[Q(s_t, a_t)]\\
&= \mathbb E_{a_t \sim \pi(a|s_t)}[ \mathbb E_{s_{t+1} \sim p(s|s_t, a_t)} [r(s_t, a_t) + V(s_{t+1})]]\\
&= \mathbb E_{a_t \sim \pi(a|s_t)}[ \mathbb E_{s_{t+1} \sim p(s|s_t, a_t)} [r(s_t, a_t) +  \mathbb E_{a_{t+1} \sim \pi(a|s_{t+1})}[ \mathbb E_{s_{t+2} \sim p(s|s_{t+1}, a_{t+1})} [r(s_{t+1}, a_{t+1}) + V(s_{t+2})]]]]\\
&\cdots\\
&= \sum_t \mathbb E_{(a_t, s_t) \sim \rho_\pi} [r( s_t, a_t)]
\end{align}
If you keep unwinding this $V(s_{t+i})$ you will get expected sum of rewards.
We can define soft state value as
\begin{align}
V(s_t) &= \mathbb E_{a_t \sim \pi(a|s_t)}[Q(s_t, a_t) + \mathcal H(\cdot|s_t)]\\
&= \mathbb E_{a_t \sim \pi(a|s_t)}[Q(s_t, a_t) + \mathbb E_{a \sim \pi(a|s_t)}[-\log(\pi(a|s_t))]]\\
&= \mathbb E_{a_t \sim \pi(a|s_t)}[Q(s_t, a_t) - \log(\pi(a_t|s_t))]
\end{align}
third equality comes from the fact that $\mathbb E_{a \sim \pi(a|s_t)}[-\log(\pi(a|s_t))]$ is nonrandom so its the same thing as if we are sampling over $\pi$ only once.In maximum entropy framework objective function would then be 
\begin{align}
J_\pi &= V_\pi(s_t)\\
&= \mathbb E_{a_t \sim \pi(a|s_t)}[Q(s_t, a_t) - \log(\pi(a_t|s_t))]\\
&= \mathbb E_{a_t \sim \pi(a|s_t)}[ \mathbb E_{s_{t+1} \sim p(s|s_t, a_t)} [r(s_t, a_t) - \log(\pi(a_t|s_t)) + V(s_{t+1})]]\\
& \cdots\\
&= \sum_t \mathbb E_{(a_t, s_t) \sim \rho_\pi} [r(s_t, a_t) -\log(\pi(a_t|s_t))]\\
&= \sum_t \mathbb E_{(a_t, s_t) \sim \rho_\pi} [r(s_t, a_t) + \mathcal H(\cdot|s_t)]
\end{align}"
How many training data is required for GAN?,"
I'm beginning to study and implement GAN to generate more datasets. I'll just try to experiment with state-of-the-art GAN models as described here https://paperswithcode.com/sota/image-generation-on-cifar-10.
The problem is I don't have a big dataset (around 1.000) for image classification, I have tried to train and test my dataset with GoogleNet and InceptionV3 and the results are mediocre. I'm afraid that GAN will require a bigger dataset than the usual image classification. I couldn't find any detailed guideline of how to prepare datasets properly for GAN (e.g. minimum images).
So, how many images are required to produce a good GAN model?
Also, I'm curious whether if I can use my image classification dataset directly to train GAN.
","['datasets', 'generative-adversarial-networks', 'image-generation', 'training-datasets', 'sample-complexity']",
How to extract face details from a image,"
I am trying to make a face login application that authenticates the user when matching the registered face and the given face.
currently, the issue is I cant extract the face descriptions from the given face when the user is taking the photo in the night or the photo has a backlight.
currently, I am using JavaScript API for face detection and face recognition in the browser and nodejs with tensorflow.js
can anyone suggest any good face detection and comparison algorithm that resolve my current issues, that will be very helpful for me
Now I am extracting face descriptions from the face and using the Euclidean distance equation is used for comparing the similarity of the images. if any good methods for comparison, please suggest
","['image-recognition', 'comparison', 'facial-recognition', 'image-segmentation']",
What is a probability distribution in machine learning?,"
If we were learning or working in the machine learning field, then we frequently come across the term ""probability distribution"". I know what probability, conditional probability, and probability distribution/density in math mean, but what is its meaning in machine learning?
Take this example where $x$ is an element of $D$, which is a dataset,
$$x \in D$$
Let's say our dataset ($D$) is MNIST with about 70,000 images, so then $x$ becomes any image of those 70,000 images.
In many papers and web articles, these terms are often denoted as probability distributions
$$p(x)$$
or
$$p\left(z \mid x \right)$$

What does $p(\cdot)$ even mean, and what kind of output does $p(\cdot)$ give?
Is the output of $p(\cdot)$ a scalar, vector, or matrix?
If the output is vector or matrix, then will the sum of all elements of this vector/matrix always be $1$?

This is my understanding,
$p(\cdot)$ is a function which maps the real distribution of the whole dataset $D$. Then  $p(x)$ gives a scalar probability value given $x$, which is calculated from real distribution $p(\cdot)$. Similar to $p(H)=0.5$ in a coin toss experiment $D={\{H,T}\}$.
$p\left(z \mid x \right)$ is another function that maps the real distribution of the whole dataset to a vector $z$ given an input $x$ and the $z$ vector is a probability distribution that sums to $1$.
Are my assumptions correct?
An example would be a VAE's data generation process, which is represented in this equation
$$p_\theta(\mathbf{x}^{(i)}) = \int p_\theta(\mathbf{x}^{(i)}\vert\mathbf{z}) p_\theta(\mathbf{z}) d\mathbf{z}$$
","['machine-learning', 'terminology', 'definitions', 'probability-distribution', 'notation']","You do not necessarily need to understand the concept of a random variable (r.v.) to understand the concept of a probability distribution, but the concept of a random variable is strictly connected to the concept of a probability distribution (given that each random variable has an associated probability distribution), so, before proceeding, you should get familiar with the concept of an r.v., which is a (measurable) function from the sample space (the set of possible outcomes of an experiment) to a measurable space (you can ignore the definition of a measurable space and assume that the codomain of the random variable is a finite set of numbers).The expression ""probability distribution"" can be ambiguous because it can be used to refer to different (even though related) mathematical concepts, such as probability measure, cumulative distribution function (c.d.f.), probability density function (p.d.f.), probability mass function (p.m.f.). If a person uses the expression ""probability distribution"", he (or she) intentionally (or not) refers to one or more of these mathematical concepts, depending on the context. However, a probability distribution is almost always a synonym for probability measure or c.d.f..For example, if I say ""Consider the Gaussian probability distribution"", in that case, I could be referring to either the c.d.f. or the p.d.f. (or both) of the Gaussian distribution. Why couldn't I be referring to the p.m.f. of the Gaussian distribution? Because the Gaussian distribution is a continuous distribution, so it is a distribution associated with a continuous random variable, that is, a random variable that can take on continuous values (e.g. real numbers), so a Gaussian distribution does not have an associated p.m.f. or, in other words, no p.m.f. is defined for the Gaussian distribution. Why don't I simply say ""Consider the p.d.f. of the Gaussian distribution."" or ""Consider a Gaussian p.d.f.""? Because it is unnecessarily restrictive, given that, if I say ""Consider the Gaussian distribution"" I am implicitly also considering a p.d.f. and c.d.f. of the Gaussian distribution.Similarly, in the case of a discrete distribution, such as the Bernoulli distribution, only the c.d.f. and p.m.f. are defined, so the Bernoulli distribution does not have an associated p.d.f.However, it is important to recall that both continuous and discrete distributions have an associated c.d.f., so the expression ""probability distribution"" almost always (implicitly) refers to a c.d.f., which is defined based on a probability measure (as stated above).In the same vein, the notation $p(x)$ can be as ambiguous as the expression ""probability distribution"", given that it can refer to different (but again related) concepts. However, $p(x)$ usually refers to a probability measure (so it refers to a probability distribution, given that a probability distribution is almost always a synonym for probability measure). In this case, assuming for simplicity that the r.v. is discrete, $p(x)$ is a shorthand for $p(X=x)$, which is also written as $\mathbb{P}(X=x)$ or $\operatorname{Pr}(X=x)$, where $X$ is a r.v., $x$ a realization of $X$ (that is, a value that the r.v. $X$ can take) and $X=x$ represents an event. Given that an r.v. is a function, the notation $X=x$ may look a bit weird.In the case of a discrete r.v., $p(x)$ can also refer to a p.m.f. and it can be defined as $p_X(x) = \mathbb{P}(X=x)$ (I added the subscript $X$ to $p$ to emphasize that this is the p.m.f. of the discrete r.v. $X$). In the case of a continuous r.v., the p.d.f. is often denoted as $f$. In the case of both discrete and continuous r.v.s, the c.d.f is usually denoted with $F$ and it is defined as $F_X(x) = \mathbb{P}(X \leq x)$, where $\mathbb{P}$ is again a probability measure (or probability distribution). The p.d.f. of a continuous r.v. is then defined as the derivative of $F$. At this point, it should be clear why a probability distribution can refer to different but related concepts, but, in any case, it always refers to a probability measure.There are also empirical distributions, which are distributions of the data that you have collected. For example, if you toss a coin 10 times, you will collect the results (""heads"" or ""tails""). You can count the number of times the coin landed on heads and tails, then you plot these numbers as a histogram, which essentially represents your empirical distribution, where the adjective ""empirical"" usually refers to the fact that there is an experiment involved.To complicate things even more, there are also multivariate random variables and probability distributions. However, all the concepts above more or less are also applicable in this case.A parametrized probability distribution, often denoted by $p_{\theta}$, is
a family of probability distributions (defined by the parameters $\theta$), rather than a single probability distribution. For example, $\mathcal{N}(0, 1)$ refers to a single Gaussian distribution with zero mean and unit variance. However, $\mathcal{N}(\mu, \sigma)$, where $\theta=(\mu,  \sigma)$ is a variable, is a family (or collection) of distributions.To conclude, it is completely understandable that you are confused, given that the terminology and notation are used inconsistently, and there are several involved concepts, which I have not extensively covered in this answer (for example, I have not mentioned the concept of a probability space). If you get familiar with the concepts of probability measures, random variables, p.m.f., p.d.f., c.d.f., etc., and how they are related, then you will start to get a better feeling of the whole picture."
"From an implementation point of view, what are the main differences between an RNN and a CNN?","
I understand that in general an RNN is good for time series data and a CNN image data, and have noticed many blogs explaining the fundamental differences in the models.
As a beginner in machine learning and coding, I would like to know from the code perspective, what the differences between an RNN and CNN are in a more practical way.
For instance, I think most CNNs dealing with image data use Conv1D  or Conv2D and MaxPooling2D layers and require reshaping input data with code looks something like this Input(shape=(64, 64, 1)).
What are some other things that distinguish CNNs from RNNs from a coding perspective?
","['deep-learning', 'convolutional-neural-networks', 'comparison', 'recurrent-neural-networks', 'implementation']","In Convolutional Neural Networks (CNNs) you have small kernels (or filters) that you slide over an input (e.g. image). The value resulting from the convolution of the filter with a subset of the image over which the filter is currently positioned is then put into its respective cell in the output of that layer. Essentially, training CNNs boils down to training small filters, for example for detecting edges or corners etc. in input data, which most frequently happens to be images indeed. The assumption here is that features can be detected locally in the input volume, which entails that the nature of the input data shall be coherent over the entire volume of input data.Recurrent Neural Networks (RNNs) do not work locally, but are applied to sequences of arbitrary input data, where one input node may receive sensor readings, while the next node receives the date on which the sensor reading was measured. Of such arbitrary data, you feed a sequence through the RNN, which always keeps its own internal state from processing the previous instance/sample in the sequence in memory while processing the next data point/sample in the sequence. Depending on the kind of recurrent cell type that is employed to construct a RNN layer, the memory of the previous internal state then affects the computation of the next internal state and/or output computed when working on the next data sample. So, information of past data points/samples is carried forward while iterating though a sequence.In short, CNNs are meant to detect local features in volume data, while RNNs preserve information over their previous internal state while processing the next data sample.Probably one of the best online resources walking you through all the related concepts step by step is the following lecture series offered by the Stanford University."
Deep Learning on time series tabular data,"
In this new book release, at the top of page 51 the authors mention that to do deep learning on time series tabular data the developer should structure the tensors such that the channels represent the time periods.
For example, with a dataset of 17 features where each row represents an hour of a day: the tensor would have 3 dimensions,

x - the 17 features
y - the # of days
z - the 24 hours in each day

So each entry in the tensor would represent that day/hour.
Is this necessary to capture time series elements? Would the DNN not learn these representations simply by breaking up the date column into: day, hour?
","['deep-learning', 'time-series']",
Can non-sequential deep learning models outperform sequential models in time series forecasting?,"
Can a CNN (or other non-sequential deep learning models) outperform LSTM (or other sequential models) in time series data? 
I know this question is not very specific, but I experienced this when predicting daily store sales and I am curious as to why it can happen.
","['machine-learning', 'convolutional-neural-networks', 'comparison', 'long-short-term-memory', 'time-series']",
How would one develop an action space for a game that is proprietary?,"
I'm currently trying to develop an RL that will teach itself to play the popular fighting game ""Tekken 7"". I initially had the idea of teaching it to play generally- against actual opponents with various levels of difficulty- but the idea proved to be rather complex. I've liquefied the goal down to ""get a non-active standing opponent to 0 health as fast as possible"". 
I have some experience with premade OpenAI environments, and tried making my own environment for this specific purpouse, but this proved to be rather difficult as there was no user friendly documentation.
Below is a DQN that was coded along with the help of a YouTube tutorial
import numpy as np
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam


class ReplayBuffer(object):
    def __init__(self, max_size, input_shape, n_actions, discrete=False):
        self.mem_size = max_size#memory size dictated
        self.discrete = discrete#determines a number of discrete values that can be inputted
        self.state_memory = np.zeros((self.memsize, input_shape))
        self.new_state_memory = np.zeros((self.mem_size, input_shape))
        dtype = np.int8 if self.discrete else np.float32
        self.action_memory = np.zeros((self.mem_size, n_actions))
        self.reward_memory = np.zeros(self.mem_size)
        self.terminal_memory = np.zeros(self.mem_size, dtype = np.float32)


    def store_transition(self, state, action, reward, state, done):
        index = self.mem_cntr % self.mem_size
        self.state_memory[index] = state
        self.new_state_memory[index] = state_
        self.reward_memory[index] = reward
        self.terminal_memory[index] = 1 - int(done)
        if self.discrete:
            actions = np.zeros(self.action_memory.shape[1])
            self.action_memory[index] = actions
        else:
            self.action_memory[index] = action
        self.mem_cntr += 1


    def sample_buffer(self, batch_size):
        max_mem = min(self.mem_cntr, self.mem_size)
        batch = np.random.choice(max_mem, batch_size)

        states = self,state_memory[batch]
        states_ = self.new_state_memory[batch]
        rewards = self.reward_memory[batch]
        actions = self.action_memory[batch]
        terminal = self.terminal_memory[batch]

        return states, actions, rewards, states_, terminal


    def build_dqn(lr, n_actions, input_dims, fcl_dims, fc2_dims):
        model = Sequential([
                    Dense (fcl_dims, input_shape = (input_dims, )),
                    Activation('relu')
                    Dense(fc2_dims),
                    Activation('relu')
                    Dense(n_actions)])

        model.comile(optimizer = Adam(lr = lr), loss = 'mse')

        return model

    class Agent(object):
        def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,
                     input_dims, epsilon_dec=0.996, epsilon_end=0.01,
                     mem_size = 1000000, fname = 'dqn_model.h5'):
            self.action_space = [i for i in range(n_actions)]
            self.n_actions = n_actions
            self.gamma = gamma
            self.epsilon = epsilon
            self.epsilon_dec = epsilon_dec
            self.epsilon_min = eps_end
            self.batch_size = batch_size
            self.model_file = fname

            self.memory = ReplayBuffer(mem_size, input_dims, n_actions,
                                       discrete = True)
            self.q_eval = build_dqn(alpha, n_actions, input_dims, 256, 256)

        def remember(self, state, action, reward, new_state, done):
            self.memory.store_transition(state, action, reward, new_state, done)

        def choose_action(self, state):
            state = state[np.newaxis, :]
            rand = np.random.radnom()
            if rand < self.epsilon:
                action = np.random.choice(self.action_space)
            else:
                actions = slef.q.eval.predict(state)
                action = np.argmax(actions)

            return action

        def learn(self):#temporal difference learning, delta between steps \
            #and learns from this
            #
            #using numpy.zero approach, only drawback \
            #is that batch size of memory must be full before learning
            if self.memory.mem_cntr < self.batch_size:
                return
            state, action, reward, new_state, done = \
                                    self.memory.sample_buffer(self.batch_size)


            action_values = np.arary(self.action_space, dtype = np.int8)
            action_indices = np.dot(action, action_values)

            q_eval = self.q_eval.predict(state)
            q_next = self.q_eval.predict(new_state)

            q_target = q.eval.copy()

            batch_index = np.arrange(self.batch_size, dtype = np.int32)

            q_target[batch_index, action_indices] = reward + \
                                    self.gamma*np.max(q_next, axis=1)*done

            _ = self.q_eval.fit(state, q_target, verbose=0)

            self.epsilon = self.epsilon*epsilon_dec if self.epsilon > \
                           self.epsilon_min else self.epsilon_min

            def save_model(self):
                self.q_eval.save(self.model.file)

            def load_model(self):
                self.q_eval = load.model(self.model_file)

","['deep-learning', 'reinforcement-learning', 'python', 'q-learning']",
How important is learning to learn for the development of AGI?,"
Some people say that abstract thinking, intuition, common sense, and understanding cause and effect are important to make AGI.
How important is learning to learn for the development of AGI?
","['philosophy', 'agi', 'meta-learning']",
How can I feed any word into a neural network?,"
I am working on an Intent detection problem for a chatbot in Java.
So I need to convert words from String to a double[] format.
I tried using wordToVec(deeplearning4j), but it does not return a vector for words not present in the training data.
e.g. My dataset for wordToVec.train() does not contain the word ""morning"". So wordToVec.getWordVector(""morning"") returns a null value.
There is no need to find the coorelation between two words(like in word2vec), but it should be able to give me some sort of vector representation for any word.
Here are some things I thought of-

I could use a fixed length hash function and convert resulatant hash into vector.(Will Hash Collision be strong enough to be an issue in this case?)
I could initialize for each word a vector of huge length as zero, and set its elements as the ASCII value-64.
e.g. Keeping Maximum vector length as 10, AND would be represented as-
[1,14,4,0,0,0,0,0,0,0], and normalize this.
Is there a better solution to this problem?

Here is the code I used to train the model-
public static void trainModel() throws IOException
    {
        //These lines simply generate the dataset into a format readable by wordToVec
        utilities.GenRawSentences.genRaw();

        dataLocalPath = ""./TrainingData/"";
        String filePath = new File(dataLocalPath, ""raw_sentences.txt"").getAbsolutePath();
        //Data Generation ends   

        SentenceIterator iter = new BasicLineIterator(filePath);
        TokenizerFactory t = new DefaultTokenizerFactory();
        t.setTokenPreProcessor(new CommonPreprocessor());

        VocabCache<VocabWord> cache = new AbstractCache<>();
        WeightLookupTable<VocabWord> table = new InMemoryLookupTable.Builder<VocabWord>()
                .vectorLength(64)
                .useAdaGrad(false)
                .cache(cache).build();

        Word2Vec vec = new Word2Vec.Builder()
                .minWordFrequency(1)
                .iterations(5)
                .epochs(1)
                .layerSize(64)
                .seed(42)
                .windowSize(5)
                .iterate(iter)
                .tokenizerFactory(t)
                .lookupTable(table)
                .vocabCache(cache)
                .build();

        vec.fit();

        //Saves the model for use in other programs
        WordVectorSerializer.writeWord2VecModel(vec, ""./Models/WordToVecModel.txt"");


","['neural-networks', 'natural-language-processing', 'word-embedding']",
Can supervised learning be used to solve the inverted pendulum problem?,"
I know that reinforcement learning has been used to solve the inverted pendulum problem.
Can supervised learning be used to solve the inverted pendulum problem? 
For example, there could be an interface (e.g. a joystick) with the cart-pole system, which the human can use to balance the pole and, at the same time, collect a dataset for supervised learning. Has this been done before?
","['machine-learning', 'reinforcement-learning', 'robotics', 'supervised-learning']",
"How can I train a neural network for another input set, without losing the learning of the previous input set?","
I read this tutorial about backpropagation.
So using this backpropagation we are training the neural network repeatedly for one input set, say [2,4], until we reach 100% accuracy of getting 1 as output. And the neural network is adjusting its weight values accordingly. So once after the neural network is trained this way, suppose we are giving another input set, say [6,8], also then will the neural network update its weight values (overwriting previous values), right? This will result in losing the previous learning, right?
","['neural-networks', 'backpropagation', 'artificial-neuron', 'sigmoid']",
What is the difference between genetic algorithms and evolutionary game theory algorithms?,"
What is the difference between genetic algorithms and evolutionary game theory algorithms?
","['comparison', 'genetic-algorithms', 'evolutionary-algorithms', 'game-theory']",
Is there an open-source implementation for graph convolution networks for weighted graphs?,"
Currently, I'm using a Python library, StellarGraph, to implement GCN. And I now have a situation where I have graphs with weighted edges. Unfortunately, StellarGraph doesn't support those graphs
I'm looking for an open-source implementation for graph convolution networks for weighted graphs. I've searched a lot, but mostly they assumed unweighted graphs. Is there an open-source implementation for GCNs for weighted graphs?
","['python', 'resource-request', 'geometric-deep-learning', 'graphs', 'graph-neural-networks']","A Comprehensive Survey on Graph Neural Networks (2019) presents a list of ConvGNN's. All of the following accept weighted graphs, and three accept those with edge weights as well:And below is a series of open source implementations of many of the above:"
How do you interpret this learning curve?,"
Loss is MSE; orange is validation loss, blue training loss. The task is NN regression (18 inputs, 2 outputs), one layer 300 hidden units.

Tuning the lr, mom, l2 regularization parameters this is the best validation loss I can obtain. Can be considered overfitting? Is 1 a bad vl loss value for a regression task?
","['neural-networks', 'machine-learning', 'objective-functions', 'learning-algorithms', 'cross-validation']",
Why is the status of artificial life software so under-developed?,"
I'm interested in self replicating artificial life (with many agents), so after reviewing the literature with the excellent Kinematic Self-Replicating Machines I started looking for software implementations. I understand that the field is still in the early stages and mainly academic, but the status of artificial life software looks rather poor in 2019. 
On wikipedia there is this list of software simulators.
Going trough the list only ApeSDK, Avida, DigiHive, DOSE, Polyword have been updated in 2019. I did not find a public repo for Biogenesis. ApeSDK, DigiHive and DOSE are single author programs. 
All in all I don't see a single very active project with a large community around (I would be happy to have missed something). And this is more surprising considering the big momentum of AI and the proliferation of many ready to use AI tools and libraries. 
Why is the status artificial life software so under-developed, when this field looks promising both from a commercial (see manufacturing, mining or space exploration applications) and academic (ecology, biology, human brain and more) perspective? Did the field underdelivered on expectations in past years and got less funding? Did the field hit a theoretical or computational roadblock? 
","['software-evaluation', 'artificial-life', 'self-replicating-machines']","I donâ€™t know for certain, but I can make a guess. This is just my opinion, some others may disagree.The field of ALife has four branches that Iâ€™m aware of:Self-Organizing/self assembly behavior. This is the application you refer to, another context itâ€™s useful is swarm control (for drone swarms, for example). While this is technically ALife, as far as Iâ€™m aware itâ€™s not really where most of the emphasis is. Swarm control and self assembly are seen as â€œdifferentâ€ problems, as machines that can work together and also build more of themselves is interesting (and potentially dangerous), but is missing out on the diversity, the open-endedness that life on earth has. Much of ALife research is focused on trying to formally define this open-endedness and coming up with systems that achieve that. Self assembly and swarm control are interesting and difficult problems, just different. This leads to the other three sides of ALife research:Coming up with environments, and running tests on them. This is a constant game of coming up with a definition that seems to capture open-endedness, then coming up with ALife sims that meet that criteria but fall short of our expectations. So new definitions are made and we repeat. Geb is a classic example: Geb has passed pretty much every test so far, but itâ€™s fairly uninspiring to watch. Most of those programs you reference chose a particular ALife paradigm, but that paradigm may not be the right one, and is often disappointing. Because we still havenâ€™t found something that really â€œlooks like lifeâ€, new paradigms and programs are constantly being created and abandoned when they fail to work (Or perhaps some would have already worked, but the computing time is too much). Thatâ€™s what youâ€™re seeing. Without any unifying theory or sim that is really convincing, I suspect itâ€™ll stay this way for a while. And because:the direction of making new simulators seems to be lacking funding and research interest, as far as I can tell. Commercial sim games seem to push the boundary these days.Fortunately thereâ€™s a sub field of cellular automata life thatâ€™s pretty interesting, its software is slightly more developed due to the overlap with cellular automata and ease of implementation, and research seems to be progressing there at an okay rate.Realistically, there seem to be two things people want: novel behavior, and novel bodies. My two cents is that these are separate problems, and achieving both is more expensive than just achieving one. But most of these sims end up not balancing development happening in both of these factors (doing this is very difficult), so one factor develops much further than the other, and this disconnect is disappointing to the sim creator. For example, Geb does behavioural diversity really well, while Karl Sims does body diversity well. Sensitivity to small details like mutation rate or genetic encoding also can be quite frustrating. Fortunately, eventually weâ€™ll sorta get behavioural diversity for free in any sim once RL/AI is really understood well.The third piece of ALife research Iâ€™m aware of is the theoretical side, which right now mostly isnâ€™t really far enough along to warrant practical implementation. One big branch of this is the learning theory side, represented by Valiantâ€™s Evolvability theory and followups. Essentially this talks about what functions are possible to evolve, and using stuff like PAC Learning theory they are able to prove some things. Some of these models are more natural than others, but itâ€™s an interesting perpendicular approach to coming up with sims and seeing if they do what we want. Maybe eventually these two approaches will meet in the middle at some point, but they havenâ€™t yet.The fourth piece is Artificial Chemistry. I recommend this paper as a somewhat dated overview. While this is technically a field of ALife, and is centered around understanding a chemical system that has the necessary emergent properties, it has broken off into applications that may have industrial relevance. For example, robust self repairing and self assembling electronic systems, DNA computing (DNA is capable of simulating arbitrary chemical reaction networks which are capable of arbitrary computing), and artificial hormone systems for automatic task assignment. This has some software developed, but much of that software isnâ€™t really considered ALife anymore since it has branched off into its own domain."
How do I determine the optimal policy in a bandit problem with missing contexts?,"
Suppose I learn an optimal policy $\pi(a|c)$ for a contextual multi-armed bandit problem, where the context $c$ is a composite of multiple context variables $c = c_1, c_2, c_3$. For example, the context is specified by three Bernoulli variables. 
Is there any literature on how to determine the optimal policy in the event where I no longer have access to one of the context variables? 
","['reinforcement-learning', 'markov-decision-process', 'multi-armed-bandits', 'contextual-bandits']",
Multicamera Tracking vs Single Fisheye Camera,"
Suppose you want to detect objects and also track objects and people. Is it better to train a model using a single fisheye camera or using multiple cameras that mimic the view of the fisheye camera? 
Also, what can be done to remove objects that are washed out? Like for very small objects, how do you make them more visible? Would multicamera tracking be better in this scenario?
","['convolutional-neural-networks', 'object-detection']",
Can I perform multiclass classification when the number of features is less than the number of targets?,"
Is it possible to perform multiclass classification on data where the number of features is less than the number of target variables? Do you have any suggestions on how to address a problem where I have 2000 target variables?
","['machine-learning', 'classification']","Of course. It only depends if those features are informative enough for the task at hand. In order to better understand the phenomenon, you can imagine 2 features displayed as points in a 2D plane. The number of possible target classes goes up to the number of clusters you can find in that plane.About the suggestion, I can only recommend the utilisation of a non-linear classifier."
What is a location-based addressing in a neural Turing machine?,"
In the neural Turing machine (NTM), the content-based addressing and location-based addressing is used for memory addressing. Content-based addressing is similar to the attention-based model, weighting each row of memory which shows the importance of each row of memory (or each location of memory). Then for location-based addressing, by using shift kernel, the attention focus is moved left or right or remains unchanged. 
What is location-based addressing? Why was location-based addressing used? What is the concept of ""for location-based addressing, by using shift kernel, the attention focus is moved left or right or remains unchanged.""? 
What is the difference between content-based addressing and location-based addressing?
","['neural-networks', 'deep-learning', 'recurrent-neural-networks', 'neural-turing-machine']",
What are examples of models for traffic sign detection that can be easily implemented?,"
I'm working on a college project about traffic sign detection and I have to choose a paper to implement it, but I have basic knowledge of TensorFlow and I'm afraid of choosing a paper that I can't implement it.
What are examples of models for traffic sign detection that can be easily implemented?
","['neural-networks', 'machine-learning', 'deep-learning', 'tensorflow', 'reference-request']",
"A generalized quadratic loss and Newton iteration for Support Vector Regression, why doesn't it generalize well?","
I'm comparing the results of an Newton optimizer for a modified version of SVM ( a generalized quadratic loss, similar to the one stated in:
A generalized quadratic loss for SVM
) with classic SVM^light for regression. The problem is that it's able to overfit the data (UCI Yacht data-set) but I can't reach the generalization results of SVM^light. I've tried several hyper-parameters grids. I'm solving the primal problem. I'll send you my code if you need it. Any suggestion?
","['objective-functions', 'support-vector-machine']",
Why do regression LSTMs learn high to low inputs significantly better than low to high?,"
The specific problem I have is learning the relation $x^2$. I have an array of 0 through 19 (input values) and a target array of 0, 1, 4, 9, 16, 25, 36 and so on all the way up to $19^2$=361.
I have the following LSTM architecture:
1 input node
1 output node
32 hidden units

Now, interestingly, I accidentally trained my network wrong, in that I forgot to reverse the expected output list when training. So I trained the network for:
$$0 \rightarrow 361 \\ 1 \rightarrow 324 \\ 2 \rightarrow 289 \\ 3 \rightarrow 256 \\ ... \\ 17 \rightarrow 4 \\ 18 \rightarrow 1 \\ 19 \rightarrow 0$$
Starting with a learning rate of 1 and halving it every 400 epochs, after 3000 epochs my error (which started somwhere in the millions) was 0.2.
However, when I went to correct this mistake, my error will hardly ever go beneath 100,000. Testing the network shows it does well in the low inputs, but once it starts to get to $16^2$ onwards, it really struggles to increase the output values past ~250.
I was just wondering if anyone has an explanation for this, as to why the LSTM struggles to learn to increase exponentially but seems to be able to learn to decrease exponentially just fine.
EDIT WITH CODE:
a = np.array([i for i in range(20)])
b = np.array([i**2 for i in range(20)])
np.random.seed(5)
ls = LSTM(1, 1, 32, learning_rate=1, regression=1)
# Input size = 1, output size = 1, hidden units = 32
if 1:
    for k in range(3000):
        total = 0
        for i in range(20):
            ls * a[i]
        for i in range(20):
            total += ls / b[i]
        if k % 400 == 0:
            ls.learning_rate *= 0.5
        print(k, "":"", total)
        ls.update_params()
for i in a:
    print(i, ls*i)
#ls.save_state('1.trainstate')
for i in range(20,30):
    print(ls*i)

Note this code uses a class I wrote using numpy. If wanted I'll include this code as well it's just that is ~300 lines and I don't expect anyone to go through all that
","['neural-networks', 'long-short-term-memory']",
How to change the architecture of my simple sequential model,"
I'm new to Deep Learning with Keras. With some tutorials online for cat vs non-cat classification, I was able to compile this simple architecture for my own classification problem. However, my target application is fire detection which essentially might have semantic differences with cats.
After training, I realized this model is accurate when the fire scene is simple and visible, but if many objects inside or fire is a bit smaller, it fails to detect. So I thought maybe I can change the architecture by increasing the complexity.
First thing came into my mind was increasing the first layer filters from 32 to 64 by changing to model.add(Conv2D(64, kernel_size = (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)))
Is it going to help? What are other best practices to change the architecture? How about increasing the number of kernels to kernel_size = (5, 5) or adding one more layer or even changing the images from grayscale to colored?
Here is my original training code:
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization
from PIL import Image
from random import shuffle, choice
import numpy as np
import os

IMAGE_SIZE = 256
IMAGE_DIRECTORY = './data/test_set'

def label_img(name):
  if name == 'cats': return np.array([1, 0])
  elif name == 'notcats' : return np.array([0, 1])

def load_data():
  print(""Loading images..."")
  train_data = []
  directories = next(os.walk(IMAGE_DIRECTORY))[1]

  for dirname in directories:
    print(""Loading {0}"".format(dirname))
    file_names = next(os.walk(os.path.join(IMAGE_DIRECTORY, dirname)))[2]

    for i in range(200):
      image_name = choice(file_names)
      image_path = os.path.join(IMAGE_DIRECTORY, dirname, image_name)
      label = label_img(dirname)
      if ""DS_Store"" not in image_path:
        img = Image.open(image_path)
        img = img.convert('L')
        img = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)
        train_data.append([np.array(img), label])

  return train_data

def create_model():
  model = Sequential()
  model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', 
                   input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Dropout(0.2))
  model.add(Flatten())
  model.add(Dense(256, activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(2, activation = 'softmax'))

  return model

training_data = load_data()
training_images = np.array([i[0] for i in training_data]).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)
training_labels = np.array([i[1] for i in training_data])

print('creating model')
model = create_model()
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print('training model')
model.fit(training_images, training_labels, batch_size=50, epochs=10, verbose=1)
model.save(""model.h5"")

","['machine-learning', 'deep-learning', 'classification', 'training', 'keras']",
Unable to train Coach for Banana-v0 Gym environment,"
I have just started playing with Reinforcement learning and starting from the basics I'm trying to figure out how to solve Banana Gym with coach.
Essentially Banana-v0 env represents a Banana shop that buys a banana for \$1 on day 1 and has 3 days to sell it for anywhere between \$0 and \$2, where lower price means a higher chance to sell. Reward is the sell price less the buy price. If it doesn't sell on day 3 the banana is discarded and reward is -1 (banana buy price, no sale proceeds). That's pretty simple. 
Ideally the algorithm should learn to set a high price on day 1 and reducing it every day if it didn't sell.
To start I took the coach-bundled CartPole_ClippedPPO.py and CartPole_DQN.py preset files and modified them to run Banana-v0 gym. 
The trouble is that I don't see any learning progress regardless what I try, even after running like 50,000 episodes. In comparison the CartPole gym successfully trains in under 500 episodes.
I would have expected some improvement after 50k episodes for such a simple task like Banana. 

Is it because the Banana-v0 rewards are not predictable? I.e. whether the banana sells or not is still determined by a random number (with success chance based on the price).
Where should I take it from here? How to identify which Coach agent algorithm I should start with and try to tune it? 
","['reinforcement-learning', 'learning-algorithms', 'gym']",
Oposite type of predictions for unbalanced dataset,"
I have a big dataset (28354359 rows) that has some blood values as features (11 features) and the label or outcome variable that tells whether a patient has a virus caused by a Neoplasm or not.
The problem with my dataset is that 2% of the patients that are in my dataset have the virus and 98% does not have the virus. 
I am mandatory to use the random forest algorithm. While my random forest model has a high accuracy scores 92%, the problem is that more than 90% of the patients that have the virus are predicted that they donâ€™t have the virus.
I want the opposite effect, I want that my random forest is likely to predict more often that a patient has the virus (even if the patient does not have the virus (ideally I donâ€™t want this side effect , but rather this than the opposite)).
The idea behind this is that performing an extra test (via an echo) could not harm the patient that has not the virus, but not testing a patient will have result terrible for the patient.
Does somebody have advice how I could tweak my random forest model for this task?
I my self experimented with the SMOTE transformation and other sampling techniques but maybe you guys have other suggestion.
I also have tried to apply a cutoff function.
","['machine-learning', 'random-forests']",
How can we make sure how well the reinforcement learning agent works on a stock dataset?,"
I read a paper, which is about Deep Reinforcement Learning and it tries to use this method on stock data set. It has been shown that it reaches the maximum return (profit). It has been implemented in Tensorflow.
My question is, how we can make sure that we achieve the maximum value? I mean, is there a parameter or value that can show us how well the RL did its job?
","['reinforcement-learning', 'testing', 'algorithmic-trading']",
How Euclidean distance algorithm calculate two different face images are match or not match in face recognition? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            



Closed 3 years ago.











Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I am trying to make a face login application. face comparison algorithm is using Euclidean distance to calculate two different face images that are the same or not the same. can anyone help me with how the Euclidean distance algorithm is working?
","['image-recognition', 'facial-recognition']",
How to set the multiple continuous actions with constraints,"
I want to build a Deep Reinforcement Learning Model for Asset allocation.
Background:
I have 7 stock indexes from different markets, and I want to build a policy to produce the action (likes whether to sell or buy index? which index? and how much?) by observing the market informations.
Question 1:
I have two idea for the output of my policy. One is to produce a vector $w$ of length 8, Each element $w_i$ represent the target ratio of the stocks we want to hold (7 stock indexes and 1 cash), so I need to set $w_i>0, $ and $\sum_{i}^{8}w_i=1.$ How to implement? I just let the Activation function in the last layer of neural network to be sigmoid and divide the sum in environment. Is this available? And it's not easy to deal with transaction process if buy and sell fee exist. And the training process slowly when I use the policy gradient.
The two is also produce a vector $w$ of length 8, For each element $w_i$ represent sell percent for stock i when $w_i$ is negative and buy percent of cash when $w_i$ is positive. It can solve the problem I meet in idea one. But I will meet a new question is cash is finite. I need to decide order of buy, in other words, which stock to buy first and buy which one later.
Question 2:
Many papers tell me to produce Distributed parameters by policy then create the action by distribution (like: normal distribution). It makes that more difficult to control the action  satisfy the condition above.
And the result whether be unstable if the action is produce by sample.
","['reinforcement-learning', 'policy-gradients']",
What is the difference between Kaldi and DeepSpeech speech recognition systems in their approach?,"
I would like to know how do Kaldi and DeepSpeech speech recognition systems differ algorithmically? Which one would be more accurate for continuous speech in time?
","['machine-learning', 'convolutional-neural-networks', 'long-short-term-memory', 'deep-neural-networks', 'speech-recognition']",
What is the relationship between robustness and adversarial machine learning?,"
I have been reading a lot of articles on adversarial machine learning and there are mentions of ""best practices for robust machine learning"". 
A specific example of this would be when there are references to ""loss of efficient robust estimation in high dimensions of data"" in articles related to adversarial machine learning. Also, IBM has a Github repository named ""IBM's Adversarial Robustness Toolbox"". 
Additionally, there is a field of statistics called 'robust statistics' but there is no clear explanation anywhere about its relation to adversarial machine learning. 
I would therefore be grateful if someone could explain what robustness is in the context of Adversarial Machine Learning.
","['neural-networks', 'machine-learning', 'ai-design', 'ai-safety', 'adversarial-ml']","A robust ML model is one that captures patterns that generalize well in the face of the kinds of small changes that humans expect to see in the real world. A robust model is one that generalizes well from a training set to a test or validation set, but the term also gets used to refer to models that generalize well to, e.g. changes in the lighting of a photograph, the rotation of objects, or the introduction of small amounts of random noise.Adversarial machine learning is the process of finding examples that break an otherwise reasonable looking model. A simple example of this is that if I give you  a dataset of cat and dog photos, in which cats are always wearing bright red bow ties, your model may learn to associate bow ties with cats. If I then give it a picture of a dog with a bow tie, your model may label it as a cat. Adversarial machine learning also often includes the ability to identify specific pieces of noise that can be added to inputs to confound a model. Therefore, if a model is robust, it basically means that it is difficult to find adversarial examples for the model. Usually this is because the model has learned some desirable correlations (e.g. cats have a different muzzle shape than dogs), rather than undesirable ones (cats have bow ties; pictures containing cats are 0.025% more blue than those containing dogs; dog pictures have humans in them more often; etc.).Approaches like GANs try to directly exploit this idea, by training the model on both true data and data designed by an adversary to resemble the true data. In this sense, GANs are an attempt to create a robust discriminator. "
Do we have anything like accuracy and loss in RNN models?,"
I have a paper about trading which has been implemented with RNN on Tensorflow. We have about 2 years of data from trading. Here are some samples :
Date, Open, High, Low, Last, Close, Total Trade Quantity, Turnover (Lacs)
2004-08-25 , 1198.7, 1198.7, 979.0, 985.0, 987.95, 17116372.0, 172587.61
2004-08-26 , 992.0, 997.0, 975.3, 976.85, 979.0, 5055400.0, 49828.65
I need to predict the the future of trading (for example, the latest 10 days ). So, how can I make sure that my model is working correctly. Do we have any ""accuracy"" or ""loss"" like what we have in Deep Learning?
","['tensorflow', 'recurrent-neural-networks']","RNN's stand for Recurrent Neural Networks which is, in fact, Deep Learning.There has to be a loss since you're dealing with supervised learning and the typical loss metrics used are the same as you would see in feedforward networks (usually binary cross-entropy), the main difference being loss would be calculated between the true label at a particular time stamp $(t)$ and the prediction made from the subset of the network until time-stamp $(t-1)$. This leads the loss to act on all timestamps. Accuracy metrics also would be used in the same way such as Mean Square Error or L1. For more details you can go through this link. Hope this was helpful!"
Is this TensorFlow implementation of partial derivative of the cost with respect to the bias correct?,"
I have a neural network for MNIST classification which I am hard coding using TensorFlow 2.0. The neural network has an input layer consisting of 784 neurons (28 * 28), one hidden layer having ""hidden_neurons"" number of neurons and an output layer having 10 neurons.
The part of the code that I want to get checked is as follows:
# Partial derivative of cost function wrt b2-
dJ_db2 = (1 / m) * tf.reshape(tf.math.reduce_sum((A2 - Y), axis = 0), shape = (1, 10))

# Partial derivative of cost function wrt b1-
dJ_db1 = (1 / m) * tf.reshape(tf.math.reduce_sum(tf.transpose(tf.math.multiply(tf.matmul(W2, tf.transpose((A2 - Y))), relu_derivative(A1))), axis = 0), shape = (1, hidden_neurons))

The notation is as follows. 

""b1"" - bias for hidden layer and has the shape (1, hidden_neurons"") 
""b2"" - bias for output layer having the shape (1, 10).
""A2"" - is the output of output layer and have the shape (m, c)
""Y"" - is one-hot encoded target and have the shape (m, c)
'm' - is number of training examples
'c' - is number of classes
""A1"" - is the output of hidden layer and has the shape (hidden_neurons, m)

I have used multiclass cross-entropy cost function. Hidden layer uses ReLU activation function, while the output layer has softmax activation function.
Are my two lines of codes for cost function wrt to ""b1"" and ""b2"" correct?
","['neural-networks', 'machine-learning', 'tensorflow', 'backpropagation', 'feedforward-neural-networks']",
What is the difference between a learning algorithm and a hypothesis?,"
What's the distinction between a learning algorithm $A$ and a hypothesis $f$?
I'm looking for a few concrete examples, if possible.
For example, would the decision tree and random forest be considered two different learning algorithms? Would a shallow neural network (that ends up learning a linear function) and a linear regression model, both of which use gradient descent to learn parameters, be considered different learning algorithms?
Anyway, from what I understand, one way to vary the hypothesis $f$ would be to change the parameter values, maybe even the hyper-parameter values of, say, a decision tree. Are there other ways of varying $f$? And how can we vary $A$?
","['comparison', 'terminology', 'computational-learning-theory', 'learning-algorithms', 'hypothesis']","In computational learning theory, a learning algorithm (or learner) $A$ is an algorithm that chooses a hypothesis (which is a function) $h: \mathcal{X} \rightarrow \mathcal{Y}$, where $\mathcal{X}$ is the input space and $\mathcal{Y}$ is the target space, from the hypothesis space $H$.For example, consider the task of image classification (e.g. MNIST). You can train, with gradient descent, a neural network to classify the images. In this case, gradient descent is the learner $A$, the space of all possible neural networks that gradient descent considers is the hypothesis space $H$ (so each combination of parameters of the neural network represents a specific hypothesis), $\mathcal{X}$ is the space of images that you want to classify, $\mathcal{Y}$ is the space of all possible classes and the final trained neural network is the hypothesis $h$ chosen by the learner $A$.For example, would the decision tree and random forest be considered two different learning algorithms?The decision tree and random forest are not learning algorithms. A specific decision tree or random forest is a hypothesis (i.e. function of the form as defined above).In the context of decision trees, the ID3 algorithm (a decision tree algorithm that can be used to construct the decision tree, i.e. the hypothesis), is an example of a learning algorithm (aka learner).The space of all trees that the learner considers is the hypothesis space/class.Would a shallow neural network (that ends up learning a linear function) and a linear regression model, both of which use gradient descent to learn parameters, be considered different learning algorithms?The same can be said here. A specific neural network or linear regression model (i.e. a line) corresponds to a specific hypothesis. The set of all neural networks (or lines, in the case of linear regression) that you consider corresponds to the hypothesis class.Anyway, from what I understand, one way to vary the hypothesis $f$ would be to change the parameter values, maybe even the hyper-parameter values of, say, a decision tree.If you consider a neural network (or decision tree) model, with $N$ parameters $\mathbf{\theta} = [\theta_i, \dots \theta_N]$, then a specific combination of these parameters corresponds to a specific hypothesis. If you change the values of these parameters, you also automatically change the hypothesis. If you change the hyperparameters (such as the number of neurons in a specific layer), however, you will be changing the hypothesis class, so the set of hypotheses that you consider.Are there other ways of varying $f$?Off the top of my head, only by changing the parameters, you change the hypothesis.And how can we vary $A$?Let's consider gradient descent as the learning algorithm. In this case, to change the learner, you could change, for example, the learning rate."
References on generalization theory and mathematical abstraction of ML concepts,"
I'd like to learn about generalization theory for machine learning algorithms. I'm looking for books and other references (in case books aren't available) that provide a gentle introduction to the field for a relative beginner like me. 
My background includes exposure to mostly undergrad mathematics and I have enough mathematical maturity to learn graduate-level topics as well.
To be more specific, I'm looking to understand more about mathematical abstraction of ML concepts (e.g. learning algorithm, hypothesis space, complexity of algorithm/hypothesis etc.), the purpose of an ML algorithm as an expected risk minimization exercise, techniques used to get bounds on generalization and so on. 
To be even more specific, I'm looking to familiarize myself with concepts, theory and techniques so that I can understand papers (at least on a basic level) like:

Generalization in Deep Learning (2019)
Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness
nbro's answer to this question

and references therein
","['research', 'reference-request', 'generalization']",
How can an AI train itself if no one is telling it if its answer is correct or wrong?,"
I am a programmer but not in the field of AI. A question constantly confuses me is that how can an AI be trained if we human beings are not telling it its calculation is correct?
For example, news usually said something like ""company A has a large human face database so that it can train its facial recognition program more efficiently"". What the piece of news doesn't mention is whether a human engineer needs to tell the AI program each of the program's recognition result is accurate or not.
Are there any engineers who are constantly telling an AI what it produced it correct or wrong? If no, how can an AI determine if the result it produces is correct or wrong?
","['machine-learning', 'terminology']",
Why is the effective branching factor used for measuring performance of a heuristic function?,"
For search algorithms with heuristic functions, the performance of heuristic functions are measured by the effective branching factor ${b^*}$, which involves the total number of nodes expanded ${N}$ and the depth of the solution ${d}$.
I'm not able to find out how different values of ${d}$ affect the performance keeping the same ${N}$. Put another way, why not use just the ${N}$ as the performance measure instead of ${b^*}$?
","['search', 'performance', 'branching-factors', 'heuristic-functions', 'informed-search']","I also walked into that trap the first few times. The difference is the following:So, you could argue that instead of comparing $b_1^*$ and $b_2^*$ of two algorithms, you can also directly compare $M_1$ and $M_2$, because $b_1^*>b_2^*\Leftrightarrow M_1>M_2$.But you can imagine an algorithm $A_2$ that expands fewer nodes than $A_1$ (so $N_1>N_2$), but also different nodes so that it generates more nodes (so $M_1<M_2$).
Since the cost is defined by the number of generated nodes, comparing $N$ might give the wrong result.The effective branching factor is more general than the number of generated nodes, because you can average $b^*$ for one algorithm over many search problems, but averaging over the number of nodes (which might differ greatly) is not possible or rather nonsensical."
"Feature extraction timeseries, model compatibility","
I've got a timeseries with sensor data (e.g. accelerometer and gyroscope). I now want to extract the activity out of it (e.g. walking, standing, driving, ...). I Followed this Jupyter Notebook. But there are some issues left.

Why do they only pick 500 rows?
What's the point of re-arranging the rows/columns?
When they build their decicion tree learner with the train data, they build it upon extracted features. But how can we then use this tree for new sensor data? Should we extract the features of the new data and pass it as input for the tree? But new sensor data might not have as many features as the train data. Eg: (ValueError: Number of features of the model must match the input. Model n_features is 321 and input n_features is 312)

","['classification', 'decision-trees', 'time-series']","there's a lot to un-pack in this question.Why do they only pick 500 rows?my guess: in order to keep the example running quickly. tsfresh usually takes a while to calculate its features.  note that when they evaluated their model, they took the last 500 samples.What's the point of re-arranging the rows/columns?answer: the data frame format that tsfresh requires in order to calculate features is that format. it is a bit of a pain...especially when you need to keep track of an id-column for other data.When they build their decicion tree learner with the train data, they build it upon extracted features. answer: yesBut how can we then use this tree for new sensor data?
Should we extract the features of the new data and pass it as input for the tree?answer: yesBut new sensor data might not have as many features as the train data. Eg: (ValueError: Number of features of the model must match the input. Model n_features is 321 and input n_features is 312)answer: yes it will. I don't know where your copy-pasted error message came from. When you generate a set of features using tsfresh, you can do it in a couple of different ways. you can generate all of them or you can generate a subset of them---they generated a subset...but then subsetted it once again(using their importance stuff...whether you use the importance methods or other methods to select features you will end up with a bunch of features you need to calculate). If you follow the procedure for generating features based on a pre-determined subset (relevant_features in their case), you will end up with the same # of features.  This needs to be stressed...don't generate tsfeatures that you don't need! as it will take foooorever."
A* and uniform-cost search are apparently incomplete,"
Consider the following diagram of a graph representing a search space.

If we start at $B$ and try to reach goal state $E$, the lowest-cost first search (LCFS) (aka uniform-cost search) algorithm fails to find a solution. This is because, $B$ selects $A$ over $C$ to expand as $f(A)=g(A)=36 < f(C)=g(C)=70$. $f(n)$ is the cost function of node $n$, and $g(n)$ is the cost of reaching node $n$ from the start state. Continuing further, from $A$, LCFS will now select $B$ to expand, which in turn will select $A$ again over $C$. This leads to an infinite loop. This shows LCFS is incomplete (not guaranteed to find a solution, if one exists).
For A*, we define $f(n)=g(n)+h(n)$, where $h(n)$ is the expected cost of reaching goal state from node $n$. If we define Manhattan distance ($L_0$ norm) for $h(\cdot)$, books (such as Artificial Intelligence: A Modern Approach (3rd Ed) by Stuart Russell and Peter Norvig) says A* is bound to find the solution (since it exists). However, I couldn't find how. Using, A*, $B$ will still select $A$ since $f(A)=36+(h(A)=40)=76 < f(C)=70+(h(C)=30+50)=150$. You see, this means, when $A$ expands back $B$, $B$ will again select $A$, and an infinite loop ensues.
What am I missing here?
","['search', 'a-star', 'norvig-russell', 'uniform-cost-search']","You forgot to calculate and take into account the costs of the actual paths. You forgot to accumulate the cost of the edges for going forward and backward multiple times!The evaluation function of uniform-cost search (UCS) is $f(n) = g(n)$, where $g(n)$ represents the cost of the path from the start node to $n$. The evaluation function of A* is $f(n) = g(n) + h(n)$, where $h(n)$ is an admissible heuristic. UCS is a special case of A*, with the admissible heuristic $h(n) = 0, \forall n$. The evaluation function is used to choose the next node to visit from the fringe, which is the set of nodes that can potentially be visited. Whenever we visit a node, we remove it from the fringe. To expand a node $X$ means to add the children of $X$ to the fringe. Whenever you visit a node, you will also expand it. Let's apply UCS to your specific example. Initially, we check whether $B$ is a goal node or not. It is not, so we expand $B$. We can add $B$ to a list of visited (or expanded) nodes (graph search) or not (tree search). Let's use the tree search, so we will not be keeping track of the expanded nodes, which means that we could expand a node more than once. $B$ has two children, $A$ and $C$, so we add them to the fringe, $\mathbb{F} = \{A, C\}$. Should we now visit $A$ or $C$? We will visit the one with the smallest value of the evaluation function, which is $A$, given that $f(A)=g(A)=36 < f(C)=g(C)=70$, so we remove $A$ from the fringe, which is now $\mathbb{F} = \{ C \}$. Is $A$ a goal node? No, so we expand it, but it only has one child, $B$, so we add $B$ to the fringe, so $\mathbb{F} = \{ C, B \}$. The cost of the path to reach $B$ by going first to $A$ and then to $B$ is $f(B) = 36 + 36 = 72$ (given that you go back and forward on the same edge, so you need to accumulate the cost of these trips!) and $f(C) = 70$, so we visit $C$, so we remove it from the fringe, which is now $\mathbb{F} = \{ B \}$.You should be able to work out the remaining search (I haven't actually done it). I suggest you watch the video Uniform Cost Search, by John Levine, who shows a concrete example of how UCS and A* work."
Are there any easy ways to create annotated training images for object detection?,"
For the purposes of object detection, are there any easy ways to create annotated training images? For example, if we have $10,000$ images and want to draw bounding boxes on 2 objects for each image, do we have to physically draw those boxes? Is that what most people do these days to create training data?
","['machine-learning', 'convolutional-neural-networks', 'datasets', 'object-detection']",
How does the decision tree implicitly do feature selection?,"
I was talking with an ex-fellow worker and he told me that the decision tree implicitly applies a feature selection. He told me that the most important feature is higher in the tree because of the usage of information gain criteria.
What does he mean with this and how does this work?
","['machine-learning', 'decision-trees', 'feature-selection']","Consider a dataset $S \in \mathbb{R}^{N \times (M + 1)}$ with $N$ observations (or examples), where each observation $S_i \in \mathbb{R}^{M + 1}$ is composed of $M$ elements, one value for each of the $M$ features (or independent variables), $f_1, \dots f_M$, and the corresponding target value $t_i$. A decision tree algorithm (DTA), such as the ID3 algorithm, constructs a tree, such that each internal node of this tree corresponds to one of the $M$ features, each edge corresponds to one value (or range of values) that such a feature can take on and each leaf node corresponds to a target. There are different ways of building this tree, based on different metrics to choose the features for each internal node and based on whether the problem is classification or regression (so based on whether the targets are classes or numeric values).For example, let's assume that the features and the target are binary, so each $f_k$ can take on only one of two possible values, $f_{k} \in \{0, 1\}, \forall k$, and $t_i \in \{0, 1\}$ (where the index $i$ correspond to the $i$th observation, while the index $k$ correspond to the $k$th column or feature of $S$). In this case, a DTA first chooses (based on some metric, for example, the information gain) one of the $M$ features, for example, $f_j$, to associate it with the root node of the tree. Let's call this root node $f_j$. Then $f_j$ will have two branches, one for each of the binary values of $f_j$. If $f_j$ were a ternary variable, then the node corresponding to $f_j$ would have three branches, and so on. The DTA recursively chooses one of the remaining features for each node of the child branches. The DTA does this until all features have already been selected. In that case, we will have reached a leaf node, which will correspond to one value of the target variable. When the DTA chooses a feature for a node, all observations of the dataset $S$ that take on the first binary value of that feature will go in the branch corresponding to that value and all other observations will go in the other branch. So, in this way, the DTA splits the dataset based on the features.The following diagram represents a final decision tree built by a DTA.You can see that the first feature selected (for the root node) by the DTA is ""Is it male?"", which is a binary variable. If yes, then, on the left branch, we have another internal node, which corresponds to another feature and, at the same time, to all observations associated with a male. However, on the right branch, we have a leaf node, which corresponds to one value of the target, which, in this case, is a probability (or, equivalently, a numerical value in the range $[0, 1]$). The shape of the tree depends on the dataset and DTA algorithm. Therefore, different datasets and algorithms might result in different decision trees.So, yes, you can view a decision tree algorithm as a feature selection or, more precisely, feature splitting algorithm. "
Multi label Classification using Keras [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed last year.







                        Improve this question
                    



I am trying to build a Multi label classification model, having dataset with different input numerical values and specific label...
Eg:
Value Label
35   X
35.8 X
29   Y
29.8 Y
39   AA
41   CB
So depending on input numerical value the model should specify its label....please note that the input values won't necessarily follow exact dataset values....eg dataset has 35 and 34.8 as input values with X as label. So if model has 35.4 as input label, the X should be output label. Bottom line is that the output label is based on range of input values instead of fixed one.. 
Can anyone help me with quick solution (example Jupyter notebook will be highly appreciated)
","['tensorflow', 'keras']",
Why do Bayesian algorithms work well with small datasets?,"
I read very often that Bayesian algorithms work well on small datasets. Why is that? I think it is because they might generalize more, but why is that? 
See also Investigating the use of Bayesian networks for small dataset problems.
","['machine-learning', 'bayesian-networks', 'naive-bayes', 'bayesian-statistics']",
How does the neural-network know how to tweak weights for a specific neuron?,"
I know backpropagation uses cost and gradient descent to tweak the weights to minimize the cost. But how does it know which weights to give more weight to in the first place? Is there something inside each neuron in the hidden layers that defines how this is an important neuron for the correct result in some way? How does the network know how to tweak those weights for that specific neuron?
","['neural-networks', 'deep-learning', 'backpropagation', 'weights']",
Normalizing Normal Distributions in Thompson Sampling for online Reinforcement Learning,"
In my implementation of Thompson Sampling (TS) for online Reinforcement Learning, my distribution for selecting $a$ is $\mathcal{N}(Q(s, a), \frac{1}{C(s,a)+1})$, where $C(s,a)$ is the number of times $a$ has been picked in $s$.
However, I found that this does not work well in some cases depending on the magnitude of $Q(s,a)$. For example, if $Q(s_i,a_1) = 100$, and $C(s_i,a_1) = 1$, then then this gives a standard deviation of 0.5, which is extremely confident even though the action has only been picked once. Compare that to $a_2$ which may be the optimal action but has never been picked, so $Q(s_i, a_2) = 0$ and $C(s_i,a_2) = 0$. It is unlikely that TS will ever pick $a_2$.
So, how do I solve this problem?
I tried normalizing the Q-values such that they range from 0 to 1, but the algorithm returns much lower total returns. I think I have to adapt the magnitude of the standard deviations relative to the Q-values as well. Doing it for 1 normal distribution is pretty straightforward, but I can't figure out how to do it for multiple distributions which have to take into consideration of the other distributions.
Edit: Counts should be $C(s,a)$ instead of $C(s)$ as Neil pointed out
","['reinforcement-learning', 'online-learning', 'normalisation', 'thompson-sampling', 'normal-distribution']",
learning object recognition of primitive shapes through transfer learning problem,"
Question on transfer learning object classification (MobileNet_v2 with 75% number of parameters) with my own synthetic data:
I made my own dataset of three shapes: triangles, rectangles and spheres. each category has 460 samples with diferent sizes, dimensions, different wobbles at edges. They look like this:



I want the network to classify these primitive shapes in other environments as well with different lighting/color conditions and image statistics.  
Even though I'm adding random crops, scaling, and brightnesses, at training step 10 it's already at 100% training and validation accuracy. Cross entropy keeps going down though. I'm using tensorflow hub. The performance of the network in the end could be better within other environments (virtual 3d space with such shapes). Also trained and tested for ~ 50 steps to see if the network is overfitting, but that doesn't work too well.
What alterations would you  recommend to generalize better? Or shouldn't I train on synthetic data at all to learn primitive shapes? If so, any dataset recommendations?
Thanks in advance
","['datasets', 'object-recognition', 'object-detection', 'transfer-learning', 'data-preprocessing']",
Are there any public real-life code examples of ML applications in Python? [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



Problems I often face at work usually differ from tutorial or book-like examples so I end up with a code that works but it's not elegant and takes too much time to write.
I wanted to ask you if there are some publicly accesible examples or repositories of Python codes that deal with machine learning development and application process but were created in a real company or organisation to develop their real-life products or services?
EDIT: What I do not think about are libraries or packages repositories such as tensorflow. I would like to see some codes of projects that for example use tensorflow to create some other product or service.
","['machine-learning', 'python', 'applications']",
What is the difference between evolutionary game theory and meta-heuristics?,"
Here is a list of meta-heuristic algorithms

Ant colony optimization, 
Ant lion optimizer, 
Artificial bee colony algorithm, 
Bat algorithm, 
Cat swarm optimization, 
Crow search algorithm, 
Cuckoo optimization algorithm, 
Cuckoo search algorithm, 
Differential evolution, 
Firefly algorithm, 
Genetic algorithm, 
Glowworm swarm optimization, 
Gravitational search algorithm, 
Grey wolf optimizer, 
Harmony search, 
Multi-verse optimizer, 
Particle swarm optimization, 
Shuffled complex evolution, 
Simulated annealing, 
Tabu search, 
Teaching-learning-based optimization

Can anyone explain the similarities and dissimilarities of evolutionary game theory and the meta-heuristic approach?
","['comparison', 'evolutionary-algorithms', 'game-theory']",
Should I train different models for detecting subsets of objects?,"
Suppose we have $1000$ products that we want to detect. For each of these products, we have $500$ training images/annotations. Thus we have $500,000$ training images/associated annotations. If we want to train a good object detection algorithm to recognize these objects (e.g. YOLO) would it be better to have multiple detection models? In other words, should we have 10 different YOLO models where each YOLO model is responsible for detecting 100 products? Or is it good enough to have one YOLO model that can detect all 1000 products? Which would be better in terms of mAP/recall/precision? 
","['machine-learning', 'convolutional-neural-networks', 'object-detection']",
How could an AI detect whether an enemy in a game can be blocked off/trapped?,"
Imagine a game played on a 10x10 grid system where a player can move up down left or right and imagine there are two players on this grid: An enemy and you. In this game, there are walls on the grid which you can't go through. The objective of this game is to block the enemy in so he can't move around the rest of the board and is effectively ""trapped"". 
I want to write an algorithm that detects which nodes on the board I as a player need to put blocks in, in order to trap the enemy. There are also some other considerations to think about. You have to be able to place the blocks before the enemy place can get out of the box. Also note more thing: You can move AND place a block in the position that you're moving to at the same time. 
Here's a picture as an example of the game.

EDIT: note that the board in the picture is 5x5, but that's okay for the purposes of the example
In this example, I could go up, then right and place a block, then right and place a block, then up and place a block. If there's more than one way of blocking off the enemy, then I should use the way that's going to give my enemy the least amount of space. 
Researching on google couldn't find me anything relevant, although it may have been because I wasn't using relevant search terms. I also thought about using a monte Carlo search tree algorithm for simultaneous games, but I would need to research into that more. 
","['game-ai', 'monte-carlo-tree-search']",
How would we define a set that contains itself within a knowledge ontology?,"
How would we define a set that contains itself within a knowledge ontology? 
I am thinking that set membership would probably inherit from a generic base class of total containment from which both physical containment and conceptual containment are derived. 

total containment


physical containment
conceptual containment


set containment



","['knowledge-representation', 'ontology']",
"In lemma 1 of the TRPO paper, why isn't the expectation over $s'âˆ¼P(s'|s,a)$?","
In the Trust Region Policy Optimization paper, in Lemma 1 of Appendix A, I didn't quite understand the transition from (21) from (20). In going from (20) to (21), $A^\pi(s_t, a_t)$ is substituted with its value. The value of $A^\pi(s_t, a_t)$ is given as $\mathbb{E}_{s'âˆ¼P(s'|s,a)}[r(s) + \gamma V_\pi(s') âˆ’ V_\pi(s)]$ at the very beginning of the proof. But when $A^\pi(s_t, a_t)$ gets substituted, I don't see the expectation (over $s'âˆ¼P(s'|s,a)$) appearing anywhere. It will be of great help if somebody lends some light on this.
","['reinforcement-learning', 'proofs', 'papers', 'trust-region-policy-optimization']",
What could be the cause of the drop of the total reward when using DQN to solve the cart-pole environment?,"
I'm trying to use DQN to solve the cart-pole environment. I have 2 networks (target and behavior). Both of them have 3 hidden layers with 24 neurons, using the ReLU activation. The loss is MSE and the optimizer is Adam. I copy the weights of the behavior network to the target network every 15 backpropagation steps. 
My agent learns. Below you can see the total reward and running average plots. 

However, it has a lot of ""drops"". Usually, after a couple of perfect sequences, it just ""kills"" the running average with a couple of very short episodes. What may be the reason for this behavior?
","['reinforcement-learning', 'dqn']",
Troubleshooting Binary Classifier,"
I trained a binary classifier using ML.NET's AutoML feature on a small dataset (compared to other, similar models I've trained that seem to work well)-around 500 rows with around 50 features. AutoML used cross-validation with 5 folds.
The training data is balanced to about 200 positive cases to 300 negative cases, which isn't an unreasonable representation of the real world based on domain knowledge.
The model's metrics are poor compared to other, similar models, e.g.:

Accuracy: 0.64
Positive Precision: 0.375
Positive Recall: 0.09
Negative Precision: 0.67
Negative Recall: 0.92
F1 Score: 0.15

When the model is run against unseen data, it predicts the negative case 99% of the time.
If the accuracy were truly as stated in the metric, a correct classification 2/3 of the time has some practical value in this application. However, the actual predictions of 99% the negative case are surely flawed.
Is the training set too small to expect reasonable results? Is there anything I can do to improve the model? 
",['machine-learning'],
Techniques and semantics in better training of deep learning models [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I'm relatively new to Deep Learning, and trying various models and datasets using Keras. I'm starting to love it!
Through-out my experimentations, I have come into some semantic questions that I don't know how they can affect the overall accuracy of my trained model. My target application is fire detection in videos (fire vs non-fire). So I'm trying to get tips and tricks from those well experienced on Deep learning, and here are my semantic questions:

Given that I have to do detection on videos, I've been mostly adding actuall frames of videos to my dataset, and less photos. Does adding photos from Google ever help (as we largen our dataset) or it's actually more considered noises and shall be removed?
I've trained a deep model (ResNet50) as well as a shallow 5-layer model. I realized the ResNet50 model is more sensitive and has a high recall (all fires are definitely detected), but has false positives as well (strong source of lights like sunlight or lamps are identified as fire). While the shallower model is 10x faster, it can miss fires if it is smaller in the image, so it's less sensitive. But also has low false positives. Is it always true? So what are techniques and tips to fix these issues in each of these models? 

For instance, the shallow model doesn't see this fire. Shall I think it's not complex enough to work well when the scene has many objects inside?


The sample code I saw resizes photos to 256x256 for training. What's the effect of bigger sizes vs smaller ones say 300x300? Can I expect while bigger sizes increase computation time, they provide higher accuracy?
The sample code also converts photos to grayscale and uses Antialiasing before passing. Does it have good effects? What if I pass the colored version as fire is mostly about colors?
When I see the model is doing bad on certain scenes (say those sun lights or lamps), I take multiple of those frames and add them to my non-fire dataset. Does it have any positive effects and being taken care of? And is it better to add multiple successive frames or just one frame is enough? 
My fire dataset has 1800 images and my non-fire dataset has 4500 images. As a rule of thumb, the bigger each class, the better? Of course the non-fire data should be bigger, but we can not add whatever on earth as non-fire so what should be the distribution of the sizes?

","['machine-learning', 'deep-learning', 'classification', 'keras', 'deep-neural-networks']","I try to answer the things I know for sure:4.Grayscaling reduces the information, which might decrease training time, but also model performance (accuracy, precision, recall). What I have seen is that grayscaling is used in for example face detection where structural information (shapes) is important and colors can be regarded as secondary. For your example you have to determine whether such structural differences between positives and negatives exist and if color is a valuable information.If possible, I would somehow alter these copies by clipping the objects (sun lights, lamps) and rotate, scale, make them transparent etc.In the end your training data should be representative of the data it should generalize to. If you would for example take my user icon as an additional non fire example, this wouldn't help your cause."
"How would the ""best function"" been constructed if there are no computationally limitations?","
I am reading the Wikipedia article on gradient boosting. There is written:

Unfortunately, choosing the best function $h$ at each step for an arbitrary loss function $L$ is a computationally infeasible optimization problem in general. Therefore, we  restrict our approach to a simplified version of the problem.

How would the ""best function"" been constructed if there are no computationally limitations?
Because the wiki gives me the idea that the model could been better, but that compromises have been made.
","['machine-learning', 'objective-functions', 'gradient-boosting']",
yolo output and how to define labels for backpropogation on it,"
I want to build the yolo architecture in keras but can't understand the basic idea behind the training of the yolo, like how to define the labels for whether there is no object there what we have to do. Do we have to take the boundary box as 0 or not include the block only for that part? It's quite confusing. 
","['deep-learning', 'backpropagation', 'object-detection']",
What is a good model for regression problem with binary features and small data?,"
I am trying to predict the solution time for riddles in which matchsticks are combined into digits and operators. An example of a matchstick riddle is 4-2=8. The solution for this riddle would be obtained by moving one matchstick from the â€˜8â€™ to the â€˜-â€™ sign, resulting in the correct equation 4+2=6. The data consists of 100 riddles and the corresponding solution times. The two types of features that are available for each riddle are:

a 23 dimensional binary vector that indicates which of the available positions are filled with matches
or
a 12-dimensional integer vector that counts the appearance of each token (10 digits, 2 operators)

Although today neural nets are very popular I am not sure that a neural net is the best choice for this particular problem. Firstly, because the data set is very small. Secondly because of the binary inputs.  What might be a more effective model for this problem ? 
","['models', 'regression']",
Why AI is (or not) a good option for the generation of random numbers?,"
Why AI is (or not) a good option for the generation of random numbers? Would GANs be suited for this purpose?
","['neural-networks', 'generative-adversarial-networks', 'randomness']",
Concrete example of latent variables and observables plugged into the Bayes' rule,"
In the context of the variational auto-encoder, can someone give me a concrete example of the application of the Bayes' rule
$$p_{\theta}(z|x)=\frac{p_{\theta}(x|z)p(z)}{p(x)}$$
for a given latent variable and observable?
I understand with VAE's we're essentially getting an approximation to $p_{\theta}(z|x)$ that models the distribution that we think approximates the latent variables, but I need a concrete example to really understand it.
","['convolutional-neural-networks', 'autoencoders', 'variational-autoencoder']","Let's assume the probability distributions are Gaussian (or normal) distributions. In other words, in the Bayes' rule\begin{align}
p(z|x)=\frac{p(x|z)p(z)}{p(x)}
\tag{1}\label{1}
\end{align}The posterior $p(z|x)$, the likelihood $p(x|z)$, the prior $p(z)$ and the evidence (or marginal) $p(x)$ are Gaussian distributions. You can assume this because Gaussian distributions are closed under conditioning and marginalization.For simplicity, let's further assume that they are univariate Gaussian distributions. Given that the Gaussian distribution is a continuous probability distribution, it has an associated probability density function (rather than a probability mass function, which is associated with discrete probability distributions, such as the Bernoulli distribution). The probability density function of the Gaussian distribution is\begin{align}
f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2} } e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \tag{2}\label{2}
\end{align}where $\mu$ and $\sigma^2$ are respectively the mean and variance of the Gaussian distribution and $x$ is a variable (similarly to a variable $x$ in any mathematical function $f(x)$). So, given a concrete value for $x$, for example, $x=1$, then $f(x=1, \mu, \sigma^2)$ is a so-called density value (rather than a probability, which a probability mass function returns, given an input). For example, let's assume that the mean $\mu=0$ and the variance $\sigma^2 = 1$, then, for $x=1$, the density will be$$
f(1 \mid 0, 1) = \frac{1}{\sqrt{2\pi} } e^{ -\frac{1}{2} }
$$So, to obtain the concrete density value, I've just replaced the concrete values of $x$, $\mu$ and $\sigma^2$ in equation \ref{2}. To calculate the posterior $p(z|x)$ in equation \ref{1}, you just need to replace the likelihood $p(x|z)$, the prior $p(z)$ and the evidence $p(x)$ with the Gaussian probability density shown in equation \ref{2}, so you will have\begin{align}
p(z|x)=\frac{f_{X\mid Z}(x \mid \mu_{X\mid Z}, \sigma^2_{X\mid Z}, z) f_{Z}(z \mid \mu_{Z}, \sigma^2_{Z})}{f_{X}(x \mid \mu_{X}, \sigma^2_{X})}
\tag{3}\label{3}
\end{align}I've explicitly added a subscript to the means and variances of each probability density, given that, for example, the mean of the probability density $f_{X\mid Z}$ might be different than the mean of the probability density $f_{Z}$ or $f_{X}$, etc. So, to get the actual density value (a real number) that represents $p(z|x)$, you just need to replace $f_{X\mid Z}$, $f_{Z}$ and $f_{X}$ with the definition of the Gaussian density function in \ref{2} with their actual mean and variance values. I'll let you do this, given that this is really just a matter of picking up a concrete value for the means and variances and doing some algebra.If you assume the posterior, the likelihood, the prior or the evidence to have a different distribution, you will do the same thing, but using the probability density or mass function of your chosen distribution.In the context of the variational auto-encoder, you will be learning the mean and variance of the distribution, so the mean and variance will not be fixed, but they will be the parameters that you want to find. However, this does not change the way you apply the Bayes' rule."
How does maximum approximation of the posterior choose a distribution?,"
I was learning about the maximum a posteriori probability (MAP) estimation for machine learning and I found a nice short video that essentially explained it as finding a distribution and tweaking the parameters to fit the observed data in a way that makes the observations most likely (makes sense). 
However, in mathematical terms, how does it determine which distribution best fits the data? 
There are so many distributions out there that it could be any of them and the parameters you could fit them could be infinitely large.
","['probability-distribution', 'bayesian-probability']","As opposed to your apparently current belief, in maximum a posteriori (MAP) estimation, you are looking for a point estimate (a number or vector) rather than a full probability distribution. The MAP estimation can be seen as a Bayesian version of the maximum likelihood estimation (MLE). Therefore, I will first remind you of the objective of MLE.Let $\theta$ be the parameters you want to find. For example, $\theta$ can be the weights of your neural network. In MLE, we want to find a point estimate (rather than a full distribution). The objective in MLE is\begin{align}
\theta^* 
&= \operatorname{argmax}_\theta p(X \mid \theta) \tag{1}\label{1}
\end{align}where $p(X \mid \theta)$ is the likelihood of the data $X$ given the parameters $\theta$. In other words, we want to find the parameters $\theta$ such that $p(X \mid \theta)$ is the highest, where $X$ is your given training data, so $X$ is fixed. The notation $p(X \mid \theta)$ can be confusing because, in a conditional probability distribution, $p(a\mid b)$, we often assume that $b$ is given and $p(a\mid b)$ is a distribution over $a$. However, in the case of MLE, $\theta$ in $p(X \mid \theta)$ is not fixed, but it is a variable, while $X$ is given and fixed. Hence we call $p(X \mid \theta)$ a likelihood rather than a probability density or mass function. Moreover, we often denote the likelihood as $\mathcal{L}(\theta; X) = p_{\theta}(X)$ (and there are other notations, but this is, in my opinion, the least confusing one), because we want to emphasize that the likelihood is actually a function of the variable $\theta$. However, this is notation can also be confusing because we equate a function of a variable $\theta$ to a probability distribution over $X$. However, you should note that $p_{\theta}(X)$ is parametrized by $\theta$.Therefore, the MLE estimation \ref{1} can also be written as follows\begin{align}
\theta^* 
&= 
\operatorname{argmax}_\theta \mathcal{L}(\theta; X) \\
&=\operatorname{argmax}_\theta p_{\theta}(X)
\tag{2}\label{2}
\end{align}
where $\theta^*$ is the point estimate of the objective function.This notation emphasizes the fact that we want to find $\theta$, such that the probability of the given data $X$ is maximized.MAP is similar to MLE, but the objective is slightly different. First of all, we assume that $\theta$ is a random variable, so we have an associated probability distribution $p(\theta)$.Recall that the Bayes' rule is the following\begin{align}
p(\theta \mid X) = \frac{p(X \mid \theta) p(\theta)}{p(X)}
\tag{3}\label{3}
\end{align}The objective function in MAP estimation is\begin{align}
\theta^* 
&= \operatorname{argmax}_\theta \frac{p(X \mid \theta) p(\theta)}{p(X)} \\
&= \operatorname{argmax}_\theta p(\theta \mid X) \tag{4}\label{4}
\end{align}Given that $p(X)$ does not depend on $\theta$, for the purposes of optimization, we can ignore it, so equation \ref{4} becomes\begin{align}
\theta^* 
&= \operatorname{argmax}_\theta p(X \mid \theta) p(\theta) \\
&= \operatorname{argmax}_\theta p(\theta \mid X)
\tag{5}\label{5}
\end{align}
which is the MAP objective.In MAP, the objective is \ref{5}, which includes a prior over $\theta$, while, in MLE, equation \ref{1}, there is no such thing. In both MAP and MLE, we want to find a point estimate (which can be a number, if you have just one parameter, or a vector of size $N$, if you have $N$ parameters).MAP is equivalent to MLE if you use a uniform prior, that is, if $p(\theta)$ is a uniform distribution.In MAP, the human (you, me, etc.) chooses the family of distributions. For example, you can assume that your parameters $\theta$ follow a Gaussian distribution, so $p(\theta)$ will be a Gaussian distribution over the parameters. Why do I say ""family""? For example, in the case of a Gaussian distribution, you have two parameters that control the shape of the distribution, namely, the mean and variance. Depending on the concrete values of these two parameters, you will have different Gaussian distributions, so you call all these Gaussian distributions a family.To find $\theta^*$, you can use an optimization method like gradient descent or, in certain cases, you can find a closed-form solution. See also Which distributions have closed-form solutions for maximum likelihood estimation?.The following blog post MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation, by Agustinus Kristiadi (a Ph.D. student in machine learning), might also be useful, so I suggest you read it. It will give you more details that I've left out on purpose to avoid cluttering this answer."
How can max-pooling be applied to find features in words?,"
I'm reading about max-pooling in a dynamic CNN paper. I can see how it can help find features in images, given that the pixel with the highest density gets pooled, but how does it help to find features in words?
","['convolutional-neural-networks', 'pooling', 'max-pooling']",
Unsupervised learning to optimize a function of the input [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I am looking to build a neural network that takes an input vector $\mathbf{x}$ and outputs a vector $\mathbf{y}$ such at $f(\mathbf{x}, \mathbf{y})$ is minimized, where $f$ is some function. The network will see many different $\mathbf{x}$ during training to adjust its weights and biases; then I will test the network by using the test set $\{\mathbf{x}_1, \dots, \mathbf{x}_n \}$ to calculate $\sum(f(\mathbf{x}_1, \mathbf{y}), \dots, f(\mathbf{x}_n, \mathbf{y}))$ to see if this sum is minimized.
However, I have no labels for the output $\mathbf{y}$. The loss function I am trying to minimize is based on the input and output, instead of the output and label.
I tried many standard Keras and TensorFlow loss functions, but they are unable to do the job. Any thoughts on how this might be achieved?
","['neural-networks', 'optimization', 'unsupervised-learning']",
Formulating MCTS with random outcomes of actions?,"
I am working on implementing MCTS for a scheduling problem where MCTS is formulated each time there are multiple jobs that need to be scheduled. When a job is executed, the resulting state of the system is random. The challenge I'm having is that the implementation I'm currently using relies on the ability to determine if a node is fully expanded. However, there are so many children of the root node that it's not feasible to expect all of them will ever be visited. Is there a suggested method of conducting MCTS in cases where nodes will not likely ever be fully expanded?
",['monte-carlo-tree-search'],
Calculation of Neural network biases in backpropagation,"
While learning neural networks I've found a basic Python working example to play with. It has 3 input nodes, 4 nodes in a hidden layer, 1 output node. 5 data sets for training.
The initial code is without biases, which I'm trying to implement, forward and back calculations. From different internet sources I see that bias is just like other weights with a static input value 1, and backpropagation calculation should be similar and simplier.
But my current code version is not working - with the same input I get very different results from ~0.002 to ~0.99.
Please help me to fix biases calculations. Probably lines marked with ???. Here is a Python 2 testing code:
import numpy as np


# Sigmoid and it's derivative
def nonlin(x, deriv=False):
    if (deriv == True):
        return x*(1-x)

    return 1/(1+np.exp(-x))


X = np.array([[0,0,1],
              [0,1,1],
              [1,0,1],
              [1,1,1],
              [1,1,1]])

Y = np.array([[0],
              [1],
              [1],
              [0],
              [0]])

# Static initial hidd. layer weights for testing
wh = np.array([[-0.16258307,  0.43597283, -0.99471565, -0.39715906],
               [-0.70551921, -0.81601352, -0.62549935, -0.30959772],
               [-0.20477763,  0.07532473, -0.15920573,  0.3694664 ]])
# Static initial output layer weights for testing
wo = np.array([[-0.59572295],
               [ 0.74949506],
               [-0.95195878],
               [ 0.33625405]])

# Hidden layer's biases
biasH = 2 * np.random.random((1, 4)) - 1  # ???
# Output neuron's bias
biasO = 2 * np.random.random((1, 1)) - 1  # ???
# Static hidden layer's biases input
biasInputH = np.array([[1, 1, 1, 1]])     # ???
# Static output layer's bias input
biasInputO = np.array([[1]])              # ???


# Number of iterations to teach
for j in xrange(60000):

    # Feedforward
    h = nonlin(np.dot(X, wh) + biasH)
    o = nonlin(np.dot(h, wo) + biasO)

    # Calculate partial derivatives & errors
    o_error = Y - o

    if (j % 10000) == 0:
        print ""Error:"" + str(np.mean(np.abs(o_error)))

    o_delta =  o_error * nonlin(o,     deriv=True)
    o_biases = o_error * nonlin(biasO, deriv=True)  # ???

    h_error =  o_delta.dot(wo.T)
    h_delta =  h_error * nonlin(h,     deriv=True)
    h_biases = h_error * nonlin(biasH, deriv=True)  # ???

    # Update weights and biases
    wo += h.T.dot(o_delta)
    wh += X.T.dot(h_delta)

    # biasH += biasInputH.dot(h_delta)  # ???
    # biasO += biasInputO.dot(o_delta)  # ???


# Try new data
data = np.array([1,0,0])

print ""weights 0:"", wh
print ""weights 1:"", wo
print ""biases 0:"",  biasH
print ""biases 1:"",  biasO
print ""input:   "",  data

h = nonlin(np.dot(data, wh))
print ""hidden:  "", h
print ""output:  "", nonlin(np.dot(h, wo))

","['neural-networks', 'python', 'backpropagation']",
NLP annotation tool online and other tools to compare performances of different NLP algorithms,"
I do text annotations (POS tagging, NER, chunking, synset) by using a specific annotation tool for Natural Language Processing. I would like to make the same annotations on different tools to compare the performances of both.
Furthermore, for I found several logical and linguistic errors in the way the algorithm was previously trained, I would like to measure the way such anomalies affect the intelligence of the chatbot (that's to say its ability to understand questions and answers made by the customers as regard to sentences which have been structured in a certain way), by comparing results with those performed by other NLP engines.
In other terms, I would like to collect some ""benchmark"" to have an idea of which level the NLP algorithm developed by the company I work with works at.
Is there any tool (open source annotation tools based on other NLP algorithms, tools to collect benchmark, etc.) which might help me to perform such a task? 
","['natural-language-processing', 'chat-bots', 'benchmarks']",
Is there any existing attempt to create a deep learning model which extracts vector paths from bitmaps?,"
I need an algorithm to trace simple bitmaps, which only contain paths with a given stroke width.
Is there any existing attempt to create a deep learning model which extracts vector paths from bitmaps?
It is obviously very easy to generate bitmaps from vector paths, so creating data for a machine learning algorithm is simple.  The model could be trained by giving both the vector and bitmap representation. Once trained, it would be able to generate the vector paths from the given bitmap.
This seems simple, but I could not find any work on this particular task. So, I suppose this problem is not fitted for current deep learning architectures, why?
The goal is to trace this kind of image, which would be drawn by hand with a thick felt pen and scanned:

So, is there a deep learning architecture fitted for this problem?
I believe this question could help me understand what is possible to do with deep learning and what is not, and why. Tracing bitmaps is a perfect example of converting sparse data to a dense abstract representation; I have the intuition one can learn a lot from this problem.
","['deep-learning', 'image-processing', 'algorithm-request', 'model-request']",
Effects of translating RL action probability through non linearity,"
I am training an RL agent (specifically using the PPO algorithm) on a game environment with 2 possible actions left or right.
The actions can be taken with varying ""force""; e.g. go left 17% or go right 69.3%. Currently, I have the agent output 21 actions - 10 for left (in 10% increments), 10 for right in 10% increments and 1 for stay in place (do nothing). In other words, there is a direct 1-1 mapping in 10% increments between the agent output and the force the agent uses to move in the environment.
I am wondering, if instead of outputting 21 possible actions, I change the action space to a binary output and obtain the action probabilities. The probabilities will have the form, say, [70, 30]. That is, go left with 70% probability and go right with 30% probability. Then I take these probabilities and put them through a non-linearity that translates to the actual action force taken; e.g an output of 70% probability to go left, may in fact translate to moving left with 63.8% force.
The non linear translation is not directly observed by the agent but will determine the proceeding state, which is directly observed.
I don't fully understand what the implications of doing this will be. Is there any argument that this would increase performance (rewards) as the agent does not need to learn direct action mappings, rather just a binary probability output?
",['reinforcement-learning'],"I don't fully understand what the implications of doing this will be. Without other matching adjustments, you will break your agent.The problem is how your new action space gets converted back into gradients to update the agent, after it has acted and needs to learn from the results. The NN component of policy function you are considering is designed to work by balancing a discrete probablility distribution. It learns by increasing the probability of actions (in the binary case, the probability of going left vs going right) that score better than a current baseline level.When interpreting the result from going 63.8% left, you have to resolve two things - which action did the agent take, and what changes to your parameters will increase the probability of taking that action. Unfortunately neither of these tasks are simple if you combine the action choices in the way you suggest. Also, you have lost exploration. The combined left/right algorithm will always output a fixed steering amount for each state. Whilst there are algorithms, like DDPG, that can work with this, it is not really possible to adapt PPO to do so.However, PPO already supports continuous action spaces directly. You can have your network output the mean and standard deviation of a distribution for how to steer, and sample from that. Then the action choice taken will directly relate to the output of the network and you can adjust the policy to make that choice more or less possible depending on results from taking it. If you are using a library implementation of PPO, then this option should be available to you."
Can mobilenet in some cases perform better than inception_v3 and inception_resnet_v2?,"
I have implemented a multi-label image classification model where I can choose which model to use, I was surprised to find out that in my case mobilenet_v1_224 performed much better (95% Accuracy) than the inception models (around 88% Accuracy), I'm using pretrained models (that I download from here and adding a final layer that I train on my own data (3000 images). I wanted to get your opinion and see if maybe I'm doing something wrong.
","['image-recognition', 'tensorflow', 'pretrained-models']",
What does the Markov assumption say about the history of state sequences?,"
Does the Markov assumption say that the conditional probability of the next state only depends on the current state or does it say that the conditional probability depends on a fixed finite number of previous states?
As far as I understand from the related Wikipedia article, the probability of the next state $s'$ to appear only depends on the current state $s$.
However, in the book ""Artificial Intelligence: A Modern Approach"" by Russell and Norvig, on page 568, they say: ""Markov assumption â€” that the current state depends on only a finite fixed number of previous states"". 
To me, the second statement seems contradictory to the first, because it may mean that a state can depend on the history of states as long as the number is fixed a finite. For example, the current state depended on the last state and the state before the last state, which is 2 sequential previous states (a finite number of states).
Is Markov assumption and Markov property the same?
","['reinforcement-learning', 'math', 'definitions', 'norvig-russell', 'markov-property']","A stochastic process has the Markov property if the probability distribution of future states conditioned on both the present and past states depends only on the present state or, more formally, the following equality holds.$$
p(s_{t+1} \mid s_{t}, s_{t-1:1}) = p(s_{t+1} \mid s_{t}), \forall t
$$The hidden Markov model (HMM) is an example of a model where the Markov property is often assumed to hold. In other words, the Markov assumption is made in the case of the HMM.There are also the variable-order (or higher-order) Markov models, where the future state can depend on the previously $n$ states or, more formally, the following equality holds.$$
p(s_{t+1} \mid s_{t}, s_{t-1:1}) = p(s_{t+1} \mid s_{t:t-n}), \forall t
$$In this context, a hidden Markov model is called a first-order Markov model ($n=0$). Therefore, there can also be second-order ($n=1$), third-order ($n=2$), etc., Markov models. In fact, there are also higher-order hidden Markov models.To conclude, the expressions Markov property and Markov assumption are not exactly interchangeable. The Markov property is an attribute that a stochastic process can be assumed to possess. In that case, the Markov assumption is made. The expression Markov property usually refers to a first-order Markov property, but it can more generally refer to a higher-order Markov property. "
How to organize model training hyperparameters,"
I am working on multiple deep learning projects, most of them in the area of computer vision. For many of them I create multiple models, try different approaches, use various model architectures. And of course I try to optimize hyperparameters for each model. 
Now, that itself works fine. However, I start to lose track of all the various parameters and model layouts I tried. The problem is, sometimes for example I want to re-train a model from a past project with a new data set, but using the same hyperparameters from the last (best) successful training. So I need to look up that project's documentation, or I have some hyperparameters saved in a text or Excel file, etc.pp.
For me that feels a bit cumbersome. I bet I am not the only one facing this problem, surely there must be a better way than ""remembering"" all the hyperparamters from all projects / models manually via text files and alike.
What are your experiences, have you found a better software / solution / approach / best practice / workflow for that? I must admit, I would welcome a software to aid with that a lot.
","['training', 'hyperparameter-optimization', 'hyper-parameters']",
What are some references that describe known filters (or kernels) and how we can create new ones?,"
I'm pursuing a master's degree in Artificial Intelligence. My final work is about Convolutional Neural Networks.
I was looking for information about filters (or kernels) of the convolutional layers. I have found this article: Lode's Computer Graphics Tutorial - Image Filtering, but I need more.
Do you know more resources about more filters (that it is known that they work) and how to create new ones?
In other words, I want to know how they work and how can I create new ones.
I've thought to create a C++ program, or with Octave, to test the new kernels.
By the way, my research will be focused on image segmentation to process MRIs.
","['convolutional-neural-networks', 'reference-request', 'image-processing', 'image-segmentation', 'filters']","I'd suggest you better understand edge detectors such as Robert or Sobel operators first to understand better how convolution operation on images extract features by constant value kernels.  Would personally recommend Gonzales and Woods for this,  as it gives a pure mathematical explanation to how and why these features are extracted.Essentially the convolution kernels used in CNN's are ones with a learned set of values for the kernel. For a better understanding of learned convolution kernels and, quite frankly, any idea under deep learning would easily recommend Deep Learning by Goodfellow et al "
How to count pixels in a object mask which is segmented using Mask R-CNN?,"
I have segmented concrete cracks from concrete structure images using Mask R-CNN. Now I need to measure the length of the segmented masked crack. 
Will the pixel counting method work? Can anyone help?
Note: The images are taken at the constant distance from the object.
","['deep-learning', 'convolutional-neural-networks', 'reference-request', 'object-detection']",
Classification with deeplearning : clean start vs continue training,"
I trained some weights to identify apples and oranges (using YOLOv3). 
If I want to be able to identify peaches, which approach is usually recommended:

Start clean and train the 3 classes. 
Train the peaches over the already-trained weights (with apples and oranges)


Only train with peaches images
Use all available training data (including apples and oranges)


This is what I have found:

If I start clean, it will take longer until I can get a good result, but the detection is usually better.
Every time I add a new class (using 2.2), the detection get worse for the already learned objects, but it takes less time until I can get a good result (however I suspect that apples and oranges become over-fitted?). 
I haven't tested 2.1, as I think that it won't be able to re-adjust the weights for the apples and the oranges.

Is the above expected? What is the recommended course of action?
","['deep-learning', 'classification', 'weights']","If the task involves only apples, orange and peaches, you should use method 1. As the number of classes is small, the network cannot generalize well to all classes. As a side note, you should start with the pretrained weights of YOLO v3 as some classes of YOLO v3 may be fruits, which can help your model converge faster. If the number of classes is large, for example a hundred different fruits, you should use method 2.2 . The model should be able to generalize to all fruits and can converage faster as many fruits look the same. This is the case of transfer learning. In original YOLO v3 training, image net weights for dark net is used for the backbone network. It accelerates the training of YOLO v3.For 2.1, it will not work as gradient descent will not consider the trained weight. The trained weights will be over written by the weights for peaches. For a recommended method, it depends on your class size. If the model will continue to add more classes, you should perhaps use 2.2 but only start using 2.2 when you have a considerable amount of classes. Hope I can help you."
When are compiled vs. interpreted languages more optimal in AI?,"
When are interpreted languages more optimal?  When are compiled languages more optimal?  What are the qualities and functions that render the so in relation to various AI methods?
","['comparison', 'programming-languages']",
Is artificial intelligence really just human intelligence?,"
Essentially, AI is created by human minds, so is the intelligence & creativity of algorithms properly an extension of human intelligence & creativity, rather than something independent?
I assume that intelligence does not necessarily require creativity, however, creativity can result from machine learning.  (A simple example is AlphaGo discovering novel strategies.)
","['philosophy', 'intelligence', 'artificial-creativity']",
Trying to get started with LISA and Lisp,"
I am trying to to do a sort of block-validator for bitcoin(and alike chains), in it I need to depending on chain and block-height only allow certain operators in the transactions scripts. One thought I had was that this might be something that LISA should be good for. (I might be wrong in this) but shouldn't something like a rule engine be a good fit for that? What I want is a good way to defines the rules for my validator on how to validate that a block and its transactions adhere to the consensus rules?
I am sort of getting to this point
(defpackage #:chain-validator
  (:shadowing-import-from #:lisa #:assert)
  (:use #:lisa
    #:cl))

(in-package :chain-validator)

(defclass chain-fundamental () ())

(defclass chain-block (chain-fundamental)
  ((height :initarg :height :initform 1)
   (chain :initarg :chain :initform :bitcoin)))

(defclass chain-tx (chain-fundamental)
  ((in-block :initarg :block :initform 'nil)
   (pk-script :initarg :pk-script)
   (is-coinbase-tx :initarg :coinbase :initform 'f)))

(defclass chain-OP (chain-fundamental)
  ((name :initarg :name)
   (op-code :initarg :op-code)))

(defrule dont-allow-op-mul-after-height-1000
    ;;; how to write this one?????
    ;; but if I only want to allow mul on a certain chain after height
    ;; 2000?
    )

(defrule startup ()
  =>
  (assert-instance
   (make-instance 'chain-OP :name :PUSHDATA4 :op-code #x4e))
  (assert-instance
   (make-instance 'chain-OP :name :EQUAL :op-code #x87))
  (assert-instance
   (make-instance 'chain-OP :name :MUL :op-code #x95))
  (let* ((genesis-blk (make-instance 'chain-block))
     (later-blk (make-instance 'chain-block :height 2500))
     (first-coinbase-tx (make-instance 'chain-tx :block genesis-blk))
     (later-coinbase-tx (make-instance 'chain-tx :block later-blk)))
    (assert-instance genesis-blk)
    (assert-instance later-blk)
    (assert-instance first-coinbase-tx)
    (assert-instance later-coinbase-tx)))


;; how can I use LISA to get the chain-OPs that are allowed for a
;; transaction belonging to a specific block at some height, I sort of
;; want to find them all so i later can verify that the pk-script part
;; only contains those OPs. Could I write rules that would acctually
;; do the validation for me? that would check if chain-tx.pk-script
;; only contains certain OPs. And if we have multiple chains, how do I
;; write the rules to take account for that?

But after that I don't know how to proceed, the questions I want LISA to answer for me are things like

What are the valid script operations for a certain block, or transaction?
Is this block or transaction valid?

Maybe what I need is primer on rule engines or a good tutorial. I just can't really get my head around how to write the rules.
","['expert-systems', 'lisp']",
Loss function for increasing the quality of the image when labels are not perfectly alligned,"
I am trying to increse the quality of the images that I gather from the microscope.  That is a acoustic microscope and there are lots of technical details but in a nutshell the low quality images and its corresponding high quality images that I gather from the same sample are not perfectly alligned because in my setting it is impossible to increase quality without removing the sample from the microscope so when I put it again it is a manual process so they are not perfectly aligned.
Output of my network will be,let's say, 256 x 256 image and its corresponding label will be high quality 256 x 256 image,in theory, of the exactly same area. If I make pixel to pixel comparison between them, for example taking MSE for the loss function, will it be able to learn ? I am not sure because pixels are not perfectly alligned, they do not represent the same area of the image(the difference is not that great but they are not perfectly alligned as I said)
","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'objective-functions']",
Finding total number of states in a POMDP,"
I've been working on a question that is posed in a document I've been reading, that models qualifying for a job as a POMDP. In this model, a person takes 3 exams, and must pass all of them in order to get the job. The person make either be qualified or not qualified in the subjects covered by each individual exam, and there is some probability that a person may be qualified for a subject covered in a particular exam, but still not pass (due to nerves). False passes are also possible as well.
A candidate is allowed a maximum of 2 exam attempts. 
In understanding this problem, I've tried to list out all possible states the person might be in (qualified / not qualified for each exam), and have found the following possible states: (where Q is qualified, and N is not qualified)
QQQ
QQN
QNN
QNQ
NNN
NQN
NNQ
NQQ
So, the total number of possible states is 8.
Have I covered all possible states? I'm wondering if there's an easier way to find out total number of states, without having to list them all out in the above way. I'm very new to this field, so any help is appreciated. 
","['reinforcement-learning', 'markov-decision-process', 'pomdp']",
What does top N accuracy mean?,"
Places205-VGG, a CNN trained model for 205 scene categories of Places Database with  2.5 million images Places205 dataset has top1 accuracy = 58.9% and top5 accuracy = 87.7%.
What does top1 and top5 (and, in general, top $N$) accuracy mean in the context of deep learning?
","['deep-learning', 'datasets', 'scene-classification', 'accuracy']","It is explained in this CrossValidated post. Top1 accuracy means the best guess (class with highest probability) is the correct result 58.9% of the time, while top5 accuracy means the correct result is in the top 5 best guesses (5 classes with highest probabilities) 87.7% of the time."
Does MMD-VAE solve the problem of blurred images of vanilla VAEs?,"
I understand that with vanilla VAEs, there are a few reasons justifying the production of blurred out images. The InfoVAE paper describes the case when the decoder is flexible enough to ignore the latent attributes and generate an averaged out image that best reduces the reconstruction loss. Thus the blurred image.
How much of the problem of blurring is really mitigated by the MMD formulation in practical experiments? If someone has experience working with MMD-VAEs, I'd like to know their opinion on what the reconstruction quality of MMD-VAEs is really like.
Also, does the replacement of the MSE reconstruction loss metric by other perceptual similarity metrics improve generated image quality?
","['neural-networks', 'deep-learning', 'generative-model', 'image-generation', 'variational-autoencoder']","[Answering my own question after 5 months of studying VAE models]The point of the MMD-VAE or InfoVAE is not exactly to emphasise on the visual quality of generated samples. It is to preserve greater amount of information through the encoding process. The MMD formulation stems from introducing a mutual coefficient factor into the Evidence Lower BOund (ELBO) loss of VAEs. Refer to the paper appendices for full derivation. This formulation improves information content in latent space and provides for more accurate approximation of the true posterior - these results have also been empirically proven in the paper.However, the InfoVAE uses pixel-wise or element-wise reconstruction loss. An element-wise reconstruction loss is likely to lead to some extent of blurriness inrespective of the prior loss term. On Github, several developers have implemented the InfoVAE model and shown their results. Here is a link to one such implementation whose results I could personally verify.From my own experimentations, I can say that even though InfoVAE may give better reconstructions for some data, there is still considerable blurriness.Perceptual similarity metrics may be learned or computed as a static function of the input image. With a learned perceptual loss, VAEs can produce much sharper images. PixelVAE and VAEGAN are well-known models with such implementations. For a static function of the image itself, reconstruction quality will depend on the nature of that function and such a model may not be very useful for all kinds of datasets. Using measures like SSIM, FSIM, we may still end up getting blurred images."
Which algorithm can I use to solve a problem with multiple objectives and constraints?,"
Consider a problem with many objectives. In my case, these are school grades for different courses (or subjects). To be more concrete, suppose that my current grade for the math course is $12/20$ and for the philosophy course is $8/20$. My objective is to get $16/20$ for the math course and $15/20$ for the philosophy course.
I have the possibility to take different courses, but I need to decide which ones. These courses can have a different impact depending on the subject. Let's say that the impact factor is in the range $[0, 1]$, where $0$ means no impact and $1$ means a big impact. Then the math course could have a big impact (e.g. $0.9$) on the grade, while maybe a philosophy course may not have such a big impact.
The overall goal is to increase all the grades as much as possible while taking into account the impact of their associated course. In my case, I can have more than two courses and subjects.
So, which algorithms can I use to solve this problem?
","['machine-learning', 'optimization', 'constraint-satisfaction-problems', 'linear-programming']",
How does deepfake technology work with multiple people in a single frame?,"
I was watching this video from corridor crew, according to them, they have used deepfake technology to create this video. I myself have never made a deepfake videos, but I have enough knowledge in the underlying technology to know that it's hard to swap a face with multiple people existing simultaneously in a frame and let alone swapping faces of multiple people in a single frame. But corridor crews video showed that multiple deepfakes can be done, and that's why I am sceptical about the video of using deepfake technology.
If the video is indeed made with deepfake technology, then what is the mechanism behind this? My own guess is that they might have masked other people and concentrated on one in a frame. Then they have used this masked frame to generate the deepfake, which then concatenated to the original frame. Do you think this is possible? Is there a research article or blog post which explains this process?
","['deep-learning', 'applications', 'generative-adversarial-networks', 'autoencoders', 'deepfakes']",
"How to deal with images of different sizes, which need to be passed to a model of fixed input size, without losing details and spatial information?","
I have the following problem while using convolutional neural networks to detect forgeries:
Resizing the image to fit the required input size may not be a good way because the forgery detection largely relies on the details of images, for example, the noise. Thus the resizing process may change/hurt the details.
Existing methods mainly use image patches (obtained from cropping) that have the same size. This way, however, will drop the spatial information.
I'm looking for some suggestions on how to deal with this problem (input size inconsistency) without leaving out the spatial information.
","['convolutional-neural-networks', 'computer-vision', 'image-recognition', 'data-preprocessing']",
How can I improve the performance of a model trained to detect vehicle poses?,"
I'm looking for some suggestions on how to improve our vehicle image recognition. We have an online marketplace where customers submit photos of their vehicles. The photos need to meet certain requirements before the advert can be approved. 
Customers are required to submit the following vehicle photos: front, back, left-side, right-side, engine (similar to the front photo but with the hood open) and instrument panel cluster.  The vehicle must be well framed in the photo, in other words, it must not be too small or so big that the edges touch the frame of the photograph. It also needs to be one of the mentioned types and the camera must be facing the vehicle directly with only small angle variations (a front photo can't include a large piece of the side of a car).
Another developer had a go and built a CNN with Keras which does alleviate some manual grind (about 20,000 photos were used for training - no annotations). The accuracy sits at around 75% for the vehicle photos but only 55% for the engine and instrument cluster. Each photo is still manually checked, but it is a case of agreeing or disagreeing with what was recognised. 
I was wondering if it wouldn't be better to detect a vehicle in the image using an existing pre-trained model like ImageAI. Use the bounding box of the vehicle to determine it is correctly placed in the frame of the photograph and within acceptable dimensions. There may be multiple vehicles in the picture so work with the most prominent one. 
At that point would it be worth trying to develop something to workout the pose of the vehicle (idea: https://github.com/johnberroa/CORY) or just do some transfer learning with whatever pre-existing trained model was used and spend some time annotating the images?

","['image-recognition', 'computer-vision', 'image-segmentation', 'transfer-learning', 'pretrained-models']",
Is there has any method to train Tensorflow AI/ML that I focus on detecting background of image more than common objects? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 3 years ago.







                        Improve this question
                    



Is there has any method to train Tensorflow AI/ML that I focus on detecting background of image more than common objects?
I'm newbie to ML field, but was assigned to do job that make an application which can detecting on showroom image/places and detecting on the floor, wall then find out what is the material/ceramic/marble/etc. product they are.
Example: This is showroom picture,

the wall and the floor of showroom are using this product material


Is it possible to do something like I described?
How to start with?
If I don't want to install Tensorflow into my computer, is there a service that can make a model to use in the device? (my goal need to use the model in Android device)
What method/type of ML should I approach 'Classification' or 'Object Detection' or other else?

","['machine-learning', 'tensorflow', 'ai-basics']","Trying to address all the questions asked in the end in the same orderMost definitely possible.I would say its best you approach this with segmentation to start with.Just use a free GPU runtime notebook service such as Google Colab or Kaggle Kernels. But you would not directly be able to integrate with the device, you'd have to keep moving input and output from your drive (on Colab). There might be a better service for the needs described, but this is the best I know on this.Your background can be segmented and the segment can work on transforms such as maybe convolutions or affine transforms to be able to get the relevant information regarding the background.Hope this was helpful!"
"In the on-policy state distribution for episodic tasks, why don't we take into account the length of the episode?","
In Sutton & Barto's ""Reinforcement Learning: An Introduction"", 2nd edition, page 199, they describe the on-policy distribution for episodic tasks in the following box:

I don't understand how this can be done without taking the length of the episode into account. Suppose a task has 10 states, has probability 1 of starting at the first state, then moves to any state uniformly until the episode terminates. If the episode has 100 time steps, then probability of the first state is proportional to $1 + 100\times 1/10$; if it has $1000$ time steps, it will be proportional to $1 + 1000\times 1/10$. However, the formula given would make it proportional to $1 + 1/10$ in both cases. What am I missing?
","['reinforcement-learning', 'sutton-barto', 'episodic-tasks', 'on-policy-distribution']","Let's first assume that there is only one action so that $\pi(a|s) = 1$ for every state - action pair which simplifies the discussion.
Now let's consider a case with 100 time steps, 10 states and uniform distribution for starting state $s_0$ with $h(s_0) = 1$. The result would be
\begin{align}
\eta(s_0) &= 1 + \sum_{i = 0}^9 \eta(s_i) \cdot p(s_0|s_i) =\\
&= 1 + \sum_{i = 0}^9 10 \cdot \frac{1}{10} = 11
\end{align}
Now let's consider a case with 1000 time steps where other settings are the same as in the first case.
\begin{align}
\eta(s_0) &= 1 + \sum_{i = 0}^{9} \eta(s_i) \cdot p(s_0|s_i) =\\
&= 1 + \sum_{i = 0}^{9} 100 \cdot \frac{1}{10} = 101
\end{align}
In the first case
\begin{equation}
\mu(s_0) = \frac{11}{9\cdot 10 + 11} = 0.1089
\end{equation}
and in the second case you have
\begin{equation}
\mu(s_0) = \frac{101}{9\cdot 100 + 101} = 0.1009
\end{equation}
so it looks like you are correct that $\mu(s)$ depends on the length of the episode, but they didn't really say that it doesn't. Obviously as the length of the episode increases so will the number of times a certain state was visited so you could say that formula implicitly depends on the number of time steps. If $h(s_i)$ is equal for every state then results would be the same in both cases regardless of number of time steps. Also, as the number of possible states grows very large, as it usually is in real problems, the results would be approaching each other as the number of states grows."
Which neural network is appropriate for measuring object dimensions from stereo images?,"
I have stereo pairs (left, right) images of concrete cracks. I want to measure the length of the crack from those image pairs. Which neural network is appropriate for measuring object dimensions from stereo images?
Note: I am insisted to use the NN-based technique only.
","['neural-networks', 'reference-request', 'object-detection']",
"Same implementation, but agent is not learning in Retro Pong Environment","
I tried to implement the exact same python coding by Andrej Karpathy to train RL agent to play Pong, except that I migrated the environment from Gym to Retro. 
Everything is the same except the action space in Retro is in indices and not in discrete as in Gym. The index has a size of 8, and index 4 and 5 are actions to move up and down.
But why the little modification has caused the agent not learning at all with running reward at -20 after over 3,000 episodes?
I have checked the frame pre-processing before input to the policy forward neural network and it seems to be normal.
As far as I know, the output from the neural network is the probability of the paddle to move upwards. So I checked it. After few thousands episodes, the probability of the agent to move up just maintained at 0.5.
I know the problem exists between the pre-processing and policy forward neural network, but I just cannot locate the problem. Appreciate if someone could help.
The whole coding is as follow:
import retro
import numpy as np
import _pickle as pickle

H = 200 # number of hidden layer neurons
batch_size = 10 # every how many episodes to do a param update?
learning_rate = 1e-4
gamma = 0.98 # discount factor for reward
decay_rate = 0.98 # decay factor for RMSProp leaky sum of grad^2
resume = False # resume from previous checkpoint?
render = True


# model initialization
D = 80 * 80 # input dimensionality: 80x80 grid
if resume:
  model = pickle.load(open('save.p', 'rb'))
else:
  model = {}
  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # ""Xavier"" initialization
  model['W2'] = np.random.randn(H) / np.sqrt(H)

grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch
rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory

def sigmoid(x): 
  return 1.0 / (1.0 + np.exp(-x)) # sigmoid ""squashing"" function to interval [0,1]

def prepro(I):
  """""" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector """"""
  I = I[35:195] # crop
  I = I[::2,::2,0] # downsample by factor of 2
  I[I == 144] = 0 # erase background (background type 1)
  I[I == 109] = 0 # erase background (background type 2)
  I[I != 0] = 1 # everything else (paddles, ball) just set to 1
  return I.astype(np.float).ravel()

def discount_rewards(r):
  """""" take 1D float array of rewards and compute discounted reward """"""
  discounted_r = np.zeros_like(r)
  running_add = 0
  for t in reversed(range(0, r.size)):
    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)
    running_add = running_add * gamma + r[t]
    discounted_r[t] = running_add
  return discounted_r

def policy_forward(x):
  h = np.dot(model['W1'], x)
  h[h<0] = 0 # ReLU nonlinearity
  logp = np.dot(model['W2'], h)
  p = sigmoid(logp)
  return p, h # return probability of taking action 2, and hidden state

def policy_backward(eph, epdlogp):
  """""" backward pass. (eph is array of intermediate hidden states) """"""
  dW2 = np.dot(eph.T, epdlogp).ravel()
  dh = np.outer(epdlogp, model['W2'])
  dh[eph <= 0] = 0 # backpro prelu
  dW1 = np.dot(dh.T, epx)
  return {'W1':dW1, 'W2':dW2}

env=retro.make(game='Pong-Atari2600',players=1)
observation = env.reset()
prev_x = None # used in computing the difference frame
xs,hs,dlogps,drs = [],[],[],[]
running_reward = None
reward_sum = 0
episode_number = 0

while True:
  if render: env.render()

  action=[0,0,0,0,0,0,0,0] #reset the rl action
  # preprocess the observation, set input to network to be difference image
  cur_x = prepro(observation)
  x = cur_x - prev_x if prev_x is not None else np.zeros(D)
  prev_x = cur_x

  # forward the policy network and sample an action from the returned probability
  aprob, h = policy_forward(x)

  rlaction = 4  if np.random.uniform() < aprob else 5 # roll the dice!
  # record various intermediates (needed later for backprop)
  xs.append(x) # observation
  hs.append(h) # hidden state
  y = 1 if rlaction == 4 else 0 # a ""fake label""
  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)
  action[rlaction]=1
  # step the environment and get new measurements
  observation, reward, done, info = env.step(action)
  reward_sum += reward
  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)


  if done: # an episode finished

    episode_number += 1

    # stack together all inputs, hidden states, action gradients, and rewards for this episode
    epx = np.vstack(xs)
    eph = np.vstack(hs)
    epdlogp = np.vstack(dlogps)
    epr = np.vstack(drs)
    xs,hs,dlogps,drs = [],[],[],[] # reset array memory

    # compute the discounted reward backwards through time
    discounted_epr = discount_rewards(epr)
    # standardize the rewards to be unit normal (helps control the gradient estimator variance)
    discounted_epr -= np.mean(discounted_epr)
    discounted_epr /= np.std(discounted_epr)

    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)
    grad = policy_backward(eph, epdlogp)
    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch

    # perform rmsprop parameter update every batch_size episodes
    if episode_number % batch_size == 0:
      for k,v in model.items():
        g = grad_buffer[k] # gradient
        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2
        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)
        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer

    # boring book-keeping
    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01
    print(('%d , %d , %f ') % (episode_number-1,reward_sum,running_reward))
    if episode_number % 20 == 0: pickle.dump(model, open('save.p', 'wb'))
    reward_sum = 0
    observation = env.reset() # reset env
    prev_x = None

","['neural-networks', 'reinforcement-learning', 'python', 'open-ai']",
Counterexamples to the reward hypothesis,"
On Sutton and Barto's RL book, the reward hypothesis is stated as 

that all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)

Are there examples of tasks where the goals and purposes cannot be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal?
All I can think of are tasks with subjective rewards, like ""writing good music"", but I am not convinced because maybe this is actually definable (perhaps by some super-intelligent alien) and we just aren't smart enough yet. Thus, I'm especially interested in counterexamples that logically or provably fail the hypothesis.
","['reinforcement-learning', 'sutton-barto', 'reward-design', 'reward-functions', 'reward-hypothesis']",
Are there any good ways of simultaneously incorporating object detection with speech recognition?,"
Are there any good ways of simultaneously incorporating object detection with speech recognition? For example, if you want to identify whether an animal is a dog or cat, we can obviously use visual features (e.g. YOLO, CNNs, etc.). But how would you incorporate speech and sound in this model?
","['reference-request', 'object-detection', 'speech-recognition']",
What could I do to this CNN to achieve a higher accuracy on the cifar10 dataset?,"
I have achieved around 85% accuracy using the following architecture:


I used a learning rate of 0.001 and trained the model over 125 epochs with a batch size of 64.
Any suggestions would be much appreciated. Thanks in advance.
","['neural-networks', 'convolutional-neural-networks']",
How to understand the 4 steps of Monte Carlo Tree Search,"
From many blogs and this one https://web.archive.org/web/20160308070346/http://mcts.ai/about/index.html
We know that the process of MCTS algorithm has 4 steps.


Selection: Starting at root nodeÂ R, recursively select optimal child nodes until a leaf nodeÂ LÂ is reached. 


What does leaf node L mean here? I thought it should be a node representing the terminal state of the game, or another word which ends the game. 
If L is not a terminal node (one end state of the game), how do we decide that the selection step stops on node L? From the terms of general algorithm, a leaf node is the one that does not have any


Expansion: IfÂ LÂ is a not a terminal node (i.e. it does not end the game) then create one or more child nodes and select oneÂ C. 


From this description I realise that obviously my previous thought incorrect. 
Then if L is not a terminal node, it implies that L should have children, why not continue finding a child from L at the ""Selection"" step? 
Do we have the children list of L at this step?
From the description of this step itself, when do we create one child node, and when do we need to create more than one child nodes? Based on what rule/policy do we select node C? 


Simulation: Run a simulated playout fromÂ CÂ until a result is achieved.


Because of the confusion of the 1st question, I totally cannot understand why we need to simulate the game. I thought from the selection step, we can reach the terminal node and the game should be ended on node L in this path. We even do not need to do ""Expansion"" because node L is the terminal node.

Backpropagation: Update the current move sequence with the simulation result. Fine.

Last question, from where did you get the answer to these questions?
Thank you
","['algorithm', 'monte-carlo-tree-search']","Imagine a game with a very clear first move, such as a game where choosing to go first if you win a coin toss brings a clear and obvious advantage.In this situation standard MCTS does little exploration down the side of the tree that branches at the win toss > let opponent start step, as the basic simulations of the rest of the game at this split quickly show the large gain you get when always starting when you win the coin toss.As a result, you would end up with a tree with very little expansion on the side of win the toss > put your opponent in, as every simulation step you do from even the most senior nodes ends with much worse expected outcome values than the alternatives on the other side of the tree where you do the correct move of always playing first.These nodes on the side of letting your opponent start have huge potential sub trees (as the whole game would still need to be played out if your opponent started), but would have very little searching down them. As a result, on this side of the tree, you would have many leaf nodes with large (but as yet unexplored, outside of the basic, early simulations down that side) sub trees that you could search if you modified the exploration vs exploration algorithm.As a basic example, take the 0/3 node at the far right on level one of the wiki example below, which would get much less attention than the much more promising 7/10 and 3/8 nodes, despite having potentially many subsequent children it could explore. If you took this node as your L node, you would expand it's children that you have not yet searched, and thus find out more about why this side of the tree is bad and update our now more granular probabilities accordingly, just as it does for the 3/3 node here:"
Semantic issues with predictions made by my trained model,"
I'm new to Deep Learning. I used Keras and trained a inception_resnet_v2 model for my binary classification application (fire detection). As suggested from my previous question of a non-X class, I prepared a dataset of 8000 images of fire, and a larger dataset for non-fire (20,000 random images) to make sure the network also sees images of non-fire to perform classification.
I trained the model, but now when trying to load the model and pass images of fire and non-fire ones, it shows same result for all of them:
[[0. 1.]]
[[0. 1.]]
[[0. 1.]]
[[0. 1.]]
[[0. 1.]]

What is going wrong? Am I doing anything wrong? Should I get the result another way?
===============================================
I know it's not SO, but this is my prediction code in case it matters:
from __future__ import print_function
from keras.models import load_model, model_from_json
import cv2, os, glob
import numpy as np
from keras.preprocessing import image

if __name__ == '__main__':
    model = load_model('Resnet_26_0.79_model_weights.h5')

    os.chdir(""test"")
    for file in glob.glob(""*.jpg""):
        img_path = file
        img = image.load_img(img_path, target_size=(300, 300))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis=0)

        dictionary = {0: 'non-fire', 1: 'fire'}

        results = model.predict(x)
        print(results)
        predicted_class= np.argmax(results)
        acc = 100*results[0][predicted_class]
        print(""Network prediction is: file: ""+ file+"", ""+dictionary[predicted_class]+"", %{:0.2f}"".format(acc))

And here is the training:
from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense, Activation, Flatten, Dropout
from keras.models import Sequential, Model
from keras.optimizers import SGD, Adam
from keras.callbacks import ModelCheckpoint
from keras.metrics import binary_accuracy
import os
import json
#==========================
HEIGHT = 300
WIDTH = 300
TRAIN_DIR = ""data""
BATCH_SIZE = 8 #8
steps_per_epoch = 1000 #1000
NUM_EPOCHS = 50 #50
lr= 0.00001
#==========================
FC_LAYERS = [1024, 1024]
dropout = 0.5

def build_finetune_model(base_model, dropout, fc_layers, num_classes):
    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = Flatten()(x)
    for fc in fc_layers:
        # New FC layer, random init
        x = Dense(fc, activation='relu')(x) 
        x = Dropout(dropout)(x)

    # New layer
    predictions = Dense(num_classes, activation='sigmoid')(x) 
    finetune_model = Model(inputs=base_model.input, outputs=predictions)
    return finetune_model

train_datagen =  ImageDataGenerator(preprocessing_function=preprocess_input, rotation_range=90, horizontal_flip=True, vertical_flip=True
                                    ,validation_split=0.2)
train_generator = train_datagen.flow_from_directory(TRAIN_DIR, target_size=(HEIGHT, WIDTH), batch_size=BATCH_SIZE
                                                    ,subset=""training"")
#split validation manually
validation_generator = train_datagen.flow_from_directory(TRAIN_DIR, target_size=(HEIGHT, WIDTH), batch_size=BATCH_SIZE,subset=""validation"")

base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(HEIGHT, WIDTH, 3))

root=TRAIN_DIR
class_list = [ item for item in os.listdir(root) if os.path.isdir(os.path.join(root, item)) ]
print (""class_list: ""+str(class_list))

finetune_model = build_finetune_model(base_model, dropout=dropout, fc_layers=FC_LAYERS, num_classes=len(class_list))

adam = Adam(lr)
# change to categorical_crossentropy for multiple classes
finetune_model.compile(adam, loss='binary_crossentropy', metrics=['accuracy'])

filepath=""./checkpoints/"" + ""Resnet_{epoch:02d}_{acc:.2f}"" +""_model_weights.h5""
checkpoint = ModelCheckpoint(filepath, monitor=[""val_accuracy""], verbose=1, mode='max', save_weights_only=False)
callbacks_list = [checkpoint]

history = finetune_model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=BATCH_SIZE, 
                                    validation_data=validation_generator, validation_steps = validation_generator.samples, 
                                       steps_per_epoch=steps_per_epoch, 
                                       shuffle=True, callbacks=callbacks_list)

","['deep-learning', 'convolutional-neural-networks', 'classification', 'training', 'keras']",
Are there any online competitions for Reinforcement Learning?,"
Kaggle is limited to only supervised learning problems. There used to be www.rl-competition.org but they've stopped.
Is there anything else I can do other than locally trying out different algorithms for various RL problems? 
","['reinforcement-learning', 'kaggle']",
How could decision tree learning algorithms cope with imbalanced classes?,"
Decision trees and random forests may or not be more suited to solve supervised learning problems with imbalanced labels (or classes) in datasets. For example, see the article Using Random Forest to Learn Imbalanced Data, this Stats SE question and this Medium post. The information across these sources does not seem to be consistent.
How could decision tree learning algorithms cope with imbalanced classes?
","['machine-learning', 'datasets', 'supervised-learning', 'decision-trees', 'random-forests']",
Can OpenAI simulations be used in real world applications?,"
I know that classical control systems have been used to solve the problem of the inverted pendulum - inverted pendulum.
But I've seen that people have also used machine learning techniques to solve this nowadays - machine learning of inverted pendulum.
I came across a video on how to apply a machine learning technique called reinforcement learning on openAI gym - OpenAI gym reinforcement learning.
My question is, can I use this simulation and use it to train a controller for a real-world application of inverted pendulum?
","['machine-learning', 'reinforcement-learning', 'applications', 'open-ai', 'control-theory']","In general, you can use a simulation to prepare and train a controller for a real world application. A good example of this being done for robotics is in the paper Autonomous helicopter flight via reinforcement learning where a Reinforcement Learning agent was trained on a model of helicopter dynamics before being used in reality. Often, as in this case, such work is done to avoid expensive failures due to the trial and error nature of RL - if an error is expensive, such as crashing a helicopter, then ideally the agent performs the checks to avoid it in simulation, by planning or some other virtual environment as opposed to in the real world.The main hurdle to completing training in simulation then transfering to the real world, is the fidelity of the simulation. The simulation's physics, including measurements of physical quantities, the size of time steps, amount of randomness/noise, should match between the simulation and the target real-world environment. If they do not match, then a learning agent could generate a policy that works in simulation, but that fails in reality.For the autonomous helicopter, the researchers used data from a human operator controlling the real helicopter, to help generate a predictive model that was used in the simulation. Can you do the same with Open AI Gym environments? Probably not, unfortunately. The main issue is that the units used are fixed in most environments, and are unlikely to closely relate to any specific real world implementation of the same kind of system. In addition, the physics is often simplified - probably a minor issue for CartPole, but a more major one for environments like LunarLander which ignores weight of fuel used and is a 2D simulation of a 3D environment.So, for instance, in CartPole environments, the following values are fixed:There are a couple of approaches you could use to work around this:Make a new version of the environment and adjust it so that values match to a real environment you want to train for. Note this may still be limited, as the physics model is still quite simple, and may not allow for the real operating characteristics of the cart motor.Use the CartPole environment as-is, not to train a controller directly, but to select hyper parameters, such as neural network size, learning rate etc. That will result in a learning agent that you are reasonably confident can learn policies with the state representation and general behaviour of your target system. You then train ""for real"" again in the physical system.You can combine these ideas, creating a best-guess controller from simulation, then refining it in a real environment by continuing the training on a real system."
Reduce same sample distance in VAE encodings,"
I'm working on a beta VAE model learning a latent representation used as a similarity metric for image registration.
One of the main problems I'm facing is that the encoder + sampler output doesn't fulfill the requirements for a mathematical metric (https://en.wikipedia.org/wiki/Metric_(mathematics)) - is there a known way of how to decrease same-sample distance after encoding + sampling as well as promoting transitivity (triangle inequality) and symmetry?
","['autoencoders', 'variational-autoencoder']",
"What is the ""semantic level""?","
I am reading the paper Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction (2018) by Yunzhe Tao et al.
In this paper, they use several times the expression ""semantic levels"". Some examples:

HRHN can adaptively select the relevant exogenous features in different semantic levels
the temporal information is usually complicated and may occur at different semantic levels
The encoder RHN reads the convolved features $(w_1,w_2,Â·Â·Â·,w_{Tâˆ’1})$ and models their temporal dependencies at different semantic levels
Then an RHN is used to model the temporal dependencies among convolved input features at different semantic levels

What is the semantic level?
","['neural-networks', 'deep-learning', 'terminology', 'papers', 'time-series']","In language theory, there are generally several admitted levels that can be studied in relation with one another or independently. The semantic level is the one dealing with the meaning of the text (""semantic"" comes from the greek and means ""to signify""). The semantic level is therefore generally independent from the syntax and even the language used to convey the message. 
Here is an interesting picture I found on the internet to illustrate my point.
EDIT: I took some time reading the paper. I think ""semantic levels"" refers here to the different neural networks layers used for the exogenous features. Here is a modified version of their figure I've drawned to make it clearer:
In particular, from what I have understood so far, the attention coefficients apply to the whole semantic level (which I find not really clearly indicated on their figure)The LHS of their figure would then be better described by this new one:Hope this helps!"
Plot class activation heatmap of Caffe Model in Python,"
Given the following 3 research papers, the authors have shown different heatmap graphical representations for features of the trained CNN models:

On the performance of Convnet feature for place recognition: link

NetVLAD: CNN architecture for weakly supervised place recognition: link

Deep Learning Features at Scale for Visual Place Recognition: link


Does anyone know the easiest heatmap implementation in Python given deploy.protxt and model_wights_bias.caffemodel files? 
PS: I am aware of the following answers and packages: answer1, package1 but they do not provide these solutions shown in figures above!
Thanks,
","['convolutional-neural-networks', 'python', 'feature-extraction', 'representation-learning']",
What are examples of books or papers on the details of convolutional neural networks?,"
I'm studying a master's degree and my final work is going to be about the convolutional neural network.
I read a lot of books and I did Convolutional Network Standford's course, but I need more.
Are there books or papers on the details of convolutional neural networks (in particular, convolutional layer)? 
","['convolutional-neural-networks', 'reference-request', 'papers', 'books']","Chapter 9 of the book Deep Learning (2016), by Goodfellow et al., describes the convolutional (neural) network (CNN), its main operations (namely, convolution and pooling) and properties (such as parameter sharing).There's also the article From Convolution to Neural Network, which first introduces the mathematical operation convolution and then describes its connection with signal processing (where images can be viewed as 2D signals) and, finally, describes the CNN."
What does end-to-end training mean?,"
In simple words, what does end-to-end training mean, in the context of deep learning?
","['deep-learning', 'training', 'terminology']","Another explanation of deep learning as an end-to-end framework is in deep learning, pre-processing or feature extraction steps are not necessary. So it only uses a single processing step, which is to train the deep learning model. In other traditional machine learning methods, some separated feature extraction steps usually required.For example in image classification, deep learning frameworks like CNN can receive a raw image and then trained to classify it directly. If we didn't use deep learning, we need to extract some features using more steps, like edge detection, corner detection, color histogram, etc.you can also watch Andrew Ng's explanation here"
Is k-fold cross-validation more effective than splitting the dataset into training and test datasets to prevent overfitting?,"
I want to prevent my model from overfitting. I think that k-fold cross-validation (because it is doing this each time with different datasets) may be more effective than splitting the dataset into training and test datasets to prevent overfitting, but a colleague (who has little experience in ML) says that, to prevent overfitting, the 70/30% split performs better than the k-fold cross-validation. In my opinion, k-fold cross-validation provides a reliable method to test the model performance.
Is k-fold cross-validation more effective than splitting the dataset into training and test datasets to prevent overfitting? I am not concerned with computational resources.
","['machine-learning', 'comparison', 'overfitting', 'cross-validation', 'k-fold-cv']","K-fold cross-validation is probably preferred in terms of completeness and generalization: you ensure that the system has seen the complete dataset for training. However, in deep learning this is often not feasible due to time and power constraints. They can both be used, and there is not one better than the other. It really depends on the specific case, the size of the dataset and the time and hardware available. Note that overfitting can be (partially) remedied by things such as dropout.To be fair: it is fine to have a discussion about this with your colleagues, but as so often there is no one correct answer. If you really want proof, you can test it out and compare them. But performance-wise (i.e. the model's predictive power), the difference will be small."
Can Google's patented ML algorithms be used commercially?,"
I just find that Google patents some of the widely used machine learning algorithms. For example:

System and method for addressing overfitting in a neural network (Dropout?)
Processing images using deep neural networks
Methods and apparatus for reinforcement learning (Q-Learning?)

Does that mean I can't use those algorithms commercially?
","['neural-networks', 'q-learning', 'dropout', 'google', 'legal']","Can you use them commercially?Yes. Is Google able to sue you any time they want?Yes. Will they do that...Probably not. Google isn't a known patent bully, I would give them the benefit of the doubt in this kind of situation and say, unless you start really giving them real trouble, they wouldn't do anything. Some companies/people know an idea can really be used for good and patent it to protect its use not to inhibit its use. By patenting the idea and setting a precedent of not suing they are effectively allowing everyone to benefit. Maybe in the future Google cloud, Azure and Amazon web services will lose some money in a legal battle, but I doubt you personally will be hit with a lawsuit."
Can you build a pure CNN phoneme classification model?,"
I was making a simple phoneme classification model for a 10 week-long class project and I ran into a small question.
Is it possible to create a model that takes a 1-second (the longest phoneme is 0.2 second but the large image is kept for context) spectrogram as input? Some people suggest creating an RNN for phoneme classification, but can you build a pure CNN phoneme classification model?
","['machine-learning', 'convolutional-neural-networks', 'ai-design', 'training']","Yes you can, a few years ago I made a simple CNN for a single Arabic phoneme classification. You can use spectogram or using MFCC / MFSC as features, as long all data has the same size (use padding or cropping if needed).You may need RNN if you want to combine some phonemes to recognize a single word or longer."
What could be the problem when a neural network with four hidden layers with the sigmoid activation function is not learning?,"
I have a large set of data points describing mappings of binary vectors to real-valued outputs. I am using TensorFlow, and would like to train a model to predict these relationships. I used four hidden layers with 500 neurons in each layer, and sigmoidal activation functions in each layer.
The network appears to be unable to learn, and has high loss even on the training data. What might cause this to happen? Is there something wrong with the design of my network?
","['neural-networks', 'deep-learning', 'tensorflow', 'activation-functions', 'hidden-layers']","When training our neural network, you need to scale your dataset in order to avoid slowing down the learning or prevent effective learning.
Try normalizing your output.
This Tutorial might help"
"What is a ""surrogate model""?","
In the following paragraph from the book Automated Machine Learning: Methods, Systems, Challenges (by Frank Hutter et al.)

In this section we first give a brief introduction to Bayesian optimization, present alternative surrogate models used in it, describe extensions to conditional and constrained configuration spaces, and then discuss several important applications to hyperparameter optimization.

What is an ""alternative surrogate model""? What exactly does ""alternative"" mean?
","['terminology', 'definitions', 'hyperparameter-optimization', 'bayesian-optimization']","Bayesian optimization (BO) is an optimization technique used to model an unknown (usually continuous) function $f: \mathbb{R}^d \rightarrow Y$, where typically $d \leq 20$, so it can be used to solve regression and classification problems, where you want to find an approximation of $f$. In this sense, BO is similar to the usual approach of training a neural network with gradient descent combined with the back-propagation algorithm, so that to optimize an objective function. However, BO is particularly suited for regression or classification problems where the unknown function $f$ is expensive to evaluate (that is, given the input $\mathbf{x} \in \mathbb{R}^d$, the computation of $f(x) \in Y$ takes a lot of time or, in general, resources). For example, when doing hyper-parameter tuning, we usually need to first train the model with the new hyper-parameters before evaluating the specific configuration of hyper-parameters, but this usually takes a lot of time (hours, days or even months), especially when you are training deep neural networks with big datasets. Moreover, BO does not involve the computation of gradients and it usually assumes that $f$ lacks properties such as concavity or linearity.There are three main concepts in BOThe surrogate model is usually a Gaussian process, which is just a fancy name to denote a collection of random variables such that the joint distribution of those random variables is a multivariate Gaussian probability distribution (hence the name Gaussian process). Therefore, in BO, we often use a Gaussian probability distribution (the surrogate model) to model the possible functions that are consistent with the data. In other words, given that we do not know $f$, rather than finding the usual point estimate (or maximum likelihood estimate), like in the usual case of supervised learning mentioned above, we maintain a Gaussian probability distribution that describes our uncertainty about the unknown $f$.The method of statistical inference is often just an iterative application of the Bayes rule (hence the name Bayesian optimization), where you want to find the posterior, given a prior, a likelihood and the evidence. In BO, you usually place a prior on $f$, which is a multivariate Gaussian distribution, then you use the Bayes rule to find the posterior distribution of $f$ given the data.What is the data in this case? In BO, the data are the outputs of $f$ evaluated at certain points of the domain of $f$. The acquisition function is used to choose these points of the domain of $f$, based on the computed posterior distribution. In other words, based on the current uncertainty about $f$ (the posterior), the acquisition function attempts to cleverly choose points of the domain of $f$, $\mathbf{x} \in \mathbb{R}^d$, which will be used to find an updated posterior.  Why do we need the acquisition function? Why can't we simply evaluate $f$ at random domain points? Given that $f$ is expensive to evaluate, we need a clever way to choose the points where we want to evaluate $f$. More specifically, we want to evaluate $f$ where we are more uncertain about it.There are several acquisition functions, such as expected improvement, knowledge-gradient, entropy search, and predictive entropy search, so there are different ways of choosing the points of the domain of $f$ where we want to evaluate it to update the posterior, each of which deals with the exploration-exploitation dilemma differently.BO can be used for tuning hyper-parameters (also called hyper-parameter optimisation) of machine learning models, such as neural networks, but it has also been used to solve other problems. In the book Automated Machine Learning: Methods, Systems, Challenges (by Frank Hutter et al.) that you are quoting, the authors say that the commonly used surrogate model Gaussian process scales cubically in the number of data points, so sparse Gaussian processes are often used. Moreover, Gaussian processes also scale badly with the number of dimensions. In section 1.3.2.2., the authors describe some alternative surrogate models to the Gaussian processes, for example, alternatives that use neural networks or random forests."
How is the general return-based off-policy equation derived?,"
I'm wondering how is the general return-based off-policy equation in Safe and efficient off-policy reinforcement learning derived
$$\mathcal{R} Q(x, a):=Q(x, a)+\mathbb{E}_{\mu}\left[\sum_{t \geq 0} \gamma^{t}\left(\prod_{s=1}^{t} c_{s}\right)\left(r_{t}+\gamma \mathbb{E}_{\pi} Q\left(x_{t+1}, \cdot\right)-Q\left(x_{t}, a_{t}\right)\right)\right]$$
If it is applied to TD($\lambda$), is this equation the forward view of TD($\lambda$)?
What is the difference between trace $c_s$ and eligibility trace?
","['reinforcement-learning', 'papers', 'temporal-difference-methods', 'eligibility-traces', 'td-lambda']",
"How should I make output layer of my neural network so that I can get outputs ranging from [-20,-1]","
I am trying to make a neural network which takes in 0 and 1 as it's input and should give me output ranging from [-20,-1].I am using three layers with sigmoid as the activation function .How should I design my output layer?Any sort of code snippet from your side will be helpful .I am using tensorflow.Please help me out with the same
","['neural-networks', 'deep-learning', 'tensorflow']",
Why embedding layer is used in the character-level Natural Language Processing models,"
Problem Background
I am working with a problem, which requires a character-level, deep learning model. Previously I was working with word-level deep NLP (Natural Language Processing) models, and in these models almost always embedding encoding was used to represent given word in a lower-dimensional vector form. Furthermore, such embedding encoding allowed for putting similar words near themselves in the new lower-dimensional vector representation (i.e. man and woman vectors were near themselves in the vector space) which improved learning. Nevertheless, I often see that people use embedding encoding in character level NLP models. Even if the character-level one-hot encoding vectors are quite small in comparison to word-level one-hot encoding vectors (about 36 to 32k rows). Furthermore, there is no much correlation between characters, there is no something like ""similar characters"" in comparison to similar words, therefore some characters in comparison to other shouldn't be put near themselves. 
Question
Why embedding encoding is used in the character-level NLP models?
","['natural-language-processing', 'word-embedding']",
Best approach for 2D Grid Image Segmentation,"
I'm working on a project where I need to extract text from grocery discount flyers like the Costco announcement below (retrieved in a random google search, Costco is not the deal here):

If I just run OCR (like with Tesseract in python):
import cv2
import pytesseract
img = cv2.imread('costco.jpg')
text = pytesseract.image_to_string(img)
print(text)

I get:

Cadbury Chocolate
variety pack packet
ere $12.99 i rom hagst 31026 2012
> 
> Je laa
> + a
> 
> Wrigleyâ€™s Excel Gum variety
> 
> Backol 24
> 
> $13.79 fom agus 26.202
OFF
Solon Extra virgin olive oil [...]

Which is a lot noisy.
My guess is that splitting the image to its base squares enchances the recognition.
However, I'm confused on how to do it. I can classify images using a CNN, but am not sure about object recognition.
Should I have a sliding window and train several ""grid box"" objects on a generic CNN and then provide this window data to be classified? How to adapt to distinct object window sizes?
","['convolutional-neural-networks', 'image-segmentation']",
Interpreting Keras Yolov3 config file [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



How does one interpret the ""min_input_size"", ""max_input_size"" and ""anchors"" fields in the Yolov3 config file here. In particular, suppose we have the following:
    ""min_input_size"":       288,

    ""max_input_size"":       448,

    ""anchors"":              [55,69, 75,234, 133,240, 136,129, 142,363, 203,290, 228,184, 285,359, 341,260]

Does the min_input_size and max_input_size indicate the maximum number of training images we can have? What do the numbers in the ""anchors"" field indicate? Are they the coordinates of the anchor boxes? Surprisingly, I have not been able to find a good explanation of many of these  fields within this file.
","['convolutional-neural-networks', 'object-detection']","The min and max input size should be the min and max image size of the input images. The numbers represent pixels in both axis of the image. The anchors represents the size of the anchor box. Anchor box does not have coordinates, only have size. Hope I can help you"
"If an image contains two distinct objects, should I create a copy of this image with distinct labels for each copy?","
Suppose we want to detect whether an object is one of the following classes: $\text{Object}_1, \text{Object}_2, \text{Object}_3$ and $\text{Person}$. Should the annotated images only contain bounding boxes for either a person or an object? In other words, suppose an image has both $\text{Object}_1$ and $\text{Person}$. Should you create a copy of this image where the first version only has a bounding box on the object and the second copy only has a bounding box on the person? 
","['computer-vision', 'datasets', 'object-detection']","You should use both classes together. Let's say you use the method you proposed. Then they will be contradicting each other as one teaches the network to recognize people, not objects and the other teaches the network to recognizes objects not person. There is no need for seperation of the two classes, unless you are making two seperate classifier. Hope I can help you."
How can I merge two datasets? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



I want to merge 2 data sets in one, but don't know the right approach to do it. The datasets are similar, the last column is the same - will or not them buy a product. In the first dataset, users who only will buy, in second - only who won't buy.   
The 1st dataset contains 500 rows and 2nd 10000 rows. What will be the right approach to merge it? How can I normalize them? And to point for an algorithm that the last column is the main sequence on what it should learn? 
Example:
id    income date will_buy

23123 200    10.5 Yes

and second dataset:
id    income date will_buy

2323  100    10.5 No

","['datasets', 'data-preprocessing']","You can use append function:To set the last column as labels, you set them as so by:So, when calling the fit method on the model you build, you set labels = labels."
What is the reason AMD Radeon is not widely used for machine learning and deep learning? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






What is the reason AMD Radeon is not widely used for machine learning and deep learning? Is it mainly an issue of lack of software? Or is Radeon's GPU not as good as NVIDIA's?
",['hardware-evaluation'],
What are the state-of-the-art results on the generalization ability of deep learning methods?,"
I've read a few classic papers on different architectures of deep CNNs used to solve varied image-related problems. I'm aware there's some paradox in how deep networks generalize well despite seemingly overfitting training data. A lot of people in the data science field that I've interacted with agree that there's no explanation on why deep neural networks work as well as they do.
That's gotten me interested in the theoretical basis for why deep nets work so well. Googling tells me it's kind of an open problem, but I'm not sure of the current state of research in answering this question. Notably, there are these two preprints that seem to tackle this question:

Generalization in Deep Learning (2019)
Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness (2019)

If anyone else is interested in and following this research area, could you please explain the current state of research on this open problem? What are the latest works, preprints or publications that attempt to tackle it?
","['deep-learning', 'research', 'reference-request', 'computational-learning-theory', 'generalization']","The paper Generalization in Deep Learning provides a good overview (in section 2) of several results regarding the concept of generalisation in deep learning. I will try to describe one of the results (which is based on concepts from computational or statistical learning theory, so you should expect a technical answer), but I will first introduce and describe the general machine learning problem and I will give a definition of the generalisation gap problem. To keep this answer relatively short, I will assume the reader is familiar with certain basic machine learning and mathematical concepts, such as expected risk minimization, but, nonetheless, I will refer the reader to more detailed explanations of the concepts (at least the first time they are mentioned). If you are familiar with the basic concepts of computational learning theory (e.g. hypotheses), you will be advantaged.In the following description, unless stated otherwise, I do not make any assumption about the nature of the variables. However, I will occasionally provide examples of concrete values for these variables.Let $x \in \mathcal{X}$ be an input and let $y \in \mathcal{Y}$ be a target. Let $\mathcal{L}$ be a loss function (e.g. MSE). Then the expected risk of a function (or hypothesis) $f$ is defined as\begin{align}
R[f] 
&= \mathbb{E}_{x, y \sim \mathbb{P}(X, Y)} \left[ \mathcal{L}(f(x), y) \right] \\
&= \int \mathcal{L}(f(x), y) d\mathbb{P}(X=x, Y=y),
\end{align} where $\mathbb{P}(X, Y)$ is the true joint probability distribution of the inputs and targets. In other words, each $(x, y)$ is drawn from the joint distribution $\mathbb{P}(X, Y)$, which contains or represents all the information required to understand the relationship between the inputs and the targets.Let $A$ be a learning algorithm or learner (e.g. gradient descent), which is the algorithm responsible for choosing a hypothesis $f$ (which can e.g. be represented by a neural network with parameters $\theta$). Let $$S_m = \{(x_i, y_i) \}_{i=1}^m$$ be the training dataset. Let $$f_{A(S_m)} : \mathcal{X} \rightarrow \mathcal{Y}$$ be the hypothesis (or model) chosen by the learning algorithm $A$ using the training dataset $S_m$.The empirical risk can then be defined as$$
R_{S_m}[f] = \frac{1}{m} \sum_{i=1}^m \mathcal{L} (f(x_i), y_i) 
$$where $m$ is the total number of training examples.Let $F$ be the hypothesis space (for example, the space of all neural networks).Let $$
\mathcal{L_F} = \{ g : f \in F , g(x, y) = \mathcal{L}(f(x), y)\}
$$ be a family of loss functions associated with the hypothesis space $F$.In machine learning, the goal can be framed as the minimization of the expected risk\begin{align}
f^*_{A(S_m)} 
&= \operatorname{argmin}_{f_{A(S_m)}} R[f_{A(S_m)}] \\
&= \operatorname{argmin}_{f_{A(S_m)}} \mathbb{E}_{x, y \sim \mathbb{P}(X, Y)} \left[ \mathcal{L}(f_{A(S_m)}(x), y) \right] \tag{1}\label{1}
\end{align}However, the expected risk $R[f_{A(S_m)}]$ is incomputable, because it is defined as an expectation over $x, y \sim \mathbb{P}(X, Y)$ (which  is defined as an integral), but the true joint probability distribution $\mathbb{P}(X, Y)$ is unknown.Therefore, we solve the approximate problem, which is called the empirical risk minimization problem \begin{align}
f^*_{A(S_m)} &= \operatorname{argmin}_{f_{A(S_m)} \in F} R_S[f_{A(S_m)}] \\
&= 
\operatorname{argmin}_{f_{A(S_m)} \in F} \frac{1}{m} \sum_{i=1}^m \mathcal{L} (f_{A(S_m)}(x_i), y_i) 
\end{align}In order to understand the generalization ability of $f_{A(S_m)}$, the hypothesis chosen by the learner $A$ with training dataset $S_m$, we need to understand when the empirical risk minimization problem is a good proxy for the expected risk minimization problem. In other words, we want to study the following problem\begin{align}
R[f_{A(S_m)}] -  R_S[f_{A(S_m)}] \tag{2}\label{2}
\end{align}which can be called the generalization gap problem. So, in generalization theory, one goal is to study the gap between the expected and empirical risks. Clearly, we would like the expected risk to be equal to the empirical risk $$R_S[f_{A(S_m)}] = R[f_{A(S_m)}]$$ because this would allow us to measure the performance of the hypothesis (or model) $f_{A(S_m)}$ with the empirical risk, which can be computed. So, if $R_S[f_{A(S_m)}] = R[f_{A(S_m)}]$, the generalization ability of $f_{A(S_m)}$ roughly corresponds to $R_S[f_{A(S_m)}]$.Therefore, in generalization theory, one goal is to provide bounds for the generalisation gap $R[f_{A(S_m)}] -  R_S[f_{A(S_m)}]$.The hypothesis $f_{A(S_m)}$ is explicitly dependent on the training dataset $S$. How does this dependency affect $f_{A(S_m)}$? Can we avoid this dependency? Several approaches have been proposed to deal with this dependency. In the following sub-section, I will describe one approach to deal with the generalization gap problem, but you can find a description of the stability, robustness and flat minima approaches in Generalization in Deep Learning.In this approach, we try to avoid the dependency of the hypothesis $f_{A(S_m)}$ by considering the worst-case generalization problem in the hypothesis space $F$$$
R[f_{A(S_m)}] -  R_S[f_{A(S_m)}] \leq \sup_{f \in F} \left( R[f] -  R_S[f] \right)
$$
where $\sup_{f \in F} \left( R[f] -  R_S[f] \right)$ is the supremum of a more general generalization gap problem, which is greater or equal to \ref{2}. In other words, we solve a more general problem to decouple the hypothesis (or model) from the training dataset $S$.If you assume the loss function $\mathcal{L}$ to take values in the range $[0, 1]$, then, for any $\delta > 0$, with probability $1 - \delta$ (or more), the following bound holds\begin{align}
\sup_{f \in F} \left( R[f] -  R_S[f] \right) \leq 2 \mathcal{R}_m \left( \mathcal{L}_F \right) + \sqrt{\frac{\log{\frac{1}{\delta}} }{2m}} \tag{3} \label{3}
\end{align}
where $m$ is the size of the training dataset, $\mathcal{R}_m$ is the Rademacher complexity of $\mathcal{L}_F$, which is the family of loss functions for the hypothesis space $F$ (defined above).This theorem is proved in Foundations of machine learning (2nd edition, 2018) by Mehryar Mohri et al.There are other bounds to this bound, but I will not list or describe them here. If you want to know more, have a look at the literature.I will also not attempt to give you an intuitive explanation of this bound (given that I am also not very familiar with the Rademacher complexity). However, we can already understand how a change in $m$ affects the bound. What happens to the bound if $m$ increases (or decreases)?There are several approaches to find bounds for the generalisation gap problem \ref{2}In section 2 of the paper Generalization in Deep Learning, bounds for problem \ref{2} are given based on the stability and robustness approaches.To conclude, the study of the generalization ability of deep learning models is based on computational or statistical learning theory. There are many more results related to this topic. You can find some of them in Generalization in Deep Learning. The studies and results are highly technical, so, if you want to understand something, good knowledge of mathematics, proofs, and computational learning theory is required."
Is it possible to train a CNN to predict the dimensions of primitive objects from point clouds?,"
Is it possible to train a convolutional neural network (CNN) to predict the dimensions of primitive objects such as (spheres, cylinders, cuboids, etc.) from point clouds?
The input to the CNN will be the point cloud of a single object and the output will be the dimensions of the object (for example, radius and height of the cylinder). The training data will be the point cloud of the object with the ground truth dimensions in a regression final layer? 
I think it is possible for images since it will similar to a bounding box detection, but I am not sure with point clouds.
","['machine-learning', 'deep-learning', 'convolutional-neural-networks', 'object-recognition', 'object-detection']",
"What's the difference between RMSE and Euclidean distance, and when to use a custom loss? [closed]","







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            



Closed 3 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I'm searching for a loss function that fits my project. Actually, I have two questions, but they are in the same direction. I take a look at the definition of the root mean squared error (RMSE) and the Euclidean distance and they look the same to me. That's why I want to know the difference between the two. What would be the difference if I use RMSE as a loss function or the euclidean distance?
The second question is how to search for a loss function. I mean I know it depends on the problem and common things are MSE for regression and cross-entropy for classification, but let's say I have a specific problem, how do I search for a loss function? 
I also saw that some people use a custom loss function and most of the deep learning frameworks allow us to define a custom loss function, but why would I want to use a custom one? How do I get the intuition that I need a custom loss function? 
To be more specific, I'm doing a project where I need to reduce the GPS error of a vehicle. I have some vehicle data and my neural network will try to predict the longitude and latitude, so it's a regression problem. That's why I thought that the Euclidean distance would make sense as a loss function, right? Now, somehow MSE also makes sense to me because it is getting the difference between prediction and ground truth. Does this make sense to you as a professional ML engineer or data scientist? And if there would be a custom loss function that you can use, what would you suggest and why?
","['neural-networks', 'deep-learning', 'comparison', 'optimization', 'objective-functions']",
How to represent integer values in sequence to sequence prediction task in encoder-decoder LSTM?,"
I have a large 2D grid having 30k rows and 35k columns, so a total of 30x35k grid cells. Each grid cell is represented by a unique integer number (identity of grid cell). I have several trajectories that passes through these grid cells. Each trajectory is represented by a sequence of numbers (that are grid cells through which the trajectory passes through). 
I want to solve the problem of trajectory prediction by giving the partial trajectory as input and predict the full trajectory. This becomes a sequence to sequence problem, where all sequences are integer values by default.
I am trying to solve this problem through encoder-decoder LSTM architecture. Most tutorials/examples regarding sequence to sequence on net are on machine translations in which vocabularies or characters are one-hot-encoded to represent the text as integer values. When I hot-encode my sequence values the one-hot vector becomes very large because there are (30x35)k grid cells, the program has given memory overflow error (because each vector has of size 1 million). 
I am confused here, do I need to treat grid identity as categorical variable? because all grid identities are numeric numbers but these identities are not comparable (like prices). 
Do I need to hot-encode my integer values in my sequence? Or is there any other alternative to solve this problem? I also appreciate if you suggest me the similar tutorials having the sequence to sequence prediction problem. 
","['deep-learning', 'keras', 'long-short-term-memory']",
Are there any general tips for troubleshooting a VAE when apparently it is not learning?,"
I am trying to train a VAE for anomaly detection. I chose one architecture from this Github repository and I adjusted the input and output to match what I need. In my case, the input (and hence the output) are a 12D vector. I tried several sizes for the latent space, but, for some reason, it's not training. From the beginning, the KL loss in almost zero (around 1e-10), while the reconstruction loss (MSE for Gaussian distribution) is around 1, and they basically vary around these values without learning anything further. 
Are there any general tips for troubleshooting a VAE (I never trained one before)?
I am pretty sure that the code is right and the data for sure has a background and signal (the ratio is 10:1), so I am not really sure what I am doing wrong.
","['generative-model', 'autoencoders', 'implementation', 'variational-autoencoder']",
What is the fastest way to train a CNN with billions of examples?,"
I have a CNN model that I need to train for a large scale genomics application. It is working well with a subset of my training data. I have scaled up to a subset of about 130 million examples and training time is very long, about 3 hours per epoch. I plan to scale up to the hundreds of billions of training examples and I anticipate training time to be not be feasible with my current design. I would appreciate feedback on how I can streamline the training or improve some aspect of my design that I may not be considering. Currently, I am training from a MongoDB. The training examples are not very large. Here is an example.
{
    'added': datetime.datetime(2019, 11, 1, 6, 13, 13, 340000),
    '_id': ObjectId('5dbbccf92464af872756022e'),
    'label': 0,
    'accession': 'GM_0001',
    'data': '34363,30450,9019,19152,8726,22128,59881,17670,15803,64454,64579,28103,52442,64951,29783,64574,652,19243,33498,14775,18803,4700,55446,53912,47645,41465,48257,16305,62071,12334,44698,24371,46515,8445,3000,61849,43228,18120,23587,11105,5453,42707,42739,46122,31285,40773,48162,16653,58783,2928,2836,21330,46947,6719,26992,8852,14520,46212,47362,43554,2147,39372,33885,59716,37384,14825,53387,58763,18065,34070,23278,15641,40237,47950,58811,40015,36880,29841,45351,14904,49660,48224,54638,50358,17202,10701,3564,4829,62655,5684,37207,49724,16369,6769,37827,38144,63885,5070,42882,48960,16178,35758,50554,54253,34556,2383,39431,30176,11482,24459,4472,53825,7764,44500,4869,50875,33037,56353,46848,30769,18729,46026,41409,2826,12092,17086',
    'name': 'Example_1'
}

The relevant data is the 'data' field which is a string of 126 integers where each integer is a value between 0 and about 65,000. The other fields are convenient, but not necessary except for the 'label' field. But even this I could insert into the front of the data field. I mention this because I don't think I necessarily need to train from a MongoDB database. 
I am using Keras 2.3.0 with TensorFlow 2.0.0. Below is an example of my code. The workflow is 1) Load a text file containing the document ids of all training examples in the MongoDB collection. I do this so I can shuffle the examples before sending them to the model for training. 2) I load the examples in batches of 50,000 using my Custom_Generator class. This class pulls the documents from the MongoDB using the list of document ids. 3) The model is trained. I use 5 epochs. I currently have 5-fold cross-validation but I know this is not feasible on the full training set. For that I will do a single train-test split. I am currently performing this on a Google Cloud instance with 2 Tesla T4 GPUs. The database is on a bucket. With the cloud I have flexibility of hardware architectures. I would appreciate any insight. This is a rather large engineering challenge for me. 
Additional background to the problem:
The objective is to classify organisms into broad classes quickly for downstream analysis. The pool of organisms I want to classify is very large (10s of thousands) and very diverse. I'm essentially reading the genomes of the organisms like a book. The genome (a series of ""A"", ""T"", ""C"", or ""G"") is processed in chunks through a hash function producing integer strings as shown above. Depending on the size of the organism genome, thousands to millions of these integer strings may be produced. So I have many thousands of organisms producing many thousands to millions of examples. To be successful, I feel like I need to capture the diversity of the genomes in the organism pool. To give an example, even though Ecoli and Salmonella are both bacteria, their genomes are quite distinct. I feel like I need to have them both represented in the training set to distinguish them from other organisms I would label as a different class. As far as reducing the dataset, I think I can get by with only training on a representative organism for a give species (since there are many unique genomes available for Ecoli, for example). This will help considerably, but I think the training data set will likely still be in the billions of examples. 
import sys
import time
from keras.utils import Sequence, to_categorical, multi_gpu_model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from sklearn.model_selection import KFold
from keras.preprocessing.sequence import pad_sequences
import numpy as np
import random
from pymongo import MongoClient
from bson import ObjectId
from sklearn.metrics import classification_report, confusion_matrix


class Custom_Generator(Sequence) :

    def __init__(self, document_ids, batch_size) :
        self.document_ids = document_ids
        self.batch_size = batch_size


    def __len__(self) :
        return (np.ceil(len(self.document_ids) / float(self.batch_size))).astype(np.int)


    def __getitem__(self, idx) :
        client = MongoClient(port=27017)
        db = client[database]
        document_ids = self.document_ids[idx * self.batch_size : (idx+1) * self.batch_size]
        query_results = db[collection].find({'_id': {'$in': document_ids}})
        batch_x, batch_y = [], []
        for result in query_results:
            kmer_list = result['kmers'].split(',')
            label = result['label']
            x = [x for x in kmer_list if len(x) > 0]
            if len(x) < 1:
                continue
            batch_x.append(x)
            one_hot_y = to_categorical(label, 5)
            batch_y.append(one_hot_y)
        batch_x = pad_sequences(batch_x, maxlen=126, padding='post')
        client.close()
        return np.array(batch_x), np.array(batch_y)


# MongoDB database, collection, and document ids of collection
database = 'db'
collection = 'collection_subset2'
docids_file = 'docids_collection_subset2.txt'
id_ls = []
# Convert docids strings to MongoDB ObjectID
with open(docids_file) as f:
    for line in f:
        id_ls.append(ObjectId(line.strip()))
random.shuffle(id_ls)

# Model
model = Sequential()
model.add(Embedding(65521, 100, input_length=126))
model.add(Conv1D(filters=25, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=30, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(1000, activation='relu'))
model.add(Dense(5, kernel_initializer=""normal"", activation=""softmax""))
metrics=['accuracy'])
parallel_model = multi_gpu_model(model, gpus=2)
parallel_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

seed = 7
batch_size = 50000

# Currently training with 5-fold CV. Will only use single test train split 
# on the full-scale dataset.
kfold = KFold(n_splits=5, shuffle=True, random_state=seed)
kfold_stats = {}
accuracy_ls = []
val_accuracy_ls = []
confusion_ls = []
for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(id_ls)):
    ids_train = np.array(id_ls)[train_idx].tolist()
    ids_test = np.array(id_ls)[test_idx].tolist()
    training_batch_generator = Custom_Generator(ids_train, batch_size)
    validation_batch_generator = Custom_Generator(ids_test, batch_size)
    print('Number of train files: %d' % len(ids_train))
    print('Number of test files: %d' % len(ids_test))
    start = time.time()
    history = parallel_model.fit_generator(
        generator=training_batch_generator,
        steps_per_epoch = int(len(ids_train) // batch_size),
        epochs = 5,
        verbose = 2,
        validation_data = validation_batch_generator,
        validation_steps = int(len(ids_test) // batch_size),
        use_multiprocessing=True
    )
    sys.stderr.write(""time to train model (seconds): %d\n""%(time.time() - start))
    sys.stderr.flush()
    print(history.history)
    fold_name = 'kfold_%s' % str(fold_idx)
    kfold_stats.update({fold_name: history.history})
    accuracy_ls.extend(history.history['accuracy'])
    val_accuracy_ls.extend(history.history['val_accuracy'])
    parallel_model.save('model_output_kfold_%s.h5' % str(fold_idx))
    print(""Kfold %s finished"" % str(fold_idx))
    Y_pred = parallel_model.predict_generator(validation_batch_generator)
    y_pred = np.argmax(Y_pred, axis=1)
    y_true = np.concatenate([np.argmax(batch[1], axis=1) for batch in validation_batch_generator])  
    print('Confusion Matrix')
    conf = confusion_matrix(y_true, y_pred)
    print(conf)
    confusion_ls.append(conf)
    print('Classification Report')
    target_names = ['Class_name_1', 'Class_name_2', 'Class_name_3', 'Class_name_4', 'Class_name_5']
    report = classification_report(y_true, y_pred, target_names=target_names)


","['neural-networks', 'convolutional-neural-networks', 'python', 'tensorflow', 'keras']",
Choosing neural network output for prediction (regression) of a dynamical system,"
Iâ€™m trying to train a neural network to approximate the output of a dynamical system $dy/dt=f\left(y(t), u(t) \right)$, namely, given $y(0)$ and $u(t_i), i=1,2...N$ I want the network to predict $y(t_i), i=1,2...N$. So far Iâ€™ve thought of several approaches, namely 

Predict the derivative $dy/dt (t_{i+1}) = f_1 \left(y(t_i), u(t_i) \right)$ and then compute $y(t_{i+1}) = dy/dt (t_{i+1}) \cdot dt + y(t_{i})$
Predict the increment $\Delta y (t_{i+1})= f_2 \left(y(t_i), u(t_i), \Delta t \right)$ and then compute $y(t_{i+1}) = \Delta y (t_{i+1}) + y(t_{i})$
Directly predict the next value $y(t_{i+1}) = f_3 \left(y(t_i), u(t_i), \Delta t \right)$

Which option is recommended?
","['neural-networks', 'machine-learning', 'prediction', 'regression', 'time-series']",
"What is ""conditioning"" on a feature?","
On page 98 of Jet Substructure at the Large Hadron Collider: A Review of Recent Advances in Theory and Machine Learning the author writes;

Redacted phase space: Studying the distribution of inputs and the
  network performance after conditioning on standard physically-inspired
  features can help to visualize what new information the network is
  using from the jet. Training the network on inputs that have been
  conditioned on specific values of known features can also be useful
  for this purpose.

I cannot find other references to conditioning features. What does that mean?
","['terminology', 'math', 'features', 'data-preprocessing', 'conditional-probability']","This is conditioning in the sense of conditional probability. The idea is that the authors have some ""standard physically-inspired features"". They are splitting the data up into bins based on the values of these features, and then training a model for each bin. They are then examining the differences between the models. Usually this is done to learn something about the benefits of using the different features, and about the relationships between features and outputs."
Is the Mask Needed for Masked Self-Attention During Inference with GPT-2,"
My understanding is that masked self-attention is necessary during training of GPT-2, as otherwise it would be able to directly see the correct next output at each iteration. My question is whether the attention mask is necessary, or even possible, during inference. As GPT-2 will only be producing one token at a time, it doesn't make sense to mask out future tokens that haven't been inferred yet.
","['natural-language-processing', 'attention', 'transformer', 'gpt', 'inference']",
Imposing physical constraints (previous knowledge) in a neural network for regression,"
I'm trying to train a neural network to do a multiple non-linear regression $y=f(x_i), i=1,2â€¦N$. So far it works good (low MSE), but some predictions $y$ are â€œnon-physicalâ€, for instance for our application it is known  from first principles that when $x_2$ increases, then $y$ also has to increase ($dy/dx_2>0$), but in some instances the neural networkâ€™s output doesnâ€™t comply with this constraint. Another example is that $y + x_5 + x_7$ should be less than a constant $K$
I thought about adding a penalty term to the loss function to enforce these constraints, but I am wondering if there is a ""harder"" way to impose such a constraint (that is, to ensure that these constraints will always hold, no only that non-physical predictions will be penalized)
","['neural-networks', 'machine-learning', 'regression']",
What needs to be done to make a fair algorithm?,"
What needs to be done to make a fair algorithm (supervised and unsupervised)?
In this context, there is no consensus on the definition of fairness, so you can use the definition you find most appropriate.
","['unsupervised-learning', 'social', 'supervised-learning', 'explainable-ai', 'algorithmic-bias']",
How to measure object size from the disparity map using CNN?,"
I am a student learning about image processing using CNN. I want to learn how to measure the object size from the disparity map obtained from left and right stereo images.
","['convolutional-neural-networks', 'computer-vision', 'object-detection', 'image-processing']",
How to measure the size of an crack which is segmented from an image using Mask-RCNN?,"
I am a masters student going to work in a project to analyze the cracks in underwater concrete structures.
I need some suggestions for data acquisition and length measurement of the crack.
I have decided to do crack segmentation using Mask-RCNN. But I don't know which methodology is best to measure the length of the cracks. While searching about this, I found many ways to measure the crack size when there is another reference object of known size in the image. But in my case, there won't be any reference object and also it is not possible to know the distance between the camera and target since it is underwater.
If the images are of stereotype, Will that solve this issue?
Can anyone help?
","['convolutional-neural-networks', 'object-detection', 'image-segmentation']",
Why would the application of boosting prevent underfitting?,"
""Why would the application of boosting prevent underfitting?""
I read in some paper that applying boosting would prevent you from underfitting. Why is that?
Source:
https://www.cs.cornell.edu/courses/cs4780/2015fa/web/lecturenotes/lecturenote13.html
","['machine-learning', 'underfitting', 'boosting']",
What evaluation metric are used for sequence-to-sequence prediction problems?,"
I am solving many sequence-to-sequence prediction problems using RNN/LSTM. 
What type of evaluation metrics can be used for sequence prediction problems?
One metric is the mean squared error (MSE) that we can give as a parameter during the training model. Currently, the accuracy of my sequence-to-sequence problems is very low. 
What are other ways through which we can compare the performance of our models?
","['recurrent-neural-networks', 'long-short-term-memory', 'sequence-modeling', 'metric']",
How can I assign agents to tasks based on time and affinity?,"
I am working on an assignment problem. 
Consider $K$ agents $A_1, \dots A_K$ and $N$ tasks $T_1, \dots T_N$. Each task has a certain time $t(T_i)$ to be completed and each agent has a matching (or affinity) value associated with each task $M_{A_j}(T_i), \forall i, j$. The goal is to assign agents to tasks, such that the matching value is maximized and the overall time to complete the tasks is minimized. Moreover, an agent can be assigned to multiple tasks. However, an agent cannot start a new task before finishing the previous one.I want to solve it with GA+MOA*algorithm What would be an admissible heuristic function?
","['ai-design', 'optimization', 'a-star', 'constraint-satisfaction-problems']",
How to calculate the size of a 3d object from an image?,"
I am wondering how to calculate the size of a 3d object in an image without knowing the focal length of the camera but the distance from the camera to the object. 
","['machine-learning', 'computer-vision', 'image-processing']","There are hundred of papers on this task some older than I am! Normally this is done by trying to form a box shape around the image than estimate the volume. This task is typically done with multiple images so the two can generate a more clear picture of the size of the object than one image alone. An object could be 'infinitely' large but its mass could be behind the surface you can see with the picture. With the height and length dimensions extracted from the image and with the distance from the camera calculating the size of said surface is fairly easy. Stack overflow about problem Frustum PointNets for 3D Object Detection From RGB-D Data (2018) By Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J. GuibasFrom contours to 3D object detection and pose estimation (2017) By Nadia Payet Sinisa Todorovic"
Training methods for bipedal robot [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I am looking to train a bipedal robot using unity as a scape with a genetic algorithm. I will import the CAD into unity so the hardware is exact. My questions:

Is Unity physics accurate enough to train a neural network that will perform in the real world?
Should I optimize the network using reinforcement learning in the real world (after trained in scape)?
I am looking to use air muscles for my build. If the physics arenâ€™t exactly right in unity (elasticity, max length, torque) will the bot still perform in the real world?
Are there any other programs that would be better than unity to train a robot inside a scape?
Any other approaches or new ideas on how to train the bot more efficiently would be greatly appreciated.

","['reinforcement-learning', 'genetic-algorithms']","Not really. Unity physics is just an approximation of an approximation. It has to look more or less real but at the same time the performances are very important, so it has not the realistic level you would hope to ""bring things to the real world"". There are some physics engine you can install that usually work a bit better. Still, don't expect ""real-world level"".Based on 1, yes. If, once you got it to the real world you still give it the chance to optimize, it can only make things better.This is difficult to say. You should try to use the best possible approximation in Unity, maybe testing multiple scenarios and seeing that they perform more or less in the same way. Then train. Then bring this ""approximated model"" to the real world and let it train more to adjust to the real physics.I don't know this. Unity seems the most versatile around (together with other game engines). This versatility is its strength and weakness, as it cannot focus on solving perfectly a single problem. It rather has to aim at generalizing as much as possible. There might be ""more specialized"" programs around, but I doubt.Just try different algorithms/methods. Usually with complex problems neuroevolutionary techniques perform better, but the amount of code is hugely higher. "
What are causative and exploratory attacks in Adversarial Machine Learning?,"
I've been researching Adversarial Machine Learning and I know that causative attacks are when an attacker manipulates training data. An exploratory attack is when the attacker wants to find out about the machine learning model. However, there is not a lot of information on how an attacker can manipulate only training data and not the test data set. 
I have read about scenarios where the attacker performs an exploratory attack to find out about the ML model and then perform malicious input in order to tamper with the training data so that the model gives the wrong output. However shouldn't such input manipulation affect both the test and training data set? How does such tampering only affect the training data set and not the test data set?
","['neural-networks', 'machine-learning', 'deep-learning', 'classification', 'adversarial-ml']","When someone is able to do a causative attack it means there is a mechanism by which they are able to input data into the network. Maybe a website where people can input their images and it outputs a guess on what is in the picture and then you click if it got it right or not. If you continue to input images and lie to it it will obviously get worse and worse if they use the user input to add to the test set. Most people are careful and don't mess around with mixing new data into the testing sample. If they did something like mixed the user input training and test then resampled something like that could occur but most people don't do that. It's bad practice and even worse than leaving your NN open to tampering from malicious user input. Information isn't really added to the knowledge in the model till it is fed into the model and backpropagation occurs.An exploratory attack is sending tons of inquiries to the model to gain information about the data set they have built into the model even to the point of extracting data about individuals pieces of data that are built into the model. Then, with this information, they could try to reconstruct the data set. They could attempt to trick the network by sending strange generated inputs. In the paper Adversarial Machine Learning (2011), by Ling Huang et al., in section 2, the authors define these terms, under the category influence.Causative - Causative attacks alter the training process through influence over the training data.Exploratory - Exploratory attacks do not alter the training process but use other techniques, such as probing the detector, to discover information
  about it or its training data.They also provide other related definitions.Integrity - Integrity attacks result in intrusion points
  being classified as normal (false negatives). Availability - Availability attacks cause so many classification errors, both false negatives and false positives, that the system becomes effectively unusable.Privacy - In a privacy violation, the adversary obtains information from the learner, compromising the secrecy or privacy of the systemâ€™s users. Targeted - In a targeted attack, the focus is on a single or small set of target points. Indiscriminate - An indiscriminate adversary has a more flexible goal that involves a very general class of points, such as â€œany false negative.â€"
Context-based gap-fill face posture-mapper GAN,"

These images are handmade, not auto-generated like they will be in production. Apologies for inaccuracies in the graph overlay.
I am trying to build an AI like that displayed in the diagram: when given a training set of images with their corresponding node maps of face/nose posture, and an image with a missing section (just a gap) with a node map, I would like it to reconstruct the initial image. My thoughts immediately went to GANs for this, but after some searching, the closest I could find were:

Face recreation without context/not filling gaps, just following pose (DeepFake)
Filling gaps in images, but with no node reference
Filling gaps from reference drawings/mappings, but with no way to provide sample images

I would like to hear about any implementations of such an algorithm, if possible optimised for faces, and if none exists, I would like to hear of how I would go about altering the generator of the GAN to work with the context/gap-fill bit (e.g a paper which talks about this idea, but doesn't implement it). Any guidance on the NN that is best for this type of task is also appreciated.
","['ai-design', 'generative-adversarial-networks', 'generative-model', 'implementation', 'image-generation']",
Pose estimation using CNNs on Point clouds,"
In the case of single shot detection of point clouds, that is the point cloud of an object is taken only from one camera view without any registration. Can a Convolutional Network estimate the 6d pose of objects (initially primitive 3D objects -- cylinders, spheres, cuboids)?
The dataset will be generated by simulating a depth sensor using a physics engine (ex:gazebo) and primitive 3D objects are spawned with known 6d pose as ground truth. The resulting training data will be the single viewed point cloud of the object with the ground truth label (6d pose)?
","['deep-learning', 'convolutional-neural-networks', 'object-recognition', 'object-detection', 'regression']","The answer is yes this is possible and here are the papers where they do almost exactly the same project you are describing above. Although none of the bellow combine gazebo, single point/single shot, 6D-pose and CNNs. In order to use synthetic data to train a model that works on real data.Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics (2017) by Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, Ken GoldbergReal-Time Seamless Single Shot 6D Object Pose Prediction (2018) by Bugra Tekin, Sudipta N. Sinha, Pascal Fua
This paper sounds like exactly what you want to do.SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again (2017) by Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, Nassir Navab 
This paper is also referenced by them.The model will be able to be trained but how effect a model trained on the synthetic data will be able to properly function on real data will be the challenge. "
"Do algorithms like OpenAI's ""think up strategies""?","
I was discussing with a friend whether current AI does anything remotely similar to 'thinking' and he argued that AIs that play games must think up strategies. 
While thinking may not be precisely defined, my understanding of algorithms like OpenAI was that they just minimize a very non-convex objective, but still play the game based on examples, and not by coming up with intentional strategies. Is my understanding incorrect?
","['deep-learning', 'philosophy', 'game-ai', 'agi', 'open-ai']",
Isn't deep fake detection bound to fail?,"
Deep fakes are a growing concern: the ability to credibly alter a video may have great (negative) impacts on our society. It is so much of a concern, that the biggest tech companies launched a specific challenge: https://deepfakedetectionchallenge.ai/.
However, from what I understand, most deep fake generation techniques rely on the use of adversarial models. One model generates a new image, while another model tries to detect if the image is doctored or not. Both models ""learn"" from being confronted with the other. 
That being said, if a good deep fake detection model emerges (from the previous challenge, or not), wouldn't it be rendered useless almost instantly by learning from it in an adversarial setting?
","['generative-adversarial-networks', 'adversarial-ml', 'deepfakes']","Not necessarily it depends on the function of the problem space for both the GANs.A real world example: a batter's reaction time and a pitchers max speed are actual bounded values based on genetics and physics. If the max speed a pitcher can pitch is greater than the max reaction time a human needs to effectively hit against them they will permanently be a better pitcher because the threshold of reaction time.We don't yet know if a maximum threshold on realistic fake image generation exists or if a threshold on detection exists.As both reach near perfect accuracy it could be that the amount of nodes needed to detect a nearly perfect generated image from real image is more neurons than atoms in the universe, or conversely the amount of nodes needed to generated a nearly perfect image could reach impossible proportion we won't know until we continue to make better and better networks that close in on the boundary of generation and detection of real vs fake images from a neural network.Edit:
Let's imagine this problem. One adversary edits an image with a colored line of pixels the goal hide the line by editing the image, the student is responsible with finding the line after the adversary changes the image. The problem can become infinitely difficult change all pixels to be the color of the line. The line is impossible to find the adversary always wins, if it finds this solution meaning it is in its reachable problem space based on its hardware capabilities and its learning model. Which we should assume it would because it's such a simple task.Deep fake detection is not bound to fail because the limit on the effectiveness of a generative model may have a steeper limit than a discriminator at near optimal performance. I have not seen any paper about this specifically and in fact I believe the discriminator has a more difficult job in most cases I just disagree with deciding at this moment that the detectors are doomed. The combination of creating real images motion and sound perfectly in sink is not a trivial problem, in some scenarios it is basically impossible."
How do I combine models trained on different data to increase classification accuracy?,"
I have two trained models. One is using a LinearSVC algorithm and is trained on numerical data from medical examination from patients with diabetic retinopathy. The second one is a neural network trained on images of retina scans from patients with the same disease.
The models predict if the patient has retinopathy or not. Both are written using Python 3.6 and Keras and have accuracy around 0.84.
Is it possible to combine those two models in any way to increase the accuracy of predictions?
I'm not sure in what way it could be achievable as they are using a different kinds of data. I have tried using ensembling methods but didn't get better scores with them.
","['neural-networks', 'python', 'image-recognition', 'keras', 'support-vector-machine']","You can try using a multi-input model. Here is a recent post with a similar discussion, with the required architecture defined in the answer.Instead of combining the separate models, you can create a model which uses image and numerical data side by side. Keras allows you to use different types of data using multi input structure via functional API. And then you can combine them to create a single machine learning model. Basic idea is like this:This image was taken from here, where you can find further details and the code implementation as well. Actually, the discussion in the introduction of that post is almost the same as your question: What to do when the inputs are:Also, section 5.1 of this blogpost details the same process."
"What do the subscripts mean in $N_{t,n,\sigma,L}$?","
A neural network can apparently be denoted as $N_{t,n,\sigma,L}$. What do these subscripts  $t, n, \sigma$ and $L$ mean? Could you link me to a paper, article or webpage with an explanation for this? 
","['neural-networks', 'math', 'definitions', 'notation']","Here is a paper with the mathematical definition of each term:Let Nt,n,Ïƒ,L be all target functions that can be implemented using a
  neural network of depth t, size n, activation function Ïƒ, and when we
  restrict the input weights of each neuron to be |w|1 + |b| â‰¤ L."
"In adversarial machine learning, how does an attacker have access to the test and training dataset in order to poison it?","
In the field of adversarial machine learning, machine learning models are vulnerable to attacks both on the test and training data set. However, how does the attacker get access to these datasets? How do these datasets get manipulated/tampered with?
","['machine-learning', 'reinforcement-learning', 'training', 'datasets', 'adversarial-ml']","We can manipulate a model's test data set if the machine learning model takes user input and uses it to resample test data set. The actual training dataset of the ML model does not get manipulated, but if we figure out the ML model through an exploratory attack (sending a lot of inquiries to the ML model to find out its nature), we can generate a training dataset which was built into the original ML model."
Is there anything theoretically revolutionary about Deep Neural Networks?,"
In recent years, we have seen quite a lot of impressive display of Deep Neural Network (DNN), as demonstrated most famously by AlphaGo and its cousin programs.
But if I understand correctly, deep neural network is just a normal neural network with a lot of layers. We know about the principles of the neural network since the 1970s (?), and a deep neural network is just the generalization of a one-layer neural network to many.
From here, it doesn't seem like the recent explosion of DNN has anything to do with a theoretical breakthrough, such as some new revolutionary learning algorithms or particular topologies that have been theoretically proven effective. It seems like DNN successes can be entirely (or mostly) attributed to better hardware and more data, and not to any new theoretical insights or better algorithms.
I would go even as far as saying that there are no new theoretical insights/algorithms that contribute significantly to the DNN's recent successes; that the most important (if not all) theoretical underpinnings of DNNs were done in the 1970s or prior.
Am I right on this? How much weight (if any) do theoretical advancements have in contributing to the recent successes of DNNs?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'computational-learning-theory']",
Which deep neural networks are appropriate for the detection of bombs?,"
This is a follow-up question from my previous post here about explosion detection. I gathered a dataset of explosions. As I'm new to Deep Learning in Keras, I'm trying to see what architecture best suits this problem, given that here we have a cloud of smoke/fire as opposed to an object. Any suggestions?
For instance, I've learned about Faster RCNN or RetinaNet, but that is mostly for object detection. Is it going to be better than say a basic ResNet50? And here real-time prediction requirements are not an issue. So shall I assume a heavier model (e.g. NASNet Large or a Resnet-152 model) is better than a basic ResNet-50 model?
","['machine-learning', 'deep-learning', 'keras', 'deep-neural-networks', 'models']",
Why does a Lipschitz continuous discriminator in GANs assure statistical boundedness?,"
I have been reading the paper which introduced spectral normalization in GANs.
At some point the paper mentions the following:

The machine learning community has been pointing out recently that the
  function space from which the discriminators are selected crucially
  affects the performance of GANs. A number of works (Uehara et al.,
  2016; Qi, 2017; Gulrajani et al., 2017) advocate the importance of
  Lipschitz continuity in assuring the boundedness of statistics.

What does it mean that the Lipschitz continuity assures the boundedness of statistics and why does that happen?
","['machine-learning', 'terminology', 'math', 'generative-adversarial-networks']","To put it simply GANs suffer from a problem of uneven learning rate. Imagine the learning rate of a pitcher and hitter if the pitcher gets to a point where they can throw much better than the hitter can hit then the hitter may fall into a 'training pit' as to be unable to ever learn how to hit from the pitcher. This follows a continues relationship in between the two learning rates where if the pitcher is becoming a much better pitcher at a faster rate they could become too good and make learning impossible for the hitter. So the rate must be 'slowed down' as to ensure the pitcher doesn't ruin the hitter.If the cone of the Lipschitz continuity function of either function is outrunning/outpacing the other than the learning for the one who is in front must be slowed down so the other catches up. Two runners trying to push each other athletically is another example. If one outpaces the other an injury may occur in the one lagging behind while trying to keep pace this happens, when the adversarial network becomes too good at generating training material that the behind network is not ready to learn with.The GAN will do best when the learning rates are adjusted to slow down the fast learner artificially. The statistics will not be bounded correctly if the learning rates are not kept in check similarly to how step size needs to be right to find local minimum and maximum. If the learning is not artificial augmented so both keep relatively same pace getting stuck at local minimum and maximum of the solution space will occur."
Two Models vs One Model for Person Detection and Object Detection,"
Is it possible to do person detection and object detection within one model? The training data would be images annotated with bounding boxes for objects and people.  Because normally object detection and person detection are done separately? Is there any research about models that simultaneously detect both people and objects?
",['object-detection'],
Keyword spotting with custom keywords and why not use speech recognition instead,"
My question regards performing keyword spotting for custom keywords and justifying the use of keyword spotting models instead of speech recognition.
I have been doing some searching around Keyword Spotting and I realized there is not so much work out there. Probably the most common dataset I have found people using is the Speech Commands Dataset. However, this dataset has only 30 keywords.
If I want keyword spotting for my own custom application, then to the best of my knowledge I need either a pre-trained model or my own data to train a model on. However, to the best of my knowledge, there is no model pre-trained on a dataset with a large enough set of keywords that is likely to cover a lot of applications. Correct me if I am wrong in this.
I have come to the conclusion that I need to train my own model and the only two ways I could train models on custom keywords is to get that data myself, either by crowdsourcing or by performing speech recognition on large datasets, picking up segments which include the words of interest and then doing some manual work to check if these segments truly include the keywords I want. Does someone think that this would be a good or bad idea and why?
Lastly, why would I even bother going the keyword detection route and not just use a speech recognition model that will recognize the words a human speaks and see if any of them match my keyword? Is the performance that much better with keyword detection?
","['machine-learning', 'speech-recognition']",
AI natural voice generator,"
I want to create a solution, which clones my voice. I tried my commercial solutions or implementation of Tacotron. Unfortunately, results not sound natural, generated voice sounds like a robot. Anybody could recommend good alternative?
","['deep-learning', 'voice-recognition']","The reason for robot like speech may be because tacotron uses griffin lim for vocoder, which cannot reproduce sound with perfection, often introducing robot like sound artifects. A vocoder is a network that transforms a transform a spectrogram image back to speech waveform. Tacotron and many other speech generation neural network uses CNN to generate spectrogram instead of raw waveforms as output. Spectrogram is a lossy representation of raw audio waveform, so a perfect reconstruction of audio waveform is not possible. Griffin-Lim is a vocoder that uses algorithmic way to transform spectrogram to audio waveform, but often introduces a robot-like quality to generated waveforms. A neural network based vocoder can solve the problem. The wavenet vocoder is often used in speech generation as it can transform the spectrogram to audio with little artifects. Many new speech generation models use the wavenet vocoder as the deafult vocoder of the generation model. For a public implementation, this is a good github repository: https://github.com/r9y9/wavenet_vocoderYou can also use the newer tacotron 2 which uses the wavenet vocoder as the default vocoder. You can check it out here: https://github.com/Rayhane-mamah/Tacotron-2"
Purpose of using actor-critic algorithms under deterministic MDP dynamics?,"
One of the main disadvantages of the MC Policy Gradient algorithm (REINFORCE) as described say here is the fact that it has high variance (returns, which we sample, will significantly vary from episode to episode). Therefore it is perfectly reasonable to use a critic to reduce variance and this is what for example Deep Deterministic Policy Gradient (DDPG) does.
Now, let's assume that we're given an MDP with completely deterministic dynamics. In that case, if we start from a specific state and follow a certain deterministic policy we will always obtain the exact same return (therefore we have zero bias and zero variance). If we follow a certain stochastic policy the return will vary depending on how much we explore, but under an almost-deterministic policy our variance will be quite small. In any case, there's no contribution to the variance from the deterministic MDP dynamics.
In deep reinforcement learning for portfolio optimization, many researchers (Xiong at al. for example) use historical market data for model training. The resulting MDP dynamics is of course completely deterministic (if historical prices are used as states) and there's no real sequentiality involved. Consequently, all return variance stems from the stochasticity of the policy itself. However, most researchers still use DDPG as a variance reduction mechanism. 
What's the point of using DDPG for variance reduction when the underlying MDP used for training has deterministic dynamics? Why not simply use the Reinforce algorithm?
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'reinforce', 'ddpg']",
How to properly use batch normalization during inference,"
I am trying to manually implement calculations of the image classification process using pre-trained weights from the MobilenetV2 network. I know how to apply filter weights to the channels, but not sure what to do with the coefficients from the batch normalization (BN) layers. The model uses BN after each convolution before ReLu6. As explained in many sources, BN has a lot of benefits during model training. The original Mobilenetv2 paper does say that they used BN during training, but nothing about using it during testing. The pre-trained MobilenetV2 model comes with BN layers which contain weights 4 x n_channels (I assume gamma, beta, mean, and std for each input featuremap in the BN layer). The following questions is:

How do I apply the four coefficients to a featuremap during inference? (This article explains it, but I still don't get it - aren't those imported coefficients already pre-calculated, so the operation on a featuremap is reduced to a multiply-add?)

The original paper on BN in section 3.1 says:

... Since the means and variances are fixed during inference,
  the normalization is simply a linear transform applied to
  each activation. It may further be composed with the scaling by Î³ and shift by Î², to yield a single linear transform
  that replaces BN(x)...

Does this mean that during inference I would use only gamma and beta coefficients to ""scale and shift"" each pixel of a corresponding feature map? That is, something like:
for ch
  for row
    for col
      out_feature[row][col][ch] = in_feature[row][col][ch] * BN[gamma][ch] + BN[beta][ch]

Could anyone, please, confirm and explain if this is correct and re-iterate what exactly is expected from the BN layer output in terms of value ranges (before ReLu6)?
","['convolutional-neural-networks', 'batch-normalization']",
"Deepfakes as ""force for good""?","
As per the law of unintended consequences, could it be that deepfakes will eventually have the opposite effect to what people currently seem to fear most. For example, once it is clear that anyone can be deepfaked to unlimited degrees of precision, wouldn't we have a situation where in regards to

pornography, revenge-porn: no one (including the person being viewed) will actually care anymore. E.g. if a movie star's account gets hacked and nude pictures are released to the public, it becomes a non-story because everone simply assumes it's one of thousands of other deepfakes that already exist.
fake-news, government propaganda: the general public will demand multiple sources, witnesses before believing the next crazy story. That, I assume, is a good thing as well. 

","['neural-networks', 'social', 'deepfakes']",
"What does AI software look like, and how is it different from other software?","
What does AI software look like? What is the major difference between AI software and other software?
","['comparison', 'implementation']","Code in AI is not in principle different from any other computer code. After all, you encode algorithms in a way that computers can process them. Having said that, there are a few points where your typical ""AI Code"" might be different:A lot of (especially early) AI code was more research based and exploratory, so certain programming languages were favoured that were not mainstream for, say, business applications. For example, much work in early AI has been coded in Lisp, and probably not much in Fortran or Cobol, which were more suited to engineering or business. Special languages were developed to make it easy to program with symbols and logic (eg Prolog).The emphasis was more on algorithms than clever/complex programming. If you look at the source code for ELIZA (there are multiple implementations in many different languages), it's really very simple.Before the advent of neural networks and (statistical) machine learning, most AI programming was symbolic, so there hasn't been much emphasis on numerical computing. This changed as probabilities and fuzziness were increasingly used, but even if using general purpose languages there would be fewer numerical calculations.Self-modifying code is inherently complex; while eg Lisp made no difference between code and data (at least not in the same way as eg C or Pascal), this would just complicate development without much gain. Perhaps in the early days this was necessary when computers had precious little memory and power and you had to work around those constraints. But these days I don't think anybody would use such techniques anymore.As modern programming languages evolved, Lisp and Prolog (which were the dominant AI languages until probably 20 to 30 years ago) have been slowly replaced by eg Python; probably because it is easier to find programmers comfortable in an imperative paradigm rather than a functional one. In general, interpreted languages would be preferred over compiled ones due to speed of development, unless performance is important.The move to deep learning has of course shifted this a lot. Now the core processing is all numeric, so you would want languages that are better with calculations than symbol handling. Interpreted languages would now mainly make up the 'glue' code to interface between compiled modules, and be used for data pre-processing. So current AI code is probably not really that different from code used in scientific computing these days.There is of course still a difference between R&D and production code. You might explore a subject using an interpreted language, and then re-code your algorithm for production in a compiled language to gain better performance. This depends on how established the area is; there will for example be ready-made libraries available for neural networks or genetic algorithms which are well-established algorithms (where performance matters).In conclusion: I don't think AI code is any more complex than any other code. Of course, that's not very exciting to portray in a film, so artistic licence is used to make it more interesting. I guess self-modifying code also enables the machines to develop their own conscience and take over the world, which is even more gripping as a story element. However, given that a lot of behaviour is nowadays in the (training/model/configuration) data rather than the algorithm, this might even be more straight forward to modify.Note: this is a fairly simplified summary based on my own experience of working in AI; other people's views might vary, without either being 'wrong'.Update 2021: I now work at a company that extracts business information/events from news data on a large scale using NLP methods. And we're using Lisp... so it's still in active, commercial use in AI."
Wouldn't convolutional neural network models work better without flattening the input in any stages?,"

The above model is what really helped me understand the implementation of convolutional neural networks, so based on that, I've got a tricky hypothesis that I want to find more about, since actually testing it would involve developing an entirely new training model if the concept hasn't already been tried elsewhere.
I've been building a machine learning project for image recognition and thought about how at certain stages we flatten the input after convoluting and max pooling, but it occurred to me that by flattening the data, we're fundamentally losing positional information. If you think about how real neurons process information based on clusters, it seems obvious that proximity of the biological neurons is of great significance rather than thinking of them as flat layers, by designing a neural network training model that takes neuron proximity into account in deciding the structure by which to form connections between neurons, so that positional information can be utilized and kept relevant, it seems that it would improve network effectiveness.
Edit, for clarification, I made an image representing the concept I'm asking about:

Basically: Pixels 1 and 4 are related to each other and that's very important information. Yes we can train our neural network to know those relationships, but that's 12 unique relationships in just a 3x3 pixel grid that our training process needs to successfully teach the network to value, whereas a model that takes proximity of neurons into consideration, like the real world brain would maintain the importance of those relationships since neurons connect more readily to others in proximity.
My question is: Does anyone know of white papers / experiments closely related to the concept I'm hypothesizing? Why would or would that not be a fundamentally better model?
","['neural-networks', 'convolutional-neural-networks', 'training', 'convolution']",
How exactly does adversarial training help in handling mode-collapse in generative networks?,"
Of my understanding mode-collapse is when there happen to be multiple classes in the dataset and the generative network converges to only one of these classes and generates images only within this class. On training the model more, the model converges to another class.
In Goodfellows NeurIPS presentation he clearly addressed how training a generative network in an adversarial manner avoids mode-collapse. How exactly do GAN's avoid mode-collapse? and did previous works on generative networks not try to address this?
Apart from the obvious superior performance (generally), is the fact that GAN's address mode-collapse make them far preferred over other ways of training a generative model?
","['neural-networks', 'deep-learning', 'training', 'generative-adversarial-networks', 'generative-model']","I don't think he said that at all. Going back to the talk you'll see he mentions mode collapse comes from the naivete of using alternating gradient-based optimization steps because then $min_{\phi}max_{\theta}L(G_\phi, D_\theta)$ starts to look a lot like $max_{\theta}min_{\phi}L(G_\phi, D_\theta)$.  This is problematic because in the latter case the generator has an obvious minimum of transforming all generated output into a single-mode that the discriminator has considered acceptable.  Since then a lot of work has been done to deal with this point of failure. Examples include Unrolled GANs (he mentions this one in the talk), where you essentially make the generator optimize what the discriminator will think $K$ steps in the future to ensure the ordering of the $min \ max$ game, and Wasserstein GANs, where you focus on a different metric that still has the same global minimum but allows for side by side training completely eliminating the ordering and failure mode, to begin with. On top of this, other work has been done as well, these are just two important examples.  Regarding how they fare against other generative models, like VAEs, there is no one is better than the other. The recent empirical success of GANs is why they are so popularly used, but we still see others being used in practice as well."
Why is the Jensen-Shannon divergence preferred over the KL divergence in measuring the performance of a generative network?,"
I have read articles on how Jensen-Shannon divergence is preferred over Kullback-Leibler in measuring how good a distribution mapping is learned in a generative network because of the fact that JS-divergence better measures distribution similarity when there are zero values in either distribution.
I am unable to understand how the mathematical formulation of JS-divergence would take care of this and also what advantage it particularly holds qualitatively apart from this edge case.
Could anyone explain or link me to an explanation that could answer this satisfactorily?
","['objective-functions', 'generative-adversarial-networks', 'probability-distribution', 'kl-divergence', 'jensen-shannon-divergence']","Lets start with question 1) how does JS-divergence handles zeros? by definition:
\begin{align}
D_{JS}(p||q) &= \frac{1}{2}[D_{KL}(p||\frac{p+q}{2}) + D_{KL}(q||\frac{p+q}{2})] \\
&= \frac{1}{2}\sum_{x\in\Omega} [p(x)log(\frac{2 p(x)}{p(x)+q(x)}) + q(x)log(\frac{2 q(x)}{p(x)+q(x)})]
\end{align}
Where $\Omega$ is the union of the domains of $p$ and $q$. Now lets assume one distribution is zero where the other is not (without loss of generality due to symmetry we can just say $p(x_i) = 0$ and $q(x_i) \neq 0$. We then get for that term in the sum
$$\frac{1}{2}q(x_i)log(\frac{2q(x_i)}{q(x_i)}) = q(x_i)\frac{log(2)}{2}$$
Which isn't undefined as it would be the KL case.  Now onto 2) In GANS why does JS divergence produce better results than KLThe asymmetry of KL divergence places an unfair advantage to one distribution over the other where in this case, its not ideal to consider it this way from an optimization perspective. Additionally KL divergences inability to handle non-overlapped distributions is crushing given that these are approximated through sampling schemes, and therefore there are no guarantees. JS solves both those issues and leads to a smoother manifold which is why its generally preferred. A good resource is this paper where they go more in detail investigating this.    "
Is it possible to create a decompiler using AI?,"
I would like to decompile a compiled file to source code.
Is it possible to use any AI technique to perform decompilation? Is there any research on this topic? If yes, can you briefly explain one of the existing approaches (just to get some insight)? I would also appreciate links to research papers on this topic.
","['machine-learning', 'deep-learning', 'reference-request']",
How is the percentage or the probablity calculated using Loss function in Facenet Model?,"
This question is related to What is the formula used to calculate the accuracy in the FaceNet model? . I know how loss is calculated in the FaceNet model , but how the loss function is used to calculate probability that this unknown person is , say Bob (0.70). Also  we don't know which is positive or negative image , we only know the Anchor (so how FaceNet finds which image is positive or negative ?) . How probability is calculated in FaceNet Model using triplet loss ?
Can we know what is the exact formula or CNN is like black box which uses some unknown method to calculate probability  ?
","['neural-networks', 'objective-functions', 'facial-recognition']","The facenet model is just the head of the model. The architecture is similar to the enocdr part of an autoencoder, but it uses supervised learning instead of unsupervised learning. The network is called a siamese network The triplet loss helps make the embeddings more representative of the input image/person, with the embedding distance being as large as possible for difference people and vice versa. However the embeddings is just the representation of the people. It doesn't contain information directly mapping to what person it is. The classification head is used after the Face Net feature extraction top is used.This method saves the calculated embeddings of people in a database. The face recognition system first calculates the embeddings of the new, unknown image to be classified. The system loop through the embeddings database and calculates the Euclidean distance of the unknown embeddings and the embeddings in the database. After computing, it chooses the smallest distance of all and compares it to a threshold set by you. If the distance is larger than the threshold, the classified class is unknown , or else the resulting class will be the embeddings which results to the lowest distance.Example code from the deeplearning.ai coursera course (great course on AI btw):
Example code of one shot face recognitionThe system have advantages ranging from no training needed and can classify unknown class. However, once the number of people in the database becomes huge, the system will run slower. It's performance may also differ depending on the quality of the reference image.This is the method used in the github repository of your previous posts. It uses a one layer nn classifying head to output the classified class.The system have the flexibility of adapting to multiple photo environment and no affecting of performance due to reference images. This method also works well for larger database of people. Triplet loss can also be removed using this method. You can train the network directly from propagating the loss with the probability predicted. However, This method requires re-training when a new person is added to the system. Multiple images of a person is also needed."
Ideas on a network that can translate image differences into motor commands?,"
I'd like to design a network that gets two images (an image under construction, and an ideal image), and has to come up with an action vector for a simple motor command which would augment the image under construction to resemble the ideal image more. So basically, it translates image differences into motor commands to make them more similar?
I'm dealing with a 3D virtual environment, so the images are snapshots of objects and motor commands are simple alterations to the 3D shape.
Probably the network needs two pre-trained CNNs with the same weights that extract image features, then output and concatenate those into a dense layer (or two), which converges into action-space. Training should probably happen via reinforcement learning
Additionally, in the end it needs recurrence, since there are multiple motor actions it needs to do in a row to get closer to the intended result.
Would there be any serious difficulties with this? Or are there any approaches to achieve the intended result? or any similar examples?
Thanks in advance
","['neural-networks', 'reinforcement-learning', 'convolutional-neural-networks', 'computer-vision', 'recurrent-neural-networks']",
Defect Detection System using Deep Learning,"
What is the general approach to defect detection in deep learning?
Would the approach be better if we try to learn the positive images (defects in images) as much as possible or we try to learn the negative images (images without blemishes) and try to single out the defects as some anomalies
Can someone point me to some architecture?
REgards
","['object-detection', 'anomaly-detection']",
How can I use deep reinforcement learning for vehicle rerouting in SUMO? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I want to use deep reinforcement learning for vehicle rerouting in SUMO, but I don't know how to start training the model. 
I've already created road network and vehicle routing in SUMO-XML files (mymap.net.xml and mymap.rou.xml). Currently, I'm trying to train the model on Jupyter Notebook, importing TraCI library to control the SUMO simulator and allow for a reinforcement learning approach. However, I'm still confused in training step. 

Do I need any traffic data to train my agent to take actions in the environment? 
How can I train based on these SUMO-XML files I created?
Is it possible to run the simulation on Windows? or I need to change to Ubuntu instead?

I would appreciate if someone could guide me. Thank you in advance.
","['deep-learning', 'reinforcement-learning']",
What technology do people use to create bots for games like LOL or Runescape?,"
I was curious about how people make AI to play games. Does anyone know of the AI used to play these games? What allows the AI to see/click the screen in real-time? Even just direction on what libraries for such tasks would be helpful. I can't imagine game developers make an API for creating bots in their games like browsers use with selenium.
","['computer-vision', 'game-ai', 'chat-bots']",
AlphaGo Zero: Does the policy head give a probability for every possible move?,"
If I understood correctly, the AlphaGo Zero network returns two values: a vector of logit probabilities p and a value v.
My question is: in this vector that it is outputted, do we have a probability for every possible action in the game? If so: does it apply a probability of 0 to actions that are not possible in that particular state? If this is true, how does the network know which actions are valid?
If not: then the network will output vectors of different sizes according to each state. Is this even feasible? And again, how will the network know which actions are valid?
Related questions but none of them covers this question in specific: 1, 2 and 3.
","['deep-learning', 'policies', 'deepmind', 'alphago-zero']",
Is it better to adjust the natural lighting (while recording the video) or to subsequently apply filters on the original video?,"
For the purpose of object detection, is it better to adjust the natural lighting (while recording the video) or to apply filters (e.g. brightness filters, etc.) on the original video to make it brighter? 
My intuition is that it shouldn't matter when you adjust the natural lighting or do it after with video filters.
","['convolutional-neural-networks', 'object-detection']","Personally, I'd say as long as the object is visible don't do either. If the model has been well built and if lighting changes would help, the convolution operation weights would learn an operation similar to contrast or brightness changes.On the other hand if the object visibility is an issue, then natural lighting changes would be better, due to the lack of potential artefacts a filter would create.So overall, I'd say natural lighting changes should be more helpful (Assuming model is built well) and brightness filters would not be very helpful as the convolution operations would learn them if they were useful, also there would be  artefacts in the input which can lead to the model learning irrelevant details.   Hope this helped! "
How to create an AI to solve a word search?,"
This at first sounds ridiculous. Of course there is an easy way to write a program to solve a wordsearch. 
But what I would like to do is write a program that solves a word-search like a human.
That is, use or invent different strategies. e.g. search randomly for the starting letter; go line-by-line; 
Probably the AI will eventually find out that going line by line looking for a given starting letter of a word is a good strategy.
Any idea how you would write such a strategy-finding AI?
I think the main ""moves"" would be things like ""move right one letter in the grid"", ""store this word in memory"", ""compare this letter with first letter in memory"" and a few more.
","['deep-learning', 'search', 'architecture', 'reasoning']",
Can I use my previous estimate of the state-action values as initialisation in GLIE-Monte Carlo Control?,"
I am trying to implement a tabular-based GLIE Monte-Carlo learning algorithm. 
So I repeat n times: 

create observations using my previous policy $\pi_{n-1}(s)$
update my state-action values using the observations generated in 1 with the monte-carlo update rule: $Q_n(s_t,a_t)= Q_n(s_t,a_t)+1/N(s_t,a_t)\times(G_t-Q_n(S_t,a_t))$
update my policy to $\pi_{n}$ using epsilon-geedy improvement with $\epsilon=1/(n+1)$. 

In step 2 I need to decide for an initial estimate $\tilde{Q}_n$. Is it a decent option to use $\tilde{Q}_n=Q_{n-1}$?
","['reinforcement-learning', 'monte-carlo-methods']","In step 2 I need to decide for an initial estimate $\tilde{Q}_n$. Is it a decent option to use $\tilde{Q}_n=Q_{n-1}$?Yes, this is a common choice. It's actually common to update the table for $\tilde{Q}$ in place, without any separate initialisation per step. The separate phases of estimation and policy improvement are easier to analyse for theoretical correctness, but in practice updates made in place can be faster because new information is used as soon as it is available.Depending on how the policy was changed, and how accurate the previous estimate was, this could place the estimates closer convergence for the next step. Often the previous estimates will be closer to the new targets than any fixed or random initialisation scheme you could set up."
Are humans superior to machines in chess?,"
A friend of mine, who is an International Master at chess, told me that humans were superior to machines provided you didn't impose the time constraints that exist in competitive chess (40 moves in 2 hours) since very often games were lost, to another human or a machine, when a bad move is made under time pressure.
So, with no time constraints and access to a library of games, the human mind remains superior to the machine is my friend's contention.  I'm an indifferent chess player and don't really know what to make of this.  I was wondering if any research had been made that could back up that claim or rebut it.
","['comparison', 'chess']",
What kind of data structures are needed to efficiently do back-propagation in a feedforward neural network?,"
In a feed-forward neural network, in order to efficiently do backpropagation, what kind of data structure is needed? 
I know the weights can just be stored in an array, and you need pointers of some kind to represent connections from one layer to the next (or just a default scheme/pattern), but is anything else needed for backpropagation to work?
","['neural-networks', 'backpropagation', 'feedforward-neural-networks']","TL;DR: You'll need to store a little bit more to perform backward passes. You'll need to store data from the forward pass. This stored information is used for calculating the gradient.Overview (warning: not trivial)I know the weights can just be stored in an arrayYou'll need a little more:To update the weights you need to keep a ""cache"" of the forward pass intermediate terms. That is, forward propagation can be seen as a series of transformations on your input $X$:
$$X\xrightarrow{\Theta^{[1]}+b^{[1]}} [ Z^{[1]} \xrightarrow{\alpha^{[1]}} A^{[1]}] \xrightarrow{\Theta^{[2]}+b^{[2]}}
\dots 
\xrightarrow{\Theta^{[L]}+b^{[L]}} [ Z^{[L]} \xrightarrow{\alpha^{[L]}} A^{[L]}]\xrightarrow{\frac{1}{m}\sum\limits_m\sum\limits_{n_L} loss\{A^{[L]},y\}} J
$$where:$Z^{[1]}=\Theta^{[1]}X+b^{[1]}$ (ie the linear part)$A^{[l]}=\alpha^{[l]}(Z^{[l]})$ (ie element wise activation over linear part)You need to store the $Z^{[l]}$ & $A^{[l]}$ terms in said ""cache."" You could store these in an array or some other similar data structure. You need these for calculating the gradient during the backwards pass. Syntax$A^{[k]}$ - this means we are indexing by layer (eg $\alpha^{[k]}$ is the activation for k-th layer)$m$ - is the number of examples in the batch$n_k$ - denotes the number of neurons in the k-th layer$L$ - the number of layers (so $n_L$ is the number of neurons in last layer)$\Theta$ - The set of all weights (notice no superscript)BackpropIn the case of neural networks the cost is a scalar function of inputs and parameters. To get backprop started calculate the scalar by matrix derivative of the cost with respect to the activations of the last layer call this matrix $dA^{[L]}$. Observe:$dA^{[L]} = \frac{\partial J(\Theta,X)}{\partial A^{[L]}}$Next, we calculate scalar-by-matrix derivative of $Z^{[L]}$. Doing this one realizes:$dZ^{[L]} = \frac{\partial J(\Theta,X)}{\partial Z^{[L]}} = dA\odot\alpha'^{[L]}(Z^{[L]})$Where $\odot$ denotes element wise (Hadamard) product.With the above one can make use of the matrix definitions for back propagation:$\text{(A)}\quad d\Theta^{[l]} = \frac{1}{m}dZ^{[l]}\times (A^{[l-1]})^T$$\text{(B)}\quad db^{[l]} = \frac{1}{m}\sum_{c=1}^m dZ^{[l](c)}$ (where the new superscript in $dZ^{[l](c)}$ denotes summing along the batch dimension )$\text{(C)} \quad dZ^{[l]}= dA^{[l]}\odot \alpha^{'[l]}(Z^{[l]})$$\text{(D)}\quad dA^{[k]} = (\Theta^{[k+1]})^T\times dZ^{[k+1]}$And of course the wight updates are:$\Theta^{[L]} \leftarrow \Theta^{[L]} - \frac{\eta}{m}d\Theta $ $b^{[L]} \leftarrow b^{[L]} - \frac{\eta}{m}db $ (where $\eta$ is the learning rate)Observe, how the forward pass terms are used during the backprop calculations. A recommendationTake the A. Ng deep learning specialization. He does a good job explaining the intuition and even has a project to implement this. Though, he does not derive the back propagation equations. You can find a not so easy derivation here."
What is the difference between reinforcement learning and AutoML?,"
My vague understanding of reinforcement learning (RL) is that it's very similar to supervised learning except that it updates on a continuous feed of data/activity, this to me sounds very similar to AutoML (which I've started to notice being used). 
Do they use different algorithms? What is the fundamental difference between RL and AutoML?
I'm after an explanation for somebody who understands technology but does not work with machine learning tools regularly. 
","['reinforcement-learning', 'comparison', 'supervised-learning', 'automated-machine-learning']","Automated machine learning (AutoML) is an umbrella term that encompasses a collection of techniques (such as hyper-parameter optimization or automated feature engineering) to automate the design and application of machine learning algorithms and models. Reinforcement learning (RL) is a sub-field of machine learning concerned with the task of making decisions and taking actions in an environment so as to maximize (long-term) reward (which is the goal of the so-called RL agent). RL is (at least partially) based on the way animals (including humans) learn. For example, the usual way of training a dog to perform a certain task is to reward it with food whenever it takes the correct action (for example, jumping, if you want the dog to jump whenever you make a certain gesture with your hand). In this case, the RL agent is the dog, the task the dog needs to perform (e.g. jumping) is the environment, food is the reward and the goal is to get food.Given that reinforcement learning (RL) is a sub-field of machine learning, then, in principle, AutoML can also be used to automate the design of RL algorithms, models or agents. For example, if you use a neural network to represent the policy (the function the determines which action to take in the environment), then you can potentially use AutoML to find the most appropriate architecture (for example, the most appropriate number of layers) for this neural network."
Using a neural network to identify a stable region within a set of data?,"
I am working on a problem in which I am attempting to find a stable region in a spiral galaxy. The PI I'm working with asked me to use machine learning as a tool to solve the problem. I have created some visualizations of my data, as bellow.

In this image, you can see there is a flat region between 0 and roughly 30 pixels, and between 90 pixels and 110 pixels. I have received suggestions to use an RNN LSTM model that can identify flat regions, but I wanted to hear other suggestions of other neural network models as well.
The PI I'm working with suggests to feed my data visualization images into a neural network and have the neural network identify said stable regions. Can this be done using a neural network, and what resources would I have to look at? Moreover, can this problem be solved with RNN LSTM? I think the premise of this was to treat the radius as some temporal dimension. I've been extensively looking for answers online, and I cannot quite seem to find any similar examples.
","['neural-networks', 'machine-learning', 'python', 'keras', 'long-short-term-memory']",
Why does the binary cross-entropy work better than categorical cross-entropy in a multi-class single label problem?,"
I was just doing a simple NN example with the fashion MNIST dataset, where I was getting 97% accuracy, when I noticed that I was using Binary cross-entropy instead of categorical cross-entropy by accident. When I switched to categorical cross-entropy, the accuracy dropped to 90%. I then got curious and tried to use binary cross-entropy instead of categorical cross-entropy in my other projects and in all of them the accuracy increased.
Now, I know that binary cross-entropy can be used in a multi-class, multi-label classification problem, but why is working better than categorical cross-entropy in a multiclass single label problem?
","['neural-networks', 'classification', 'objective-functions', 'cross-entropy']",
Is there a detailed description or implementation of an end-to-end speech recognition system?,"
I am currently trying to implement an end-to-end speech recognition system from scratch, that is, without using any of the existing frameworks (like TensorFlow, Keras, etc.). I am building my own library, where I am trying to do a polynomial approximation of functions (like exponential, log, sigmoid, ReLU, etc). I would like to have access to a nice description of the neural networks involved in an end-to-end speech recognition system, where the architecture (the layers, activation functions, etc.) is clearly laid out, so that I can implement it.
I find most of the academic or industry papers citing various previous works, toolkits or papers, making it tedious for me. I am new to the field, so I am having more difficulty, so looking for some help here. 
","['neural-networks', 'natural-language-processing', 'research', 'reference-request', 'speech-recognition']",
"If an heuristic is not admissible, can it be consistent?","
I am solving a problem in which, according to the given values, the heuristic is not admissible. According to my calculation from other similar problems, it should be consistent, as well as keeping in mind the values, but the solution says it's not consistent either. Can someone tell why? 
","['search', 'proofs', 'admissible-heuristic', 'consistent-heuristic', 'heuristic-functions']",
"Are perfect and imperfect information games modelled as fully and partially observable environments, respectively?","
In perfect information games, the agent can see all the moves performed in the past. Besides, it can observe the next action that will be put into practice by the opponent. 
In this case, can we say that perfect information games are actually a fully observable environment? If we reach this conclusion, I guess that imperfect information becomes a partially observable environment?
","['comparison', 'markov-decision-process', 'game-theory', 'pomdp', 'imperfect-information']",
"How can I define the relations, preconditions and effects of each operator for the Sokoban puzzle?","
I would like to solve the Sokoban puzzle, which consists in moving a character in a 2D map to push boulders into target cells. Each turn, the player can move to an adjacent cell (no diagonals) if it is empty, or push a boulder one step further. To push a boulder, the player needs to stand next to it, and the space behind the boulder needs to be empty (no other boulders and no walls).
I'm using the STRIPS planner, and I am having a hard time defining the fixed and dynamic relations and also the preconditions and effects of each operator for this puzzle.
","['planning', 'strips', 'sokoban-puzzle']",
AI with conflicting objectives?,"
A recent question on AI and acting recalled me to the idea that in drama, there are not only conflicting motives between agents (characters), but a character may themselves have objectives that are in conflict.
The result of this in performance is typically nuance, but also carries the benefit of combinatorial expansion, which supports greater novelty, and it occurs to me that this would be a factor in affective computing.  
(The actress Eva Green is a good example, where her performances typically involve indicating two or more conflicting emotions at once.)
It occurs to me that this can even arise in the context of a formal game where achieving the most optimal outcome requires managing competing concerns.

Is there literature or examples of AI with internal conflicting objectives?

","['machine-learning', 'reference-request', 'game-theory', 'goal-based-agents', 'affective-computing']",
Which books or papers clearly explain the relation between Ising models and deep neural networks?,"
I am looking for a book or paper which clearly explains the relationship between Ising models and deep neural networks.
Can anyone provide any references?
","['neural-networks', 'comparison', 'reference-request', 'papers', 'books']",
Deep audio fingerprinting for word search,"
Simply speaking, I'm trying to somehow search an audio clip for a list of words, and if found, I mark the time stamps. My use-case is profanity check with a list of pre-defined profane words.
Is there any successfull approaches, samples, tools or APIs, possibly through deep learning, to perform this? I'm new to audio processing.
","['machine-learning', 'deep-learning', 'natural-language-processing', 'voice-recognition', 'audio-processing']",
Detailed explaination of Facenet Model for face recogniton?,"
Can some one explain how Facenet model works in detail and simple words .
","['neural-networks', 'deep-learning', 'facial-recognition']","Facenet is a Siamese network. It's basic architecture is this:

The input(a face) is fed through a deep convolutional neural network and also a fully connected layer at the end. The fully connected layer at the end output an embedding of the input image which is a predefined size. The embedding can contain feature that human understand or maybe not. The embedding represent the input image, just in a ""compressed"" form. To further explain that, let me give an example. Let's say that you have to describe a face. What's will you say? You will probably say something like the face is round, the eyes are blue, it is a female face and more. The neural network is doing what you are doing, describing the face, but using numbers instead of words. To do a face recognition task, the network take an pre taken image of the list of people to recognize and the unknown new data from the people to be recognised. It then feed both images into the network and get the embeddings. The network then calculate the distance of the two embeddings, using some metric such as squared error or absolute error. In the image it uses the squared error. If the error is below a certain threshold, the face is recognised. If not, it then loops through the other pre taken images in the set of faces of the system and do the task again. The system stores embedding of the pre taken images before hand. For training the FaceNet, triplet loss is used. Triplet loss has been explained in another of your post, by me. What is the formula used to calculate the accuracy in the FaceNet model?Basically, the model is trained using the triplet loss as it can train the network to output a similar embedding for the same person and a very different embedded for a different person. Sometimes, a binary classification end is also used. It removed the need of the triplet loss and outputs a number from 0 to 1 for similarity instead. This removes the triplet loss part.Hope my answer can help you and have a nice day!"
Why can't LSTMs tell a long story?,"
There is a recent trend in people using LSTMs to write novels. I havenâ€™t attempted this myself. From what Iâ€™m hearing, they can tell a story, but it seems they lose the context of the story rather quickly. After which they begin constructing new, but not necessarily related constructs. 
Can they construct a plot in the long term?
","['deep-learning', 'recurrent-neural-networks', 'long-short-term-memory', 'applications']",
Are deep learning models more prone to overfitting than machine learning ones?,"
In my opinion, deep learning algorithms and models (that is, multi-layer neural networks) are more sensitive to overfitting than machine learning algorithms and models (such as the SVM, random forest, perceptron, Markov models, etc.). They are capable of learning more complex patterns. At least that's how I look at it. Still, I and my colleagues disagree about this and I cannot really find any information about this. My colleagues say that deep learning algorithms are hardly vulnerable to overfitting.
Are there statements (or opinions) about this aspect?
","['machine-learning', 'deep-learning', 'comparison', 'overfitting']","Your reasoning isn't wrong. Deep Neural Networks (DNNs) have a much larger capacity than simpler ML algorithms (excluding NNs) and can easily memorize even a very complex dataset and overfit. DNNs, however, are so effective because they usually are applied on tasks that are harder, so it's not as easy to overfit. For example an image classifier might be trained on a dataset with millions of images; a task much harder to overfit on.In cases where this isn't possible (e.g. an image classification task with a couple thousand images), transfer learning is used. You can initialize your weights from a model pre-trained on a large dataset, use its already-trained feature extraction layers and simply fine tune the last layer.Data augmentation also helps a lot here, which effectively increases size of the training set and discourages the DNN from memorizing the samples. It is so effective that it is used even in large datasets, where it is harder to overfit.Additionally, DNNs employ several methods to prevent them from overfitting. The most prominent of these is dropout, which is a very effective regularizer. Batch Normalization has also proven an effective regularizer. SGD allows you to explore more parameters than GD, which also is effective against overfitting. Finally, early stopping and parameter norm penalties aren't uncommon in DNNs."
Are embeddings in multi-lingual language models comparable across languages?,"
Facebook has just pushed out a bigger version of their multi-lingual language model XLM, called XLM-R. My question is: do these kind of multi-lingual models imply, or even ensure, that their embeddings are comparable between languages? That is, are semantically related words close together in the vector space across languages?
Perhaps the most interesting citation from the paper that is relevant to my question (p. 3):

Unlike Lample and Conneau (2019), we do not use language embeddings,
  which allows our model to better deal with code-switching.

Because they do not seem to make a distinction between languages, and there's just one vocabulary for all trained data, I fail to see how this can be truly representative of semantics anymore. The move away from semantics is increased further by the use of BPE, since morphological features (or just plain, statistical word chunks) of one language might often not be semantically related to the same chunk in another language - this can be true for tokens themselves, but especially so for subword information.
So, in short: how well can the embeddings in multi-lingual language models be used for semantically comparing input (e.g. a word or sentence) of two different languages?
","['deep-learning', 'natural-language-processing', 'transformer', 'language-model']",
Self-organizing map using weighted non-euclidean distance to minimize variance of predictions,"
Let's say I have a dataset, each item/row of which has $\mathit{X + 1}$ characteristics where the last characteristic (i.e., the $\mathit{1}$) represents the some value I want to predict, $\mathit{Y}$, based on a SOM trained on the $\mathit{X}$ characteristics.  I want to organize the dataset into groups such that each group has a small variance among the respective $\mathit{Y}$ values.  I believe I could do this by using a non-Euclidean distance to find the Best Matching Unit (BMU) based on applying weights to each dimension.
For example, given a node at (0,0) and weights for dimension $\mathit{x}$ of 1 and dimension $\mathit{y}$ of 2, a data point at (3,2) would have a weighted distance of 5 from the node, calculated as follows:
$\sqrt{\mathit{(1 * (3 - 0)) ^ 2 + (2 * (2 - 0)) ^ 2}}$
I don't think a simple linear regression would work to determine the weights because it would not take advantage of clustering.
The goal would be, for a new data point, to approximate a probability distribution of outcomes based on similarly-profiled data points in the training set (i.e., retrieve all of the training results with the same BMU and analyze the results).  I think this might essentially just be replicating a deep feedforward network, but I'd like to try it.
Is there a way I could achieve this by modifying a SOM model or using a similar technique?
","['feedforward-neural-networks', 'self-organizing-map']",
What is convergence in machine learning?,"
I came across this answer on Quora, but it was pretty sparse.  I'm looking for specific meanings in the context of machine learning, but also mathematical and economic notions of the term in general.
","['neural-networks', 'machine-learning', 'terminology', 'convergence']",
How does the policy gradient's derivative work?,"
I am trying to understand the policy gradient method using a PyTorch implementation and this tutorial.
My first question is about the end result of this gradient derivation,
\begin{aligned}
\nabla \mathbb{E}_{\pi}[r(\tau)] &=\nabla \int \pi(\tau) r(\tau) d \tau \\
&=\int \nabla \pi(\tau) r(\tau) d \tau \\
&=\int \pi(\tau) \nabla \log \pi(\tau) r(\tau) d \tau \\
\nabla \mathbb{E}_{\pi}[r(\tau)] &=\mathbb{E}_{\pi}[r(\tau) \nabla \log \pi(\tau)]
\end{aligned}
Mainly in this equation 
$$\nabla \mathop{\mathbb{E}_\pi }[r(\tau )] = \mathop{\mathbb{E}_\pi }[r(\tau )\nabla log \pi (\tau )]$$
Does expectation follow a distributive or associative property?
I know that expectations of a function can be written as below
$$\mathop{\mathbb{E}}[f(x)] =\sum p(x)f(x)$$
Then can we rewrite the first equations as
$$\mathop{\mathbb{E}_\pi }[r(\tau )\nabla log \pi (\tau )] \\= \mathop{\mathbb{E}_\pi }[r(\tau )] \,\, \mathop{\mathbb{E}_\pi }[\nabla log \pi (\tau )] \\= \sum p(\tau)r(\tau ) \,\, \sum p(\tau)\nabla log \pi (\tau ) \\
= p(\tau) \sum r(\tau ) \nabla log \pi (\tau )$$
The problem is when I compare  this to PyTorch implementation (line 71-74)
for log_prob, R in zip(policy.saved_log_probs, returns):
    policy_loss.append(-log_prob * R)
optimizer.zero_grad()
policy_loss = torch.cat(policy_loss).sum()

The  pytorch implementation simply multiplied log probability and reward -log_prob * R and then summed the vector torch.cat(policy_loss).sum() there is no $p(\tau)$. What is really happening here?
The second question is the multiplication of log probability and reward in PyTorch implementation -log_prob * R, PyTorch implementation has a negative log probability and derived equation has a positive one $\mathop{\mathbb{E}_\pi }[r(\tau )\nabla log \pi (\tau )]$. What is the need for multiplying log probability with a negative value in PyTorch implementation?
I have only a basic understanding of maths and that's why I am asking this question here.

Edit: found a better derivation of above equation https://youtu.be/Ys3YY7sSmIA?t=3622
","['reinforcement-learning', 'policy-gradients', 'pytorch', 'reinforce', 'expectation']","You cannot do this:$\mathop{\mathbb{E}_\pi }[r(\tau )\bigtriangledown log \pi (\tau )] \\= \mathop{\mathbb{E}_\pi }[r(\tau )] \,\, \mathop{\mathbb{E}_\pi }[\bigtriangledown log \pi (\tau )]$That is because $r(\tau )$ and $\bigtriangledown log \pi (\tau )$ are correlated by their dependence on $\tau$. In a simpler concrete example, if your expectation was over simple equiprobable discrete distribution where $\tau$ could be any integer in range $[1,10]$, then $\mathop{\mathbb{E} }[\tau^2] = 38.5$ whilst $\mathop{\mathbb{E} }[\tau]\mathop{\mathbb{E} }[\tau] = 30.25$The  pytorch implementation simply multiplied log probability and reward -log_prob * R and then summed the vector torch.cat(policy_loss).sum() there is no $p(\tau)$. What is really happening here?The purpose of transforming the gradient into an expectation $\mathbb{E}$ for the policy gradient theorem, is so that you can estimate it using samples taken from the distribution. Typically, you don't know $p(\tau)$, but you do know that if you follow the same process where $p(\tau)$ applies (i.e. measure the return from the environment whilst following the policy represented by the policy function) that you will get an unbiased sample from that distribution.So what is going on here is that you throw away the outer expectation $\mathbb{E}_{\pi}[]$ and replace it with a stochastic estimate for the same value based on taking samples. The samples are naturally obtained with distribution described by $p(\tau)$, if you follow the policy function when making action choices.The second question is the multiplication of log probability and reward in pytorch implementation -log_prob * R, pytorch implementation has a negative log probability and derived equation has a positive one $\mathop{\mathbb{E}_\pi }[r(\tau )\bigtriangledown log \pi (\tau )]$. What is the need for multipling log probability with negative value in pytorch implementaion?I don't know the code, but this very likely because of a sign change brought on by considering how to respond to the gradient estimate. There is a clue in the use of the name ""loss"". To maximise return in policy gradient methods, you can perform gradient ascent based on the estimated gradient as the goal is to find higher values. However, it is more usual in NN libraries to perform gradient descent in order to minimise a loss function. That is a likely cause of the sign reversal here."
What happens when the output length in the brevity penalty is zero?,"
The brevity penalty is defined as
$$bp = e^{(1- r/c)},$$
where $r$ is the reference length and $c$ is the output length.
But what happens if the output length gets zero? Is there any standard way of coping with that issue?
","['natural-language-processing', 'machine-translation', 'bleu']","Division by zero is not mathematically defined. A usual or standard way of dealing with this issue is to raise an exception. For example, in Python, the exception ZeroDivisionError is raised at runtime if you happen to divide by zero. If you execute the following programYou will getHowever, if you want to avoid this runtime exception, you can check for division by zero and deal with this issue in a way that is appropriate for your program (without needing to terminate it).In the paper BLEU: a Method for Automatic Evaluation of Machine Translation that introduced the BLEU (and brevity penalty) metric, the authors defined the brevity penalty as\begin{align}
BP =
\begin{cases}
1, & \text{if } c > r\\
e^{(1- r/c)} & \text{if } c \leq r\\
\end{cases} \label{1} \tag{1}
\end{align}This definition does not explicitly take into account the division by zero. The Python package nltk does not raise an exception, but it (apparently, arbitrarily) returns zero when c == 0. Note that the BLEU metric ranges from 0 to 1. For example, if you execute the following programYou will getIn the example above, the only human reference (translation) is reference1 = list(""hello"") and the only candidate (the machine translation) is an empty list. However, if references = [] (you have no references), then you will get the error ValueError: min() arg is an empty sequence, where references are used to look for the closest reference (the closest human translation) to the  candidate (the machine translation), given that there could be more than one human reference translation, and one needs to be chosen to compute the brevity penalty, with respect to your given candidate. In fact, in the documentation of the brevity_penalty function, the following comment is written where hypothesis is a synonym for candidate (the machine translation) and the length of the candidate is $c$ in the formula \ref{1} (and c in the example above).To answer your second question more directly, I don't think there's a standard way of dealing with the issue, but I've not fully read the BLEU paper yet."
creating your own dataset similar to cityscapes format,"
I'm trying to train a neural network with my own dataset. The neural network can accept the cityscape format. 
Is there any application that can give mask/segmented image, instance image, label IDs images and JSON file, similar to cityscape dataset format? 
Basically, I want to create my own dataset similar to the cityscape dataset format. 
","['training', 'datasets', 'image-segmentation']",
How is gradient being calculated in Andrej Karpathy's pong code?,"
I was going through the code by Andrej Karpathy on reinforcement learning using a policy gradient. I have some questions from the code.

Where is the logarithm of the probability being calculated? Nowhere in the code I see him calculating that. 
Please explain to me the use of dlogps.append(y - aprob) line. I know this is calculating the loss, but how is this helping in a reinforcement learning environment, where we don't have the correct labels?
How is policy_backward() working? How are the weights changing to the loss function mentioned above? More specifically, what's dh here?

","['deep-learning', 'reinforcement-learning', 'backpropagation', 'policy-gradients']","Given a probability p, the corresponding odds are calculated as p / (1 â€“ p). For example if p=0.75, the odds are 3 to 1: 0.75/0.25 = 3.The logit function is simply the logarithm of the odds: logit(x) = log(x / (1 â€“ x)).Sigmoid near logp is like follows:The inverse of the logit function is the sigmoid function. That is, if you have a probability p, sigmoid(logit(p)) = p.Source: [1]For example in Pong we could wait until the end of the game, then take the reward we get (either +1 if we won or -1 if we lost), and enter that scalar as the gradient for the action we have taken (DOWN in this case). In the example below, going DOWN ended up to us losing the game (-1 reward). So if we fill in -1 for log probability of DOWN and do backprop we will find a gradient that discourages the network to take the DOWN action for that input in the future (and rightly so, since taking that action led to us losing the game).Roughly speaking the backpropagation is correcting the weights backwards the network after the round is done. More thorough explanation is in the mentioned comments section.Sources:[1] https://www.google.com/amp/s/nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/amp/"
How could I locate certain words or numbers in a financial statement?,"
I would like to code a script that could locate a specific word or number in a financial statement. Financial statements roughly contain the same information, they are however not identical and organized in the same way. My thought is that by using Tensorflow I could train a neural network to locate the specific words or numbers for me. I am thinking that if I label different text and numbers in 1000 financial statements and use them to train the neural network, it will then be able to identify these numbers or words in all financial statements. For example, tell it in all 1000 training statements which number that is the profit of the company. Then when I give it an unseen financial statement, it should be able to identify which number that is the profit.
Is this doable? I have been working with coding in python for a couple of months and so far I've built some web scrapers and integrated them with twitter, slack and google sheets. However, this would be my first AI related project. I would be very grateful for all your thoughts on this project and if anyone could steer me in the right direction by sharing relevant tutorials.
Thanks a lot!
","['neural-networks', 'machine-learning', 'deep-learning', 'python', 'tensorflow']",
Which machine learning algorithms can be used to build a recommendation system?,"
I am working on building a recommendation engine. I need to build a model that recommends similar items. Currently, I am using the Nearest Neighbor algorithm present in sklearn.neighbors package. 
I am working in finance domain, similarity can based on the ""Supplier"", ""Buyer"", ""Industry type"" etc.
I have attached sample data in the image below

Is there any better machine learning algorithms/packages in Python for the same?
","['machine-learning', 'python', 'recommender-system']",
Does a neural network exist that can learn every possible training data?,"
The universal approximation theorem states, that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $R^n$.
Michael Nielsen states

No matter what the function, there is guaranteed to be a neural network so that for every possible input, $x$, the value $f(x)$ (or some close approximation) is output from the network.

So, for continuous functions, this seems plausible. Interestingly, in the same article, Nielsen mentioned ""any function"".
Later, he writes

However, even if the function we'd really like to compute is discontinuous, it's often the case that a continuous approximation is good enough.

The last statement leaves open a gap, to ask how well an approximation practically can be.
Let's ignore contradictory input/output training pairs like $f(0)=0$ and $f(0)=1$, which actually don't event represent a function anyway.
Furthermore, assume that the training data is generated randomly, which would practically result in a discontinuous function.
How does a neural network learn such data? Will a learning algorithm always be able to find a neural network that approximates the function represented by the input-output pairs?
","['neural-networks', 'computational-learning-theory', 'universal-approximation-theorems']","The branch of AI research that answers questions like this is called computational learning theory.For the specific question you have asked, the universal approximation theorem does indeed prove that any function can be modeled by a sufficiently wide neural network. The definition of a function includes the requirement that each input be mapped to exactly one output, so contradictory labels in training data are excluded explicitly.Here is a rough sketch that can provide an intuition behind why this is true. This is not a proper proof, but it gives you an idea of the power of ""finite number of neurons"" in a hidden layer:It should seem natural then that a sufficiently wide neural network can memorize all of an input pattern. Since memorization is sufficient for ""learning"" to fit a pattern this should give you an intuition for the ability of neural networks to fit things."
How do you build a language model to predict the contextual similarity between two documents?,"
How do you build a language model to predict the contextual similarity between two documents?
","['natural-language-processing', 'language-model']",
What is the complexity of policy gradient algorithms compared to discrete action space algorithms?,"
I am using a policy gradient algorithm (actor-critic) for wireless networks. The policy gradient-based algorithm helps because it considers continuous action space. 
But how much does a policy gradient-based algorithm contribute to the complexity of the involved neural networks, compared to discrete action space algorithms (like Q-learning)? Moreover, in terms of computation, how do policy gradient algorithms (for continuous action spaces) compare to discrete action space algorithms?
","['neural-networks', 'comparison', 'q-learning', 'policy-gradients', 'time-complexity']",
Can neural networks deal with unbounded numbers as inputs?,"
I want to train an ANN. The problem is that the input features are completely unbounded (There are no boundaries as maximum and minimum for them).
For example, the following input vectors $(42, 54354354)$ and $(0.4, 47239847329479324732984732947)$ are both valid.
I know RNNs that can add up input neurons, which are pretty similar to my case, but the number of the digits was limited in all of the implementations.
Is there a way to implement an ANN that can add up the input numbers of any magnitude?
","['neural-networks', 'machine-learning', 'deep-learning']",
Sutton & Barto's notation $V_{t+n}$ in Chapter 7: $n$-step Bootstrapping,"
Until Chapter 6 of Sutton & Barto's book on Reinforcement Learning, the authors use $V$ for the current estimate of a state value. Equation (6.1), for example, shows:
$$ V(S_t) \leftarrow V(S_t) + \alpha[G_t - V(S_t)]\ \ \ \ \ \ (6.1)$$
However, on Chapter 7 they add a subscript to $V$. The first time this appears is on page 143 when they define the return from $t$ to $t+1$:
$$ G_{t:t+1} \dot{=} R_{t+1} + \gamma V_t(S_{t+1})$$
and say that $V_t : \mathcal{S} \rightarrow  \mathbb{R}$ is ""the estimate at time $t$ of $v_\pi$.""
At first I thought I understood this as a natural consequence of considering $n$ steps ahead in the future and needing an extra index to go over the $n$ steps. But then this stopped making sense when I realized that an estimate for a state must be consolidated, no matter at which of $n$ steps that is coming from. After all, a state $s$ has a single value to estimate, $v_\pi(s)$, and that does not depend on $t$.
Then I thought that they are just taking into account that there are many successive estimates of $V$ as the algorithm progresses, so $V_t$ is just the estimate after processing the $n$ steps starting at time $t$. In other words, the subscript would be a rigorous mathematical way of denoting the sequence of algorithmic updates. But this does not make sense either since even in Chapter 6 and before, the estimate is also successively updated. See Equation (6.1), for example. The $V$ on the left-hand side is a different variable from the one on the right-hand side (this is why they must use $\leftarrow$ indicating an assignment as opposed to a mathematical equality with $=$). It could have easily been written with an index as well.
So, what is the purpose of the new index for $V$ in Chapter 7, and why is it more important at this particular chapter?
Edit and elaboration: Going back to the text, it seems to me that the new subscript is indeed added as an attempt for greater clarity, even though the subscript-less notation $V$ from previous chapters might have been kept (and in fact it is still used in the pseudo-code in page 144).
It seems the authors wanted to stress that the update of $V$ happens not only for every trace of $n$ steps, but also at every one of those steps.
However, I think this introduced a technical error, because suppose we just learned from an 10-step episode ($T=10$), using $n = 3$. Then the latest estimate of $v_\pi$ is $V_{T-1} = V_{10 - 1} = V_{9}$. Then at the next episode, the first time $V_{t + n}$ is used to inform a target update, it will be for $\tau = 0$ (from the pseudo-code), which implies $t - n + 1 = 0$, so $t = n - 1$, that is, $V_{t+n}=V_{n-1+n}=V_{2n-1}=V_5$, which is not the most up-to-date estimate $V_9$ of $v_\pi$.
Of course the problem would be easily solved if we simply set the next used estimate $V_{2n + 1}$ to be equal to the last episode's $V_{T-1}$, but to avoid confusion this would have to be explicitly stated somewhere.
","['reinforcement-learning', 'sutton-barto', 'notation']","So, what is the purpose of the new index for $V$ in Chapter 7, and why is it more important at this particular chapter?My guess would be that your intuition is correct, and that it's mostly introduced just to clarify exactly which ""version"" of our value function approximator is going to be used in any particular equation. In previous chapters, which discuss single-step update rules, I guess the authors assumed there was less potential for confusion, and therefore no need to clarify this. Without the clarification, some people might for instance wonder if we should use $V_t$ for our value estimates of an $n$-step return $G_{t+n}$, regardless of how large $n$ is.However, I think this introduced a technical error, because suppose we just learned from an 10-step episode ($T=10$), using $n = 3$. Then the latest estimate of $v_\pi$ is $V_{T-1} = V_{10 - 1} = V_{9}$. Then at the next episode, the first time $V_{t + n}$ is used to inform a target update, it will be for $\tau = 0$ (from the pseudo-code), which implies $t - n + 1 = 0$, so $t = n - 1$, that is, $V_{t+n}=V_{n-1+n}=V_{2n-1}=V_5$, which is not the most up-to-date estimate $V_9$ of $v_\pi$.Once we start considering a situation with more than a single episode, the $V_t$ notation becomes quite confusing. You should read $V_t$ as ""the value function approximator that we have available at time $t$ of the current episode"". So, if we were to use the symbol $V_0$ within the context of a second episode, that would be identical to what was referred to as $V_T$ in the context of the first episode. The $V_t$ notation can be convenient if we're thinking about our equations with our minds in ""math-mode"", but becomes highly confusing once we start thinking about practical implementations involving multiple episodes -- this is probably why they did not include it in the pseudocode.If you really wanted to use the subscript-notation in the pseudocode, you'd have to add an extra term in the subscript that adds up all the durations of all previous episodes. If we then try to work out your example situation, we'd run into another problem though... we'd want to use $V_{t+n+T} = V_{2n-1+T} = V_{15}$ at the first iteration where $\tau = 0$ in the second episode. But, across the two episodes, only $13$ steps have passed, so this does not yet exist! You run into the same issue if you try to work out what happened when $\tau = 0$ in the first episode: applying exactly the same reasoning as in your quote, we would have wanted to use $V_5$ after only $3$ time steps passed in the first episode.The problem here is that you're trying to use the variable named $t$ in the pseudocode as the subscript for $V$. To get a better idea of what's going on here, let's loop back to the previous page and examine the definition of the $n$-step return:$$G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}).$$Ok, we've got that. Now, let's take another look at the update rule in which we use this quantity:$$V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha \left[ G_{t:t+n} - V_{t+n-1} (S_t) \right].$$Ok. So $V_{t+n-1}$ appears three times in the update rule. Two times explicitly, estimating the value of $S_t$, and once more ""hidden"" in the definition of $G_{t:t+n}$, where it is used to estimate the value of $S_{t+n}$. Note very carefully what it is that this update rule is doing; it's updating the state estimate of $S_t$. If you now look at the pseudocode again, you'll see a comment on the line where $\tau$ is computed: ($\tau$ is the time whose state's estimate is being updated)!What this means, is that in the pseudocode, you should be using $\tau$ as the subscript for $V$! If you do that, it'll at least be correct for the first episode. In the pseudocode, the update rule looks like:$$V(S_{\tau}) \gets V(S_{\tau}) + \dots$$Plugging in the subscripts from the mathematical definition leads to:$$V_{\tau + n}(S_{\tau}) \gets V_{\tau + n - 1}(S_{\tau}) + \dots$$Since the pseudocode defines $\tau = t - n + 1$, we can substitute above:$$
\begin{aligned}
V_{t - n + 1 + n}(S_{t - n + 1}) &\gets V_{t - n + 1 + n - 1}(S_{t - n + 1}) + \dots\\
V_{t + 1}(S_{t - n + 1}) &\gets V_{t}(S_{t - n + 1}) + \dots
\end{aligned}$$ and now it should make sense again from a practical point of view. At every time step $t$, where $t$ measures number of steps of experience that we have simulated, we simply use the latest value function $V_t$ we have available at that time for bootstrapping in the update rule. When $t + 1 < n$, $S_{t - n + 1}$ is undefined. In these cases, the above update rule doesn't work, which makes sense intuitively because we have not yet progressed far enough into the episode to be capable of compute $n$-step returns."
What are methods human actors use to imitate robots?,"
Robot technology is usually thought from an engineering perspective. A human programmer writes a software this executed in a robot who is doing a task. 
But what would happen, if the project is started with the opposite goal? The idea is, that the human becomes the robot by himself. That means, the human is using makeup to make his face more mechanically, buys special futuristic clothing which mirrors the light and imitates in a roleplay the working of a kitchen robot.
What are methods human actors use to imitate robots?
","['social', 'human-like', 'mythology-of-ai']","The great acting teacher Stella Adler wrote about mannerisms being a powerful tool for actors. Method acting in general focuses on natural performances based roughly on understanding the mindset of the character portrayed.  It's possible actors who have portrayed androids have observed industrial robots to inform their physicality, and many performances convey the idea, via movement, of a mechanical inner structure.  (It is often said that an ""actor's body is their instrument"".)What is more interesting is actors trying to convey the cognitive structure of the androids.  With Arnold, and Terminator robots in general, the baseline performance is decidedly robotic, to convey their inhumanity. But the more advanced Terminators are able to mimic naturalistic human mannerisms, and even established human characters, to trick humans. Lieutenant Data often used head motions, such as cocking his head slightly, to convey computation.  Here the character arc involved working to become more human, as this character draws heavily on Pinocchio, the wooden puppet that became a boy.  Overall Data's performance conveyed a lack of emotion, a definite reference to the logic-oriented Mr. Spock, although I recall episodes where Data experimented with ""emotional circuits"" and ""humor circuits"", where the output was intentionally inconsistent with natural human behavior.Blade Runner, where the Tyrell Corporation's motto was ""More Human than Human"", presented the cutting edge Nexus-6 androids as having emotions, but, due to their artificially short life-spans, were portrayed as childlike in trying to reconcile extremely powerful feelings.  The Voight-Kampff Test, a form of Turing Test, used in the film to identify androids, relied on the emotional response to questions.  The key plot point of Do Androids Dream of Electric Sheep, the novel the film was based on, utilized what would be formalized as evolutionary game theory to hypothesize that empathy is a natural function of intelligence sufficiently advanced.  Deckard, who may or may not have been an android, and Rachel, who definitely was, are both capable of love.  This capacity informed their performances, to the extent that the androids came off as more human than the actual humans, due to the depth of their emotion.  This is also reflected in Blade Runner 2049 via the girlfriend-bot Joi, who us the most limited android, but the most human character in the film per her capacity to love (or at least simulate it.)In the recent HBO Westworld reboot, the Androids replicate natural human mannerisms when playing their designated roles, but reset to more mechanical mannerisms when acting under their own agency.  This is reflected in Ex Machina, where the android mimics human emotions to pass a Turing Test and trick the human subject, only to revert to purely alien mannerisms after the android is free.  (""Alien"" here used in the sense of non-human--it's possible the android is sentient as it seems to convey some degree of emotion in regarding the simulated human skin it will wear.)The most interesting recent android performance may come from the recent Alien: Covenant where Michael Fassbender plays two identical androids, David and Walter, which have two distinct neural structures.  (David has the capacity to be creative, where Walter cannot.  In the film it is mentioned that David made people uncomfortable, so the creative functions were removed from subsequent models.)  The key difference in the performance seems to be that David demonstrates passion, and even emotions, where Walter is more clearly ""robotic"".In general, the underlying approach of actors seems to have been to show the androids being distinct from humans, drawing a clear, though sometimes subtle, contrast.   Actors portraying androids have typically utilized robotic mannerisms to convey an artificial entity."
What is the formula used to calculate the loss in the FaceNet model?,"
The FaceNet model returns the loss of the predictions and ground-truth classes. How is this loss calculated?
","['neural-networks', 'objective-functions', 'facial-recognition']","The loss function used is the triplet loss function. 

Let me explain it part by part.The $f^a_i$ means the anchor input image. The $f^p_i$ means the postive input image, which corresponds to the same people as the anchor image. The $f^n_i$ corresponds to the negative sample, which is a different person(input image) then the anchor image. The first part, $||f^a_i - f^p_i||^2_2$ basically calculates the distance between the anchor image output features and the postive image output features, which you want the distance to be as small as possible as the input is the same person. For the second part, $||f^a_i - f^n_i||^2_2$ , it calculates the distance of the output features of the anchor image and the negative image. You wnat the distance to be as large as possible as they are not the same person. Finally, the $\alpha$ term is a constant(hyperparameter) that adds to the loss to prevent negative loss. The loss function optimizes for the largest distance between the anchor and negative sample and the smallest distance of the positive and anchor sample. It cleverly combines both metrics into one loss function. It can optimize for both case simultaneously in one loss function. If there is no negative sample, the model will not be able to differciate different person and vice versa.Hope I can help you and have a nice day!"
Can a Video Game Characters Behavior be directed by a NN?,"
So, Iâ€™m looking into some dynamic ways in which one can drive the behavior of a video game character. Specifically an NPC (Non playable character) that will be observable from the players point of view. Something Iâ€™d like to clarify from the start is what I mean by behavior. Since video games are understood visually, then I would qualify behavior to be anything visual, such as gestures, mannerisms or actions in any local space. 
Letâ€™s take a common archetype as an example. Weâ€™ll say we want the behavior of a villain. My first thought, was to use videos as training data. Videos of specific subjects or actors in a villainous scene (think Frankenstein, Dracula, Emperor Palpatine in Star Wars etc...) in hopes that an understanding of their mannerisms, body language and gestures could be captured and later applied to 3D animation dynamically.
I do understand that anything 3D typically requires rigging and animation. Iâ€™m currently not exactly sure how to marshal the data from one format (video analyses) to 3D animation. I thought Iâ€™d start working on the concept from high level first.
Any thoughts?
",['neural-networks'],
Convergence of semi-gradient TD(0) with non-linear function approximation,"
I am looking for a result that shows the convergence of semi-gradient TD(0) algorithm with non-linear function approximation for on-policy prediction. Specifically, the update equation is given by (borrowing notation from Sutton and Barto (2018))
$$\mathbf w \leftarrow \mathbf w +\alpha [R + \gamma \hat v(S', \mathbf w) - \hat v(S, \mathbf w)] \nabla \hat v(S, \mathbf w)$$
where $\hat v(S, \mathbf w)$ is the approximate value function parameterized by $\mathbf w$.
Sutton and Barto (2018) mention that the above update equation converges when $\hat v$ is linear in $\mathbf w$. But I couldn't find a similar result for non-linear function approximation. Any help would be greatly appreciated.
","['reinforcement-learning', 'convergence', 'function-approximation', 'temporal-difference-methods', 'on-policy-methods']",
"Which part of ""Perceptrons: An Introduction to Computational Geometry"" tells that a perceptron cannot solve the XOR problem?","
In the book ""Perceptrons: An Introduction to Computational Geometry"" by Minsky and Papert (1969), which part of this book tells that a single-layer perceptron could not solve the XOR problem?
I have been already scanned it, but I did not find the part. Or am I missing something?
","['machine-learning', 'perceptron', 'books', 'xor-problem']",
Is the Assumption-based Truth Maintenance System still used?,"
Is the Assumption-based Truth Maintenance System still used to maintain consistency while explicitly accounting for assumptions?
","['applications', 'knowledge-representation']",
Segmentation of a static object in a video,"
I've videos from a mounted camera on a helmet and the manually segmented labels (mask) of them.
The mask is valid through the entire video, only the scene vary. 
In different videos the camera is mounted differently on top of the helmet.
Things I've tried:

Training semantic segmentation on frame-mask pairs
Training semantic segmentation of concatenated frames with the mask.
Averaging consecutive frames and calculating the time-wise std of the pixels, and feeding the NN with this as an input.
ensembling (averaging) segmentation results from N frames
Usage of classic background subtraction techniques such as MOG2 (worse)

Although DNN models achieve 99% accuracy during training, in some of my test videos the model is missing large part of the helmet..
I'm certain this task can achieve ~100% accuracy even for never-before-seen examples. 
Do you have some ideas? 
","['computer-vision', 'image-segmentation']",
Have any AI's been able to decode human vision 'thoughts',"
I believe I saw an article about an AI that was able to decode human vision 'brain-waves' in real-time, which would create a blurry image of what the human was seeing.
This AI Decodes Your Brainwaves and Draws What You're Looking at
Is anyone aware where I can find this?
","['human-like', 'brain']","""Have any AI's been able to decode human vision 'thoughts'"" ~ Albert (Stack Exchange user, OP)This is technology that can produce pictures of what the user is thinking about through scanning a brain.""Is anyone aware where I can find this?"" ~ Albert (Stack Exchange user, OP)Emotiv is the most accessible commercial model (circa. late 2019).The OP is probably interested in consumer brainâ€“computer interfaces (also known as BCIs). These are varied technologies which range from:--This Wikipedia page, < https://en.wikipedia.org/wiki/Consumer_brain%E2%80%93computer_interfaces >, compares different models of BCIs.There also some very serious ethical issues regarding being able to ""read brains."" I mentioned the medical use; and I hope it goes in this direction.Deep philosophical discussions can be had on whether it is appropriate to read the brain of a supposed criminal. (I would personally say no.)(https://plato.stanford.edu/entries/neuroethics/)[I can't comment on the technical details of this. It is outside of my purview. For example, if you need information about the Python-BCI interface, you will need an expert.][This is not medical and/or legal advice. This is theoretical discussion.]"
Why is this ResNet50 misclassifying objects?,"
I'm new to Deep Learning, and I have some conceptual problems. I followed a simple tutorial here, and trained a model in Keras to do image classification on 10 classes of logos. I prepared 10 classes with each class having almost 100 images. My trained Resnet50 model performs exceptionally great when the image is one of those 10 logos, with 1.00 probability. But the problem is if I pass a non-logo item, a random image totally unrelated visually, still it marks it as one of those logos with close to 1.00 probability! 
I'm confused. Am I missing anything? Why is this happening? How to find a solution? I need to find logos in video frames. But right now, with a high possbility each frame is marked as a logo!
Here is my simple training code:
def build_finetune_model(base_model, dropout, fc_layers, num_classes):
    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = Flatten()(x)
    for fc in fc_layers:
        # New FC layer, random init
        x = Dense(fc, activation='relu')(x) 
        x = Dropout(dropout)(x)

    # New softmax layer
    predictions = Dense(num_classes, activation='softmax')(x) 
    finetune_model = Model(inputs=base_model.input, outputs=predictions)
    return finetune_model
finetune_model = build_finetune_model(base_model, dropout=dropout, fc_layers=FC_LAYERS, num_classes=len(class_list))
adam = Adam(lr=0.00001)
finetune_model.compile(adam, loss='categorical_crossentropy', metrics=['accuracy'])
filepath=""./checkpoints/"" + ""ResNet50"" + ""_model_weights.h5""
checkpoint = ModelCheckpoint(filepath, monitor=[""acc""], verbose=1, mode='max')
callbacks_list = [checkpoint]

history = finetune_model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, 
                                       steps_per_epoch=steps_per_epoch, 
                                       shuffle=True, callbacks=callbacks_list)

plot_training(history)

","['machine-learning', 'deep-learning', 'classification', 'image-recognition', 'keras']",
"How can I ""measure"" an object using Computer Vision techniques and neural networks?","
I would like to develop a neural network to measure the distance between two opposite sides of an object in an image (in a similar way that the fractional caliper tool measures an object).
So, given an image of an object, the neural network should produce the depth or height of the object.
Which computer vision techniques and neural networks could I use to solve this problem?
","['neural-networks', 'computer-vision', 'image-processing', 'algorithm-request']",
Do models train better if the labelling information is more specific (or dense)?,"
I'm working on a project where there is a limited dataset of videos (about 200). We want to train a model that can detect a single class in the videos. That class can be of multiple different types of shapes (thin wire, a huge area of the screen, etc).
There are three options on how we can label this data:

Image classification (somewhere in the image is this class)
Bounding box (in this area, there is the class)
Semantic segmentation (these pixels are the class)

My assumption is that if the model was trained on semantic segmentation data it would perform slightly better than bounding box data.  I'm also assuming it would perform way better than if the model only learned on image classification data.  Is that correct?
","['convolutional-neural-networks', 'image-recognition', 'object-detection', 'image-segmentation', 'data-labelling']",
Which AI technique is best suited to discovering non-linear relationships in data?,"
I am interested in exploring whether AI techniques can derive hidden patterns of relationships in a data set.  For example, from among house size, lot size, age of house and asking price, what formula best predicts selling price?
In explorations around how this might be done, I tried to use a neural network to solve for a predictable relationship between two variables to predict a third, so I trained my neural network with inputs consisting of the length of two sides of a triangle, and the result being the length of the hypotenuse.  It couldn't get it to work.
I was told by somebody who understands all this better than me that the reason it failed is because conventional neural networks are not good at modeling non-linear relationships.
If that is true, I wonder if there is some other AI technique that could 'derive' a network modeling the Pythagorean theorem from a training data set with better results than a normal neural network?
","['neural-networks', 'machine-learning', 'algorithm-request', 'non-linear-regression']","For example, from among house size, lot size, age of house and asking price, what formula best predicts selling price?There is no general formula for this. Search for neural network regression and you can get started. The AI technique or any prediction algorithm in general will learn a function that maps from the input feature vector $(x_1, ...,x_n)$, where each of the element in the vector is a measurement on the $\text{predictors/independent variables/regressors}$ to the $\text{variable of interest/dependent variables}$ i.e. $\text{selling price}$I was told by somebody who understands all this better than me that the reason it failed is because conventional neural networks are not good at modeling non-linear relationships.The statement is incorrect. In fact the opposite is true. CNNs are known for modeling non-linear relationships. Examples are the highly successful image classification CNN architectures like Inception, ResNet, etc."
What is the maximum number of dichotomies in a square?,"
I am new to machine learning. I am reading this blog post on the VC dimension.
$\mathcal H$ consists of all hypotheses in two dimensions $h: R^2 â†’ \{âˆ’1, +1 \}$, positive inside some square boxes and negative elsewhere.
An example.

My questions:

What is the maximum number of dichotomies for the 4 data points? i.e calculate mH(4)
It seems that the square can shatter 3 points but not 4 points. The $\mathcal V \mathcal C$ VC dimension of a square is 3. What is the proof behind this?

","['machine-learning', 'computational-learning-theory', 'vc-dimension']",
Is it possible to vectorise a CNN?,"
I am trying to write a CNN from scratch and am wondering if it is possible to vectorize the convolution step.
For example, if I had a dataset of 500 RGB images of size 32x32x3, and wanted the first convolutional layer to have 64 filters, how would I go about the vectorization of this layer?
Currently, I am running through all 500 images in a for loop, convoluting individually. I do this for all the images up to the flattening stage (where it essentially becomes a normal NN again), at which point I can implement the normal vectorisation approach to get to my output, etc.
A holistic overview of the process would be appreciated, as I am struggling to get my head around it and am struggling to find and information on the matter online.
","['neural-networks', 'convolutional-neural-networks', 'convolution']",
How can I test my trained network on the next unavailable hour?,"
I have data of 695 hours. I use the first 694 hours to train the network and I use 695th hour to validate it. Now my goal is to predict the next hour.
How I can use my trained network to predict the next hour, that is, the 696th hour (which I do not have access to)?
","['neural-networks', 'machine-learning', 'prediction', 'matlab', 'time-series']","You need to have access to the 696th hour (or successive hours), otherwise, you cannot test your model. An alternative would be, for example, to train your model on the first 693 hours, validate it on the 694th hour, and test it on the 695th hour."
TensorFlow 2.0 - Normalizing input to DNN (on structured data) [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



I have a structured dataset of around 100 gigs, and I am using DNN for classification in TF 2.0. Because of this huge dataset, I cannot load entire data in memory for training. So, I'll be reading data in batches to train the model.
Now, the input to the network should be normalized and for that, I need training dataset mean and SD. I have been reading TensorFlow docs to get info on how to normalize features when reading data in batches. But, couldn't find one. though I found this article, it is only for the case where entire data can be loaded in memory.
So, If any of you have worked on creating such a TensorFlow data pipeline for normalizing input features while loading data in batches and training model, It would be helpful.
","['neural-networks', 'deep-learning', 'tensorflow', 'datasets']",
Is batch normalization not suitable for non-gaussian input?,"
I generate some non-Gaussian data, and use two kinds of DNN models, one with BN and   the other without BN.
I find that the model DNN with BN can't predict well.  
The codes is shown as follow:
import numpy as np
import scipy.stats
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense,Dropout,Activation, BatchNormalization

np.random.seed(1)

# generate non-gaussian data
def generate_data():
    distribution = scipy.stats.gengamma(1, 70, loc=10, scale=100)
    x = distribution.rvs(size=10000)
    # plt.hist(x)
    # plt.show()
    print ('[mean, var, skew, kurtosis]', distribution.stats('mvsk'))

    y = np.sin(x) + np.cos(x) + np.sqrt(x)
    plt.hist(y)
    # plt.show()
    # print(y)
    return x ,y 

x, y = generate_data()

x_train = x[:int(len(x)*0.8)]
y_train = y[:int(len(y)*0.8)]
x_test = x[int(len(x)*0.8):]
y_test = y[int(len(y)*0.8):]


def DNN(input_dim, output_dim, useBN = True):
    '''
    å®šä¹‰ä¸€ä¸ªDNN model
    '''
    model=Sequential()

    model.add(Dense(128,input_dim= input_dim))
    if useBN:
        model.add(BatchNormalization())
    model.add(Activation('tanh'))
    model.add(Dropout(0.5))

    model.add(Dense(50))
    if useBN:
        model.add(BatchNormalization())
    model.add(Activation('tanh'))
    model.add(Dropout(0.5))

    model.add(Dense(output_dim))
    if useBN:
        model.add(BatchNormalization())
    model.add(Activation('relu'))

    model.compile(loss= 'mse', optimizer= 'adam')
    return model

clf = DNN(1, 1, useBN = True)
clf.fit(x_train, y_train, epochs= 30, batch_size = 100, verbose=2, validation_data = (x_test, y_test))

y_pred = clf.predict(x_test)
def mse(y_pred, y_test):
    return np.mean(np.square(y_pred - y_test))
print('final result', mse(y_pred, y_test))

The input x is like this shape:

If I add BN layers, the result is shown as follows:
Epoch 27/30
 - 0s - loss: 56.2231 - val_loss: 47.5757
Epoch 28/30
 - 0s - loss: 55.1271 - val_loss: 60.4838
Epoch 29/30
 - 0s - loss: 53.9937 - val_loss: 87.3845
Epoch 30/30
 - 0s - loss: 52.8232 - val_loss: 47.4544
final result 48.204881459013244

If I don't add BN layers, the predicted result is better:
Epoch 27/30
 - 0s - loss: 2.6863 - val_loss: 0.8924
Epoch 28/30
 - 0s - loss: 2.6562 - val_loss: 0.9120
Epoch 29/30
 - 0s - loss: 2.6440 - val_loss: 0.9027
Epoch 30/30
 - 0s - loss: 2.6225 - val_loss: 0.9022
final result 0.9021717561981543

Anyone knows the theory about why BN is not suitable for non-gaussian data ?
","['neural-networks', 'deep-learning', 'deep-neural-networks', 'batch-normalization']",
How is the rollout from the MCTS implemented in both of the AlphaGo Zero and the AlphaZero algorithms?,"
In the vanilla Monte Carlo tree search (MCTS) implementation, the rollout is usually implemented following a uniform random policy, that is, it takes random actions until the game is finished and only then the information gathered is backed up.
I have read the AlphaZero paper (and the AlphaGo Zero too) and I didn't find any information on how the rollout is implemented (maybe I missed it).
How is the rollout from the MCTS implemented in both the AlphaGo Zero and the AlphaZero algorithms?
","['monte-carlo-tree-search', 'alphazero', 'implementation', 'alphago-zero']",
"What does ""immediate vector-valued feedback"" mean?","
In the book Artificial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep Learning, James Stone says

With supervised learning, the response to each input vector is an output vector that receives immediate vector-valued feedback specifying the correct output, and this feedback refers uniquely to the input vector just received; in contrast, each reinforcement learning output vector (action) receives scalar-valued feedback often sometime after the action, and this feedback signal depends on actions taken before and after the current action.

I fail to understand the part formatted in bold. Once we have a set of labeled examples (feature vector and label pairs), where is the ""feedback"" coming from? Testing and validation results of our calibrated model (say a neural network based one)? 
","['machine-learning', 'terminology', 'supervised-learning']","By ""immediate vector-valued feedback"", they probably mean exactly the label in the ""labeled examples"" you mentioned."
Dataset for floating objects detection [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.







                        Improve this question
                    



I am looking for a dataset, which I could train a model to detect people/boats/surfboards, etc., from a drone view.
Has anyone seen a dataset that could be useful for this purpose?
I have some photos made by me (like this one below), but I need more data. Of course, the best will be if data will be labeled, but, if someone has seen an unlabeled dataset with videos/photos like that below, please share the link to it.
Sample photos I am looking for:


","['deep-learning', 'datasets']","If someone will be looking for a dataset for maritime SAR (Search and Rescue) purposes in the future, we have created the first publicly free to use for academic research dataset of this type:
http://afo-dataset.pl/en/download/"
Why use a recurrent neural network over a feedforward neural network for sequence prediction?,"
If recurrent neural networks (RNNs) are used to capture prior information, couldn't the same thing be achieved by a feedforward neural network (FFNN) or multi-layer perceptron (MLP) where the inputs are ordered sequentially?
Here's an example I saw where the top line of each section represents letters typed and the next row represents the predicted next character (red letters in the next row means a confident prediction).

Wouldn't it be simpler to just pass the $X$ number of letters leading up to the last letter into an FFNN?
For example, if $X$ equaled 4, the following might be the input to the FFNN
S, T, A, C => Prediction: K

","['deep-learning', 'recurrent-neural-networks', 'feedforward-neural-networks', 'multilayer-perceptrons', 'long-short-term-memory']","Different model structures encode different assumptions - while we often make simplifying assumptions that aren't strictly correct, some assumptions are more wrong than others.For example, your proposed structure of ""just pass the $X$ number of letters leading up to the last letter into an FFNN"" makes an assumption that all the information relevant for the decision is fully obtainable from the $X$ previous letters, and $(X+1)$st and earlier input letters are not relevant - in some sense, an extension of the Markov property. Obviously, that's not true in many cases, there are all kinds of structures where long term relationships matter, and assuming that they don't lead to a model that intentionally doesn't take such relationships into account. Furthermore, it would make an independence assumption that the effect of $X$th, $(X-1)$st and $(X-2)$nd elements on the current output is entirely distinct and separate, you don't make an assumption that those features are related, while in most real problems they are.The classic RNN structures also make some implicit assumptions, namely, that only the preceding elements are relevant for the decision (which is wrong for some problems, where information from the following items is also required), and that the transformative relationship between the input, output and the passed-on state is the same for all elements in the chain, and that it doesn't change over time; That's also not certainly true in all cases, this is quite a strong restriction, but that's generally less wrong than the assumption that the last $X$ elements are sufficient, and powerful true (or mostly true) restrictions are useful (e.g. the No Free Lunch Theorem applies) for models that generalize better; just like e.g. enforcing translational invariance for image analysis models, etc."
What is the difference between the ant system and the max-min ant system?,"
I'm studying ant colony optimization. I'm trying to understand the difference between the ant system (AS) and the max-min ant system (MMAS) approaches. As far as I found out, the main difference between these 2 is that in AS the pheromone trail is updated after all ants have finished the tour (it means all ants participate in this update), but in MMAS, only the best ant updates this value. Am I right? Is there any other significant difference?
","['comparison', 'swarm-intelligence', 'ant-colony-optimization']",
"Has machine learning been combined with logical reasoning (for example, PROLOG)?","
There are mainly two different areas of AI at the moment. There is the ""learning from experience"" based approach of neural networks. And there is the ""higher logical reasoning"" approach, with languages like LISP and PROLOG.
Has there been much overlap between these? I can't find much!
As a simple example, one could express some games in PROLOG and then use neural networks to try to play the game.
As a more complicated example, one would perhaps have a set of PROLOG rules which could be combined in various ways, and a neural network to evaluate the usefulness of the rules (by simulation). Or even create new PROLOG rules.  (Neural networks have been used for language generation of a sort, so why not the generation of PROLOG rules, which could then be evaluated for usefulness by another neural network?)
As another example, a machine with PROLOG rules might be able to use a neural network to be able to encode these rules into some language that could be in turn decoded by another machine. And so express instructions to another machine.
I think, such a combined system that could use PROLOG rules, combine them, generate new ones, and evaluate them, could be highly intelligent. As it would have access to higher-order logic. And have some similarity to ""thinking"".
","['neural-networks', 'machine-learning', 'reference-request', 'prolog', 'neurosymbolic-ai']",
The best way of classifying a dataset including classes with high similarity? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I have a dataset which has two very similar classes (men wrestling, women wrestling). I've used InceptionV3 as a classifier to solve the problem of classifying this dataset. Unfortunately, the accuracy of this classifier doesn't hit more than 70%. Is there any suggestion about how I can overcome this problem or any other similar problems?
","['classification', 'tensorflow', 'keras']",
Should I use my redundant feature as an auxiliary output or as another input feature?,"
For example, given a face image, and you want to predict the gender. You also have age information for each person, should you feed the age information as input or should you use it as auxiliary output that the network needs to predict?
How do I know analytically (instead of experimentally) which approach will be better? What is the logic behind this?
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'feature-selection']","For extra input that does not matter, you should not input it to the network.Feature selection, the process of finding and selecting the most
  useful features in a dataset, is a crucial step of the machine
  learning pipeline. Unnecessary features decrease training speed,
  decrease model interpretability, and, most importantly, decrease
  generalization performance on the test set.Source: A Feature Selection Tool for Machine Learning in PythonAs the source says, unnecessary features decreases accuracy and training speed. Moreover, they have no mapping to the labels, so they won't end up being used. They are unnecessary and adding them will only cause you trouble. Hope this helps you and have a nice day!"
Analyzing vibration using machine learning,"
I would like a few suggestions on an idea that I have -
I am trying to make a musical instrument (percussion), whilst just having a PVC disc. I am hitting the disc in a variety of styles (in order to produce a variety of sounds correspondingly), just like the way the actual percussion instrument is hit. I am converting the mechanical vibrations on the PVC disc to an electrical signal using a transducer, performing an FFT analysis of the different strokes, and trying to identify the stroke which is hit. Using this technique, I could get an accuracy of only 80 percent. I would like it to be extremely accurate ( more than 95 percent recognition). I was using only frequency as the parameter used to distinguish the sounds.
Now, I am thinking that if I could use other parameters too in order to identify the stroke, I might be able to get the required accuracy. I am thinking of resorting to Machine Learning for this. I am kind of new to this and would like to know what I might need to know before I proceed with this idea.
Any help would be greatly appreciated.
","['machine-learning', 'signal-processing']",
How does the automated temperature adjustment step work in Soft Actor-Critic?,"
In section 5 of the paper Soft Actor-Critic Algorithms and Applications, it is proposed an optimization problem to obtain an optimal temperature parameter $\alpha^*_t$. First, one uses the original evaluation and improvement steps to estimate $Q_t^*$ and $\pi_t^*$, and then one somehow solves the optimization problem:
$$\alpha_t^* = \arg\min_{\alpha_t} \mathbb E _{a_t\sim\pi^*}\left[\alpha_t(-\log\pi_t^*(a_t|s_t;\alpha_t)-H)\right]\text .$$
As far as I understand, we should use our current estimate of $\pi_t^*$ to solve that problem. Since it was obtained from a previous $\alpha_{t-1}^*$, in practice, it is not dependent on $\alpha_t$ and so the optimization problem becomes a linear problem with the only restriction being $\alpha_t\geq0$.
Here comes my problem: under this rationale, if $\alpha_t$ is a scalar independent of both state $s_t$ and action $a_t$, the value of the cost function is just proportional to $\alpha_t$ and so the solutions are either $0$ or $\infty$, depending on the sign of the expected value (something similar happens if $\alpha_t^*=\alpha_t^*(s_t,a_t)$). However, the whole idea of introducing this parameter is to account optimally for the exploration of the policy.
What is the correct way to solve this optimization problem along with the evaluation and improvement steps? I am particularly interested in the tabular case. Also, is there any explanation why they use a negative minimum entropy $H$ when the entropy is always positive?
By the way, in the approximate case, the current official implementation seems to be doing just that: moving $\alpha_t^*$ up or down a little bit (closer to $\infty$ or 0, respectively), depending on the magnitude of the expected value. I guess one could do the same for the tabular case, modifying the $\alpha_t^*$ only a little bit in each step, but this seems rather suboptimal.
","['reinforcement-learning', 'optimization', 'actor-critic-methods', 'dynamic-programming', 'soft-actor-critic']",
Why are researchers focused on deep learning based stereo depth/disparity methods instead of non deep learning ones?,"
In recent years if you are working on stereo depth/disparity algorithms, it seems like you will only ever get your paper accepted to CVPR/ICCV/ECCV if there's some deep learning involved in it. A lot of authors published their code on github and I've tried out multiple of them and here is what I observed. None of these deep learning based methods generalized well. Almost all methods trained on the KITTI dataset (street images) or the scene flow dataset (synthetic images). These methods perform well when the test data is similar to the training data, but fails miserably on other kinds of test data (e.g. close up human) whereas a classical traditional computer vision based method like PatchMatch would generate decent results. In my opinion, no matter how well these new deep learning methods perform on the KITTI benchmark, it's nearly useless in the real world. 
I understand deep learning has the potential to approximate any non-linear function when there's enough quality training data and unlimited computation, but ground truth depth/disparity cannot be labeled by manual labor like a cat-dog classification problem. That means the ground truth training data has to come from traditional computer vision algorithms or hardware or be synthetic. Traditional computer vision algorithms are not even close to perfect yet but the research pretty much stifled because of deep learning. The ground truth of the KITTI dataset comes from a hardware LIDAR, but it's extremely sparse. If we align multiple scans from LIDAR in order to form a dense result, that's relying on some type of SLAM which again is relying on an imperfect traditional computer vision algorithm. There is no sign of hardware that can generate accurate dense depth that is coming out soon. As for synthetic data, it doesn't accurately represent real data. Since there isn't even a good way to obtain training data for stereo depth/disparity, why are the researchers so fixated on building complex deep neural nets to solve stereo depth/disparity nowadays?
",['deep-learning'],
What is a good language for expressing replacement or template rules?,"
Say I have a game like tic-tac-toe or chess. Or some other visual logic based problem. 
I could express the state of the game as a string. (or perhaps a 2D array)
I want to be able to express the possible moves of the game as rules which change the string. e.g. replacement rules. 
I looked into regex as a possibility but this doesn't seem powerful enough. For example, one can't have named patterns which one can use again. (e.g. if I wanted to name a pattern called ""numbers_except_for_8"". To be used again.
And it also should be able to express things like ""repeat if possible"".
In other words I need some simple language to express rules of a game that has:

modularity
simpleness
can act on other rules (self referential)

There are languages like LISP but these on the other hand seem too complicated. (Perhaps there is no simple language hence why the English language is so complicated).
I did read once about a generalised board game solving software program which had a way to express the rules of a game. But I can't seem to find a reference to it anywhere.
As an example rules for tic tac toe might be:
Players-Turn:
""Find a blank square""->""Put an X in it""->Oponent's turn
Oponents-Turn:
""Find a blank square""->""Put an O in it""->Player's turn
So I think the ingredients for rules are: searching for patterns, determining if an object is of a particular type (which might be the same as the first ingredient), and replacing.
","['programming-languages', 'meta-rules']",
Who is working on explaining the knowledge encoded into machine learning models? [duplicate],"







This question already has answers here:
                                
                            




Which explainable artificial intelligence techniques are there?

                                (3 answers)
                            

Closed 2 years ago.



The thing about machine learning (ML) that worries me is that ""knowledge"" acquired in ML is hidden: we usually can't explain the criteria or methods used by the machine to provide an answer when we ask it a question.
It's as if we asked an expert financial analyst for advice and he/she replied, ""Invest in X""; then when we asked ""Why?"", the analyst answered, ""Because I have a feeling that's the right thing for you to do.""  It makes us dependent on the analyst.
Surely there are some researchers trying to find ways for ML systems to encapsulate and refine their ""knowledge"" into a form that can then be taught to a human or encoded into a much simpler machine.  Who, if any, are working on that?
","['machine-learning', 'reference-request', 'knowledge-representation', 'explainable-ai']",
What is the meaning of the square brackets in ant colony optimization?,"
I'm studying the paper ""Minimizing Total Tardiness on a Single Machine Using Ant Colony Optimization"" which has proposed to use Ant colony optimization to SMTWTP.
According to this paper:

Each artificial ant iteratively and independently decides which job to
append to the sub-sequence generated so far until all jobs are
scheduled, Each ant generates a complete solution by selecting a job $j$
to be on the $i$-th position of the sequence. This selection process is
influenced through problem-specific heuristic information called
visibility and denoted by $\eta_{ij}$ as well as pheromone trails denoted by $\tau_{ij}$. The former is an indicator of how good the choice of that job
seems to be and the latter indicates how good the choice of that job
was in former runs. Both matrices are only two dimensional as a
consequence of the reduction in complexity

They have proposed this formula for the probability that job $j$ be selected to be processed on position $i$ (page 9 of the linked paper):
$$
\mathcal{P}_{i j}=\left\{\begin{array}{cl}
\frac{\left[\tau_{i j}\right]^{\alpha}\left[\eta_{i j}\right]^{\beta}}{\sum_{h \in \Omega}\left[\tau_{i h}\right]^{\alpha}\left[\eta_{i h}\right]^{\beta}} & \text { if } j \in \Omega \\
0 & \text { otherwise }
\end{array}\right.\tag{1}\label{1}
$$
but I can't understand what $[]$ surrounding $\eta_{ij}$ and $\tau_{ij}$ indicates. Does it show that these values are matrices?
","['papers', 'swarm-intelligence', 'notation', 'ant-colony-optimization']","The square brackets $[]$ in $[\tau_{ij}]^\alpha$ and $[\eta_{ij}]^\beta$ may be just a way of emphasing that the elements $\tau_{ij} \in \mathbb{R}$ and $\eta_{ij} \in \mathbb{R}$ of respectively the matrices $\mathbf{\tau} \in \mathbb{R}^{n \times n}$ and $\mathbf{\eta} \in \mathbb{R}^{n \times n}$ (where $n$ is the number of nodes in the graph) are respectively raised to $\alpha$ and $\beta$, so they could have used also other type of brackets, for example, $()$. It may also be a way of indicating that $[\tau_{ij}]$ and $[\eta_{ij}]$ are $1 \times 1$ matrices or vectors that contain respectively the scalars $\tau_{ij}$ and $\eta_{ij}$, so you are multiplying matrices or vectors (dot product). This notation is also used in the paper that introduced the ant colony system (ACS) (and it is probably used in many other papers related to ant colony optimization). See equation 1 of Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem (1997) by Dorigo and Gambardella."
Best way to train Neural Network for Voice Commands?,"
I want to build a Voice Assistance using Tensorflow, Like Google Assistance, So that I can give commands like:
Open Camera
Send Message
Play Music
ETC

I know I can use pre-trained model for Voice Recognition but this is not my problem. If I know correctly, What Neural Network does it learn from your input and output and creates a best algorithms. 
So I want to know Is it possible to somehow train my network for my commands so that I don't have to HARD-CODED them because it is hard to remember so many command?
Is Google also Hard-Coded the commands for Google Voice Assistance?
Sorry, If I'm unable to explain this to you :P
",['machine-learning'],
CNN multi output scores and evaluation,"
I am building a CNN with two outputs. I still have to put time in the network itself, but I was trying to get a good evaluation/classification report of the results. My code is the following:
scores = model.evaluate(data_test, [Y1_test, Y2_test], verbose=0)

for i, j in zip(model.metrics_names, scores):
    print(i,'=', j)

Output:
loss = 5.124477842579717
Y1_output_loss = 1.3782909
Y2_output_loss = 4.10769
Y1_output_accuracy = 0.6304348
Y2_output_accuracy = 0.54347825

Not great, but that is not the point. My code for the classification repot is as follows:
Y1_pred, Y2_pred = model.predict(data_test)
Y1_true, Y2_true = Y1_test.argmax(axis=-1), Y2_test.argmax(axis=-1)
Y1_pred, Y2_pred = Y1_pred.argmax(axis=-1), Y2_pred.argmax(axis=-1)


print(classification_report(Y1_true, Y1_pred))
print(classification_report(Y2_true, Y2_pred))

Output:
Classification report Y1
              precision    recall  f1-score   support

           0       0.20      0.33      0.25         6
           3       0.00      0.00      0.00         3
           6       0.00      0.00      0.00         6
           8       0.00      0.00      0.00         2
           9       0.00      0.00      0.00         7
          10       0.03      0.50      0.06         2
          11       0.00      0.00      0.00         3
          12       0.00      0.00      0.00         7
          13       0.00      0.00      0.00         2
          14       0.00      0.00      0.00         7
          15       0.00      0.00      0.00         1

    accuracy                           0.07        46
   macro avg       0.02      0.08      0.03        46
weighted avg       0.03      0.07      0.04        46


Classification report Y2
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         9
           2       0.00      0.00      0.00        10
           3       0.15      1.00      0.26         7
           4       0.00      0.00      0.00         9
           5       0.00      0.00      0.00         6
           6       0.00      0.00      0.00         2
           7       0.00      0.00      0.00         3

    accuracy                           0.15        46
   macro avg       0.02      0.14      0.04        46
weighted avg       0.02      0.15      0.04        46

Now the average accuracy is extremely low suddenly, so I have the feeling it isn't lining up correctly. But I don't see where? 
Thank you all
","['convolutional-neural-networks', 'classification', 'prediction']",
Would models like U-Net be able to segment objects which has label based on its surrounding context?,"
Suppose that we want to segment a red blob from the image, normally you will have a class for this red blob e.g. 0. And every red blob you detected will have a class of 0.
But, in my case, I want that the model will look at the surrounding context, e.g., if the red blob is surrounded by blue blobs, it should be classified as class 1, instead of 0. Like the following image.

Is this something easily achieve able with U-Net or other models (you can suggest)?
In my case, the context can be more difficult than this, e.g., if there are blue and green surrounding you, you will have another class.
","['neural-networks', 'convolutional-neural-networks', 'u-net']",
How to use TPU for real-time low-latency inference?,"
I use Google's Cloud TPU hardware extensively using Tensorflow for training models and inference, however, when I run inference I do it in large batches. The TPU takes about 3 minutes to warm up before it runs the inference. But when I read the official TPU FAQ, it says that we can do real-time inference using TPU. It says the latency is 10ms which for me is fast enough but I cannot figure out how to write code that does this, since every time I want to pass something for inference I have to start the TPU again.
My goal is to run large Transformer-based Language Models in real-time on TPUs. I guessed that TPUs would be ideal for this problem. Even Google seems to already do this.
Quote from the official TPU FAQ:

Executing inference on a single batch of input and waiting for the
  result currently has an overhead of at least 10 ms, which can be
  problematic for low-latency serving.

","['natural-language-processing', 'tensorflow', 'transformer', 'google', 'inference']",
"What does ""class-level discriminative feature representation"" mean in the paper ""Semi-Supervised Deep Learning with Memory""?","
I am reading the paper Semi-Supervised Deep Learning with Memory (2018) by Yanbei Chen et al.  The topic is the classification of images using semi-supervised learning. The authors use a term on page 2 in the middle of the page that I am not familiar with. They write:

The key to our framework design is two-aspect: (1) the class-level discriminative feature representation and the network inference uncertainty are gradually accumulated in an external memory module; (2) this memorised information is utilised to assimilate the newly incoming image samples on-the-fly and generate an informative unsupervised memory loss to guide the network learning jointly with the supervised classification loss

I am not sure what the term discriminative feature representation means.
I know that a discriminative model determines the decision boundary between the classes, and examples include: Logistic Regression (LR), Support Vector Machine (SVM), conditional random fields (CRFs) and others.
Moreover, I know that, in machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.
Any insights on the definition of this term much appreciated.
","['terminology', 'papers', 'representation-learning', 'semi-supervised-learning', 'discriminative-model']","Discriminative models give the probability of an element in the feature space $x \in X$ belonging to a class $y \in Y$, i.e. $p(Y|X)$, where $Y$ is the set of classes in a classification problem.The discriminative feature representation in this context means the feature map/s  which are the output/s of the convolutional layers of the backbone (the convolutional layers of the neural network) which (presumably) differ on the basis of their class, and can be used to discriminate which class the original image belongs to, in the case of image classification."
Which hyper-parameters are considered in neural architecture search?,"
I want to understand automatic Neural Architecture Search (NAS). I read already multiple papers, but I cannot figure out what the actual search space of NAS is / how are classical hyper-parameters considered in NAS?
My understanding:
NAS aims to find a good performing model in the search space of all possible model architectures using a certain search- and performance estimation strategy.
There are architecture-specific hyper-parameters (in the most simple feed-forward network case) like the number of hidden layers, the number of hidden neurons per layer as well as the type of activation function per neuron
There are classical hyper-parameters like learning rate, dropout rate, etc.
What I don't understand is:
What exactly is part of the model architecture as defined above? Is it only the architecture-specific hyper-parameters or also the classical hyper-parameters? In other words, what is spanning the search space in NAS: Only the architecture-specific hyper-parameters or also the classical hyper-parameters?
In case only the architecture-specific hyper-parameters are part of the NAS search space, what about the classical hyper-parameters? A certain architecture (with a fixed configuration of the architecture-specific hyper-parameters) might perform better or worse depending on the classical hyper-parameters - so not taking into account the classical hyper-parameters in the NAS search space might result in a non-optimal ultimate model architecture, or not?
","['neural-networks', 'machine-learning', 'hyper-parameters', 'neural-architecture-search']",
How to assign rewards in a non-Markovian environment?,"
I am quite new to the Reinforcement Learning domain and I am curious about something. It seems to be the case that the majority of current research assumes Markovian environments, that is, future states of the process depend only upon the present state, not on the sequence of events that preceded it. I was curious about how we can assign rewards when the Markovian property doesn't hold anymore. Do the state-of-the-art RL theory and research support this? 
","['reinforcement-learning', 'rewards', 'markov-decision-process', 'environment', 'markov-property']","Dealing with a Non-Markovian process is unusual in Reinforcement Learning. Although some explicit attempts have been made, the most common approach when confronted with a non-Markovian environment is to try and make the agent's representation of it Markovian. After reducing Agent's model of the dynamics to a Markovian process, rewards are assigned from the environment in exactly the same way as before. The environment simply sends the agent a reward signal in response to each action.The Markovian assumption is essentially a formalism of the idea that the future can be predicted from the present. It says that if you know the dynamics of a system, and you know the state of the system now, you know everything you need to predict the state of the system later, and how we got to this state is not important. Formally, we write this as $P(s_tâˆ£s_{tâˆ’1:0})=P(s_tâˆ£s_{tâˆ’1})$.That said, the models we use in AI are usually simplifications of the real world. When we simplify the world, we can introduce non-Markovian dynamics. However, if the model grows too complex, and the state space too large, learning will take too long. The goal is then to define a state space that is small enough to be learnable, and not too bad an approximation of the real dynamics of the world. AI researchers have several tools to do this.As a working example, imagine that the future position of a robot depends mainly on the current position, and current velocity, along with the action the robot takes right now. Using these variables to define a state, we get almost Markovian dynamics, but as the robot moves over time, its battery drains and the movements become very slightly more imprecise. If we wanted to remove this error, we can:As an example, consider the process of setting the price of a good. The agent's reward is non-Markovian, because sales increase or decline gradually in response to price changes. However, they don't depend on all of the history of prices. Imagine that instead they depend on the last 5 prices (or the last k). We can use technique (2) above to expand the agent's model of what a state is. The now the agent learns that when prices have been $p_{t-1:t-5}$ in the last 5 steps, and it sets the price at time $t$ to some value, it's reward is $x$. Since the reward depends only on the prices now, and in the last 5 steps, the agent is now learning a Markovian process, even though the original process is non-Markovian, and the reward function is non-Markovian. No changes are made to the reward function, or the environment, only the agent's model of the environment."
How can we print weights per iteration in a simple feed forward MLP for an specific class?,"
im working on a project in which I have to make a multi-layer perceptron with two hidden layers with 3 nodes in each. The target value in my data contains 8 unique values/classes. One of the tasks states ""For the most popular class CYT plot weight values per iteration for the last layer (3 weights and bias)"".  My question is ""does this statement make sense""?  I can access the weights and biases of a layer but I don't get what are  weight values for a specific class and how to access them
","['deep-learning', 'tensorflow', 'keras', 'feedforward-neural-networks', 'multilayer-perceptrons']","A common model used for this kind of classification task is to have one output neuron per class. So, for example, neuron 1 may have a loss function that is related to outputting ""1"" for examples of class 1, and ""0"" for examples of other classes. Neuron 2 may be asked to do the same, but for class 2 rather than class 1.If you use a model of this kind, you can pull the weights for each neuron in the final output layer. It sounds like this is what you are being asked to plot."
How to handle classification with label updates?,"
Suppose that my task is to label news articles; that is, to classify which category a news article belongs to. Using the labelled data (with old labels) that I have, I have trained a model for this.
For relevancy purposes, certain labels may be split into multiple new labels. For example, 'Sports' may split into 'Sports' and 'E-Sports'. Because of these new labels, I will need to retrain my model. However, my training data is labelled with the old labels. What can I do to address these 'label updates'?
My idea: Perhaps use some unsupervised clustering method (K-means?) to split the data with the old labels into the new labels. (But how can we be certain that which cluster has what new label?) Then use this 'updated' data to train a model. Is this correct?
","['machine-learning', 'classification']",
Three step threshold in Facenet model of face recogniton,"
Suppose i trained the images of two people say Bob , Thomas .When i run the algorithm to detect the face of a totally different person from these two say John , then John is recognized as Bob or Thomas.How to avoid this ?
I am studying a face recognition model on GitHub(link) which uses Facenet model. Problem is when an unknown image (the image which is not in training data set) is given to identify , it identifies the unknown person as one the person in the data set .I searched on web and i found i need to increase threshold value .I guess i need to increase the threshold. But when i am increasing the threshold value to 0.99,0.99,99 then only it is rejecting the unknown image (image of the person who is not in data set) and sometimes even rejecting the image of person who is in dataset.
I guess by increasing the threshold value what we are assuring is that an image is classified as one of the person in training data only when they are close enough.
How to make changes so that the model works properly ?And can someone explain Threshold in Facenet model better.
","['neural-networks', 'machine-learning']","The problem originated because of the nature of the code. Code:
https://github.com/AISangam/Facenet-Real-time-face-recognition-using-deep-learning-Tensorflow/blob/master/classifier.pyAs you see the code uses a SVC (Support Vector Classifier) to classify the classes. The SVC (or SVM) does not have an extra class for unknown class.For the threshold variable, it is used in face detection, aka drawing a bounding box around the face for FaceNet to classify it. Code:https://github.com/AISangam/Facenet-Real-time-face-recognition-using-deep-learning-Tensorflow/blob/master/identify_face_image.pyAs you can see, the threshold variable is only used in detecting the bounding box.Code for getting class name:You can see that no unknown class can be found. You can try adding another threshold value and check if the predictions maximum value is lower than the threshold value. I have little experience in tensor flow so this is just a proof of concept, not sure if it will work.By the way, because of the nature of triplet loss, you don't have to add and extra class to the SVC/SVM as the embedding model is locked and not trained, so unknown class embeddings will be very different to the known class. However you can try either approach.Hope it can help you can have a nice day!"
Positioning of batch normalization layer when converting strided convolution to convolution + blurpool,"
I'm trying to replace the strided convolutions of Keras' MobileNet implementation with the ConvBlurPool operation as defined in the Making Convolutional Networks Shift-Invariant Again paper. In the paper, a ConvBlurPool is implemented as follows:
$$
Relu \circ Conv_{k,s} \rightarrow Subsample_s \circ Blur_m \circ Relu \circ Conv_{k,1}
$$
where k is the convolution's output kernels, s is the stride, m is the blurring kernel size and the subsample+blur is implemented as a strided convolution with a constant kernel.
My issues start when batch normalization enter the picture.
In MobileNet, a conv block is defined as follows (omitting the zero-padding):
$$
Relu \circ BatchNorm \circ Conv_{k,s}
$$
I am leaning towards converting it to:
$$
Subsample_s \circ Blur_m \circ Relu \circ BatchNorm \circ Conv_{k,1}
$$
i.e., putting the BN before the activation as it's normally done. This is not equivalent though, because the first BN operates on the downsampled signal.
Another possibility would be:
$$
BatchNorm \circ Subsample_s \circ Blur_m \circ Relu \circ Conv_{k,1}
$$
with the BN as last operation. This is also not equivalent, because now the BN comes after the ReLu.
Is there any reason to prefer one option over the other? Are there any other options I'm not considering?
","['deep-learning', 'convolutional-neural-networks']","After finding the paper authors' Github, I saw that, although they only have a MobileNet V2 model implemented, they choose the Subsample-after-ReLu option (the first one in the question).
Although this doesn't fully answer my question, I'll take ""the paper authors do it this way"" as enough reason to prefer this over the alternative."
Is a VGG-based CNN model sometimes better for image classfication than a modern architecture?,"
I have an image classification task to solve, but based on quite simple/good terms:

There are only two classes (either good or not good)
The images always show the same kind of piece (either with or w/o fault)
That piece is always filmed from the same angle & distance
I have at least 1000 sample images for both classes

So I thought it should be easy to come up with a good CNN solution - and it was. I created a VGG16-based model with a custom classifier (Keras/TF). Via transfer learning I was able to achieve up to 100% validation accuracy during model training, so all is fine on that end.
Out of curiosity and because the VGG-based approach seems a bit ""slow"", I also wanted to try it with a more modern model architecture as the base, so I did with ResNet50v2 and Xception. I trained both similar to the VGG-based model, tried it with several hyperparameter modifications, etc. However, I was not able to achieve a better validation accuracy than 95% - so much worse than with the ""old"" VGG architecture.
Hence my question is: 

Given these ""simple"" (always the same) images and only two classes, is the VGG model probably a better base than a modern network like ResNet or Xception? Or is it more likely that I messed something up with my model or simply got the training/hyperparameters not right?

","['neural-networks', 'deep-learning', 'convolutional-neural-networks', 'residual-networks', 'vgg']","VGG is a more basic architecture which uses no residual blocks. Reset usually perform better then VGG due to it's more layers and residual approach. Given that resnet-50 can get 99% accuracy on MNIST and 98.7% accuracy on CIFAR-10, it probably should achieve better than VGG network. Also, the validation accuracy should not be 100%. You could try increasing the size of your validation set to improve accuracy on validation. VGG network should perform worst than ResNet in most scenario, but experimenting is the way to go. Try and experiment more to get a method that works for your data. Hope that I can help you and have a nice day!"
Neural network for reinforcement learning,"
Iâ€™m using a simple neural network to solve a reinforcement learning problem.
The configuration is:
X-inputs: The current state
Y-outputs: The possible actions
Whenever the network yields a â€œgoodâ€ solution, i â€œrewardâ€ the network by training it a number of times.
Whenever the network yields a â€œbadâ€ or â€œneutralâ€ solution, i ignore it.
This seems to be working somewhat, but from what i read, everyone else (in broad terms) seems to be using a 2 neural network configuration for similar tasks. (Policy network and value network)
Am i missing something? - and are there any obvious caveats of the â€œsingle networkâ€ method i am using?
Supplemental question: Are there other methods of â€œrewardingâ€ a network, aside from simply training it?
Thanks,
","['neural-networks', 'machine-learning', 'reinforcement-learning', 'rewards']",
How am I getting same results 30 times faster than in original HER paper?,"
I am reproducing the results from Hindsight Experience Replay by Andrychowicz et. al. In the original paper they present the results below, where the agent is trained for 200 epochs.
200 epochs * 800 episodes * 50 time steps = 8,000,000 total time steps.

I try to reproduce the results but instead of using 8 cpu cores, I am using 19 CPU cores.
I train the FetchPickAndPlace for 120 epochs, but with only 50 episodes per epoch. Therefore 120 * 50 * 50 = 300,000 iterations. I present the curve below:
ï¿¼
and logger output for the first two epochs:
ï¿¼
Now, as can be seen from my tensorboard plot, after 30 epochs we get a steady success rate very close to 1. 30 epochs * 50 episodes * 50 time steps = 75,000 iterations. Therefore it took the algorithm 75,000 time steps to learn this environment.
The original paper took approximately 50 * 800 * 50 = 2,000,000 time steps to achieve the same goal.
How is it that in my case the environment was solved nearly 30 times faster? Are there any flaws in my workings above? 
NB: This was not a one off case. I tested again and got the same results.
Post on Reddit: https://www.reddit.com/r/reinforcementlearning/comments/dpjwfu/getting_same_results_with_half_the_number_of/
","['deep-learning', 'reinforcement-learning', 'open-ai']",
What is the name of an AI whose primary goal is to create a better AI?,"
A general AI x creates another AI y which is better than x.
y creates an AI better than itself.
And so on, with each generation's primary goal to create a better AI.
Is there a name for this.
By better, I mean survivability, ability to solve new problems, enhance human life physically and mentally, and advance our civilization to an intergalactic civilization to name a few.
","['terminology', 'evolutionary-algorithms', 'agi']",
Solving the dead time problem for control using reinforcement learning,"
There are several occasion that reinforcement learning can be used as a control mean.
The action is for example the set target temperature (which in many occasions change with time) and the state is for example the current temperature and other variables. The policy is then the control mean that is going to be learnt using the reinforcement learning.
As there is a dead time (input lag) and time delay in the real world, how can one propose to tackle this problem when using reinforcement learning as a control mean? Thank you.
","['reinforcement-learning', 'control-problem', 'control-theory']",
Training network with 4 GPUs performance is not exactly 4 times over one GPU why? [closed],"







                                This post is about a programming issue or bug.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






Training neural network with 4 GPUs using pyTorch, performance is not even 2 times (btw 1 & 2 times) compare to using one GPU. From Nvidia-smi we see GPU usage is for few milliseconds and next 5-10 seconds looks like data is off-loaded and loaded for new executions (mostly GPU usage is 0%). Is there any way in pyTorch to improve the data upload and offload for the GPU execution.   
","['training', 'pytorch', 'gpu']","Your dataset class probably have a lot of preprocessing code. You should use a dataloader. It will prefetch data from the dataset when the GPU is processing. Also, you can process all the data beforehand and save to a file. Multiple GPU cannot scale as the GPU have to get all data to one GPU to calculate the loss. The performance of 4 GPU is around 3.5x. A large batch size also would help as each GPU will have 1/4 the batch size. a batch size of 64-128 is good for 4 GPU. See the following example code fro CIFAR-10 for multi gpu code. It have dataloaders and dataparallel. Source: https://github.com/dnddnjs/pytorch-multigpu/blob/master/data_parallel/train.pyHope I can help you and have a nice day!"
What is the difference between success rate and reward when dealing with binary and sparse rewards?,"
In OpenAI Gym ""reward"" is defined as:

reward (float): amount of reward achieved by the previous action. The
scale varies between environments, but the goal is always to increase
your total reward.

I am training Hindsight Experience Replay on Fetch robotics environments, where rewards are sparse and binary indicating whether or not the task is completed. The original paper implementing HER uses success rate as a metric in its plots, like so:

On page 5 of the original paper, it is stated that the reward is binary and sparse.
When I print the rewards obtained during a simulation of FetchReach-v1 trained with HER, I get the following values. The first column shows the reward and the second column shows the episode length.

As can be seen, at every time step, I am getting a reward, sometimes I get a $-1$ reward at every time step throughout the episode for a total of $-50$. The maximum reward I can achieve throughout the episode is $0$.
Therefore my question is: What is the reward obtained at each time-step? What does it represent and how is this different from the success rate?
","['reinforcement-learning', 'terminology', 'papers', 'reward-functions', 'hindsight-experience-replay']","Page 6 of the paper describes the exact reward functions, and why they were used:Goals: Goals describe the desired position of the object (a box or a
  puck depending on the task) with some fixed tolerance of $\epsilon$ i.e. $G = \mathcal{R}^3$
   and $f_g(s) = [|g âˆ’ s_{object}| â‰¤ \epsilon]$, where $s_{object}$ is the position of
  the object in the state s. The mapping from states to goals used in
  HER is simply $m(s) = s_{object}$. Rewards: Unless stated otherwise we use binary and sparse rewards $r(s, a, g) = âˆ’[f_g(s 0 ) = 0]$ where $s'$ is
  the state after the execution of the action a in the state s. We
  compare sparse and shaped reward functions in Sec. 4.4.So, at least in the base version (which I believe is your fetchreach-v1), the agent receives a reward of -1 for every timestep spent more than $\epsilon$ from the goal state, and a reward of 0 for every timestep spent within $\epsilon$ of the goal state. Thus, a score of -5.0 would seem to correspond to the agent moving directly to the goal and staying there, while a score of -50.0 would correspond to the agent failing to reach the goal state entirely."
Are there well-established ways of mixing different inputs (e.g. image and numbers)?,"
I am interested in the possibility of having extra input along with the main data. For instance, a medical application that would rely mostly on an image: how could one also account for sex, age, etc.? 
It is certainly possible to put the output of a CNN and other data into, say, a densely connected network; but it seems inefficient. Are there well-established ways of doing something like this?
","['convolutional-neural-networks', 'architecture']",
What does 'democratizing AI' exactly mean?,"
In my AI literature research, I often notice authors use the term 'democratizing AI', especially in the AutoML area. For example in the abstract (last sentence) of this paper:

LEAF therefore forms a foundation for democratizing and improving AI, as well as making AI practical in future applications.

I think I have an idea of what this means, but I would like to ask you for some more specific answers.
","['terminology', 'social', 'automated-machine-learning']","In this particular context, ""Democratize"" means to make more accessible to people.Thus, ""Democratizing AI"" means to make AI softwares and AI programming available, accessible and easy to use for the vast majority of people."
How to learn using DDPG in python solely using a timeseries datasets,"
I have a lengthy timeseries datasets which contains several variables (from sensors etc) to be classified as actions or states. Providing they are successfully done, I want to learn a control policy using DDPG. 
But I have no knowledge of the environment. 
How can I learn my policy off-line only by using these datasets without having any model of the environment? After learning off-line first, then the policy can then be used to learn and control online later in a certain real-world environment. 
First, I know that experience buffer can be used to store the datasets. How should you set the buffer size in this case? 
From what I understand, DDPG needs lots of data to be used for learning.
Should I build an environment model using the specified datasets? Or I don't really need this step?
All of these will be implemented in Python and maybe with the help of another tools if needed. There are some implementation of DDPG available so it is not the main problem, but this implementation must be tweaked to solve my proposed problem. Normally the implemented DDPG in Python requires a Gym-environment as an input so I must change it to satisfy my needs as I don't need Gym for my use case. And these implementations in Python are somehow on-line codes so you need to interact directly with the environment model for the algorithm to be working.
Can someone help me tackle this  problem or give me some advice regarding this? I can help giving more details if needed. Thank you.
Regards
","['reinforcement-learning', 'python', 'datasets', 'control-problem', 'ddpg']",
Why they use KL divergence in Natural gradient?,"
Natural gradient aims to do a steepest descent on the ""function"" space, a manifold that is independent from how the function is parameterized. It argues that the steepest descent on this function space is not the same as steepest descent on the parameter space. We should favor the former. 
Since, for example in a regression task, a neural net could be interpreted as a probability function (Gaussian with the output as mean and some constant variance), it is ""natural"" to form a distance on the manifold under the KL-divergence (and a Fisher information matrix as its metric). 
Now, if I want to be creative, I could use the same argument to use ""square distance"" between the outputs of the neural nets (distance of the means) which I think is not the same as the KL. 
Am I wrong, or it is just another legit way? Perhaps, not as good?
","['machine-learning', 'deep-learning']",
"When using Neural Architecture Search, how are the hyper-parameters chosen?","
I have read a lot about NAS, but I still do not understand one concept: When setting up a neural network, hyperparameters (such as the learning rate, dropout rate, batch size, filter size, etc.) need to be set up.
In NAS, only the best architecture is decided, e.g. how many layers and neurons. But what about the hyperparameters? Are they randomly chosen?
","['neural-networks', 'hyper-parameters', 'neuroevolution', 'neural-architecture-search']","It's not clearly stated (it's not stated at all on Wikipedia), but, after a bit of searching, I found an answer here about a third of the way down the page:The best performing architecture observed during the training of the controller is taken, and a grid search is performed over some basic hyperparameters such as learning rate and weight decay in order to achieve near STOTA (state of the art) performance.So, as a direct answer: The norm; A grid search."
"Why do small datasets require more samples, while big datasets require fewer samples in negative sampling?","
In the deep learning specialization course by Andrew Ng, in the video Sequence Models (minute 4:13), he says that in negative sampling we have to choose a sample of words from the corpus to train rather than choosing the whole corpus. But he said that, for smaller datasets, we need a bigger number of samples, for example, 5-20, and, for larger datasets, we need a smaller sample, for example, 2-5. By sample, I am referring to the number of words along with the target word we have taken to train the model.
Why do small datasets require more samples, while big datasets require fewer samples?
","['recurrent-neural-networks', 'sequence-modeling']",
Precise description of one-shot learning,"
I am working on classifying the Omniglot dataset, and the different papers dealing with this topic describe the problem as one-shot learning (classification). I would like to nail down a precise description of what counts as one-shot learning.
It's clear to me that in one-shot classification, a model tries to classify an input into one of $C$ classes by comparing it to exactly one example from each of the $C$ classes.
What I want to understand is:

Is it necessary that the model has never seen the input and the target examples before, for the problem to be called one-shot?

Goodfellow et. al. describe one-shot learning as an extreme case transfer learning where only one labeled example of the transfer task is presented. So, it means they are considering the training process as a kind of continuous transfer learning? What has the model learned earlier, that is being transferred?


","['terminology', 'definitions', 'transfer-learning', 'one-shot-learning']",
Should the policy parameters be updated at each time step or at the end of the episode in REINFORCE?,"
REINFORCE is a Monte Carlo policy gradient algorithm, which updates weights (parameters) of policy network by generating episodes. Here's a pseudo-code from Sutton's book (which is same as the equation in Silver's RL note):

When I try to implement this with my own problem, I found something strange. Here's implementation from Pytorch's official GitHub:
def finish_episode():
    R = 0
    policy_loss = []
    returns = []
    for r in policy.rewards[::-1]:
        R = r + args.gamma * R
        returns.insert(0, R)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + eps)
    for log_prob, R in zip(policy.saved_log_probs, returns):
        policy_loss.append(-log_prob * R)
    optimizer.zero_grad()
    policy_loss = torch.cat(policy_loss).sum()
    policy_loss.backward()
    optimizer.step()
    del policy.rewards[:]
    del policy.saved_log_probs[:]

I feel like there's a difference between the above two. In Sutton's pseudo-code, the algorithm updates $\theta$ for each step $t$, while the second code (PyTorch's one) accumulate loss and update $\theta$ with the summation, i.e. after each episode. 
I tried to search other implementation of REINFORCE, and I found that most of the implementations follow the second form, update after each generated episodes. 
To check whether both give the same result, I changed the second code as 
def finish_episode():
    R = 0
    policy_loss = []
    returns = []
    for r in policy.rewards[::-1]:
        R = r + args.gamma * R
        returns.insert(0, R)
    returns = torch.tensor(returns)
    returns = (returns - returns.mean()) / (returns.std() + eps)
    for log_prob, R in zip(policy.saved_log_probs, returns):
        optimizer.zero_grad()
        loss = -log_prob * R
        loss.backward()
        optimizer.step()

...

and run it, which gives different result (if my code has no problem). 
So they are not the same, and I think the last one is more close to the original pseudo-code of REINFORCE. What am I missing now? Is it okay because the results are approximately same? (I'm not sure about this claim)
However, in some sense, I think Pytorch's implementation is the right version of REINFORCE. In Sutton's pseudo-code, episode is generated first, so I think $\theta$ shouldn't be updated at each step and should be updated after the total loss is computed. If $\theta$ is updated at each step, then such $\theta$ might be different with the original $\theta$ that used to generate the episode.  
","['reinforcement-learning', 'pytorch', 'implementation', 'sutton-barto', 'reinforce']","The essence of your observation is that Sutton's version of REINFORCE is taking into consideration all of the trajectory to compute the returns, while in the pytorch version only the future is taken into consideration, hence going in reverse to sum the future rewards and ignore the previous rewards. The consequence is that future actions are not punished for early mistakes. The guys at OpenAI refer to this as reward-to-go, but, personally, I find that it resembles Monte Carlo On Policy Control without Exploring Starts or First Visit from Sutton's book.You can find more on REINFORCE and Policy Gradient in Spinning Up RL: Part3: Intro to policy gradient -  Don't let the past distract you.Also, something to note is that even in Sutton's version, the whole trajectory is unrolled, i.e. the episode completes, and then the weights get updated. Otherwise it stops being  a Monte Carlo Method and it becomes a TD method. In addition, you can't make a change on a single point because sampling is a non differentiable operation, instead, the gradient is estimated by collecting a lot of trajectories."
What exactly is a hidden state in an LSTM and RNN?,"
I'm working on a project, where we use an encoder-decoder architecture. We decided to use an LSTM for both the encoder and decoder due to its hidden states. In my specific case, the hidden state of the encoder is passed to the decoder, and this would allow the model to learn better latent representations.
Does this make sense?
I am a bit confused about this because I really don't know what the hidden state is. Moreover, we're using separate LSTMs for the encoder and decoder, so I can't see how the hidden state from the encoder LSTM can be useful to the decoder LSTM because only the encoder LSTM really understands it.
","['recurrent-neural-networks', 'long-short-term-memory', 'hidden-layers', 'seq2seq', 'encoder-decoder']","This is my own understanding of the hidden state in a recurrent network. If it's wrong, please, feel free to let me know.Let's consider the following two input and output sequences\begin{align}
X &= [a, b, c, d, \dots,y , z]\\
Y &= [b, c, d, e, \dots,z , a]
\end{align}We will first try to train a multi-layer perceptron (MLP) with one input and one output from $X$ and $Y$. Here, the details of the hidden layers don't matter.We can write this relationship in maths as$$f(x)\rightarrow y$$where $x$ is an element of $X$ and $y$ is an element of $Y$ and $f(\cdot)$ is our MLP.After training, if given the input $a = x$, our neural network will give an output $b = y$ because $f(\cdot)$ learned the mapping between the sequence $X$ and $Y$.Now, instead of the above sequences, try to teach the following sequences to the same MLP.\begin{align}
X &= [a,a,b,b,c,c,\cdots, y,z,z]\\
Y &= [a,b,c,\cdots, z,a,b,c, \cdots, y,z]
\end{align}More than likely, this MLP will not be able to learn the relationship between $X$ and $Y$. This is because a simple MLP can't learn and understand the relationship between the previous and current characters.Now, we use the same sequences to train an RNN. In an RNN, we take two inputs, one for our input and the previous hidden values, and two outputs, one for the output and the next hidden values.$$f(x, h_t)\rightarrow (y, h_{t+1})$$Important: here $h_{t+1}$ represents the next hidden value.We will execute some sequences of this RNN model. We initialize the hidden value to zero.If we look at the above process we can see that we are taking the previous hidden state values to compute the next hidden state. What happens is while we iterate through this process prev_hidden = next_hidden it also encodes some information about our sequence which will help in predicting our next character."
"Which solutions could I use to solve a multi-armed ""multi-bandit"" problem?","
Problem
I have 66 slot machines. For each of them, I have 7 possible actions/arms to choose from. At each trial, I have to choose one of 7 actions for each and every one of the 66 slots. The reward depends on the combination of these actions, but the slots are not equal, that is, pulling the same arm for different slots gives different results. I do not care about an initial state or feature vector, as the problem always starts from the same setting (it is not contextual). My reward depends on how I pull one of the 7 arms of all of the 66 bandits simultaneously, where, as said, each slot has its own unique properties towards the calculation of the total reward. Basically, the action space is a one-hot encoded 66x7 matrix.
My solution
I ignored the fact that I do not care about a feature vector or state and I treated the problem using a deep NN with a basic policy-gradient algorithm, where I increase directly the probability of each action depending on the reward I get. The state simply does not change, so the NN receive always the same input. This solution does work effectively in finding an approximately optimal strategy, however, it is very computationally expensive and something tells me I am overkilling the problem.
However, I do not see how I could apply standard solutions to MAB, such as epsilon-greedy. I need simultaneity between the different ""slot machines"", and, if I just take each possible permutation as a different action, in order to explore them with greedy methods, I get way too many actions (in the order of $10^{12}$). I have not found in the literature something similar to this multi-armed multi-bandit problem and I am clueless if anything like that has ever been considered - perhaps I am overthinking it and this can be somehow reduced to a normal MAB?
","['reinforcement-learning', 'policy-gradients', 'markov-decision-process', 'multi-armed-bandits', 'combinatorial-optimization']","Although you can frame your problem as a bandit problem or RL, it has other workable interpretations. Critical information from your comments is that:Total reward is not a simple sum of all the results from 66 different machines. There are interactions between machines.Total reward is deterministic.This looks like a problem in combinatorial optimisation. There are many possible techniques you can throw at this. Which ones work best will depend on how nonlinearities and dependence between choices on different machines affect the end results.With deterministic results, if changes between machines were completely isolated, you could search each machine in turn, because you can treat all other 65 components as a constant if you don't change their settings. That would be very simple to code and take $7 \times 66 = 462$ steps to find the optimimum result.In the worst case, the dependencies are so strong and chaotic that there is essentially no predictable difference between changing a single machine's setting and all of them. Pseudo-random number generators and secure hashing functions have this property, as do many quite simple physical systems with feedback loops.In the worst case, there will be a ""magic setting"" with best results, and only a brute force search through all combinations of levers will find it. In order to apply any more efficient search method, you have to assume that the response to combinations of levers is not quite so chaotic.It seems likely from your description, that the best search algorithm is going to be somewhere between simple machine-by-machine optimisation and an exhaustive global search. However, it is hard to tell just where on that spectrum it lies.There are a few different ways to frame it as reinforcement learning. For instance, you could use current switch combination as state, and run 66 switch changes as an ""episode"".I would suggest that genetic algorithms are a good match for this search task, assuming there is at least some local-only effect that means combining two good solutions is likley to result in a third good solution. Genetic algorithms don't need calculations for gradients, and fit nicely with discrete combinations. Your genome can simply be the 66 different switch positions, and the fitness rating your black box score for those positions.Plenty of other combinatorial search algorithms are available. Enough to fill a book or two. One place you could look for inspiration is Clever Algorithms: Nature-Inspired Programming Recipes which is a free PDF."
What are the features get from a feature extraction using a CNN?,"
I've just started to learn CNN and somewhere I have read if I remove the last FCL I will get the features extracted from the input image but... what are those features?
Are they numbers? Labels? An image location (x,y) where there is a line.
I want to use these features on a one shot network, but I can't imagine how to use them if I don't know what they are.
","['convolutional-neural-networks', 'feature-extraction', 'meta-learning']","You get what we call high-level features, which are basically abstract representations of the parts that carry information in the image you want to classify.Imagine you want to classify a car. The image you feed your network could be a car on a road with a driver and trees and clouds, etc. The network, however, if you've trained it to recognize cars, will try to focus on parts of the image regarding a car. The final layers will have learned to extract an abstract representation of a car from the image (this means a low-resolution car-like shape). Now your final FC layers will try to classify the image from these high-level features. In this example, you would have an FC layer that learns to classify a car if this this abstract car-like figure is present in the image. Likewise, if it isn't present it won't classify it as a car. By accessing these high-level features, you essentially have a more compact and meaningful representation of what the image represents (based always on the classes that the CNN has been trained on).By visualizing the activations of these layers we can take a look on what these high-level features look like.The top row here is what you are looking for: the high-level features that a CNN extracts for four different image types."
Which linear algebra book should I read to understand vectorized operations?,"
I am reading Goodfellow's book about neural networks, but I am stuck in the mathematical calculus of the back-propagation algorithm. I understood the principle, and some Youtube videos explaining this algorithm shown step-by-step, but now I would like to understand the matrix calculus (so not basic calculus!), that is, calculus with matrices and vectors, but especially everything related to the derivatives with respect to a matrix or a vector, and so on.
Which math book could you advise me to read?
I specify I studied 2 years after the bachelor in math school (in French: mathÃ©matiques supÃ©rieures et spÃ©ciales), but did not practice for years.
","['neural-networks', 'reference-request', 'linear-algebra', 'books', 'calculus']","If you already have two years of a bachelor's of mathematics, I recommend part I of the book that you're mentioning. That part of the book reviews the main mathematics used in the optimization of neural nets (in part 1), and then actually goes through the various models in detail in the later parts. The review is done at a level that is suitable for someone who has already studied these topics, but needs a refresher.The book Matrix Differential Calculus with Applications in Statistics and Econometrics covers more advanced topics, which might also be what you are looking for. There is also the related Wikipedia article."
How do I predict the occurrence of rare events?,"
I am trying to predict crime. I have data with factors: location, keyword description of the crime, time crime occurred and so on. This is for crimes that occurred in the past. 
I would like to treat the prediction of crimes as a binary classification problem. In this model, the data I have collected would form the ""positive"" examples: they are all examples of a crime happening. However, I am unsure what to use for the negative examples. 
Obviously, most of the time there is no crime at the location, but can I use this as negative data? For example, if I know there was a crime at 7pm at location X, and no other crimes there, should I generate new negative data points for every hour except 7pm?
Ideally, I want to create probabilities of crime based on a set of factors.
","['machine-learning', 'ai-design', 'applications', 'prediction']","It might be more informative to:Label each combination of location, type, and time of crime with a crime rate. For example, theft, in Crystal City, at 11pm at night, occurs 20 times per year, or 0.4 times per resident per year.Predict the crime rate, rather than individual events.This avoids the need to have explicit examples of ""non-crime"", and lets you instead directly learn something related to the probabilities of crimes being committed (the rate)."
Which model can I use for this problem with multiple inputs and outputs?,"
Which model is the most appropriate for this problem with multiple inputs and outputs?
The data set is
A1, A2, A3, A4, A5, A6, B1, B2, B3, B4

where A1, A2, A3, A4, A5, A6 are the inputs and B1, B2, B3, B4 the outputs (this is what I want the model to predict).
What an LSTM be appropriate for this task? Any advice or hint would be much appreciated. Also if anyone can share already done examples, it would really help me a lot.
","['ai-design', 'tensorflow', 'keras', 'prediction']",
How can I use feature extraction in CNN with image segmentation?,"
I'm just started to learn about meta learning and CNN and in most paper that I've read they mention to have one CNN to feature extraction. These features will help the another network.
I don't know what is feature extraction (I don't know what are those features) but I'm wondering if I can use it on image segmentation.
The idea is to use the first network to feature extraction without doing image classification, and pass those features to the other network.
My question is:
How can I use feature extraction in CNN on image segmentation?
","['convolutional-neural-networks', 'feature-extraction']","Feature extraction is a way that people use pretrained model to extract information from input data. For example, image segmentation task may use the VGG network or other image classifying network for feature extraction. The output of the last convolution layer is taken. Then, the features are feed into the untrained network to get outputs. The bottom network for image segmentation usually consists of upsampling and convolutional layers. Then output of size of original image is resulted in teh main network. Hope I can help you"
What could be the cause of the drop in the reward in A3C?,"
The mean episodic reward is generally increasing, but it has spontaneous drops, and I'm not sure of their cause.

The problem has a sparse reward, batch size=2000, entropy_coefficient=0.1, other hyper-parameters are pretty standard.
Has anyone seen this kind of behavior? What could be the cause these drops in the reward(not enough exploration, too sparse rewards, the state not expressive enough, etc.)?
","['reinforcement-learning', 'deep-rl', 'policy-gradients', 'rewards', 'actor-critic-methods']",
Why RNNs often use just one hidden layer?,"
Did I get it right, that RNNs most often have just one hidden neuron layer? Is there a reason for that? Will RNNs with several hidden layers in each cell work worse?
Thank you!! 
","['neural-networks', 'recurrent-neural-networks']","Definitely you can have multiple hidden layers in RNN. One the most common approaches to determine the hidden units is to start with a very small network (one hidden unit) and apply the K-fold cross validation ( k over 30 will give very good accuracy) and estimate the average prediction risk. Then you will have to repeat the procedure for increasing growing networks, for example for 1 to 10 hidden units or more if needed.However, in my experience, if you are interested to get best possible accuracy, you should start with small number of hidden layers and more simple structure, and if you are not satisfied with the corresponding accuracy, then we should go on increasing the learning rate by fixed but small small steps and each time start training fresh. "
How many weights does the max-pooling layer have?,"
How many weights does the max-pooling layer have?
For example, if there are 10 inputs, a pooling filter of size 2, stride 2, how many weights, including bias, does a max-pooling layer have?
","['convolutional-neural-networks', 'hyper-parameters', 'filters', 'pooling', 'max-pooling']","A max-pooling layer doesn't have any trainable weights. It has only hyperparameters, but they are non-trainable. The max-pooling process calculates the maximum value of the filter, which consists of no weights and biases. It is purely a way to downscale the data to a smaller dimension."
Is this idea to calculate the required number of hidden neurons for a single hidden layer neural network correct?,"
I have an idea to find the optimal number of hidden neurons required in a neural network, but I'm not sure how accurate it is.
Assuming that it has only 1 hidden layer, it is a classification problem with 1 output node (so it's a binary classification task), has N input nodes for N features in the data set, and every node is connected to every node in the next layer.
I'm thinking that to ensure that the network is able to extract all of the useful relations between the data, then every piece of data must be linked to every other piece of data, like in a complete graph. So, if you have 6 inputs, there must therefore be 15 edges to make it complete. Any more and it will be recomputing previously computed information and any less will be not computing every possible relation.
So, if a network has 6 input nodes, 1 hidden node, 1 output node. There will be 6 + 1 connections. With 6 input nodes, 2 hidden nodes, and 1 output node, there will be 12 + 2 connections. With 3 hidden nodes there will be 21 connections. Therefore, the hidden layer should have 3 hidden nodes to ensure all possibilities are covered.
This answer discusses another method. For the sake of argument, I've tried to keep both examples using the same data. If this idea is computed with 6 input features, 1 output node, $\alpha = 2$, and 60 samples in the training set, this would result in a maximum of 4 hidden neurons. As 60 samples is very small, increasing this to 600 would result in a maximum of 42 hidden neurons.
Based on my idea, I think there should be at most 3 hidden nodes and I can't imagine anymore being useful, but would there be any reason to go beyond 3 and up to 42, like in the second example?
","['neural-networks', 'deep-learning', 'hyperparameter-optimization', 'hidden-layers', 'hyper-parameters']","I have an idea to find the optimal number of hidden neurons required in a neural network but I'm not sure how accurate it is.It's a complete non-starter, and there is a no such calculation possible in the general case (real-valued inputs to a neural network). Even with one input neuron it is not possible. That is because even with one input, the output can be an arbitrarily complex mapping to classes. A good example with two inputs that would require an infinite number of hidden neurons to supply a simple classifier would be classifying x,y points as being in the Mandelbrot set. In some, more constrained, examples, with well-defined functions, you can construct a minimal neural network that solves the problem perfectly. For instance a neural network model of XOR can be made with two hidden neurons (and six links). However, this kind of analysis is limited to simple problems. You might be able to come up with some variation of your idea if all inputs were boolean, and the neural network limited to some combined bitwise logic on all the inputs.Your idea of matching number of edges to number of possible interactions between inputs does not work because you are only considering the most basic kind of interaction between two variables, whilst variables can in practice combine in all sorts of ways to form a function. In addition, each neuron in a hidden layer works with a linear weighted sum, plus a fixed transformation function. This is in no way guaranteed to match the function shape that you are trying to approximate with the neural network. An analogy that you might be aware of is discrete Fourier transforms - it is possible to model any part of a function by combine sine and cosine waves of different frequencies, but some functions will require many such waves in order to be represented accurately.Your link to the answer in Cross Validated Stack Exchange gives you a rule of thumb that the writers find often works with the kinds of data that they work with. This is useful experience. You can use such rules as the starting point for searching for architecture that works on your problem. This will likely be more useful than your idea based on counting the possible variable interactions. However, in both cases, the most important step is to perform a test with some unseen examples, and to search for the best neural network architecture for your problem.There are things you can do with variable interactions though. For instance, try looking for linear correlations between simple polynomial combinations of variables and your target variable, e.g. plot $x_1 x_2$ vs $y$ or $x_3^2 x_4$ vs $y$ . . . you may find some combinations have a clear signal implying a relationship. Take care if you do this sort of thing though, if you test very many of these, you will find a linear relationship purely by chance that looks good initially but turns out to be a dud when testing (it's a form of overfitting). So you should generally test a lot less than the size of your dataset, and limit yourself to some modest maximum total power."
How to implement loss function of H-GAN model,"
I was trying to implement the loss function of  H-GAN. Here is my code . But it seem somethings wrong, maybe is recognition loss on z (EQ 9). I used the EQ 5 on MISO to calculate it. Here is my code: 
def recognition_loss_on_z(self,latent_code, r_cont_mu, r_cont_var):
    eplison = (r_cont_mu - latent_code) / (r_cont_var+1e-8)
    return -tf.reduce_mean(tf.reduce_sum(-0.5*tf.log(2*np.pi*r_cont_var+1e-8)-0.5*tf.square(eplison), axis=1))/(config.batch_size * config.latent_dim)

And I calculated loss function: 
    self.z_mean, self.z_sigm = self.Encode(self.images)
    self.z_x = tf.add(self.z_mean, tf.sqrt(tf.exp(self.z_sigm))*self.ep)

    self.D_pro_logits, self.l_x_real, self.Q_y_style_given_x_real, continuous_mu_real, continuous_var_real  = self.discriminator(self.images, training=True, reuse=False)
    self.De_pro_tilde, self.l_x_tilde, self.Q_y_style_given_x_tidle, continuous_mu_tidle, continuous_var_tidle= self.discriminator(self.x_tilde, training=True, reuse = True)
    self.G_pro_logits, self.l_x_fake, self.Q_y_style_given_x_fake, continuous_mu_fake, continuous_var_fake = self.discriminator(self.x_p, training=True, reuse=True)

    tidle_latent_loss = self.recognition_loss_on_z(self.z_x, continuous_mu_tidle,continuous_var_tidle)
    real_latent_loss  = self.recognition_loss_on_z(self.z_x, continuous_mu_real,continuous_var_real)
    fake_latent_loss =  self.recognition_loss_on_z(self.zp, continuous_mu_fake,continuous_var_fake)

And discriminator: 
    def discriminator(self, x_var,training=False, reuse=False):
    with tf.variable_scope(""discriminator_recongnizer"") as scope:
        if reuse==True:
            scope.reuse_variables()
        conv1 = tf.nn.leaky_relu(batch_normalization(conv2d(x_var, output_dim = 64 , kernel_size=6, name='dis_R_conv1'),training = training,name='dis_bn1', reuse = reuse), alpha =0.2)
        conv2 = tf.nn.leaky_relu(batch_normalization(conv2d(conv1, output_dim = 128 , kernel_size=4, name='dis_R_conv2'),training = training ,name='dis_bn2', reuse = reuse), alpha =0.2)
        conv3 = tf.nn.leaky_relu(batch_normalization(conv2d(conv2, output_dim = 128 , kernel_size=4, name='dis_R_conv3'),training = training,name='dis_bn3', reuse = reuse), alpha =0.2)
        conv4 = conv2d(conv3, output_dim = 256 , kernel_size=4, name='dis_R_conv4')
        lth_layer = conv4
        conv4 = tf.nn.leaky_relu(batch_normalization(conv4, training=training, name='dis_bn4', reuse = reuse),alpha =0.2)
        conv4 = tf.reshape(conv4,[-1, 256*8*8])
        #Discriminator
        with tf.variable_scope('discriminator'):
            d_output = fully_connect(conv4, output_size=1, scope='dr_dense_2')
        with tf.variable_scope('dis_q'):
            fc_r = tf.nn.leaky_relu(batch_normalization(fully_connect(conv4, output_size=256 + config.style_classes, scope='dis_dr_dense_3'), training=training, name='dis_bn_fc_r', reuse=reuse), alpha=0.2)
            continuous_mu = fully_connect(fc_r, output_size=256, scope='dis_dr_dense_mu')
            continuous_var = tf.exp(fully_connect(fc_r, output_size=256, scope='dis_dr_dense_logvar'))
            style_predict = fully_connect(fc_r, output_size=config.style_classes, scope='dis_dr_dense_y_style')

        return d_output,lth_layer,style_predict,continuous_mu,continuous_var   

Does anyone have experience with that, please tell me where I was wrong. Thanks you so much, I really appreciate that!
","['tensorflow', 'objective-functions', 'generative-adversarial-networks']",
What are some books/papers that deal with fundamental and philosophical issues of ML and relate it to the global discourse of AIs?,"
In my experience, most of the time, when people talk about AI nowadays they mostly mean machine learning. Despite this, ML is usually seen as a mere technique to build high-performance software.
I rarely see people discuss the foundational questions of it, such as: from which ""philosophy"" of AI did ML emerge? Why is ML compelling in AI research, if not by its performance? What are the fundamental differences between statistical/probabilistic AI and logical AI? For reference, this hasn't even been mentioned in my master-level course on machine learning. Even myself I used to have a distaste for ML because I thought it was just mindless data-crunching.
But, lately, I've been reading through ""Probability Theory: The Logic Of Science"" and I'm starting to appreciate the theoretical side of ML, for instance, how Bayesian probability can be seen as a model of plausible reasoning in humans, and how probability theory extends logic (motivating, maybe, why probabilistic AIs were the next logical [no pun intended] step after logical AI). I would like now to delve deeper into the topic.
What are some books/papers that deal with fundamental and philosophical issues of ML and relate it to the global discourse of AIs?
","['machine-learning', 'reference-request', 'papers', 'philosophy', 'books']",
Which neural network algorithms can be used to map motion vectors in image processing?,"
I'm working on finding out the motion vectors of objects in images. The inputs are the images of objects in motion. The outputs of neural network are the object name, direction of object vector and prediction of next vector change. 
There are different 3D ConvNets I'm considering as a baseline like ReMotENet. I would appreciate if you would recommend any interesting papers in MoCap domain and any existing neural networks performing similar task.
","['neural-networks', 'convolutional-neural-networks', 'image-processing']",
What is the relationship between fuzzy logic and objective bayesian probability?,"
I understand fuzzy logic is a variant of formal logic where, instead of just 0 or 1, a given sentence may have a truth value in the [0..1] interval. Also, I understand that logical probability (objective bayesian) understands probability as an extension of logic, where uncertainity is taken into account. To me they sound rather similar (they both extend formal logic by modelling truth as a continuos interval between 0 and 1).
My question is, what is the relationship between these two concepts?. What is the difference, and what are the differences in AI approaches based upon these two formal systems?
","['fuzzy-logic', 'bayesian-probability']",
What is the double sample problem in reinforcement learning?,"
According to the SBEED: Convergent Reinforcement Learning with
Nonlinear Function Approximation for convergent reinforcement learning, the Smoothed Bellman operator is a way to dodge the double sample problem? Can someone explain to me what the double sample problem is and how SBEED solves it?
",['reinforcement-learning'],"The double sampling problem is referenced in Chaper 11.5 Gradient Descent in the Bellman Error in Reinforcement Learning: An Introduction (2nd edition).From the book, this is the full gradient descent (as opposed to semi-gradient descent) update rule for weights of an estimator that should converge to a minimal distance from the Bellman error:$$w_{t+1} = w_t + \alpha[\mathbb{E}_b[\rho_t[R_{t+1} + \gamma\hat{v}(S_{t+1},\mathbf{w})] - \hat{v}(S_{t},\mathbf{w})][\nabla\hat{v}(S_{t},\mathbf{w})- \gamma\mathbb{E}_b[\rho_t\nabla\hat{v}(S_{t+1},\mathbf{w})]]$$[...] But this is naive, because the equation above involves the next state, $S_{t+1}$, appearing in two
expectations that are multiplied together. To get an unbiased sample of the product,
two independent samples of the next state are required, but during normal interaction
with an external environment only one is obtained. One expectation or the other can be
sampled, but not both.Basically, unless you have an environment that you can routinely re-wind and re-sample to get two independent estimates (for $\hat{v}(S_{t+1},\mathbf{w})$ and $\nabla\hat{v}(S_{t+1},\mathbf{w})$) then the update rule that naturally arises from gradient descent on the Bellman error does will work any better than other approaches, such as semi-gradient methods. If you can do this rewind process on every step, then it may be worth it because of the guarantees of convergence, even in off-policy with non-linear approximators.The paper proposes a workaround for this issue, keeping the robust convergence guarantees, but dropping the need to collect two independent samples of the same estimate on each step."
What are the best algorithms for image segmentation tasks?,"
I recently started looking for networks that focus on image segmentation tasks related to biomedical applications. I could not miss the publication U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) by Ronneberger, Fischer, and Brox. However, as deep learning is a fast-growing field and the article was published more than 4 years ago, I was wondering if anyone knows other algorithms that yield better results for image segmentation tasks?  And if so, do they also use a U-shape architecture (i.e. contraction path then expansion path with up-conv)? 
","['machine-learning', 'convolutional-neural-networks', 'reference-request', 'image-segmentation', 'u-net']",
How to Extract Information from the Image,"
I'm trying to extract some particular information from the image(png).
I tried to extract the text using the below code
import cv2
import pytesseract
import os
from PIL import Image
import sys

def get_string(img_path):
    # Read image with opencv
    img = cv2.imread(img_path)

    # Convert to gray
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # Apply dilation and erosion to remove some noise
    kernel = np.ones((1, 1), np.uint8)
    img = cv2.dilate(img, kernel, iterations=1)
    img = cv2.erode(img, kernel, iterations=1)

    # Write the image after apply opencv to do some ...
    cv2.imwrite(""thres.png"", img)
    # Recognize text with tesseract for python
    result = pytesseract.image_to_string(Image.open(""invoice.png""))
    os.remove(""invoice.png"")

    return result

if __name__ == '__main__':
    from sys import argv

    if len(argv)<2:
        print(""Usage: python image-to-text.py relative-filepath"")
    else:
        print('--- Start recognize text from image ---')
        for i in range(1,len(argv)):
            print(argv[i])
            print(get_string(argv[i]))
            print()
            print()

        print('------ Done -------')

But I want to extract data from particular fields.
Such as

 a) INVOICE NO.
 b) CUSTOMER NO.
 c) SUBTOTAL
 d) TOTAL
 e) DATE


How can I extract the required information from the below image ""invoice""?
PFB

","['machine-learning', 'deep-learning', 'natural-language-processing', 'python', 'image-processing']",
Deepmind Spriteworld run_demo.py not found [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



I'm trying to run the Deepmind Spriteworld demo described on the project's GitHub page, but I'm not finding run_demo.py in the distribution and the closest sounding file, demo_ui.py doesn't launch a UI  when run (tried both on Linux and Windows).
How should the Deepmind Spriteworld demo UI be launched?
","['reinforcement-learning', 'deepmind']",
Generate Image with Artificial intelligence [closed],"







                                This post is about specific software, hardware, datasets, or pre-trained models.
                                
                            











 After having gone through the help center, if you think this post is on-topic but has just been formulated incorrectly, please, edit it to clarify how it's on-topic and then vote to re-open it.


Closed 1 year ago.







                        Improve this question
                    



I am pretty new to Artificial Intelligence programming, however i do understand the basic concept. 
I have an idea in my mind: 
Import a JPEG Image, 
Convert this Image into a 2D Array (x,y values + r g b values). 
Then create a second array with same (xy) values wit rgb all set to 0,0,0.
Now i want to build an AI Layer which will try to lower the error factor between the arrays until they are equal (the rgb values in the second array are equal to the first array (error factor 0) ).
I would prefer to do it in Java. Any suggestions to librarys or example that can help me get started? Thanks for any help. 
","['machine-learning', 'training', 'neurons', 'java']","For recreating an image exactly the same as the original, you can use an autoencoder. This basically use AI Layers to encode the image raw pixel values to a vector of floats, drastically decreasing the representing vector. Afterwards another AI Layer increases the dimensions back to the original image. The method does not required labels, as it only refer to teh image to encode it to a vector of features. For implementing in java, there is not a lot 9f resources. However, you can check this library out: https://deeplearning4j.org/
For the implementation, see this: https://github.com/eclipse/deeplearning4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/unsupervised/variational/VariationalAutoEncoderExample.java
For the method, you can see this python tutorial and implement it in java. https://towardsdatascience.com/autoencoders-in-keras-c1f57b9a2fd7For generating completely new image, you can try GAN(Generative Adverserial Network). This generates completely new images from a random noise image. The noise image is passed through a generator which is a CNN(convolutional neural network) and get a result image. The result image is then feed to  a discriminator(CNN as well) to classify if that image is fake or real. The generator and discriminator compete and slowly gets better. For java implementation, see this: https://github.com/wmeddie/dl4j-gansHope I can help you and have a nice day!"
Is there any time-varying directed graph dataset?,"
I am interested in the node classification task for graph data. So far,I've tried it with the Cora dataset, but it is an undirected graph and has word attributes as features. I want to extend this task to a time-varying directed graph. Does anybody know about this kind of dataset?
","['machine-learning', 'convolutional-neural-networks', 'classification', 'geometric-deep-learning', 'graph-theory']",
"Why ""Exploratory moves do not result in any learning""?","
In Chapter 1 of the book Reinformcement Learning An Introduction 2nd Edition by Richard S. Sutton and Andrew G.Barto, there is one statement ""Exploratory moves do not result in any learning"".
This sentence is in Figure 1.1.

Figure 1.1: A sequence of tic-tac-toe moves. The solid black lines represent the moves taken during a game; the dashed lines represent moves that we (our reinforcement learning player) considered but did not make. Our second move was an exploratory move, meaning that it was taken even though another sibling move, the one leading to eâ‡¤, was ranked higher. Exploratory moves do not result in any learning, but each of our other moves does, causing updates as suggested by the red arrows in which estimated values are moved up the tree from later nodes to earlier nodes as detailed in the text.

It confuses me. In my understanding, exploration should contribute to learning in almost all RL algorithms. So, why does the book state ""Exploratory moves do not result in any learning"" in this case?
","['machine-learning', 'reinforcement-learning']","I believe this is a pedagogical decision. Because the sentence occurs in the first chapter of the book, I think the authors are trying to avoid the objection that a neophyte might make: learning from random movements seems like it will cause you to learn strange behaviors. Certainly, the statement is inaccurate. We need only reach page 26 to see a counterexample: The Q-learning equation (2.1) sums over all actions, not just the greedy ones. This also applies to the temporal difference learning method specified in Chapter 6, which is the method Figure 1.1 is discussing."
What happened after the second AI winter?,"
I know this is a very general question, but I'm trying to illustrate this topic to people who are not from the field, and also my understanding is very limited since I'm just a second-year physics student with a basic understanding of R and Python. My point is, I'm not trying to say anything wrong here. 
So according to Wikipedia, after the second AI winter, which happened because expert systems didn't match expectations of the general public and of scientists, AI made a recovery ""due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields (such as statistics, economics and mathematics), and a commitment by researchers to mathematical methods and scientific standards"". 
What I'm trying to understand now is whether the rise of AI is rather connected to greater computational power available to the public or whether there have been fundamental mathematical advances that I'm not aware of. If the latter is the case (because according to my understanding, the mathematical models behind neural networks are rooted in the 70s and 80s), I would appreciate examples. 
Again, please don't be offended by the general character of this question, I know it is probably really hard to answer correctly, however, I'm just trying to give a short historic introduction to the field to a lay audience and wanted to be clear in that regard.
","['history', 'ai-winter']",
What are the differences between Yolo v1 and CenterNet?,"
I recently read a new paper (late 2019) about a one-shot object detector called CenterNet. Apart from this, I'm using Yolo (V3) one-shot detector, and what surprised me is the close similarity between Yolo V1 and CenterNet.
First, both frameworks treat object detection as a regression problem, each of them outputs a tensor that can be seen as a grid with cells (below is an example of an output grid).

Each cell in this grid predicts an object class, a box offset relative to the cell's position and a box size. The only major difference between Yolo V1 and CenterNet is that Yolo also predicts an object confidence score, that is represented in CenterNet by the class score. Yolo also predicts 2 boxes.
In brief, the tensor at one cell position is Class + B x (Conf + Size + Off) for Yolo V1 and Class + Size + Off for CenterNet.
The training strategy is quite similar too. Only the cell containing the center of a ground truth is responsible for that detection and thus affects the loss. Cells near the ground truths center (base on the distance for CenterNet and IoU for Darknet) have a reduced penalty in the loss (Focal Loss for CenterNet vs and tuned hyper parameter for Yolo).
The loss functions have near the same structure (see above) except that L1 is preferred in CenterNet while Yolo uses L2, among other subtleties.

My point is not that Yolo V1 and CenterNet are the same â€” there are not â€” but they are far closer that it appears at first glance.
The problem is that recent papers like CenterNet (CornerNet, ExtremeNet, Triplet CenterNet, MatrixNet) all claim to be ""Keypoint-based detector"" while they are not so much different than regular ""anchor-based"" detectors (that are preconditioned regressors in fact).
Instead I think that the biggest difference between Yolo and CenterNet is the backbone that has a bigger resolution for CenterNet (64x64) while Darknet has 7 or 8 only.

My Question is: do you see a major difference between the two concepts that I may have missed and that could explain the performance gap? I understand that new backbones, new loss functions and better resolutions can improve the accuracy but is there a structural difference between the two approaches?
","['deep-learning', 'comparison', 'object-detection', 'yolo', 'center-net']",
Why does the discriminator minimize the cross-entropy while the generator maximize it?,"
In his original GAN paper Goodfellow gives a game theoretic perspective for GANs:
\begin{equation} 
\underset{G}{\min}\, \underset{D}{\max}\, V\left(D,G \right) = 
\mathbb{E}_{x\sim\mathit{p}_{\textrm{data}}\left(x \right)} \left[\textrm{log}\, D \left(x \right) \right]
+ \mathbb{E}_{z\sim\mathit{p}_{\textrm{z}}\left(z \right)} \left[\textrm{log} \left(1 - D \left(G \left(z \right)\right)\right) \right]
\end{equation}
I think I understand this formula, at least it makes sense to me.
What I don't understand is that he writes in his NIPS tutorial:

In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes the same cross-entropy.

Why does he write that the discriminator minimizes the cross-entropy while the generator maximizes it? Shouldn't it be the other way around? At least that is how I understand $\underset{G}{\min}\, \underset{D}{\max}\, V\left(D,G \right)$.
I guess this shows that I have a fundamental error in my understanding. Could anyone clarify what I'm missing here?
","['neural-networks', 'machine-learning', 'math', 'generative-adversarial-networks', 'generative-model']","Your mistake is that you think that the referenced $V(D,G)$ is the definition of the cross-entropy! Indeed, the cross-entropy is defined based on the negative value of the $V(D,G)$. Hence, if you consider the minus behind the $V(D,G)$ ($-V(D,G)$) the sentence will be meaningful."
Why is the transformer for time series forecasting faster than RNN?,"
I've been reading different papers which implements the Transformer for time series forecasting. Most of the them are claiming that the training time is significantly faster then using a normal RNN. From my understanding when training such a model, you can encode the input in parallel, but the decoding is still sequential unless you're using teacher-forcing. 
What makes the transformer faster than RNN in such a setting? Is there something that I am missing?
","['recurrent-neural-networks', 'transformer', 'prediction', 'attention', 'sequence-modeling']",
Is it possible for value-based methods to learn stochastic policies?,"
Is it possible for value-based methods to learn stochastic policies? I'm trying to get a clear picture of the different categories for RL algorithms, and while doing so I started to think about settings where the optimal policy is stochastic (POMDP), and if it is possible to learn this policy for the ""traditional"" value-based methods
If it is possible, what are the most common methods for doing this?
","['reinforcement-learning', 'value-functions', 'pomdp', 'stochastic-policy', 'value-based-methods']","Is it possible for value-based methods to learn stochastic policies? Yes, but only in a limited sense, due to the ways it is possible to generate stochastic policies from a value function. For instance, the simplest exploratory policy used by SARSA and Monte Carlo Control, $\epsilon$-greedy, is stochastic. SARSA natually learns the optimal $\epsilon$-greedy policy for any fixed value of $\epsilon$. That is not quite the same as learning the optimal policy, but might still be useful in a non-stationary environment where exploration is always required and the algorithm is forever learning online.You can also use other functions to generate stochastic policies from value functions. For instance, sampling from the Boltzmann distribution over action values using a temperature parameter to decide relative priorities between actions with different action values.However, all these approaches share the problem that they cannot converge towards an optimal stochastic policy. The policies are useful for mangaging exploration, but will only be optimal in the limited sense of optimal given the fixed policy generator or by chance. There is no way for a purely value-based method to learn a conversion from values to an optimal balance of probabilities for action choice.For strict MDPs this is not an issue. If the MDP has the Markov property in the state representation, then there will always be a deterministic optimal policy, and value-based methods can converge towards it. That may include reducing $\epsilon$ in $\epsilon$-greedy approaches or the temperature in Gibbs sampling, when using an on-policy method.I started to think about settings where the optimal policy is stochastic(POMDP), and if it is possible to learn this policy for the ""traditional"" value-based methodsIt isn't.To resolve this you need to add some kind of policy function and a mechanism to search for better policies directly by modifying that function. Policy Gradient methods are one approach, but you could include genetic algorithms or other search methods too under this idea.It may still be useful to use a value-based method as part of a policy search, to help evaluate changes to the policy. This is how Actor-Critic works."
What are the societal risks associated with AI?,"
What are the actual risks to society associated with the widespread use of AI? Outside of the use of AI in a military context.
I am not talking about accidental risks or unintentional behaviour - eg, a driver-less car accidentally crashing.
And I am not talking about any transitional effects when we see the use of AI being widespread and popular.  For instance I have heard that the widespread use of AI will make many existing jobs redundant, putting many people out of work.  However this is true of any major leap forward in technology (for example the motor car killed off the stable/farrier industries).  The leaps forward in technology almost always end up creating more jobs than were lost in the long run.
I am interested in long term risks and adverse effects stemming directly from the widespread use of AI in a non-military sense.  Has anybody speculated on the social or psychological impacts that AI will produce once it has become popular?
","['social', 'neo-luddism', 'risk-management']","The biggest risk is algorithmic bias. As more and more decision-making processes are taken on by AI systems, there will be an abdication of responsibility to the computer; people in charge will simply claim the computer did it, and they cannot change it.The real problem is that training data for machine learning often contains bias, which is usually ignored or not recognised. There was a story on BBC Radio about someone whose passport photo was rejected by an algorithm because he supposedly had his mouth open. However, he belonged to an ethnic group which has larger lips than Caucasian whites, but the machine could not cope with that.There is a whole raft of examples where similar things happen: if you belong to a minority group, machine learning can lead to you being excluded, just because the algorithms will have been trained on training data that was too restricted.Update: Here is a link to a BBC News story about the example I mentioned."
Can alpha-beta pruning be used for applications apart from games?,"
Can alpha-beta pruning/ minimax be used for systems apart from games? Like for selecting the right customer for a product, etc. (the typical data science problems)? I have seen people do it, but can't understand how. Can someone help me understand that?
Can I do something like if - find two criteria on which customers can buy product depends on like gender and age. Find the probability for all the customers depending on age and gender if they can buy it.
like if there are 3 customers - there probability to buy a product on the basis of their age and gender is - Customer 1 - (20%, 30%), Customer 2 - (30%, 60%), Customer 3 - (40%, 20%). here the x and y represents - (probability based on age, probability based on gender ). Probability is probability to buy the product.
For minimax, will it be correct if one player(max) tries to select the customer on basis of gender and other player(min) on basis of age. so, one can be max and one can be min. 
Dont know if this correct or not, but just a idea.
","['applications', 'data-science', 'minimax', 'alpha-beta-pruning']","Thinking about this more, the answer is in fact yes, but not for the application you mention.You cannot use alpha-beta pruning to learn a model to predict customer outcomes, because it is only useful for domains where you are concerned about an adversary. In finding a customer model, there is no reason to worry about someone coming in and forcing you to make bad decisions about the optimization of the model. Consequentially, there is no reason to use minimax search, and thus, to use alpha-beta pruning.There are applications other than (video) games where you could use these techniques though. For example, there are security games. In these ""games"" we want to use AI to find a strategy to protect an airport. It is reasonable to try and design our model under the assumption that someone else wants to break it. You could use Alpha-Beta pruning here (although in practice, more sophisticated algorithms are used). "
Unable to achieve expected outputs using NEAT for the snake game,"
I am trying to implement NEAT for the snake game. My game logic is ready, which is working properly and NEAT configured. But even after 100 generations with 200 genomes per generation, the snakes perform very poorly. It barely ever eats more than 2 food. Below is the snip of the eval_genome function:
def eval_genome(genomes, config):
    clock = pygame.time.Clock()
    win = pygame.display.set_mode((WIN_WIDTH, WIN_HEIGHT))
    for genome_id, g in genomes:
        net = neat.nn.FeedForwardNetwork.create(g, config)
        g.fitness = 0
        snake = Snake()
        food = Food(snake.body)
        run = True
        UP = DOWN = RIGHT = LEFT = MOVE_SNAKE = False
        moveToFood = 0
        score = 0
        moveCount = 0
        while run:
            pygame.time.delay(50)
            clock.tick_busy_loop(10)
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    run = False
            snakeHeadX = snake.body[0]['x']
            snakeHeadY = snake.body[0]['y']
            snakeTailX = snake.body[len(snake.body)-1]['x']
            snakeTailY = snake.body[len(snake.body)-1]['y']
            snakeLength = len(snake.body)
            snakeHeadBottomDist = WIN_HEIGHT - snakeHeadY - STEP
            snakeHeadRightDist = WIN_WIDTH - snakeHeadX - STEP
            foodBottomDist = WIN_HEIGHT - food.y - STEP
            foodRightDist = WIN_WIDTH - food.x - STEP
            snakeFoodDistEuclidean = math.sqrt((snakeHeadX - food.x)**2 + (snakeHeadY - food.y)**2)
            snakeFoodDistManhattan = abs(snakeHeadX - food.x) + abs(snakeHeadY - food.y)
            viewDirections = snake.checkDirections(food, UP, DOWN, LEFT, RIGHT)
            deltaFoodDist = snakeFoodDistEuclidean

            outputs = net.activate((snakeHeadX, snakeHeadY, snakeHeadBottomDist, snakeHeadRightDist, snakeTailX, snakeTailY, snakeLength, moveCount, moveToFood, food.x, food.y, foodBottomDist, foodRightDist, snakeFoodDistEuclidean, snakeFoodDistManhattan, viewDirections[0], viewDirections[1], viewDirections[2], viewDirections[3], viewDirections[4], viewDirections[5], viewDirections[6], viewDirections[7], deltaFoodDist))

            if (outputs[0] == max(outputs) and not DOWN):
                snake.setDir(0,-1)
                UP = True
                LEFT = False
                RIGHT = False
                MOVE_SNAKE = True
            elif (outputs[1] == max(outputs) and not UP):
                snake.setDir(0,1)
                DOWN = True
                LEFT = False
                RIGHT = False
                MOVE_SNAKE = True
            elif (outputs[2] == max(outputs) and not RIGHT):
                snake.setDir(-1,0)
                LEFT = True
                UP = False
                DOWN = False
                MOVE_SNAKE = True
            elif (outputs[3] == max(outputs) and not LEFT):
                snake.setDir(1,0)
                RIGHT = True
                UP = False
                DOWN = False
                MOVE_SNAKE = True
            elif (not MOVE_SNAKE):
                if (outputs[0] == max(outputs)):
                    snake.setDir(0,-1)
                    UP = True
                    MOVE_SNAKE = True
                elif (outputs[1] == max(outputs)):
                    snake.setDir(0,1)
                    DOWN = True
                    MOVE_SNAKE = True
                elif (outputs[2] == max(outputs)):
                    snake.setDir(-1,0)
                    LEFT = True
                    MOVE_SNAKE = True
                elif (outputs[3] == max(outputs)):
                    snake.setDir(1,0)
                    RIGHT = True
                    MOVE_SNAKE = True  

            win.fill((0, 0, 0))
            food.showFood(win)
            if(MOVE_SNAKE):
                snake.update()
                newSnakeHeadX = snake.body[0]['x']
                newSnakeHeadY = snake.body[0]['y']
                newFoodDist = math.sqrt((newSnakeHeadX - food.x)**2 + (newSnakeHeadY - food.y)**2)
                deltaFoodDist = newFoodDist - snakeFoodDistEuclidean
                moveCount += 1
                if (newFoodDist <= snakeFoodDistEuclidean):
                    g.fitness += 1
                else:
                    g.fitness -= 10
            snake.show(win)
            if(snake.collision()):
                if score != 0:
                    print('FINAL SCORE IS: '+ str(score))
                g.fitness -= 50
                break

            if(snake.eat(food,win)):
                g.fitness += 15
                score += 1
                if score == 1 :
                    moveToFood = moveCount
                    # foodEatenMove = pygame.time.get_ticks()/1000
                else:
                    moveToFood = moveCount - moveToFood
                food.foodLocation(snake.body)
                food.showFood(win)

Additionally, I am putting the definition of the checkDirections function. What it does is returns an array of size 8 corresponding to 8 directions where each value can be either 0 (not food or body), 1(food found but no body), 3(body found but no food), or 4(both body and food found).
def checkDirections(self, food, up, down, left, right):
        '''
        x+STEP, y-STEP
        x+STEP, y+STEP
        x-STEP, y-STEP
        x-STEP, y+STEP
        x+STEP, y
        x, y-STEP
        x, y+STEP
        x-STEP, y
        '''
        view = []
        x = self.xdir
        y = self.ydir

        view.append(self.check(x, y, STEP, -STEP, food.x, food.y))
        view.append(self.check(x, y, STEP, STEP, food.x, food.y))
        view.append(self.check(x, y, -STEP, -STEP, food.x, food.y))
        view.append(self.check(x, y, -STEP, STEP, food.x, food.y))
        view.append(self.check(x, y, STEP, 0, food.x, food.y))
        view.append(self.check(x, y, 0, -STEP, food.x, food.y))
        view.append(self.check(x, y, 0, STEP, food.x, food.y))
        view.append(self.check(x, y, -STEP, 0, food.x, food.y))

        if up == True:
            view[6] = -999
        elif down == True:
            view[5] = -999
        elif left == True:
            view[4] == -999
        elif right == True:
            view[7] == -999        
        return view

    def check(self, x, y, xIncrement, yIncrement, foodX, foodY):
        value = 0
        foodFound = False
        bodyFound = False
        while (x >= 0 and x <= WIN_WIDTH and y >= 0 and y <= WIN_HEIGHT):
            x += xIncrement
            y += yIncrement
            if (not foodFound):
                if (foodX == x and foodY == y):
                    foodFound = True
            if (not bodyFound):
                for i in range(1, len(self.body)):
                    if ((x == self.body[i]['x']) and (y == self.body[i]['y'])):
                        bodyFound = True
            if (not bodyFound and not foodFound):
                value = 0
            elif (not bodyFound and foodFound):
                value = 1
            elif (bodyFound and not foodFound):
                value = 2
            else:
                value = 3
        return value

I am using sigmoid as the activation function. Although I have tried with tanh and relu as well with no luck. Below is the NEAT config file that I am using:
[NEAT]
fitness_criterion     = max
fitness_threshold     = 10000
pop_size              = 200
reset_on_extinction   = False

[DefaultGenome]
# node activation options
activation_default      = sigmoid
activation_mutate_rate  = 0.0
activation_options      = sigmoid

# node aggregation options
aggregation_default     = sum
aggregation_mutate_rate = 0.0
aggregation_options     = sum

# node bias options
bias_init_mean          = 0.0
bias_init_stdev         = 1.0
# was 30 max and -30 for min bias
bias_max_value          = 100.0
bias_min_value          = -100.0
bias_mutate_power       = 0.5
bias_mutate_rate        = 0.7
bias_replace_rate       = 0.3

# genome compatibility options
compatibility_disjoint_coefficient = 1.0
compatibility_weight_coefficient   = 0.5

# connection add/remove rates
conn_add_prob           = 0.8
conn_delete_prob        = 0.56

# connection enable options
enabled_default         = True
# below was 0.01
enabled_mutate_rate     = 0.3

feed_forward            = True
initial_connection      = full

# node add/remove rates
node_add_prob           = 0.7
node_delete_prob        = 0.4

# network parameters
num_hidden              = 0
num_inputs              = 24
num_outputs             = 4

# node response options
response_init_mean      = 1.0
response_init_stdev     = 0.0
response_max_value      = 30.0
response_min_value      = -30.0
response_mutate_power   = 0.0
response_mutate_rate    = 0.0
response_replace_rate   = 0.0

# connection weight options
weight_init_mean        = 0.0
weight_init_stdev       = 1.0
weight_max_value        = 30
weight_min_value        = -30
weight_mutate_power     = 0.5
weight_mutate_rate      = 0.8
weight_replace_rate     = 0.1

[DefaultSpeciesSet]
compatibility_threshold = 3.0

[DefaultStagnation]
species_fitness_func = max
max_stagnation       = 20
species_elitism      = 2

[DefaultReproduction]
elitism            = 2
survival_threshold = 0.2

If anyone has any insights or thoughts that could help improve the performance of the snake AI, please let me know.
","['neural-networks', 'game-ai', 'genetic-algorithms', 'neat', 'neuroevolution']",
Neural networks for sports betting,"
I want to design a neural network that can be used for predicting sports scores for betting, specifically for American football.  What Iâ€™d like to do is create a kind of profile for each game based on the specific strengths and weaknesses of each team.
For example, letâ€™s say two teams have the following characteristics:
Team A:

Passing Offense Rating: 5
Rushing Offense Rating: 2

Team B:

Passing Defense Rating: 3
Rushing Defense Rating: 4

Iâ€™d like to be able to search for historical games where two teams have similar profiles.  I could perhaps then narrow it down to games with profiles that have statistically significant historical outcomes (i.e., certain types of matchups are likely to result in similar results).
In reality, Iâ€™d have dozens of team characteristics to compare.  I would then need to assign weights of importance to each characteristic, which could be used to further ensure the effective selection of similar games.
I think I could do this like a convolutional neural network where there is an additional filter applied to the characteristics for the weights.
Are there any other ways that are specifically applicable to this strategy?
","['neural-networks', 'convolutional-neural-networks', 'pattern-recognition', 'algorithm-request', 'model-request']",
Why is there a sigmoid function in the hidden layer of a neural network? [duplicate],"







This question already has answers here:
                                
                            




What is the purpose of an activation function in neural networks?

                                (5 answers)
                            

Closed 3 years ago.




I got this slide from CMU's lecture notes. The $x_i$s on the right are inputs and the $w_i$s are weights that get multiplied together then summed up at each hidden layer node. So I'm assuming this is a node in the hidden layer.  
What is the mathematical reason for taking the sum of the weights and inputs and inputting that into a sigmoid function? Is there something the sigmoid function provides mathematically or provides some sort of intuition useful for the next layer?
","['neural-networks', 'activation-functions', 'hidden-layers', 'function-approximation']","Let us suppose we have a network without any functions in between. Each layer consists of a linear function. i.eConsider a 2 layer neural network, the outputs from layer one will be:
    x2 = W1*x1 + b1
Now we pass the same input to the second layer, which will beOh no! We still got a linear function. No matter how many layers we add, we will still get a linear function. In that case, our network will never be able to approximate any non linear functions.So what is the solution?We will simply add some non linear functions in between. These functions are called activation functions. Some of these functions include:and there are a lot more of them.Yay! Our network is no more linear!We have a lot of different non linear functions, and each of them serve a different purpose.For example, ReLU is simple and computationally cheap.
    ReLU(x) = max(0, x)
Sigmoid outputs are between 0 and 1.
tanh is similar to sigmoid, but zero centered, with outputs from -1 to 1
Softmax is usually used if you want to represent any vector as a discrete probability distribution.Hope you are having a great day!"
Can we say: the more we increase the numbers of cross validation the less likely it is that we overfit?,"
Based on the answer of my previous question:
How can I avoid overfitting when doing parameter tuning?
Can we say: the more we increase the numbers K of cross validation the less likely it is that we overfit?
",['cross-validation'],"In general, no.There is a tradeoff between making the validation set for each fold smaller, and having more folds in total.As an example, if you have $N$ folds for $N$ datapoints, each fold will have only a single datapoint in its validation set. The validation accuracy of a model on a single datapoint is not a reliable estimator for the test performance of the model. In fact, you can construct examples where the error is arbitrarily large.For this reason, people sometimes use Bootstrap Validation if they need a very large number of folds. In practice though, most people just us 10 folds, and that's ""good enough"". "
How can I avoid overfitting when doing parameter tuning?,"
I very often applied a grid search to tune the parameters of my supervised model. I have the feeling that parameter tuning will eventually (very often) lead to overfitting? Is this crazy to say?
Is there a way that we can apply grid search in such a way that it will not overfit?
","['machine-learning', 'overfitting', 'hyperparameter-optimization']","Yes. Usually you would use cross validation to avoid overfitting during parameter tuning. If your dataset is large enough, and you don't try too many parameter combinations, this will work well, because to ""get lucky"" and overfit, a parameter combination will need to work very well on many variations of the problem, which is less likely than working well on just one set of data."
How does text classification reduce manpower costs?,"
(I apologize for the title being too broad and the question being not 'technical')
Suppose that my task is to label news articles. This means that given a news article, I am supposed to classify which category that news belong to. Eg, 'Ronaldo scores a fantastic goal' should classify under 'Sports'.
After much experimentation, I came up with a model that does this labeling for me. It has, say, 50% validation accuracy. (Assume that it is the best)
And so I deployed this model for my task (on unseen data obviously). Of course, from a probabilistic perspective, I should get roughly 50% of the articles labelled correctly. But how do I know that which labels are actually correct and which labels need to be corrected? If I were to manually check (say, by hiring people to do so), how is deploying such a model better than just hiring people to do the classification directly? (Do not forget that the manpower cost of developing the model could have been saved.)
","['classification', 'utility']",There are several advantages:
How to calculate the optimal placements for settlements in Catan without an ML algorithm?,"
Is it possible to calculate the best possible placements for settlements in Catan without using an ML algorithm?
While it is trivial to simply add up the numbers surrounding the settlement (highest point location), I'm looking to build a deeper analysis of the settlement locations. For example, if the highest point location is around a sheep-sheep-sheep, it might be better to go to a lower point location for better resource access. It could also weight for complementary resources, blocking other players from resources, and being closer to ports.
It seems feasible to program arithmetically, yet some friends said this is an ML problem. If it is ML, how would one go about training, as the gameboard changes every game?
","['ai-design', 'game-ai', 'game-theory', 'combinatorial-games', 'symbolic-ai']",
Is there a tool to convert from the brat standoff format to CoNLL-U format? [closed],"







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 2 years ago.















Obsolete. This question is obsolete because the content is out of date. It is not currently accepting new answers or interactions.
                                
                            






I've been searching for a tool to convert from the brat standoff format to the CoNLL-U format, so that to use it as a parsing corpus model to the spaCy library.
Can you help me?
","['natural-language-processing', 'resource-request']",
Why feed actions in later layer in Q network?,"
I read the DDPG paper, in which the authors state that the actions are fed only later to their Q network:

Actions were not included until the 2nd hidden layer of Q. (Sec 7, Experiment Details)

So does that mean, that the input of the first hidden layer was simply the state and the input of the second hidden layer the output of the first hidden layer concatenated with the actions?
Why would you do that? To have the first layer focus on learning the state value independent of the selected action? How would that help?
Is this just a small little tweak or a more significant improvement?
","['neural-networks', 'reinforcement-learning', 'q-learning', 'markov-decision-process', 'ddpg']","So does that mean, that the input of the first hidden layer was simply the state and the input of the second hidden layer the output of the first hidden layer concatenated with the actions?Yes.Why would you do that? To have the first layer focus on learning the state value independent of the selected action? How would that help?Neural networks hidden layers learn representations that get progressively closer to a linear relationship with the target, layer by layer.So the first layer would not be learning the state value per se, but some representation of the state that was better related to the action value at the output.Neural network architectures are often established by experimentation, so I expect here they tried the idea and the performance was OK. If they do not give alternative architectures in the paper, then the precise reason is not clear.I can try a few guesses:Concatenating the actions with the states in the input layer would result in more parameters for the neural network to achieve the same accuracy, running slightly slower. That's because more weights would be required to link the input layer with the first hidden layer.Separating the layers of state and action inputs is a form of regularisation, as the first layer has to produce features that are useful for all possible actions.You don't want the neural network to construct a reverse map $\pi(a|s) \rightarrow Q(s,a)$, you want it to independently assess the action values to do its job as a critic. By having the states and actions presented in different laters, this reduces the chance of the neural network finding shortcuts due to ""recognising the policy"" to predict the values. That's because the state and action pairings may repeat (so be recognised), but the first layer activations change over time (so even with repeated state/action pairs, this is a new representation to learn from).Is this just a small little tweak or a more significant improvement?I don't know, and suggest looking through previous work by the same authors in case they describe the approach in more detail."
Why is my implementation of REINFORCE algorithm for portfolio optimization not converging?,"
I'm trying to implement the Reinforce algorithm (Monte Carlo policy gradient) in order to optimize a portfolio of 94 stocks on a daily basis (I have suitable historical data to achieve this). The idea is the following: on each day, the input to a neural network comprises of the following: 

historical daily returns (daily momenta) for previous 20 days for each of the 94 stocks
the current vector of portfolio weights (94 weights)

Therefore states are represented by 1974-dimensional vectors. The neural network is supposed to return a 94-dimensional action vector which is again a vector of (ideal) portfolio weights to invest in. Negative weights (short positions) are allowed and portfolio weights should sum to one. Since the action space is continuous I'm trying to tackle it via the Reinforce algorithm. Rewards are given by portfolio daily returns minus trading costs. Here's a code snippet:
class Policy(nn.Module):
    def __init__(self, s_size=1974, h_size=400, a_size=94):
        super().__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)
        self.state_size = 1974
        self.action_size = 94
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        means = self.forward(state).cpu()
        m = MultivariateNormal(means,torch.diag(torch.Tensor(np.repeat(1e-8,94))))
        action = m.sample()
        action[0] = action[0]/sum(action[0])
        return action[0], m.log_prob(action)

Notice that in order to ensure that portfolio weights (entries of the action tensor) sum to 1 I'm dividing  by their sum. Also notice that I'm sampling from a multivariate normal distribution with extremely small diagonal terms since I'd like the net to behave as deterministically as possible. (I should probably use something similar to DDPG but I wanted to try out simpler solutions to start with).
The training part looks like this:
optimizer = optim.Adam(policy.parameters(), lr=1e-3)

def reinforce(n_episodes=10000, max_t=10000, gamma=1.0, print_every=1):
    scores_deque = deque(maxlen=100)
    scores = []
    for i_episode in range(1, n_episodes+1):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        for t in range(max_t):
            action, log_prob = policy.act(state)
            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action.detach().flatten().numpy())
            rewards.append(reward)
            if done:
                break 
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        discounts = [gamma**i for i in range(len(rewards)+1)]
        R = sum([a*b for a,b in zip(discounts, rewards)])

        policy_loss = []
        for log_prob in saved_log_probs:
            policy_loss.append(-log_prob * R)
        policy_loss = torch.cat(policy_loss).sum()

        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 10:
            print('Episode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))
            print(scores[-1])

    return scores, scores_deque

scores, scores_deque = reinforce()

Unfortunately, there is no convergence during training even after fiddling with the learning rate so my question is the following: is there anything blatantly wrong with my approach here and if so, how should I tackle this?
","['reinforcement-learning', 'training', 'convergence', 'reinforce', 'portfolio-optimization']",
Is there any disadvantage of the maximum number of fitness function call as a stop criterion?,"
I'm studying different stop criteria in genetic algorithms and the advantages and disadvantages of each of them for evaluating different algorithms. One of these methods is the max number of fitness function calls (max NFFC), so that we define a value for max NFFC and, if the number of fitness function calls reached this value, the algorithm will stop. Fitness function is called for calculating the fitness of the initial population and whenever a crossover or mutation happens (if parents are chosen as offspring there is no need to compute fitness function).
I searched if there is a disadvantage or limitation about using this stop criterion, but I didn't find anything. So, I wanted to know if applying this stop criterion in my algorithm has any disadvantages or there is nothing wrong with using this criterion.
","['genetic-algorithms', 'fitness-functions', 'stopping-conditions']",
When should I create a custom loss function?,"
I'm using a neural network to solve a multi regression problem because I'm trying to predict continuous values. To be more specific, I'm making a tracking algorithm to track the position of an object, I'm trying to predict two values, the latitude and longitude of an object.
Now, to calculate the loss of the model, there are some common functions, like mean squared error or mean absolute error, etc., but I'm wondering if I can use some custom function, like this, to calculate the distance between the two longitude and latitude values, and then the loss would be the difference between the real distance (calculated from the real longitude and latitude) and the predicted distance (calculated from the predicted longitude and latitude). These are some thoughts from me, so I'm wondering if such an idea would make sense?
Would this work in my case better than using the mean squared error as a loss function?
I had another question in mind. In my case, I'm predicting two values (longitude and latitude), but is there a way to transform these two target values to only one value so that my neural network can learn better and faster? If yes, which method should I use? Should I calculate the summation of the two and make that as a new target? Does this make sense?
","['neural-networks', 'deep-learning', 'python', 'objective-functions', 'regression']",
"How does the Kullback-Leibler divergence give ""knowledge gained""?","
I'm reading about the KL divergence on Wikipedia. I don't understand how the equation gives ""information gained"" as it says in the ""Interpretations"" section

Expressed in the language of Bayesian inference, ${\displaystyle D_{\text{KL}}(P\parallel Q)}$ is a measure of the information gained by revising one's beliefs from the prior probability distribution $Q$ to the posterior probability distribution $P$

I was under the impression that KL divergence is a way of measure the difference between distributions (used in autoencoders to determine the difference between the input and the output generated from latents).
How does the equation $$D_{KL}(P \| Q)=\sum P(x) \log \left( \frac{P(X)}{Q(X)} \right)$$ give us a divergence? Also, in encoding and decoding algorithms that use KL divergence, is the goal to minimize $D_{KL}(P \| Q)$?
","['machine-learning', 'terminology', 'variational-autoencoder', 'information-theory', 'kl-divergence']","You can know it better, if you know the concept of entropy:Information entropy is the average rate at which information is produced by a stochastic source of data. The information content (also called the surprisal) of an event ${\displaystyle E}$  is an increasing function of the reciprocal of the ${\displaystyle p(E)}$ of the event, precisely ${\displaystyle I(E)=-\log _{2}(p(E))=\log _{2}(1/p(E))}$. Shannon defined the entropy Î— of a discrete random variable ${\textstyle X}$ with possible values ${\textstyle \left\{x_{1},\ldots ,x_{n}\right\}}$ and probability mass function ${\textstyle \mathrm {P} (X)}$ as:
$${\displaystyle \mathrm {H} (X)=\operatorname {E} [\operatorname {I} (X)]=\operatorname {E} [-\log(\mathrm {P} (X))].}$$
Here ${\displaystyle \operatorname {E} }$ is the expected value operator, and $I$ is the information content of ${\displaystyle I(X)}$ is itself a random variable. The entropy can explicitly be written as
$${\displaystyle \mathrm {H} (X)=-\sum _{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})}}$$
where $b$ is the base of the logarithm used.Now, the KL divergence, try to find the cross entropy of two probability distributions.So, what is the cross entropy:The cross entropy between two probability distributions ${\displaystyle p}$ and ${\displaystyle q}$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution ${\displaystyle q}$, rather than the true distribution  ${\displaystyle p}$.The cross entropy for the distributions ${\displaystyle p}$ and  ${\displaystyle q}$ over a given set is defined as follows:
$${\displaystyle H(p,q)=\operatorname {E} _{p}[-\log q]}.$$
The definition may be formulated using the Kullbackâ€“Leibler divergence${\displaystyle D_{\mathrm {KL} }(p\|q)}$ of  ${\displaystyle q}$ from ${\displaystyle p}$ (also known as the relative entropy of ${\displaystyle p}$ with respect to ${\displaystyle q}$).
$${\displaystyle H(p,q)=H(p)+D_{\mathrm {KL} }(p\|q)},$$
where ${\displaystyle H(p)}$ is the entropy of ${\displaystyle p}$.Now, as we want to approximate a distribution function $p$ with other distribution function $q$, we want to minimize the cross entropy between these two. The first part $H(p)$ could not be changed as we want to find a distribution $q$ and $p$ is given. Hence, we need to minimize $KL$ divergence of these two, to minimize the cross entropy and better approximation for the $p$ distribution."
When should I use a linear activation instead of ReLU?,"
I have read this post:
How to choose an activation function?.
There is enough literature about activation functions, but when should I use a linear activation instead of ReLU? 
What does the author mean with ReLU when I'm dealing with positive values, and a linear function when I'm dealing with general values.?
Is there a more detail answer to this?
","['neural-networks', 'deep-learning', 'comparison', 'activation-functions']","The activation function you choose depends on the application you are building/data that you have got to work with. It is hard to recommend one over the other, without taking this into account.Here is a short-summary of the advantages and disadvantages of some common activation functions:
https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/What does the author mean with ReLU when I'm dealing with positive values, and a linear function when I'm dealing with general values.ReLU is good for inputs > 0, since ReLU = 0 if input < 0(which would kill the neuron, if the gradient is = 0)To remedy this, you could look into using a Leaky-ReLU instead.
(Which avoids killing the neuron by returning a non-zero value in the cases of input <= 0)"
Confusion about temporal difference learning [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I have a couple of small questions about the David Silver lecture about reinforcement learning, lecture slides  (slides 23, 24). More specifically it is about the temporal difference algorithm: 
$$V(s_{t}) \leftarrow  V(s_t)+ \alpha \left[ G_{t+1}+\gamma V(s_{t+1})- V(s_t) \right]$$
where $\gamma$ is our discount rate and $\alpha $ the learning rate. 
In the example given in the lecture slides we observe the following paths: 
$(A,1,B,0), (B,1), (B,1), (B,1), (B,1), (B,1), (B,1), (B,0)$
Meaning for the first trajectory we are in state $A$, get reward $1$, get to state $B$ and get reward $0$ and the game finishes. For the second trajectory we start in state $B$, get reward $1$ and the game finishes ...
Let`s say we initialize all states with value $0$ and choose $\alpha=0.1, \gamma=1$
My first question is whether the following ""implementation"" of the $TD(0)$ algorithm for the first two of the above observed trajectories correct?

$V(a)\leftarrow0 + 0.1(1+0-0)= 0.1; \quad V(b)\leftarrow0+0.1(1+0-0)=0.1$
$V(b)\leftarrow0.1+(0.1)(1+0-0.1)= 0.19$ 

? If so, why dont we use the updated value function for $V(b)$ to also update our value for $V(a)$?
My third question is about the statement that 

$TD(0)$ converges to solution of max likelihood Markov model

this means that if we keep sampling and apply the $TD(0)$ algorithm that the thereby obtained solution converges towards the ML-estimate of that sample using the Markov Model? Why dont we just use the ML-estimate immediately?
","['reinforcement-learning', 'temporal-difference-methods', 'reinforce']",
What is the underlying model of IBM Watson Assistant and Microsoft LUIS?,"
As I stated in my question, I would like to know the underlying pipeline and machine learning models that are used to classify intents and identify entities in IBM Watson Assistant and Microsoft LUIS services.
I searched on different websites and the documentations of those services, but I did not find anything. However, there are some blogs mentioned that IBM Watson is trained using one billion words from Wikipedia but there is no reference to support that claim.
I highly appreciate if anyone could refer me to a doc/blog that answers my question.
Thanks in advance :)
","['machine-learning', 'deep-learning', 'natural-language-processing', 'chat-bots']",
Is there a deep learning-based architecture for digit localisation?,"
I'm new to object detectors and segmentation. I want to localize digits on a plate as fast as possible. All images of the dataset are normalized to $300 \times 60$. There are different approaches to solve the problem. For example, binarization + connected component labeling, vertical and horizontal projection. The aforementioned approaches fail in ambient lights, noises, and shadows. Also, there are other approaches such as STN-OCR (based on convolutional recurrent neural networks) that need a lot of plates with different composition of numbers. I have limited plates with the same numbers (about 1000 different numbers) but totally 10000 plates in different illuminations and noises. I have a good OCR (without segmentation), so I need a network just localize digits. 
Is there any deep learning-based architecture for this purpose? Can I use faster RCNN? Yolo? SSD?
I trained Faster RCNN in Matlab, but it detects too many random bounding boxes for each plate. What could be the problem?
","['deep-learning', 'optical-character-recognition', 'image-segmentation']",
How can I remove the noise from an EEG signal?,"
I am working on a project that takes signals from the brain, preprocesses them, and then makes the machine learn about what human is thinking about. I am struck on preprocessing the signal (incoming from the EEG). I am having a problem when I attempt to remove noise. I used SVM but to no avail. I need some other suggestions from experts who have worked on a project similar to this. What can I do to preprocess the signal?
","['machine-learning', 'signal-processing', 'data-preprocessing']",
Using ML to encypher data for production,"
I am looking for research and experience working with ML models to ingest data for tasks, like text analysis, and creates a system that copies (or in other words enciphers) the input data, to then reproduce it in the future without the original.
I'm interested in how ML models can be used in this way to obfuscate information without too much information loss by the model, e.g. overfitting on purpose to create a new representation of the input information.
","['machine-learning', 'autoencoders']",
What's going on in the equation of the variational lower bound?,"

I don't really understand what this equation is saying or what the purpose of the ELBO is. How does it help us find the true posterior distribution?
","['deep-learning', 'statistical-ai', 'variational-autoencoder', 'evidence-lower-bound']",
What would be the most effective self-learning algorithm for a 7 player social deduction game?,"
There's this 7 player social deduction game called Secret Hitler, and I have been trying to find a self-learning AI algorithm to learn how to play this game for a while.  Basically, four players are given a liberal role, two players are given a fascist role, and 1 player is given a hitler role.  The liberals and hitler do not know any other roles, and the fascists know everyone's roles.  During a turn, a president elects a chancellor based on a yes/no vote and then the government passes a policy (either liberal or fascist) that is drawn from a randomized deck. At certain points in the game, different special abilities come into play, like executing a player or investigating their role. To win the game, the liberals must either enact 5 liberal policies or kill hitler; the fascists must enact 6 fascist policies or get hitler enacted as the chancellor after 3 fascist policies have been enacted.  
Now, there are other details that are irrelevant that I didn't mention, but those are the general rules. It seems simple enough to build a visual implementation in a language like Java, but there are so many moving pieces that I would have to account for.  I doubt that simply making random moves at first and learning off of the bad/good moves would work, because I need a way for agents to make moves based on which roles they know.  
Unfortunately, AlphaZero wouldn't work here, and I'm struggling to find any algorithm that would work for this (or any other social deduction game).  Do I have to write my own algorithm?  I'm slightly confident that this is a case of supervised learning where I can give weight to the nodes in a neural network that correspond to wins, but please correct me if I'm incorrect. 
","['neural-networks', 'machine-learning', 'deep-learning', 'game-ai', 'games-of-chance']","It is very likely that you want an algorithm like Counterfactual Minimax Regret. This algorithm has several variants, but they differ mostly in their efficiency.CFR is the algorithm that was used to solve 2-Player Poker, although the solution comes from one of its more advanced versions. The algorithm is highly applicable to other games of incomplete information and games with roles, like Secret Hitler. Essentially, it learns how to act so that the actions it takes leak as little information to the other players as possible about its role (or hand of cards), while also learning about the roles of other players from their actions, and maximizing its chances of winning the game."
What is the best approach for multivariable and multivariate regression?,"
I want to build a multivariable and multivariate regression model in Keras (with TensorFlow as backend), that is, a regression model with multiple values as input (multivariable) and output (multivariate). 
The independent variables are, for example, the length, weight, force, etc., and the dependent variables are the torque, friction, heat, temperature, etc.
What is the best approach to achieve that? Any guidance before I start?  (If anyone can share any example code/notebook/code would be great as well).
","['machine-learning', 'keras', 'regression']","You create a model like this very easily with keras. Follow these steps.Before building a machine learning model, you must explore your data first. An approach is to use libraries to visualize the data as graphs. You can use the tool pandas-profilinghttps://towardsdatascience.com/exploring-your-data-with-just-1-line-of-python-4b35ce21a82dYou should look for the following:You should look out for them and use appropriate data cleaning or filtering method to clean them. For example, Nan or missing value may be substituted with 0 and outliers will be removed. Input features may also be normalized.Keras is a easy tool for building machinea learning model. For how to build a basic MLP ( Multi-Layer Perceptron), you can refer to the example code and the resource.https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/After creating the basic model, you can start experimenting with stuff like:You will gain more experience as you go. There is a lot of ways to improve the performance of the model.Here are things to beware:Hope I can help you and have fun!"
What are the risks associated with regulating AI?,"
As part of a research project for college, I would like to understand what many of you astern to be the risks associated with regulating Artificial Intelligence. Such as whether regulation is too risky in regards to limiting progress or too risky in regards to uninformed regulation.
","['philosophy', 'social', 'risk-management']",
What are examples of promising AI/ML techniques that are computationally intractable?,"
To produce tangible results in the field of AI/ML, one must take theoretical results under the lens of computational complexity.
Indeed, minimax effectively solves any two-person ""board game"" with win/loss conditions, but the algorithm quickly becomes untenable for games of large enough size, so it's practically useless asides from toy problems.
In fact, this issue seems to cut at the heart of intelligence itself: the Frame Problem highlights this by observing that any ""intelligent"" agent that operates under logical axioms must somehow deal with the explosive growth of computational complexity.
So, we need to deal with computational complexity: but that doesn't mean researchers must limit themselves with practical concerns. In the past, multilayered perceptrons were thought to be intractable (I think), and thus we couldn't evaluate their utility until recently. I've heard that Bayesian techniques are conceptually elegant, but they become computationally intractable once your dataset becomes large, and thus we usually use variational methods to compute the posterior, instead of naively using the exact solution.
I'm looking for more examples like this: What are examples of promising (or neat/interesting) AI/ML techniques that are computationally intractable (or uncomputable)?
","['machine-learning', 'reference-request', 'computational-complexity', 'intractability']",
Is there any measure of separability of classes?,"
I want to know if there is a measure of how well two classes in Y are separable (linearly or not) based on their features in X. Easiest way of explaining this is to compare it to correlation coefficients, the higher the correlation the higher possiblity for successful regression based on given feature (at least in theory). 
Is there any measure that will tell me how well classes are separated based on input data features, before training a ML model? 
","['machine-learning', 'classification', 'statistical-ai']",Are you thinking something like Information Gain?Information Gain basically uses the concept of information entropy to determine if splitting a variable is useful.
Is there any open source implementation of the SBEED learning algorithm?,"
Are there are any openly available implementations of the SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation paper?
","['reinforcement-learning', 'function-approximation', 'resource-request']",
What can I do with an autoencoder? [duplicate],"







This question already has answers here:
                                
                            




What are the purposes of autoencoders?

                                (4 answers)
                            

Closed 3 years ago.



I cannot find information in detail about autoencoder
What can I do with an autoencoder (and how can I do this), practically speaking? 
What does the encoder (this part I think I understand) and a decoder (could not find much about this) part do? Can it for example show on an explainable way how patterns in the data are being represented?
I read some papers that say that it can be used to denoise the input, how does this work? (Am I changing the values of my input)
Is it true that an autoencoder can be also done with PCA (if we assume linearity)?
","['machine-learning', 'autoencoders']",
What's a good generative model for creating valid formats of a person's name?,"
I'm trying to come up with a generative model that can input a name and output all valid formats of it.  
For example, ""Bob Dylan"" could be an input and the gen model will output ""Dylan, Bob"", ""B Dylan"", ""Bob D"" and any other type of valid formatting of a person's name.  So given my example the gen model doesn't seem that complicated to build, but it also has to handle stuff like ""Dylan, Bob"" and ""B Dylan"", but obviously the 2nd one it shouldn't output ""Bob Dylan"" as a potential output cause inferring that requires more than just ""B Dylan"". Any ideas for a good Generative Model for this?
",['generative-model'],
Is there a way to understand neural networks without using the concept of brain?,"
Is there a way to understand, for instance, a multi-layered perceptron without hand-waving about them being similar to brains, etc?
For example, it is obvious that what a perceptron does is approximating a function; there might be many other ways, given a labelled dataset, to find the separation of the input area into smaller areas that correspond to the labels; however, these ways would probably be computationally rather ineffective, which is why they cannot be practically used. However, it seems that the iterative approach of finding such areas of separation may give a huge speed-up in many cases; then, natural questions arise why this speed-up may be possible, how it happens and in which cases.
One could be sure that this question was investigated. If anyone could shed any light on the history of this question, I would be very grateful.
So, why are neural networks useful and what do they do? I mean, from the practical and mathematical standpoint, without relying on the concept of ""brain"" or ""neurons"" which can explain nothing at all.
","['neural-networks', 'machine-learning', 'function-approximation', 'history']",
Maximize loss on non-target variable,"
I have a neural network that should be able to classify documents to target label A. The problem is that the network is actually classifying label B, which is an easier task.
To make the problem more clear: I need to classify documents from different sources. In the training data each source occurs repeatedly, but the network should be able to work on unknown sources. All documents from a single source have the same class. In this case, it is easier to identify sources than the target label so in practice the network is not really identifying the target label, but the source.
The solution to this problem is making sure that the model is bad at identifying the sources in the training data, while still attaching the right target labels.
I think the first step is to get two output layers, one for the target label and one for identifying which source it is from. My approach fails however at the training procedure: I want to minimize the loss on the target output, but maximize the loss on the non-target output. But if I maximize the loss on that non-target output, that does not mean that the network 'unlearns' the non-target labels. So the main question for the non-target output is:
TLDR; How do I define a training procedure that minimizes the loss on a non-target output layer, and then maximizes that loss on all layers before it. My goal is to have a network that is good at classifying label A, but bad at a related label B. If anyone wants to give a code example, my prefered framework is PyTorch.
","['neural-networks', 'objective-functions']",
Understanding the partial derivative with respect to the weight matrix and bias,"
Say we have the layer $X W + b = Y$.

I want to get $\frac{dL}{dW}$ and we assume I have $\frac{dL}{dY}$.
So all I need is to find $\frac{dY}{dW}$. I know that it should be $X^T\frac{dL}{dY}$ but don't understand why. please explain.
I want to get $\frac{dL}{db}$ and we assume I have $\frac{dL}{dY}$.
So all I need is to find $\frac{dY}{db}$. I know that it should be $\sum(\frac{dL}{dY})_i$ (I mean sum the rows) but I don't understand why. please explain.

Thanks :)
","['neural-networks', 'machine-learning', 'backpropagation', 'gradient-descent']",
How can I determine the bias and variance of a random forrest?,"
On this website https://scikit-learn.org/stable/modules/learning_curve.html, the authors are speaking about variance and bias and they give a simple example of how works in a linear model.
How can I determine the bias and variance of a random forest?
","['machine-learning', 'random-forests', 'bias-variance-tradeoff']","To gain a good understanding of this, I recommend first reading about the trade-off between bias and variance in ML and AI methods.A great article on this topic that I recommend as a light mathematical introduction is this:
https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229In short:
Bias represents the models effort to generalize samples, as opposed to Variance that represents the models effort to conform to new data.
A high bias, low variance model will thus look more like a straight(underfitted) line, while a low bias, high variance model will look jagged and all-over the place(overfitted).In essence, you need to find a balance between the two to avoid both overfitting(high variance, low bias) and underfitting(high bias, low variance) for your specific application.But how can I determine this for a model such as a Random Forrest classifier?To determine your models bias and variance configuration(if either is too high/low), you can look at the models performance on the validation and test set.
The very reason we divide our data into training-validation-test sets, is so that we can validate the models performance when it is presented with samples it has not seen during training."
How to interpret this learning curve plot,"
Bellow I have a Learning Curve plot How should I interpret this plot for my random forrest algorithm (the second one the most complex one)? 
Which one is the best?



","['machine-learning', 'cross-validation', 'random-forests']","Note the X index is training set size. For the first and second case, teh training set size starts at 0(or 1). The model will overfit certainly at that data size. When data size increases, the model overfits less and less and eventually the model have enough data samples that it won't overfit. The data size continue to increase and the model performance increase as well. To a certain point, the validation loss increase start to diminish and the model slightly overfits the samples.  For the third graph, it seems like originally teh loss is low and started to increase.
Hopes it help"
Why is my neural network giving me wildly incorrect error and not changing accuracy?,"
My full code is as follows. I have tried to whittle it down to just the code that matters, but the problem I have is that i'm not sure what part of my network code is producing the problem. I've removed my code that loads and sifts through the CSV data because then my code would be too long.
#include <iostream>
#include <array>
#include <random>
#include <chrono>
#include <iomanip>
#include <fstream>
#include <algorithm>
#include <iomanip>
#include <variant>
#include <unordered_set>


typedef std::variant<std::string,std::uint_fast16_t,bool,float> CSVType;

/* ... functions to load CSV data ... */

typedef float DataType;
typedef DataType (*ActivationFuncPtr)(const DataType&);

DataType step(const DataType& x, const DataType& threshold)
{
    return x >= threshold ? 1 : 0;
}

DataType step0(const DataType& x)
{
    return step(x,0);
}

DataType step05(const DataType& x)
{
    return step(x,0.5);
}

DataType sigmoid(const DataType& x)
{
    return DataType(1) / (DataType(1) + std::exp(-x));
}

DataType sigmoid_derivative(const DataType& x)
{
    return x * (DataType(1) - x);
}

template<std::size_t NumInputs>
class Neuron
{
public:

    Neuron()
    {
        RandomiseWeights();
    }

    void RandomiseWeights()
    {
        std::generate(m_weights.begin(),m_weights.end(),[&]()
        {
            return m_xavierNormalDis(m_mt);
        });
        m_biasWeight = 0;

        for(std::size_t i = 0; i < NumInputs+1; ++i)
            m_previousWeightUpdates[i] = 0;
    }

    DataType FeedForward(const std::array<DataType,NumInputs> inputValues)
    {
        DataType output = m_biasWeight;
        for(std::size_t i = 0; i < inputValues.size(); ++i)
            output += inputValues[i] * m_weights[i];

        m_inputValues = inputValues;

        return output;
    }

    std::array<DataType,NumInputs> Backpropagate(const DataType& error)
    {
        std::array<DataType,NumInputs> netInputOverWeight;
        for(std::size_t i = 0; i < NumInputs; ++i)
        {
            netInputOverWeight[i] = m_inputValues[i];
        }

        DataType netInputOverBias = DataType(1);

        std::array<DataType,NumInputs> errorOverWeight;
        for(std::size_t i = 0; i < NumInputs; ++i)
        {
            errorOverWeight[i] = error * netInputOverWeight[i];
        }

        DataType errorOverBias = error * netInputOverBias;

        for(std::size_t i = 0; i < NumInputs; ++i)
        {
            m_outstandingWeightAdjustments[i] = errorOverWeight[i];
        }
        m_outstandingWeightAdjustments[NumInputs] = errorOverBias;

        DataType errorOverNetInput = error;

        std::array<DataType,NumInputs> errorWeights;
        for(std::size_t i = 0; i < NumInputs; ++i)
        {
            errorWeights[i] = errorOverNetInput * m_weights[i];
        }

        return errorWeights;
    }

    void AdjustWeights(const DataType& learningRate, const DataType& momentum)
    {
        for(std::size_t i = 0; i < NumInputs; ++i)
        {
            DataType adjustment = learningRate * m_outstandingWeightAdjustments[i] + momentum * m_previousWeightUpdates[i];
            m_weights[i] = m_weights[i] - adjustment;
            m_previousWeightUpdates[i] = adjustment;
        }
        DataType adjustment = learningRate * m_outstandingWeightAdjustments[NumInputs] + momentum * m_previousWeightUpdates[NumInputs];
        m_biasWeight = m_biasWeight - adjustment;
        m_previousWeightUpdates[NumInputs] = adjustment;
    }

    const std::array<DataType,NumInputs>& GetWeights() const
    {
        return m_weights;
    }

    const DataType& GetBiasWeight() const
    {
        return m_biasWeight;
    }

protected:

    static std::mt19937 m_mt;
    static std::uniform_real_distribution<DataType> m_uniformDisRandom;
    static std::uniform_real_distribution<DataType> m_xavierUniformDis;
    static std::normal_distribution<DataType> m_xavierNormalDis;

    std::array<DataType,NumInputs> m_weights;
    DataType m_biasWeight;

    std::array<DataType,NumInputs+1> m_previousWeightUpdates;
    std::array<DataType,NumInputs+1> m_outstandingWeightAdjustments;

    std::array<DataType,NumInputs> m_inputValues;
};

template<std::size_t NumInputs>
std::mt19937 Neuron<NumInputs>::m_mt(std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::system_clock::now().time_since_epoch()).count());

template<std::size_t NumInputs>
std::uniform_real_distribution<DataType> Neuron<NumInputs>::m_uniformDisRandom(-1,1);

template<std::size_t NumInputs>
std::uniform_real_distribution<DataType> Neuron<NumInputs>::m_xavierUniformDis(-std::sqrt(6.f / NumInputs+1),std::sqrt(6.f / NumInputs+1));

template<std::size_t NumInputs>
std::normal_distribution<DataType> Neuron<NumInputs>::m_xavierNormalDis(0,std::sqrt(2.f / NumInputs+1));

template<std::size_t NumNeurons>
class ActivationLayer
{
public:

    ActivationLayer()
    :
        m_outputs({})
    {}

    virtual std::array<DataType,NumNeurons> GetOutputs() const final
    {
        return m_outputs;
    }

    virtual void CompleteBackprop(const DataType& learningRate, const DataType& momentum) final
    {
    }

protected:
    std::array<DataType,NumNeurons> m_outputs;
};

template<std::size_t NumNeurons>
class SigmoidActivation : public ActivationLayer<NumNeurons>
{
public:

    virtual std::array<DataType,NumNeurons> FeedForward(const std::array<DataType,NumNeurons>& inputValues)
    {
        for(std::size_t i = 0; i < NumNeurons; ++i)
            ActivationLayer<NumNeurons>::m_outputs[i] = sigmoid(inputValues[i]);
        return ActivationLayer<NumNeurons>::m_outputs;
    }

    virtual std::array<DataType,NumNeurons> Backpropagate(const std::array<DataType,NumNeurons> errors)
    {
        std::array<DataType,NumNeurons> backpropErrors;
        for(std::size_t i = 0; i < NumNeurons; ++i)
            backpropErrors[i] = errors[i] * sigmoid_derivative(ActivationLayer<NumNeurons>::m_outputs[i]);
        return backpropErrors;
    }
};

template<std::size_t NumInputs, std::size_t NumNeurons>
class FullyConnectedLayer
{
public:

    FullyConnectedLayer()
    :
        m_neurons([=]()
        {
            std::array<Neuron<NumInputs>,NumNeurons> neurons;
            for(Neuron<NumInputs>& n : neurons)
                n = Neuron<NumInputs>();
            return neurons;
        }())
    {
    }

    virtual std::array<DataType,NumNeurons> FeedForward(const std::array<DataType,NumInputs>& inputValues)
    {
        std::array<DataType,NumNeurons> outputValues;
        for(std::size_t i = 0; i < NumNeurons; ++i)
            outputValues[i] = m_neurons[i].FeedForward(inputValues);
        return outputValues;
    }

    /** \brief Take a sum of errors for each node and produce the errors for each input node in the previous layer.
     *
     */

    virtual std::array<DataType,NumInputs>
    Backpropagate(const std::array<DataType,NumNeurons> errors)
    {
        std::array<std::array<DataType,NumInputs>,NumNeurons> errorValues;
        for(std::size_t i = 0; i < NumNeurons; ++i)
        {
            errorValues[i] = m_neurons[i].Backpropagate(errors[i]);
        }
        std::array<DataType,NumInputs> returnErrors;
        std::fill(returnErrors.begin(),returnErrors.end(),0);
        for(std::size_t i = 0; i < NumNeurons; ++i)
        {
            for(std::size_t j = 0; j < NumInputs; ++j)
            {
                returnErrors[j] += errorValues[i][j];
            }
        }
        return returnErrors;
    }

    virtual void CompleteBackprop(const DataType& learningRate, const DataType& momentum)
    {
        for(Neuron<NumInputs>& n : m_neurons)
            n.AdjustWeights(learningRate, momentum);
    }

    const Neuron<NumInputs>& operator[](const std::size_t& index) const
    {
        return m_neurons[index];
    }

    std::array<std::array<DataType,NumInputs>,NumNeurons> GetWeights() const
    {
        std::array<std::array<DataType,NumInputs>,NumNeurons> weights;
        for(std::size_t i = 0; i < NumNeurons; ++i)
        {
            weights[i] = m_neurons[i].GetWeights();
        }
        return weights;
    }

protected:
    std::array<Neuron<NumInputs>,NumNeurons> m_neurons;
};

template<std::size_t I = 0, typename FuncT, typename... Tp>
inline typename std::enable_if<I == sizeof...(Tp)>::type for_each(std::tuple<Tp...> &, FuncT)
{
}

template<std::size_t I = 0, typename FuncT, typename... Tp>
inline typename std::enable_if<I < sizeof...(Tp)>::type for_each(std::tuple<Tp...>& t, FuncT f)
{
    f(std::get<I>(t)); // call f, passing the Ith element of the std::tuple t and the existing output O
    for_each<I + 1, FuncT, Tp...>(t, f); // process the next element of the tuple with the new output
}

template<std::size_t I = 0, typename FuncT, typename... Tp>
inline typename std::enable_if<I == sizeof...(Tp)>::type for_each(const std::tuple<Tp...> &, FuncT)
{
}

template<std::size_t I = 0, typename FuncT, typename... Tp>
inline typename std::enable_if<I < sizeof...(Tp)>::type for_each(const std::tuple<Tp...>& t, FuncT f)
{
    f(std::get<I>(t)); // call f, passing the Ith element of the std::tuple t and the existing output O
    for_each<I + 1, FuncT, Tp...>(t, f); // process the next element of the tuple with the new output
}

template<std::size_t I = 0, typename FuncT, typename O, typename FinalOutput, typename... Tp>
inline typename std::enable_if<I == sizeof...(Tp)>::type for_each_get_final_output(std::tuple<Tp...> &, FuncT, O o, FinalOutput& finalOutput)
{
    finalOutput = o;
}

template<std::size_t I = 0, typename FuncT, typename O, typename FinalOutput, typename... Tp>
inline typename std::enable_if<I < sizeof...(Tp)>::type for_each_get_final_output(std::tuple<Tp...>& t, FuncT f, O o, FinalOutput& finalOutput)
{
    auto newO = f(std::get<I>(t),o); // call f, passing the Ith element of the std::tuple t and the existing output O
    for_each_get_final_output<I + 1, FuncT, decltype(newO), FinalOutput, Tp...>(t, f, newO, finalOutput); // process the next element of the tuple with the new output
}

template<std::size_t I = 0, typename FuncT, typename O, typename... Tp>
inline typename std::enable_if<I == 0>::type for_each_reverse_impl(std::tuple<Tp...>& t, FuncT f, O o)
{
    f(std::get<0>(t),o);
}

template<std::size_t I = 0, typename FuncT, typename O, typename... Tp>
inline typename std::enable_if<(I > 0)>::type
for_each_reverse_impl(std::tuple<Tp...>& t, FuncT f, O o)
{
    auto newO = f(std::get<I>(t),o); // call f, passing the Ith element of the std::tuple t and the existing output O
    for_each_reverse_impl<I - 1, FuncT, decltype(newO), Tp...>(t, f, newO); // process the next element of the tuple with the new output
}

template<typename FuncT, typename O, typename... Tp>
inline void for_each_reverse(std::tuple<Tp...>& t, FuncT f, O o)
{
    for_each_reverse_impl<sizeof...(Tp)-1, FuncT, O, Tp...>(t, f, o);
}

enum class LOSS_FUNCTION : std::uint_fast8_t
{
    MEAN_SQUARE_ERROR,
    CROSS_ENTROPY
};

class ValidationOptions
{
public:
    enum class METRIC : std::uint_fast8_t { NONE, ACCURACY, LOSS };

    ValidationOptions()
    :
        m_validationSplit(.3f),
        m_enableLoss(false),
        m_lossFunction(LOSS_FUNCTION::MEAN_SQUARE_ERROR),
        m_enableAccuracy(false),
        m_outputFilter([](const DataType& x){ return x; }),
        m_earlyStoppingMetric(METRIC::NONE),
        m_earlyStoppingPatience(1.f),
        m_earlyStoppingDelta(1.f),
        m_earlyStoppingNumEpochsAverage(1)
    {}

    ValidationOptions& Loss(const bool enable = true, LOSS_FUNCTION lossFunction = LOSS_FUNCTION::MEAN_SQUARE_ERROR)
    {
        m_enableLoss = enable;
        m_lossFunction = lossFunction;
        return *this;
    }

    ValidationOptions& Split(const float dataSplitValidation)
    {
        m_validationSplit = dataSplitValidation;
        return *this;
    }

    ValidationOptions& Accuracy(const bool enable = true, ActivationFuncPtr outputFilter = [](const DataType& x){return x;})
    {
        m_enableAccuracy = enable;
        m_outputFilter = outputFilter;
        return *this;
    }

    ValidationOptions& EarlyStop(const bool enable = true,
                                 const METRIC metric = METRIC::ACCURACY,
                                 const float patience = .1f,
                                 const DataType delta = .01,
                                 const std::size_t epochNumToAverage = 10)
     {
         if(enable == false)
            m_earlyStoppingMetric = METRIC::NONE;
         else
            m_earlyStoppingMetric = metric;

         m_earlyStoppingPatience = patience;
         m_earlyStoppingDelta = delta;
         m_earlyStoppingNumEpochsAverage = epochNumToAverage;

         return *this;
     }

     float GetValidationSplit() const { return m_validationSplit; }
     bool Loss() const { return m_enableLoss; }
     LOSS_FUNCTION GetLossFunction() const { return m_lossFunction; }
     bool Accuracy() const { return m_enableAccuracy; }
     ActivationFuncPtr GetOutputFilter() const { return m_outputFilter; }
     METRIC GetEarlyStoppingMetric() const { return m_earlyStoppingMetric; }
     float GetEarlyStoppingPatience() const { return m_earlyStoppingPatience; }
     DataType GetEarlyStoppingDelta() const { return m_earlyStoppingDelta; }
     std::size_t GetEarlyStoppingNumEpochsAvg() const { return m_earlyStoppingNumEpochsAverage; }

protected:

    float m_validationSplit;        /**< Percentage of the data set aside for validation */

    bool m_enableLoss;
    LOSS_FUNCTION m_lossFunction;   /**< Loss function to use */

    bool m_enableAccuracy;
    ActivationFuncPtr m_outputFilter;   /**< When measuring accuracy data is passed through this */

    METRIC m_earlyStoppingMetric;                   /**< The metric used to stop early */
    float m_earlyStoppingPatience;                  /**< Percentage of total epochs to wait before stopping early */
    DataType m_earlyStoppingDelta;                  /**< The amount that the early stopping metric needs to change in a single step before stopping */
    std::size_t m_earlyStoppingNumEpochsAverage;    /**< The number of epochs averaged over to smooth out the stopping metric */
};

template<typename... Layers>
class NeuralNetwork
{
public:

    NeuralNetwork(Layers... layers)
    :
        m_layers(std::make_tuple(layers...))
    {

    }

    template<std::size_t NumFeatures, std::size_t NumOutputs, std::size_t NumTrainingRows>
    void Fit(const std::size_t& numberEpochs,
             const std::size_t& batchSize,
             DataType learningRate,
             const DataType& momentum,
             std::array<std::array<DataType,NumFeatures>,NumTrainingRows>& trainingData,
             std::array<std::array<DataType,NumOutputs>,NumTrainingRows>& trainingOutput,
             const ValidationOptions validationOptions,
             const bool linearDecayLearningRate = true,
             std::ostream& outputStream = std::cout)
    {
        std::size_t epochNumber = 0;

        // need to support more than just MSE to measure loss
        std::vector<DataType> lastEpochLoss(validationOptions.GetEarlyStoppingNumEpochsAvg(),0);
        DataType lastEpochLossAverage = std::numeric_limits<DataType>::max();

        std::vector<DataType> lastValidationAccuracys(validationOptions.GetEarlyStoppingNumEpochsAvg(),0);
        DataType lastValidationAccuraryAvg = 0;

        std::vector<std::size_t> randomIndices(NumTrainingRows,0);
        for(std::size_t i = 0; i < NumTrainingRows; ++i)
            randomIndices[i] = i;

        std::random_shuffle(randomIndices.begin(),randomIndices.end());
        // take some percentage as validation split
        // we do this by taking the first percentage of already shuffled indices and removing them
        // from what is available
        std::size_t numValidationRecords = NumTrainingRows*validationOptions.GetValidationSplit();
        std::size_t numTrainingRecords = NumTrainingRows - numValidationRecords;
        std::vector<std::size_t> validationRecords(numValidationRecords);
        for(std::size_t i = 0; i < numValidationRecords; ++i)
        {
            std::size_t index = randomIndices.back();
            randomIndices.pop_back();
            validationRecords[i] = index;
        }

        while(epochNumber < numberEpochs)
        {
            // shuffle the indices so that they are pulled into each batch randomly each time
            std::random_shuffle(randomIndices.begin(),randomIndices.end());

            DataType epochLoss = 0;

            std::tuple<Layers...> backupLayers = m_layers;

            for(std::size_t batchNumber = 0; batchNumber < std::ceil(numTrainingRecords / batchSize); ++batchNumber)
            {
                std::array<DataType,NumOutputs> propagateError = {0};

                std::size_t startIndex = batchNumber * batchSize;
                std::size_t endIndex = startIndex + batchSize;
                if(endIndex > numTrainingRecords)
                    endIndex = numTrainingRecords;

                DataType batchLoss = 0;

                for(std::size_t index = startIndex; index < endIndex; ++index)
                {
                    std::size_t row = randomIndices[index];
                    const std::array<DataType,NumFeatures>& dataRow = trainingData[row];
                    const std::array<DataType,NumOutputs>& desiredOutputRow = trainingOutput[row];

                    // Feed the values through to the output layer
                    // use of ""auto"" is so this lambda can be used for all layers without
                    // me needing to do any fucking around
                    std::array<DataType,NumOutputs> finalOutput;
                    for_each_get_final_output(m_layers, [](auto& layer, auto o)
                    {
                        return layer.FeedForward(o);
                    }, dataRow, finalOutput);

                    DataType totalError = 0;
                    for(std::size_t i = 0; i < NumOutputs; ++i)
                    {
                        if(validationOptions.GetLossFunction() == LOSS_FUNCTION::MEAN_SQUARE_ERROR)
                            totalError += std::pow(desiredOutputRow[i] - finalOutput[i],2.0);
                        else if(validationOptions.GetLossFunction() == LOSS_FUNCTION::CROSS_ENTROPY)
                        {
                            if(NumOutputs == 1)
                            {
                                // binary cross entropy
                                totalError += (desiredOutputRow[i] * std::log(1e-15 + finalOutput[i]));
                            }
                            else
                            {
                                // cross entropy
                            }
                        }
                    }

                    batchLoss += totalError;
                }

                batchLoss *= DataType(1) / (endIndex - startIndex);

                for(std::size_t i = 0; i < NumOutputs; ++i)
                    propagateError[i] = batchLoss;

                // update after every batch
                for_each_reverse(m_layers, [](auto& layer, auto o)
                {
                    auto errors = layer.Backpropagate(o);
                    return errors;
                }, propagateError);

                // once backprop is finished, we can adjust all the weights
                for_each(m_layers, [&](auto& layer)
                {
                    layer.CompleteBackprop(learningRate,momentum);
                });

                epochLoss += batchLoss;
            }

            epochLoss *= DataType(1) / numTrainingRecords;

            lastEpochLoss.erase(lastEpochLoss.begin());
            lastEpochLoss.push_back(epochLoss);
            DataType avgEpochLoss = 1.f * std::accumulate(lastEpochLoss.begin(),lastEpochLoss.end(),0.f) / (epochNumber < validationOptions.GetEarlyStoppingNumEpochsAvg() ? epochNumber+1 : lastEpochLoss.size());

            if(validationOptions.GetEarlyStoppingMetric() == ValidationOptions::METRIC::LOSS
               && epochNumber > numberEpochs * validationOptions.GetEarlyStoppingPatience()
               && avgEpochLoss > lastEpochLossAverage + validationOptions.GetEarlyStoppingDelta())
            {
                // the loss average has decreased, so we should go back to the previous run and exit
                std::cout   << ""Early exit Loss Avg \n""
                            << ""Last Epoch: "" << lastEpochLossAverage << ""\n""
                            << ""This Epoch: "" << avgEpochLoss << std::endl;
                m_layers = backupLayers;
                break;
            }

            lastEpochLossAverage = avgEpochLoss;

            // check for the error against the reserved validation set
            std::size_t numCorrect = 0;
            for(std::size_t row = 0; row < validationRecords.size(); ++row)
            {
                const std::array<DataType,NumFeatures>& dataRow = trainingData[row];
                const std::array<DataType,NumOutputs>& desiredOutputRow = trainingOutput[row];

                std::array<DataType,NumOutputs> finalOutput;
                for_each_get_final_output(m_layers, [](auto& layer, auto o)
                {
                    return layer.FeedForward(o);
                }, dataRow, finalOutput);

                bool correct = true;
                for(std::size_t i = 0; i < NumOutputs; ++i)
                {
                    if(validationOptions.GetOutputFilter()(finalOutput[i]) != desiredOutputRow[i])
                        correct = false;
                }
                if(correct)
                    ++numCorrect;
            }

            DataType validationAccuracy = DataType(numCorrect) / numValidationRecords;

            lastValidationAccuracys.erase(lastValidationAccuracys.begin());
            lastValidationAccuracys.push_back(validationAccuracy);
            DataType avgValidationAccuracy = std::accumulate(lastValidationAccuracys.begin(),lastValidationAccuracys.end(),0.f) / (epochNumber < validationOptions.GetEarlyStoppingNumEpochsAvg() ? epochNumber+1 : lastValidationAccuracys.size());

            if(validationOptions.GetEarlyStoppingMetric() == ValidationOptions::METRIC::ACCURACY
               && epochNumber > numberEpochs * validationOptions.GetEarlyStoppingPatience()
               && avgValidationAccuracy < lastValidationAccuraryAvg - validationOptions.GetEarlyStoppingDelta())
            {
                // the accuracy has decreased, so we should go back to the previous run and exit
                std::cout   << ""Early exit validation accuracy \n""
                            << ""Last Epoch: "" << lastValidationAccuraryAvg << ""\n""
                            << ""This Epoch: "" << avgValidationAccuracy << std::endl;
                m_layers = backupLayers;
                break;
            }

            lastValidationAccuraryAvg = avgValidationAccuracy;

            outputStream << epochNumber << "","" << epochLoss << "","" << avgEpochLoss << "","" << validationAccuracy << "","" << avgValidationAccuracy << std::endl;

            learningRate -= learningRate / (numberEpochs-epochNumber);

            ++epochNumber;
        }
    }

    template<std::size_t NumFeatures, std::size_t NumOutputs, std::size_t NumEvaluationRows>
    void Evaluate(std::array<std::array<DataType,NumFeatures>,NumEvaluationRows> inputData,
                  std::array<std::array<DataType,NumOutputs>,NumEvaluationRows> correctOutputs,
                  DataType& loss,
                  DataType& accuracy,
                  ActivationFuncPtr outputFilter = [](const DataType& x){return x;})
    {
        loss = 0;

        std::size_t numCorrect = 0;

        for(std::size_t row = 0; row < NumEvaluationRows; ++row)
        {
            const std::array<DataType,NumFeatures>& dataRow = inputData[row];
            const std::array<DataType,NumOutputs>& outputRow = correctOutputs[row];

            // Feed the values through to the output layer

            std::array<DataType,NumOutputs> finalOutput;
            for_each_get_final_output(m_layers, [](auto& layer, auto o)
            {
                layer.FeedForward(o);
                return layer.GetOutputs();
            }, dataRow, finalOutput);

            DataType thisLoss = 0;
            for(std::size_t i = 0; i < NumOutputs; ++i)
                thisLoss += outputRow[i] - finalOutput[i];
            loss += thisLoss * thisLoss;

            bool correct = true;
            for(std::size_t i = 0; i < NumOutputs; ++i)
            {
                if(outputFilter(finalOutput[i]) != outputRow[i])
                    correct = false;
            }
            if(correct)
                ++numCorrect;
        }

        loss *= DataType(1) / NumEvaluationRows;
        accuracy = DataType(numCorrect) / NumEvaluationRows;
    }

    template<std::size_t NumFeatures, std::size_t NumOutputs, std::size_t NumRecords>
    void Predict(std::array<std::array<DataType,NumFeatures>,NumRecords> inputData,
                 std::array<std::array<DataType,NumOutputs>,NumRecords>& predictions,
                 ActivationFuncPtr outputFilter = [](const DataType& x){return x;})
    {
        for(std::size_t row = 0; row < NumRecords; ++row)
        {
            const std::array<DataType,NumFeatures>& dataRow = inputData[row];

            // Feed the values through to the output layer

            std::array<DataType,NumOutputs> finalOutput;
            for_each_get_final_output(m_layers, [](auto& layer, auto o)
            {
                return layer.FeedForward(o);
            }, dataRow, finalOutput);

            for(std::size_t i = 0; i < NumOutputs; ++i)
                predictions[row][i] = outputFilter(finalOutput[i]);
        }
    }

protected:
    std::tuple<Layers...> m_layers;
};

main()
{
    std::vector<std::vector<CSVType>> trainingCSVData;
    /* load training CSV data */

    std::vector<std::vector<CSVType>> testCSVData;
    /* load test CSV data */

    std::cout << std::fixed << std::setprecision(80);

    std::ofstream file(""error_out.csv"", std::ios::out | std::ios::trunc);
    if(!file.is_open())
    {
        std::cout << ""couldn't open file"" << std::endl;
        return 0;
    }

    file << std::fixed << std::setprecision(80);

    /*
        Features
        1   pClass 1
        2   pClass 2
        3   pClass 3
        4   Sex female 1, male 0
        5   Age normalised between 0 and 1  age range 0 to 100
        6   Number siblings between 0 and 1   num range 0 to 8
        7   Number of parents / children        num range 0 to 9
        8   Ticket cost   between 0 and 1       num range 0 to 512.3292
        9   embarked S
        10  embarked Q
        11  embarked C
    */

    std::array<std::array<DataType,29>,891> inputData;
    std::array<std::array<DataType,1>,891> desiredOutputs;

    /* ... data that loads the titanic data into a series of features. Either class labels or normalised values (like age) */

    NeuralNetwork neuralNet{
        FullyConnectedLayer<29,256>(),
        SigmoidActivation<256>(),
        FullyConnectedLayer<256,1>(),
        SigmoidActivation<1>()
    };

    neuralNet.Fit(300,
                  1,
                  0.05,
                  0.25f,
                  inputData,
                  desiredOutputs,
                  ValidationOptions().Accuracy(true,step05).Loss(true,LOSS_FUNCTION::CROSS_ENTROPY).Split(0.3),
                  false,
                  file);

    file.close();

    return 0;
}

The data used is from the titanic problem that you can download from Kaggle here.
The typical output file that's being generated is like this:
0,-4.91843843460083007812500000000000000000000000000000000000000000000000000000000000,-4.91843843460083007812500000000000000000000000000000000000000000000000000000000000,0.65168541669845581054687500000000000000000000000000000000000000000000000000000000,0.65168541669845581054687500000000000000000000000000000000000000000000000000000000
1,-6.14257431030273437500000000000000000000000000000000000000000000000000000000000000,-6.14257431030273437500000000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
2,-6.43130302429199218750000000000000000000000000000000000000000000000000000000000000,-6.43130302429199218750000000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
3,-6.58864736557006835937500000000000000000000000000000000000000000000000000000000000,-6.58864736557006835937500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
4,-6.70884752273559570312500000000000000000000000000000000000000000000000000000000000,-6.70884752273559570312500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
5,-6.78206682205200195312500000000000000000000000000000000000000000000000000000000000,-6.78206682205200195312500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
6,-6.86832284927368164062500000000000000000000000000000000000000000000000000000000000,-6.86832284927368164062500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
7,-6.92110681533813476562500000000000000000000000000000000000000000000000000000000000,-6.92110681533813476562500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
8,-6.96584081649780273437500000000000000000000000000000000000000000000000000000000000,-6.96584081649780273437500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
9,-7.02414274215698242187500000000000000000000000000000000000000000000000000000000000,-7.02414274215698242187500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000
10,-7.06421041488647460937500000000000000000000000000000000000000000000000000000000000,-7.06421041488647460937500000000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000,0.65543073415756225585937500000000000000000000000000000000000000000000000000000000

(shortened for space)
This was previously working when the error I fed back through backprop was just the difference between the correct result and the prediction. But I've since been told that I should be propagating back the Loss Functions error, which I then implemented as Binary Cross Entropy.
So either:

I should not feed back the error from the loss function
I am calculating the loss incorrectly
My back propagation code is wrong
I've got something horrible happening in my validation loop

","['neural-networks', 'backpropagation']",
Reinforcement learning number of episodes per epoch not matching with paper,"
I am trying to reproduce results presented in this paper. On page 4, the authors state:

... we train for 50 epochs (one epoch consists of 19*2*50 = 1900 full
  episodes), which amounts to a total of 4.75*10^6 timesteps.

The 1900 episodes are broken down into Rollouts per MPI worker (2) * Number of MPI Workers (19) * Cycles per epoch (50), as shown in the hyper parameters section on page 10.
When testing on my local machine, using the GitHub Baselines repo, I am using 1 MPI worker and the following hyperparams:
'n_cycles': 50,  # per epoch
'rollout_batch_size': 2,  # per mpi thread

By the same calculation, this means that I should have 1*50*2 = 100 episodes per epoch.
However when I run her on FetchReach-v1 turns out I only have 10 episodes per epoch. Here is a log sample:
Training...
---------------------------------
| epoch              | 0        |
| stats_g/mean       | 0.893    |
| stats_g/std        | 0.122    |
| stats_o/mean       | 0.269    |
| stats_o/std        | 0.0392   |
| test/episode       | 10       |
| test/mean_Q        | -0.602   |
| test/success_rate  | 0.5      |
| train/episode      | 10       |  <-- 10 episodes/epoch
| train/success_rate | 0        |
---------------------------------

Why is there this discrepancy? Any suggestions would be appreciated.
","['deep-learning', 'reinforcement-learning']",
Is it possible to use the GPT-2 model for time-series data prediction?,"
Is it possible and how trivial (or not) might it be (if possible) to retrain GPT-2 on time-series data instead of text?
","['natural-language-processing', 'prediction', 'time-series', 'gpt']",
How should I interpret this validation plot?,"
Bellow I have a validation plot
How should I interpret this validation plot?
Is my data underfitting? What else can be seen from this?
Which one is the best?
What does it mean that the right line is growing and green line decrease (slightly) for example after 15?

Second random forrest

","['machine-learning', 'cross-validation', 'random-forests']","This is a sign of overfitting.As you make your trees deeper, it becomes possible to ""memorize"" the data: each leaf of the tree is just a single point. The trees begin to learn patterns that do not exist. When you try out these patterns on new data (which is what cross-validation is imitating), then the patterns do not work, and your model fails to generalize.The main piece of information to draw from this plot is that the optimum tree depth is about 15."
How to plot Loss Landscape with more than 2 weights in the network,"
For a single neuron with 2 weights, I can plot the loss landscape and it looks like this (OR data, sigmoid activation, MAE loss):

But, when the neuron accepts more inputs, which means more than 2 weights required, or when there are more neurons, more layers in the network; how should the 3D loss landscape be plotted?
","['machine-learning', 'gradient-descent', 'plotting', 'loss', 'weights']","It seems not possible to plot loss values (z) against all combinations of weights in all layers, especially when the network is big with thousands or millions of params; in that case, the number of points to plot is too too big.And also, the 3D space can't be used to plot more than 3 dimensions.However, with a deep network with lots of weights, these can be plotted:Example plot when there are 2 neurons in the layer right before output layer (of 1 neuron):"
How can neural networks be used to generate rather than classify?,"
In my experience with Neural Nets, I have only used them to take input vectors and return binary output.
But, here in a video, https://youtu.be/ajGgd9Ld-Wc?t=214, Kai Fu Lee, renowned AI Expert shows a deep net which takes thousands of samples of Trump's speeches and generates output in the Chinese Language.
In short, how can deep nets/neural nets be used to generate output rather than giving answer yes or no? Additionally, how are these nets being trained? Can anyone here provide me a simple design to nets that are capable of doing that?
","['neural-networks', 'deep-learning', 'natural-language-processing', 'recurrent-neural-networks']","Think of a neural network as a universal function approximator (With infinite width under a set of constraints this is actually provable). Now when discussing generation in the context you have provided, you essentially want to draw from some distribution $p(y|c)$ where $y$ is your output and $c$ is your context or input.  Theorem: For any distribution $\Omega$, if we take $z \sim \mathcal{N}(0,I)$, there exists a function $f$ where $f(z) \sim \Omega$. Given the above theorem (for the purposes of this post I don't need to prove it, but its very similar to the universal approximation theorem proof) and if we take neural networks as a pseudo-universal function approximator, if we have a valid objective or training procedure that can learn the parameters of $f$, sampling is as easy as sampling $\mathcal{N}(0,I)$ and then applying $f$.  So the trick really is finding a good training procedure, and this is where you see GANs, VAEs and other models/schemes come into play.  Everything I've said above works really well when there isn't autocorrelation like in text, but when there is, the above methodology would result in a combinatorially large output space which isn't realistic with a vocabulary size usually spanning somewhere between a couple thousand and a couple hundred thousand. So to handle this they model the joint by taking advantage of that autocorrelation by modeling the joint probability as its bayesian decomposition. 
$$p(\vec w) = p(w_0)\prod_{i=1}^{N-1}p(w_i|w_{<i})$$
Now that there is a framework to efficiently model this type of output, were back into the position as before where we are looking for clever training schemes. In this case you'll see commonly RNN's or other sequential model training with teacher forcing (@nbro described this in his answer too), or using GAN like compositions using either reinforcement learning to handle the lack of differentiability in sampling or using approximations like Gumbel-Softmax or Intermediate Loss Sampling (method I actually developed)  I hope this answered your question."
How do I determine the most appropriate classifier for a certain problem?,"
Consider a Bayesian classifier used in spam e-mail filtering. It converts an e-mail to a vector, most of the time using the bag-of-words method. Although it learns first before getting employed, it can be made to work as an online system, i.e. it can be used to filter and learn from examples even after deployment.
Now, on the other hand, now comes the perceptron. It calculates a mean vector of spam and not spam, and then classifies them into the appropriate categories. The model adjusts the mean vectors each time it makes mistakes.
Now, comes neural nets, they too are capable of taking a vector-like bag of words or image pixels of dogs and cats and classify them into yes or no.
So, while designing and implementing them into the system, how to determine which one of the methods (Bayesian classifier, perceptron or neural network) is the most appropriate for a given situation or task? One factor to consider is the time complexity (or speed), but what are other factors, and how to rank them? 
","['neural-networks', 'classification', 'comparison', 'perceptron', 'bayesian-networks']",
"Is artificial intelligence and, in particular, neural networks being used in real-world critical applications?","
Is artificial intelligence and, in particular, neural networks being used in real-world critical applications and devices?
I had a discussion with my colleague who states that nobody would use artificial intelligence, especially neural nets, for critical stuff, like technical devices or sensors.
I'm only aware of the problem of neural nets being so-called black-boxes, but, nevertheless, I think it is possible to make an NN robust so that it matches the demands of daily processes, also in sensitive fields like health care, energy market, self-driving cars, and so on. Yet I cannot underline this.
Does somebody have more insights or other information, opinions and so on? I appreciate any meaningful answer.
","['neural-networks', 'applications']","This is as much an ethical concern as a practical one.AI systems are already reaching or exceeding human performance in many critical areas. Consider the detection of common cancers, where AI systems match or exceed humans. Another good example is Tesla's Autopilot, which is actually safer than human drivers, but gets a lot of bad press when it makes a mistake. Both of these systems are likely using Neural Networks, possibly alongside other heuristic or rule-drive approaches.The issue isn't whether these systems can be ""safe enough"" for everyday use. They are safe enough in a societal sense already. The concern is that the people who die when these systems make mistakes are randomly selected, whereas the people who die when a human performs the work die because a human makes a mistake (usually). This is difficult to accept for the same reason that some people are scared of flying, even though it is many times safer than driving the same distance: there is a loss of control, and ""good"" people may die through no fault of their own, or perhaps through events that are no one's fault.Whether we use this systems will thus probably depend on the application. In Medicine it's easy to see a case: we can do more tests than before for the same price. People who can afford to have a doctor review the machine's decisions are probably no worse off. People who couldn't afford this already are better off (they get a diagnosis with some positive predictive power value now, instead of none at all). In driving, it's more complicated, and will probably require further development. No one knows for sure how good self-driving cars can get, but they'll probably get somewhat better over time. Maybe they'll get good enough that they more or less never kill people, or maybe they won't.I actually think your friend is way off about the use cases he thinks AI is not suitable for though. Check out this article in Military Embedded Systems. In applications where decisions are made on a pure cost/benefit basis (and not based on people's gut feelings about morality), AI systems are actually easier to adopt, and are already often better than human operators. This trend seems likely to increase in the future, so ""technical devices and sensors"", which are often black-boxes to the lay public anyway, seem like they are among the first things to go."
How to choose the weights for a linear combination of heuristic functions?,"
I need to write a minimax algorithm with alpha-beta pruning in limited time for the 2048 game. I know expectimax is better for this work. 
Assume I wrote different heuristic functions. If I want to write an evaluation function as a linear combination of these heuristic functions, do I have to give random weights, or can I calculate the optimal weights with some optimization algorithm? 
","['search', 'minimax', 'heuristics', 'alpha-beta-pruning', 'expectiminimax']",
Which courses in computer science and logic are relevant to Machine Learning?,"
Although I have a decent background in math, I'm trying to understand which courses from CS and logic to look into. My aim is to get into a Machine Learning PhD program.
",['machine-learning'],"I worked as a professor for a time, and often advised students on this. For a PhD in machine learning, I think the ideal background is:Together these will allow you to read and write the code that even highly optimized versions of ML algorithms are written in, and to understand what might be going wrong within them.2. AI & ML courses, usually offered through a CS departmentYou may also take other AI courses, but they are not as common to see offered to undergraduates, so many students wait until graduate school:Together, these will give you the broadest possible background in AI & ML. These can allow you to find new applications of ML, or to pull AI techniques from one area into another as you need.3. Statistics coursesMuch stronger would be to also take courses in:These courses allow you to reason formally and comfortably about uncertainty. They also give you the correct framework for answering questions about whether your ML algorithm is working, and what patterns an ML algorithm uncovers mean.4. Mathematics coursesThese courses give you the basic mathematical fluency to understand most machine learning algorithms well."
Assigning Weighting Factors,"
I have a hypothetical example that closes to my research problem:
Assume you are a boss and you have different types of tasks that you need to assign to your employee. Sensitive task (very classified), and task that requires high skills. So you need to assign a sensitive task (government document) to the trusted employee. While the other task (e.g. statistical analysis ) can be assigned to employee who is more creative and smart. Now every day you have many tasks that need to be done and have a large number of employees with a number of crowdsources (freelancers). 
You have an outcome and history of trust and performance along of failure rate of assigned task on that day of these employees as:

As you can see here on day 1: the trust of emp 111 is good, so on that day, he had a low failure rate of the sensitive task. While his performance is low, and that made other task failed a lot.
So now assume you have a sensitive task coming, and you have a pool of workers.
The basic equation might not good here: Trust + Performance. I need to weigh each factor based on the type of tasks. 
Trust x w1  +  performance x w2  ::::: w1 is high coefficient when sensitive is coming. 
Any idea of how I model these issues. 
","['machine-learning', 'deep-learning']",
How to show Sauer's Lemma when the inequalities are strict or they are equalities?,"
I have the following homework.

We proved Sauer's lemma by proving that for every class $H$ of ï¬nite VC-dimension $d$, and every subset $A$ of the domain,
$$
\left|\mathcal{H}_{A}\right| \leq |\{B \subseteq A: \mathcal{H} \text { shatters } B\} | \leq \sum_{i=0}^{d}\left(\begin{array}{c}{|A|} \\ {i}\end{array}\right)
$$
Show that there are cases in which the previous two inequalities are strict (namely, the $\leq$ can be replaced by $<$) and cases in which they can be replaced by equalities. Demonstrate all four combinations of $=$ and $<$.

How can I solve this problem?
","['computational-learning-theory', 'pac-learning', 'vc-dimension', 'homework']",
"An ""elevator pitch"" breakdown of areas of applications for Reinforcement Learning & Neural Networks vs. Genetic Algorithms","
I'm looking for an ""elevator pitch"" breakdown of areas of applications for Reinforcement Learning & Neural Networks vs. Genetic Algorithms, both actual and theoretical.
Links are welcome, but please provide some explanation.
","['neural-networks', 'reinforcement-learning', 'comparison', 'genetic-algorithms', 'applications']",
Emotional Speech Synthesis [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



We are a team of computer science our graduation project about EmotionalSpeech Synthesis.
We've found valuable information like research papers and WaveNet, Tacotron. 
A website (https://www.voicery.com/)
we were hoping to get to know more information from you.
We need more details what should we start with to grasp the fundamentals to build this idea, what is the architecture to be used in this project, whether there are papers, a GitHub Repository containing helpful documentation, datasets, some other resources, previous knowledge. 
","['neural-networks', 'deep-learning', 'recurrent-neural-networks', 'reference-request', 'emotional-intelligence']","Since I am working with audio synthesis projects, I can suggest you to go through the recently released paper ""MelNet: A Generative Model for Audio in the Frequency Domain"", in this architecture human like speech is synthesized, I think this might be better than WaveNet where the generated audio does contain robotic nature, I think this will definitely help you with Emotional speech synthesis as it generates human-like voices.
You can check for online demos as well.DemoHere Is the Paper!Related YouTube Video Hope this helps!"
How can an AI play Flow Free?,"
The game ""Flow Free"" in which you connect coloured dots with lines is very popular. A human can learn techniques to play it.
I was wondering how an AI might approach it. There are certain rules of thumb that a human learns, e.g. connecting dots on the edges one should keep to the edge.
Most of the time it appears the best approach is a depth-first search, e.g. one tries very long paths to see if they work. Combined with rules of thumbs and inferences such as ""don't leave gaps"". Also ""Don't cut off one dot from another dot of the same colour"".
But there are ways to ""not leave gaps"" such as keep within one square of another line. That humans seem to be able to grasp but seems harder for an AI to learn.
In fact I wonder if the rule of thumb ""keep close to other lines"" might even require some kind of internal language.
I mean to even understand the rules of the game one would think one would need language. (Could an ape solve one of these puzzles? I doubt it.)
So basically I'm trying to solve how an AI could come up with these technqiues for solving puzzles like Flow Free. (Techniques that might not work in all cases).
Perhaps, humans have an innate understanding of concepts such as ""keep close to the wall"" and ""don't double back on yourself"" and can combine them in certain ways. Also we are able to spot simple regions quickly bounded by objects.
I think a built in understanding of ""regions"" would be key. And the key concept that dots can't be joined unless they are in the same region. And we have got to a dead-end if:

There is an empty region
There is a region with a dot without it's pair

Still I don't think this is enough. 
","['game-ai', 'game-theory']","A few of us have spent quite a bit of time thinking about this. I summarised our work in a Medium article here: https://towardsdatascience.com/deep-learning-vs-puzzle-games-e996feb76162Would love to hear what you think.Spoiler: so far, good old SAT seems to beat fancy AI algorithms!"
"""AI will kill us all! The machines will rise up!"" - what is being done to dispel such myths?","
Science Fiction has frequently shown AI to be a threat to the very existence of mankind. AI systems have often been the antagonists in many works of fiction, from 2001: A Space Odyssey through to The Terminator and beyond.
The Media seems to buy into this trope as well.  And in recent years we have had people like Elon Musk warn us of the dangers of an impending AI revolution, stating that AI is more dangerous than nukes.
And, apparently, experts think that we will be seeing this AI revolution in the next 100 years.
However, from my (albeit limited) study of AI, I get the impression that they are all wrong. I am going to outline my understanding below, please correct me if I am wrong:

Firstly, all of these things seem to be confusing Artificial Intelligence with Artificial Consciousness.  AI is essentially a system to make intelligent decisions, whereas AC is more like the ""self-aware"" systems that are shown in science fiction.

Not AI itself, but intelligence and intelligent decision-making algorithms are something we've been working with and enhancing since before computers have been around.  Moving this over to an artificial framework is fairly easy.  However, consciousness is still something we are learning about.  My guess is we won't be able to re-create something artificially if we barely understand how it works in the real world.

So, my conclusion is that no AI system will be able to learn enough to start thinking for itself, and that all our warnings of AI are completely unjustified.

The real danger comes from AC, which we are a long, long way from realizing because we are still a long way off from defining exactly what consciousness is, let alone understanding it.



So, my question is, assuming that my understanding is correct, are any efforts are being made by companies or organizations that work with AI to correct these popular misunderstandings in sci-fi, the media, and/or the public?
Or are the proponents of AI ambivalent towards this public fear-mongering?
I understand that the fear mongering is going to remain popular for some time, as bad news sells better than good news. I am just wondering if the general attitude from AI organizations is to ignore this popular misconception, or whether a concerted effort is being made to fight against these AI myths (but unfortunately nobody in the media is listening or cares).
","['social', 'artificial-consciousness']","Nothing.  Its in almost everyone's favor for it to stay that way financially. Having non-technical individuals associate AI with terminators makes a perception that the field has greater capabilities than it does $\rightarrow$ this leads to grants, funding, etc...  Is there any negative? Yes. Misconceptions always have drawbacks. We see the creation of dumb ethics boards and such cough cough Elon Musk.But if history has anything to say about this, as the field gains popularity (which it is dnagerously quick), information will spread by definition, and eventually misconceptions will be laid to rest.Note that this answer is biased and based upon my own opinions"
Recurrent Neural Network to track distance from origin,"
I have a game/simulation that takes a vector of encoded sequences of moves (up, down, left, right). Let's say that these are sequential step taken by an ant moving in a 2D space, starting from the origin. The moves are generated randomly. 
I want to know for any game, if the ant gets farther than a certain distance y from the origin (although it might even be closer than y at the end of the game). I would like to classify games into ""ant gets further away than y"" with value of one, or zero for ""ant does not get further away than y"". I don't need an AI for this task, I have set this objective as a training goal for myself. 
I am able to tell if the last position is past y or not, using a regular feed forward network, I believe it is easier because it is as easy as summing up all the moves, regardless of the order. But to tell if the ant got past y and then got back, that still needs to return one.
I thought I might be able to reach my objective through an RNN, encoding the moves as a sequence of one-hot encoded sequential directions to move towards. Currently, I am using one hidden layer (I tried with different sizes ranging from 10 to 100), backpropagating the loss only at the last step of a single training on a vector, but it seems like the RNN total loss doesn't decrease at all.
Is there any obivious flaw in my simulation, or in the neural network model? Is there a category of problems this could belong to?
",['recurrent-neural-networks'],"This kind of problem does not really have a name other than ""toy problem"" since no-one needs to teach an AI to add up, multiply or divide* - there are already far more reliable and far faster ways to achieve that on any computer. What you are doing here is essentially vector addition, applying a distance metric then setting a true/false value based on a comparison. It would be 2 or 3 lines of code in most high-level programming languages.Neural networks can learn any function though, so in theory what you want to do is possible. You should not expect results to be perfect, a statistical learner never actually learns the analytical form of a function or process, just the rough ""shape"" of it.I have not done your experiment. However, your idea to use a RNN seems reasonable. With the details you have given, I can offer a few pieces of general advice:Use a modern RNN gated architecture, either LSTM or GRU. That's because the point in the sequence of moves where you want to set a ""distance exceeded"" toggle could be many steps away from the end of a game. The simplest RNNs (with direct loop backs from output to input within a layer) can easily suffer from vanishing gradients in this situation, whilst LSTM and GRU architectures are designed to deal with it.Generate a lot of training data. You will need many examples of both categories before any neural network will home in on what is causing one class or the other to occur. The learning is based on statistics, not reason.Take a look at related LTSM examples that learn to add binary numbers. Repeat those even simpler experiments first, then move on to your own problem. This will avoid some beginner mistakes with poor choices of hyper-parameters, bad implementations etc.* Humans of course do use intelligence, reasoning and logic to learn reliable procedures for addition, multiplication and division. No doubt someone could be interested in how an AI can replicate that, without starting from built-in capabilties of a CPU (which of course the humans designed and built those procedures into the system at a low level). However, that's at a higher level of AI research than we are dealing with here. "
How do we compute the target value when the agent ends up in the terminal state?,"
I am working on a deep reinforcement learning problem. Throughout the episode, there is a small positive and negative reward for good or bad decisions. In the end, there is a huge reward for the completion of the episode. So, this reward function is quite sparse.
This is my understanding of how DQN works. The neural network predicts quality values for each possible action that can be taken from a state $S_1$. Let us assume the predicted quality value for an action $A$ is $Q(S_1, A)$, and this action allows the agent to reach $S_2$.
We now need the target quality value $Q_\text{target}$, so that using $Q(S_1, A)$ and $Q_\text{target}$ the temporal difference can be calculated, and updates can be made to the parameters of the value network.
$Q_\text{target}$ is composed of two terms. The immediate reward $R$ and the maximum quality value of the resulting state that this chosen action leaves us in, which can be denoted by $Q_\text{future} = \text{max}_a Q(S_2, a)$, which is in practice obtained by feeding the new state $S_2$ into the neural network and choosing (from the list of quality values for each action) the maximum quality value. We then multiply the discount factor $\gamma$ with this $Q_\text{future}$ and add it to the reward $R$, i.e. $Q_\text{target} = R + \gamma \text{max}_a Q(S_2, a) = R + \gamma Q_\text{future}$.
Now, let us assume the agent is in the penultimate state, $S_1$, and chooses the action $A$ that leads him to the completion state, $S_2$, and gets a reward $R$.
How do we form the target value $Q_\text{target}$ for $S_1$ now? Do we still include the $Q_\text{future}$ term? Or is it only the reward in this case? I am not sure if $Q_\text{future}$ even has meaning after reaching the final state $S_2$. So, I think that, for the final step, the target value must simply be the reward. Is this right?
","['reinforcement-learning', 'q-learning', 'dqn', 'deep-rl']","Now, let us assume the agent is in the penultimate state, $S_1$, and
chooses the action $A$ that leads him to the completion state, $S_2$,
and gets a reward $R$.How do we form the target value $Q_\text{target}$ for $S_1$ now? Do we still include the $Q_\text{future}$ term? Or is it only the reward
in this case?Your term ""completion state"" is commonly called ""terminal state"". In a terminal state, there are no more actions to take, no more time steps, and no possibility to take any action. So, by definition, in your state $S_2$, the expected future reward is $0$.Mathematically, this is often noted like $v(S_T) = 0$ or $q(S_T,\cdot) = 0$ with the $T$ standing for last time step of the episode, and dot standing in for the fact that no action need to be supplied, or the specific action value is not relevant. So, therefore using your terms, $Q_\text{future} = \text{max}_a Q(S_2, a) = 0$That makes the equations work in theory, but does not explain what to do in code. In practice in your code, you would do as you suggest and use just the reward when calculating the TD target for $Q(S_1, A)$. This is typically done using an if block around the done condition e.g.Of course, the details depend on how you have structured and named your variables. You will find code similar to this in most DQN implementations though.You should not really try to learn $V(S_2)$ or $Q(S_2, A)$, or calculate TD target starting from $S_2$, because the result should be $0$ by definition."
Resizing effects on image recognition,"
I have been building a multilabel image classification model using inception v3, which uses images of size 299x299, I have been wondering what are the effects of feeding images of rectangular shapes for example (or arbitrary resolutions) on the performance of the model, and if I can define requirements for how the data should be to ensure optimal performance, what would those requirements be ?
Intuitively, I think that square images would perform better than rectangular images, is this true?
","['neural-networks', 'image-segmentation']",
When and how to use a mix of loss functions for back-propagation?,"
I am trying to understand the best loss function to be used with a convolutional neural network. I came to know that we can mix two loss functions. Can any body share in what case was it done and how?
","['convolutional-neural-networks', 'backpropagation', 'objective-functions']",
Finetuning GPT-2 twice for particular style of writing on a particular topic,"
Sorry if this is a stupid question. I'm just starting out in ML and am working with gpt-2 for text generation.
My situation is that I have to generate text in a particular field for eg. family businesses, which pretrained gpt-2 is unlikely to have much ""training"" with. Besides the topic, I also need to generate the text in the style of one particular writer (for eg. incorporating their turns of phrase etc) This particular writer hasn't written much about the family business topic unfortunately, but has written about other topics.
It occurred to me that I can take gpt-2, finetune it on a large corpus of material on family businesses, and then finetune the new model on the written material of the particular writer.
Would this be the right way to achieve my objective of creating content on family businesses in the style of this particular writer?
Any suggestions on what sort of stuff I should keep in mind while doing this?
Any help is much appreciated.
","['natural-language-processing', 'training']",
Are there deep networks that can differentiate object class from individual object?,"
We usually categorize objects in a hierarchy of classes. Let us say crow vs bird. In addition, classes can be ""messy"", for instance a crow can be also a predator, but not all birds are predators. 
My question is, can deep networks represent these hierarchies easily? Has anybody studied that? (I could not find anything at all).
","['classification', 'deep-neural-networks', 'object-recognition']",
Why is my implementation of Q-learning not converging to the right values in the FrozenLake environment?,"
I am trying to learn tabular Q learning by using a table of states and actions (i.e. no neural networks). I was trying it out on the FrozenLake environment. It's a very simple environment, where the task is to reach a G starting from a source S avoiding holes H and just following the frozen path which is F. The $4 \times 4$ FrozenLake grid looks like this
SFFF
FHFH
FFFH
HFFG

I am working with the slippery version, where the agent, if it takes a step, has an equal probability of either going in the direction it intends or slipping sideways perpendicular to the original direction (if that position is in the grid). Holes are terminal states and a goal is a terminal state.
Now I first tried value iteration which converges to the following set of values for the states
[0.0688909  0.06141457 0.07440976 0.05580732 0.09185454 0. 0.11220821 0.         0.14543635 0.24749695 0.29961759 0. 0.         0.3799359  0.63902015 0.        ]

I also coded policy iteration, and it also gives me the same result. So I am pretty confident that this value function is correct.
Now, I tried to code the Q learning algorithm, here is my code for the Q learning algorithm
def get_action(Q_table, state, epsilon):
    """"""
    Uses e-greedy to policy to return an action corresponding to state
    
    Args:
        Q_table: numpy array containing the q values
        state: current state
        epsilon: value of epsilon in epsilon greedy strategy
        env: OpenAI gym environment 
    """"""
    return env.action_space.sample() if np.random.random() < epsilon else np.argmax(Q_table[state]) 


def tabular_Q_learning(env):
    """"""
    Returns the optimal policy by using tabular Q learning
    
    Args:
        env: OpenAI gym environment
        
    Returns:
        (policy, Q function, V function) 
    """"""
    
    # initialize the Q table
    # 
    # Implementation detail: 
    # A numpy array of |x| * |a| values
    
    Q_table = np.zeros((env.nS, env.nA))
    
    # hyperparameters
    epsilon = 0.9
    episodes = 500000
    lr = 0.81

    
    for _ in tqdm_notebook(range(episodes)):
        # initialize the state
        state = env.reset()
        
        if episodes / 1000 > 21:
            epsilon = 0.1
        
        t = 0
        while True: # for each step of the episode
            # env.render()
            # print(observation)
         
            # choose a from s using policy derived from Q 
            action = get_action(Q_table, state, epsilon) 
            
            # take action a, observe r, s_dash
            s_dash, r, done, info = env.step(action)
            
            # Q table update 
            Q_table[state][action] += lr * (r + gamma * np.max(Q_table[s_dash]) - Q_table[state][action])
            
            state = s_dash
            
            t += 1
            
            if done:
                # print(""Episode finished after {} timesteps"".format(t+1))
                break
        # print(Q_table)
    
    policy = np.argmax(Q_table, axis=1)
    V = np.max(Q_table, axis=1)
    
    return policy, Q_table, V

I tried running it and it converges to a different set of values which is following [0.26426802 0.03656142 0.12557195 0.03075882 0.35018374 0. 0.02584052 0.         0.37657211 0.59209091 0.15439031 0. 0.         0.60367728 0.79768863 0.        ]
I am not getting, what is going wrong. The implementation of Q learning is pretty straightforward. I checked my code, it seems right.
Any pointers would be helpful.
","['reinforcement-learning', 'q-learning', 'value-iteration', 'policy-iteration', 'frozen-lake']","I was able to solve the problem.The main issue for non-convergence was that I was not decaying the learning rate appropriately. I put a decay rate of $-0.00005$ on the learning rate lr, and subsequently Q-Learning also converged to the same value as value iteration."
"What are sim2sim, sim2real and real2real?","
Recently, I always hear about the terms sim2sim, sim2real and real2real. Will anyone explain the meaning/motivation of these terms (in DL/RL research community)?
What are the challenges in this research area?
Anything intuitive would be appreciated!
","['deep-learning', 'reinforcement-learning', 'computer-vision', 'terminology', 'robotics']",
What are the standard problems for CNNs and LSTMs?,"
What are the standard (or baseline) problems (or at least common ones) for CNNs and LSTMs? As an example, for a feed-forward neural net, a common problem is the XOR problem.
Is there a standard problem like this for CNNs and LSTMs? I think for a CNN the standard test is to try it on MNIST, but I'm not sure of an LSTM.
","['convolutional-neural-networks', 'long-short-term-memory']","It's more domain- or task-specific. There is no obvious baseline anymore because these models and this field has evolved into too large of an ecosystem. Nonetheless, I'll list a couple of notorious examples below.Image classification: Detection/segmentation: Pose estimation: Text classification: Question answering: Translation: This is just a taste. There are tons more both in each category and the number of categories, a good source is the Papers with Code website.  Therefore, there is no single standard problem, given that there are too many that all in one shape or form use CNNs or RNNs (or others). "
"In image classification, why do we usually minimize a cost function rather than maximizing it?","
I was watching a video about policy gradients by Andrej Karpathy. At 10:00, it shows an equation for supervised learning for image classification.
$$\max\sum _{i} \log p(y_i \mid x_i)$$
I have worked with image classification models before, but I always minimized a cost function (aka loss function). I have also never seen someone maximizing a cost function for image classification in the wild.

So, what are the advantages of a minimizing loss function over a maximizing loss function in image classification?

Other than RL, which problems do we solve by maximizing a cost function?


","['deep-learning', 'terminology', 'image-recognition', 'objective-functions', 'policy-gradients']",
How can I combine the readings of multiple lidars into 1 point cloud? [closed],"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 2 years ago.







                        Improve this question
                    



I have a car with 8 lidars, each with a field of view of 60 degrees. My car looks like this:

How can I merge all the lidar readings into 1 point cloud?
","['autonomous-vehicles', 'robots', 'signal-processing']",
How is the reCaptcha useful for Google?,"
I am wondering where Google uses the result from deep learning of reCaptcha (how can a system that knows to recognize street signs is useful somewhere? how they profit from it?)
","['intelligence-testing', 'google', 'captcha']","At this time, Google is aggressively pursuing research into AI of many sorts. All AI at this point is constrained by the size and accuracy of available training data. Getting human curated data is expensive and time consuming. Crowdsourcing through Captcha gets access to that without paying directly. Why roads specifically? It can hardly be a coincidence that Google's holding company also owns a self driving car company Waymo. They need machines to learn what road signs look like in real life, so that they can respond to them. "
Are there methods that allow deep networks to learn object categorization in a self-supervised way?,"
When training a deep network to learn object classification from a set like ImageNet, we minimize the cross entropy between the ground truth and the predicted  categories. This is done in a supervised way. It is my understanding that you can separate categories in an unsupervised way using principal component analysis, but I have never seen that in a deep network. I am curious if this can be done easily in the last case. One possible way to do this would be to minimize a loss that favors categorization into one-hot vectors (this would only guarantee that an image is classified into a single category, rather that the correct category, though). Has this been done, or is there any reason why not?
","['deep-learning', 'object-recognition', 'unsupervised-learning']","I did an experiment, took a trained densenet121 and kept the bottom layers. I trained the FC layer to a softmax and then to a lambda layer that normalized the vector. I trained the network with imagenet to make the outputs the most far a away from (1,1,1,1,1...1) as possible, so I would get one hot vectors. I did, but the network trained to a single category (put all in the same hot vector). Then I added a penalty that encourages to make vectors different is their features are also different. it improved a little, a dozen categories instead of one, bout noting close to a thousand, the available number.I am posting this so nobody wastes his time in a silly idea like this one."
Why hasn't deep learning been used for word level alignment?,"
I've been exploring word-level alignments tools such as MGIZA and it seems to me that there hasn't been any new tool for this problem. Are neural networks not suitable to solve this problem or simply no interest in the area to build new tools?
","['deep-learning', 'natural-language-processing']",
Do neurons of a neural network model a linear relationship?,"
I'm certain that this is a very naive question, but I am just beginning to look more deeply at neural networks, having only used decision tree approaches in the past.  Also, my formal mathematics training is more than 30 years in the past, so please be kind. :)
As I'm reading FranÃ§ois Chollet's book on Deep Learning with Python, I'm struck that it appears that we are effectively treating the weights (kernel and biases) as terms in the standard linear equation ($y=mx+b$). At page 72 of the book, the author writes
output = dot(W, input) + b
output = (output < 0 ? 0 : output)

Am I reading too much into this, or is this correct (and so fundamental I shouldn't be asking about it)?
","['neural-networks', 'activation-functions', 'neurons']","In a neural network (NN), a neuron can act as a linear operator, but it usually acts as a non-linear one. The usual equation of a neuron $i$ in layer $l$ of an NN is $$o_i^l = \sigma(\mathbf{x}_i^l \cdot  \mathbf{w}_i^l + b_i^l),$$ where $\sigma$ is a so-called activation function, which is usually a non-linearity, but it can also be the identity function, $\mathbf{x}_i^l$ and $\mathbf{w}_i^l$ are the vectors that respectively contain the inputs and the weights for neuron $i$ in layer $l$, and $b_i^l \in \mathbb{R}$ is a bias. Similarly, the output of a layer of a feed-forward neural network (FFNN) is computed as$$\mathbf{o}^l = \sigma(\mathbf{X}^l \mathbf{W}^l + \mathbf{b}^l).$$In your specific example, you set the new weight to $0$, if the output of the linear combination is less than $0$, else you use the output of the linear combination. This is the definition of the ReLU activation function, which is a non-linear function."
How do we choose the activation function for each hidden node? [duplicate],"







This question already has answers here:
                                
                            




How to choose an activation function for the hidden layers?

                                (3 answers)
                            

Closed 3 years ago.



I am new to neural networks. I would like to use them as a fitting or forecasting method.
A simple NN model that does not contain hidden layers, that is, the input nodes are directly connected to the outputs nodes, represents a linear model.  Nonlinearity begins to appear in an ANN model when we have hidden nodes, in which a nonlinear function is assigned to the hidden nodes, and using minimization their weights are determined.
How do we choose the non-linear activation function that should be assigned to each hidden neuron?
","['neural-networks', 'linear-regression', 'hidden-layers', 'regression', 'forecasting']",
Is this learning rate schedule increasing the learning rate?,"
I was reading a PyTorch code then I saw this learning rate scheduler:
def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):
    """"""
    Learning rate scheduler
    :param optimizer:
    :param warmup_iters:
    :param warmup_factor:
    :return:
    """"""
    def f(x):
        if x >= warmup_iters:
            return 1
        alpha = float(x) / warmup_iters
        return warmup_factor * (1 - alpha) + alpha

    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)

and this is where the function is called:
if epoch == 0:
    warmup_factor = 1. / 1000
    warmup_iters = min(1000, len(data_loader) - 1)

    lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)

As I understood it gradually increase learning rate until it reach initial learning rate. Am I correct? Why we need to increase learning rate? As I know for better learning in Neural Networks we decrease learning rate. 
","['neural-networks', 'machine-learning', 'deep-learning', 'pytorch', 'learning-rate']","The higher (or smaller) the learning rate, the higher (or, respectively, smaller) the contribution of the gradient of the objective function, with respect to the parameters of the model, to the new parameters of the model. Therefore, if you progressively increase (or decrease) the learning rate, then you will accelerate (or, respectively, slow down) the learning process, so later training examples have higher (or, respectively, smaller) influence on the parameters of the model.In your example, the function warmup_lr_scheduler returns an object of class LambdaLR, initialized with a certain optimizer and the function f, which is defined asThe documentation of torch.optim.lr_scheduler.LambdaLR says that the function f should compute a multiplicative factor given an integer parameter epoch, so x is a training epoch. If the epoch x is greater than or equal to warmup_iters, then 1 is returned, but anything multiplied by 1 is itself, so, when the epoch x is greater than a threshold, warmup_iters (e.g. 1000), then the initial learning rate is unaffected. However, when x < warmup_iters, the multiplicative factor is given bywhich is a function of the epoch x. The higher the epoch x, the higher the value of alpha, so the smaller (1 - alpha) and warmup_factor * (1 - alpha). Note that float(x) / warmup_iters will never be greater than 1 because x is never greater than warmup_iters. So, effectively, as the epoch increases, warmup_factor * (1 - alpha) tends to 0 and alpha tends to 1. The learning rate can only increase if you multiply it with a constant greater than 1. However, this can only happen if warmup_factor > 1. You can verify this by solving the inequality warmup_factor * (1 - alpha) + alpha > 1.To conclude, the initial learning rate is not being increased, but the learning process starts with a smaller learning rate than the given learning rate, for a warmup_iters epochs, then, after warmup_iters epochs, it uses the initially given learning rate (e.g. 0.002)."
Best approach for online Machine Translation with few hundred of samples?,"
I want to implement a model that improves itself with the passage of time. 
My main task is to build a machine translator (from English to Urdu).. The problem I am facing is that I have very little dataset available to train. Even if I create a corpus still there is a possibility of that corpus having poor translations due to outdated word choice for my native language. 
I was thinking to create a model which predicts output and user tells whether it is correct or not. Or maybe suggests a better translation. 
Now I have two options. 

Take that input from end user, append it to my dataset and retrain the model. (I don't know whether it is even possible or not at production level).
Second is to reinforce that data into previous system. So far I only came to know about Online learning or Reinforcement learning (Q-learning, as my data is very small and even if user is training still not going to be in millions of sentences)

Am I on the right track, and how can I progress with either of these two options? Is there any prebuilt solution similar to this?
","['reinforcement-learning', 'sequence-modeling', 'machine-translation', 'online-learning']",
What will change when workstations will have ARM Machine Learning Processors onboard?,"
lately we read that many manufacturers are forcing ARM architectures to be used on future workstations. One of ARM's recent announcements is a machine learning processor. What will change in terms of computing performance if ARM architectures become new standard, and these kinds of ML-focused chips are found in most devices?
","['machine-learning', 'performance']",
How to train a model to extract custom and unknown entities,"
I'm trying to figure out how to extract specific text from an utterance by a user.
I need to extract ""unknown"" text from a short and simple text.  In this case, the user wants to create a list.  everything in the {} is unknown text. As it doesn't belong to a specific entity such as food, athletes, movies, etc.


create a new {groceries} list
create a list {movies}
create a new list {movies}
create a list and call it {books}
create a new list and give it the name {stamps}
create a list with the title {red ketchup}
create another list called {rotten food}


the above list is but a small sample of all the different ways that a user can say he wants to create a list.
In everything that I have seen, it's all based on existing entities for the NER and when someone says that it's custom, I found that it just means we have to train a specific set of words and hope for the best.  If I add one more word that isn't trained, it fails to get the data.
But in this case, the user can say anything such as ""old shoes"", ""schools I want to go to"", ""Keanu Reeves movies"".  So I cannot see how I could possibly train it.
With Spacy, I followed this example (https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/train_intent_parser.py) and it mostly works in getting the proper titles. However, I have to train it for every different phrase to work.
For example, if a user says

create a beautiful new list and give it the name {stamps}

the word beautiful causes it to fail and now I have to train for that as well. At this rate, we are looking at millions of phrases to train.
before Spacy, we tried Dialogflow and Rasa.  At each point, it's about training phrases but the more we train, the more one thing worked and another broke.
At this point, we have tried and overall had good intent detection success but when it comes to extracting data such as this, I'm starting to look like a deer in a headlight.
We are new to NLP and while we've had a lot of good progress and over the past few weeks, we cannot seem to find any articles written on this specific problem and whether it can be solved.  Dialogflow has the concept of any entity but they recommend avoiding it and it works 2 out of 3 times when things get complicated.
The goal is to detect which of these words is the title based training. Can it be done? and if so, what's the approach?
Any code, hints or articles that might get us started would be appreciated.
",['training'],
Is machine learning required for deep learning?,"
The answers to this Quora question say it's OK to ignore machine learning and start right away with deep learning.
Is machine learning required or is useful for understanding (theoretically and practically) deep learning? Can I start right away with deep learning or should I cover machine learning first? In what way machine learning useful for deep learning? (leave the mathematics part - I'm ok with it).
","['machine-learning', 'deep-learning', 'comparison']","NODeep learning is itself a huge subject area with serious applications in NLP, Computer Vision, Speech and Robotics.
You should learn deep learning from scratch like understanding forward propagation, back propagation, how weights are updated etc.. instead of using high level frameworks like keras, pytorch.
It's OK to use them once you understand the basics to save time and code complexity, but remember ""surely"" you don't need machine learning for that.Since you are familiar with the mathematics part, I would suggest you to straight away jump into Deep Learning. Note that deep learning is inspired by how the brain works.Yes you may start right away, start with the hello world problems ""MNIST DIGIT Classification"" if you know little image processing.
Start with a simple neural network model from scratch, then use keras (very easy) and then proceed to CNN ...
You may start with simple problems in other fields too (NLP, Speech)
I suggest Andrew Yangs, course in Machine Learning (within this he explains a neural network model for MNIST I guess).You will understand that in machine learning you sit down and find useful features in the dataset yourself, but in deep learning it happens automatically
(Learn Deep learning in detail and come back and read this, you will understand exactly what I mean!)
If you learn Machine learning and then go to deep learning, you will realise that it was unnecessary .if you interested in this field of AI, jump into deep learning right now!"
Is the gradient at a layer independent of the activations of the previous layers?,"
Is the gradient at a layer (of a feed-forward neural network) independent of the activations of the previous layers?
I read this in a paper titled Mean Field Residual Networks: On the Edge of Chaos (2017). I am not sure how far this is true, because the error depends on those activations.
","['neural-networks', 'deep-learning', 'backpropagation', 'gradient-descent', 'gradient']",
"What does the notation $[m]=\{1, \ldots, m\}$ mean in the equation of the empirical error?","
The empirical error equation given in the book Understanding Machine Learning: From Theory to Algorithms is 

My intuition for this equation is: total wrong predictions divided by the total number of samples $m$ in the given sample set $S$ (Correct me if I'm wrong). But, in this equation, the $m$ takes $\{ 1, \dots, m \}$. How is this actually calculated, as I thought it should be one number (the size of the sample)?
","['machine-learning', 'computational-learning-theory', 'notation', 'books']","This is a commonly used notation in theoretical computer science.$[m]$ is not the variable $m$, but is instead the set of integers from $1$ to $m$ inclusive. The empirical error equation thus reads in English:The cardinality of a set consisting of the elements $i$ of the set of integers $[m]$ such that the hypothesis given input $x_i$ disagrees with label $y_i$, normalized by $m$. "
"Prove that in such cases, it is possible to ï¬nd an ERM hypothesis for $H_n$ in the unrealizable case in time $O(mnm^{O(n)})$","
Let $H_1$ , $H_2$ ,... be a sequence of hypothesis classes for binary classiï¬cation. 
Assume that there is a learning algorithm that implements the ERM rule in the realizable case such that the output hypothesis of the algorithm for each class $H_n$ only depends on $O(n)$ examples out of the training set. Furthermore, assume that such a hypothesis can be calculated given these $O(n)$ examples in time $O(n)$, and that the empirical risk of each such hypothesis can be evaluated in time $O(mn)$. 
For example, if $H_n$ is the class of axis aligned rectangles in $R^n$ , we saw that it is possible to ï¬nd an ERM hypothesis in the realizable case that is deï¬ned by at most $2n$ examples. 
Prove that in such cases, it is possible to ï¬nd an ERM hypothesis for $H_n$ in the unrealizable case in time $O(mnm^{O(n)})$.
","['machine-learning', 'time-complexity', 'pac-learning', 'homework']",
"How do you game an automatic trading system by messing with data, as opposed to hacking the algorithm itself?","
There was a recent question on adversarial AI applications, which led me to start digging around.
Here my interest is not general, but specific:
How do you game an automatic trading system by messing with data, as opposed to hacking the algorithm itself?
","['reference-request', 'adversarial-ml', 'algorithmic-trading', 'adversarial-attacks']",
Confused with backprop in pytorch with BCE loss [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 1 year ago.







                        Improve this question
                    



I've a prediction matrix(P) of dimension 3x3 and one-hot encoded label matrix(L) of dimension 3x3 as shown below.
    |0.5 0.3 0.1|      |1 0 0|
P = |0.3 0.2 0.1|  L = |0 1 0|
    |0.2 0.5 0.8|      |0 0 1|

each column in 'P' corresponds to prediction of a label in 'L'

How is the BCELoss calculated using pytorch?, my experimentation by giving these two matrices as parameters to loss function yielded me poor results and pytorch's loss calculation function doesn't disclose on how loss calculation is done for this case.
How is the loss averaged for each instance and across the a batch?
if loss is calculated column wise and averaged for each instance and across the batch, then how can loss be backprop'd in pytorch?

Thanks in advance.
","['backpropagation', 'objective-functions', 'pytorch']",Hope I can help you
Any explanation why multiple linear layers work better than a single linear layer in practice?,"
It is a well-known math fact that composition of linear/affine transformations is still linear/affine. For a naive example,
$\textbf{A}_1\textbf{A}_2\textbf{x}$ is simply $\textbf{A}\textbf{x}$ where $\textbf{A}=\textbf{A}_1\textbf{A}_2$
Any one knows why in practice multiple linear layers tend to work better, even though it is mathematically equivalent to a single linear layer? Any reference is appreciated!
",['neural-networks'],
Is there any research on the development of attacks against artificial intelligence systems?,"
Is there any research on the development of attacks against artificial intelligence systems?
For example, is there a way to generate a letter ""A"", which every human being in this world can recognize but, if it is shown to the state-of-the-art character recognition system, this system will fail to recognize it? Or spoken audio which can be easily recognized by everyone but will fail on the state-of-the-art speech recognition system. 
If there exists such a thing, is this technology a theory-based science (mathematics proved) or an experimental science (randomly add different types of noise and feed into the AI system and see how it works)? Where can I find such material?  
","['image-recognition', 'reference-request', 'voice-recognition', 'ai-security', 'adversarial-ml']","Yes, there is some research on this topic, which can be called adversarial machine learning, which is more an experimental field.An adversarial example is an input similar to the ones used to train the model, but that leads the model to produce an unexpected outcome. For example, consider an artificial neural network (ANN) trained to distinguish between oranges and apples. You are then given an image of an apple similar to another image used to train the ANN, but that is slightly blurred. Then you pass it to the ANN, which unexpectedly predicts the object to be an orange.Several machine learning and optimization methods have been used to detect the boundary behaviour of machine learning models, that is, the unexpected behaviour of the model that produces different outcomes given two slightly different inputs (but that correspond to the same object). For example, evolutionary algorithms have been used to develop tests for self-driving cars. See, for example, Automatically testing self-driving cars with search-based procedural content generation (2019) by Alessio Gambi et al."
Matrix-output for FFNN?,"
Turns out that it looks like I will be approximating a 100x10 matrix in my project thesis. II have the following equation
$y = Dx$, 
where $y$ is $(100 \times 1)$, $D$ is $100 \times 10$ and $x$ is $10 \times 1$ 
It is the transformation matrix $D$ I will be approximating by iterating over quite alot of pairs (x,y). I was wondering if it is possible to output a $(100 \times 10)$ matrix as output in a FFNN-architecture, or is there any other ways to do it? As this is not an image or anything similar which gives rise to pooling etc., my guess is that CNN is not ideal here.
So tldr: Is it easy to use feed-forward architecture to approximate an matrix?
EDIT: I figured out that $D$ obviously doesn't need to be a mtrxi, just have 100 output nodes, duh. Thanks.
",['feedforward-neural-networks'],
Can multiple activation functions be replaced with a single activation function?,"
I'm just started to learn deep learning and I have a question about this neural network:

I think $h_1$, $h_j$ and $h_n$ are perceptrons. So, if they are perceptrons, all of them will have an activation function.
I'm wondering if it is possible to have only one activation function, and sum the output to all of the perceptrons, and pass that sum to that activation function. And the output of this activation function will be $y$.
I will have this network, where $H1$, $Hj$ and $Hn$ don't have activation function:

The input for the activation function will be the sum of the outputs of $H1$, $Hj$ and $Hn$ without been processed by an activation function.
Is that possible (or is it a good idea)?
","['deep-learning', 'activation-functions']","TL;DR: This is possible but removing the activations will decrease the expressivity of the network because it will become mathematically equivalent to a single neuron.Mathematical ExplanationThe outputs of your intermediate neurons (in the absence of activation functions) are now:$\text{(1)}\quad H_i(x) = \sum_{j=1}^nw_{ij}\cdot x_j+b_i$You are then summing each $H_i$:$\text{(2)}\quad\mathcal{H(x)}=\sum_{i=1}^nH_i(x)$You then pass $\mathcal{H(x)}$ to some activation say $g$:$\text{(3)}\quad\hat y = g(\mathcal{H(x)})$The trouble is that the inner term $\mathcal{H(x)}$ mathematically reduces to a single linear operation on $x$. Proof:Thus, without the non-linear activations (3) mathematically reduces to a single neuron. "
How to choose the suitable Neural Network Architecture for Regression Tasks,"
so I'm working on a Project where I want to predict the Vehicle Position from the Vehicle Data like speed, acceleration etc.. now the data that I have comes also with a timestamp for each sample ( I mean that I have also a timestamp feature).
at first I thought that I should get rid of that timestamp feature because it is not relevant to my Project, I mean logically, I will not need a timestamp feature to predict the vehicle position, that didn't make sense to me when I first took a look at the dataset. I thought other features like speed, acceleration, braking pressure etc.. are more important and I thought also that the solution for this Problem would be to use a normal Deep NN or RBFNN for making this Prediction. recently, I read some papers that shows how a Convolutional NN can be also used for regression and that confused me to choose the Architecture needed for my Project. this Week I also watched a Tutorial where a RNN/ LSTM was implemented for regression Tasks. 
Now I'm very confused which architecture should I use for my Project. I also noticed that maybe if I used that timestamp feature, I can maybe use an RNN/LSTM Network for this Task but I don't know if my dataset can be seen as time-series dataset, actually the vehicle position doesn't depend on the time as far as I can tell.
Hopefully can someone answer me based on Experience. It would be also great to have some Papers or references where I can look for more. 
","['neural-networks', 'deep-learning', 'python', 'prediction', 'regression']",
Backpropagation: Chain Rule to the Third Last Layer,"
I'm trying to solve dLoss/dW1. The network is as in picture below with identity activation at all neurons:

Solving dLoss/dW7 is simple as there's only 1 way to output:
$Delta = Out-Y$
$Loss = abs(Delta)$
The case when Delta>=0, partial derivative of Loss over W7 is:
$\dfrac{dLoss}{dW_7} = \dfrac{dLoss}{dOut} \times \dfrac{dOut}{dH_4} \times \dfrac{dH_4}{dW_7} \\
= \dfrac{d(Out-Y)}{dOut} \times \dfrac{d(H_4W_{13} + H_5W_{14})}{dH_4} \times \dfrac{d(H_1W_7 + H_2W_8 + H_3W_9)}{dW_7} \\
= 1 \times W_{13} \times H_1$
However, when solving dLoss/dW1, the situation is very different, there are 2 chains to W1 through W7 and W10, and now, how should the chain for $\dfrac{dLoss}{dW_1}$ be?
Furthermore, at an arbitrary layer, with all outputs of all layers already calculated plus all gradients of weights on the right side also calculated, what should a formula for $\dfrac{dLoss}{dW}$ be?
","['math', 'backpropagation', 'gradient-descent', 'derivative']","I finally solved it out but it's long, it's not only chain rule, it includes quotient rule too. And, this is only the third last layer, once a DNN has more layers then it's more complex.$\dfrac{dLoss}{dW_1} = \dfrac{d}{dW_1}(Out-Y) = \dfrac{d}{dW_1}Out = \dfrac{d}{dW_1}(H_4W_{13} + H_5W_{14}) \\= \dfrac{d}{dW_1}H_4W_{13} + \dfrac{d}{dW_1}H_5W_{14} \\= W_{13} \times \dfrac{d}{dW_1}H_4 + W_{14} \times \dfrac{d}{dW_1}H_5 \\= W_{13} \times \dfrac{d}{dW_1}(H_1W_7 + H_2W_8 + H_3W_9) + W_{14} \times \dfrac{d}{dW_1}(H_1W_{10} + H_2W_{11} + H_3W_{12}) \\= W_{13} \times \dfrac{d}{dW_1}H_1W_7 + W_{14} \times \dfrac{d}{dW_1}H_1W_{10} \\= W_{13}W_7 \times \dfrac{d}{dW_1}H_1 + W_{14}W_{10} \times \dfrac{d}{dW_1}H_1 \\= (W_{13}W_7 + W_{14}W_{10}) \times \dfrac{d}{dW_1}(X_1W_1 + X_2W_2) \\= (W_{13}W_7 + W_{14}W_{10}) \times X_1$"
How is the distance between pointers in Stochastic Universal Sampling determined?,"
I'm studying about different selection methods in genetic algorithms. My question is about the Stochastic Universal Sampling (SUS) selection method. I know that each individual will occupy a segment of the line according to its fitness value and then equally spaced pointers will be placed over this line.
I want to know how the distance between pointers is determined. I have seen 1/6 and 1/4 as the distance between pointers. I want to choose the number of pointers dynamically according to the situation. I want to know what conditions or factors affect the determination of this distance. For example, when do we decide to choose 1/4 as distance? I want to know if it is possible to change the number of samples in each iteration according to different conditions or situations. If so, what are these conditions?
","['genetic-algorithms', 'selection-operators', 'stochastic-universal-sampling']","As originally conceived in James Baker's 1989 paper Reducing bias and inefficiency in the selection algorithm, Stochastic Universal Sampling accepts a population containing $N$ individuals, and a number of parents to sample, denoted $n$. Assuming fitness values are normalized so that they sum up to $N$, at each step, a new pointer is placed a step equal in size to the fraction $\frac{N}{n}$ ahead of the location of the previous pointer (and the location of the first pointer is set to a random value in the range [0, $\frac{N}{n}$) ). So, for example, if you want to sample 6 individuals from a population of size 10, you would make steps of size $\frac{10}{6}$, spacing your pointers at even intervals of $\frac{10}{6}$.Modern implementations, like the one on Wikipedia sometimes do not document this fact clearly, although it is apparent what is intended if you already understand the method. They often write the step size as $\frac{F}{n}$, where $F$ is the total fitness of the population, without discussing its relation to the size of the population. The extra normalization step is actually not essential, so modern implementations generally seem to skip it.So in summary, the step size $\frac{F}{n}$ used if fitness values of a population sum to $F$, and you want to select $n$ individuals. If you want to select more individuals, use a higher value for $n$. If you want to select fewer, use a lower value for $n$, which updates your step size accordingly.Values of this parameter of $\frac{1}{4}$ or $\frac{1}{6}$ suggest that the implementation may be normalizing the sum of fitness values is being normalized to N, and then using the parameter as a multiplicative factor automatically. This is a fairly reasonable design. You could interpret these values as ""Select $\frac{1}{4}$ of the population"" and ""Select $\frac{1}{6}$ of the population"". Note that this sort of bumps your question up a level: how do you pick the fraction of the population to keep? That question doesn't have a clean answer, and picking it is generally an art developed by experts through practice. It is very closely related to the exploration/exploitation tradeoff. Some ways you might pick $n$:"
Can a neural network be used to detect sine waves?,"
I am recording the vibrations of an AC Motor (50Hz Europe) and I am trying to find out whether it is powered on or not. When I record these vibrations, I basically get the vibration values ($-1$ to $+1$) over time.
I would like to develop a program to detect the presence of a 50Hz sine wave on a steady stream of input data. I will have $X$ and $Y$ measurements, where $X$ represents amplitude and $Y$ the time (sampled at 100Hz - it is possible to increase the sample rate to 200Hz or 400Hz at max)
Is this a task suited for a neural network, and if so, would it be less efficient than other means of detection?
","['neural-networks', 'pattern-recognition', 'signal-processing']","Is this a task suited for a neural network Yes. You have choices in fact:A fully-connected network would be simplest architecture, and would work if you gave it some time window of samples (e.g. every 0.5 seconds or every 50 samples) and supervised training data - sets of samples with sensor readings and the ground truth value of whether the motor was on or not.A 1D convolutional neural network would likely be most efficient and robust to train, and would take the same inputs and outputs as the fully-connected network.A recurrent neural network would be tricker to train, but a nicer design as you could feed it samples one at a time. The input would be the current sample, and output the probability that the motor was on. When training this, you would also want to provide it transitions between the motor being on and off. The nice feature about this is that it should give you quick feedback about whether the motor was on or off - with the caveat that it may be more likely to trigger intermittent false positives, so a little extra post-processing might be required.All of the above require you to collect training data, ideally in situations identical to planned use of the detector. So if the motor is mounted somewhere that could experience other vibrations, a few of those kind of scenarios should be simulated with motor both on and off.and if so, would it be less efficient than other means of detection?In terms of computing power and effort on your part, you may find that an off-the-shelf Fast Fourier Transform (FFT) library function with a simple threshold at your target frequency will make a robust and simple detector, with no need for a neural network.Typically for specific frequency detection you would take a window of samples, adjust them (using e.g. Hamming window) to reduce edge effects which would appear as frequencies in the conversion, and then run FFT. This combination is so common that you may find it already combined in the FFT library. For more on this, you would want to ask in Signal Processing Stack Exchange, where use of FFT is well understood.If the environment is noisy or the target frequency can drift (making it hard to set a simple threshold) then you could also combine FFT with a neural network. This combination can solve much more complicated signal detection, and is used in speech processing for instance.sampled at 100Hz - it is possible to increase the sample rate to 200Hz or 400Hz at maxFor reliably detecting a 50Hz signal, I would say that 200Hz sample rate is minimum. The theorectical minimum is 100Hz (i.e. twice the signal frequency) but may give you problems with noise and the possibility that your sample points just happen to fall on low amplitude parts of the oscillations, making it look like the motor is off even when it is on."
How mAP is unfair evaluation metric for Object Detection?,"
The following figure is from the last page in YOLOv3 paper highlighting how mAP is unfair metric for evaluating Object Detectors:

The figure shows two hypothetical Object Detector results which the author say they give the same perfect mAP, while visually the first detector is clearly more accurate than the other.
According to my understanding, the two detectors do not give the same mAP. This is how I calculate it for each detector:
Detector 1, 'Dog' class AP table:
 ______________________________________ 
| Object  | True? | Precision | Recall |
|_________|_______|___________|________|
| Dog_99% | Yes   |     1     |    1   |  
|_________|_______|___________|________|
Hence, AP_dog = 1

Detector 1, 'Person' class AP table:
 ________________________________________
| Object    | True? | Precision | Recall |
|___________|_______|___________|________|
|Person_99% | Yes   |     1     |    1   |  
|___________|_______|___________|________|
Hence, AP_person = 1
And by continuing doing so for the other 7 classes in the dataset, mAP=1. 

Detector 2, 'Dog' class AP table:
 ______________________________________ 
| Object  | True? | Precision | Recall |
|_________|_______|___________|________|
| Dog_48% | Yes   |     1     |    1   |  
|_________|_______|___________|________|
Hence, AP_dog = 1

Detector 2, 'Bird' class AP table:
 _______________________________________
| Object   | True? | Precision | Recall |
|__________|_______|___________|________|
| Bird_90% | Yes   |     1     |    1   | 
| Bird_89% | No    |     0.5   |    1   | 
|__________|_______|___________|________|
Hence, AP_bird = 0.75
And by continuing doing so for the other 7 classes in the dataset, mAP is less than 1 because AP for at least one class is less than one (AP_bird).

Hence, according to my understanding mAP for the first detector is 1, and for the second detector is less than 1. What is the mistake I'm doing in the calculation? Or is there some assumption in the paper that I'm not considering?
",['object-detection'],
An architecture for classifying distance from origin for a sum of vectors?,"
I built a three-layer neural network (first is 1D convolutional and the remaining two are linear). It takes an input of 5 angles in radians, and outputs two numbers from 0 to 1, which are respectively the probability of failure or success. The NN is trained in a simulation.
The simulation goes this way: it takes 5 angles in radians and calculates the vector sum of 5 vectors having $x$ as module and $\alpha$ as angles (taken from the input). It returns $1$ if the vector sum has a module greater than $y$, or $0$ if it is less than $y$.
My intention is to be able to tell sequences of radians that will generate vectors with a sum greater than $y$ in module from the ones which won't.
Which would be the best configuration to achieve this? Is the configuration I set up (1D convolution layer + 2 linear layers) efficient? If so, would it be easy to find the right size for the convolution? Or should I just remove it?
I noticed that if I change the order of the input angles the output of the simulation will be the same. Is there a particular configuration you should use when dealing with these cases?
","['neural-networks', 'convolutional-neural-networks', 'ai-design']",
What is the difference between a semantic network and an ontology?,"
What is the difference between a semantic network and an ontology? How are they related? I have not found any article that describes how these two concepts are related.
","['comparison', 'knowledge-representation', 'ontology', 'knowledge-base', 'semantic-networks']",
How can I teach a computer to play N64 games using Neural Nets?,"
I would like to work on a project where I teach an NN to play N64 games. 
To my current understanding, I would need an emulator?
I can do the Machine Learning side of it, im just unsure how I can give the NN access to the game's controls, such as left, right, up or down?
Where could I find more information on doing so and is using an Emulator the right path to take?
","['neural-networks', 'reinforcement-learning']",The common options are:
"What are examples of thought experiments against or in favour of strong AI, apart from the Chinese room argument?","
The Chinese Room argument against strong AI overlooks the fact that ""the man in the room"" is acting as a macro-scale ""neurotransmitter"" of the larger system in which he resides. It does not rule out strong AI, it simply reduces to an enigmatic question: where does understanding ""reside"" and how does it epiphenomenally emerge?
What are other examples of thought experiments against or in favor of strong AI (apart from the Chinese room argument) or extensions or refutations to known experiments?
","['reference-request', 'philosophy', 'agi', 'chinese-room-argument']","An excellent book summarizing the development of thought in this area over several hundred years is Mind Design II, edited by John Haugeland. This book contains a collection of essays written by the major thinkers in this area through until about the 1990s. A brief summary of some of the major ideas is:Turing publishes his seminal paper in AI, in 1950. In this paper, Turing anticipates nearly all later objections to AI, and summarizes reasonable refutations of them. This paper contains a number of short thought experiments. One of the more notable ones is the Turing Test. A key observation Turing makes is that if a machine behaves in a way indistinguishable from a human, then any arguments used to reject its claim to intelligence (and even to consciousness) appear to work equally well on other humans. Searle doesn't really address this in his later argument. Basically, I have no way to know for sure whether you have subjective experiences of the same kind I have. The fact that you're made of roughly the same kind of meat as I am seems like a shaky explanation. For example, would humanoid aliens that act like us also be intelligent? What if it were found that their brains took some radically different form from ours? What if it were found that their brains were really networks of transistor-like machines?The Cognativist revolution spanned a number of fields starting in the 1960s. This movement held that behaviorism was wrong for two basic reasons. First, Behaviorism does not account for the subjective feelings of existence or understanding. Second, Behaviorism could not explain phenomena like language, that appeared to involve reasoning logically about symbols. Notable authors in this period are Chomsky and Fodor, who argued that minds were essentially computer programs that happened to be running on brains instead of computers. These ideas dominated AI, Psychology and Linguistics for about 30-40 years. Fodor's works contain a number of thought experiments involving language and programs."
Indoor positioning with variable number of distance measurements in tensorflow,"
Currently I have a setup where I'm determining the position of a transmitter using the RSSI of 4 receivers. Its a simple feed-forward network with some hidden layers, where the input is the RSSI values, and the output is a 2d coordinate.
Now, if I decide to add/remove receivers, I have to train the network again, since the input size changes. This is not ideal, since the receivers can move around, dissapear, etc. I have looked at some alternatives, but being pretty new to machine learning, it's difficult to pick which direction to go.
I have looked at a potential solution (stolen from another question), but I'm lost at how to implement it using tensorflow:

Any help is appreciated.
",['tensorflow'],
Are these two TRPO objective functions equivalent?,"
In the TRPO paper, the objective to maximize is (equation 14)
$$
\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim q}\left[\frac{\pi_\theta(a|s)}{q(a|s)} Q_{\theta_\text{old}}(s,a) \right]
$$
which involves an expectation over states sampled with some density $\rho$, itself defined as
$$
\rho_\pi(s) = P(s_0 = s)+\gamma P(s_1=s) + \gamma^2 P(s_2=s) + \dots
$$
This seems to suggest that later timesteps should be sampled less often than earlier timesteps, or equivalently sampling states uniformly in trajectories but adding an importance sampling term $\gamma^t$.
However, the usual implementations simply use batches made of truncated or concatenated trajectories, without any reference to the location of the timesteps in the trajectory.
This is similar to what can be seen in the PPO paper, which transforms the above objective into (equation 3)
$$
\mathbb{E}_t \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)} \hat A_t \right]
$$
It seems that something is missing in going from $\mathbb{E}_{s\sim \rho}$ to $\mathbb{E}_t$ in the discounted setting.
Are they really equivalent?
","['reinforcement-learning', 'policy-gradients', 'proximal-policy-optimization', 'trust-region-policy-optimization']",
Are there any deep learning techniques to use the content of an image for another image?,"
Are there any machine learning or deep learning techniques to use the content of an image for another image?
More specifically, suppose I take a photo of a notebook. I get the angle, lighting, and perspective perfect. Now I copy an image I found online that contains text or handwriting. Is there a machine learning technique that would now draw this writing in my notebook?
Just asking if it's possible before I attempt to hire someone.
","['machine-learning', 'deep-learning', 'computer-vision', 'reference-request', 'image-processing']",
Is there a way to do reinforcement learning in POMDP?,"
Are there any algorithms to use reinforcement learning to learn optimal policies in partially observable Markov decision process (POMDP) i.e. when the state is not perfectly observed? More specifically, how does one update the belief state using Bayes' rule when the update Q kernel is not known?
","['reinforcement-learning', 'reference-request', 'pomdp']",
Why is awareness of itself such a point when speaking about AI?,"
Why is awareness of itself such a point when speaking about AI? Does it always mean a starting point for apocalyptic nightmares to occur when such a level is reached or is it just a classical example about what could be a really abstract thing that machine cannot easily posses?
I would sleep my nights far more calmfully if the situation was the latter, and I understand the first does not automatically  happen. The main thing I would like to discover is the starting point - which approach came first historically? Or is there another view point in the historical first occurrence of self awareness term?
","['philosophy', 'agi', 'history', 'self-awareness']","Neil Slater has it right, there most probably is no fear of AI self awareness as some starting point of evil series of things to happen.Wikipedia [1] puts self awareness talks to sci-fi section, among stories, not a real thing. Self-awareness is among a list of terms that tries to make machines or aliens similarly human than ordinary people and uses that as a method of story telling.Self-awareness or other humanlike skills that we posses that the machines don't have yet and will not have in near future can twist minds and make a seed of conspiracy theories, but at least the exhaustive Wikipedia overview on topic did not speak anything about AI.Maybe the concept of humanlike behaviour materialises on our minds as a term of self-awareness but my source puts the origin to different category.[1] https://en.m.wikipedia.org/wiki/Self-awareness"
What is the benefit of using identity mapping layers in deep neural networks like ResNet?,"
As I understand, ResNet has some identity mapping layers, whose task is to create the output as the same as the input of the layer. The ResNet solved the problem of accuracy degrading. But what is the benefit of adding identity mapping layers in intermediate layers?
What's the effect of these identity layers on the feature vectors that will be produced in the last layers of the network? Is it helpful for the network to produce better representation for the input? If this expression is correct, what is the reason?
","['neural-networks', 'deep-learning', 'residual-networks']","TL;DR: Deep networks have some issues that skip connections fix.To address this statement:As I understand Resnet has some identity mapping layers that their task is to create the output as the same as the input of the layerThe residual blocks don't strictly learn the identity mapping. They are simply capable of learning such a mapping. That is, the residual block makes learning the identity function easy. So, at the very least, skip connections will not hurt performance (this is explained formally in the paper).From the paper:Observe: it's taking some of the layer outputs from earlier layers and passing their outputs further down and element wise summing these with the the outputs from the skipped layers. These blocks may learn mappings that are not the identity map.From paper (some benefits):$$\boldsymbol{y} = \mathcal{F}(\boldsymbol{x},\{W_i\})+\boldsymbol{x}\quad\text{(1)}$$The shortcut connections in Eqn.(1) introduce neither extra parameter nor computation complexity. This is not only attractive in practice but also important in our comparisons between plain and residual networks. We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).An example of a residual mapping from the paper is $$\mathcal{F} = W_2\sigma_2(W_1\boldsymbol{x})$$That is $\{W_i\}$ represents a set of i weight matrices ($W_1,W_2$ in the example) occurring in the layers of the residual (skipped) layers. The ""identity shortcuts"" are referring to performing the element wise addition of $\boldsymbol{x}$ with the output of the residual layers.So using the residual mapping from the example (1) becomes:$$\boldsymbol{y} = W_2\sigma_2(W_1\boldsymbol{x})+\boldsymbol{x}$$In short, you take the output $\boldsymbol{x}$ of a layer skip it forward and element wise sum it with the output of the residual mapping and thus produce a residual block. Limitations of deep networks expressed in paper:When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example.The skip connections and hence the residual blocks allow for stacking deeper networks while avoiding this degradation issue. Link to paperI hope this helps. "
What are examples of optimization problems that can be solved using genetic algorithms?,"
I'm trying to learn how genetic algorithms can solve optimization problems. I have already learned how genetic algorithms can solve the knapsack, TSP and set cover problems. I'm looking for some other similar optimization problems, but I have not found any.
Would you please mention some other famous optimization problems that can be solved by using genetic algorithms?
","['genetic-algorithms', 'optimization', 'applications', 'evolutionary-algorithms']",
What are the real-life applications of transfer learning?,"
What are the real-life applications of transfer learning in machine learning? I am particularly interested in industrial applications of the concept.
","['machine-learning', 'applications', 'transfer-learning']","One application I know of being used in industry is of image classification, by only training the last layer of one of the inception models released by Google,  with the desired number of classes. I can't provide specific details. Transfer learning is useful when:You do not have the resources (time, processing power, etc.) to train a DL  model from scratch.You can compromise a bit on accuracy. "
Can digital computers understand infinity?,"
As a human being, we can think infinity. In principle, if we have enough resources (time etc.), we can count infinitely many things (including abstract, like numbers, or real).
For example, at least, we can take into account integers. We can think, principally, and ""understand"" infinitely many numbers that are displayed on the screen. Nowadays, we are trying to design artificial intelligence which is capable at least human being. However, I am stuck with infinity. I try to find a way how can teach a model (deep or not) to understand infinity. I define ""understanding' in a functional approach. For example, If a computer can differentiate 10 different numbers or things, it means that it really understand these different things somehow. This is the basic straight forward approach to ""understanding"".
As I mentioned before, humans understand infinity because they are capable, at least, counting infinite integers, in principle. From this point of view, if I want to create a model, the model is actually a function in an abstract sense, this model must differentiate infinitely many numbers. Since computers are digital machines which have limited capacity to model such an infinite function, how can I create a model that differentiates infinitely many integers?
For example, we can take a deep learning vision model that recognizes numbers on the card. This model must assign a number to each different card to differentiate each integer. Since there exist infinite numbers of integer, how can the model assign different number to each integer, like a human being, on the digital computers? If it cannot differentiate infinite things, how does it understand infinity?
If I take into account real numbers, the problem becomes much harder.
What is the point that I am missing? Are there any resources that focus on the subject?
","['deep-learning', 'philosophy', 'agi']","I think this is a fairly common misconception about AI and computers, especially among laypeople. There are several things to unpack here.Let's suppose that there's something special about infinity (or about continuous concepts) that makes them especially difficult for AI. For this to be true, it must both be the case that humans can understand these concepts while they remain alien to machines, and that there exist other concepts that are not like infinity that both humans and machines can understand. What I'm going to show in this answer is that wanting both of these things leads to a contradiction.The root of this misunderstanding is the problem of what it means to understand. Understanding is a vague term in everyday life, and that vague nature contributes to this misconception.If by understanding, we mean that a computer has the conscious experience of a concept, then we quickly become trapped in metaphysics. There is a long running, and essentially open debate about whether computers can ""understand"" anything in this sense, and even at times, about whether humans can! You might as well ask whether a computer can ""understand"" that 2+2=4. Therefore, if there's something special about understanding infinity, it cannot be related to ""understanding"" in the sense of subjective experience.So, let's suppose that by ""understand"", we have some more specific definition in mind. Something that would make a concept like infinity more complicated for a computer to ""understand"" than a concept like arithmetic. Our more concrete definition for ""understanding"" must relate to some objectively measurable capacity or ability related to the concept (otherwise, we're back in the land of subjective experience). Let's consider what capacity or ability might we pick that would make infinity a special concept, understood by humans and not machines, unlike say, arithmetic.We might say that a computer (or a person) understands a concept if it can provide a correct definition of that concept. However, if even one human understands infinity by this definition, then it should be easy for them to write down the definition. Once the definition is written down, a computer program can output it. Now the computer ""understands"" infinity too. This definition doesn't work for our purposes.We might say that an entity understands a concept if it can apply the concept correctly. Again, if even the one person understands how to apply the concept of infinity correctly, then we only need to record the rules they are using to reason about the concept, and we can write a program that reproduces the behavior of this system of rules. Infinity is actually very well characterized as a concept, captured in ideas like Aleph Numbers. It is not impractical to encode these systems of rules in a computer, at least up to the level that any human understands them. Therefore, computers can ""understand"" infinity up to the same level of understanding as humans by this definition as well. So this definition doesn't work for our purposes.We might say that an entity ""understands"" a concept if it can logically relate that concept to arbitrary new ideas. This is probably the strongest definition, but we would need to be pretty careful here: very few humans (proportionately) have a deep understanding of a concept like infinity. Even fewer can readily relate it to arbitrary new concepts. Further, algorithms like the General Problem Solver can, in principal, derive any logical consequences from a given body of facts, given enough time. Perhaps under this definition computers understand infinity better than most humans, and there is certainly no reason to suppose that our existing algorithms will not further improve this capability over time. This definition does not seem to meet our requirements either.Finally, we might say that an entity ""understands"" a concept if it can generate examples of it. For example, I can generate examples of problems in arithmetic, and their solutions. Under this definition, I probably do not ""understand"" infinity, because I cannot actually point to or create any concrete thing in the real world that is definitely infinite. I cannot, for instance, actually write down an infinitely long list of numbers, merely formulas that express ways to create ever longer lists by investing ever more effort in writing them out. A computer ought to be at least as good as me at this. This definition also does not work.This is not an exhaustive list of possible definitions of ""understands"", but we have covered ""understands"" as I understand it pretty well. Under every definition of understanding, there isn't anything special about infinity that separates it from other mathematical concepts.So the upshot is that, either you decide a computer doesn't ""understand"" anything at all, or there's no particularly good reason to suppose that infinity is harder to understand than other logical concepts. If you disagree, you need to provide a concrete definition of ""understanding"" that does separate understanding of infinity from other concepts, and that doesn't depend on subjective experiences (unless you want to claim your particular metaphysical views are universally correct, but that's a hard argument to make).Infinity has a sort of semi-mystical status among the lay public, but it's really just like any other mathematical system of rules: if we can write down the rules by which infinity operates, a computer can do them as well as a human can (or better)."
â€œOutside-inâ€ versus â€œInside-outâ€ machine learning,"
A little background... Iâ€™ve been on-and-off learning about data science for around a year or so, however, I started thinking about artificial intelligence a few years ago.  I have a cursory understandings of some common concepts but still not much depth.  When I first learned about deep learning, my automatic response was â€œthatâ€™s not how our minds do it.â€  Deep learning is obviously an important topic, but Iâ€™m trying to think outside the black box.
I think of deep learning as being â€œoutside-inâ€ in that a model has to rely on examples to understand (for lack of a better term) that some dataset is significant.  However, our minds seem to know when something is significant in the absence of any prior knowledge of the thing (i.e., â€œinside-outâ€).
Hereâ€™s a thing:

I googled â€œIKEA hardwareâ€ to find that.  The point is that you probably donâ€™t know what this is or have any existing mental relationship between the image and anything else, but you can see that itâ€™s something (or two somethings).  I realize there is unsupervised learning, image segmentation, etc., which deal with finding order in unlabeled data, but I think this example illustrates the difference between the way we tend to think about machine learning/AI and how our minds actually work.
More examples:
1)

2)

3)

Letâ€™s say that #1 is a stock chart.  If I were viewing the chart and trying to detect a pattern, I might mentally simplify the chart down to #2.  That is, the chart can be simplified into a horizontal segment and a rising segment.
For #3, letâ€™s say this represents log(x).  Even though itâ€™s not a straight line, someone with no real math background could describe it as an upward slope that it decreasing as the line gets higher.  That is, the line can still be reduced to a small number of simple ideas.
I think this simplification is the key to the gap between how our minds work and what currently exists in AI.  Iâ€™m aware of Fourier transforms, polynomial regression, etc., but I think thereâ€™s a more general process of finding order in sensory data.  Once we identify something orderly (i.e., something that canâ€™t reasonably be random noise), we label it as a thing and then our mental network establishes relationships between it and other things, higher order concepts, etc.
Iâ€™ve been trying to think about how to use decision trees to find pockets of order in data (to no avail yet - I havenâ€™t figured out to apply it to all of the scenarios above), but Iâ€™m wondering if there are any other techniques or schools of thought that align with the general theory.
","['machine-learning', 'deep-learning', 'decision-trees', 'image-segmentation']",
Can reinforcement learning be utilized for creating a simulation?,"
According to the definition, the AI agent has to play a game by it's own. A typical domain is the blocksworld problem. The AI determines which action the robot in a game should execute and a possible strategy for determine the action sequence is reinforcement learning. Colloquial spoken, reinforcement learning leads to an AI agent who can play games.
Before a self-learning character can be realized, the simulation has to be programmed first. That is an environment which contains the rules for playing blocksworld or any other game. The environment is the house in which the AI character operates. Can the Q-learning algorithm be utilized to build the simulation itself?
",['reinforcement-learning'],"Can the Q-learning algorithm be utilized to build the simulation itself?Only in the presence of a meta-environment, or meta-simulation where the goals of creating the original simulation are encoded in the states, available actions and rewards.A special case of this might be in model-learning planning algorithms where there exists a ""real"" environment to refer to, and the agent benefits from exploring it and constructing a statistical model that it can then use to create an approximate simulation of the outcomes of sequences of actions. The Dyna-Q algorithm, which is a simple extension of Q-learning, is an example of this kind of model building approach. The simulation is very basic - it simply replays previous relevant experience. But you could consider this as an example of the agent constructing a simulation.Getting an agent to act like a researcher and actually design and/or code a simulation from scratch would require a different kind of meta-environment. It is theoretically possible, but likely very hard to implement in general way - even figuring out the reward scheme to express the goals of such an agent could be a challenge. I'm not aware of any examples, but entirely possible someone has attempted this kind of meta agent, because it is an interesting idea.Possibly the simplest example would be a gridworld meta-environment where a ""designer"" agent could select the layout of objects in the maze, with the goal of making a second ""explorer"" agent's task progressivley more difficult. The designer would be ""creating the simulation"" only in a very abstract way though, by setting easy-to-manage parameters of the environment, not writing low level code. There is not much difference between the approach above and having two opposing agents playing a game. It is different from a turn-based game like chess in that each agent would complete a full episode, and then be rewarded by the outcome at the end of the combined two episodes. There are some similarities to GANs for image generation."
TF Keras: How to turn this probability-based classifier into single-output-neuron label-based classifier,"
Here's a simple image classifier implemented in TensorFlow Keras (right click to open in new tab): https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb
I altered it a bit to fit with my 2-class dataset. And the output layer is:
Dense(2, activation=tf.nn.softmax);

The loss function and optimiser are still the same as in the example in the link above.
loss_fn   = tf.losses.SparseCategoricalCrossentropy();
optimizer = tf.optimizers.Adam();

I wish to turn it into a classifier with single output neuron as I have only 2 classes in dataset, and sigmoid does the 2 classes good. Tried some combinations of output activation functions + loss functions + optimisers, but the network doesn't work any more (ie. it doesn't converge).
For example, this doesn't work:
//output layer
Dense(1, activation=tf.sigmoid);

//loss and optim
loss_fn   = tf.losses.mse;
optimizer = tf.optimizers.Adagrad(1e-1);

Which combination of output activation + loss + optimiser should work for the single-output-neuron model? And generically, which loss functions and optimisers should pair well?
","['classification', 'tensorflow', 'keras', 'objective-functions', 'regression']","Advice from Neil, yes, output targeting class labels is still classification.Output range is in contiguous range:Output targeting class labels is classification:Multiple combinations of loss functions and optimisers can make the single-neuron output layer work, with different configs for them. Note that learning rates of different optimisers are different, some take 1e-1, some need 1e-3 for good training.For example, this combination should work:From my trying out, these other combinations also work for single output neuron with my data (Adam, Adamax, Nadam, RMSprop work when learning_rate=1e-3 instead of 1e-1):"
Should AI be mortal by design? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 2 years ago.







                        Improve this question
                    



There are the 3 Asimovâ€™s laws:

A robot may not injure a human being or, through inaction, allow a
human being to come to harm.
A robot must obey orders given to it by human beings, except where
such orders would conflict with the first law.
A robot must protect its own existence as long as such protection
does not conflict with the first or second law.

These laws are based on morality, which assumes that robots have sufficient agency and cognition to make moral decisions.
Additionally there are alternative laws of responsible robotics:

A human may not deploy a robot without the humanâ€“robot work
system meeting the highest legal and professional standards of
safety and ethics.
A robot must respond to humans as appropriate for their roles.
A robot must be endowed with sufficient situated autonomy to
protect its own existence as long as such protection provides smooth
transfer of control to other agents consistent the first and second
laws

Thinking beyond morality, consciousness and the AI designer's professionalism to incorporate safety and ethics into the AI design. 
Should AI incorporate irrefutable parent rules for AI to be inevitably mortal by design?  
How to assure AI can be deactivated if necessary the way the deactivation procedure cannot be worked around by the AI itself, even at the cost of the AI termination as its inevitable destiny?

EDIT: to explain reasoning behind the main question. 
Technological solutions are often based on observing biology and nature.
In evolutionary biology, for example, research results of birds mortality, show
potential negative effect of telomere shortening (dna) on life in general.

telomere length (TL) has become a biomarker of increasing interest
  within ecology and evolutionary biology, and has been found to predict
  subsequent survival in some recent avian studies but not others. 
  (...) We performed a meta-analysis on these estimates and found an overall significant
  negative association implying that short telomeres are associated with
  increased mortality risk

If such research is confirmed in general, then natural life expectancy is limited by design of its DNA, ie by design of its cell-level code storage. I assume this process of built-in mortality cannot be effectively worked around by a living creature.
A similar design could be incorporated in any AI design, to assure its vulnerability and mortality, in the sense a conscious AI could otherwise recover and restore its full health state and continue to be up and running infinitely.
Otherwise a simple turn off switch could be disabled by the conscious AI itself.

References
Murphy, R. and Woods, D.D., 2009. Beyond Asimov: the three laws of responsible robotics. IEEE intelligent systems, 24(4), pp.14-20.
Wilbourn Rachael V., Moatt Joshua P., Froy Hannah, Walling Craig A., Nussey Daniel H. and Boonekamp Jelle J. The relationship between telomere length and mortality risk in non-model vertebrate systems: a meta-analysis373Phil. Trans. R. Soc. B
","['philosophy', 'death']",
Why does an LSTM cycle on initialisation?,"
I initialised an LSTM with Xavier initialisation, although I've found this occurs for all initialisations I have tested. When initialised, if the LSTM is tested with a random input, it will get stuck in a cycle, either over a few characters or just one. Example output:
nhhbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb

f(b(bf(bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb

kk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,mkk,,m

I've also noticed the LSTM is particularly bad in this way, that even when trained it has a tendency to get stuck in loops like this. It seems it has difficulty truly retaining context strong enough to over power the input, activation and output gates with only the forget gate. Is there an explanation for this?
",['long-short-term-memory'],"This doesn't necessarily answer the question, but it does give some possible solutions to mitigate the problem. Apparently, the emphasised looping behaviour above is a result of improper initialisation, in which I was initialising with only positive and 0 weights. The looping behavior is largely diminished by proper initialisation, however it is not totally removed. See below for examples:(These are samples from 6 different uniquely initialised LSTM's on the exact same architecture).I have also found that this common behaviour is typically combated by adding a certain degree of randomness in feeding in the LSTM's output. This is normally done by interpreting the output at time step $t$ of the LSTM as a probability distribution, used to pick the next value fed into the LSTM at $t+1$. This helps break non-confident looping behaviour which is where looping behaviour most commonly occurs through some quick experimentation, while still mostly retaining confident predictions.I also tested this by randomising the input a little bit, using a probability distribution where 90% of the time the correct input is selected and 10% a random one is, and back-propagating as normal. This didn't seem to have much of an effect on the looping behaviour, although perhaps might work as a good form of regularisation. I am yet to test it.You can also use the temperature method in LSTMs, which explained really well here: https://cs.stackexchange.com/q/79241/20691."
Does it matter if it's a bot or a human generating text? Doesn't it come down to the content?,"
It was noted today that automated text generation is advancing at a rapid pace, potentially accelerating.
As bots become more and more capable of passing turing tests, especially in single iterations, such as social media posts or news blurbs, I have to ask: 

Does it matter where a text originates, if the content is strong?

Strength here is used in the sense of meaning.  To elucidate my argument I'll present an example.  (It helps to know the Library of Babel, an infinite memory array where every possible combination of characters exists.)

An algorithm is set up to produce aphorisms. The overwhelming majority of the output is gibberish, but among the junk is an incredibly profound observation emerges that changes the way people think about a subject or issue.

Where the bot just spams social media, the aphorism in question is identified because it recieves a high number of reposts by humans, who, in this scenario, provide the mechanism for finding the needle (the profound aphorism) in the haystack (the junk output).
Does the value of the insight depend on the cognitive quality of the generator, in the sense of having to understand the statement?
A real world example would be Game 2, Move 37 in the AlphaGo vs. Lee Sedol match.
","['natural-language-processing', 'philosophy', 'semantics']",
What is the state of the art AI training technique for imperfect information 2 player turn based games?,"
As far as I can tell (correct me if I'm wrong), Alphazero (with MCTS and neural network heuristic function RL) is the state of the art training method for turn based, deterministic, perfect information, complete information, two player, zero sum games.
But what is the state of the art for turn based, imperfect information games, that have 2 players, complete information, and is zero sum? (Deterministic or stochastic.) Examples include Battleship and most 2 player card games.
Are there standard games, or other tests by which this is measured? Is the criteria I offered for type of game not specific enough to narrow the answer down properly?
If the state of the art involves supervised learning (data set of manually played games), then what's the state of the art for pure reinforcement learning, if there is one?
","['ai-design', 'training', 'game-theory', 'markov-decision-process', 'imperfect-information']",
What is the difference between the definition of a stationary policy in reinforcement learning and contextual bandit?,"
A stationary policy is a function that maps a state to a probability distribution of actions. 
In a contextual bandit problem, a state itself does not include the history. But in a reinforcement learning problem, the history can be used to define a state. In this case, does the history include the rewards revealed thus far as well? If so, the policy is not stationary anymore I guess. 
As the title says, I think I am confused about the difference in the definitions of stationary (and/or non-stationary) policy between reinforcement learning and contextual bandit.
","['machine-learning', 'reinforcement-learning', 'comparison', 'definitions', 'stationary-policy']","What is the difference between the definition of a stationary policy in reinforcement learning and contextual bandit?There is no difference. A policy decides which action to take in each state. This is usually split into deterministic policies of the form $\pi(s) : \mathcal{S} \rightarrow \mathcal{A}$ and stochastic policies of the form $\pi(s|a) : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}, \text{Pr}[A_t=a |S_t=s]$. When we say that a policy is static, it means that the mapping from state to action - or distribution of actions - does not change over the time that we are interested in. This definition applies equally in the Reinforcement Learning (RL) and Contextual Bandit settings.But in a reinforcement learning problem, the history can be used to define a state. It can be used this way, but is not required. What is required is the Markov property i.e. that identifying the state also determines all the allowed transitions and rewards, plus their probabilities of occurring given an action. In this case, does the history include the rewards revealed thus far as well? If those can affect the future state transitions and rewards yes. If they generally do not, then you would usually exclude them from the state description.If so, the policy is not stationary anymore I guess.What you are proposing is a system that changes its state depending on rewards seen so far. As above, this is not necessary to define a RL problem, but it is allowed in RL. Whilst in a contextual bandit you assume no rules apply to state transitions, and this is a key difference between contextual bandit and RL settings.The policy is stationary if its mapping rules remain unchanged.Your proposed addition to the state does not require changing the policy. The policy is a function of the state. It may choose a different action depending on this historical aspect of the state, but it can still remain a fixed function - its input may change, but the policy function itself does not need to change to account for that."
How to train a transformer text-to-text model on counterexamples?,"
Is it possible to update the weights of a vanilla transformer model using counterexamples alongside examples?
For example, from the PAWS data set, given the phrases ""Although interchangeable, the body pieces on the 2 cars are not similar."" and ""Although similar, the body parts are not interchangeable on the 2 cars."" we have the label 0 because it is a counterexample, whereas for the phrases ""Katz was born in Sweden in 1947 and moved to New York City at the age of 1."" and ""Katz was born in 1947 in Sweden and moved to New York at the age of one."" we have the label 1 because it is a positive example of a valid paraphrase.
My goal is to use the transformer model to generate paraphrases, and I am attempting to build a GAN but could not find any references for updating the transformer text-to-text model using counterexamples.
","['neural-networks', 'natural-language-processing', 'generative-adversarial-networks', 'transformer']",
"Trying to separate spiral data with neural network, learning tensorflow","
I am learning how to use tensorflow without keras, just to make sure I understand tensorflow directly. 
I created a spiral-looking datasets with 100 points of each class (200 total), and I created a neural network to classify it. For the life of me I can't figure out how to get a good accuracy. Do you mind looking at my code to see what I did wrong? 
From what I've gleaned from various forums, it seems like if I do 4 hidden layers and 14 neurons per layer, I should be able to perfectly separate this dataset. I tried with with learning rate of 0.01, and 20k epochs. 
I've tried different combinations of activation functions (tanh, sigmoid, relu, even alternating between them), but the best that I've gotten is around 60 percent, whereas people with less layers and less neurons have gotten close to 90 percent. 
What I did NOT do is to create additional features (for example, r and theta), and this was intentional. I'm just curious to see if I can do this by looking at x and y alone. 
The code is pasted below, and it includes the code to create the data (and it plots the data).
Thank you in advance!
import numpy as np;
import matplotlib.pyplot as plt;
import tensorflow as tf;
import random;

# Part A
# Gather data and plot
def chooseTrainingBatch(X, Y, n, k):
    indices = range(0,n)
    chosenIndices = random.choices(indices,k=k)
    batchX = X[chosenIndices, :]
    batchY = Y[chosenIndices, :]
    return (batchX, batchY)


def doLinearClassification(n, learning_rate=1, epochs=20, num_hidden_layer_1=100, num_of_layers=4, batch_size = 20):
    d = 1;
    plt.figure();
    X = np.zeros([2*n, 2]);
    Y = np.zeros([2*n, 2]);
    for t in np.arange(1,n+1,d):
        r1 = 50 + 0.2*t;
        r2 = 30 + 0.4*t;
        phi1 = -0.06*t + 3;
        phi2 = -0.08*t + 2;

        x1 = r1 * np.cos(phi1);
        y1 = r1 * np.sin(phi1);
        x2 = r2 * np.cos(phi2);
        y2 = r2 * np.sin(phi2);

        plt.scatter(x1, y1, c='b');
        plt.scatter(x2, y2, c='r');

        X[t-1,0] = x1;
        X[t-1,1] = y1;
        Y[t-1,0] = 1;

        X[n+t-1,0] = x2;
        X[n+t-1,1] = y2;
        Y[n+t-1,1] = 1;


    # declare the training data placeholders
    x = tf.placeholder(tf.float32, [None, 2])
    y = tf.placeholder(tf.float32, [None, 2])

    # Weights connecting the input to the hidden layer 1
    W1 = tf.Variable(tf.random_normal([2, num_hidden_layer_1]))
    b1 = tf.Variable(tf.random_normal([num_hidden_layer_1]))

    # activation of hidden layer 1
    hidden_1_out = tf.nn.relu(tf.add(tf.matmul(x, W1), b1))

    last_hidden_out = hidden_1_out
    for i in range(1,num_of_layers):    
        # weights connecting the hidden layer i-1 to the hidden layer i
        next_W = tf.Variable(tf.random_normal([num_hidden_layer_1, num_hidden_layer_1]))
        next_b = tf.Variable(tf.random_normal([num_hidden_layer_1]))

        # activation of hidden layer
        if (i%2 == 0):
            next_hidden_out = tf.nn.tanh(tf.add(tf.matmul(last_hidden_out, next_W), next_b))
        else:    
            next_hidden_out = tf.nn.tanh(tf.add(tf.matmul(last_hidden_out, next_W), next_b))

        # update for next loop
        last_hidden_out = next_hidden_out

    # and the weights connecting the last hidden layer to the output layer
    W_end = tf.Variable(tf.random_normal([num_hidden_layer_1, 2]))
    b_end = tf.Variable(tf.random_normal([2]))

    # activation of output layer
    y_ = tf.nn.sigmoid(tf.add(tf.matmul(last_hidden_out, W_end), b_end))

    # loss function
    y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped) + (1 - y) * tf.log(1 - y_clipped), axis=1))

    # add an optimiser
    optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)

    # finally setup the initialisation operator
    init_op = tf.global_variables_initializer()

    # define an accuracy assessment operation
    correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))
    blah = tf.add(tf.matmul(last_hidden_out, W_end), b_end)
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    # For the purpose of separating dataset, testing data is the entire set
    testingX = X;
    testingY = Y;

    # start the session
    with tf.Session() as sess:
       # initialise the variables
       sess.run(init_op)
       for epoch in range(epochs):
            this_cost = 0
            trainingX, trainingY = chooseTrainingBatch(X, Y, n, batch_size)
            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: trainingX, y: trainingY})
            this_cost += c
            print(""Epoch:"", (epoch + 1), ""cost ="", ""{:.3f}"".format(this_cost))

       #debugging statements
       #this_y = sess.run(y, feed_dict={x: testingX, y: testingY})        
       #print(""y"")
       #print(this_y)
       #this_blah = sess.run(blah, feed_dict={x: testingX, y: testingY})
       #print(""blah"")
       #print(this_blah)
       #this_y_ = sess.run(y_, feed_dict={x: testingX, y: testingY})
       #print(""y_"")
       #print(this_y_)
       this_accuracy = sess.run(accuracy, feed_dict={x: testingX, y: testingY})
       print(""Accuracy: "", this_accuracy)
       return this_accuracy


# this is the real thing
#doLinearClassification(100, learning_rate = 0.01, epochs=20000, num_hidden_layer_1=14, num_of_layers=4, batch_size=20);

# this is just to debug the code
doLinearClassification(20, learning_rate = 0.01, epochs=200, num_hidden_layer_1=14, num_of_layers=4, batch_size=2);
```

","['neural-networks', 'classification', 'tensorflow']",
Looking or the simplest framework to train keypoint detector,"
I currently use an object detector to detect an object and specific parts of it (a crop and its stem). Such detector is not the best choice for detecting parts that could be represented by a point (typically a stem) so I'm planning on moving to a keypoint detector.
After reading the literature it appears that there are many solutions. I'm particularly interested in using a hourglass network to predict a set of heatmaps abstracting the keypoints positions.
The problem is that many of the existing frameworks are dedicated to human pose estimation (for instance OpenPose) but I don't need all this complexity.
By best choice for now is this framework that is a Tensorflow implementation of Hourglass Networks but it is still too specific to human pose estimation.
Do you have any suggestion of frameworks that best suit my application? i.e. simple keypoint detection.
",['deep-learning'],
Can two admissable heuristics not dominate each other?,"
I am working on a project for my artificial intelligence class. I was wondering if I have 2 admissible heuristics, A and B, is it possible that A does not dominate B and B does not dominate A?  I am wondering this because I had to prove if each heuristic is admissible and I did that, and then for each admissible heuristic, we have to prove if each one dominates the other or not. I think I have a case that neither dominates the other and I was wondering if maybe I got the admissibility wrong because of that.
","['search', 'proofs', 'heuristics', 'admissible-heuristic']","This is possible. Admissibility only asserts that the heuristic will never overestimate the true cost. With that being said, it is possible for one heuristic in some cases to do better than another and vice-versa. Think of it as a game of rock paper scissors.Specifically, you may find that sometimes $h_1 < h_2$ and in other times $h_2 < h_1$, where $h_1$ and $h_2$ are admissible heuristics. Thus, by definition, neither strictly dominates the other. In fact, there is a way to ""combine"" the two admissible heuristics to get the best of both using:$$h_3 = \max(h_1, h_2)$$"
Markov property in maze solving problem in reinforcement learning,"
By definition, every state in RL has Markov property, which means that the future state depends only on the current state, not the past states. 
However, I saw that in some case we can define a state to be the history of observations and actions taken so far, such as $s_t = h_t = o_1a_1\dots o_{t-1}a_{t-1}o_t$. I think maze solving can be of that case since the current state, or the current place in a maze, clearly depends on which places the agent has been and which ways the agent has taken so far. 
Then it seems that the future states naturally depend on the past states and the past actions as well. What am I missing?
","['machine-learning', 'reinforcement-learning', 'markov-decision-process']","Hi Hunnam and welcome to our community!By definition, every state in RL has Markov property, which means that the future state depends only on the current state, not the past states. No this is not exactly correct.
We can use RL to solve problems with the Markov Property exactly because the current state is a sufficient statistic of the future. In other words, the state encodes the distribution of future states.Note that the state isn't necessarily the observations. As you point out in the next paragraph:"
What are the examples of agents that is represent these characteristics?,"
I'm looking for examples of AI systems or agents that best represent these five characteristics (one example for each characteristics):

Reactivity 
Proactivity
Adaptability
Sociability 
Autonomy

It would be better if its machine learning-based application.
","['machine-learning', 'intelligent-agent', 'multi-agent-systems']",
How would one implement a multi-agent environment with asynchronous action and rewards per agent?,"
In a single agent environment, the agent takes an action, then observes the next state and reward:
for ep in num_episodes:
    action = dqn.select_action(state)
    next_state, reward = env.step(action)

Implicitly, the for moving the simulation (env) forward is embedded inside the env.step() function.  
Now in the multiagent scenario, agent 1 ($a_1$) has to make a decision at time $t_{1a}$, which will finish at time $t_{2a}$, and agent 2 ($a_2$) makes a decision at time $t_{1b} < t_{1a}$  which is finished at $t_{2b} > t_{2a}$. 
If both of their actions would start and finish at the same time, then it could easily be implemented as: 
for ep in num_episodes:
    action1, action2 = dqn.select_action([state1, state2])
    next_state_1, reward_1, next_state_2, reward_2 = env.step([action1, action2])

because the env can execute both in parallel, wait till they are done, and then return the next states and rewards. But in the scenario that I described previously, it is not clear how to implement this (at least to me). Here, we need to explicitly track time, a check at any timepoint to see if an agent needs to make a decision, Just to be concrete:
for ep in num_episodes:
    for t in total_time:
       action1 = dqn.select_action(state1)
       env.step(action1) # this step might take 5t to complete. 
       as such, the step() function won't return the reward till 5 t later. 
        #In the mean time, agent 2 comes and has to make a decision. its reward and next step won't be observed till 10 t later. 

To summarize, how would one implement a multiagent environment with asynchronous action/rewards per agents?
","['reinforcement-learning', 'ai-design', 'multi-agent-systems']","The cleanest solution from a theoretical point of view is to switch over to a hierarchical framework, some framework that supports temporal abstraction. My favourite one is the options framework as formalised by Sutton, Precup and Singh.The basic idea is that the things that you consider ""actions"" for your agents become ""options"", which are ""large actions"" that may take more than a single primitive time step. When an agent selects an option, it will go on ""auto-pilot"" and keep selecting primitive actions at the more primitive, fine-grained timescale as dictated by the last selected option, until that option has actually finished executing. In your case, you could:Since all legal choices for agents in your scenario appear to be options, i.e. you do not allow agents to select primitive actions at the more fine-grained timescale, you would only have to implement ""inter-option"" learning in your RL algorithms; there would be no need for ""intra-option"" learning.In practice, if you only have a small number of agents and have options that take relatively large amounts of time, you don't have to actually loop through all primitive time-steps. You could, for example, compute the primitive timestamps at which ""events"" should be executed in advance, and insert these events to be processed into an event-handling queue based on these timestamps. Then you can always just skip through to the next timestamp at which an event needs handling. With ""events"" I basically mean all timesteps at which something should happen, e.g. timesteps where an option ends and a new option should be selected by one or more agents. Inter-option Reinforcement Learning techniques are basically oblivious to the existence of a more fine-grained timescale, and they only need to operate at precisely these decision points where one option ends and another begins."
"What are the exact meaning of ""lower-order structure"" and ""higher-order structure"" in this paper?","
I recently read a paper on community detection in networks. In the paper EdMot: An Edge Enhancement Approach for Motif-aware Community Detection, the authors consider the ""lower-order structure"" of the network at the level of individual nodes and edges. And they mentioning about some ""higher-order structure"" method. The point is, what is the exact meaning (definition) of lower- and higher-order structure in a network? 
","['terminology', 'geometric-deep-learning', 'graphs']",
How to calculate multiobjective optimization cost for ordinary problems?,"
What I did:
Created a population of 2D legged robots in a simulated environment. Found the best motor rotation values to make the robots move rightward, using an objective function with Differential Evolution (could use PSO or GA too), that returned the distance moved rightward. Gradient descent used for improving fitness.  
What I want to do:
Add more objectives. To find the best motor rotation, with the least motion possible, with the least jittery motion, without toppling the body upside down and making the least collision impact on the floor.  
What I found: 

Spent almost two weeks searching for solutions, reading research
papers, going through tutorials on Pareto optimality, installing
libraries and trying the example programs.
Using pairing functions to create a cost function wasn't good
enough.
There are many multi-objective PSO, DE, GA etc., but they seem
to be built for solving some other kind of problem.

Where I need help: 

Existing multi objective algorithms seem to use some pre-existing
minimization and maximization functions (Fonseca, Kursawe, OneMax,
DTLZ1, ZDT1, etc.) and it's confusing to understand how I can use my
own maximization and minimization functions with the libraries.
(minimize(motorRotation), maximize(distance),
minimize(collisionImpact), constant(bodyAngle)).
How do I know which is the best Pareto front to choose in a
multi-dimensional space? There seem to be ways of choosing the
top-right Pareto front or the top-left or the bottom-right or
bottom-left. In multi-dimensional space, it'd be even more varied.
Libraries like Platypus, PyGMO, Pymoo etc. just define the problem using
problem = DTLZ2(), instantiate an algorithm algorithm =
NSGAII(problem) and run it algorithm.run(10000), where I assume
10000 is the number of generations. But since I'm using a legged
robot, I can't simply use run(10000). I need to assign motor values
to the robots, wait for the simulator to make the robots in the
population move and then calculate the objective function cost. How
can I achieve this?  
Once the pareto optimal values are found, how is it used to create a
cost value that helps me determine the fittest robot in the
population?

","['genetic-algorithms', 'evolutionary-algorithms', 'gradient-descent']","I eventually used the keep-efficient function from this answer to calculate the Pareto front and used the k-means function to calculate the centroid of the front. This gave me the approximate knee-point of the front, which is usually the optimal solution. One of the calculations was to maximise the distance moved in x direction (dx) vs. minimising the energy consumed (e), so since the x axis needed positive maximization and the y axis needed minimization, I inverted the y axis, since min(f(y)) = - max(-f(y)). This helped get the pareto front toward the top right side of the graph and both the x and y axes were maximization objectives. The optimal point calculated was the robot that had the best fitness."
What order should I learn about Neural Networks? [closed],"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 3 years ago.







                        Improve this question
                    



I'm interested in learning about Neural Networks and implementing them. I'm particularly interested in GANs and LSTM networks.
I understand perceptrons and basic Neural Network configuration (sigmoid activation, weights, hidden layers etc). But what topics do I need to learn in order, to get to the point where I can implement GAN or LSTM.
I intend to make an implementation of each in C++ to prove to myself that I understand. I haven't got a particularly good math background, but I understand most math-things when they are explained.
For example, I understand backpropagation, but I don't really understand it. I understand how reinforced learning is used with backpropagation, but not fully how you can have things like training without datasets (like tD-backgammon). I don't quite understand CNNs, especially why you might make a particular architecture.
If for each ""topic"" there was a book or website or something for each it would be great.
",['neural-networks'],
"Unable to replicate Figure 2.1 from ""Reinforcement Learning: An Introduction""","
The author explains in 2.2 Action-Value Methods:

To roughly assess the relative effectiveness of the greedy and $\varepsilon $-greedy methods, we compared them numerically on a suite of test problems. This is a set of 2000 randomly generated n-armed bandit tasks with n = 10. For each action, a, the rewards were selected from a normal (Gaussian) probability distribution with mean Q*(a) and variance 1. The 2000 n-armed bandit tasks were generated by reselecting the Q*(a) 2000 times, each according to a normal distribution with mean 0 and variance 1. Averaging over tasks, we can plot the performance and behavior of various methods as they improve with experience over 1000 plays, as in Figure 2.1. We call this suite of test tasks the 10-armed testbed. 


But doing my best, the replication yields something nearer to :

I think I am misunderstanding how the author took the averages.
Here is my code:
from math import exp

import numpy
import matplotlib.pyplot as plt


def act(action, Qstar):
    return numpy.random.normal(Qstar[action], 1)


def run(epsilon):
    history = [0 for i in range(1000)]

    for task in range(1, 2000):
        Qstar = [numpy.random.normal(0, 1) for i in range(10)]
        Q = [0 for i in range(10)]
        for t in range(1, 1001):
            if numpy.random.randint(0, 100) < epsilon:
                action = numpy.random.randint(0, len(Q))
            elif t == 0:
                action = 0
            else:
                averages = [q/t for q in Q]
                action = averages.index(max(averages))

            reward = act(action, Qstar)
            Q[action] += reward

            history[t-1] += reward

    return [elem/2000 for elem in history]


if __name__ == '__main__':
    plt.plot(run(10), 'b', label=""É›=0.1"")
    plt.plot(run(1), 'r', label=""É›=0.01"")
    plt.plot(run(0), 'g', label=""É›=0"")
    plt.xlabel('Plays')
    plt.ylabel('Reward')
    plt.legend()
    plt.show()

","['reinforcement-learning', 'greedy-ai']","You are calculating the average reward for each action (i.e. bandit arm) incorrectly. You cannot calculate this simply with a list comprehension, and you need to keep a second list storing the number of times each action was taken.The correct calculation is to divide the total reward obtained from each action by the number of times that action was taken. You are dividing all actions by the total number of actions taken. Adding:results in working code, and I can generate this graph using 200 samples per method, which looks like a noisier version of the one in the figure you referenced:"
"Why is embedding important in NLP, and how does autoencoder work?","
People say embedding is necessary in NLP because if using just the word indices, the efficiency is not high as similar words are supposed to be related to each other. However, I still don't truly get it why.
The subword-based embedding (aka syllable-based embedding) is understandable, for example:
biology   --> bio-lo-gy
biologist --> bio-lo-gist

For the 2 words above, when turning them into syllable-based embeddings, it's good because the 2 words will be related to each other due to the sharing syllables: bio, and lo.
However, it's hard to understand the autoencoder, it turns an index value into vector, then feed these vectors to DNN. Autoencoder can turn vectors back to words too.
How does autoencoder make words related to each other?
","['deep-learning', 'natural-language-processing', 'tensorflow', 'word-embedding', 'hidden-layers']","The information you are probably missing is that word embeddings are learned on the basis of context. For example, you might try to predict a vector for a word from the wordvectors of the other words in the same sentence. This way word vectors of words that occur in similar contexts will turn out to be similar. You can think of it as word vectors not encoding the word themselves but the contexts in which they are used. Of course ultimately that is the same. "
Intelligent reflecting surface,"
I wanted to know about Intelligent reflecting surface (IRS) technology. 
what is the application of IRS in wireless communication? 
what are the competitive advantages over existing technologies?
","['machine-learning', 'swarm-intelligence', 'intelligence']",
Why does having a fixed policy change a Markov Decision Process to a Markov Reward Process?,"
If a policy is fixed, it is said that a Markov Decision Process (MDP) becomes a Markov Reward Process (MRP).
Why is this so? Aren't the transitions and rewards still parameterized by the action and current state? In other words, aren't the transition and reward matrices still cubes?
From my current train of thought, the only thing that is different is that the policy is not changing (the agent is not learning the policy). Everything else is the same.
How is it that it switches to an MRP, which is not affected by actions?
I am reading ""Deep Reinforcement Learning Hands-On"" by Maxim Lapan, which states this. I have also found this statement in online articles, but I cannot seem to wrap my head around it.
","['reinforcement-learning', 'markov-decision-process', 'policies', 'markov-reward-process']","If a policy is fixed, it is said that an MDP becomes an MRP.I would change the phrasing slightly here, to:If a policy is fixed, an MDP can be accurately modeled as an MRP.Why is this so? Aren't the transitions and rewards still parameterized by the action and current state? In other words, aren't the transition and reward matrices still cubes?The transition and reward matrices remain the same in the MDP, but it is possible to flatten them into an equivalent MRP, because in terms of observations of next state and reward, the action that is taken is just part of the transition rules - if the policy is fixed, then so are all the probabilities for next state and reward.More concretely, if you have an MDP with $|\mathcal{A}|$ transition matrices $P_{ss'}^a$ and a fixed policy $\pi(a|s)$, then you can create a combined transition matrix with a sum:$$P_{ss'} = \sum_{a \in \mathcal{A}} \pi(a|s) P_{ss'}^a$$ and you can similarly reduce the reward function. Once you have done so you have data that describe an MRP.How is it that it switches to an MRP which is not affected by actions?If the MDP represents a real system where actions are still being taken by an agent, then of course those are still present within the system, and still affect it. The difference is that if you know the agent's policy, then the action choice is predictable, and the MRP representation covers the full definition of probabilities of observed state transitions and rewards."
Why is having low variance important in offline policy evaluation of reinforcement learning?,"
Intuitively, I understand that having an unbiased estimate of a policy is important because being biased just means that our estimate is distant from the truth value. 
However, I don't understand clearly why having lower variance is important. Is that because, in offline policy evaluation, we can have only 'one' estimate with a stream of data, and we don't know if it is because of variance or bias when our estimate is far from the truth value? Basically, variance acts like bias. 
Also, if that is the case, why having variance is preferable to having a bias?
","['reinforcement-learning', 'policies', 'bias-variance-tradeoff']","Having low variance is important in general as it reduces the number of samples needed to obtain accurate estimates. This is the case for all statistical machine learning, not just reinforcement learning.In general, if you are estimating a mean or expected quantity by taking many samples, the variation in the error is proportional to $\frac{\sigma}{\sqrt{N}}$ for a direct arithemtic mean of all samples, and behaves similarly for other averaging approaches (such as recency-weighted means using a learning rate). The bounds on accuracy can be made better by either increasing $N$ i.e. taking more samples, or by decreasing the variance $\sigma^2$.So anything you can do to reduce variance in your measurements has a direct consequence of reducing the number of samples required to achieve the same degree of accuracy.In the case of off-policy reinforcement learning, there is added variance - compared to on-policy learning - due to different probabilities of taking an action in behaviour and target policies. This is due to the need to adjust reward signals using importance sampling - multiplying by the importance sampling ratio will make the reward signal vary more (in fact it can become unbounded). This is not really any more of a challenge than any other source of variance, but as it interferes with the goal of speedy learning, a lot of research effort has been put into methods that reduce the variance."
Why do very deep non resnet architectures perform worse compared to shallower ones for the same iteration? Shouldn't they just train slower?,"
My understanding of the vanishing gradient problem in deep networks is that as backprop progresses through the layers the gradients become small, and thus training progresses slower. I'm having a hard time reconciling this understanding with images such as below where the losses for a deeper network are higher than for a shallower one. Should it not just take longer to complete each iteration, but still reach the same level if not higher of accuracy?

","['deep-learning', 'training', 'backpropagation', 'architecture']",
"What are the implications of the ""No Free Lunch"" theorem for machine learning?","
The No Free Lunch (NFL) theorem states (see the paper Coevolutionary Free Lunches by David H. Wolpert and William G. Macready)

any two algorithms are equivalent when their performance is averaged across all possible problems

Is the ""No Free Lunch"" theorem really true? What does it actually mean? A nice example (in ML context) illustrating this assertion would be nice.
I have seen some algorithms which behave very poorly, and I have a hard time believing that they actually follow the above-stated theorem, so I am trying to understand whether my interpretation of this theorem is correct or not. Or is it just another ornamental theorem like Cybenko's Universal Approximation theorem?
","['machine-learning', 'optimization', 'proofs', 'no-free-lunch-theorems']","This is a really common reaction after first encountering the No Free Lunch theorems (NFLs). The one for machine learning is especially unintuitive, because it flies in the face of everything that's discussed in the ML community. That said, the theorem is true, but what it means is open to some debate.To restate the theorem for people who don't know it, the NFL theorem for machine learning is really a special case of the NFL theorem for local search and optimization. The local search version is easier to understand. The theorem makes the following, somewhat radical claim:Averaged across all possible optimization problems, the average solution quality found by any local search algorithm you choose to use is exactly the same as the average solution quality of a local ""search"" algorithm that just generates possible solutions by sampling uniformly at random from the space of all solutions.Another formulation, when people want an even stronger reaction, is to say that if you want to find the best solution to a problem, it's just as good to try things that seem to be making your solution iteratively worse as it is to try things that seem to be making your solution iteratively better. On average, both these approaches are equally good. Okay, so why is this true? Well, the key is in the details. Wolpert has sometimes described the theorem as a specialization of Hume's work on the problem of induction. The basic statement of the problem of induction is: we have no logical basis for assuming that the future will be like the past. Logically, there's no reason that the laws of physics couldn't all just radically change tomorrow. From a purely logical perspective, it's totally reasonable that the future can be different from the past in any number of ways. Hume's problem is that, in general the future is like the past in a lot of ways. He tried to formulate a philosophical (logical) argument that this needed to be so, but basically failed.The No Free Lunch theorems say the same thing. If you don't know what your search space looks like, then if you iteratively refine your guess at what a good solution looks like, in response to the observations you've made in the past about what good solutions look like (i.e. learning from data), then it's just as likely that operation you make helps as it is that it hurts. That's why the ""averaged over all possible optimization problems"" part is key. For any optimization problem where hill climbing is a good strategy after $k$ moves, we can make one that is identical, except that the kth hill climbing move leads to an awful solution. The actual proof is more subtle than that, but that's the basic idea.A very brief lay summary might be:A machine learning algorithim can only be made to work better on some kinds of problems by being made to work worse on another kind of problem.So what does this mean in a practical sense? It means that you need to have some apriori reason for thinking that your algorithm will be effective on a particular problem. Exactly what a good reason looks like is the subject of vigorous debate within the ML community. This is very closely related to the bias/variance tradeoff.Some common responses are:Regardless, it's indisputable that some algorithms are better than others, in certain sub-domains (we can see this empirically). NFL tells us that to be better there, they need to be worse somewhere else. The question up for debate is whether the ""somewhere else"" is real problems, or purely artificial ones."
"Given a list of integers $\{c_1, \dots, c_N \}$, how do I find an integer $D$ that minimizes the sum of remainders $\sum_i c_i \text{ mod } D$?","
I have a set of fixed integers $S = \{c_1, \dots, c_N \}$. I want to find a single integer $D$, greater than a certain threshold $T$, i.e. $D > T \geq 0$, that divides each $c_i$ and leaves remainder $r_i \geq 0$, i.e. $r_i$ can be written as $r_i = c_i \text{ mod } D$, such that the sum of remainders is minimized.
In other words, this is my problem
\begin{equation}
\begin{aligned}
D^* \quad = \text{argmin}_D&  \sum_i c_i \text{ mod } D \\
\textrm{subject to} &\quad D > T
\end{aligned}
\end{equation}
If the integers have a common divisor, this problem is easy. If the integers are relatively co-prime however, then it is not clear how to solve it.
The set $|S| = N$ can be around $10000$, and each element also has a value in tens of thousands.
I was thinking about solving it with a genetic algorithm (GA), but it is kind of slow. I want to know is there any other way to solve this problem.
","['machine-learning', 'genetic-algorithms', 'optimization', 'constrained-optimization']",
How can I reduce the GPU memory usage with large images?,"
I am trying to train a CNN-LSTM model. The size of my images is 640x640. I have a GTX 1080 ti 11GB. I am using Keras with the TensorFlow backend.
Here is the model.
img_input_1 = Input(shape=(1, n_width, n_height, n_channels))
conv_1 = TimeDistributed(Conv2D(96, (11,11), activation='relu', padding='same'))(img_input_1)
pool_1 = TimeDistributed(MaxPooling2D((3,3)))(conv_1)
conv_2 = TimeDistributed(Conv2D(128, (11,11), activation='relu', padding='same'))(pool_1)
flat_1 = TimeDistributed(Flatten())(conv_2)
dense_1 = TimeDistributed(Dense(4096, activation='relu'))(flat_1)
drop_1 = TimeDistributed(Dropout(0.5))(dense_1)
lstm_1 = LSTM(17, activation='linear')(drop_1)
dense_2 = Dense(4096, activation='relu')(lstm_1)
dense_output_2 = Dense(1, activation='sigmoid')(dense_2)
model = Model(inputs=img_input_1, outputs=dense_output_2)

op = optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.001)

model.compile(loss='mean_absolute_error', optimizer=op, metrics=['accuracy'])

model.fit(X, Y, epochs=3, batch_size=1)

Right now, using this model, I can only use the training data when the images are resized to 60x60, any larger and I run out of GPU memory.
I want to use the largest possible size as I want to retain as much discriminatory information as possible. (The labels will be mouse screen coordinates between 0 - 640).
Among many others, I found this question: How to handle images of large sizes in CNN?
Though I am not sure how I can ""restrict your CNN"" or ""stream your data in each epoch"" or if these would help.
How can I reduce the amount of memory used so I can increase the image sizes?
Is it possible to sacrifice training time/computation speed in favor of higher resolution data whilst retaining model effectiveness?
Note: the above model is not final, just a basic outlay.
","['convolutional-neural-networks', 'tensorflow', 'training', 'keras', 'gpu']","As the other links suggest, you basically have four options:"
CNN clasification model loss stuck at same value,"
I have CNN model to classify 2 classes. (Yes or No)
I use categorical_crossentropy loss and softmax activation at the end. 
For input I use image with all 3 channels, for output I use One hot encoded vector ([0,1] or [1,0])
I have function that guaranty me, that each batch I have same number of one and another class, so the classes are not unevenly represented. 
What happened when I train the model is that I am stuck at same loss while trening,...
I assume that model predict always same class and half in batch has loss 0 half of them max, so that bring it to 8 all the time,...
What could went wrong?
The network is something like this : 
x = Conv2D(16, (3, 3), padding='same')(input_img)
x = LeakyReLU(0.1)(x)
x = Conv2D(32 , (3, 3), padding='same')(x)
x = LeakyReLU(0.1)(x)
x = MaxPooling2D((2, 2))(x)
x = Dropout(0.25)(x)
x = Conv2D(32 , (3, 3), padding='same')(x)
x = LeakyReLU(0.1)(x)
x = Conv2D(48 , (3, 3), padding='same')(x)
x = LeakyReLU(0.1)(x)
x = MaxPooling2D((2, 2))(x)
x = Dropout(0.25)(x)

x = Flatten()(x)
x = Dense(4096)(x)
x = LeakyReLU(0.1)(x)
x = Dropout(0.5)(x)
x = Dense(2048)(x)
x = LeakyReLU(0.1)(x)
x = Dropout(0.5)(x)
out = Dense(2, activation='softmax', name='table')(x)

model = Model(input_img, out)
model.compile(optimizer='adam', loss= 'categorical_crossentropy')

Training Loss: 

","['convolutional-neural-networks', 'objective-functions', 'categorical-data']",
What is the best variant of darknet to use?,"
pjreddie's official darknet version (link from official website here) has been forked several times. In particular I've come accross AlexeyAB's fork through this tutorial. I assume the tutorial's author used AlexeyAB's fork because he wanted to use it on a Windows machine, which pjreddie's darknet cannot do AFAIK.
I am not really concerned about that (I am a linux user), but I am very interested about the half precision option (CUDNN_HALF) that AlexeyAB's darknet has, whereas pjreddie's darknet does not. Of course I've checked that this option was handled by the graphic card (RTX2080) we use at my office.
Nevertheless, I wonder: how stable/robust is that fork? Of course I want high-performing software, but I also want a certain level of stability! On the other hand, the latest commit on pjreddie's darknet is back from September 2018 (ie 1 year old), whereas AxeleyAB's darknet is activeâ€¦
More broadly, there seems to be a lot of darknet forks: which ones to prefer?
What does the neural network community think?
",['neural-networks'],
Why is batch gradient descent performing worse than stochastic and minibatch gradient descent?,"
I have implemented a neural network from scratch (only using numpy) and I am having problems understanding why the results are so different between stochastic/minibatch gradient descent and batch gradient descent:

The training data is a collection of point coordinates (x,y).
The labels are 0s or 1s (below or above the parabola).

As a test, I am doing a classification task.
My objective is to make the NN learn which points are above the parabola (yellow) and which points are below the parabola (purple).
Here is the link to the notebook: https://github.com/Pign4/ScratchML/blob/master/Neural%20Network.ipynb

Why is the batch gradient descent performing so poorly with respect
to the other two methods?
Is it a bug? But how can it be since the code is almost identical to the
minibatch gradient descent?
I am using the same (randomly chosen with try and error) hyperparameters for all three
neural networks. Does batch gradient descent need a more accurate
technique to find the correct hyperparameters? If yes, why so?

","['python', 'gradient-descent']","Assuming the problem at hand is a classification (Above or Below parabola), this is probably because of the nature of Batch gradient descent. Since the gradient is being calculated on the whole batch, it tends to work well on only convex loss functions. The reason for why batch gradient descent is not working too great probably is because of the high number of minimas in the error manifold, ending up learning nothing relevant. You can change the loss function and observe the change in results, they might not be great (Batch GD usually isn't) but you'll be able to see differences.You can check this out for more on the differences between the three. Hope this helped!"
how to use Softmax action selection algorithm in atari-like game,"
I'm currently writing a program using keras (python 3) to play a game similar to Atari games, only in this one there are objects moving in the screen in different angles and directions (in most of Atari games I've encountered the objects you need to shoot are static). The agent's aim is to shoot them.
after executing every action I get feedback from the environment: I get the locations of all the objects on the screen, the locations of the collisions that happened, my position (angle of the turret) and the total score (from which I can calculate the reward)
I defined that each state will consists from the parameters mentioned above.
I want to use softmax algorithm in order to choose the next action, but I'm not sure how to do it. I'd be very grateful if anyone could help me or refer me to a source that can explain the syntax? currently I'm using decay epsilon-greedy algorithm.
Thank you very much for your time and attention.
","['reinforcement-learning', 'python', 'keras']",
"In the graph search version of A*, can I stop the search the first time I encounter the goal node?","
I am going through Russel and Norvig's Artificial Intelligence: A Modern Approach (3rd edition). I was reading the part regarding the A* algorithm

A* graph search version is optimal when heuristic is consistent and tree search version optimal when heuristic is just admissible.

The book gives the following graph search algorithm.

The above algorithm says that pop the node from the frontier set and expand it (assuming not in explored set), and add its children to frontier only if child not in frontier or explored set. 
Now, if I apply the same to A* (assuming a consistent heuristic) and suppose I find goal state (as a child of some node) for the first time I add it to the frontier set. Now, according to this algorithm, if the goal state is already in the frontier set, it must never be added again (this implies never be updated/promoted right?).
I have a few questions.

I might as well stop the search when I find goal state for the first time as a child of some node and not wait till I pop the goal state from the frontier?
Does a consistent heuristic guarantee that when I add a node to the frontier set I have found the optimal path to it? (because if I don't update it or re-add it with updated cost (according to the graph search algorithm, the answer to the question must be yes.)  

Am I missing something? Because it also states that, whenever A* selects the node for expansion, the optimal path to that node is found and doesn't say that when a node is added to the frontier set, the optimal path is found.
So, I'm pretty confused, but I think the general graph search definition (in the above image) is misleading.
","['definitions', 'search', 'a-star', 'consistent-heuristic', 'graph-search']","No, the optimal path is found when you pop the goal state. If you stop the search when you first add the goal state then the final path may not be optimal. "
Is it possible to do K-nearest-neighbours before training DNN,"
The following X-shape alternated pattern can be separated quite well and super fast by K-nearest Neighbour algorithm (go to https://ml-playground.com to test it):

However, DNN seems to face great struggles to separate that X-shape alternated data. Is it possible to do K-nearest before DNN, ie. set the DNN weights somehow to simulate the result of K-nearest before doing DNN training?
Another place to test the X-shape alternated data: https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html
","['neural-networks', 'deep-learning', 'learning-algorithms', 'greedy-ai', 'symbolic-ai']","There are two factors that will change the ability of a deep neural network to fit a given dataset: either you need more data, or a deeper and wider network. Since the pattern is only 2-d, it can likely be approximated by some sort of simple periodic function. A DNN can approximate periodic functions pretty well, so the issue is probably that you don't have enough data. If you have an apriori belief that the pattern is well approximated by K-nearest neighbors, then you could do the following:"
RF: How to deal with environments changing state due to external factors,"
I have a use case where the state of the environment could change due to random events in between time steps that the agent takes actions. For example, at t1, the agent takes action a1 and is given the reward and the new state s1. Before the agent takes the next action at t2, some random events occurred in the environment that altered the state. Now when the agent takes action at t2, it's now acting on ""stale information"" since the state of the environment had changed. Also, the new state s2 will represent changes not only due to the agent's action, but also due to the prior random events that occurred. In the worst case, the action could possibly have become invalid for the new state that was introduced due to these random events occurred within the environment.
How do we deal with this? Does this mean that this use case is not a good one to solve with RF? If we just ignore these changing states due to the random events in the environment, how would that affect the various learning algorithms? I presume that this is not a uncommon or unique problem in real-life use cases...
Thanks!
Francis
",['reinforcement-learning'],
What is the impact of using multiple BMUs for self-organizing maps?,"
Here's a sort of a conceptual question. I was implementing a SOM algorithm to better understand its variations and parameters. I got curious about one bit: the BMU (best matching unit == the neuron that is more similar to the vector being presented) is chosen as the neuron that has the smallest distance in feature space to the vector. Then I updated it and its neighbours.
This makes sense, but what if I used more than one BMU for updating the network? For example, suppose that the distance to one neuron is 0.03, but there is another neuron with distance 0.04. These are the two smallest distances. I would use the one with 0.03 as the BMU.
The question is, what would be the expected impacts on the algorithm if I used more than one BMU? For example, I could be selecting for update all neurons for which the distance is up to 5% more than the minimum distance.
I am not asking for code. I can implement it to see what happens. I am just curious to see if anyone have any insight on the pros and cons (except additional complexity) of this approach.
","['neural-networks', 'self-organizing-map']",
Looking for GAN paper with spiral image,"
I am looking for a GAN paper I have read a while ago, but unfortunately cannot find it again. I think it compared GANs and other methods (like CVAEs) w.r.t. how they handle multi-modal data, not sure about the CVAEs though.
What I know is that they created different synthetic toy datasets with multiple modes to analyze this. I remember one plotting of a blue spiral of this data on a white background. Any guesses?
","['generative-adversarial-networks', 'reference-request']",
"Can a model, retrained on images classified previously by itself, increase its accuracy?","
Let's assume I have a CNN model trained to categorize some objects on the images. By using this model I find more categorized images. If I now retrain this model on data set that consists old set and newly categorised images is there a chance that such new model will have higher accuracy? Or maybe because new data posses only information that could be found on initial set, model will have similar/lower accuracy?
Please let me know if something unclear. 
","['convolutional-neural-networks', 'image-recognition', 'classification']","The most likely outcome of this approach is wasted time and very little effect on accuracy.There will be changes to the model. Some will be beneficial and improve the model, but some will backfire making it worse.For instance:The model predicts with probability 0.4 that an image is in a certain class. It is the highest prediction, and actually true. It will be added to the training dataset with a ""ground truth"" of probability 1.0, so on balance more and better data has been added to the data set. This will improve generalisation, as whatever caused the relatively low 0.4 value initially - e.g. a pose or lighting variation - will now be covered correctly in the training set.The model predicts with probability 0.4 that an image is in a certain class. It is the highest prediction, and actually false. It will be added to the dataset with ""ground truth"" of probability 1.0 for the wrong class. This will weaken associations to the correct class for similar input images, meaning for exaple that a certain pose or lighting difference that is already causing problems for the model will be used to incorrectly classify images in future.These two scenarios will occur, on average, at a rate determined by the model's current test accuracy. So if your current model is 90% accurate, 1 in 10 images in your new training data will be mislabelled. This will  ""lock in"" the current errors at the same rate on average as they already occur.The effect may be a drift up or down in accuracy as the model will definitely change due to the new training data. However you have little to no control over how this drift effect goes if you are not willing or able to oversee the automatic classifications generated on new data by the model.There are a few ways to get some improvement unsupervised from new data. For instance:Build an autoencoder from the early convolutional layers of your model and train it to re-generate all inputs as outputs. This should help it learn important features of the variations in data that you are using. Once this training is done, discard the decoder part of the auto-encoder and add your classifier back in to fine tune it. This may help if you have only a small amount of labelled data, but a lot of unlabelled data.Use a model that has better accuracy than yours to auto-label the data. This might seem a little chicken-and-egg, but you may be able to create such a model using ensemble techniques. The ensemble model could be too awkward to use in production, but may still be used in an auto-labeling pipeline to improve your training data.Note you may get even better results simply ignoring the extra unlabeled data, and instead fine tuning a high quality ImageNet-trained model on the labeled data you already have - saving yourself a lot of effort. Depends on the nature of the images, and how much labeled data you are already working with."
What is the relationship between the size of the hidden layer and the size of the cell state layer in an LSTM?,"
I was following some examples to get familiar with TensorFlow's LSTM API, but noticed that all LSTM initialization functions require only the num_units parameter, which denotes the number of hidden units in a cell.
According to what I have learned from the famous colah's blog, the cell state has nothing to do with the hidden layer, thus they could be represented in different dimensions (I think), and then we should pass at least 2 parameters denoting both #hidden and #cell_state. 
So, this confuses me a lot when trying to figure out what the TensorFlow's cells do. Under the hood, are they implemented like this just for the sake of convenience or did I misunderstand something in the blog mentioned?

","['neural-networks', 'tensorflow', 'recurrent-neural-networks', 'long-short-term-memory']",
How does Atlas from Boston Dynamics have human-like movement?,"
Discussing the video More Parkour Atlas, a friend asked how the robot's movement was so similar to the one from a real human and wondering how this is achieved?
To my knowledge, this is not something the developer ""programmed"", but instead emerged from the learning algorithm.
Could you provide an overview and some reference on how this is achieved?
","['reference-request', 'robotics', 'human-like', 'robots']",
"TD losses are descreasing, but also rewards are decreasing, increasing sigma","
I'm using Q-learning with some extensions such as noisy linear layers, n-steps and double DQN.
The training, however, isn't that successful, my rewards are descreasing over time after a steep increase at the beginning:

But what's interesting is that my td loss is also descreasing:

The sigma magnitudes of the noisy linear layers which control the exploration are strangely increasing, and also, seems to converge. I expected it to reduce uncertainty over time, but the opposite is the case.

Another intresting thing, and that's probably why my loss is decreasing: The model tends to generate always the same transition, which is why the episodes are ending early and the rewards are getting lower. My experience replay is full of this single transition (around 99 percent of the buffer).
What could be the reason? Which things I should check? Is there anything I could try? I'm also willing to add information, just comment what could be of interest.
","['reinforcement-learning', 'q-learning', 'dqn']",
Will parameter sweeping on one split of data followed by cross validation discover the right hyperparameters?,"
Let's call our dataset splits train/test/evaluate. We're in a situation where we require months of data. So we prefer to use the evaluation dataset as infrequently as possible to avoid polluting our results. Instead, we do 10 fold cross validation (CV) to estimate how well the model might generalize.
We're training deep learning models that take between 24-48 hours each, and the process of parameter sweeping is obviously very slow when performing 10-fold cross validation.
Does anyone have any experience or citations for how well parameter sweeping on one split of the data followed by cross validation (used to estimate how well it generalizes) works?
I suspect it's highly dependent on the distribution of data and local minima & maxima of the hyper parameters, but I wanted to ask.
","['machine-learning', 'deep-learning', 'hyperparameter-optimization', 'cross-validation', 'generalization']",
What is the term for datasets that are themselves composed of datasets?,"
As computers are getting bigger better and faster, the concept of what constitutes a single datum is changing.
For example, in the world of pen-and-paper, we might take readings of temperature over time and obtain a time-series in which an individual datum is a time, temperature pair.  However, it is now common to desire classifications of entire time-series, in the context of which our entire temperature time-series would be but a single data point in a data set consisting of a great number of separate time-series.  In image processing, an $(x,y,c)$ triple is not a datum, but a whole grid of such values is a single datum.  With lidar data and all manner of other fields things that were previously considered a dataset are now best thought of as a datum.
What is the term for datasets that are themselves composed of datasets?
The term ""metadata"" is occupied, I should think.
Are there any papers that talk about this transition from datasets of data to datasets of datasets? And what the implications are for data scientists and researchers?
","['machine-learning', 'reference-request', 'terminology', 'datasets']",
Is the agent aware of a possible different set of actions for each state?,"
I have a use case where the set of actions is different for different states. Is the agent aware of what actions are valid for each state, or is the agent only aware of the entire action space (in which case I guess the environment needs to discard invalid actions)?
I presume the answer is yes, but I would like to confirm.
","['reinforcement-learning', 'markov-decision-process', 'action-spaces']",
Is there a neural network that can output a unit vector that is parallel to the input vector?,"
I'm wondering if there is a NN that can achieve the following task:
Output a unit vector that is parallel to the input vector. i.e., input a vector $\mathbf{v}\in\mathbb{R}^d$, output $\mathbf{v}/\|\mathbf{v}\|$. The dimension $d$ can be fixed, say $2$.
To achieve this, it seems to me that we need to use NN to do three functions: square, square-root, and division. But I don't know if a NN can do all of these.
","['neural-networks', 'machine-learning', 'deep-learning', 'model-request']",
How to deal with large (or NaN) neural network's weights?,"
My weights go from being between 0 and 1 at initialization to exploding into the tens of thousands in the next iteration. In the 3rd iteration, they become so large that only arrays of nan values are displayed.
How can I go about fixing this?
Is it to do with the unstable nature of the sigmoid function, or is one of my equations incorrect during backpropagation which makes my gradients explode?
import numpy as np
from numpy import exp
import matplotlib.pyplot as plt
import h5py

# LOAD DATASET
MNIST_data = h5py.File('data/MNISTdata.hdf5', 'r')
x_train = np.float32(MNIST_data['x_train'][:])
y_train = np.int32(np.array(MNIST_data['y_train'][:,0]))
x_test = np.float32(MNIST_data['x_test'][:])
y_test = np.int32(np.array(MNIST_data['y_test'][:,0]))
MNIST_data.close()

##############################################################################
# PARAMETERS 
number_of_digits = 10 # number of outputs
nx = x_test.shape[1] # number of inputs ... 784 --> 28*28
ny = number_of_digits
m_train = x_train.shape[0]
m_test = x_test.shape[0]
Nh = 30 # number of hidden layer nodes
alpha = 0.001
iterations = 3
##############################################################################
# ONE HOT ENCODER - encoding y data into 'one hot encoded'
lr = np.arange(number_of_digits)
y_train_one_hot = np.zeros((m_train, number_of_digits))
y_test_one_hot = np.zeros((m_test, number_of_digits))
for i in range(len(y_train_one_hot)):
  y_train_one_hot[i,:] = (lr==y_train[i].astype(np.int))
for i in range(len(y_test_one_hot)):
  y_test_one_hot[i,:] = (lr==y_test[i].astype(np.int))

# VISUALISE SOME DATA
for i in range(5):
  img = x_train[i].reshape((28,28))
  plt.imshow(img, cmap='Greys')
  plt.show()

y_train = np.array([y_train]).T
y_test = np.array([y_test]).T
##############################################################################
# INITIALISE WEIGHTS & BIASES
params = { ""W1"": np.random.rand(nx, Nh),
           ""b1"": np.zeros((1, Nh)),
           ""W2"": np.random.rand(Nh, ny),
           ""b2"": np.zeros((1, ny))
          }

# TRAINING
# activation function
def sigmoid(z):
  return 1/(1+exp(-z))

# derivative of activation function
def sigmoid_der(z):
  return z*(1-z)

# softamx function
def softmax(z):
  return 1/sum(exp(z)) * exp(z)

# softmax derivative is alike to sigmoid
def softmax_der(z):
  return sigmoid_der(z)

def cross_entropy_error(v,y):
  return -np.log(v[y])

# forward propagation
def forward_prop(X, y, params):
  outs = {}
  outs['A0'] = X
  outs['Z1'] = np.matmul(outs['A0'], params['W1']) + params['b1']
  outs['A1'] = sigmoid(outs['Z1'])
  outs['Z2'] = np.matmul(outs['A1'], params['W2']) + params['b2']
  outs['A2'] = softmax(outs['Z2'])
  
  outs['error'] = cross_entropy_error(outs['A2'], y)
  return outs

# back propagation
def back_prop(X, y, params, outs):
  grads = {}
  Eo = (y - outs['A2']) * softmax_der(outs['Z2'])
  Eh = np.matmul(Eo, params['W2'].T) * sigmoid_der(outs['Z1'])
  dW2 = np.matmul(Eo.T, outs['A1']).T
  dW1 = np.matmul(Eh.T, X).T
  db2 = np.sum(Eo,0)
  db1 = np.sum(Eh,0)
  
  grads['dW2'] = dW2
  grads['dW1'] = dW1
  grads['db2'] = db2
  grads['db1'] = db1
#  print('dW2:',grads['dW2'])
  return grads

# optimise weights and biases
def optimise(X,y,params,grads):
  params['W2'] -= alpha * grads['dW2']
  params['W1'] -= alpha * grads['dW1']
  params['b2'] -= alpha * grads['db2']
  params['b1'] -= alpha * grads['db1']
  return 

# main
for epoch in range(iterations):
  print(epoch)
  outs = forward_prop(x_train, y_train, params)
  grads = back_prop(x_train, y_train, params, outs)
  optimise(x_train,y_train,params,grads)
  loss = 1/ny * np.sum(outs['error'])
  print(loss)
  
```

","['neural-networks', 'machine-learning', 'exploding-gradient-problem']","This problem is called exploding gradients, resulting in an unstable network that at best cannot learn from the training data and at worst results in NaN weight values that can no longer be updated.One way to assure it is exploding gradients, is if loss is unstable and not improving, or if loss shows NaN value during training. Apart from the usual gradient clipping and weights regularization that are recommended, I think the problem with your network is the architecture. 30 is an abnormally high number of nodes for 2 layer perceptron model. Try increasing number of layers and reducing nodes per layer. - This is under the assumption that you're experimenting with MLP's, because for the problem above, convolutional neural networks seem like an obvious way to go. If unexplored - definitely check out CNN's  for digit recognition, two layer models will surely work better there.Hope this helped!"
How do neural network topologies affect GPU/TPU acceleration?,"
I was thinking about different neural network topologies for some applications. However, I am not sure how this would affect the efficiency of hardware acceleration using GPU/TPU/some other chip.
If, instead of layers that would be fully connected, I have layers with neurons connected in some other way (some pairs of neurons connected, others not), how is this going to affect the hardware acceleration?
An example of this is the convolutional networks. However, there is still a clear pattern, which perhaps is exploited by the acceleration, which would mean that if there is no such pattern, the acceleration would not work as well?
Should this be a concern? If so, is there some rule of thumb for how the connectivity pattern is going to affect the efficiency of hardware acceleration?
","['neural-networks', 'convolutional-neural-networks', 'training', 'architecture', 'hardware']","The topology of a neural network can have a significant impact on the performance of GPU and TPU acceleration.The most important factor is the number of layers and the connectivity between them.Simple topologies require less data movement and can be more easily parallelized. a shallower network with fewer layers will often be faster to train on a GPU or TPU than a deeper network with more layers. This is because each layer in a neural network must be fully connected to the previous and next layers, and a deep network will have many more connections than a shallow network.Additionally, the activation function used for each layer can also impact performance. ReLU is a common activation function that is often used in networks that are accelerated by GPUs or TPUs.GPU is widely used in neural network applications due to a large number of ALU units which helps in faster data processing (multiplication and summation operations in NN), and also the GPU caches, which help in data reuse.The GPU is capable of merging multiple data access requests using the controllers, and it helps in massive parallel and pipelined processing.
GPU is a temporal architecture paradigm with a large number of ALUs, but the ALUs lack direct data communication, and they communicate using direct memory access.GPU has around $3,000â€“5,000$ ALU But the Von Neumann bottleneck exists in GPU due to the access to registers and the shared memory for intermediate data storage in every ALU operation.The GPU has specialized libraries for CNN acceleration like fbfft (Vasilache et al., 2015). While using a high working set, the shared memory cannot be used, and there is a need for global memory access in GPU, and this leads to more memory footprints and memory access.TPU is a custom-made ASIC with a matrix processor which is specially designed for neural networks. it effectively handles the addition and multiplication in neural nets at a very high speed with very little power consumption.The von Neumann bottleneck in CPU and GPU is overcome in TPU with the systolic array structure. TPU v2 single processor has 16-bit two 128 Ã— 128 systolic arrays with 32,768 ALUs.Systolic array in TPU helps in data reuse which makes the performance high and execution energy efficient in CNN.The two-dimensional multiply unit helps in matrix multiplication faster compared to the one-dimensional multiply units in CPU and GPU.In TPU, eight-bit integers are used in place of the 32-bit floating-point operations, and this makes the computations faster and memory efficient.Unlike CPU and GPU TPU drops features that are not used in the neural network, which helps in saving energy.CNN implementation in TPU will have both TPU and CPU usage in parallel to run the linear and non-linear elements in CNN.In CNN the convolution and classification layer is executed in TPUsince it is a GEMM operation, and the Pooling and Flattening are executed in the CPU.Here is the paper that analyzed CNN model performance for the three-image processing application in GPU/TPU platforms in Colab for various batch sizes. The analysis was done by varying the final feed-forward network and the hidden layers, and this gives an inference on how the performance is affected when the model structure changes
Fig(a): Training time for single and multiple convolutional layer networks.In the paper, analysis was done using a single convolutional layer followed by all other layers for the mask detection application for binary class, and the training time was analyzed for GPU and TPU for both networks. The training time increases when the convolutional layers are removed because the number of nodes gets more, and thereby training time increases. The training time for Single layer convolution and multiple-layer Convolution for different batch sizes is shown in Fig(a).The analysis clearly shows that the time decreases when the number of convolutions increases due to a reduction in the number of nodes. The training time is less for the multiple layers CNN compared to single-layer CNN and also with an increase in the batch size.
Fig(b): Execution time for multi and binary classes.The overall training time for each of the three applications was in GPU and TPU for both binary, and multiple classifications were analyzed and shown in Fig(b).From Fig(b), it is clear that compared to TPU, GPU has a low time for execution of the CNN. This occurs due to the bottleneck that occurs in TPU due to the in-between CPU access.GPU: GPU performs well for small batches and gives better flexibility and easy programming. For small data, batch sizes GPU fits better due to the execution pattern in wraps and scheduling id easy on-stream multiprocessors. For large dataset and network models, GPU performs well by optimizing memory reuse. In fully connected neural networks, weight reuse is less, so as the model size increases, this leads to high memory traffic. In GPU, the memory bandwidth makes it practical for applications with memory requirements. Large neural networks work better on GPU compared to CPU due to the extra parallelism feature. For fully connected neural networks, GPU works better compared to CPU, but for large batch sizes, TPU performs well.TPU: TPU performs well on CNN with large batches to give high throughput in training time using the systolic array structure. Large batches of data are needed for the full utilization of the matrix multiply units in the systolic array of TPU. In CNN, the speedup increases with batch size. For enormous batch sizes and complex CNN, TPU is the best because of the spatial reuse characteristics of CNNs. But in fully connected networks, the weight reuse is less, and so TPU is not preferred.Different neural network topologies may require different amounts of resources (memory, computational power, etc.), which could affect the efficiency of hardware acceleration. There is no easy rule of thumb, however, as it depends on the specific architecture and implementation."
What are all the different kinds of neural networks used for? [closed],"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            



Closed 2 years ago.











Locked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.
                                
                            






I found the following neural network cheat sheet (Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data). 

What are all these different kinds of neural networks used for? For example, which neural networks can be used for regression or classification, which can be used for sequence generation, etc.? I just need a brief overview (1-2 lines) of their applications.
","['neural-networks', 'machine-learning', 'deep-learning', 'applications']","I agree that this is too broad, but here's a 1 sentence answer for most of them. The ones I left out (from the bottom of the chart) are very modern, and very specialized. I don't know much about them, so perhaps someone who does can improve this answer.Deep Convolutional Network: Like a feed-forward network, but each node is really  a bank of nodes learning a convolution from the layer
before it. This essentially allows it to learn filters, edge
detectors, and other patterns of interest in video and audio
processing.Deep Deconvolutional Network: Opposite of a Convolutional Network in some sense. Learn a mapping from features that represent edges or
other high level properties of some unseen image, back to the pixel
space. Generate images from summaries.DCIGN: Essentially an auto-encoder made of a DCN and a DN stuck together. Used to learn generative models for complex images like
faces.Generative Adversarial Network: Used to learn generative models for complex images (or other data types) when not enough training data is
available for a DCIGN. One model learns to generate data from random
noise, and the other learns to classify the output of the first
network as distinct from whatever training data is available."
Is randomness necessary for AI?,"
Is randomness (either true randomness or simulated randomness) necessary for AI? If true, does it mean ""intelligence comes from randomness""? 
If not, can a robot lacking the ability to generate random numbers be called an artificial general intelligence?
","['philosophy', 'randomness']",
Puzzle solving AI?,"
I have a book containing lots of puzzles with instructions like:
""Find a path through all the white squares.""

""Connect each black circle to a white circle with a straight line without crossing lines"".

""Put the letters A to G in the grid so that no letters are repeated in any row or collumn""

I thought it might be fun to (1) Try and write a program to solve each individual puzzle (2) Write a program that can solve more general problems, and even more interesting (3) Try to write a program that parses the English instructions and then solves the problem.
I think that in general there would be common themes like, ""draw a path"", ""connect the dots"", ""place the letters in the grid"", and so forth. 
The program would have general knowledge of thing like squares, cells, rows, collumns, colours, letters, numbers and so on.
I wondered if there is anything similar out there already?
If an AI could read instructions and solve the puzzles could we say that it is in someway intelligent?
",['game-ai'],
What kind of neural network architecture is suitable for variable length block-like time series data?,"
I'm not sure what this type of data is called, so I will give an example of the type of data I am working with:

A city records its inflow and outflow of different types of vehicles every hour. More specifically, it records the engine size. The output would be the pollution level X hours after the recorded hourly interval. 

It's worth noting that the data consists of individual vehicle engine size, so they cant be aggregated. This means the 2 input vectors (inflow and outflow) will be of variable length (different number of vehicles would be entering and lraving every hour) and I'm not sure how to handle this. I could aggregate and simply sum the number of vehicles, but I want to preserve any patterns in the data. E.g. perhaps there is a quick succession of several heavy motorbike engines, denoting a biker gang have just entered the city and are known to ride recklessly, contributing more to pollution than the sum of its parts.
Any insight is appreciated. 
","['neural-networks', 'prediction', 'time-series']",
How is Average Recall (AR) calculated for an object detection model?,"
After looking around the internet (including this paper, I cannot seem to find a satisfactory explanation of the Average Recall (AR) metric. On the COCO website, it describes AR as: ""the maximum recall given a fixed number of detections per image, averaged over categories and IoUs"".
What does ""maximum recall"" mean here?
I was wondering if someone could give a reference or a high level overview of the AR calculation algorithm.
Thanks!
","['machine-learning', 'deep-learning', 'object-detection']",
How can we conclude that an optimization algorithm is better than another one,"
When we test a new optimization algorithm, what the process that we need to do?For example, do we need to run the algorithm several times, and pick a best performance,i.e., in terms of accuracy, f1 score .etc, and do the same for an old optimization algorithm, or do we need to compute the average performance,i.e.,the average value of accuracy or f1 scores for these runs, to show that it is better than the old optimization algorithm? Because when I read the papers on a new optimization algorithm, I don't know how they calculate the performance and draw the train-loss vs iters curves, because it has random effects, and for different runs we may get different performance and different curves. 
","['optimization', 'convergence', 'performance', 'hyperparameter-optimization']",
Is there a GAN that can be used for sequence prediction?,"
I want to use a GAN for sequence prediction, in a similar way that we use RNNs for sequence prediction. I want to test its performance in comparison with RNNs. Is there a GAN that can be used for sequence prediction?
","['deep-learning', 'applications', 'prediction', 'generative-adversarial-networks']",
What sort of Neural Network is best suited to predicting a future purchase?,"
I have previously implemented a Neural Network with Back-Propagation that was able to learn Tic-tac-toe and could go pretty well at Connect-4.
Now I'm trying to do a NN that can make a prediction. The idea is that I have a large set of customer purchase history, so people I can ""target"" with marketing, others I can't (maybe I just have a credit-card number but no email address to spam). I've a catalogue of products that changes on a monthly basis with daily updates to stock.
My original idea was to use the same NN that I've used before, with inputs like purchased y/n for each product and an output for each product (softmax to get a weighted prediction). But I get stuck at handling a changing catalog. I'm also not sure if I should lump everyone in together or sort of generate a NN for each person individually (but some people would have very little purchase history, so I'd need to use everyone else as the training set).
So I thought I'd need something with some ability to use the purchase data as a sequence, so purchased A, then B, then C etc. But reviewing something like LSTM, I kind of think it's still not right.
Basically, I know how to NN for a game-state sort of problem. But I don't know how to do it for this new problem.
","['neural-networks', 'long-short-term-memory']",
How would an AI understand grids?,"
OK, now I think an AI must view grids in a different way to computers.
For example a computer would represent a grid like this:
cells = [[1,2,3],[4,5,6],[7,8,9]] = [row1,row2,row3]

That is a grid is 3 rows of 3 cells. 
But... that's not how a human sees it. A human sees a grid as made of 3 rows and 3 collumns somehow intersecting.
If an AI is built on some mathematical logic like set theory, it's like a set of rows which in turn is a set of cells.
So what would be a way to represent a grid in a computer that is more ""human"". And doesn't favor either rows or columns? Or is there some mathematical or programmatical description of a grid that treats rows and columns as equivalent? 
","['ai-design', 'knowledge-representation']",
How to change this RNN text classification code to become text generation code?,"
I can do text classification with RNN, in which the last output of RNN (rnn_outputs[-1]) is used to matmul with output layer weight and plus bias. That is getting a word (class name) after the last T in the time dimension of RNN.
The matter is for text generation, I need a word somewhere in the middle of time dimension, eg.:
t0  t1    t2  t3
The brown fox jumps

For this example, I have the first 2 words: The, brown.
How to get the next word ie. ""fox"" using RNN (LSTM)? How to convert the following text classification code to text generating code?
Source code (text classification):
import tensorflow as tf;
tf.reset_default_graph();

#data
'''
t0      t1      t2
british gray    is => cat (y=0)
0       1       2
white   samoyed is => dog (y=1)
3       4       2 
'''
Bsize = 2;
Times = 3;
Max_X = 4;
Max_Y = 1;

X = [[[0],[1],[2]], [[3],[4],[2]]];
Y = [[0],           [1]          ];

#normalise
for I in range(len(X)):
  for J in range(len(X[I])):
    X[I][J][0] /= Max_X;

for I in range(len(Y)):
  Y[I][0] /= Max_Y;

#model
Inputs   = tf.placeholder(tf.float32, [Bsize,Times,1]);
Expected = tf.placeholder(tf.float32, [Bsize,      1]);

#single LSTM layer
#'''
Layer1   = tf.keras.layers.LSTM(20);
Hidden1  = Layer1(Inputs);
#'''

#multi LSTM layers
'''
Layers = tf.keras.layers.RNN([
  tf.keras.layers.LSTMCell(30), #hidden 1
  tf.keras.layers.LSTMCell(20)  #hidden 2
]);
Hidden2 = Layers(Inputs);
'''

Weight3  = tf.Variable(tf.random_uniform([20,1], -1,1));
Bias3    = tf.Variable(tf.random_uniform([   1], -1,1));
Output   = tf.sigmoid(tf.matmul(Hidden1,Weight3) + Bias3);

Loss     = tf.reduce_sum(tf.square(Expected-Output));
Optim    = tf.train.GradientDescentOptimizer(1e-1);
Training = Optim.minimize(Loss);

#train
Sess = tf.Session();
Init = tf.global_variables_initializer();
Sess.run(Init);

Feed = {Inputs:X, Expected:Y};
for I in range(1000): #number of feeds, 1 feed = 1 batch
  if I%100==0: 
    Lossvalue = Sess.run(Loss,Feed);
    print(""Loss:"",Lossvalue);
  #end if

  Sess.run(Training,Feed);
#end for

Lastloss = Sess.run(Loss,Feed);
print(""Loss:"",Lastloss,""(Last)"");

#eval
Results = Sess.run(Output,Feed);
print(""\nEval:"");
print(Results);

print(""\nDone."");
#eof

","['natural-language-processing', 'classification', 'tensorflow', 'recurrent-neural-networks', 'generative-model']","I found out how to switch it (the code) to do text generation task, use 3D input (X) and 3D labels (Y) as in the source code below:Source code:"
Algorithm to solve a fault independent of its type,"
I am looking to plan a solution for a workspace fault and not hardware faults. 
Consider a task where a robot has to move balls from one place to another. In case it faces any condition which is outside the task for eg. someone snatches the ball from the robot while it is transferring or the robot drops the balls in between. These are some example faults that could occur, many other might be possible. I am trying to build a generalized algorithm that so that the robot can find a way to resolve unexpected changes itself.
I currently have an FSM for the whole task. Any fault that somehow changes any of the state machine variables should be considered. For instance, there are faults that deal with obstacles that may come in the way. 
But there might be faults for example a cloth in front of the camera. This fault should be corrected by a human since the robot cannot manage that. All the faults like that are out of the scope of the robot. 
Any suggestion or ideas related to the algorithm will be helpful. 
","['learning-algorithms', 'robotics', 'problem-solving', 'path-planning', 'path-finding']",
Neural network does not give out the required out put?,"
Made a neural network using tensor flows that was supposed matches an Ip to one of the 7 type of vulnerabilities and gives out what type of vulnerability that IP has.

    model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(50, activation=tf.nn.relu),
  tf.keras.layers.Dense(7, activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])



model.fit(xs, ys, epochs=500)

The output of print(model.predict([181271844])) when this command is executed should be one of the numbers from 1 to 7 but the out put its gives is 

[[0.22288103 0.20282331 0.36847615 0.11339897 0.04456346 0.02391759
    0.02393949]]

I can't seem to figure out what the problem is.
","['neural-networks', 'tensorflow']","The numbers you are seeing as output are a probability vector. This is a common output format for multi-class classification models.In this case, you can interpret the vector as saying:If you want to get a concrete label out of this, the easiest choice is to compute and return the index of the maximum element. "
How can we find find the input image which maximizes the class-probability for an ANN?,"
Let's assume we have an ANN which takes a vector $x\in R^D$, representing an image, and classifies it over two classes. The output is a vector of probabilities $N(x)=(p(x\in C_1), p(x\in C_2))^T$ and we pick $C_1$ iff $p(x\in C_1) \geq 0.5$. Let the two classes be $C_1= \texttt{cat}$ and $C_2= \texttt{dog}$. Now imagine we want to extract this ANN's idea of ideal cat by finding $x^* = argmax_x N(x)_1$. How would we proceed? I was thinking about solving $\nabla_xN(x)_1=0$, but I don't know if this makes sense or if it is solvable.
In short, how do I compute the input which maximizes a class-probability?
","['neural-networks', 'generative-model']","In deep networks there is actually a wide variety of solutions to the problem, but if you need to find one, any easy way to do this is just through normal optimization schemes
$$\hat x = argmin_x \ L(y,x)$$
where $L(y,x)$ is your loss function. Since ANN's are generally differentiable you can optimize this iteratively with some form gradient descent scheme:
$$x^{i+1} \leftarrow x^{i} - \lambda \nabla_{x^i}L(y,x^i)$$
where $\lambda$ is your learning rate."
Why evolutionary training of neural networks is not popular?,"
Evolutionary algorithms are mentioned in some sources as a method to train a neural network (finding weights, not hyperparameters). However, I have not heard about one practical application of such an idea yet.
My question is, why is that? What are the issues or limitations with such a solution that prevented it from practical use?
I am asking because I am planning on developing such an algorithm and want to know what to expect and where to put most attention.
","['neural-networks', 'training', 'gradient-descent', 'evolutionary-algorithms', 'neuroevolution']","The main evolutionary algorithm used to train neural networks is Neuro-Evolution of Augmenting Topoloigies, or NEAT. NEAT has seen fairly widespread use. There are thousands of academic papers building on or using the algorithm.NEAT is not widely used in commercial applications because if you have a clean objective function, a topology that is optimized for gradient decent via backpropogation, and an implementation that is highly optimized for a GPU, you are almost certainly going to see better, faster, results from a conventional training process. Where NEAT is really useful is if you want to do something weird, like train to maximize novelty, or if you want to try to train neurons that don't have cleanly decomposable gradients. Basically, you need to have any of the usual reasons you might prefer an evolutionary algorithm to hill-climbing approaches:"
How do I determine which variables/features have the strongest relationship with each other?,"
This is my problem:
I have 10 variables that I intend to evaluate two by two (in pairs). I want to know which variables have the strongest relationships with each other. And I'm only interested in evaluating relationships two by two. Well, one suggestion would be to calculate the pairwise correlation coefficient of these variables. And then list the pairs with the highest correlation coefficient to the lowest correlation. That way I would have a ranking between the most correlated to the lowest correlated pairs.
My question is: Is there anything analogous in the world of artificial intelligence to the correlation coefficient calculation? That is, what tools can the world of AI / Machine Learning offer me to extract this kind of information? So that in the end I can have something like a ranking among the most ""correlated"" pairs from the point of view of AI / Machine Learning?
In other words, how do I know which variable among these 10 best ""relates"" (or ""correlates"") with variable 7, for example?
","['machine-learning', 'correlation']","It sounds like you have a series of data points, each with 10 related measurements, and you want automatically assess which of the measurements are most closely related to each other.You are right that the correlation coefficient is a good choice for this.Other techniques used in some AI algorithms include the Information Gain measurement (where you measure the reduction in entropy of one variable that follows from partitioning on another one), and embedded feature selection approaches, like the one in this paper."
What is the difference between random and sequential sampling from the reply memory?,"
I was working on an RL problem and I am confused at one specific point. We use replay memory so that the network learns about previous actions and how these actions lead to a success or a failure. 
Now, to train the neural network, we use batches from this replay or experience memory. But here's my confusion.
Some places like this extract random (non-sequential) batches from the memory to train the neural network but Andrej Karpathy uses the sequential data to train the network. 
Can someone tell me why there's the difference?
","['machine-learning', 'reinforcement-learning', 'dqn', 'experience-replay']",
"LSTM network doesn't converge, what should be changed? [closed]","







Closed. This question is off-topic. It is not currently accepting answers.
                                
                            











 This question does not appear to be about artificial intelligence, within the scope defined in the help center.


Closed 3 years ago.







                        Improve this question
                    



I'm testing out TensorFlow LSTM layer text generation task, not classification task; but something is wrong with my code, it doesn't converge. What changes should be done?
Source code:
import tensorflow as tf;

# t=0  t=1    t=2  t=3     
#[the, brown, fox, is,     quick]
#   0  1      2    3       4
#[the, red,   fox, jumps,  high]
#   0  5      2    6       7

#t0 x=[[the],  [the]]
#   y=[[brown],[red]]
#t1 ...
#t2
#t3
bsize = 2;
times = 4;

#data
x = [];
y = [];
#t0        the:     the:
x.append([[0/6],   [0/6]]); #normalise: x divided by 6 (max x)
#          brown:   red:
y.append([[1/7],   [5/7]]); #normalise: y divided by 7 (max y)
#t1
x.append([[1/6],   [5/6]]);
y.append([[2/7],   [2/7]]);
#t2
x.append([[2/6],   [2/6]]);
y.append([[3/7],   [6/7]]);
#t3
x.append([[3/6],   [6/6]]);
y.append([[4/7],   [7/7]]);

#model
inputs  = tf.placeholder(tf.float32,[times,bsize,1]) #4,2,1
exps    = tf.placeholder(tf.float32,[times,bsize,1]);

layer1  = tf.keras.layers.LSTMCell(20) 
hids1,_ = tf.nn.static_rnn(layer1,tf.split(inputs,times),dtype=tf.float32);

w2      = tf.Variable(tf.random_uniform([20,1],-1,1));
b2      = tf.Variable(tf.random_uniform([   1],-1,1));
outs    = tf.sigmoid(tf.matmul(hids1,w2) + b2);

loss  = tf.reduce_sum(tf.square(exps-outs))
optim = tf.train.GradientDescentOptimizer(1e-1)
train = optim.minimize(loss)

#train
s    = tf.Session();
init = tf.global_variables_initializer();
s.run(init)

feed = {inputs:x, exps:y}
for i in range(10000):
  if i%1000==0:
    lossval = s.run(loss,feed)
    print(""loss:"",lossval)
  #end if
  s.run(train,feed)
#end for

lastloss = s.run(loss,feed)
print(""loss:"",lastloss,""(last)"");
#eof

Output showing loss values (a little different every run):
loss: 3.020703
loss: 1.8259083
loss: 1.812584
loss: 1.8101325
loss: 1.8081319
loss: 1.8070083
loss: 1.8065354
loss: 1.8063282
loss: 1.8062303
loss: 1.8061805
loss: 1.8061543 (last)

Colab link:
https://colab.research.google.com/drive/1TsHjmucuynCPOgKuo4a0hiM8B8UaOWQo
","['deep-learning', 'tensorflow', 'optimization', 'long-short-term-memory', 'convergence']","writing here my suggestion, because i haven't earned the right to comment yet.Your main ""problem"" could be your loss function. It converges, this is why your loss value is decreasing. So I suggest to let it maybe train longer.Alternatively you could change the loss function to fit your need. For example you could use:You will get a smaller loss value which decreases clearly after every output.I hope this helps :)"
Why doesn't stability in prediction imply stability in control in off-policy reinforcement learning?,"
Prediction's goal is to get an estimate of a performance of a policy given a specific state. 
Control's goal is to improve the policy wrt. the prediction. 
The alternation between the two is the basis of reinforcement learning algorithms. 
In the paper â€œSafe and Efficient Off-Policy Reinforcement Learning.â€ (Munos, 2016), the section 3.1) ""Policy evaluation"" assumes that the target policy is fixed, while the section 3.2) ""Control"" extends to where the target policy is a sequence of policies improved by a sequence of increasingly greedy operations. 
This suggests that even a proof of convergence is established with a fixed target policy, one cannot immediately imply that of the case where the target policy is a sequence of improving policies.
I wonder why it is the case. If an algorithm converges under a fixed target policy assumption, any policy during the chain of improvement should have no problem with this algorithm as well. With the merit of policy improvement, each policy in sequence is increasingly better hence converging to an optimal policy.
This should be obvious from the policy improvement perspective and should require no further proof at all?
",['reinforcement-learning'],
What are the differences between stability and convergence in reinforcement learning?,"
The terms are mentioned in the paper: An Emphatic Approach to the Problem of off-Policy Temporal-Difference Learning (Sutton, Mahmood, White; 2016) and more, of course.
In this paper, they proposed the proof of ""stability"" but not convergence.
It seems that stability is guaranteed if the ""key matrix"" is shown to be positive definite. However, convergence requires more than that.
I don't understand the exact difference between the two.
","['reinforcement-learning', 'comparison', 'papers', 'convergence', 'stability']",
Why would you implement the position-wise feed-forward network of the transformer with convolution layers?,"
The Transformer model introduced in ""Attention is all you need"" by Vaswani et al. incorporates a so-called position-wise feed-forward network (FFN):

In addition to attention sub-layers, each of the layers in our encoder
  and decoder contains a fully connected feed-forward network, which is
  applied to each position separately and identically. This consists of
  two linear transformations with a ReLU activation in between.
$$\text{FFN}(x) = \max(0, x \times {W}_{1} + {b}_{1}) \times {W}_{2} + {b}_{2}$$
While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is ${d}_{\text{model}} = 512$, and the inner-layer has dimensionality ${d}_{ff} = 2048$.

I have seen at least one implementation in Keras that directly follows the convolution analogy. Here is an excerpt from attention-is-all-you-need-keras.
class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)

Yet, in Keras you can apply a single Dense layer across all time-steps using the TimeDistributed wrapper (moreover, a simple Dense layer applied to a 2D input implicitly behaves like a TimeDistributed layer). Therefore, in Keras a stack of two Dense layers (one with a ReLU and the other one without an activation) is exactly the same thing as the aforementioned position-wise FFN. So, why would you implement it using convolutions?
Update 
Adding benchmarks in response to the answer by @mshlis:
import os
import typing as t
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import numpy as np

from keras import layers, models
from keras import backend as K
from tensorflow import Tensor


# Generate random data

n = 128000  # n samples
seq_l = 32  # sequence length
emb_dim = 512  # embedding size

x = np.random.normal(0, 1, size=(n, seq_l, emb_dim)).astype(np.float32)
y = np.random.binomial(1, 0.5, size=n).astype(np.int32)


# Define constructors

def ffn_dense(hid_dim: int, input_: Tensor) -> Tensor:
    output_dim = K.int_shape(input_)[-1]
    hidden = layers.Dense(hid_dim, activation='relu')(input_)
    return layers.Dense(output_dim, activation=None)(hidden)


def ffn_cnn(hid_dim: int, input_: Tensor) -> Tensor:
    output_dim = K.int_shape(input_)[-1]
    hidden = layers.Conv1D(hid_dim, 1, activation='relu')(input_)
    return layers.Conv1D(output_dim, 1, activation=None)(hidden)


def build_model(ffn_implementation: t.Callable[[int, Tensor], Tensor], 
                ffn_hid_dim: int, 
                input_shape: t.Tuple[int, int]) -> models.Model:
    input_ = layers.Input(shape=(seq_l, emb_dim))
    ffn = ffn_implementation(ffn_hid_dim, input_)
    flattened = layers.Flatten()(ffn)
    output = layers.Dense(1, activation='sigmoid')(flattened)
    model = models.Model(inputs=input_, outputs=output)
    model.compile(optimizer='Adam', loss='binary_crossentropy')
    return model


# Build the models

ffn_hid_dim = emb_dim * 4  # this rule is taken from the original paper
bath_size = 512  # the batchsize was selected to maximise GPU load, i.e. reduce PCI IO overhead

model_dense = build_model(ffn_dense, ffn_hid_dim, (seq_l, emb_dim))
model_cnn = build_model(ffn_cnn, ffn_hid_dim, (seq_l, emb_dim))


# Pre-heat the GPU and let TF apply memory stream optimisations

model_dense.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)
%timeit model_dense.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)

model_cnn.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)
%timeit model_cnn.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)

I am getting 14.8 seconds per epoch with the Dense implementation:
Epoch 1/1
128000/128000 [==============================] - 15s 116us/step - loss: 0.6332
Epoch 1/1
128000/128000 [==============================] - 15s 115us/step - loss: 0.5327
Epoch 1/1
128000/128000 [==============================] - 15s 117us/step - loss: 0.3828
Epoch 1/1
128000/128000 [==============================] - 14s 113us/step - loss: 0.2543
Epoch 1/1
128000/128000 [==============================] - 15s 116us/step - loss: 0.1908
Epoch 1/1
128000/128000 [==============================] - 15s 116us/step - loss: 0.1533
Epoch 1/1
128000/128000 [==============================] - 15s 117us/step - loss: 0.1475
Epoch 1/1
128000/128000 [==============================] - 15s 117us/step - loss: 0.1406

14.8 s Â± 170 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

and 18.2 seconds for the CNN implementation. I am running this test on a standard Nvidia RTX 2080.
So, from a performance perspective there seems to be no point in actually implementing an FFN block as a CNN in Keras. Considering that the maths are the same, the choice boils down to pure aesthetics. 
","['deep-learning', 'keras', 'convolution', 'transformer', 'feedforward-neural-networks']",
Which movies have the most realistic artificial intelligence?,"
I want to give some examples of AI via movies to my students. There are many movies that include AI, whether being the main character or extras.
Which movies have the most realistic (the most possible or at least close to being made in this era) artificial intelligence?
","['applications', 'social', 'academia']","Just A Rather Very Intelligent System (J.A.R.V.I.S.) in Iron Man (and related films, such as The Avengers) is something (a personal assistant) that people are already trying to develop, so JARVIS is a quite realistic artificial intelligence. Examples of existing personal assistants are Google Assistant (integrated into Google Home devices), Cortana, Siri and Alexa. There are other virtual assistants, but, unfortunately, there aren't many reliable open-source ones. Note that JARVIS is way more intelligent and capable than the other mentioned personal assistants.Similarly, HAL 9000, in 2001: A Space Odyssey, is a sentient artificial intelligence which can be considered a personal assistant."
"How does the network know which objects to track in the paper ""Label-Free Supervision of Neural Networks with Physics and Domain Knowledge""?","
I was reading the paper Label-Free Supervision of Neural Networks with Physics and Domain Knowledge, published at AAAI 2017, which won the best paper award.
I understand the math and it makes sense. Consider the first application shown in the paper of tracking falling objects. They train only on multiple trajectories of the said pillow, and during the evaluation, they claim that they can track any other falling object (which may not be pillows).
I am unable to understand how that happens? How does the network know which object to track? Even during the training, how does it know that it's the pillow that it's supposed to track?
The network is trained to fit a parabola. But any parabola could fit it. There are infinite such parabolas.
","['neural-networks', 'machine-learning', 'unsupervised-learning', 'papers']",
"In deep learning, do we learn a continuous distribution based on the training dataset?","
At least at some level, maybe not end-to-end always, but deep learning always learns a function, essentially a mapping from a domain to a range. The domain and range, at least in most cases, would be multi-variate. 
So, when a model learns a mapping, considering every point in the domain-space has a mapping, does it try to learn a continuous distribution based on the training-set and its corresponding mappings, and map unseen examples from this learned distribution? Could this be said about all predictive algorithms? 
If yes, then could binary classification be compared to having a hyper-plane (as in support vector classification) in a particular kernel-space, and could the idea of classification problems using hyper-planes be extended in general to any deep learning problem learning a mapping? 
It would also explain why deep learning needs a lot of data and why it works better than other learning algorithms for simple problems.
","['neural-networks', 'deep-learning', 'probability-distribution', 'computational-learning-theory']","Well, there are some questions here...Does it (Deep Learning) try to learn a continuous distribution based
on the training-set and its corresponding mappings, and map unseen
examples from this learned distribution?Yes. Talking about Deep Artificial Neural Networks, they try to learn continuous distribution using continuous activation functions in each neuron. Therefore, the output is also a continuous function to represent a continuous probability distribution. The issue with the unseen examples is the need for similar examples in the training set; otherwise, the weights and bias of the network will not be tuned in the regions of space around the unseen example. Imagine a Neural Network learning a function y = x, if we only present values between 0 and 10 during training, we should expect it to only make good predictions for y for values of x ranging from 0 to 10. It doesn't mean that it won't predict for other values, but the predictions will not be so accurate or nowhere close to the expectations. That is because the network is not trying to guess what was the function used to generate y, but it is simply trying to adjust its parameters to make its internal functions generate the expected y for the given x. That is why Deep Neural Networks require a lot of data. In a unidimensional space is easier to provide examples that cover the subset of the domain we want our network to learn. When we use multidimensional space, we need a lot more examples to have a good representation of the hyperspace used as domain.Could this (map unseen examples) be said about all predictive algorithms?Yes, it should. Otherwise, the algorithm would not be able to generalize well. A good predictive algorithm is the one that can predict unseen examples using fewer training samples.Could the idea of classification problems using hyper-planes be
extended in general to any Deep Learning problem learning a mapping?In the case of Deep Neural Networks, the result is more like, for a given input value, return the probability of it belonging to a class. For binary classification, the network will have a single output. The sigmoid function modulates this output to ranges between 0 and 1. We can interpret the output as the probability of belonging to one out of two possible classes. To know the probability for the other class, we subtract it from 1. For three or more classes, we will need three or more outputs ranging from 0 to 1, and each output is the probability of belonging to one of the classes. In this case, the outputs are also normalized by a softmax function, that guarantees that the sum of all outputs is equal to 1, as a probability distribution.Would also explain why Deep Learning needs a lot of data and why it
works better mostly than other Learning algorithms for simple
problems.Already partially explained... The need for a lot of data is to have a good representation of the hyperplane used as the domain.
The Deep Neural Networks work well because of their power to represent different models. They are a very 'flexible' functions that can be bent to approximate the relation existent between the data in the training set and the expected target. Simpler algorithms, as linear models, for instance, have less representation power, they are limited to a smaller set of models. Even though many models can be linearly approximated (because the input and output almost follow a linear relation), the neural network will be able to learn the nuances of the dataset better. This can also be the curse of neural networks, because they may try to learn every detail of the training set that wasn't really relevant and true for other cases and this concept is called overtraining... but is a discussion for another topic."
"When training a DQN, how should we update the value of actions that were not taken?","
Let's say that we have three actions. The highest-valued action of the three choices is the first. When training the DQN, what do we do with the other two, as we don't have a target for them, since they weren't taken?
I've seen some code that leaves the target for off actions as whatever the prediction returned, which feels a bit wrong to me as two or more similar behaving actions might never be differentiated well after random action selection dwindles.
I've also seen some implementations that set the target for all actions to zero and only adjust the target for the action taken. This would help regarding action differentiation long term, but it also puts more reliance on taking random actions for any unfamiliar states (I believe) as an off action might never be taken otherwise.
","['reinforcement-learning', 'training', 'q-learning', 'dqn', 'deep-rl']","The loss function for DQN algorithm is
\begin{equation}
L(\theta_i) = \mathbb E_{s, a, r, s'} [(y - Q(s, a;\theta_i))^2]
\end{equation}
Like you said, we only take one action per timestep. We can only shift weights of the network that had the effect in calculating action value $Q(s, a)$ for that particular action that we took. For that action, variable $y$ would have value
\begin{equation}
y = r + \gamma \max_{a'} Q(s', a', \theta^-_i)
\end{equation}
and we would have standard form of DQN loss. We calculate the gradient of that loss with respect to network parameters $\theta$, backprop it and slightly shift weights so to more accurately estimate $Q(s, a)$ for that specific state-action pair.We never took other actions. Since we never took them we didn't get reward $r$ and we can't estimate $\max_{a'} Q(s', a', \theta^-_i)$ for those other actions. We don't want to change weights that had effect on calculating actions values for those other actions because we have no way of estimating how accurate they were. Standard approach is to set
\begin{equation}
y = Q(s, a;\theta_i)
\end{equation}
this way the loss for those other actions would be $0$ and it would result in not changing the weights that had influence in calculating their action values.  Setting target $y$ to $0$ for all other actions would mean that we want $Q(s, a)$ for them all to slightly shift to $0$. That would not be correct since we have no way of knowing their true value. I think you misinterpreted that part in the implementations."
What is the best way to find the similarities between two text documents?,"
I would like to develop a platform in which people will write text and upload images. I am going to use Google API to classify the text and extract from the image all kinds of metadata. In the end, I am going to have a lot of text which describes the content (text and images). Later, I would like to show my users related posts (that is, similar posts, from the content point of view).
What is the most ppropriate way of doing this? I am not an AI expert and the best approach from my prescriptive it to have some tools, like google API or Apache Lucene search engine, which can hide the details of how this is done.
","['natural-language-processing', 'classification', 'recommender-system']",
How to map X to Y for TensorFlow RNN training data,"
Usually for DNN, I have the training data of matching X (2D) to Y (2D), for example, XOR data:
X = [[0,0],[0,1],[1,0],[1,1]];
Y = [[0],  [1],  [1],  [0]  ];

However, RNN seems strange, I don't get it how to match X to Y, input of RNN layer is 3D and output is 2D (rightclick to open in new tab): https://colab.research.google.com/drive/17IgFuxOYgN5fNO9LKwDijEBkIeWNPas6
import tensorflow as tf;

x = [[[1],[2],[3]], [[4],[5],[6]]];
bsize = 2;
times = 3;

#3d input
input = tf.placeholder(tf.float32, [bsize,times,1]);

cell  = tf.keras.layers.LSTMCell(20);
rnn   = tf.keras.layers.RNN(cell);
hid   = rnn(input);

sess = tf.Session();
init = tf.global_variables_initializer();
sess.run(init);

#results in 2d
print(sess.run(hid, {input:x}));

The example data seen on https://www.tensorflow.org/tutorials/sequences/recurrent are:
 t=0  t=1    t=2  t=3     t=4
[the, brown, fox, is,     quick]
[the, red,   fox, jumped, high]

How to map these data from X (3D input for RNN layer) to Y (2D)? (Y is 2D because RNN layer output is 2D).
","['deep-learning', 'tensorflow', 'recurrent-neural-networks', 'long-short-term-memory', 'deep-neural-networks']",I found out how to get 3D output from LSTMCell so that I can matmul with output weights + biases and subtract with expected values:Source code:
What is the State-of-the-Art open source Voice Cloning tool right now?,"
I would like to clone a voice as precisely as possible. Lately, impressive models have been released that only need about 10 s of voice input (cf. https://github.com/CorentinJ/Real-Time-Voice-Cloning), but I would like to go beyond that and clone a voice even more precisely (with subsequent text-to-speech using that voice). It doesn't matter if I have to provide minutes or hours of voice inputs.
","['natural-language-processing', 'speech-synthesis']",
Is AGI likely to be developed in the next decade?,"
AI experts like Ben Goertzel and Ray Kurzweil say that AGI   will be developed in the coming decade. Are they credible?
","['agi', 'futurism']","As a riff on my answer to this question, which is about the broader concern of the development of the singularity, rather than the narrower concern of the development of AGI:I can say that among AI researchers I interact with, it far more common to view the development of AGI in the next decade as speculation (or even wild speculation) than as settled fact.This is borne out by surveys of AI researchers, with 80% thinking ""The earliest that machines will be able to simulate learning and every other
aspect of human intelligence"" is in ""more than 50 years"" or ""never"", and just a few percent thinking that such forms of AI are ""near"". It's possible to quibble over what exactly is meant by AGI, but it seems likely that for us to reach AGI, we'd need to simulate human-level intelligence in at least most of its aspects. The fact that AI researchers think this is very far off suggests that they also think AGI is not right around the corner.I suspect that the reasons AI researchers are less optimistic about AGI than Kurzweil or others in tech (but not in AI), are rooted in the fact that we still don't have a good understanding of what human intelligence is. It's difficult to simulate something that we can't pin down. Another factor is that most AI researchers have been working in AI for a long time. There are countless past proposals for AGI frameworks, and all of them have been not just wrong, but in the end, more or less hopelessly wrong. I think this creates an innate skepticism of AGI, which may perhaps be unfair. Nonetheless, expert opinion on this one is pretty well settled: no AGI this decade, and maybe not ever!"
Doubt on formulating cost function for GloVe,"
I'm reading the notes here and have a doubt on page 2 (""Least squares objective"" section). The probability of a word $j$ occurring in the context of word $i$ is $$Q_{ij}=\frac{\exp(u_j^Tv_i)}{\sum_{w=1}^W\exp(u_w^Tv_i)}$$
The notes read:

Training proceeds in an on-line, stochastic fashion, but the implied global cross-entropy loss can be calculated as $$J=-\sum_{i\in corpus}\sum_{j\in context(i)}\log Q_{ij}$$
  As the same words $i$ and $j$ can occur multiple times in the corpus, it is more efficient to first group together the same values for $i$ and $j$:
  $$J=-\sum_{i=1}^W\sum_{j=1}^WX_{ij}\log(Q_{ij})$$

where $X_{ij}$ is the total number of times $j$ occurs in the context of $i$ and the value of co-occuring frequency is given by the co-occurence matrix $X$. This much is clear. But then the author states that the denominator of $Q_{ij}$ is too expensive to compute, so the cross entropy loss won't work.

Instead, we use a least square objective in which the normalization factors in $P$ and $Q$ are discarded:
  $$\hat J=\sum_{i=1}^W\sum_{j=1}^WX_i(\hat P_{ij}-\hat Q_{ij})^2$$
  where $\hat P_{ij}=X_{ij}$ and $\hat Q_{ij}=\exp(u_j^Tv_i)$ are the unnormalized distributions.

$X_i=\sum_kX_{ik}$ is the number of times any word appears in the context of $i$. I don't understand this part. Why have we introduced $X_i$ out of nowhere? How is $\hat P_{ij}$ ""unnormalized""? Is there a tradeoff in switching from softmax to MSE? 
(As far as I know, softmax made total sense in skip gram because we were calculating scores corresponding to different words (discrete possibilities) and matching the predicted output to the actual word - similar to a classification problem, so softmax makes sense.)
","['natural-language-processing', 'word-embedding', 'glove']",
Training Keras Towards Or Against Analog Value?,"
For example, if I want to do a cat and mouse AI, the cat would wish to minimize the time taken for it to catch the mouse and the mouse would want to maximize that time. The time is analog and thus I cannot use a traditional Xy method but need another method that goes like this:
network.train_against_value(X, y, determinator)
Here, X is more like where the cat and mouse are. y is where the cat or mouse should move, and determinator is the time taken for the mouse to be caught, where the mouse wishes to maximize this value through its output of y and the cat wishes to minimize it. There is one Xy pair for each decision made by the cat and mouse, but one determinator throughout one game. Many games are played to train the AI.
Example: X: (300, 300, 200, 200) -> (mousex, mousey, catx, caty)
Y: (1,3) -> (xmove, ymove) direction, the numbers are then tuned by code for the actual movement to be always 1.
Determinator: 50 -> time for mouse to be caught in seconds
Where it would train so that with every X inputted it outputs a y so that determinator is minimum. Is there a method for train_towards_value as well? If there is no prebuilt method, how do I create one? What is the technical name for this kind of training?
I have two neural networks for the cat and mouse, where the cat is slower than the mouse but is larger and could eat the mouse. Just consider the mouse is difficult to control from the neural network because of inefficiencies so that it is possible for the cat to catch the mouse. 
",['keras'],"What is the technical name for this kind of training?The name for the problem is Sequential Decision Making or Optimal Control.There are a few different approaches you can take when solving this kind of problem. However, I think that the way that you are describing your project, Reinforcement Learning (RL) would match your approach the best.I cannot use a traditional Xy method but need another method that goes like this:network.train_against_value(X, y, determinator)Although this method signature could probably be made to work by re-structuring one or other RL frameworks, it is more usual when using RL with neural networks to treat the experience-gathering and scoring systems - the core ideas behind RL - as a data generator for either a supervised learning problem or in some cases directly producing the output layer gradients.One approach you could use is called Deep Q Networks (DQN) which effectively generates mini-batches for supervised learning of a neural network, based on gathering experiences.In brief, DQN trains a neural network to predict what you are calling the ""determinator"", but that RL would call the ""value"" of each action. So you may move the action choice (y) to the input of the neural network, or alternatively predict multiple values (one output for each possble action) - the RL theory is the same for each approach here, it is an implementation detail. The agent (cat or mouse) would then pick the action with the highest predicted value by default (with the cat's values being the negative of the mouse's), trying other actions at random whilst training so as to learn all values. Whenever the agent gained new experience, it would add that to the training data so it could improve its predictions.RL is a complex subject in its own right, and to begin understanding it properly you would do well to start by tackling even simpler problems that don't require neural networks or running two agents against each other. There is a good introductory book on the subject with the option to read a free PDF version: Reinforcement Learning: An Introduction (Sutton & Barto)"
"Get the position of an object, out of an image","
I have some images with a fixed background and a single object on them which is placed, in each image, at a different position on that background. I want to find a way to extract, in an unsupervised way, the positions of that object. For example, us, as humans, would record the x and y location of the object. Of course the NN doesn't have a notion of x and y, but i would like, given an image, the NN to produce 2 numbers, that preserve as much as possible from the actual relative position of objects on the background. For example, if 3 objects are equally spaced on a straight line (in 3 of the images), I would like the 2 numbers produced by the NN for each of the 3 images to preserve this ordering, even if they won't form a straight line. They can form a weird curve, but as long as the order is correct that can be topologically transformed to the right, straight line. Can someone suggest me any paper/architecture that did something similar? Thank you! 
","['neural-networks', 'convolutional-neural-networks', 'image-recognition', 'architecture']",
How could I compute in real-time the similarity between tickets?,"
I'm dealing with a ""ticket similarity task"".
Every time new tickets arrive at the help desk (customer service), I need to compare them and find out about similar ones.
In this way, once the operator responds to a ticket, at the same time he can solve the others similar to the one solved.
I expect an input ticket and all the other tickets with their similarity in output.
I thought about using DOC2VEC, but it requires training every time a new ticket enters.
What do you recommend?
","['machine-learning', 'word-embedding', 'similarity']",
Neural Network training on one example to try overfitting leads to strange predictions,"
tldr; if I train the network on 1 training example, the outcome sometimes makes no sense at all, sometimes is as expected. If I train it on more examples and higher iterations, the network, which produces two outcomes (p and v) always predicts exactly 0 for v and I would like to change that.
In the following post I will provide all code necessary to reproduce the problem.
I am training a neural network on the same input. The wanted outcome for a value ""v"" is 1. If I create the network and train it, sometimes the predicted outcome will be 1, sometimes it will be -1.
 Also, the loss seems to flip between 0 and 4 during training epochs.
 Additionally, the loss blows up immensly, even though both losses for the outcome layers are close to zero.
 I do not understand where this behaviour comes from. I used Leaky-ReLU to make sure it can handle negative input, I used a high learning rate to make sure the data in this example is sufficient on the training, and the input is the same all the time.
My Neural network looks like this:
input_layer = keras.Input(shape=(6,7),)    
formatted_input_layer = keras.layers.Reshape((6,7, 1))(input_layer)       
conv_layer1 = self.create_conv_layer(formatted_input_layer)
res_layer1 = self.create_res_layer(conv_layer1)
res_layer2 = self.create_res_layer(res_layer1)
res_layer3 = self.create_res_layer(res_layer2)
res_layer4 = self.create_res_layer(res_layer3)
policy_head = self.create_policy_head(res_layer4)
value_head = self.create_value_head(res_layer4)
model = keras.Model(inputs=input_layer, outputs=[policy_head, value_head])
optimizer = keras.optimizers.SGD(lr=args['lr'],momentum=args['momentum'])
model.compile(loss = {'policy_head' : 'categorical_crossentropy', 'value_head' : 'mean_squared_error'}, optimizer=optimizer, loss_weights={'policy_head':0.5, 'value_head':0.5})

Methods for the different layers:
conv_layer:
def create_conv_layer(self, input_layer):
    conv_layer = keras.layers.Conv2D(filters=256,
                                     kernel_size=3,
                                     strides=1,
                                     padding='same',
                                     use_bias=False,
                                     data_format=""channels_last"",
                                     activation = ""linear"",
                                     kernel_regularizer = keras.regularizers.l2(0.0001))(input_layer)
    conv_layer= keras.layers.BatchNormalization(axis=-1)(conv_layer)
    conv_layer = keras.layers.LeakyReLU()(conv_layer)
    return conv_layer

res_layer:  
def create_res_layer(self, input_layer):
        conv_layer = self.create_conv_layer(input_layer)
        res_layer = keras.layers.Conv2D(filters=256,
                                         kernel_size=3,
                                         strides=1,
                                         padding='same',
                                         use_bias=False,
                                         data_format=""channels_last"",
                                         activation = ""linear"",
                                         kernel_regularizer = keras.regularizers.l2(0.0001))(conv_layer)
        res_layer= keras.layers.BatchNormalization(axis=-1)(res_layer)
        res_layer = keras.layers.add([input_layer, res_layer])
        res_layer = keras.layers.LeakyReLU()(res_layer)
        return res_layer

policy head:  
def create_policy_head(self, input_layer):
        policy_head = keras.layers.Conv2D(filters=2,
                                          kernel_size=1,
                                          strides=1,
                                          padding='same',
                                          use_bias = False,
                                          data_format='channels_last',
                                          activation='linear',
                                          kernel_regularizer = keras.regularizers.l2(0.0001))(input_layer)
        policy_head = keras.layers.BatchNormalization(axis=-1)(policy_head)
        policy_head = keras.layers.LeakyReLU()(policy_head)
        policy_head = keras.layers.Flatten()(policy_head)
        policy_head = keras.layers.Dense(units = 7,
                                         use_bias = False,
                                         activation = 'softmax',
                                         kernel_regularizer = keras.regularizers.l2(0.0001),
                                         name = ""policy_head""
                                         )(policy_head)
        return policy_head

value head:  
def create_value_head(self, input_layer):
        value_head = keras.layers.Conv2D(filters=1,
                                          kernel_size=1,
                                          strides=1,
                                          padding='same',
                                          use_bias = False,
                                          data_format='channels_last',
                                          activation='linear',
                                          kernel_regularizer = keras.regularizers.l2(0.0001))(input_layer)
        value_head = keras.layers.BatchNormalization(axis=-1)(value_head)
        value_head = keras.layers.LeakyReLU()(value_head)  
        value_head = keras.layers.Flatten()(value_head)        
        value_head = keras.layers.Dense(units = 21,
                                         use_bias = False,
                                         activation = 'linear',
                                         kernel_regularizer = keras.regularizers.l2(0.0001)
                                         )(value_head)
        value_head = keras.layers.LeakyReLU()(value_head)      
        value_head = keras.layers.Dense(units = 1,
                                         use_bias = False,
                                         activation = 'tanh',
                                         kernel_regularizer = keras.regularizers.l2(0.0001),
                                         name = ""value_head""                                     
                                         )(value_head)
        return value_head

                                )(value_head)

The way I am testing my NN:
    canonicalBoard = np.zeros(shape = (6,7), dtype=int) 


    Pi = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] 

    trainExamples = [[canonicalBoard, Pi, 1]]*50        

    nnetwrapper.train(trainExamples)

    board = canonicalBoard[np.newaxis, :, :]

    p, v = nnetwrapper.nnet.model.predict(board)

which results in the training looking like this:
Epoch 1/10
50/50 [==============================] - 4s 71ms/step - loss: 1.6829 - policy_head_loss: 1.9459 - value_head_loss: 1.0000
Epoch 2/10
50/50 [==============================] - 1s 15ms/step - loss: 2.3218 - policy_head_loss: 3.8470 - value_head_loss: 0.3768
Epoch 3/10
50/50 [==============================] - 1s 13ms/step - loss: 4456112.5000 - policy_head_loss: 0.9027 - value_head_loss: 0.8510
Epoch 4/10
50/50 [==============================] - 1s 14ms/step - loss: 16085884.0000 - policy_head_loss: 0.0945 - value_head_loss: 3.9925
Epoch 5/10
50/50 [==============================] - 1s 14ms/step - loss: 32722448.0000 - policy_head_loss: 2.6572 - value_head_loss: 4.0000
Epoch 6/10
50/50 [==============================] - 1s 14ms/step - loss: 52690084.0000 - policy_head_loss: 9.6345 - value_head_loss: 3.1810e-12
Epoch 7/10
50/50 [==============================] - 1s 14ms/step - loss: 74703120.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 4.0000
Epoch 8/10
50/50 [==============================] - 1s 14ms/step - loss: 97784832.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 4.0000
Epoch 9/10
50/50 [==============================] - 1s 14ms/step - loss: 121202520.0000 - policy_head_loss: 2.0802e-05 - value_head_loss: 4.0000
Epoch 10/10
50/50 [==============================] - 1s 14ms/step - loss: 144415040.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 4.0000

and my prediction looking like this:
p: [[0. 0. 0. 0. 0. 1. 0.]]  v: [[-1.]]

another outcome could be:
Epoch 1/10
50/50 [==============================] - 4s 82ms/step - loss: 1.6829 - policy_head_loss: 1.9459 - value_head_loss: 1.0000
Epoch 2/10
50/50 [==============================] - 1s 17ms/step - loss: 2.2826 - policy_head_loss: 2.0001 - value_head_loss: 2.1454
Epoch 3/10
50/50 [==============================] - 1s 16ms/step - loss: 1718694.1250 - policy_head_loss: 0.5434 - value_head_loss: 3.9772
Epoch 4/10
50/50 [==============================] - 1s 14ms/step - loss: 6204218.0000 - policy_head_loss: 9.4180e-05 - value_head_loss: 0.0000e+00
Epoch 5/10
50/50 [==============================] - 1s 14ms/step - loss: 12620835.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 0.0000e+00
Epoch 6/10
50/50 [==============================] - 1s 14ms/step - loss: 20322232.0000 - policy_head_loss: 7.7489 - value_head_loss: 0.0000e+00
Epoch 7/10
50/50 [==============================] - 1s 14ms/step - loss: 28812526.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 1.5966
Epoch 8/10
50/50 [==============================] - 1s 14ms/step - loss: 37715012.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 0.0000e+00
Epoch 9/10
50/50 [==============================] - 1s 15ms/step - loss: 46747064.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 0.0000e+00
Epoch 10/10
50/50 [==============================] - 1s 14ms/step - loss: 55699992.0000 - policy_head_loss: 1.1921e-07 - value_head_loss: 0.0000e+00

with the predictions:
p: [[0. 0. 0. 0. 0. 1. 0.]]  v: [[1.]]

which are the correct ones as I would have expected.
How come on some trainings, my NN doesnt fit the data at all?   I wanted to start the training process for a whole week now, but before I do so I want to make sure there are no errors in the way I layed out my NN.   And this looks like I am missing something here.
And here at the predict / train methods of my neuralnet, (the NN is part of an alpha-zero replica and the involved game is connect4, I omitted these in the example to make it easier to actually replicate the problem. This is why you see some transform operations in predict and train methods)
def train(self, examples):
        input_boards, target_pis, target_vs = list(zip(*examples))       
        input_boards = np.asarray(input_boards)
        target_pis = np.asarray(target_pis)
        target_vs = np.asarray(target_vs)        
        logger.debug(""Passing to nn: x: {}, y: {}, batch_size: {}, epochs: {}"".format(input_boards, [target_pis, target_vs], self.args[""batch_size""], self.args[""epochs""]))
        self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = self.args[""batch_size""], epochs = self.args[""epochs""])


def predict(self, board):
        board = board.nn_board_2d
        # preparing input
        board = board[np.newaxis, :, :] # this has to be done for the conv2d to work

        # run
        pi, v = self.nnet.model.predict(board)
        return pi[0], v[0]

The parameters I used for this example:
'lr': 0.2
'dropout': 0.1
'epochs': 10
'num_channels': 512,
'filters': 256
'momentum':0.9

EDIT: As soon as I use a lower learning rate and more iterations, my p changes, but my v stays exactly at 0. This is what was bothering me in the first place:
Epoch 1/10
118/118 [==============================] - 5s 44ms/step - loss: 1.8037 - policy_head_loss: 2.4305 - value_head_loss: 0.7578
Epoch 2/10
118/118 [==============================] - 2s 14ms/step - loss: 1.1666 - policy_head_loss: 1.7723 - value_head_loss: 0.1416
Epoch 3/10
118/118 [==============================] - 2s 13ms/step - loss: 1.0987 - policy_head_loss: 1.6832 - value_head_loss: 0.0948
Epoch 4/10
118/118 [==============================] - 2s 13ms/step - loss: 1.0430 - policy_head_loss: 1.5787 - value_head_loss: 0.0876
Epoch 5/10
118/118 [==============================] - 2s 13ms/step - loss: 0.9943 - policy_head_loss: 1.4859 - value_head_loss: 0.0826
Epoch 6/10
118/118 [==============================] - 2s 13ms/step - loss: 0.9469 - policy_head_loss: 1.3959 - value_head_loss: 0.0777
Epoch 7/10
118/118 [==============================] - 2s 13ms/step - loss: 0.9046 - policy_head_loss: 1.3180 - value_head_loss: 0.0707
Epoch 8/10
118/118 [==============================] - 2s 13ms/step - loss: 0.8629 - policy_head_loss: 1.2403 - value_head_loss: 0.0647
Epoch 9/10
118/118 [==============================] - 2s 14ms/step - loss: 0.8068 - policy_head_loss: 1.1344 - value_head_loss: 0.0583
Epoch 10/10
118/118 [==============================] - 2s 13ms/step - loss: 0.7335 - policy_head_loss: 0.9922 - value_head_loss: 0.0538
I0916 14:24:38.595780  7116 trainingonly1NN.py:232] Values for empty board: new network: P : [0.20057818 0.11068489 0.15129891 0.20042823 0.13117987 0.04705378
 0.15877616]  v: 0

","['neural-networks', 'deep-learning', 'architecture', 'overfitting']",
Why does estimation error increase with $|H|$ and decrease with $m$ in PAC learning?,"
Why does estimation error increase with $|H|$ and decrease with $m$ in PAC learning?
I came across this statement in the section 5.2 of the book ""understanding machine learning: from theory to algorithms"". You just search ""increases (logarithmically)"" in your browser and then you can find the sentence.
I just can't understand the statement. And there is no proof in the book either. What I would like to do is prove that estimation error $\epsilon_{est}$",,
Metrics of quality of parameter space exploration,"
Considering a black box optimization problem on non-linear, non-convex function where we want to minimize an objective function.
One way to assess the quality of an optimizer is to look at the best solution it finds. However that doesn't give us any information on how much of the parameter space the optimizer had to explore to come up with these solutions. 
Therefore I was wondering if there are metrics quantifying how much of the parameter space is explored ?
","['algorithm', 'genetic-algorithms', 'optimization']",
How could artificial intelligence harm us?,"
We often hear that artificial intelligence may harm or even kill humans, so it might prove dangerous.
How could artificial intelligence harm us?
","['philosophy', 'social', 'neo-luddism']","There are many valid reasons why people might fear (or better be concerned about) AI, not all involve robots and apocalyptic scenarios. To better illustrate these concerns, I'll try to split them into three categories.This is the type of AI that your question is referring to. A super-intelligent conscious AI that will destroy/enslave humanity. This is mostly brought to us by science-fiction. Some notable Hollywood examples are ""The terminator"", ""The Matrix"", ""Age of Ultron"". The most influential novels were written by Isaac Asimov and are referred to as the ""Robot series"" (which includes ""I, robot"", which was also adapted as a movie).The basic premise under most of these works are that AI will evolve to a point where it becomes conscious and will surpass humans in intelligence. While Hollywood movies mainly focus on the robots and the battle between them and humans, not enough emphasis is given to the actual AI (i.e. the ""brain"" controlling them). As a side note, because of the narrative, this AI is usually portrayed as supercomputer controlling everything (so that the protagonists have a specific target). Not enough exploration has been made on ""ambiguous intelligence"" (which I think is more realistic).In the real world, AI is focused on solving specific tasks! An AI agent that is capable of solving problems from different domains (e.g. understanding speech and processing images and driving and ... - like humans are) is referred to as General Artificial Intelligence and is required for AI being able to ""think"" and become conscious.  Realistically, we are a loooooooong way from General Artificial Intelligence! That being said there is no evidence on why this can't be achieved in the future. So currently, even if we are still in the infancy of AI, we have no reason to believe that AI won't evolve to a point where it is more intelligent than humans.Even though an AI conquering the world is a long way from happening there are several reasons to be concerned with AI today, that don't involve robots!
The second category I want to focus a bit more on is several malicious uses of today's AI. I'll focus only on AI applications that are available today. Some examples of AI that can be used for malicious intent:DeepFake: a technique for imposing someones face on an image a video of another person. This has gained popularity recently with celebrity porn and can be used to generate fake news and hoaxes. Sources: 1, 2, 3With the use of mass surveillance systems and facial recognition software capable of recognizing millions of faces per second, AI can be used for mass surveillance. Even though when we think of mass surveillance we think of China, many western cities like London, Atlanta and Berlin are among the most-surveilled cities in the world. China has taken things a step further by adopting the social credit system, an evaluation system for civilians which seems to be taken straight out of the pages of George Orwell's 1984.Influencing people through social media. Aside from recognizing user's tastes with the goal of targeted marketing and add placements (a common practice by many internet companies), AI can be used malisciously to influence people's voting (among other things). Sources: 1, 2, 3. Hacking.Military applications, e.g. drone attacks, missile targeting systems.This category is pretty subjective, but the development of AI might carry some adverse side-effects. The distinction between this category and the previous is that these effects, while harmful, aren't done intentionally; rather they occur with the development of AI. Some examples are:Jobs becoming redundant. As AI becomes better, many jobs will be replaced by AI. Unfortunately there are not many things that can be done about this, as most technological developments have this side-effect (e.g. agricultural machinery caused many farmers to lose their jobs, automation replaced many factory workers, computers did the same). Reinforcing the bias in our data. This is a very interesting category, as AI (and especially Neural Networks) are only as good as the data they are trained on and have a tendency of perpetuating and even enhancing different forms of social biases, already existing in the data. There are many examples of networks exhibiting racist and sexist behavior. Sources: 1, 2, 3, 4."
How do policy gradients compute an infinite probability distribution from a neural network,"
Do neural networks compute the probability distribution for policy gradient methods. If so, how do they compute an infinite probability distribution? How do you represent a continuous action policy with a neural network?
","['neural-networks', 'reinforcement-learning', 'policy-gradients']",
How do we determine whether a heuristic function is better than another?,"
I am trying to solve a maze puzzle using the A* algorithm. I am trying to analyze the algorithm based on different applicable heuristic functions.
Currently, I explored the Manhattan and Euclidean distances. Which other heuristic functions are available? How do we compare them? How do we know whether a heuristic function is better than another?
","['comparison', 'search', 'a-star', 'admissible-heuristic', 'heuristic-functions']",
Threshold selection for Siamese network hyper-parameter tuning,"
I'm interested in modeling a Siamese network for facial verification. I've already written a simple working model that inputs feature vectors generated from two CNNs with shared weights then outputs a similarity score (euclidean distance.)
Here is a similar model found within the Keras documentation (this Siamese network joins two networks comprised of fully connected layers) The model also uses a euclidean distance metric.
The threshold used in that example when computing the accuracy of the model is 0.5. The similarity scores generated by the model run from that training script roughly ranges from 0 to 1.68. My model outputs scores ranging from 0 to 1.96.
I would suppose that the choice of threshold when working with a similarity metric could be determined by finding the threshold value that maximizes an appropriate metric (e.g. F1 score) on a test set.
Now when it comes to parameter tuning using a validation set - done so to choose the appropriate optimization and regularization parameters and model architecture to generate scores in the first place, how do I determine what value to set as the threshold? This threshold would be important to calculating the performance metric of each model generated during the parameter search - needed to determine what final model architecture and parameter set to use when training. Also, would the method for choosing the threshold change should I choose a different distance metric (e.g. cosine similarity)? 
I've already done a parameter search using an arbitrarily-set threshold of 0.5. I'm unsure if this would reflect best practices or if it ought to be adjusted when using a different distance metric.
Thank you for any help offered. Please let me know if any more details on my part are necessary to facilitate a better discussion in this thread.
","['machine-learning', 'convolutional-neural-networks', 'facial-recognition', 'hyperparameter-optimization', 'similarity']",
Clustering of very high dimensional data and large number of examples without losing info in dimensions,"
I'm trying to get a grasp on scalability of clustering algorithms, and have a toy example in mind. Let's say I have around a million or so songs from $50$ genres. Each song has characteristics - some of which are common across all or most genres, some of which are common to only a few genres and some of which are genre-specific.
Common attributes could be something like song duration, artist, label, year, album, key, etc. Genre-specific attributes could be like lead guitarist, trombone player, conductor, movie name (in case of movie soundtracks), etc. Assume that there are, say, $2000$ attributes across all possible genres.
The aim is to identify attributes that characterize subgenres of these genres. So of course, let's say for rock I can just collect all the attributes for all rock songs, but even that set of attributes may be too broad to characterize the rock genre - maybe there are some that are specific to subgenres and so I won't have the desired level of granularity.
Note that for the purpose of this example, I'm not assuming that I already know the subgenres a priori. For example, I'm not going to categorize songs into subgenres like post rock, folk rock, etc. and then pick out attributes characterizing them. I want to discover subgenres on the basis of clustering, if that makes sense.
In a nutshell, about a million songs belonging to $50$ genres and all songs collectively have $2000$ attributes. So for each song I'll make a vector in $\mathbb{R}^{2000}$ - each dimension corresponds to an attribute. If that attribute is present for that song, the corresponding element of the vector is $1$, otherwise $0$ (e.g. a jazz-related attribute will be $0$ for a rock song). Now I want to do genre-wise clustering. For each genre, not only do I want to cluster songs of that genre into groups, but I also want to get an idea which attributes are the most important to characterize individual groups.
On the basis of this clustering, I can identify subgenres (e.g. of rock music) that I can characterize using a subset of the $2000$ attributes. 
My first question is: is there a better way to initialize the problem than forming 2000-dimensional vectors of ones and zeros?
Secondly, given the vast number of dimensions and examples, what clustering methods could be tried? From what I've surveyed, there are graph-based clustering methods, hierarchical, density-based, spectral clustering and so on. Which of these would be best for the toy example? I've heard that one can project the points on to a lower-dimensional subspace, then do clustering. But I also want which attributes define different clusters. Since attributes are encoded in the dimensions, with dimensionality reduction techniques I'll lose information about the attributes. So now what?
","['clustering', 'dimensionality-reduction']",
Image classification with an associated matrix,"
I have a dataset of images with 9 different classes. However, there are different categories with the same type of associated image and only can be differentiated with an associated matrix in my specific problem.
I want to train a neural network with the images and the associated matrix as inputs. What type of architecture is good to use? Or where can I find bibliography about it?
","['neural-networks', 'machine-learning', 'deep-learning', 'convolutional-neural-networks', 'ai-design']","The topic is multimodal neural networks.Here some repositories that i hope will help me a lot https://docs.google.com/presentation/d/1z8-GeTXvSuVbcez8R6HOG1Tw_F3A-WETahQdTV38_uc/edit#slide=id.g1ea5aac985_0_892https://github.com/prml615/prmlhttps://github.com/husseinmozannar/multimodal-deep-learning-for-disaster-responsehttps://github.com/guillaume-be/multimodal-avitoThanks to kbrose, his suggestion led me to this type of architectures."
Handwritten digits recognition during the process of writing,"
I know how to train a NN for recognizing handwritten digits (e.g. using the MNIST database).
I'm wondering how to accomplish the same ""online"", which is during the process of writing e.g. I'm writing down a digit, and during the process and while keeping ""the pen down"", the NN recognizes the digit.
To make it simpler, I could assume one stroke-order only for each digit (e.g. each digit is written with a certain order of strokes, like for digit 1 is just a vertical line going up to down).
Which is the most suitable NN for the purpose and how to accomplish this?
","['neural-networks', 'handwritten-characters']",
"In Monte Carlo learning, what do you do when an end state is reached, after having recorded the previously visited states and taken actions?","
When you train a model using Monte Carlo-based learning the state and action taken at each step is recorded, and then at some point an end state is reached and the agent receives some reward - what do you do at that point?
Let's say there were 100 steps taken to reach this final reward state, would you update the full rollout of those 100 state/action/rewards and then begin the next episode, or do you then 'bubble up' that final reward to the previous states and update on those as well?
E.g. 

Process an update for the full 100 experiences. Can either stop here, or...
Bubble up the final reward to the 99th step and process an update for the 99 state/action/reward.
Bubble up the final reward to the 98th step and process an update for the 98 state/action/reward.
and so on right the way to the first step...

Or, do you just process an update for the full 100-step roll-out and that's it?
Or perhaps these are two different approaches?  Is there a situation where you'd one rather than the other?
","['reinforcement-learning', 'monte-carlo-methods']","I am assuming you are asking about Monte Carlo simulation for value estimates, perhaps as part of a Monte Carlo control learning agent.The basic approach of all value-based methods is to estimate an expected return, often the action value $Q(s,a)$ which is a sum of expected future reward from taking action $a$ in state $s$. Monte Carlo methods take a direct and simple approach to this, which is to run the environment to the end of an episode and measure the return. This return is a sample out of all possible returns, so it can just be averaged with other observed returns to obtain an estimate. A minor complication is that the return depends on the current policy, and in control scenarios that will change, so the average needs to be recency-weighted for control e.g. using a fixed learning rate $\alpha$ in an update like $Q(s,a) \leftarrow Q(s,a) + \alpha(G - Q(s,a))$Given this, you can run pretty much any approach that calculates the returns from observed state/action pairs. You will find that the ""bubble up"" approach is used commonly - the process usually termed backing up - working backwards from the end of the episode.If you have an episode from $t=0$ to $t=T$ and records of states, actions, rewards $s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3 . . . s_{T-1}, a_{T-1}, r_T, s_T$ (note indexing, reward follows state/action, there is no $r_0$ and no $a_T$), then the following algorithm could be used to calculate individual returns $g_t$:$g \leftarrow 0$for $t = T-1$ down to $0$:$\qquad g \leftarrow r_{t+1} + \gamma g$$\qquad Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha(g - Q(s_t,a_t))$This working backwards is an efficient way to process rewards and assign them with discounting to action values for all state, action pairs observed in the episode.Or perhaps these are two different approaches?It would be valid to calculate only the return for the first state/action, and randomly select state/actions to start from (called exploring starts). Or in fact take any arbitrary set of estimates generated this way. You don't have to use all return estimates, but you do need to have an algorithm that is guaranteed to update values of all state/action pairs in the long term.Is there a situation where you'd one rather than the other?Most usually you will see the backed up return estimates to all observed state/action pairs, as this is more sample efficient, and Monte Carlo is already a high variance method that requires lots of samples to get good estimates (especially for early state/action pairs at the start of long episodes).However, if you work with function approximation such as neural networks, you need to avoid feeding in correlated data to learn from. A sequence of state/action pairs from a single episode are going to be correlated. There are a few ways to avoid that, but one simple approach could be to take just one sample from each rollout. You might do this if rollouts could be run very fast, possibly simulated on computer. But other alternatives may be better - for instance put all the state, action, return values into a data set, shuffle it after N episodes and learn from everything."
How to express accuracy of a regression ANN that uses MSE loss function?,"
I have a regression MLP network with all input values between 0 and 1, and am using MSE for the loss function. The minimum MSE over the validation sample set comes to 0.019. So how to express the 'accuracy' of this network in 'lay' terms? If RMSE is 'in the units of the quantity being estimated', does this mean we can say: ""The network is on average (1-SQRT(0.019))*100 =  86.2% accurate""? 
Also, in the validation data set, there are three 'extreme' expected values. The lowest MSE results in predicted values closer to these three values, but not as close to all the other values, whereas a slightly higher MSE results in the opposite - predicted values further from the 'extreme' values but more accurate relative to all other expected values (and this outcome is actually preferred in the case I'm dealing with). I assume this can be explained by RMSE's sensitivity to outliers? 
","['neural-networks', 'machine-learning', 'regression', 'accuracy', 'mean-squared-error']",
Can an RL algorithm trained in one environment be successful in a different one?,"
Can an RL algorithm trained in one environment be successful in a different one?
For example, if I train a model to go through one labyrinth, could this model also go through a different but similar labyrinth or would it need a new training process?
By similar, I mean like these two:


But with this one being not similar:

","['reinforcement-learning', 'environment']","Can an RL algorithm trained in one environment be succesfull on a different one? Strictly the answer here is ""no"". You train an agent to solve a single environment. If a second environment is similar enough, you can probably re-train the agent on the new environment with little or no changes to the agent, and to get results just as good in the second agent. In some cases, you could even start with the trained agent and ""fine tune"" it to the new environment. That probably would not work well with the labyrinth example though, especially if the start and end points are moved.Example: If I train a model to go through one labyrinth, could this model also go through different but similar labyrinth or would it need a new training process?This could be subtly different. It is down to how you define the environment. It is possible to define an environment not as ""this maze"" but for example ""all possible mazes in a 30x20 grid pattern"". To do so, you need to make the maze configuration part of the environment's state. Expanding from a singular environment example to all similar environments has a cost. In the labyrinth example, this is significant. A single 30x20 maze has 600 states, which is a trivial problem in RL. All possible mazes of the same size has closer to $2^{1200}$ states, which is much larger and requires different kinds of approaches, different RL methods.You would expect to train on many example mazes (typically a randomly generated new one for each episode) and use some kind of generalisation - e.g. a neural network, maybe a CNN due to the grid design - when handling the value function or policy function. Training time for a single maze will be under a second, and the policy will be perfect. Training time for all possible mazes measured in hours or days and the policy will still get things wrong from time-to-time."
What are criteria for ML model to be satisfactory for commercial use?,"
I often read ""the performance of the system is satisfactory"" or "" when your model is satisfactory"". 
But what does it mean in the context of Machine Learning? 
Are there any clear and/or generic criteria for Machine Learning model to be satisfactory for commercial use? 
Is decision what model to choose or whether additional model adjustments or improvements are needed based on data scientist experience, customer satisfaction or benchmarking academic or market competition results?
","['machine-learning', 'applications']","The answer is ""when it works well enough to perform the task that you have set it"".
It is a good idea to set your performance criteria in advance so that you can clearly identify the goal that you are trying to achieve and also so that you will know if the model is likely to be successful or not. "
Improving Recall of a Certain Class,"
Let's say that we have a test data set with $20,000$ observations for which we want to make a binary prediction for. When we apply our best trained model to this data set (e.g. logistic regression with threshold = 0.5, data_size = 4000 rows, 5 fold cv), only about $1 \%$ of the predictions are positive. That is, $p(\text{positive}) \geq 0.5$ is true only for about $1 \%$ of the predictions. We expect many more positives since the recall of the positive class of the best trained model is about $40 \%$. If we manually lower the threshold to $0.45$, then about $10 \%$ of the predictions are positive. Assume that the $20,000$ observations come from the same distribution as the training/validation data and are independent samples.
Questions.

Why would a a model with decent recall for the positive class predict very few positives in the out of sample data? 
If (1) is true, then is it appropriate to lower the threshold for positive (e.g. $0.5$ to $0.45$) to increase the number of predicted positives in the test set?

","['prediction', 'performance', 'hyperparameter-optimization']",
CBIR Evaluation on contextually different data,"
How good would a CBIR system trained on a dataset, for example, DELF, trained on the Google Landmarks dataset, perform when evaluated on a contextually different dataset such as the WANG or the COREL dataset without retraining?
","['neural-networks', 'deep-learning', 'image-processing', 'feature-extraction']",
Why isn't my Neural Network based calculator working?,"
I am playing around with neural networks in Tensorflow and I figured an interesting test would be whether I can write a calculator using a Tensorflow Neural Network.
I started with simple addition and it kinda worked (so given 2, 4 it would get around 5.9 or 6.1).
Then I wanted to add the ability to calculate using ""+"", ""-"", and ""*"".
Here is the code I came up with in the end:
import numpy as np
import tensorflow as tf
from random import randrange

def generate_input(size):
    nn_input = []
    for i in range(0,size):
        symbol = float(randrange(3))
        nn_input.append([
                float(randrange(1000)),
                float(randrange(1000)),
                1 if symbol == 0 else 0,
                1 if symbol == 1 else 0,
                1 if symbol == 2 else 0,
                ])
    return nn_input

def generate_output(input_data):
    return [[generate_single_output(i)] for i in input_data]

def generate_single_output(input_data):
    plus = input_data[2]
    minus = input_data[3]
    multiplication = input_data[4]

    if (plus):
        return input_data[0] + input_data[1]

    if (minus):
        return input_data[0] - input_data[1]

    if (multiplication):
        return input_data[0] * input_data[1]

def user_input_to_nn_input(user_input):
    symbol = user_input[1]
    return np.array([[
            float(user_input[0]),
            float(user_input[2]),
            1 if symbol == '+' else 0,
            1 if symbol == '-' else 0,
            1 if symbol == '*' else 0,
            ]])


if __name__ == '__main__':
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(5,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1),
        ])

    model.compile(tf.keras.optimizers.RMSprop(0.001), loss=tf.keras.losses.MeanSquaredError())


    input_data = np.array(generate_input(10000))
    output_data = np.array(generate_output(input_data))

    model.fit(input_data, output_data, epochs=20)

    while True:
        user_calculation = input(""Enter expression (e.g. 2 + 3):"")
        user_input = user_calculation.split()
        nn_input = user_input_to_nn_input(user_input)
        print(model.predict(nn_input)[0][0])


The idea is built on this tutorial: https://www.tensorflow.org/tutorials/keras/basic_regression
The input is 5 fields: number 1, number 2, plus?, minus?, multiplication?
Where the last 3 are simply 1 or 0 depending on whether that is the calculation I am trying to do.
As an output for say [1,4,1,0,0] I would expect 1 + 4 = 5
for [1,4,0,1,0] I would expect 1 - 4 = -3 etc.
For some reason though the numbers I am getting are completely off and seem random.
Basically I am trying to understand what I went wrong?
The data being input to the NN seems correct and the model is based on the model used in the tutorial I quoted (and the problems seem fairly similar so I expect if one would work the other would too).
","['neural-networks', 'tensorflow']","A neural network is not good at selecting a function based on those 3 input parameters, because of the way a neuron is setup.What you should do is either make a neural network for each operation, or use different input neurons for each operation. E.g. 2 input neurons for the addition operation, 2 for the multiplication, and 2 for the minus. 6 inputs in total of which 4 will always be 0. This will make it easier for the neural network to calculate the result."
Can ML be used to curve fit data based on dataset of example fits?,"
Say I have x,y data connected by a function with some additional parameters (a,b,c):
$$ y = f(x ; a, b, c) $$
Now given a set of data points (x and y) I want to determine a,b,c. If I know the model for $f$, this is a simple curve fitting problem. What if I don't have $f$ but I do have lots of examples of y with corresponding a,b,c values? (Or alternatively $f$ is expensive to compute, and I want a better way of guessing the right parameters without a brute force curve fit.) Would simple machine-learning techniques (e.g. from sklearn) work on this problem, or would this require something more like deep learning?
Here's an example generating the kind of data I'm talking about:
import numpy as np                                                                                                                                                                                                                           
import matplotlib.pyplot as plt                                                                                                                                                                                                              

Nr = 2000                                                                                                            
Nx = 100                                                                                                             
x = np.linspace(0,1,Nx)                                                                                              

f1 = lambda x, a, b, c : a*np.exp( -(x-b)**2/c**2) # An example function                                             
f2 = lambda x, a, b, c : a*np.sin( x*b + c)        # Another example function                                        
prange1 = np.array([[0,1],[0,1],[0,.5]])                                                                             
prange2 = np.array([[0,1],[0,Nx/2.0],[0,np.pi*2]])                                                                   
#f, prange = f1, prange1                                                                                             
f, prange = f2, prange2                                                                                              

data = np.zeros((Nr,Nx))                                                                                             
parms = np.zeros((Nr,3))                                                                                             
for i in range(Nr) :                                                                                                 
    a,b,c = np.random.rand(3)*(prange[:,1]-prange[:,0])+prange[:,0]                                                  
    parms[i] = a,b,c                                                                                                 
    data[i] = f(x,a,b,c) + (np.random.rand(Nx)-.5)*.2*a                                                              

plt.figure(1)                                                                                                        
plt.clf()                                                                                                            
for i in range(3) :                                                                                                  
    plt.title('First few rows in dataset')                                                                           
    plt.plot(x,data[i],'.')                                                                                          
    plt.plot(x,f(x,*parms[i]))                                                                                       


Given data, could you train a model on half the data set, and then determine the a,b,c values from the other half?
I've been going through some sklearn tutorials, but I'm not sure any of the models I've seen apply well to this type of a problem. For the guassian example I could do it by extracting features related to the parameters (e.g. first and 2nd moments, %5 and .%95 percentiles, etc.), and feed those into an ML model that would give good results, but I want something that would work more generally without assuming anything about $f$ or its parameters.
","['machine-learning', 'deep-learning', 'scikit-learn']",
How well can a CNN distinguish an object from its class?,"
A convolutional neural network (CNN) can easily predict the class of an object in an image. 
Can a CNN distinguish the Pisa Tower from other buildings, or Hagia Sophia from other mosques easily? If it can, how many training images can be sufficient? Do I need thousands of training images of that specific thing to distinguish it?
(This is a term project recommendation about deep neural networks, so I need to understand its feasibility.)
","['deep-learning', 'convolutional-neural-networks', 'computer-vision']","To help you understand the feasibility of your project these posts could be a good start:
https://datascience.stackexchange.com/questions/13181/how-many-images-per-class-are-sufficient-for-training-a-cnnenter link description herehttps://stats.stackexchange.com/questions/226672/how-few-training-examples-is-too-few-when-training-a-neural-networkThat being said, the short answer would be it depends. It depends how precise you want to be, what is the difficulty of the task, the infrastructure you have for the training etc.For the images, you should not worry, you could start with the ImageNet dataset:
For construction and buildings:
http://www.image-net.org/explore?wnid=n04341686For mosquees:
http://www.image-net.org/synset?wnid=n03788195You can then use data augmentation techniques to enhance the size of you training set. Here is a library I have used in the past which helped me greatly to achieve this task: https://github.com/aleju/imgaugHope that helps!"
"How can the derivative of a neural network be calculated, given no mathematical expression?","
Neural networks (NNs) are used as approximators in reinforcement learning (RL). To update the policy in RL, the actor network's gradients w.r.t its weights are needed. Since NN doesn't have a mathematical expression to work with, how can its derivatives be calculated?
","['neural-networks', 'reinforcement-learning', 'backpropagation', 'actor-critic-methods', 'policy-gradients']","I think what you mean to ask is how can differentiation occur when there's no obvious neural network function to differentiate?Don't worry - lots of people get confused about this, because it seems like an obvious hole in the puzzle. As mentioned by @AtillaOzgur, neural networks use partial differentiation through backpropagation. First, take the output of all the neurons (except the one you're about to differentiate by) as a function:The above diagram represents the output of one neuron. Do this for every neuron in your network until you have a set. Let's call this set function NN. The output of NN (given all your neuron outputs) is what you'd normally plug into your RL policy.You then differentiate NN by a single neuron (n) as shown:$$\frac{\partial NN}{\partial n} = \lim_{h\to0} \left(\frac{NN(\text{all other neuron outputs}, n + h) - NN(\text{all other neuron outputs}, n)}{h} \right)$$In reality however, it's the partial derivative of the activation function (A) with respect to the output of a single neuron (n):$$\frac{\partial A}{\partial n}$$So, depending on your activation function, you just plug in your neuron output to a certain expression and you've found the value by which to update your neural network.I hope this helps. Deep learning is definitely a field with a learning curve, but places like StackExchange are great resources to help you out."
Can maximum likelihood be used as a classifier?,"
I am confused in understanding the maximum likelihood as a classifier. I know what is Bayesian network and I know that ML is used for estimating the parameters of models. Also, I read that there are two methods to learn the parameters of a Bayesian network: MLE and Bayesian estimator.
The question which confused me are the following.

Can we use ML as a classifier? For example, can we use ML to model the user's behaviors to identify the activity of them? If yes, How? What is the likelihood function that should be optimized? Should I suppose a normal distribution of users and optimize it?
If ML can be used as a classifier, what is the difference between ML and BN to classify activities? What are the advantages and disadvantages of each model?

","['classification', 'bayesian-networks', 'maximum-likelihood']",
"Network doesn't converge with ReLU or Leaky ReLU, but works well with sigmoid/tanh","
I have these training data to separate, the classes are rather randomly scattered:

My first attempt was using tf.nn.relu activation function, but output was stuck with whatever number of training steps. So I guessed it could be because of dead ReLU units, thus I changed the activation function in hidden layers to tf.nn.leaky_relu, but it's still no good.
It works when all hidden layers come with tf.sigmoid, yes, but why doesn't ReLU work here? Is it because of dead ReLU units, or exploding gradients, or anything else?
Source code (TensorFlow):
#core
import time;

#libs
import tensorflow        as tf;
import matplotlib.pyplot as pyplot;

#mockup to emphasize value name
def units(Num):
  return Num;
#end def

#PROGRAMME ENTRY POINT==========================================================
#data
#https://i.imgur.com/uVOxZR7.png
X = [[1,1],[1,2],[1,3],[2,1],[2,2],[2,3],[3,1],[3,2],[3,3],[4,1],[4,2],[4,3],[5,1],[6,1]];
Y = [[0],  [1],  [0],  [1],  [0],  [1],  [0],  [2],  [1],  [1],  [1],  [0],  [0],  [1]  ];
Max_X      = 6;
Max_Y      = 2;
Batch_Size = 14;

#normalise
for I in range(len(X)):
  X[I][0] /= Max_X;
  X[I][1] /= Max_X;
  Y[I][0] /= Max_Y;
#end for

#model
Input     = tf.placeholder(dtype=tf.float32, shape=[Batch_Size,2]);
Expected  = tf.placeholder(dtype=tf.float32, shape=[Batch_Size,1]);

#RELU DOESN'T WORK, DEAD RELU? SIGMOID WORKS BUT SLOW.
#CHANGE TO tf.sigmoid OR tf.tanh AND IT WORKS:
activation_fn = tf.nn.leaky_relu;

#1
Weight1   = tf.Variable(tf.random_uniform(shape=[2,units(60)], minval=-1, maxval=1));
Bias1     = tf.Variable(tf.random_uniform(shape=[  units(60)], minval=-1, maxval=1));
Hidden1   = activation_fn(tf.matmul(Input,Weight1) + Bias1);

#2
Weight2   = tf.Variable(tf.random_uniform(shape=[60,units(50)], minval=-1, maxval=1));
Bias2     = tf.Variable(tf.random_uniform(shape=[   units(50)], minval=-1, maxval=1));
Hidden2   = activation_fn(tf.matmul(Hidden1,Weight2) + Bias2);

#3
Weight3   = tf.Variable(tf.random_uniform(shape=[50,units(40)], minval=-1, maxval=1));
Bias3     = tf.Variable(tf.random_uniform(shape=[   units(40)], minval=-1, maxval=1));
Hidden3   = activation_fn(tf.matmul(Hidden2,Weight3) + Bias3);

#4
Weight4   = tf.Variable(tf.random_uniform(shape=[40,units(30)], minval=-1, maxval=1));
Bias4     = tf.Variable(tf.random_uniform(shape=[   units(30)], minval=-1, maxval=1));
Hidden4   = activation_fn(tf.matmul(Hidden3,Weight4) + Bias4);

#5
Weight5   = tf.Variable(tf.random_uniform(shape=[30,units(20)], minval=-1, maxval=1));
Bias5     = tf.Variable(tf.random_uniform(shape=[   units(20)], minval=-1, maxval=1));
Hidden5   = activation_fn(tf.matmul(Hidden4,Weight5) + Bias5);

#out
Weight6   = tf.Variable(tf.random_uniform(shape=[20,units(1)], minval=-1, maxval=1));
Bias6     = tf.Variable(tf.random_uniform(shape=[   units(1)], minval=-1, maxval=1));
Output    = tf.sigmoid(tf.matmul(Hidden5,Weight6) + Bias6);

Loss      = tf.reduce_sum(tf.square(Expected-Output));
Optimiser = tf.train.GradientDescentOptimizer(1e-1);
Training  = Optimiser.minimize(Loss);

#training
Sess = tf.Session();
Init = tf.global_variables_initializer();
Sess.run(Init);

Feed   = {Input:X, Expected:Y};
Losses = [];
Start  = time.time();

for I in range(10000):
  if (I%1000==0):
    Lossvalue = Sess.run(Loss, feed_dict=Feed);
    Losses   += [Lossvalue];
    
    if (I==0):
      print(""Loss:"",Lossvalue,""(first)"");
    else:
      print(""Loss:"",Lossvalue);
  #end if
  
  Sess.run(Training, feed_dict=Feed);
#end for

Lastloss = Sess.run(Loss, feed_dict=Feed);
Losses  += [Lastloss];
print(""Loss:"",Lastloss,""(last)"");

Finish = time.time();
print(""Time:"",Finish-Start,""seconds"");

#eval
print(""\nEval:"");
Evalresults = Sess.run(Output,feed_dict=Feed).tolist();
for I in range(len(Evalresults)):
  Evalresults[I] = [round(Evalresults[I][0]*Max_Y)];
#end for
print(Evalresults);
Sess.close();

#result: diagram
print(""\nLoss curve:"");
pyplot.plot(Losses,""-bo"");
#eof

","['tensorflow', 'gradient-descent', 'activation-functions', 'relu', 'sigmoid']","I don't think it is dead ReLU units as a main cause, although they may be happening as part of the NN failing.The NN architecture is too complex for the given task (too deep, too many neurons) and that means that any problems you have with other design choices will tend to get amplified. It could be that your NN is close to diverging on the given data and architecture, and that sigmoid is more resilient to it. I'd suggest the following changes:Dropping the learning rate, try 0.01 or even 0.001Normalising the two input features. NNs like to work with data that is mean 0, standard deviation 1, although there is some flexibility here, your values ranging from 0 to 6 are probably starting to cause minor problemsLook at standard initialisation routines for weights, available within TensorFlow framework, such as Glorot uniform. Your random -1 to +1 is probably too high a range for the given network, and NNs - especially ""deep"" NNs with 3+ hidden layers - are very sensitive to how initial weights are set.Simplify the network architecture a little. Five hidden layers and 200 neurons seems a bit much for your goal of over-fitting to this small data set. Try something like 3 hidden layers and 50 neurons.Your output layer and loss function are designed for a regression task, but you mention that the goal is to identify classes. You need a softmax layer and multiclass log loss for predicting exclusive classes."
"is a ""word prediction"" problem, applicable using HMMs? [closed]","







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed last year.







                        Improve this question
                    



I'm learning HMMs and decided to model a problem for learning purposes. I came to this idea of word predicting by letters.
here is the model :
while typing, the word is typed letter by letter, so we can consider them as series of observations.
let's say we have just 4 words in our database:

Tap
Trip
Trap
Trigger

and we want to predict the word after 1,2 or 3 written letters.
we have to define states and HMM parameters (state transitions, emissions and priors).
our [hidden ?] states would be :

[ ][ ][ ][ ][ ][ ][ ] : no observations. I chose 7 [ ] because of the longest word
T [ ][ ][ ][ ][ ][ ]
T A [ ][ ][ ][ ][ ]
T R [ ][ ][ ][ ][ ]
â€¦ .

and we have to learn the transition probabilities of each state pairs (the pairs which come after each other like T,TR and TA. but not  for TA and TR).
our prior probabilities are 1/3 because we have 3 words. but we may change it by learning which word is used more frequently.

Now, I have these questions :

is HMM suitable for this kind of problem ?
are my assumptions (about states and prior probabilities) correct ?
the states get out of hand when the word count increases. making the model very complex. is that deduction true? 
how the emission probabilities are defined in this model ?
what do I miss in case of parameters or definitions ?

I'm a new contributor, be nice to me. regards :) 
",['hidden-markov-model'],"I think a HMM is overkill for this problem. You kind of have 'hidden' states, but they are very limited and dependent on the full sequence of previous states, which you probably want to avoid to make best use of the HMM's features. It also, as you rightly say, leads to a proliferation in states: each dictionary item adds as many states as it has letters to the model.The way I would approach this it to use a trie which contains your dictionary, and then you traverse the trie as the user types characters. At each point you have a sub-trie which gives you the possible completions of the word. You could even (if available) augment this with probabilities to guess the most likely word (though this does not take into account the previous word in the sentence, which might be more useful).If you want to learn about HMMs, one possible application would be a weather forecast (I thought this was an example Lawrence Rabiner uses in his tutorial, but it's actually on the HMM Wikipedia page). Or, if you want to work with texts, use parts-of-speech tagging. You want to have something where each observation can belong to several possible states (eg light can belong to adj, noun, and verb). Here you would have the words as observations and the part-of-speech tags as states, which keeps the model at a reasonably small size (and is thus easier to train)."
Are there RL techniques to deal with incremental action spaces?,"
Let's say we have a problem that can be solved by some RL algorithms (DQN, for example, because we have discrete action space). At first, the action space is fixed (the number of actions is $n_1$), and we have already well trained an offline DQN model. Later, we have to add more actions for some reasons (and the number of action is now $n_2$, where $n_2 > n_1$).
Are there some solutions to update the value function or policy (or the neural network) with only minor changes?
","['reinforcement-learning', 'reference-request', 'action-spaces']",
Probabilistic classification - normalize results,"
I have a probabilistic classifier that produces a distribution over my 3 classes - C1, C2, C3.
I want to compare some new points I'm classifying to each other, to see which one is the best fit for a specific class.
for example:.
for a new point X1 the classifier will output something like [0.2, 0.2, 0.6]
for another new point X2 it will produce [0.2, 0.4, 0.4] 
so for both X1, X2 - the chosen class would be C3.
Now I want to know - which of X1, X2 is a better fit to C3
I cannot simply choose the one with the highest probability for C3, because it's probability for C3 depends on it's probabilities for the other classes. X1 got 0.6 and X2 0.4, but it's possible that X2 is closer to C3 in the hyper-plane than X1, it is just less unique for C3 than X1, and therefor X1 got a higher probability. 
here's a visual, in 2 dimensions:

X2, who got a lower probability, is clearly a better fit to the Red class then X1, which is truly unique to the Red class, but is further from the class cluster
My questions are:

how do I normalize the results of a probabilistic classifier so I can compare predictions to each other?
given an output of a probabilistic classier - how can I get the actual distance from the probabilities. It must be possible because there's an exact mapping between a set of probabilities to a point in the classified hyper-plan.

Thanks a lot!
Amir 
","['classification', 'probability', 'probability-distribution']",
How is the actor-critic algorithm guaranteed to converge?,"
From my understanding, the critic evaluates the policy (actor) following dynamic programming (DP) or approximate dynamic programming (ADP) scheme, which should converge to the optimal value function after sufficient iterations. The policy (actor) then updates its parameter w.r.t the optimal value function using gradient methods. This policy evaluation and improvement circle are repeated until neither the critic nor the actor changes anymore. 
How's guaranteed to converge as a whole? Is there any mathematical proof? Is it possible that it may converge to a local optimal point instead of a global one?
","['reinforcement-learning', 'actor-critic-methods', 'convergence']",
"When we are working on an AI project, does the context (academia, industry or competition) make the process different?","
When we are working on an AI project, does the domain/context (academia, industry, or competition) make the process different?
For example, I see in the competition most participants even winners use the stacking model, but I have not found anyone implementing it in the industry. How about the cross-validation process, I think there is a slight difference in industry and academia.
So, does the context/domain of an AI project will make the process different? If so, what are the things I need to pay attention to when creating an AI project based on its domain?
","['ai-design', 'academia']",I cannot comment about the process for AI for academia. I can compare AI for competitions and AI for business. To clarify whatever I say is about ML not any other AI techniques. The process might be different for other techniques. But most of things that I say are general enough that I am assuming should still apply.The main difference that I saw while doing ML for a competition vs. for a business was that of focus. When doing it for a competition for Kaggle the focus was mainly creating the modelWhen doing it for business what is differentHope this gives some idea about the differences in AI for competitions and AI for business.
Why do both sine and cosine have been used in positional encoding in the transformer model?,"
The Transformer model proposed in ""Attention Is All You Need"" uses sinusoid functions to do the positional encoding. 
Why have both sine and cosine been used? And why do we need to separate the odd and even dimensions to use different sinusoid functions?
","['deep-learning', 'papers', 'transformer', 'machine-translation', 'positional-encoding']",
Is Value Iteration better than Policy Iteration for first few iterations?,"
In Policy Iteration (PI), the action generated by the policy, whether it's optimal or not w.r.t the current value function $v(s)$. Whereas, in Value Iteration, the action is greedily generated w.r.t current $v(s)$, which is an approximation of the objective function (as I understand). As a consequence, in the first few iterations, will Value Iteration perform better than Policy Iteration?
","['reinforcement-learning', 'comparison', 'value-iteration', 'policy-iteration']",
Can policy iteration use only the immediate reward for updates?,"
Is it still a policy iteration algorithm if the policy is updated optimizing a function of the immediate reward instead of the value function?
","['reinforcement-learning', 'value-iteration', 'policy-iteration']","Is it still a policy iteration algorithm if the policy is updated optimizing a function of the immediate reward instead of the value function?Technically yes.The value update step in Policy Iteration is:$$v(s) \leftarrow \sum_{r,s'}p(r,s'|s,\pi(s))(r + \gamma v(s'))$$The discount factor $\gamma$ can be set to $0$, making the update:$$v(s) \leftarrow \sum_{r,s'}p(r,s'|s,\pi(s))r$$However, there are two key details that are important, and make this a technically yes, not some alternative way of solving problems:Changing discount factor $\gamma$ changes what it means for an agent to act optimally. Setting it to zero means that the agent will prioritise only its immediate reward signal, and make no long-term decisions at all. This would be useless for instance if the task was to escape a maze in minimum time.Technically there is still a value function being updated. The function $v(s)$ is still the expected future reward, just we have set it to only care a very short step into the future. So short that it doesn't care what the value of the next state is, so the next state does not appear in any updates.Due to the lack of bootstrapping between states, all the data for optimal behaviour is already available in the reward distribution. So the entire MDP can be solved with a single sweep through all the states. Or it could just be solved on-demand using $\pi(s) = \text{argmax}_a[\sum_{r,s'}p(r,s'|s,a)r]$ for any state, making a policy iteration process redundant.However, with these caveats in mind, yes this is still policy iteration. It is the same update process, just with a particular choice of one of the parameters."
How to calculate the false positives and negatives?,"
I have a huge amount of data and I want to calculate my false positive and false negative. Is there a software that can help me determine it?
","['deep-learning', 'reference-request', 'data-science', 'statistical-ai']",
"Why will the sigmoid function be 1 (and 0), if we use a fully connected layer that produces a big enough positive (or negative, respectively) output?","
I am using a fully connected neural network that uses a sigmoid activation function. If we feed a big enough input, the sigmoid function will finally become 1 or 0. Is there any solution to avoid this?
Will this lead to classical sigmoid problems vanishing gradient or exploding gradient?
","['neural-networks', 'machine-learning', 'deep-learning', 'activation-functions', 'sigmoid']",
How important is the choice of the initial state?,"
Is it crucial to always have the same initial (starting) state for Reinforcement Learning, for example, for Q-learning or DQN?
Or it can vary?
","['reinforcement-learning', 'dqn']","The initial state can vary during in both training and use, and how you decide to do this makes very little difference to Q-learning. The important factor is whether all state/action pairs relevant to optimal behaviour can be reached. As there is already randomness in any exploring policy, and in many environments as part of state transitions and reward functions, adding some more at the start is not an issue.More formally, you can take any existing environment with states $S_1, S_2, S_3 ... S_n$ plus defined actions and rewards. Add a special fixed start state $S_{0}$ with one special action $A_{0}$ allowed. Make the state transition matrix following that action any distribution you like over the other states, and reward $0$. It is clearly a valid MDP, and is identical to your original MDP in terms of value and policy functions for states $S_1, S_2, S_3 ... S_n$. For all intents and purposes to the agent (which gets no meaningful policy choice in $S_0$), it starts in the original MDP in some randomly chosen state.That is as long as the variations are not made to deliberately exclude some of the state space during training that it would need later, or otherwise used to ""attack"" the agent and prevent it learning. Q-learning proofs of convergence assume that all state/action pairs are reachable an infinite number of times in the limit of infinite training time. Of course in practice this is never achieved, but clearly if you excluded some important state from ever being seen at the start by choosing to start in a way that it is never reachable, then the agent would never learn about it.You can use a highly variable start state for training to your advantage. It is a form of exploration, and truly exploring starts (that could start from any state/action pair, so you would want to randomise the first action choice) allow you to skip any further exploration and still guarantee convergence to an optimal policy - i.e. you could learn Q values on-policy using a deterministic policy. This will not necessarily make things converge faster, but does go to demonstrate that random start states are not a barrier to RL.There are a couple of minor/partial exceptions to ""do what you like"":If the environment is otherwise highly deterministic, adding a random start may increase the difficulty in optimising it, as more states will be reachable, even with an optimal policy, therefore the state space over which optimal behaviour needs to be discovered may be larger and take longer to learn.Policy Gradient methods, like REINFORCE, optimise the policy towards maximising return from a known distribution of start states. So you can vary start state, but you should really stick to a standard start procedure, including a fixed distribution of allowed start states. Similarly, if you want to report a standard score for your agent, it is common to quote the expected return from the start states. Again that doesn't mean you have a fixed start state, but you should use a fixed distribution of start states to assess your agent.Many standard environments, like Open AI's Gym environments CartPole and LunarLander, include some amount of randomness in the start state, so that the agent has to solve a more general problem than just generating a fixed sequence of actions that always works from the beginning."
Is Mask R-CNN suited to solve a multi-class classification problem where the classes are related?,"
I want to create a model to solve a multi-class classification problem.
Here are more details about my problem.

Every picture contains only one object

The background is very simple

All objects belong to the same family of objects (for example, all objects are knives), but there are different specific subtypes

the model will learn and predict the name of the object (example: the model learn all types of knives, and when it get an image it will tell us the name of the knife)


To be clear, let's say I have 50 types of knives, and the output of the model has to recognize the correct name of the knife. Knife name could be:

Chef's Knife,
Heavy Duty Utility Knife,
Boning Knife, etc.

To solve this problem, I have started to use annotated, segmented (masked) images (COCO-like dataset) and the Mask R-CNN model.
As a first step, I got a prediction, but I really don't know if I'm on the right way.
For this problem, Mask R-CNN could be the solution, or it is impossible to recognize a tiny difference between two objects from the same class (for example Chef's Knife, Heavy Duty Utility Knife)?
","['deep-learning', 'computer-vision', 'object-recognition', 'mask-rcnn', 'multiclass-classification']",
Is it still called linear separation with a layer of more than 1 neuron,"
A single neuron will be able to do linear separation. For example, XOR simulator network:
x1 --- n1.1
   \  /    \
    \/      \
             n2.1 
    /\      /
   /  \    /
x2 --- n1.2 

Where x1, x2 are the 2 inputs, n1.1 and n1.2 are the 2 neurons in hidden layer, and n2.1 is the output neuron.
The output neuron n2.1 does a linear separation. How about the 2 neurons in hidden layer? 
Is it still called linear separation (at 2 nodes and join the 2 separation lines)? or polynomial separation of degree 2?
I'm confused about how it's called because there are curvy lines in this wiki article: https://en.wikipedia.org/wiki/Overfitting


","['deep-learning', 'math', 'deep-neural-networks', 'artificial-neuron', 'linear-regression']","I found out the curvy zigzag green line is not polynomial as if it were polynomial, a vertical line won't cut that curvy line more than 1 time. It's the combination of straight lines (linear separation) of multiple neurons in the same layer. So it's linear separation ('linear' by previous_layer_output*weight, 'separation' by activation function), at multiple nodes."
What are the advantages of the Kullback-Leibler over the MSE/RMSE?,"
I've recently encountered different articles that are recommending to use the KL divergence instead of the MSE/RMSE (as the loss function), when trying to learn a probability distribution, but none of the articles are giving a clear reasoning why the first is better than the others.
Could anyone give me a strong argument why the KL divergence is suitable for this?
","['comparison', 'objective-functions', 'optimization', 'mean-squared-error', 'kl-divergence']","In the context of Variational Inference (VI): the KL allows you to move from the unknown posterior $p(z \mid x)$, to the known joint $p(z,x)=p(x|z)p(z)$ and optimize only the ELBO. You cannot do this with L2.$p(z|x)$ is the desired posterior, of which you cannot calculate the evidence (i.e., using Bayes formula we can set: $p(z|x) = \frac{p(x|z)p(z)}{\int_z p(x|z)p(z)dz}$, and you can't calculate the integral in the denominator [also denoted by $p(x)$] due to it's intractability).Now suppose $q$ is a Variational distribution (e.g. a family of Gaussians which you can control), VI tries to approximates $p(z|x)$ by $q$ by minimizing their KL divergence.$$KL(q(z)||p(z|x)) = \int_z q(z) \log \frac{q(z)}{p(z|x)}dz = \mathbb E_q[\log q(z)]-\mathbb E_q[\log \frac{p(x|z)p(z)}{p(x)}] =$$$$ -\mathbb E_q[\log p(x|z)p(z)] + \mathbb E_q[\log q(z)] + \mathbb E_q[\log p(x)] = -ELBO(q) + \log p(x)
$$Since you're only optimizing $q$ (it's the only thing you can control), you can discard the unknown and difficult to compute normalizing constant $p(x)$.If you would use the (squared) L2 norm you would get:
$$\int_z [q(z)-p(z|x)]^2dz = \int_z [q^2(z)-2q(z)p(z|x)+p^2(z|x)]dz $$
While the 3rd term doesn't depend on q, the 2nd term does, and it also requires $p(x)$ to compute."
What is a non-starving policy in reinforcement learning?,"
In the paper, Eligibility Traces for off-Policy Policy Evaluation (2010), by Doina Precup et al., mentioned the term ""non-starving"" many times. The specific use of the term was like ""non-starving policy"" in the context of off-policy learning. 
A specific mention of the term

we consider a method that requires nothing of the behavior
  policy other than that it be non-starving, i.e., that it
  never reaches a time when some state-action pair is never
  visited again.

What does the thing look like intuitively? Why is it required?
","['reinforcement-learning', 'definitions', 'papers', 'off-policy-methods']","A non-starving policy is a (behavior) policy that is theoretically guaranteed to visit each state and take all possible actions from each state an infinite number of times, so that to always update $Q(s, a)$, $\forall s, \forall a$, an infinite number of times. In the context of off-policy prediction, this criterion implies that any trajectory will have no zero probability under a behavior policy. As a consequence, the experience from the behavior policy sufficiently covers the possibilities of any target policy.An example of a non-starving policy is the $\epsilon$-greedy policy, which, with $0 < \epsilon \leq 1$ (which is usually a small number between $0$ and $1$) probability, takes a random action from a given state, and, with $1-\epsilon$ probability, takes the current best action, that is, the action with the highest value from a given state, according to the current value function."
Which AI algorithm is great for mapping between two XML files,"
My work colleague got a project with a lot of work that is not hard or complicated.  The problem is simple but it is a lot of work. 
We have two XML files with a lot of variables in it. Not only is the XML files flatten but you would have classes with classes in them that can reach an absurd amount of depth. 
The problem comes in that the one file is a request that is received and the other is a response. The request needs to map its variables to the response variables. A simple tool could be built to solve this solution but the problem comes in that there are rules for certain variables. Some variables in the request have arithmetic involve, sometimes with other variables, or some don't get mapped if other variables are present. 
I was thinking about Genetic Programming when I heard about this problem. If all the rules could be defined then it should be able to build a tree that would represent the desired output which is the XML response. 
Will it work and if not do you think there is an AI algorithm that can solve this? 
","['machine-learning', 'problem-solving', 'genetic-programming']","The standard tool to work with XML files is XSLT. You may not need AI to solve this problem. But.. you have to learn how to program with XSLT ;)
On Windows you can use MSXML, if you work from C++ - msxsl.exe, C# has internal supoort for XSLT. That is what I know about. There are also non-MS tools."
Reinforcement learning with hints or reference model,"
In Reinforcement Learning, when I train a model, it comes up with its own set of solutions. For example, if I am training a robot to walk, it will come up with its own walking gait, such as this Deep Mind robot that has learned to walk in a bizarre gait. It can surely walk/run although the movements does not quite look like a human.  
I was wondering how can I train a model by providing it some kind of reference motion data? For example, if I collect motion data from a walking human and then provide it to the training, can the training be made learn the walking movements that looks similar to the reference motion data? 
Searching online I did find some links that shows this is possible. For example, here is a research where the researchers did exactly what I am trying to do, they fed motion data captured from humans to a simulation and made it learn the movements. 
So, my question is: How can I give some kind of hints or reference data to a reinforcement learning model instead of just leaving it all by itself? how exactly is this done? What is it even called? What are some terms and keywords that I can search for to learn more? 
Many thanks in advance
","['machine-learning', 'deep-learning', 'reinforcement-learning', 'training', 'reference-request']",
I am trying to create a network with dynamic connections for every different training example,"
I have developed a new algorithm which has dynamic connections. it is based off of the following paper:
https://www.researchgate.net/publication/334226867_Dynamic_Encoder_Network_to_Process_Noise_into_Images
i have figured out how to effect the dynamism. The pseudo code is in the diagram.
Additionally i would really like to know how to do the rest of the code in terms of a general overview. i would need to have the same deconvolutional layer for all training examples, as well as different current_weight_matrices for the different training examples. so how do i go about adding the dynamic layers to the deconv neural network . Will i have to have many copies of the same network?
","['python', 'algorithm']",
How many hidden layers are needed for this training data set,"
I'm trying to separate classes in 3D space, the data are as in the sketch below:

There are 3 classes: 0,1,2; and with the look into the sketch, it seems that I need 3 planes to separate the classes, thus how many hidden layers should be in the DNN? Any roughly how many neurons in each layer?
Some say the number of hidden layers is roughly the number of separation times, so I put 3 hidden layers and it worked! But any reasons behind that simple measure?
","['neural-networks', 'machine-learning', 'deep-neural-networks', 'hidden-layers', 'regression']","You need to perform Hyperparameter Tuning to identify -There parameters are only related to how you build your model. There are others that relate to training like batch size, number of epochs and so on. Your model's performance ultimately depends on how well you tune your hyperparameters.Also note that hyperparameter tuning is a trial and error task because it depends on several factors that may not be obvious to us. With experience, experts do build certain thumb rules about what may be the right choice, but there is no way to generalize it. ""Some say the number of hidden layers is roughly the number of separation times"" - is just another thumb rule. You simply need to find out what best suits your scenario."
Can two neural networks be better instead of one with a categorical feature?,"
Let's assume, that I have a neural network with few numerical features and one binary categorical feature. The network in this case is used for regression. I wonder if such a neural network can properly adjust to two different states of the categorical feature or maybe training two separate networks, according to these two states can be a better idea in the sense of smaller achievable error(assuming I have enough data for each of the two states)? The new model will use simple 'if statement' on the begining of the regression process and use proper network accordingly.
","['neural-networks', 'regression', 'categorical-data']",
Sliding Window Detection,"
Suppose that we have a labeled training set of $n$ closely cropped images of cars $(x_1, y_1) , \dots, (x_n, y_n)$. We then train a CNN on this. Let's say we have $m$ test images. Then for each of the $m$ images, do we use the trained CNN on a cropped out portion of the box to detect whether there is a car or not? If the object is large, wouldn't having a large sliding window have better performance than a smaller sliding window?
",['convolutional-neural-networks'],
Is it compulsary to normalize the dataset if doing so can negatively impact a Binary Logistic regression performance?,"
I am using raw data set with 4 feature variables (Total Cholesterol, Systolic Blood Pressure, Diastolic Blood Pressure, and Cigraeette count) to do a Binominal Classification (find stroke likelihood) using Logistic Regression Algorithm.
I made sure that the class counts are balanced. i.e., an equal number of occurrences per class.
using Python + sklearn, the problem is that the classification performance gets very negatively-impacted when I try to normalize the dataset using
 X=preprocessing.StandardScaler().fit(X).transform(X)

or
  X=preprocessing.MinMaxScaler().fit_transform(X)

So before normalizing the dataset:
         precision    recall  f1-score   support

      1       0.70      0.72      0.71        29
      2       0.73      0.71      0.72        31

avg / total   0.72      0.72      0.72        60

while after normalizing the dataset (the precision of class:1 decreased significantly)
             precision    recall  f1-score   support
      1       0.55      0.97      0.70        29
      2       0.89      0.26      0.40        31

 avg / total  0.72      0.60      0.55        60

Another observation that I failed to find an explanation to is the probability of each predicted class.
Before the normalization:
 [ 0.17029846  0.82970154]
 [ 0.47796534  0.52203466]
 [ 0.45997593  0.54002407]
 [ 0.54532438  0.45467562]
 [ 0.45999462  0.54000538]

After the normalization ((for the same test set entries))
 [ 0.50033247  0.49966753]
 [ 0.50042371  0.49957629]
 [ 0.50845194  0.49154806]
 [ 0.50180353  0.49819647]
 [ 0.51570427  0.48429573] 

Dataset description is shown below:
       TOTCHOL    SYSBP    DIABP  CIGPDAY   STROKE
count  200.000  200.000  200.000  200.000  200.000
mean   231.040  144.560   81.400    4.480    1.500
std     42.465   23.754   11.931    9.359    0.501
min    112.000  100.000   51.500    0.000    1.000
25%    204.750  126.750   73.750    0.000    1.000
50%    225.500  141.000   80.000    0.000    1.500
75%    256.250  161.000   90.000    4.000    2.000
max    378.000  225.000  113.000   60.000    2.000

SKEW is
TOTCHOL    0.369
SYSBP      0.610
DIABP      0.273
CIGPDAY    2.618
STROKE     0.000

Is there a logical explanation for the decreased precision?
Is there a logical explanation for the very-close-to-0.5 probabilities?
","['machine-learning', 'datasets', 'logistic-regression', 'batch-normalization', 'scikit-learn']",
Do the eigenvectors represent the original features?,"
I've got a test dataset with 4 features and the PCA produces a set of 4 eigenvectors, e.g., 
EigenVectors: [0.7549043055910286, 0.24177972266822534, -0.6095588015369825, -0.01000612689310429]
EigenVectors: [0.0363767549959317, -0.9435613299702559, -0.3290509434298886, -0.009706951562064631]
EigenVectors: [-0.001031816289317291, 0.004364438034564146, 0.016866154627905586, -0.999847698334029]
EigenVectors: [-0.654824523403971, 0.2263084929291885, -0.7210264051508555, -0.010499173877772439]

Do the eigenvector values represent the features from the original dataset? E.g., is feature 1 & 2 explaining most of the variance in eigenvector 1?
Am I interpreting the results correct to say that features 1 and 2 are therefore the most important in my dataset since PC1 represents 90% of the variance?  
I'm trying to map back to the original features but am unsure how to interpret the results. 
","['unsupervised-learning', 'features', 'principal-component-analysis']",
